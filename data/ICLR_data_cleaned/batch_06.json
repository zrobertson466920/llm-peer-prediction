[
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSUPPRESSING THE HETEROGENEITY: A STRONG FEATURE EXTRACTOR FOR FEW-SHOT SEGMENTATION\n\nZhengdong Hu1,2∗, Yifan Sun2, Yi Yang3 † 1 ReLER, Centre for Artificial Intelligence, University of Technology Sydney, Australia 2 Baidu Inc. 3 CCAI, College of Computer Science and Technology, Zhejiang University huzhengdongcs@gmail.com , sunyf15@tsinghua.org.cn , yangyics@zju.edu.cn\n\nABSTRACT\n\nThis paper tackles the Few-shot Semantic Segmentation (FSS) task with focus on learning the feature extractor. Somehow the feature extractor has been overlooked by recent state-of-the-art methods, which directly use a deep model pretrained on ImageNet for feature extraction (without further fine-tuning). Under this background, we think the FSS feature extractor deserves exploration and observe the heterogeneity (i.e., the intra-class diversity in the raw images) as a critical challenge hindering the intra-class feature compactness. The heterogeneity has three levels from coarse to fine: 1) Sample-level: the inevitable distribution gap between the support and query images makes them heterogeneous from each other. 2) Region-level: the background in FSS actually contains multiple regions with different semantics. 3) Patch-level: some neighboring patches belonging to a same class may appear quite different from each other. Motivated by these observations, we propose a feature extractor with Multi-level Heterogeneity Suppressing (MuHS). MuHS leverages the attention mechanism in transformer backbone to effectively suppress all these three-level heterogeneity. Concretely, MuHS reinforces the attention / interaction between different samples (query and support), different regions and neighboring patches by constructing cross-sample attention, cross-region interaction and a novel masked image segmentation (inspired by the recent masked image modeling), respectively. We empirically show that 1) MuHS brings consistent improvement for various FSS heads and 2) using a simple linear classification head, MuHS sets new states of the art on multiple FSS datasets, validating the importance of FSS feature learning.\n\n1\n\nINTRODUCTION\n\nFew-shot semantic segmentation (FSS) aims to generalize the semantic segmentation model from base classes to novel classes, using very few support samples. FSS depicts a potential to reduce the notoriously expensive pixel-wise annotation and has thus drawn great research interest. However, we observe that the current research has been biased towards partial component of the FSS framework. Concretely, an FSS framework typically consists of a feature extractor and a matching head, while the recent state-of-the-art methods (Zhang et al. (2019); Tian et al. (2020b); Li et al. (2021a); Xie et al. (2021b); Wu et al. (2021); Zhang et al. (2021a); Li et al. (2020)) all focus on the matching head. They pay NO effort on learning the feature extractor and adopt a ImageNet-pretrained model without any fine-tuning.\n\nUnder this background, we think the FSS feature extractor deserves exploration and take a rethink on the corresponding challenge. Some prior literature (Tian et al. (2020b); Zhang et al. (2021b)) argue that the challenge is mainly because the limited support samples are insufficient for finetuning a large feature extractor (e.g., ResNet-50 (He et al. (2016))), therefore leading to the overfitting problem. We hold a different perspective and observe the heterogeneity (i.e., the intra-class diversity in the raw images) as a critical challenge hindering the intra-class compactness of FSS features. Although the heterogeneity is not a unique problem in FSS (e.g., it does exist in the\n\n∗Zhengdong Hu makes his part of work during internship in Baidu Inc. †Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: The heterogeneity from three levels. (a) shows the sample-level heterogeneity between the support and the query. The “cow” in the support is adopted to segment “cattle” in the query, in spite of their different appearance. (b) shows the region-level heterogeneity in the background. When the foreground object is the “rider”, the “horse” should share the same class (BG:background) with “grass”. (c) shows the patch-level heterogeneity among neighboring patches. The color of upper and lower part of body is different.\n\ncanonical segmentation as well), its challenge is significantly amplified by the few-shot setting. In our viewpoint, the heterogeneity has three levels from coarse to fine:\n\n• Sample-level heterogeneity exists between the query and support images due to their distribution gap. For example, in Fig. 1 (a), the foreground objects (“cow” and “cattle”) in the support and query images look quite different, although they both belong to a same semantic class “cow”.\n\n• Region-level heterogeneity exists (mostly) in the background, which actually contains multiple regions with different semantics. In Fig. 1 (b), “horse” in the image is a foreground region when the support object is another horse. However, when the support object shifts to a “rider”, the horse in the image should be merged into the background, resulting in the region-level heterogeneity.\n\n• Patch-level heterogeneity exists among neighboring patches which belong to a same semantic class but have significant appearance variations. For example, in Fig. 1 (c), the upper and lower body of a single person are in different colors, therefore introducing patch-level heterogeneity.\n\nMotivated by these observations, we propose an FSS feature extractor with Multi-level Heterogeneity Suppressing (MuHS). MuHS adopts the transformer backbone and leverages the attention mechanism to suppress all these three-level heterogeneity. Our choice of using the transformer backbone is natural: the attention mechanism provides strong potential for constructing long-range dependencies across samples, regions and patches. Concretely, MuHS reinforces the attention / interaction between different samples (query and support), different regions and neighboring patches by constructing cross-sample attention, cross-region interaction and a novel masked image segmentation, respectively. To be more specific, these attention / interaction are as below:\n\n(i) Cross-Sample Attention. In popular transformers, the attention is within each single sample and does not cross multiple samples. In contrast, MuHS constructs cross-sample attention with a novel design of “linking” tokens. In each transformer layer, we use some linking tokens to connect all the patch tokens from the query and support samples simultaneously, therefore efficiently propagating information across different samples.\n\n(ii) Cross-Region Interaction. In popular transformers, the attention usually encourages feature interaction (absorption) between similar patch tokens. In contrary to this common practice, MuHS enforces additional feature absorption between patch tokens from dissimilar regions in the background. Such a cross-region interaction smooths the background and suppresses the region-level heterogeneity.\n\n(iii) Masked Image Segmentation. Inspired by the recent masked image modeling (MIM), MuHS randomly masks some patch tokens and makes partial prediction for the existing patches. Afterwards, MuHS fills trainable mask tokens and encourages the deep model to make the holistic prediction for complete patches, yielding a novel masked image segmentation. The learned capacity of inferring the semantics of the masked patches from neighboring patches suppresses the patch-level heterogeneity.\n\n2\n\n(a) sample-level“cow” “cattle”(b) region-level(c) patch-level(support query)BG: “horse” + “grass” white & dark part of bodyPublished as a conference paper at ICLR 2023\n\nIn MuHS, the above three components respectively mitigate a corresponding type of heterogeneity and achieve complementary benefits for few-shot semantic segmentation. Empirically, we show that using MuHS to replace the frozen feature extractor (pretrained on ImageNet) brings consistent improvement for multiple popular FSS heads. Importantly, since the MuHS feature has relatively good intra-class compactness, we simply cooperate it with a linear classification head and achieve new state of the art on multiple FSS datasets. For example, on PASCAL-5i, MuHS achieves 69.1% mIoU under 1-shot setting.\n\nOur main contributions are summarized as follows: First, we shift the FSS research focus from the matching head to the feature extractor and reveal the heterogeneity as an important challenge. Second. we propose Multi-level Heterogeneity Suppressing (MuHS). MuHS utilizes novel crosssample attention, cross-region interaction and masked image segmentation to suppress the heterogeneity from three levels. Third, we conduct extensive experiments to validate the effectiveness of the proposed MuHS. Experimental results confirm that MuHS is compatible to multiple FSS heads and achieves new state of the art using a simple linear classification head.\n\n2 RELATED WORKS\n\nFew-shot Segmentation Methods With Focus On Matching Head. The recent state-of-the-art FSS methods (Zhang et al. (2022); Tian et al. (2020b); Li et al. (2021a); Xie et al. (2021b); Wu et al. (2021); Jiao et al. (2022); Lang et al. (2022); He et al. (2023a); Siam et al. (2019); He et al. (2023b)) focus on learning the matching head (based on a frozen CNN feature extractor). Some methods (Zhang et al. (2019); Tian et al. (2020b); Li et al. (2021a); Xie et al. (2021b); Lang et al. (2022); Ding et al. (2018)) generate a prior-map based on the similarity between samples (query and support) and adopt convolution based matching heads to further improve the segmentation accuracy. Zhang et al. (2021b); Lu et al. (2021) proposed transformer-based matching head and perform attention mechanism to aggregate features from support to query. Moreover, some methods (Min et al. (2021); Hong et al. (2021)) propose 4D convolutions to fully extract multi-level features.\n\nDifferent from these recent progresses, this paper focuses on learning the feature extractor. The proposed MuHS feature extractor brings general improvement for a battery of matching heads and achieves state-of-the-art accuracy with a simple linear classification head.\n\nTransformers for Visual Recognition. Recently, transformers are introduced to computer vision tasks, e.g., image classification (Dosovitskiy et al. (2020); Vaswani et al. (2021)), segmentation (Wang et al. (2021); Xie et al. (2021a); Li et al. (2021b; 2022); Zhou et al. (2021)), detection (Carion et al. (2020); Zhu et al. (2020); Bar et al. (2022)) and have shown promising performance.\n\nUnder FSS scenario, we observe three-level heterogeneity (i.e., sample-level, region-level, patchlevel), which hinders intra-class compactness of FSS features. We think the attention mechanism in the transformer provides strong potential for constructing long-range dependencies across samples, regions and patches. Therefore, the proposed MuHS adopts the transformer network as its backbone and utilizes the characteristics of transformer to suppress all these three-level heterogeneity in a unified framework.\n\nMasked Image Modeling. Masked modeling methods (Devlin et al. (2018); Radford et al. (2018; 2019)) are wildly utilized in NLP tasks. BERT (Devlin et al. (2018)) utilizes a “masked language model” (MLM) to randomly masks input and predict the original vocabulary id of the masked tokens. Motivated by BERT, BEIT (Bao et al. (2021)) proposes “masked image modeling” (MIM) to perform self-supervised learning on Vision task. It randomly masks some proportion of image patches and replaces them with a mask tokens. Recently, SimMIM (Xie et al. (2022)), MAE (He et al. (2022)) simplify the MIM designs and improves transformers.\n\nAn important component of MuHS is inspired by the masked image modeling to suppress the patchlevel heterogeneity. Different from the popular self-supervised scheme, MIS is fully supervised. It masks out some query patches at the input and yet maintains the holistic prediction for segmentation, yielding a novel Masked Image Segmentation (MIS) learning.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Overview of the proposed MuHS. (a) Following the vision transformer backbone, MuHS splits the support and query images into patches, embeds each patch into a patch token and feeds all the patch tokens into the stacked transformer block. Based on the backbone, MuHS 1) inserts a Cross-Sample Attention module between the query and support features to suppress the samplelevel heterogeneity (as in Section 3.2), 2) appends an extra Cross-Region Interaction module upon the original self-attention layer to suppress the region-level heterogeneity (as in Section 3.3), and 3) integrates a novel Masked Image Segmentation task to suppress the patch-level heterogeneity (as in Section 3.4). (b) The detailed structure of the transformer block with Cross-Region Interaction. (c) The detailed structure of the Cross-Sample Attention.\n\n3 METHODS\n\n3.1 OVERVIEW\n\nIn the few-shot segmentation (FSS) task, we have a training set and a testing set with no labeling overlap. The testing set consists of a support set S and a query set Q, which are from the same novel category (unseen in the training set). To set up a N -shot scenario, we randomly sample s , ..., I N N support images {I 1 s } from S to recognize the same-semantic region in the query image Iq ∈ Q.\n\ns } with corresponding masks {Y 1\n\ns , ..., Y N\n\nDuring training, we follow the popular meta-learning scheme (Tian et al. (2020a); Pambala et al. (2021)) and construct a meta-task,i.e., sampling N support and one query from the training set into each episode. The proposed MuHS feature extractor is based on the transformer backbone consisting of L transformer blocks, as illustrated in Fig. 2 (a). Given an input image, we split it into non-overlapping image patches, linearly embed them into patch tokens, and feed the patch tokens into the MuHS feature extractor. We utilize Xq, Xs to denote the embedding of query tokens and support tokens, respectively. Specifically for the query image, we randomly discard some patch tokens. Based on the transformer backbone, MuHS has three major components as below:\n\n1) MuHS enforces a Cross-Sample Attention between the support and query through some linking tokens. These linking tokens are trainable and update themselves by absorbing all query and support tokens simultaneously. In the following transformer block, the updated state of the linking tokens are absorbed by the query and support, respectively. Therefore, it facilitates interaction between the query and support with relatively low computational cost. The details are in Section 3.2.\n\n2) Within each transformer block, MuHS appends an additional Cross-Region Interaction upon the original self-attention layer. While the original self-attention encourages interaction among similar\n\n4\n\nTransformer block with Cross-Region Interaction masked query (random mask image)L ×Masked Image Segmentation M ×Cross-Sample Attention ......Matching Head supportCross-Sample Attentionsupport tokens query tokens support tokens query tokens partialpredictionholistic predictionSelf-AttentionLayer NormCross-RegionInteractionFFNlinking tokens ............Transformer block with Cross-Region Interaction Feature Extractor...Matching Head Transformer block with Cross-Region Interaction ......(a)(c)(b)Filling Mask Tokensnull tokenLTLT.........MM...Published as a conference paper at ICLR 2023\n\npatch tokens, the Cross-Region Interaction promotes interaction among dissimilar regions in the background. The details are in Section 3.3.\n\n3) Based on the output of the MuHS feature extractor, we use a matching head (e.g., the linear classification head) to make partial prediction for the existing patches of the query image (note that some query patches are discarded at the input layer). Afterwards, we fill trainable mask tokens to the incomplete query patch tokens, and input them into a Masked Image Segmentation model consisting of M transformer decoder. The Masked Image Segmentation aims to make holistic prediction of the input query, regardless of the discarded patches. The details are in Section 3.4.\n\n3.2 CROSS-SAMPLE ATTENTION\n\nThe Cross-Sample Attention mechanism constructs interaction between support and query so as to suppress the sample-level heterogeneity, as illustrated in Fig. 2 (c). Specifically, we use linking tokens Xlink ∈ RC×D (C embedding vectors with D dimensions ) to interact query and the support. At the input block, we initialize two linking tokens X 0 link with the mean features of the foreground and background region of the support sample. Then the linking tokens are updated block by block through cross-attention, which is formulated as:\n\nX i+1\n\nlink = Attcrs(Que(X i\n\nlink), Key({X i\n\ns, X i\n\nq, X i\n\nlink}), Val({X i\n\ns, X i\n\nq, X i\n\nlink})),\n\n(1)\n\nwhere X i+1 link denotes the updated linking tokens, {, } denotes the concatenation operation. Que, Key, Val are the operation to calculate query embedding, key embedding and value embedding q and linking tokens X i of support tokens X i link in i-th MuHS Block, respectively.\n\ns, query tokens X i\n\nGiven the updated linking tokens, we then update the support and the query patch tokens by:\n\nX i+1\n\ns = Att(Que(X i\n\ns), Key({X i\n\ns, X i+1\n\nlink}), Val({X i\n\ns, X i+1\n\nlink}))\n\nX i+1\n\nq = Att(Que(X i\n\nq), Key({X i\n\nq, X i+1\n\nlink}), Val({X i\n\nq, X i+1\n\nlink})),\n\n(2)\n\nwhere X i denotes the embedding of Linking tokens updated by Eq. 1.\n\nq are the D dimensions embedding of support and query tokens in i-th Block. X i+1\n\ns, X i\n\nlink\n\nSince the linking tokens already absorb information from all the query and support tokens (Eq. 1), in the subsequent Eq. 2, they propagate the absorbed information onto the support and the query tokens, therefore facilitating a mediate interaction between the support and query samples. Compared with directly constructing patch-to-patch attention between the query and support sample, our solution with linking tokens has the advantage of high efficiency. Specifically, the patch-to-patch attention incurs quadratic complexity against the patch token numbers, while using the linking tokens only incurs linear complexity.\n\n3.3 CROSS-REGION INTERACTION\n\nThe Cross-Region Interaction (Fig. 2 (b)) is appended upon the self-attention layer in Eq. 2 within each MuHS block to encourage the interaction between different regions in the background. To this end, we use the ground-truth label to split the background in each query image (during training) into multiple regions. Specifically, some regions in the background actually belong to some annotated foreground classes but are merged into the background because the current training episode focuses on a different foreground class. We denote these regions as temporary background, and the remaining part (which has no foreground annotations) as constant background. Correspondingly, we use x[tb] and x[cb] to distinguish the tokens from the temporal and the constant background regions, respectively, and use x[f ] to represent the tokens in the foreground-of-interest in the current episode.\n\nThe Cross-Region Interaction compares the cosine distance between x[tb] and x[cb] after the selfattention layer (Eq. 2) and makes x[cb] absorb information from the dissimilar x[tb] ∈ X [tb] by:\n\nx[cb]′\n\n= x[cb] + sof tmax(1 − x[cb]·X [tb]\n\n|x[cb]|·|X [tb]| ) · X [tb],\n\n(3)\n\nwhere x[cb]′\n\ndenotes updated constant background tokens embedding.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nRegion-level triplet loss. Besides the above Cross-Region Interaction smoothing the background across different regions through attention, we further use a region-level triplet loss to pull close the constant background tokens and temporal background tokens on the last MuHS transformer block. The region-level triplet loss is enforced on the final output state of the background tokens by:\n\nwhere D(., .) is the cosine distance between two tokens.\n\nLtri = max(D(x[tb], x[cb]) − D(x[tb], x[f ]), 0)),\n\n(4)\n\n3.4 MASKED IMAGE SEGMENTATION\n\nThe Masked Image Segmentation (MIS) model is appended upon the feature extractor of MuHS. It makes two types of prediction, i.e., a partial prediction from a matching head and a holistic prediction from an additional “decoder + matching head”. The details are as below:\n\n• Partial prediction: We recall that MuHS randomly discards some patches of the input query image during training. Correspondingly, the output tokens X L q from the L-layer MuHS feature extractor are incomplete. Given these existing output patches, we use a matching head (linear classification head) to make partial prediction in Fig. 2 (a). Specifically, we calculate the foreground / background mean features with the support tokens X L s and the ground-truth object mask to correspondingly derive the foreground / background proxies wf and wb. During training, the supervision on a specified query token xL\n\nq is:\n\nf xL exp(wT where yq is corresponding label of the query token xL otherwise yq=0).\n\nLpartial(xL\n\nq ) = − log\n\nyq exp(wT\n\nb xL q )\n\nq )+exp(wT\n\nq )+(1−yq) exp(wT f xL b xL q ) q (yq=1, if yq belongs to the foreground;\n\n(5)\n\n,\n\n• Holistic prediction: In addition, we fill mask tokens with trainable positional embeddings to construct the full set of the query patches (as shown in Fig. 2 (a)). Therefore, the output tokens X M from the Masked Image Segmentation model are complete. We make holistic prediction for q\nall the query patches from an additional matching head, the weight matrix from which is calculated by foreground / background mean features with the support tokens X M from the Masked Image Segmentation model and the ground-truth object mask. The holistic prediction is supervised under cross-entropy loss Lholistic, which is similar as in Eq. 5 and is thus omitted here.\n\ns\n\nWe note that Masked Image Segmentation model is only utilized for training. During testing, we feed all the query patches into MuHS feature extractor and compare each query token against these proxies to classify each patch into the foreground / background.\n\nOverall Training. During the training stage, we successively perform the feature extractor and Masked Image Segmentation model for the query prediction. The overall training loss is as follows: L = Lpartial + α · Ltri + β · Lholistic\n\n(6)\n\nwhere α and β are weighting factors.\n\n4 EXPERIMENTS\n\n4.1 DATASETS.\n\nWe evaluate the proposed MuHS on two datasets: PASCAL-5i (Shaban et al. (2017)) and COCO20i (Nguyen & Todorovic (2019)). PASCAL-5i consists of PASCAL VOC 2012 (Everingham et al. (2010)) and additionally annotations from SDS (Hariharan et al. (2014)). We divide 20 classes into 4 splits and each split has 5 classes. During evaluation on one split (5 classes), we have other three splits (15 classes) for training. We randomly sample 1000 pairs of support and query in each split testing. COCO-20i is built from COCO2014 (Lin et al. (2014)). We divide 80 classes into 4 splits and each split has 20 classes. During evaluation on one split (20 classes), we have other three splits (60 classes) for training. We randomly sample 20000 pairs of support and query in each split testing.\n\nFollowing previous works (Tian et al. (2020b); Zhang et al. (2021b)), we compare the performance on testing splits by using mean intersection over union (mIoU) as our evaluation metrics.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nModel Method\n\nS0 PGNet 56.0 RPMM 55.2 61.7 PFENet CyCTR 67.8 64.3 HSNet BAM 69.0 54.7 DAN 59.6 RePRI PFENet 60.5 69.3 CyCTR HSNet 67.3 67.2 Baseline Ours 71.2\n\n1-shot S2 50.6 52.6 55.4 58.0 60.3 67.6 57.8 62.2 54.4 56.5 62.0 60.4 67.0\n\nS3 Mean 56.0 50.4 56.3 50.7 60.8 56.3 64.2 58.0 64.0 60.5 67.8 61.1 58.2 51.6 59.4 47.2 60.1 55.9 64.3 58.6 66.2 63.1 63.9 59.6 69.1 66.6\n\nS1 66.9 66.9 69.5 72.8 70.7 73.6 68.6 68.6 69.4 72.7 72.3 68.3 71.5\n\nS0 54.9 56.3 63.1 71.1 70.3 70.6 57.9 66.2 62.8 73.5 71.8 74.2 75.7\n\nS1 67.4 67.3 70.7 73.2 73.2 75.1 69.0 71.4 70.4 74.0 74.4 77.1 77.8\n\n5-shot S2 51.8 54.5 55.8 60.5 67.4 70.8 60.1 67.0 54.9 58.6 67.0 75.2 78.6\n\nS3 Mean 53.0 51.0 57.9 57.5 67.1 67.2 54.9 57.7 57.6 60.2 68.3 72.6 74.7\n\n56.8 57.3 61.9 65.6 69.5 70.9 60.5 65.6 61.4 66.6 70.4 74.8 76.7\n\nRes50\n\nRes101\n\nDeiT-B\n\nTable 1: Comparison with the state of the arts on PASCAL-5i for 1-shot and 5-shot setting.\n\nModel Method\n\nRes50\n\nRes101\n\nDeiT-B\n\nRePRI HSNet BAM DAN PFENet HSNet Baseline Ours\n\nS0 32.0 36.3 43.4 -\n34.3 37.2 40.7 44.0\n\nS1 38.7 43.1 50.6 -\n33.0 44.1 44.3 50.0\n\n1-shot S2 32.7 38.7 47.5 -\n32.3 42.4 47.8 49.1\n\nS3 Mean 34.1 33.1 39.2 38.7 46.2 43.4 24.4 -\n32.4 30.1 41.2 41.3 43.1 39.5 47.4 46.3\n\nS0 39.3 43.3 49.3 -\n38.5 45.9 51.3 53.6\n\nS1 45.4 51.3 54.2 -\n38.6 53.0 56.5 60.5\n\n5-shot S2 39.7 48.2 51.6 -\n38.2 51.8 53.6 57.3\n\nS3 Mean 41.8 45.0 49.6 -\n34.3 47.1 52.1 55.2\n\n41.6 46.9 51.2 29.6 37.4 49.5 53.4 56.7\n\nTable 2: Comparison with the state of the arts on COCO-20i for 1-shot and 5-shot setting.\n\n4.2\n\nIMPLEMENTATION DETAILS\n\nWe adopt DeiT-B/16 (Touvron et al. (2021)) (pretrained on Imagenet (Deng et al. (2009))) as our backbone. We use SGD optimizer and set the learning rate as 9e-4. We randomly crop images to 480 × 480 and follow the data augmentation in PFENet (Tian et al. (2020b)). For PASCAL-5i, we train 50 epochs with batch size 4. For COCO-20i, we train 30 epochs and set batch size to 16. The proposed MuHS is trained on Pytorch with 4 NVIDIA A100 GPUS. More details are in Sec A.2. 4.3 COMPARISON WITH THE STATE OF THE ARTS\n\nWe compare MuHS with the existing state of the arts on PASCAL-5i and COCO-20i. The results on two datasets are summarized in Table. 1 and Table. 2, respectively.\n\nFrom Table. 1, we clearly observe the superiority of MuHS on PASCAL-5i . First, comparing MuHS against the DeiT-B baseline, we find MuHS achieves considerable improvements. For example, under the 1-shot and 5-shot settings, MuHS outperforms the DeiT-B baseline by +5.2% and +1.9% mIoU, respectively. Second, MuHS surpasses all the state-of-the-art methods by a clear margin (especially under the 5-shot setting). For example, MuHS clearly outperforms the most competing method BAM (Lang et al. (2022)) by +1.3% and +5.8% mIoU on 1-shot and 5-shot, respectively. We note that all the competing methods use sophisticated matching heads, while our MuHS only uses a simple linear classification head. We thus infer that the superiority of our MuHS mainly comes from the discriminative features, indicating that MuHs is a strong feature extractor for FSS.\n\nThe observation on COCO-20i (Table. 2) is similar, i.e., MuHS improves the DeiT-B baseline and presents superiority against all the competing methods. For example, under the 5-shot settings, MuHS surpasses the DeiT-B baseline by +3.3% mIoU and surpasses BAM (Lang et al. (2022)) by +5.5%, mIoU, respectively.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: MuHS feature is compatible to multiple FSS matching heads\n\nComponents\n\nBaseline Baseline + Cross-Sample Attention Baseline + Cross-Region Interaction Baseline + Masked Image Segmentation All (MuHS)\n\nPASCAL-5i\n\nCOCO-20i\n\n1-shot 63.9 65.8 66.3 66.9 69.1\n\n5-shot 74.8 75.3 75.5 75.8 76.7\n\n1-shot 43.1 45.8 46.1 46.3 47.4\n\n5-shot 53.4 54.7 54.8 55.1 56.7\n\nTable 3: Ablation studies of our proposed method under 1-shot and 5-shot setting.\n\n4.4 COMPATIBILITY TO STATE-OF-THE-ART MATCHING HEADS.\n\nSince the proposed MuHS focuses on learning the feature extractor, we are interested at its compatibility to different FSS matching heads. Specifically, we investigate two popular FSS heads, i.e., PFENet (Tian et al. (2020b)) and CyCTR (Zhang et al. (2021b)). We compare our MuHS feature against two frozen CNN feature (i.e., ResNet50, ResNet101, following their original practice), as well as the DeiT-B feature. The results are summarized in Fig. 3, from which we draw three observations as below:\n\nFirst, compared with the frozen CNN features, the frozen DeiT-B features considerably decreases accuracy for both the PFENet and CyCTR heads. For example, the frozen DeiT-B feature is inferior than the frozen ResNet50 feature by ↓ 3.1% , ↓ 3.2% mIoU on PFENet and CyCTR, respectively. It is somehow surprising given that the transformer feature usually achieves better discriminative ability than the CNN counterparts. However, we think the above observation is actually reasonable, because both the PFENet and CyCTR heads (as well as most matching heads in prior literature) are specifically designed for CNN features and lack consideration for the transformer features.\n\nSecond, our MuHS features significantly outperform the CNN features, although the employed heads is specifically designed for the CNN features. For example, using the PFENet head, MuHS feature surpasses the ResNet101 feature by ↑ 7.70%. We infer that although there are some weaknesses for cooperating the transformer feature with these heads (as in observation 1), the benefits of suppressing the heterogeneity in MuHS dominate. Therefore, MuHS brings improvement over the CNN features and shows good compatibility against these matching heads.\n\nThird, comparing the achieved mIoU of “MuHS + PFENet (CyCTR) head” against the “MuHS + linear classification head” (in Table. 1), we find that the latter is slightly better. This result is consistent with the above two observations: MuHS is the superior feature for the PFENet and CyCTR heads (observation 2), while those two heads are not the superior heads for MuHS and other transformer features (observation 1). Therefore, we recommend using the simple linear classification head for MuHS currently. That being said, we are optimistic towards a good embrace between MuHS and future matching heads in the FSS community.\n\n4.5 ABLATION STUDIES\n\nMuHS adopts Cross-Sample Attention, Cross-Region Interaction and Masked Image Segmentation to reinforce the interaction between different samples, regions and neighboring patches and suppress\n\n8\n\n60.860.157.767.8505560657075Res50Res101DeiT-BMuHS64.264.36168505560657075Res50Res101DeiT-BMuHS(a) PFENet(b) CyCTRPublished as a conference paper at ICLR 2023\n\nFigure 4: Visualization of the segmentation results on PASCAL-5i. (a) MuHS suppresses the sample-level heterogeneity between cattle (query) and cow (support), therefore improving the recall of the “cattle” pixels in the query. (b) MuHS suppresses the region-level heterogeneity and merges “horse” into the background of the query, when the support object is “person”. (c) Due to its capacity of suppressing the patch-level heterogeneity, MuHS recognizes the upper part of body, although there is no such a clue (i.e., white upper body) from the support.\n\nthe heterogeneity in FSS. We investigate their benefits through ablation, as shown in Table 3. We draw two observations:\n\nFirst, all the three modules can bring considerable improvements, independently. For example, under 1-shot on PASCAL-5i, adding Cross-Sample Attention, Cross-Region Interaction and Masked Image Segmentation alone improves the baseline by +1.9%, +2.4% and +3.0%, respectively.\n\nSecond, MuHS integrating all the three components achieves further improvement, e.g., +5.2% on PASCAL-5i under 1-shot setting. It indicates that these three components suppressing different heterogeneity achieve complementary benefits. We supplement more ablation studies in Sec A.5.\n\nVisualization of heterogeneity suppression. We visualize some segmentation results in Fig. 4 to intuitively understand how the proposed MuHS suppresses the heterogeneity and improves FSS. In Fig. 4 (a), due to the sample-level heterogeneity (between cattle and cow), the baseline fails to recall many foreground pixels. In contrast, the proposed MuHS significantly improves the recall on the foreground pixels due to its capacity of suppressing the sample-level heterogeneity. In Fig. 4 (b), the region-level heterogeneity makes the baseline to classify many pixels on the horse into the foreground (person). In contrast, MuHS smooths the background and thus remove the distraction from the horse. In Fig. 4 (c), MuHS successfully merges the upper part of body into the foreground “person” by suppressing patch-level heterogeneity. Additionally, we visualize the feature distribution before and after eliminating heterogeneity in Sec A.4 and plot the convergence curves in Sec A.6.\n\n5 CONCLUSION\n\nThis paper proposes a feature extractor with Multi-level Heterogeneity Suppressing (MuHS) for fewshot semantic segmentation(FSS). Based on the transformer backbone, MuHS sets up novel crosssample attention, cross-region interaction and the masked image segmentation. The cross-sample attention efficiently propagates information across different samples. The cross-region interaction facilitates feature absorption between dissimilar regions within the background. The masked image segmentation utilizes the contextual information to infer the labels for discarded (masked) patch tokens so as to reinforce the capacity of contextual inference. These three modules respectively suppress the heterogeneity from three different levels, therefore improving the intra-class compactness of the FSS features. Extensive experiments on two popular FSS datasets demonstrate the effectiveness of MuHS and the achieved results are on par with the state of the art.\n\n9\n\nsupportground truthbaselineMuHS(b)(a)(c)queryPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nThis paper can help to improve the semantic segmentation accuracy with the limited labeled samples. It may be applied to automatic driving system to improve safety when the system needs to recognize unseen objects. We will explore more applications of few-shot segmentation. Moreover, we will try to improve the reliability of few-shot segmentation systems to reduce potential problems.\n\nREPRODUCIBILITY STATEMENT\n\nIn the main text, we describe the two datasets for evaluation, (i.e., The MuHS is reproducible. PASCAL-5i and COCO-20i) and the details about the experimental implementation. We supply the analysis of some hyper parameters in appendix.\n\nREFERENCES\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nAmir Bar, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson. Detreg: Unsupervised pretraining with region priors for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14605–14615, 2022.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213–229. Springer, 2020.\n\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nHenghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Context contrasted feature and gated multi-scale aggregation for scene segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2393–2402, 2018.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303–338, 2010.\n\nBharath Hariharan, Pablo Arbel ́aez, Ross Girshick, and Jitendra Malik. Simultaneous detection and\n\nsegmentation. In European conference on computer vision, pp. 297–312. Springer, 2014.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n\nnition. (CVPR), June 2016.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nShuting He, Henghui Ding, and Wei Jiang. Semantic-promoted debiasing and background disIn Proceedings of the IEEE conference on\n\nambiguation for zero-shot instance segmentation. computer vision and pattern recognition (CVPR), 2023a.\n\nShuting He, Henghui Ding, and Wei Jiang. Primitive generation and semantic-related alignment for universal zero-shot segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2023b.\n\nSunghwan Hong, Seokju Cho, Jisu Nam, and Seungryong Kim. Cost aggregation is all you need for\n\nfew-shot segmentation. arXiv preprint arXiv:2112.11685, 2021.\n\nSiyu Jiao, Gengwei Zhang, Shant Navasardyan, Ling Chen, Yao Zhao, Yunchao Wei, and Humphrey Shi. Mask matching transformer for few-shot segmentation. In Advances in Neural Information Processing Systems, 2022.\n\nChunbo Lang, Gong Cheng, Binfei Tu, and Junwei Han. Learning what not to segment: A new perspective on few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8057–8067, 2022.\n\nGen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun, Jonghyun Kim, and Joongkyu Kim. Adaptive prototype learning and allocation for few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8334–8343, 2021a.\n\nGuangrui Li, Guoliang Kang, Wu Liu, Yunchao Wei, and Yi Yang. Content-consistent matching for domain adaptive semantic segmentation. In European Conference on Computer Vision, pp. 440–456. Springer, 2020.\n\nGuangrui Li, Guoliang Kang, Yi Zhu, Yunchao Wei, and Yi Yang. Domain consensus clustering for universal domain adaptation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021b.\n\nLiulei Li, Tianfei Zhou, Wenguan Wang, Jianwu Li, and Yi Yang. Deep hierarchical semantic In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n\nsegmentation. Recognition, pp. 1246–1257, 2022.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European\n\nDoll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. conference on computer vision, pp. 740–755. Springer, 2014.\n\nZhihe Lu, Sen He, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and Tao Xiang. Simpler is better: Fewshot semantic segmentation with classifier weight transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8741–8750, 2021.\n\nJuhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6941–6952, 2021.\n\nKhoi Nguyen and Sinisa Todorovic. Feature weighting and boosting for few-shot segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 622–631, 2019.\n\nAyyappa Kumar Pambala, Titir Dutta, and Soma Biswas. Sml: Semantic meta-learning for few-shot\n\nsemantic segmentation. Pattern Recognition Letters, 147:93–99, 2021.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-\n\nstanding by generative pre-training. 2018.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nAmirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for\n\nsemantic segmentation. arXiv preprint arXiv:1709.03410, 2017.\n\nMennatullah Siam, Boris Oreshkin, and Martin Jagersand. Adaptive masked proxies for few-shot\n\nsegmentation. arXiv preprint arXiv:1902.11123, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nPinzhuo Tian, Zhangkai Wu, Lei Qi, Lei Wang, Yinghuan Shi, and Yang Gao. Differentiable metalearning model for few-shot semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 12087–12094, 2020a.\n\nZhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Prior IEEE transactions on pattern\n\nguided feature enrichment network for few-shot segmentation. analysis and machine intelligence, 2020b.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021.\n\nAshish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12894– 12904, 2021.\n\nKaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot image semantic segmentation with prototype alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9197–9206, 2019.\n\nYuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8741–8750, 2021.\n\nZhonghua Wu, Xiangxi Shi, Guosheng Lin, and Jianfei Cai. Learning meta-class memory for fewshot semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 517–526, 2021.\n\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077–12090, 2021a.\n\nGuo-Sen Xie, Jie Liu, Huan Xiong, and Ling Shao. Scale-aware graph neural network for few-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5475–5484, 2021b.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9653–9663, 2022.\n\nBingfeng Zhang, Jimin Xiao, and Terry Qin. Self-guided and cross-guided learning for few-shot In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n\nsegmentation. Recognition, pp. 8312–8321, 2021a.\n\nChi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5217–5226, 2019.\n\nGengwei Zhang, Guoliang Kang, Yi Yang, and Yunchao Wei. Few-shot segmentation via cycleconsistent transformer. Advances in Neural Information Processing Systems, 34:21984–21996, 2021b.\n\nJian-Wei Zhang, Yifan Sun, Yi Yang, and Wei Chen. Feature-proxy transformer for few-shot seg-\n\nmentation. In Advances in Neural Information Processing Systems, 2022.\n\nTianfei Zhou, Liulei Li, Xueyi Li, Chun-Mei Feng, Jianwu Li, and Ling Shao. Group-wise learning for weakly supervised semantic segmentation. IEEE Transactions on Image Processing, 31:799– 811, 2021.\n\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nIn the appendix, we supply the details which are not described in the main text due to space limitation. In Section A.1, we analyze the impact of some hyper-parameters. In Section A.2, we add more implementation details. In Section A.3, we adopt one more dataset to evaluate performance. In Section A.4, we compare the feature distribution before and after eliminating heterogeneity. In Section A.5, we supply more ablation experiments. In Section A.6, we plot the convergence curves of the proposed MuHS and recent state-of-the-art methods.\n\nA.1 HYPER-PARAMETERS ANALYSIS.\n\nWe analyze the impact of important hyper-parameters, i.e., α and β in Eq. 6. and investigate the impact of model depth and masked ratio of the proposed Masked Image Segmentation (MIS) in Section 3.4. The experiments are reported on split-0 of PASCAL-5i, under 1-shot setting.\n\nFigure 5: Analysis on the hyper-parameters.\n\nIn Fig. 5 (a), we evaluate the impact of α, which controls the weight of triplet loss in Eq. 6. We observe that the accuracy first increases (when α increases from 0 to 0.6) and then decreases (when α further increases to 1.0). We set α = 0.6 as the weight factor.\n\nIn Fig. 5 (b), we evaluate the impact of β, which controls the weight of holistic prediction loss in Eq. 6. We observe that the accuracy first increases (when β increases from 0 to 0.5) and then decreases (when β further increases to 1.0). We set β = 0.5 as the weight factor.\n\nMIS layers mIoU 70.5 70.7 71.2 70.8\n\n5 6\n7 8\n\nMask ratio mIoU 70.4 70.9 71.2 71.0\n\n3% 5% 7% 9%\n\nTable 4: Analysis on the MIS model depth\n\nTable 5: Analysis on the masked ratio for MIS\n\nIn Table 4, we analyse the impact of Masked Image Segmentation (MIS) model depth. We observe that the 7-layer MIS model can achieve the best accuracy. In Table 5, we analyse the impact of masked ratio for MIS model. We observe that randomly masking out 7% patches can achieve the best accuracy.\n\nA.2\n\nIMPLEMENTATION DETAILS.\n\nWe supply the implementation details of how to transform a patch token into pixel-wise mask map and the scheme to generate the discarded patches.\n\n13\n\n6567697173757779656769717375777900.20.40.60.81 5-shot1-shot5-shot mIOU(%)1-shot mIOU(%)(a) the analysis of αα6567697173757779656769717375777900.20.40.60.81 5-shot1-shot(b) the analysis of ββ5-shot mIOU(%)1-shot mIOU(%)Published as a conference paper at ICLR 2023\n\n• To transform a patch into pixel-wise mask map, we follow the common practice (PANet (Wang et al. (2019)), PFENet (Tian et al. (2020b))) of spatially up-sampling. Specially, we first obtain the softmax scores for each patch token through the classification head. Then, the score maps are upsampled through bi-linear interpolation to match the size of the input image. Finally, we use argmax operation to generate pixel-wise mask map.\n\n• To generate the discarded patches, we randomly shuffle the patch tokens and then mask the rear of the token sequence. This operation is the same as in other MIM methods (e.g., MAE (He et al. (2022)) ).\n\nA.3 EVALUATION ON MORE DATASETS.\n\nWe evaluate one more dataset, i.e., Cityscapes (Cordts et al. (2016)), an urban street-scene dataset. We use 15 classes (out of the commonly-used 19 classes) to construct the base set and use the other 4 classes (i.e., “sky”, “person”, “car”, “bicycle”) for the novel set.\n\nCityscapes\n\nBaseline CyCTR MuHS 15.2\n\n13.1\n\n25.2\n\nTable 6: Comparison with the state of the art and the baseline on cityscapes for 1-shot setting.\n\nBased on this newly-generated few-shot segmentation benchmark, we compare the proposed MuHS against the baseline and a most recent state-of-the-art method CyCTR(Zhang et al. (2021b)). From the Table 6, we observe MuHS significantly surpasses the baseline and CyCTR by +12.1% and +10% mAP, respectively. We note that the cityscapes is challenging for few-shot segmentation task. We infer it is because in cityscapes, the number of semantic classes appearing in a single image is much larger, therefore increasing the challenge from region-level heterogeneity.\n\nA.4 COMPARISON OF THE FEATURE DISTRIBUTION.\n\nTo better understand how MuHS suppresses the three types of heterogeneity, we use t-SNE visualization to compare the feature distribution before and after MuHS in Fig. 6.\n\nFigure 6: Visualization of the feature distribution of the baseline and MuHS. We evaluate the class of “person”. Red and Blue denotes the background and foreground features in the query, respectively. We additionally use Green to denote foreground features of the support to visualize the distribution gap between the support and the query.\n\nWe correspondingly draw two observations. First, the intra-class distributions of both background and the foreground become more compact, indicating that MuHS effectively suppresses the regionlevel and patch-level heterogeneity. Second, the foreground from the support and query images become closer, indicating that MuHS effectively suppresses the sample-level heterogeneity. These observations are consistent with the segmentation examples in Fig. 4. This experiment also intuitively validates that MuHS improves intra-class feature compactness.\n\n14\n\n(a)(b)Published as a conference paper at ICLR 2023\n\nA.5 ABLATIONS ON MORE VARIANTS.\n\nWe recall that all the three components (i.e., cross-sample attention, cross-region interaction and masked image segmentation) can bring considerable improvements, independently and integrating all of them achieves further improvements. For better investigating the efficiency, we supply more ablation studies by adding three more variants, as shown in Table 7. Each variant combines two components out of cross-sample attention, cross-region interaction and masked image segmentation can still improve the baseline.\n\nCross-Sample Atten Cross-Region Inter MIS\n\n✓\n\n✓ ✓\n\n✓\n\n✓\n\n✓\n\n✓ ✓\n\n✓\n\n✓ ✓\n✓\n\nPASCAL-5i 63.9 65.8 66.3 66.9 67.4 67.8 68.2 69.1\n\nTable 7: Ablation studies on more variants of three components for PASCAL-5i.\n\nA.6 CONVERGENCE CURVES.\n\nIn Fig. 7, we plot the convergence curves of CyCTR, PFENet and MuHS. Compared with CyCTR and PFENet, MuHS costs 4 × less training epochs (50 epochs) with fewer time (8 hours) and achieves higher accuracy on PASCAL-5i. The comparison is based on Pytorch and NVIDIA A100 GPU.\n\nFigure 7: The convergence curves of recent methods and MuHS on split-0 of PASCAL-5i.\n\n15\n\n4050607080050100150200mIoUepochsMuHSCyCTRPFENet",
    "reference": "# Summary Of The Paper\n\nDifferent from most existing methods, this paper investigates the FSS problem from the perspective of feature extraction. The authors claim that the heterogeneity of the sample-level, region-level and patch-level of support-query pair is the main obstacle to alleviate the intra-class variation. Thus, this paper proposes 3 novel modules (i.e. cross-sample attention, cross-region interaction and masked image segmentation) to tackle the above 3 problems, respectively. The proposed method sets new states of the art on many benchmarks.\n\n# Strength And Weaknesses\n\nStrength:\n(1) Investigation of the FSS problem from the perspective of feature extraction, which is interesting and is ignored in previous research.\n(2) Summarizing the heterogeneity between support-query pair and within query image itself from 3 aspects: sample-level, region-level and patch level is a good idea.\n(3) The idea of leverage cross-sample attention, cross-region interaction and masked image segmentation to suppress the intra-class variation is reasonable and rather novel.\n(4) The proposed method advances the state-of-the-art results.\n\nWeaknesses:\n(1) The number of “linking token” is set to 2 in this paper (i.e. foreground and background) manually. However, the authors claim there exists heterogeneity in different regions of background. Is it reasonable to only set 1 token for background? \n(2) The demonstration of some notions are obscure, e.g. the dimension of   X_s^i and X_q^i should be explicitly pointed out for better understanding. \n(3) some details are missing or not explained.  e.g., how to transform a patch into pixel-wise mask map? What is the principle to generate discarded patches?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe idea of suppressing the intra-class variation to cope with heterogeneity is quite interesting for FSS problem. The proposed 3 modules (i.e. cross-sample, cross-region, MIS) seems to be reasonable. The organization of this paper is clear. I believe it is not difficulty to reproduce the proposed method.\n\n# Summary Of The Review\n\nThe proposed method is interesting and rather novel, achieving appealing results. The paper is also well written and easy to follow.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSAM AS AN OPTIMAL RELAXATION OF BAYES\n\nThomas M ̈ollenhoff & Mohammad Emtiyaz Khan RIKEN Center for Advanced Intelligence Project Tokyo, Japan\n\nthomas.moellenhoff,emtiyaz.khan\n\n@riken.jp\n\n{\n\n}\n\nABSTRACT\n\nSharpness-aware minimization (SAM) and related adversarial deep-learning methods can drastically improve generalization, but their underlying mechanisms are not yet fully understood. Here, we establish SAM as a relaxation of the Bayes objective where the expected negative-loss is replaced by the optimal convex lower bound, obtained by using the so-called Fenchel biconjugate. The connection enables a new Adam-like extension of SAM to automatically obtain reasonable uncertainty estimates, while sometimes also improving its accuracy. By connecting adversarial and Bayesian methods, our work opens a new path to robustness.\n\n1\n\nINTRODUCTION\n\nSharpness-aware minimization (SAM) (Foret et al., 2021) and related adversarial methods (Zheng et al., 2021; Wu et al., 2020; Kim et al., 2022) have been shown to improve generalization, calibration, and robustness in various applications of deep learning (Chen et al., 2022; Bahri et al., 2022), but the reasons behind their success are not fully understood. The original proposal of SAM was geared towards biasing training trajectories towards flat minima, and the effectiveness of such minima has various Bayesian explanations, for example, those relying on the optimization of description lengths (Hinton & Van Camp, 1993; Hochreiter & Schmidhuber, 1997), PAC-Bayes bounds (Dziugaite & Roy, 2017; 2018; Jiang et al., 2020; Alquier, 2021), or marginal likelihoods (Smith & Le, 2018). However, SAM is not known to directly optimize any such Bayesian criteria, even though some connections to PAC-Bayes do exist (Foret et al., 2021). The issue is that the ‘max-loss’ used in SAM fundamentally departs from a Bayesian-style ‘expected-loss’ under the posterior; see Fig. 1(a). The two methodologies are distinct, and little is known about their relationship.\n\nHere, we establish a connection by using a relaxation of the Bayes objective where the expected negative-loss is replaced by the tightest convex lower bound. The bound is optimal, and obtained by Fenchel biconjugates which naturally yield the maximum loss used in SAM (Fig. 1(a)). From this, SAM can be seen as optimizing the relaxation of Bayes to find the mean of an isotropic Gaussian posterior while keeping the variance fixed. Higher variances lead to smoother objectives, which biases the solution towards flatter regions (Fig. 1(b)). Essentially, the result connects SAM and Bayes through a Fenchel biconjugate that replaces the expected loss in Bayes by a maximum loss.\n\nWhat do we gain by this connection? The generality of our result makes it possible to easily combine the complementary strengths of SAM and Bayes. For example, we show that the relaxedBayes objective can be used to learn the variance parameter, which yields an Adam-like extension of SAM (Alg. 1). The variances are cheaply obtained from the vector that adapts the learning rate, and SAM’s hyperparameter is adjusted for each parameter dimension via the variance vector. The extension improves the performance of SAM on standard benchmarks while giving comparable uncertainty estimates to the best Bayesian methods.\n\nOur work complements similar extensions for SGD, RMSprop, and Adam from the Bayesian deeplearning community (Gal & Ghahramani, 2016; Mandt et al., 2017; Khan et al., 2018; Osawa et al., 2019). So far there is no work on such connections between SAM and Bayes, except for a recent empirical study by Kaddour et al. (2022). Husain & Knoblauch (2022) give an adversarial interpretation of Bayes, but it does not cover methods like SAM. Our work here is focused on connections to approximate Bayesian methods, which can have both theoretical and practical impact. We discuss a new path to robustness by connecting the two fields of adversarial and Bayesian methodologies.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) SAM vs Bayes\n\n(b) Higher variances give smoother objectives\n\nFigure 1: Panel (a) highlights the main difference between SAM and Bayes. SAM uses max-loss 0, σ2)). (the red dot), while Bayes uses expected-loss (the blue dot shows (cid:96)(θ + (cid:15)) for an (cid:15) Our main result in Theorem 2 connects the two by using the optimal convex relaxation (Fenchel biconjugate) of the negative expected loss. It shows that the role of ρ and σ are exactly the same, and a SAM minimizer obtained for a fixed ρ can always be recovered from the relaxation for some σ. An example is shown in Panel (b) for a loss (gray line) with 3 local minima indicated by A, B, and C. The expected loss is smoother (blue line) but the relaxation, which upper bounds it, is even more smooth (red line). Higher σ give smoother objectives where sharp minima A and C slowly disappear. The SAM minimizer is shown with a red star which matches the minimizer of the relaxation.\n\n∼ N\n\n((cid:15)\n\n|\n\n2 SAM AS AN OPTIMAL RELAXATION OF BAYES\n\nIn SAM, the training loss (cid:96)(θ) is replaced by the maximum loss within a ball of radius ρ > 0 around RP , as shown below for an (cid:96)2-regularized problem, the parameter θ\n\nΘ\n\n∈\n\n⊂\n\nESAM(θ; ρ, δ) = sup\n\n(cid:15)\n\nρ\n\n(cid:107)\n\n(cid:107)≤\n\n(cid:96)(θ + (cid:15)) +\n\nδ 2 (cid:107)\n\nθ\n\n2,\n\n(cid:107)\n\n(1)\n\nwith δ > 0 as the regularization parameter. The use of ‘maximum’ above differs from Bayesian strategies that use ‘expectation’, for example, the variational formulation by Zellner (1988),\n\n(q) = Eθ\n\nL\n\nq [\n\n(cid:96)(θ)]\n\n∼\n\n−\n\n−\n\nDKL(q(θ)\n\n(cid:107)\n\np(θ)),\n\n(2)\n\nwhere q(θ) is the generalized posterior (Zhang, 1999; Catoni, 2007), p(θ) is the prior, and DKL( ) is the Kullback-Leibler divergence (KLD). For an isotropic Gaussian posterior q(θ) = (θ\n\n(θ\n\n· (cid:107) · m, σ2I) and prior p(θ) = |\n\nN\n\n|\n\n0, I/δ), the objective with respect to the mean m becomes δ\n2 (cid:107)\n\n(0,σ2I) [(cid:96)(m + (cid:15)(cid:48))] +\n\nm\n\n∼N\n\n2,\n\n(cid:107)\n\n(3)\n\nEBayes(m; σ, δ) = E(cid:15)(cid:48)\n\nN\n\nwhich closely resembles Eq. 1, but with the maximum replaced by an expectation. The above objective is obtained by simply plugging in the posterior and prior, then collecting the terms that depend on m, and finally rewriting θ = m + (cid:15)(cid:48).\n\nThere are some resemblances between the two objectives which indicate that they might be connected. For example, both use local neighborhoods: the isotropic Gaussian can be seen as a ‘softer’ version of the ‘hard-constrained’ ball used in SAM. The size of the neighborhood is decided by their respective parameters (σ or ρ in Fig. 1(a)). But, apart from these resemblances, are there any deeper connections? Here, we answer this question.\n\nWe will show that Eq. 1 is equivalent to optimizing with respect to m an optimal convex relaxation of Eq. 3 while keeping σ fixed. See Theorem 2 where an equivalence is drawn between σ and ρ, revealing similarities between the mechanisms used in SAM and Bayes to favor flatter regions. Fig. 1(b) shows an illustration where the relaxation is shown to upper bound the expected loss. We will now describe the construction of the relaxation based on Fenchel biconjugates.\n\n2.1 FROM EXPECTED-LOSS TO MAX-LOSS VIA FENCHEL BICONJUGATES\n\nWe use results from convex duality to connect expected-loss to max-loss. Our main result relies on Fenchel conjugates (Hiriart-Urruty & Lemar ́echal, 2001), also known as convex conjugates. Con-\n\n2\n\nρθsup|ε|<ρl(θ+ε)SAM:Bayes:Eε∼N(0,σ2)[l(θ+ε)]Fenchel Biconjugate-505-10Expected-LossBiconjugateLossSAM minimizer-505-505SAM minimizerSAM minimizerVariance = 0.5Variance = 2Variance = 4BACBACBAC-505-10Expected-LossBiconjugateLoss-505-505SAM minimizerSAM minimizerSAM minimizerVariance = 0.5Variance = 2Variance = 4BACBACBACPublished as a conference paper at ICLR 2023\n\n(a) Duality of exponential family\n\n(b) Biconjugate of expected loss\n\n(c) Biconjugate and expected loss in (ω, v)-parameterization\n\nM\n\nFigure 2: Panel (a) shows the dual-structure of exponential-family for a Gaussian defined in Eq. 6. lies above the parabola, while Ω is a half space. Panel (b) shows f (μ) and its biThe space conjugate f ∗∗(μ) with the blue mesh and red surface respectively. We use the loss from Fig. 1(b). 0 (black curve), The f ∗∗ function perfectly matches the original loss at the boundary where v clearly showing the regions for three minima A, B and C. Panel (c) shows the same but with respect to the standard mean-variance (ω, v) parameterization. Here, convexity of f ∗∗ is lost, but we see more clearly the negative (cid:96)(ω) appear at v 0 (black curve). At this limit, we also get (cid:96)(ω), but for higher variances, both f and f ∗∗ are smoothed version of (cid:96)(ω). f (μ) = f ∗∗(μ) = The slices at v = 0.5, 2, and 4 give rise to the curves shown in Fig. 1(b).\n\n→\n\n→\n\n−\n\n∈ V\n\nsider a function f (u), defined over u space v\n\n∈ U\n\n, the Fenchel biconjugate f ∗∗ and conjugate f ∗ are defined as follows,\n\nand taking values in the extended real line. Given a dual\n\nf ∗∗(u) = sup\n\nv(cid:48), u (cid:104)\n\n(cid:105) −\n\nf ∗(v(cid:48)),\n\nf ∗(v(cid:48)) = sup\n\nu(cid:48)(cid:48)\n\n∈ U\n\nu(cid:48)(cid:48), v(cid:48)\n\n(cid:104)\n\n(cid:105) −\n\nf (u(cid:48)(cid:48)),\n\n(4)\n\nv(cid:48)\n\n∈V\n\nwhere convex functions. Moreover, the biconjugate is the optimal convex lower bound: f (u)\n\ndenotes the inner product. Note that f need not be convex, but both f ∗ and f ∗∗ are\n\nf ∗∗(u).\n\n, (cid:104)·\n\n·(cid:105)\n\n≥\n\nWe will use the conjugate functions to connect the max-loss in SAM to the expected-loss involving regular, minimal exponential-family distributions (Wainwright et al., 2008) of the following form1,\n\nqλ(θ) = exp (\n\nλ, T(θ)\n\nA(λ)) ,\n\n(5)\n\n(cid:104)\n\n(cid:105) −\n\n∈\n\nwith natural parameter λ measure of 1 with Ω = ∞} be used to define conjugates. For every λ of the expectation parameter μ = Eθ qλ [T(θ)]. The space of all valid μ satisfies μ = all such μ form the interior of a dual space which we denote by μ using the inverse λ =\n\nΩ, sufficient statistics T(θ), log-partition function A(λ), and a base . Dual spaces naturally arise for such distributions and can Ω, there exists a unique ‘dual’ parameterization in terms A(λ), and . We can also write λ in terms of\n\nA∗(μ), and in general, both μ and λ are valid parameterizations.\n\nλ : A(λ) <\n\nM\n\n∇\n\n∈\n\n{\n\n∼\n\nIn what follows, we will always use μ to parameterize the distribution, denoting qμ(θ). As an example, for a Gaussian\n\nω, v) over scalar θ with mean ω and variance v > 0, we have,\n\n(θ\n\nT(θ) = (θ, θ2), μ = (cid:0)ω, ω2 + v(cid:1) , λ = (ω/v,\n\n1/(2v)) .\n\n(6)\n\nis the epigraph of parabola (ω, ω2), and Ω is a negative half space Because v > 0, the space because the second natural parameter is constrained to be negative; see Fig. 2(a). The dual spaces can be used to define the Fenchel conjugates.\n\nM\n\n−\n\n∇\n\nN\n\n|\n\nWe will use the conjugates of the ‘expected negative-loss’ acting over μ\n\nf (μ) = Eθ\n\nqμ [\n\n(cid:96)(θ)] .\n\n∼\n\n−\n\n1The notation used in the paper is summarized in Table 10 in App. H.\n\n3\n\ninterior(\n\n),\n\nM\n\n∈\n\n(7)\n\n∇A∗∇AλμΩMBiconjugateExpected LossOriginal negative lossBACABCBiconjugateExpected LossOriginal negative lossBACABCPublished as a conference paper at ICLR 2023\n\nFor all μ / ∈\n\nM\n\ninterior(\n\n), we set f (μ) = +\n\n. Using Eq. 4, we can obtain the biconjugate,\n\n∞ (cid:34)\n\nf ∗∗(μ) = sup\n\nλ(cid:48), μ\n\nλ(cid:48)\n\nΩ (cid:104)\n\n(cid:105) −\n\nsup\n\nμ(cid:48)(cid:48)\n\n∈ where we use λ(cid:48) and μ(cid:48)(cid:48) to denote arbitrary points from Ω and to the change of coordinates μ(cid:48) =\n\nA(λ(cid:48)).\n\n∈M\n\n∇\n\nμ(cid:48)(cid:48), λ(cid:48)\n\n(cid:104)\n\nEθ\n\n(cid:48)(cid:48) [\n\nμ\n\nq\n\n∼\n\n−\n\n(cid:105) −\n\n(cid:96)(θ) ]\n\n,\n\n(8)\n\n(cid:35)\n\n, respectively, and avoid confusion\n\nM\n\nWe show in App. A that, for an isotropic Gaussian, the biconjugate takes a form where max-loss ω, vI), automatically emerges. We define μ to be the expectation parameter of a Gaussian m, σ2I). The equation below and λ(cid:48) to be the natural parameter of a (different) Gaussian shows the biconjugate with the max-loss included in the second term, (cid:20)\n\n(θ\n\n(θ\n\nN\n\nN\n\n(cid:21)\n\n(cid:21)\n\n(cid:20)\n\n|\n\n|\n\nf ∗∗(μ) = sup m,σ\n\nEθ\n\nqμ\n\n∼\n\n−\n\n1 2σ2 (cid:107)\n\nθ\n\nm\n\n−\n\n2 (cid:107)\n\nsup (cid:15)\n\n−\n\n(cid:96)(m + (cid:15))\n\n−\n\n1 2σ2 (cid:107)\n\n(cid:15)\n\n2 (cid:107)\n\n.\n\n(9)\n\nThe max-loss here is in a proximal form, which has an equivalence to the trust-region form used in Eq. 1 (Parikh & Boyd, 2013, Sec. 3.4). The first term in the biconjugate arises after simplification term in Eq. 8, while the second term is obtained by using the theorem below to switch of the the maximum over μ(cid:48)(cid:48) to a maximum over θ, and subsequently over (cid:15) by rewriting θ = m + (cid:15). Theorem 1. For a Gaussian distribution that takes the form (5), the conjugate f ∗(λ(cid:48)) can be obtained by taking the supremum over Θ instead of\n\nλ(cid:48), μ (cid:104)\n\n, that is,\n\n(cid:105)\n\nsup\n\nμ(cid:48)(cid:48)\n\n∈M\n\nμ(cid:48)(cid:48), λ(cid:48) (cid:104)\n\n(cid:105) −\n\nEθ\n\nqμ(cid:48)(cid:48) [\n\n∼\n\n−\n\n(cid:96)(θ)] = sup\n\nT(θ), λ(cid:48)\n\n[\n\n(cid:105) −\n\n(cid:96)(θ)] ,\n\n−\n\n(10)\n\nΘ (cid:104)\n\nθ\n\n∈\n\nM\n\nassuming that (cid:96)(θ) is lower-bounded, continuous and majorized by a quadratic function. Moreover, there exists λ(cid:48) for which f ∗(λ(cid:48)) is finite and, assuming (cid:96)(θ) is coercive, dom(f ∗)\n\nΩ.\n\n⊆\n\nA proof of this theorem is in App. B, where we also discuss its validity for generic distributions.\n\nThe red surface in Fig. 2(b) shows the convex lower bound f ∗∗ to the function f (shown with the blue mesh) for a Gaussian with mean ω and variance v (Eq. 6). Both functions have a parabolic . Fig. 2(c) shows the same but with respect to the standard shape which is due to the shape of mean-variance parameterization (ω, v). In this parameterization, convexity of f ∗∗ is lost, but we see more clearly the negative (cid:96)(ω) appear at v 0 (shown with the thick black curve). At the limit, we get f (μ) = f ∗∗(μ) = (cid:96)(ω), but for higher variances, both f and f ∗∗ are smoothed versions of (cid:96)(ω). Fig. 2(c) shows three pairs of blue and red lines. The blue line in each pair is a slice of the function f , while the red line is a slice of f ∗∗. The values of v for which the functions are plotted are 0.5, 2, and 4. The slices at these variances are visualized in Fig. 1(b) along with the original loss.\n\nM\n\n→\n\n−\n\nThe function f ∗∗(μ) is a ‘lifted’ version of the one-dimensional (cid:96)(θ) to a two-dimensional μ-space. Such liftings have been used in the optimization literature (Bauermeister et al., 2022). Our work connects them to exponential families and probabilistic inference. The convex relaxation preserves the shape at the limit v 0, while giving smoothed lower bounds for all v > 0. For a high enough variance the local minima at A and C disappear, biasing the optimization towards flatter region B of the loss. We will now use this observation to connect to SAM.\n\n→\n\n2.2 SAM AS AN OPTIMAL RELAXATION OF THE BAYES OBJECTIVE\n\nWe will now show that a minimizer of the SAM objective in Eq. 1 can be recovered by minimizing a relaxation of the Bayes objective where expected-loss f is replaced by its convex lower bound f ∗∗. The new objective, which we call relaxed-Bayes, always lower bounds the Bayes objective of Eq. 2,\n\nsup μ\n\n∈M\n\n(qμ)\n\nL\n\n≥\n\n(cid:26)\n\nsup μ\n∈M (cid:20)\n\nf ∗∗(μ)\n\n−\n\nDKL(qμ(θ)\n\n= sup μ\n\n∈M\n\nsup m,σ −\n\nsup (cid:15)\n\n(cid:96)(m + (cid:15))\n\n1 2σ2 (cid:107)\n\n(cid:15)\n\n2 (cid:107)\n\n−\n\n(cid:107) (cid:21)\n\np(θ))\n\n(cid:18)\n\nqμ(θ)\n\nDKL\n\n−\n\n1\n\nZ\n\n(cid:107)\n\np(θ)e−\n\n1\n\n2σ2 (cid:107)\n\nθ\n\nm\n\n−\n\n(cid:107)\n\n2(cid:19)\n\n+ log\n\n(11)\n\n(cid:27)\n\n,\n\nZ\n\nwhere in the second line we substitute f ∗∗(μ) from (9), and merge Eθ 2] with the KLD term. denotes the normalizing constant of the second distribution in the KLD. The relaxedBayes objective has a specific difference-of-convex (DC) structure because both f ∗∗ and KLD are\n\nqμ [ 1\n\n2σ2 (cid:107)\n\nm\n\n−\n\nZ\n\nθ\n\n(cid:107)\n\n∼\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nconvex with respect to μ. Optimizing the relaxed-Bayes objective relates to double-loop algorithms where the inner-loop uses max-loss to optimize for (m, σ), while the outer loop solves for μ. We will use the structure to simplify and connect to SAM.\n\nDue to the DC structure, it is possible to switch the order of maximization with respect to μ and (m, σ), and eliminate μ by a direct maximization (Toland, 1978). The maximum occurs when the KLD term is 0 and the two distributions in its arguments are equal. A detailed derivation is in App. C, which gives us a minimization problem over (m, σ) (cid:21)\n\n(cid:20)\n\nErelaxed(m, σ; δ(cid:48)) =\n\nsup (cid:15)\n\n(cid:96)(m + (cid:15))\n\n−\n\n1 2σ2 (cid:107)\n\n(cid:15)\n\n2 (cid:107)\n\n+\n\nδ(cid:48) 2 (cid:107) (cid:124)\n\nm\n\n2 (cid:107)\n\nP 2\n(cid:123)(cid:122) log\n\n−\n\n=\n\n−\n\nZ\n\nlog(σ2δ(cid:48)) ,\n(cid:125)\n\n(12)\n\nwhere δ(cid:48) = 1/(σ2 + 1/δ0) and we use a Gaussian prior p(θ) = 0, I/δ0). This objective uses max-loss in the proximal form, which is similar to the SAM loss in Eq. 1 with the difference that the hard-constraint is replaced by a softer penalty term.\n\n(θ\n\nN\n\n|\n\nThe theorem below is our main result which shows that, for a given ρ, a minimizer of the SAM objective in Eq. 1 can be recovered by optimizing Eq. 12 with respect to m while keeping σ fixed. Theorem 2. For every (ρ, δ), there exist (σ, δ(cid:48)) such that\n\narg min θ\n\nΘ E\n\n∈\n\nΘ E\n\nm\n\n∈\n\nSAM(θ; ρ, δ) = arg min\n\nrelaxed(m, σ; δ(cid:48)),\n\n(13)\n\nassuming that the SAM-perturbation satisfies\n\n= ρ at a stationary point.\n\n(cid:15)\n\n(cid:107)\n\n(cid:107)\n\nA proof is given in App. D and essentially follows from the equivalences between proximal and trust-region formulations. The result establishes SAM as a special case of relaxed-Bayes, and shows that ρ and σ play the exactly same role. This formalizes the intuitive resemblance discussed at the beginning of Sec. 2, and suggests that, in both Bayes and SAM, flatter regions are preferred for higher σ because such values give rise to smoother objectives (Fig. 1(b)).\n\nThe result can be useful for combining complementary strengths of SAM and Bayes. For example, uncertainty estimation in SAM can now be performed by estimating σ through the relaxed-Bayes objective. Another way is to exploit the DC structure in μ-space to devise new optimization procedures to improve SAM. In Sec. 3, we will demonstrate both of these improvements of SAM.\n\nIt might also be possible to use SAM to improve Bayes, something we would like to explore in the future. For Bayes, it still is challenging to get a good performance in deep learning while optimizing for both mean and variance (despite some recent success (Osawa et al., 2019)). Often, non-Bayesian methods are used to set the variance parameter (Plappert et al., 2018; Fortunato et al., 2018; Bisla et al., 2022). The reasons behind the bad performance remain unclear. It is possible that a simplistic posterior is to blame (Foong et al., 2020; Fortuin et al., 2022; Coker et al., 2022), but our result suggests to not disregard the opposite hypothesis (Farquhar et al., 2020). SAM works well, despite using a simple posterior, and it is possible that the poor performance of Bayes is due to poor algorithms, not simplistic posteriors. We believe that our result can be used to borrow algorithmic techniques from SAM to improve Bayes, and we hope to explore this in the future.\n\n3 BAYESIAN-SAM TO OBTAIN UNCERTAINTY ESTIMATES FOR SAM\n\nIn this section, we will show an application of our result to improve an aspect of SAM. We will derive a new algorithm that can automatically obtain reasonable uncertainty estimates for SAM. We will do so by optimizing the relaxed-Bayes objective to get σ. A straightforward way is to directly optimize Eq. 12, but we will use the approach followed by Khan et al. (2018), based on the naturalgradient method of Khan & Rue (2021), called the Bayesian learning rule (BLR). This approach has been shown to work well on large problems, and is more promising because it leads to Adam-like algorithms, enabling us to leverage existing deep-learning techniques to improve training (Osawa et al., 2019), for example, by using momentum and data augmentation.\n\nTo learn variances on top of SAM, we will use a multivariate Gaussian posterior qμ(θ) = 1, where s is a vector of preciN sion values (inverse of the variances). The posterior is more expressive than the isotropic Gaussian\n\nω, V) with mean m and a diagonal covariance V = diag(s)−\n\n(θ\n\n|\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Our Bayesian-SAM (bSAM) is a simple modification of SAM with Adam . Just add the red boxes, or use them to replace the blue boxes next to them. The advantage of bSAM is that it automatically obtains variances σ2 through the vector s that adapts the learning rate (see line 11). Two other main changes are in line 3 where Gaussian sampling is added, and line 5 where the vector s is used to adjust the perturbation (cid:15). The change in line 8 improves the variance by adding the regularizer δ and a constant γ (just like Adam), while line 9 uses a Newton-like update where s is used instead of √s. We denote by a , the element-wise multiplication, division |\nand absolute value, respectively. N denotes the number of training examples.\n\nb, a/b and\n\na |\n\n·\n\nInput: Learning rate α, β1, L2-regularizer δ, SAM parameter ρ, β2 = 0.999, γ = 10−\n\n(NN weight init), scale s\n\n0 1 , and momentum vector gm ←\n\n←\n\n8 0.1 0\n\n←\n\n1: Initialize mean ω 2: while not converged do ω + e, where e 1\nB\n\n(cid:80) i\n\n←\n\n3:\n\n4:\n\ng\n\nθ\n\n5:\n\n6: 7:\n\n8:\n\n9:\n\n∈M ∇ g\ns\n\ng g\n(cid:107) (cid:80) i\n\n(cid:107) 1\nB\n\n∈M ∇ β1gm + (1\n\nβ2 s + (1\n\n←\n\nρ\n\n(cid:15)\n\n←\n\ng(cid:15) ← gm ←\n\n←\n\ns\n\nω\n\nω\n\nα\n\n←\n\n−\n\n| ∼ N (cid:96)i(θ) where\n\n{\n\n(cid:96)i(ω + (cid:15))\n\nβ1) (g(cid:15) + δω)\n\n− β2) (g(cid:15) + δω)2 gm s\n\n− gm √s+γ\n\n10: end while 11: Return the mean m and variance σ2\n\n1/(N\n\ns)\n\n·\n\n←\n\n(e\n\n0, σ2), σ2\n\n1/(N\n\ns)\n\n←\n\n·\n\n{\n\nis minibatch of B examples\n\nM Rescale by a vector s, instead of a scalar\n\nSample to improve the variance\n\n}\n\ng\n\n(cid:107)\n\n(cid:107)\n\nused in SAM\n\n}\n\n(cid:0)√s\n\ng\n\n|\n\n· |\n\n+ δ + γ(cid:1)\n\nImproved variance estimate\n\n{\n\n}\n\nScale the gradient by s instead of √s\n\n{\n\n}\n\nused in the previous section, and is expected to give better uncertainty estimates. As before, we will use a Gaussian prior p(θ) =\n\n0, I/δ0).\n\n(θ\n\nN\n\n|\n\nThe BLR uses gradients with respect to μ to update λ. The pair (μ, λ) is defined similarly to Eq. 6. We will use the update given in Khan & Rue (2021, Eq. 6) which when applied to Eq. 11 gives us,\n\nλ\n\n(1\n\nα)λ + α [\n\nf ∗∗(μ) + λ0] ,\n\n(14)\n\n−\n\n←\n\n∇ δ0I/2) is the natural parameter of p(θ). This is where α > 0 is the learning rate and λ0 = (0, λ0; see Khan & Rue (2021, Eq. 23). obtained by using the fact that For α = 1, the update is equivalent to the difference-of-convex (DC) algorithm (Tao & An, 1997). The BLR generalizes this procedure by using α < 1, and automatically exploits the DC structure in μ-space, unlike the naive direct minimization of (12) with gradient descent.\n\nμDKL(qμ(θ)\n\n− p(θ)) = λ\n\n∇\n\n−\n\n(cid:107)\n\n∇\n\nf ∗∗, which is obtained by maximizing a variant of Eq. 9, that replaces The update (14) requires the isotropic covariance by a diagonal one. For a scalable method, we solve them inexactly by using the ’local-linearization’ approximation used in SAM, along with an additional approximation to do single block-coordinate descent step to optimize for the variances. The rest of the derivations is similar to the previous applications of the BLR; see Khan et al. (2018); Osawa et al. (2019). The details are in App. E, and the resulting algorithm, which we call Bayesian-SAM is shown in Alg. 1.\n\nbSAM differs slightly from a direct application of Adam (Kingma & Ba, 2015) on the SAM objective, but uses the same set of hyperparameters. The differences are highlighted in Alg. 1, where bSAM is obtained by replacing the blue boxes with the red boxes next to them. For SAM with Adam, we use the method discussed in Bahri et al. (2022); Kwon et al. (2021) for language models. The main change is in line 5, where the perturbation (cid:15) is adapted by using a vector s, as opposed to SAM which uses a scalar . Modifications in line 3 and 8 are aimed to improve the variance estimates because with just Adam we may not get good variances; see the discussion in Khan et al. (2018, Sec 3.4). Finally, the modification in line 9 is due to the Newton-like updates arising from BLR (Khan & Rue, 2021) where s is used instead of √s to scale the gradient as explained in App. E.\n\ng\n\n(cid:107)\n\n(cid:107)\n\nDue to their overall structural similarity, the bSAM algorithm has a similar computational complexity to Adam with SAM. The only additional overhead compared to SAM-type methods is the computation of a random noise perturbation which is negligible compared to gradient computations.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n(a) Exact Bayes\n\n(b) bSAM\n\n(c) SAM (point est.)\n\n(d) SAM (Laplace)\n\nFigure 3: For a 2D logistic regression, bSAM gives predictive uncertainties that are similar to those obtained by an exact Bayesian posterior. White areas show uncertain predictive probabilities around 0.5. SAM’s point estimate gives overconfident predictions, while Laplace leads to underconfidence.\n\nModel / Data\n\nResNet-20-FRN CIFAR-10\n\nResNet-20-FRN CIFAR-100\n\nMethod\n\nSGD SAM-SGD SWAG VOGN Adam SAM-Adam bSAM (ours)\n\nSGD SAM-SGD SWAG VOGN Adam SAM-Adam bSAM (ours)\n\nAccuracy ↑ (higher is better)\n\nNLL ↓ (lower is better)\n\nECE ↓ (lower is better)\n\nAUROC ↑ (higher is better)\n\n86.55(0.35) 87.49(0.26) 86.80(0.10) 87.30(0.24) 80.85(0.98) 85.26(0.15) 88.72(0.24)\n\n55.82(0.97) 58.58(0.59) 56.53(0.40) 59.83(0.75) 39.73(0.97) 53.25(0.80) 62.64(0.33)\n\n0.0839(0.003) 0.56(0.014) 0.0710(0.003) 0.49(0.019) 0.0774(0.001) 0.53(0.017) 0.0315(0.003) 0.38(0.004) 0.1317(0.011) 0.83(0.063) 0.46(0.007) 0.0228(0.002) 0.34(0.005) 0.0163(0.002)\n\n0.1695(0.005) 1.91(0.025) 0.0989(0.005) 1.60(0.022) 0.1604(0.004) 1.86(0.018) 1.44(0.019) 0.0756(0.005) 2.29(0.045) 0.0295(0.018) 1.71(0.035) 0.0401(0.005) 1.32(0.001) 0.0311(0.003)\n\n0.878(0.005) 0.891(0.004) 0.880(0.006) 0.890(0.003) 0.820(0.013) 0.874(0.004) 0.903(0.003)\n\n0.811(0.004) 0.827(0.003) 0.814(0.004) 0.830(0.002) 0.775(0.004) 0.818(0.005) 0.841(0.004)\n\nTable 1: Comparison without data augmentation. The results are averaged over 5 random seeds, with standard deviation shown in the brackets. The best statistical significant performance is shown in bold. bSAM (gray row) gives either comparable or better performance than the rest. Small scale experiments on MNIST are in Table 6 in App. F.8.\n\n4 NUMERICAL EXPERIMENTS\n\nIn this section, we present numerical results and show that bSAM brings the best of the two worlds together. It gives an improved uncertainty estimate similarly to the best Bayesian approaches, but also improves test accuracy, just like SAM. We compare performance to many methods from the deep learning (DL) and Bayesian DL literature: SGD, Adam (Kingma & Ba, 2015), SWAG (Maddox et al., 2019), and VOGN (Osawa et al., 2019). We also compare with two SAM variants: SAM-SGD and SAM-Adam. Both are obtained by inserting the perturbed gradients into either SGD or Adam, as suggested in Foret et al. (2021); Bahri et al. (2022); Kwon et al. (2021). We compare these methods across three different neural network architectures and on five datasets of increasing complexity.\n\nThe comparison is carried out with respect to four different metrics evaluated on the validation set. The metrics are test accuracy, the negative log-likelihood (NLL), expected calibration error (ECE) (Guo et al., 2017) (using 20 bins) and area-under the ROC curves (AUROC). For an explanation of these metrics, we refer the reader to Osawa et al. (2019, Appendix H). For Bayesian methods, we report the performance of the predictive distribution (cid:98)p(y , θ) q(θ) dθ, which we | D approximate using an average over 32 models drawn from q(θ).\n\n) = (cid:82) p(y\n\n| D\n\n4.1\n\nILLUSTRATION ON A TOY EXAMPLE\n\nFig. 3 compares the predictive probability on a simple Bayesian logistic regression problem (Murphy, 2012, Ch. 8.4). For exact Bayes, we use numerical integration, which is feasible for this toy 2D problem. For the rest, we use diagonal Gaussians. White areas indicate probabilities around\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nModel / Dataset\n\nResNet-20-FRN / CIFAR-10\n\nResNet-20-FRN / CIFAR-100\n\nResNet-20-FRN / TinyImageNet\n\nResNet-18 / CIFAR-10\n\nResNet-18 / CIFAR-100\n\nMethod\n\nSGD SAM-SGD Adam SAM-Adam bSAM (ours)\n\nSGD SAM-SGD Adam SAM-Adam bSAM (ours)\n\nSGD SAM-SGD Adam SAM-Adam bSAM (ours)\n\nSGD SAM-SGD bSAM (ours)\n\nSGD SAM-SGD bSAM (ours)\n\nAccuracy ↑ (higher is better)\n\nNLL ↓ (lower is better)\n\nECE ↓ (lower is better)\n\nAUROC ↑ (higher is better)\n\n91.68(0.26) 92.29(0.39) 89.97(0.27) 91.57(0.21) 92.16(0.16)\n\n66.48(0.10) 67.27(0.22) 61.76(0.67) 65.34(0.32) 68.22(0.44)\n\n52.01(0.36) 52.25(0.26) 49.04(0.38) 51.17(0.45) 52.90(0.35)\n\n94.76(0.11) 95.72(0.14) 96.15(0.08)\n\n76.54(0.26) 78.74(0.19) 80.22(0.28)\n\n0.0397(0.002) 0.29(0.008) 0.0266(0.003) 0.25(0.004) 0.0610(0.003) 0.41(0.021) 0.26(0.004) 0.0329(0.002) 0.23(0.003) 0.0057(0.002)\n\n0.0524(0.004) 1.20(0.007) 0.0481(0.001) 1.19(0.011) 1.66(0.049) 0.1582(0.006) 1.23(0.012) 0.0166(0.003) 0.0258(0.003) 1.10(0.013)\n\n1.98(0.007) 0.0330(0.002) 1.97(0.013) 0.0155(0.002) 0.0502(0.004) 2.14(0.024) 2.02(0.014) 0.0460(0.004) 1.94(0.009) 0.0199(0.003)\n\n0.0304(0.001) 0.21(0.006) 0.14(0.004) 0.0134(0.001) 0.12(0.002) 0.0049(0.001)\n\n0.0501(0.002) 0.98(0.007) 0.79(0.007) 0.0445(0.002) 0.70(0.008) 0.0311(0.003)\n\n0.915(0.002) 0.920(0.003) 0.900(0.003) 0.918(0.001) 0.925(0.001)\n\n0.846(0.002) 0.848(0.002) 0.826(0.003) 0.847(0.002) 0.857(0.004)\n\n0.832(0.002) 0.827(0.005) 0.820(0.004) 0.828(0.004) 0.831(0.001)\n\n0.926(0.006) 0.949(0.003) 0.954(0.001)\n\n0.869(0.003) 0.887(0.003) 0.892(0.003)\n\nTable 2: Comparison with data augmentation. Similar to Table 1, the shaded row show that bSAM consistently improves over the baselines and is the overall best method.\n\n0.5, while red and blue are closer to 0 and 1 respectively. We see that the bSAM result is comparable to the exact posterior and corrects the overconfident predictions of SAM. Performing a Laplace approximation around the SAM solution leads to underconfident predictions. The posteriors are visualized in Fig. 5(c) and other details are discussed in App. F.1. We show results on another small toy example (’two moons’) in App. F.2.\n\nWe can also use this toy problem to see the effect of using f vs f ∗∗ in the Bayesian objective. Fig. 7 in App. F.3 shows a comparison of optimizing Eq. 11 to an optimization of the original Bayesian objective Eq. 2. For both, we use full covariance to avoid the inaccuracies arising due to a diagonal covariance assumption. We find that the posteriors are very similar for both f and f ∗∗, indicating that the gap introduced by the relaxation is also small.\n\n4.2 REAL DATASETS\n\nWe first perform a set of experiments without any data augmentation, which helps us to quantify the improvements obtained by the regularization induced by our Bayesian approach. This is useful in applications where it is not easy to do data-augmentation (for example, tabular or time series data). All further details about hyperparameter settings and experiments are in App. G. The results are shown in Table 1, and the proposed bSAM method overall performs best with respect to test accuracy as well as uncertainty metrics. bSAM also works well when training smaller networks on MNIST-like datasets, see Table 6 in App. F.8.\n\nIn Table 2 we show results with data augmentations (random horizontal flipping and random cropping). We consider CIFAR-10, CIFAR-100 and TinyImageNet, and still find bSAM to perform favorably, although the margin of improvement is smaller with data augmentation. We note that, for SGD on CIFAR-10 in Table 2, the accuracy (91.68% accuracy) is slightly better than the 91.25% reported in the original ResNet-paper (He et al., 2016, Table 6), by using a similar ResNet-20 architecture with filter response normalization (FRN), which has 0.27 million parameters.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: bSAM is less sensitive to the choice of ρ than both SAM-SGD and SAM-Adam. We show accuracy and NLL. Plots for the other metrics are in App. F.4.\n\nbSAM not only improves uncertainty estimates and test accuracy, it also reduces the sensitivity of SAM to the hyperparameter ρ. This is shown in Fig. 4 for SAM-SGD, SAM-Adam and bSAM, where bSAM shows the least sensitivity to the choice of ρ. Plots for other metrics are in App. F.4.\n\nTo understand the reasons behind the gains obtained with bSAM, we do several ablation studies. First, we study the effect of Gaussian sampling in line 3 in Alg. 1, which is not used in SAM. Experiments in App. F.5 show that Gaussian sampling consistently improves the test accuracy and uncertainty metrics. Second, in App. F.6, we study the effect of using posterior averaging over the quality of bSAM’s predictions, and show that increasing the number of sampled models consistently improves the performance. Finally, we study the effect of “m-sharpness”, where we distribute the mini-batch across several compute nodes and use a different perturbation for each node (m refers to the number of splits). This has been shown to improve the performance of both SAM (Foret et al., 2021) and Bayesian methods (Osawa et al., 2019). In App. F.7, we show that larger values of m lead to better performance for bSAM too. In our experiments, we used m = 8. For completeness, we show how m-sharpness is implemented in bSAM in App. E.4.\n\n5 DISCUSSION\n\nIn this paper, we show a connection between SAM and Bayes through a Fenchel biconjugate. When the expected negative loss in the Bayes objective is replaced by the biconjugate, we obtain a relaxed Bayesian objective, which involves a maximum loss, similar to SAM. We showed that SAM can be seen as optimizing the mean while keeping variance fixed. We then derived an extension where the variances are optimized using a Bayesian learning algorithm applied to the relaxation. The numerical results show that the new variant improves both uncertainty estimates and generalization, as expected of a method that combines the strengths of Bayes and SAM.\n\nWe expect this result to be useful for researchers interested in designing new robust methods. The principles used in SAM are related to adversarial robustness which is different from Bayes. We show that the Fenchel conjugate is the bridge between the two, and we used it to obtain an extension of SAM. Since SAM gives an excellent performance on many deep-learning benchmark, the techniques used there will be useful in improving Bayesian methods. Finally, the use of convex duality can be useful to improve theoretical understanding of SAM-like methods. It is possible to use the results from convex-optimization literature to study and improve adversarial as well as Bayesian methods. Our work may open such directions and provide a new path for research in improving robustness. Expectations with respect to distributions arise naturally in many other problems, and often sampling is used to approximate them. Our paper gives a different way to approximate such expectations using convex optimization rather than sampling. Such problems may also benefit from our results.\n\nACKNOWLEDGEMENTS\n\nWe would like to thank Pierre Alquier for his valuable comments and suggestions. This work is supported by the Bayes duality project, JST CREST Grant Number JPMJCR2112.\n\n9\n\n10−410−310−210−1100ρ203040506070Accuracy(higherisbetter)ResNet20-FRN/CIFAR-100SAM-SGDSAM-AdambSAM10−410−310−210−1100ρ123456NLL(lowerisbetter)ResNet20-FRN/CIFAR-100SAM-SGDSAM-AdambSAMPublished as a conference paper at ICLR 2023\n\nAUTHOR CONTRIBUTIONS STATEMENT\n\nList of Authors: Thomas M ̈ollenhoff (T.M.), Mohammad Emtiyaz Khan (M.E.K.).\n\nT.M. proposed the Fenchel biconjugates, their connections to SAM (Theorems 1 and 2), and derived the new bSAM algorithm. M.E.K. provided feedback to simplify these. T.M. conducted all the experiments with suggestions from M.E.K. Both authors wrote the paper together.\n\nREFERENCES\n\nPierre Alquier. User-friendly introduction to PAC-Bayes bounds. arXiv:2110.11216, 2021. 1\n\nDara Bahri, Hossein Mobahi, and Yi Tay. Sharpness-aware minimization improves language model\n\ngeneralization. In Association for Computational Linguistics (ACL), 2022. 1, 6, 7\n\nHartmut Bauermeister, Emanuel Laude, Thomas M ̈ollenhoff, Michael Moeller, and Daniel Cremers. Lifting the convex conjugate in Lagrangian relaxations: A tractable approach for continuous Markov random fields. SIAM J. Imaging Sci., 15(3):1253–1281, 2022. 4\n\nDevansh Bisla, Jing Wang, and Anna Choromanska. Low-pass filtering SGD for recovering flat optima in the deep learning optimization landscape. International Conference on Artificial Intelligence and Statistics (AISTATS), 2022. 5\n\nOlivier Catoni. PAC-Bayesian Supervised Classification. Number 56. Institute of Mathematical\n\nStatistics Lecture Notes – Monograph Series, 2007. 2\n\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform ResNets without pretraining or strong data augmentations. In International Conference on Learning Representations (ICLR), 2022. 1\n\nBeau Coker, Wessel P Bruinsma, David R Burt, Weiwei Pan, and Finale Doshi-Velez. Wide meanfield Bayesian neural networks ignore the data. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2022. 5\n\nImre Csisz ́ar and Frantiˇsek Mat ́uˇs. Closures of exponential families. Annals of probability, pp.\n\n582–600, 2005. 15\n\nSteven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex\n\noptimization. J. Mach. Learn. Res. (JMLR), 17(1):2909–2913, 2016. 20\n\nGintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Conference on Uncertainty in Artificial Intelligence (UAI), 2017. 1\n\nGintare Karolina Dziugaite and Daniel M Roy. Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors. In International Conference on Machine Learning (ICML), 2018. 1\n\nSebastian Farquhar, Lewis Smith, and Yarin Gal. Liberty or depth: Deep bayesian neural nets do not need complex weight posterior approximations. Advances in Neural Information Processing Systems, 33:4346–4357, 2020. 5\n\nAndrew Foong, David Burt, Yingzhen Li, and Richard Turner. On the expressiveness of approximate inference in Bayesian neural networks. Advances in Neural Information Processing Systems, 33, 2020. 5\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations (ICLR), 2021. 1, 7, 9, 17, 22\n\nVincent Fortuin, Adri`a Garriga-Alonso, Florian Wenzel, Gunnar R ̈atsch, Richard Turner, Mark In Interna-\n\nvan der Wilk, and Laurence Aitchison. Bayesian neural network priors revisited. tional Conference on Learning Representations (ICLR), 2022. 5\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. In International Conference on Learning Representations Noisy networks for exploration. (ICLR), 2018. 5\n\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning (ICML), 2016. 1\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\n\nnetworks. In International Conference on Machine Learning (ICML), 2017. 7\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 8, 23\n\nGeoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the\n\ndescription length of the weights. In Conference on Learning Theory (COLT), 1993. 1\n\nJean-Baptiste Hiriart-Urruty and Claude Lemar ́echal. Fundamentals of Convex Analysis. Springer,\n\n2001. 2\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997. 1\n\nHisham Husain and Jeremias Knoblauch. Adversarial interpretation of Bayesian inference.\n\nIn\n\nInternational Conference on Algorithmic Learning Theory, 2022. 1\n\nAlexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of bayesian neural nets via local linearization. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2021. 19\n\nPavel Izmailov, Sharad Vikram, Matthew D. Hoffman, and Andrew Gordon Wilson. What are bayesian neural network posteriors really like? In International Conference on Machine Learning (ICML), 2021. 23\n\nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. FantasIn International Conference on Learning\n\ntic generalization measures and where to find them. Representations (ICLR), 2020. 1\n\nChi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? In International Conference on Machine Learning (ICML), 2020. 16\n\nJean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner. A fair comparison of two popular flat minima optimizers: Stochastic weight averaging vs. sharpness-aware minimization. arXiv:2202.00661, 2022. 1\n\nMohammad Emtiyaz Khan and Haavard Rue. The Bayesian learning rule. arXiv:2107.04562, 2021.\n\n5, 6, 17, 20\n\nMohammad Emtiyaz Khan, Wu Lin, Voot Tangkaratt, Zuozhu Liu, and Didrik Nielsen. Variational adaptive-newton method for explorative learning. In NIPS Workshop on Advances in Approximate Bayesian Inference, 2017. 17\n\nMohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable Bayesian deep learning by weight-perturbation in Adam. In International Conference on Machine Learning (ICML), 2018. 1, 5, 6, 17\n\nMinyoung Kim, Da Li, Shell X Hu, and Timothy Hospedales. Fisher SAM: Information geometry and sharpness aware minimisation. In International Conference on Machine Learning (ICML), 2022. 1\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\n\nConference on Learning Representations (ICLR), 2015. 6, 7\n\nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: Adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning (ICML), 2021. 6, 7\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYong Liu, Siqi Mai, Minhao Cheng, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Random sharpness-aware minimization. Advances in Neural Information Processing Systems (NeurIPS), 2022. 21\n\nWesley J. Maddox, Pavel Izmailov, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 7\n\nLuigi Malag`o, Matteo Matteucci, and Giovanni Pistone. Towards the geometry of estimation of distribution algorithms based on the exponential family. In Foundations of Genetic Algorithms (FOGA), 2011. 15\n\nStephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate\n\nBayesian inference. J. Mach. Learn. Res. (JMLR), 18:1–35, 2017. 1\n\nKevin Patrick Murphy. Machine learning: a probabilistic perspective. MIT Press, 2012. 7, 19\n\nKazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard E Turner, Rio Yokota, and Mohammad Emtiyaz Khan. Practical deep learning with Bayesian principles. Advances in Neural Information Processing Systems (NeurIPS), 2019. 1, 5, 6, 7, 9, 18, 23\n\nNeal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1:\n\n123–231, 2013. 4, 16\n\nMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. In International Conference on Learning Representations (ICLR), 2018. 5\n\nCarl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning.\n\nMIT Press, 2006. 15\n\nRalph Tyrrell Rockafellar and Roger J.-B Wets. Variational Analysis. Springer, 1998. 17\n\nSamuel L. Smith and Quoc V. Le. A Bayesian perspective on generalization and stochastic gradient\n\ndescent. In International Conference on Learning Representations (ICLR), 2018. 1\n\nPham Dinh Tao and Le Thi Hoai An. Convex analysis approach to DC programming: Theory,\n\nalgorithms and applications. Acta Math. Vietnam., 22(1):289–355, 1997. 6\n\nJohn F Toland. Duality in nonconvex optimization. J. Math. Anal. Appl., 66(2):399–415, 1978. 5\n\nMartin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational\n\ninference. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008. 3\n\nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gener-\n\nalization. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 1\n\nArnold Zellner. Optimal information processing and Bayes’s theorem. The American Statistician,\n\n42(4):278–280, 1988. 2\n\nTong Zhang. Theoretical analysis of a class of randomized regularization methods. In Conference\n\non Learning Theory (COLT), 1999. 2\n\nYaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial model perturbation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA DERIVATION OF THE FENCHEL BICONJUGATE\n\nHere, we give the derivation of the Fenchel Biconjugate given in Eq. 9. For an isotropic Gaussian, we have T(θ) = (θ, θθ(cid:62)) and λ(cid:48) = (m/σ2, 1/(2σ2)I). We will use the following identity to simplify,\n\n−\n\nλ(cid:48), T(θ) (cid:104)\n\n(cid:105)\n\n=\n\nm(cid:62) σ2 θ + Tr\n\n(cid:19)\n\n(cid:18)\n\nI\n\n2σ2 θθ(cid:62)\n\n−\n\n=\n\n1 2σ2 (cid:107)\n\nm\n\n−\n\nθ\n\n−\n\n2 +\n\n(cid:107)\n\n1 2σ2 (cid:107)\n\nm\n\n2.\n\n(cid:107)\n\nUsing this, the first term in Eq. 8 becomes\n\nλ(cid:48), μ (cid:105) (cid:104)\n\n= Eθ\n\nλ(cid:48), T(θ) qμ [ (cid:104)\n\n∼\n\n] = Eθ (cid:105)\n\nqμ\n\n∼\n\n(cid:20)\n\n−\n\n1 2σ2 (cid:107)\n\nm\n\nθ\n\n2 (cid:107)\n\n−\n\n(cid:21)\n\n+\n\n1 2σ2 (cid:107)\n\nm\n\n2.\n\n(cid:107)\n\nSimilarly, for the second term in Eq. 8, we use Theorem 1 and Eq. 15 to simplify,\n\n(15)\n\n(16)\n\nsup\n\nμ(cid:48)(cid:48)\n\n∈M\n\nμ(cid:48)(cid:48), λ(cid:48) (cid:104)\n\n(cid:105) −\n\nEθ\n\nqμ[\n\n∼\n\n−\n\n(cid:96)(θ)] = sup\n\nλ(cid:48), T(θ)\n\n+ (cid:96)(θ)\n\n(cid:105)\n\nθ\n\n∈\n\nΘ (cid:104) (cid:20)\n\n= sup θ\nΘ (cid:20)\n\n∈\n\n=\n\nsup (cid:15)\n\n1 2σ2 (cid:107)\n\nθ\n\nm\n\n2 +\n\n(cid:107)\n\n−\n\n−\n\n(cid:21)\n\nm\n\n2 + (cid:96)(θ)\n\n(cid:107)\n\n(cid:96)(m + (cid:15))\n\n1 2σ2 (cid:107)\n\n(cid:15)\n\n−\n\n2 (cid:107)\n\n1 2σ2 (cid:107)\n\nm\n\n2,\n\n(cid:107)\n\n(17)\n\n1 2σ2 (cid:107) (cid:21)\n\n+\n\nwhere in the last step we have performed a change of variables θ (16), we arrive at Eq. 9.\n\n−\n\nm = (cid:15). Subtracting (17) from\n\nB PROOF OF THEOREM 1\n\nTheorem 1. For a Gaussian distribution that takes the form (5), the conjugate f ∗(λ(cid:48)) can be obtained by taking the supremum over Θ instead of\n\n, that is,\n\nsup\n\nμ(cid:48)(cid:48)\n\n∈M\n\nμ(cid:48)(cid:48), λ(cid:48)\n\n(cid:104)\n\n(cid:105) −\n\nEθ\n\nqμ(cid:48)(cid:48) [\n\n∼\n\n−\n\n(cid:96)(θ)] = sup\n\nT(θ), λ(cid:48)\n\n[\n\n(cid:105) −\n\n(cid:96)(θ)] ,\n\n−\n\n(10)\n\nΘ (cid:104)\n\nθ\n\n∈\n\nM\n\nassuming that (cid:96)(θ) is lower-bounded, continuous and majorized by a quadratic function. Moreover, there exists λ(cid:48) for which f ∗(λ(cid:48)) is finite and, assuming (cid:96)(θ) is coercive, dom(f ∗)\n\nΩ.\n\n⊆\n\nProof. We first pull out the expectation in the left-hand side of Eq. 10,\n\nsup\n\nμ(cid:48)(cid:48)\n\n∈M\n\nμ(cid:48)(cid:48), λ(cid:48)\n\n(cid:104)\n\n(cid:105) −\n\nEθ\n\n∼\n\nqμ(cid:48)(cid:48) [\n\n−\n\n(cid:96)(θ)] = sup\n\nμ(cid:48)(cid:48)\n\n∈M\n\nEθ\n\n∼\n\nλ(cid:48), T(θ) qμ(cid:48)(cid:48) [ (cid:105) (cid:104) (cid:123)(cid:122) (cid:124) =J(θ)\n\n].\n\n+ (cid:96)(θ) (cid:125)\n\nWe will denote the expression inside the expectation by J(θ), and prove that the supremum over μ(cid:48)(cid:48) is equal to the supremum over θ. We will do so for two separate cases where the supremum J\n\nis finite, and infinite respectively.\n\nΘ J(θ) <\n\n= supθ\n\n∈ is finite: For this case, our derivation proceeds in two steps, where we show that both the\n\nWhen J ∗\nfollowing two inequalities are true at the same time,\n\n∞\n\n∗\n\nsup\n\nμ(cid:48)(cid:48)\n\n∈M\n\nEθ\n\n∼\n\nqμ(cid:48)(cid:48) [J(θ)]\n\nJ\n\n,\n\n∗\n\n≤\n\nsup\n\nμ(cid:48)(cid:48)\n\n∈M\n\nEθ\n\n∼\n\nqμ(cid:48)(cid:48) [J(θ)]\n\n.\n\nJ\n\n∗\n\n≥\n\nThis is only possible when the two are equal, which will establish the result.\n\nTo prove the first inequality, we take a sequence\n\nθt\n\n∞t=1 for which J(θt)\n\nJ\n\nwhen sending t\n\n. By the definition of supremum, there always exists such a sequence. From the sequence\n\n∞ we will now construct a sequence This will establish that the supremum over μ(cid:48)(cid:48) is larger or equal than the supremum over θ. is only larger or equal, since hypothetically there could be another sequence of achieves higher value. We will see soon that this cannot be the case.\n\n→ ∞t=1 ∞t=1 for which the expectation of J(θ) also converges to J .\n∗ It ∞t=1 which\n\nμ(cid:48)(cid:48)t }\n\nμ(cid:48)(cid:48)t }\n\n→\n\nθt\n\n{\n\n}\n\n{\n\n}\n\n{\n\n{\n\n∗\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nμ(cid:48)(cid:48)t } We use the sequence ∞t=1 ⊂ }\nspace of symmetric positive-definite matrices with the sequence such that Σt see that the expectation Eθ [J(θ)] converges to the supremum as follows:\n\n∞t=1 where μ(cid:48)(cid:48)t = (θt, Σt + θtθ(cid:62)t ) with\n\nΣt\n\n{\n\n{\n\nqμ(cid:48)(cid:48)\n\nt\n\n∼\n\nP\n\nSP +\n\n×\n\nis taken in the 0. Now, we can\n\n→\n\nEθ\n\nqμ(cid:48)(cid:48)\n\nt\n\n∼\n\nlim t\n→∞\n\n[J(θ)] = lim →∞\n\nt\n\nE(cid:15)\n\n∼N (cid:104)\n\n(cid:104)\n\nJ\n\n(0,I)\n\nJ\n\nlim t\n→∞\n\n(cid:16)\n\n(cid:16)\n\nθt + Σ1/2 t (cid:15)\n\nθt + Σ1/2 t (cid:15)\n\n(cid:17)(cid:105)\n\n(cid:17)(cid:105)\n\n= E(cid:15)\n\n(0,I)\n\n∼N\n\nJ(θ).\n\n= sup RP θ\n\n∈\n\nIn the above steps, we used the fact that J is Gaussian-integrable and continuous. A sufficient condition for the integrability is that (cid:96)(θ) is lower-bounded and majorized by quadratics. With this, we have established that the supremum over μ(cid:48)(cid:48) is at least as large as the one over θ.\n\nTo prove the second inequality, we will show that the supremum over μ(cid:48)(cid:48) is also less or equal than the supremum over θ. To see that, we note that J(θ(cid:48)) Θ. This implies that for any distribution qμ(cid:48)(cid:48), we have that Eθ(cid:48) qμ(cid:48)(cid:48) [J(θ(cid:48))] . Since the inequality holds for any distribution, Eθ(cid:48) qμ(cid:48)(cid:48) [J(θ(cid:48))] we also have that supμ(cid:48)(cid:48) . Having established both inequalities, we now know that the suprema in (10) agrees whenever the supremum over θ is finite.\n\nfor all θ(cid:48)\n\n≤ ≤\n\nJ J\n\n∈M\n\n≤\n\n∈\n\nJ\n\n∼\n\n∼\n\n∗\n\n∗\n\n∗\n\n∗\n\nis infinite: For such cases, we will show that supμ(cid:48)(cid:48)\n\nWhen J , which will ∞\n∞t=1 for which complete the proof. Again, by definition of supremum, there exists a sequence }\n. This means that for any M > 0 there is a parameter θt(M ) in the J(θt) sequence such that J(θt(M )) > M . Now from the previous part of the proof, we know that we can construct a sequence qμ(cid:48)(cid:48) t of Gaussians whose expectation converges to J(θt(M )). Since the above holds for any M > 0, the supremum over the expectation is also +\n\nqμ(cid:48)(cid:48) [J(θ)] = θt\n\nwhen t\n\n→ ∞\n\n→ ∞\n\n∈M\n\n{\n\n∼\n\n.\n\nConditions for existence of λ(cid:48) for which J ∗\nconditions when there exist at λ(cid:48) for which J ∗\n\nis finite: The final step of our proof is to give the is finite, and therefore the result is non-trivial.\n\nEθ\n\n∞\n\nSince (cid:96)(θ) is majorized by a quadratic function, there exists a constant c matrix A such that the following bound holds:\n\n×\n\nP\n\nSP +\n\n∈\n\n(cid:96)(θ)\n\n≤\n\nc + b(cid:62)θ + 1\n\n2 θ(cid:62)Aθ.\n\nR, vector b\n\nRP and\n\n∈\n\n∈\n\nThen for the candidate λ(cid:48) = ( (cid:104) upper bound for any θ. Hence, the supremum is finite for this choice of λ(cid:48)\n\n2 A) we know that J(θ) = (cid:96)(θ) +\n\nb,\n\n−\n\n−\n\nλ(cid:48), T(θ) Ω.\n\n(cid:105) ≤\n\n1\n\nc has a finite\n\nDomain of f ∗(λ(cid:48)): For a Gaussian distribution, valid (λ(cid:48)1, λ(cid:48)2) We now show that if λ(cid:48) / ∈\nthat d(cid:62)λ(cid:48)2d\n\n. Since λ(cid:48)2 θt\n\n0. Taking the sequence\n\nΩ, then f ∗(λ(cid:48)) = +\n\n0 and λ(cid:48)1(cid:62)d\n\n⊀ 0, there exists a direction d ∞t=0 with θt = t\n\nΩ satisfy the constraint λ(cid:48)2 ≺ d we see\n\n0. = 0 such\n\n∞\n\n∈\n\n∈\n\n·\n\n≥\n\n≥\n\n{ J(θt) = tλ(cid:48)1(cid:62)d + t2d(cid:62)λ(cid:48)2d + (cid:96)(td)\n\n} (cid:96)(td)\n\n≥ for any sequence\n\n→ ∞ θt\n\nas t\n\n, → ∞\n\n∞t=1 with\n\nθt\n\n. (cid:107) → ∞\n\n(cid:107)\n\n{\n\n}\n\nsince coercivity of (cid:96)(θ) implies that (cid:96)(θt)\n\n→ ∞\n\nSome additional comments on the assumptions used in the theorem are in order.\n\n1. Any reasonable loss function that one would want to minimize is lower-bounded, so this\n\nassumption is always satisfied in practice.\n\n2. The existence of an upper-bounding quadratic holds for functions with Lipschitz continuous gradient, which is a standard assumption in optimization known as the descent lemma. In practice, our conjugates are also applicable to cases where this assumption is not satisfied. In such a case, one can consider the conjugate function only in a local neighbourhood around the current solution by restricting the set of admissible θ in the supremum. It may not matter that the bounding quadratic will eventually intersect the loss far away outside of our current region of interest.\n\n3. We assumed that (cid:96)(θ) is continuous since virtually all loss functions used in deep learning Interestingly, if (cid:96)(θ) is not continuous, the conjugate function f ∗ will\n\nare continuous. involve its semicontinous relaxation.\n\n14\n\n(cid:54) Published as a conference paper at ICLR 2023\n\n4. The coercivity assumption on (cid:96)(θ) is another standard assumption in optimization. For non-coercive losses, there could exist quadratic upper-bounds which are flat in some direction. This would lead to a dual variable λ(cid:48) at the boundary of Ω.\n\nThe above Theorem 1 also holds for other exponential family distributions such as scalar Bernoulli and Gamma distributions, but we do not give more details here because they are not relevant for connecting to SAM. There are also cases where the result does not hold. The simplest case is a Gaussian with fixed covariance. The above proof does not work there, because the closure (see e.g., Csisz ́ar & Mat ́uˇs (2005); Malag`o et al. (2011)) of the family of distributions does not contain all Dirac delta distributions. Intuitively, the requirement is that we should be able to approach any delta as a limit of a sequence of members of the family, which excludes the fixed-covariance case.\n\nC DERIVATION OF THE EQUIVALENT RELAXED OBJECTIVE (12)\n\nInserting the biconjugate (9) into the relaxed Bayes objective (11) yields\n\n(cid:20)\n\nsup m,σ −\n\nsup (cid:15)\n\n(cid:96)(m + (cid:15))\n\n(cid:21)\n\n1 2σ2 (cid:107)\n\n(cid:15)\n\n2 (cid:107)\n\n−\n\nsup μ\n\n∈M\n\nwhere we expanded the KLD as DKL(qμ(θ)\n\n+ Eθ\n\nqμ\n\n∼\n\n− p(θ)) = Eθ\n\n(cid:107)\n\n(cid:20)\n\n1 2σ2 (cid:107)\n\nθ\n\n2 + log p(θ)\n\nm\n\n(cid:107)\n\n−\n\n−\n\n(cid:21)\n\nlog qμ(θ)\n\n,\n\nqμ [log qμ(θ)]\n\n∼\n\n−\n\nEθ\n\n∼\n\nqμ [log p(θ)].\n\nInterchanging the order of maximization and noticing that the first term does not depend on μ, we can find a closed-form expression for the maximization in μ as follows:\n\nEθ\n\nqμ\n\n∼\n\n(cid:20)\n\n−\n\n1 2σ2 (cid:107)\n\nθ\n\nm\n\n(cid:107)\n\n−\n\n2 + log p(θ)\n\nlog qμ(θ)\n\n−\n\n(cid:21)\n\nsup μ\n\n∈M\n\n(cid:20)\n\nlog\n\n1\n\nexp\n\n(cid:18)\n\n1 2σ2 (cid:107)\n\nθ\n\n−\n\n(cid:19)\n\n2\n\nm\n\n(cid:107)\n\np(θ)\n\n−\n\n(cid:21)\n\nlog qμ(θ)\n\n+ log\n\nZ\n\nZ (cid:18) DKL\n\nqμ(θ)\n\n1\n\nZ\n\n(cid:107)\n\n− (cid:18)\n\nexp\n\n−\n\n1 2σ2 (cid:107)\n\nθ\n\nm\n\n2 (cid:107)\n\n−\n\n(cid:19)\n\n(cid:19)\n\np(θ)\n\n= sup μ\n\n∈M\n\nEθ\n\nqμ\n\n∼\n\nZ −\n\ninf ∈M\n\nμ\n\n= log\n\n= log\n\n.\n\nZ\n\nexp (cid:0)\n\n1\n\n2σ2 (cid:107)\n\nθ\n\n−\n\n−\n\nm\n\n2(cid:1) p(θ). (cid:107)\n\nIn the last step, the KLD is attained at zero with qμ(θ) = 1\n\nFinally, we compute\n\nas\n\nZ (cid:90)\n\n=\n\nexp\n\nZ\n\n(cid:18)\n\n−\n\n= (2πσ2)P/2\n\nθ\n\n1 2σ2 (cid:107) (cid:90)\n\nZ\n\n(cid:19)\n\nm\n\n2 (cid:107)\n\n−\n\np(θ) dθ\n\n(2πσ2)−\n\nP/2 exp\n\n= (σ2)P/2(σ2 + 1/δ0)−\n\nP/2 exp\n\n(cid:18)\n\n−\n\n(cid:18)\n\n−\n\n−\n\nθ\n\n1 2σ2 (cid:107) m\n2(σ2 + 1\n\n2 (cid:107)\n\n(cid:107)\n\nδ )\n\n(cid:19)\n\np(θ) dθ\n\nm\n\n2 (cid:107)\n\n(cid:19)\n\n,\n\nwhere in the last step we inserted the normalization constant for the product of the two Gaussians\n\nI) as for example given in Rasmussen & Williams (2006, App. 2).\n\n(θ\n\nm, σ2I) and\n\n(θ\n\n0, 1 δ0\n\n|\n\nN This gives the following expression\n\nN\n\n|\n\nlog\n\nZ\n\n=\n\n=\n\nP 2\n\nP 2\n\nlog(σ2)\n\n−\n\nlog(σ2) +\n\nP 2\n\nP 2\n\nlog(σ2 + 1/δ0)\n\n1 2(σ2 + 1 δ0\n\n−\n\nlog(δ(cid:48))\n\nδ(cid:48) 2 (cid:107)\n\nm\n\n2,\n\n(cid:107)\n\n−\n\nm\n\n) (cid:107)\n\n(cid:107)\n\n2\n\n(18)\n\nwhere δ(cid:48) = 1/(σ2 + 1/δ0). Switching to a minimization over m, σ2 and inserting the result for the exact μ-solution (18) leads to the final objective (cid:20)\n\n(cid:21)\n\n1 2σ2 (cid:107)\n\n(cid:15)\n\n2 (cid:107)\n\nδ(cid:48) 2 (cid:107)\n\nm\n\n2 (cid:107)\n\n−\n\nP 2\n\nlog(σ2)\n\nlog(δ(cid:48)),\n\nP 2\n\n−\n\ninf m,σ2\n\nsup (cid:15)\n\n(cid:96)(m + (cid:15))\n\n−\n\nwhich is the energy function in (12).\n\n+\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nD PROOF OF THEOREM 2\n\nTheorem 2. For every (ρ, δ), there exist (σ, δ(cid:48)) such that\n\narg min θ\n\nΘ E\n\n∈\n\nΘ E\n\nm\n\n∈\n\nSAM(θ; ρ, δ) = arg min\n\nrelaxed(m, σ; δ(cid:48)),\n\n(13)\n\nassuming that the SAM-perturbation satisfies\n\n= ρ at a stationary point.\n\n(cid:15)\n\n(cid:107)\n\n(cid:107)\n\nProof. We will show that any stationary point of Erelaxed when σ and δ(cid:48) are chosen appropriately. The correspondence between the stationary points follows directly from the equivalences between proximal and trust-region formulations (Parikh & Boyd, 2013, Sec. 3.4).\n\nESAM is also a stationary point of\n\nTo show this, we first write the two objectives in terms of perturbation (cid:15)(cid:48) and (cid:15) respectively,\n\nmin m\n\nsup (cid:15)(cid:48)\n\nmin θ\n\nsup (cid:15)\n\n(cid:96)(m + (cid:15)(cid:48)) +\n\n1 2σ2 (cid:107)\n\n(cid:15)(cid:48)\n\n2 +\n\n(cid:107)\n\n(cid:96)(θ + (cid:15)) + i\n\n{(cid:107)\n\n(cid:15)\n\n(cid:107) ≤\n\n+\n\nρ\n\n}\n\nδ(cid:48) 2 (cid:107) δ\n2 (cid:107)\n\nm\n\n2,\n\n(cid:107)\n\nθ\n\n2,\n\n(cid:107)\n\nwhere i otherwise.\n\n{(cid:107) · (cid:107) ≤\n\nρ\n\n}\n\nis the indicator function which is zero inside the (cid:96)2-ball of radius ρ and +\n\n∞\n\nSince these are min-max problems, we need an appropriate notion of “stationary point”, which we consider here to be a local Nash equilibirum (Jin et al., 2020, Prop. 3). At such points, the first derivatives in θ and (cid:15)(cid:48) (or (cid:15)) vanish. Taking the derivatives and setting them to 0, we get the following conditions for the two problems,\n\nδ(cid:48)m ∗\n\nδθ\n\n∗\n\n=\n\n=\n\n−∇\n\n−∇\n\n(cid:96)(m\n\n(cid:96)(θ\n\n∗\n\n),\n\n+ (cid:15)(cid:48) ∗\n),\n\n∗ + (cid:15)\n\n∗\n\n=\n\n=\n\n(cid:15)(cid:48) ∗\n\n(cid:15)\n\n∗\n\n(cid:96)(m\n\n∇\n\nσ2 ρ\nμ ∇\n\n−\n\n−\n\n(cid:96)(θ\n\n∗\n\n),\n\n+ (cid:15)(cid:48) ∗\n),\n\n∗ + (cid:15)\n\n∗\n\nρ is for some constant multiplier μ > 0. Here, we used the assumption that the constraint active at our optimal point. Since the constraint is active, the negative gradient is an element of the (non-trivial) normal cone, which gives us the multiplier μ in the second line above.\n\n∗(cid:107) ≤\n\n(cid:107)\n\n(cid:15)\n\nThe two equations are structurally equivalent. Any pair (m ) satisfying the first line also satisfies ∗\nthe second line for σ2 = ρ/μ and δ(cid:48) = δ. The opposite is also true when using the pair (θ ) in the first line, which proves the theorem. We assume that the constraint in the perturbation is active, = ρ. This assumption would be violated if there exists a local maximum within a ρ-ball that is, ∗(cid:107) around the parameter θ . However, such a local maximum is unlikely to exist since the parameter θ\n\nis determined by minimization and tends to lie inside a flat minimum.\n\n, (cid:15)(cid:48) ∗\n\n, (cid:15)\n\n(cid:107)\n\n(cid:15)\n\n∗\n\n∗\n\n∗\n\n∗\n\nE DERIVATION OF THE BSAM ALGORITHM\n\nIn this section of the appendix, we show how the natural gradient descent update (14) leads us to the proposed bSAM algorithm shown in Alg. 1. As mentioned in the main text, for bSAM we consider a ω, V) has diagonal covariance. generalized setting where our Gaussian distribution qμ(θ) = This corresponds to the following setting:\n\n(θ\n\nN\n\n|\n\nT(θ) = (θ, θθ(cid:62)), μ = (cid:0)ω, ω2 + diag(s)−\n\n1(cid:1) , λ = (cid:0)s\n\nω,\n\n1\n\n2 diag(s)(cid:1) .\n\n(19)\n\n· 1 denotes the entries of the inverse diagonal covariance\n\n−\n\nHere, ω matrix. All operations (squaring, multiplication) are performed entrywise.\n\nRP is the mean and diag(s) = V−\n\n∈\n\nE.1 THE GRADIENT OF THE BICONJUGATE\n\nFirst, we discuss how to compute that gradient of the Fenchel biconjugate, as the BLR update (14) requires it in every iteration. The diagonal Gaussian setup leads to a slightly generalized expression\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nfor the Fenchel biconjugate function:\n\n(cid:20)\n\nf ∗∗(μ) =\n\n−\n\nm\n\n∈\n\nmin RP ,b (cid:20)\n\n∈\n\nRP +\n\nsup RP (cid:15)\n\n∈\n\n=\n\nm\n\nmin RP ,b\n\n∈\n\nRP +\n\n∈\n\nsup RP (cid:15)\n\n∈\n\n(cid:96)(m + (cid:15))\n\n(cid:96)(m + (cid:15))\n\n(cid:21)\n\n1 2 (cid:107)\n\n−\n\nΣ−\n\n1\n\n2 (cid:15)\n\n2\n\n(cid:107)\n\n+ Eθ\n\nqμ\n\n∼\n\n(cid:20) 1\n\nΣ−\n\n2 (cid:107)\n\n(cid:21)\n\n2\n\n1\n\n2 (θ\n\nm)\n\n(cid:107)\n\n−\n\n1 2 (cid:107)\n\n−\n\nΣ−\n\n1\n\n2 (cid:15)\n\n2\n\n(cid:107)\n\n(cid:21)\n\n+\n\n1 2 (cid:107)\n\nΣ−\n\n1\n\n2 (ω\n\nm)\n\n2 +\n\n(cid:107)\n\n−\n\n1 2\n\ntr(Σ−\n\n1V), (20)\n\nwhere Σ = diag(1/b) denotes the diagonal covariance matrix with 1/b on its diagonal.\n\nIt follows from results in convex analysis, for instance from Rockafellar & Wets (1998, Propo- ), that is, the gradient of the biconjugate can be ,\nsition 11.3), that ∗\nconstructed from the optimal solution (m ∗\n\n) of the optimization problem (20).\n\n1 f ∗∗(μ) = (Σ− ∗\n\n− , b ∗\n\n2 b\n\nm\n\n∇\n\n∗\n\n1\n\nIn (20) there are three optimization steps: over (cid:15), m, and b. For the first two, we will make very similar approximation to what is used for SAM (Foret et al., 2021), and simply add an additional optimization over b. To simplify computation, we will make an additional approximation. Instead of an iterative joint minimization in m and b, we will perform a single block-coordinate descent step where we first minimize in m for fixed b = s and then afterwards in b for fixed ω = m. This will give rise to a simple algorithm that works well in practice.\n\nOptimization with respect to (cid:15). To simplify the supremum in (cid:15) we will use SAM’s technique of local linearization (Foret et al., 2021) with one difference. Instead of using the current solution to 1). This is preferable linearize, we will sample a random linearization point θ from a Bayesian perspective and is also used in the other Bayesian variants (Khan et al., 2018). Linearization at the sampled θ then gives us,\n\nω, diag(s)−\n\n∼ N\n\n(θ\n\n|\n\nUsing this approximation in (20), we get a closed form solution for (cid:15)\n\n(cid:96)(m + (cid:15))\n\n(cid:96)(θ) +\n\n(cid:96)(θ), m + (cid:15)\n\n≈\n\n(cid:104)∇\n\nθ\n\n.\n\n(cid:105)\n\n−\n\n= Σ\n\n(cid:96)(θ).\n\n∇\n\n∗\n\nOptimization with respect to m. To get a closed-form solution for the optimization in m, we again consider a linearized loss: (cid:96)(m + (cid:15)) . Moreover, we set (cid:105) (cid:15) = V\n\n(cid:96)(θ), corresponding to a fixed b = s. This reduces the optimization of (20) to\n\n(cid:96)(ω + (cid:15)), m\n\n(cid:96)(ω + (cid:15)) +\n\n(cid:104)∇\n\n−\n\n≈\n\nω\n\nm\n\n∗\n\n= arg min m\n\nRP\n\n∈\n\n(cid:96)(ω + (cid:15)), m (cid:105)\n\n(cid:104)∇\n\n+\n\n1 2 (cid:107)\n\nΣ− ∗\n\n1/2\n\n(m\n\n2,\n\nω)\n\n(cid:107)\n\n−\n\n(21)\n\nwhich has closed form solution m\n\n= ω\n\nΣ\n\n−\n\n∗∇\n\n∗\n\n(cid:96)(ω + (cid:15)) which we here leave to depend Σ ∗\n\n.\n\n∇\n\nOptimization with respect to Σ via b. mization over b while fixing m = ω also has a simple closed-form solution as shown below,\n\ninto (20) with the linearized loss, the mini-\n\nInserting (cid:15)\n\n∗\n\nb\n\n∗\n\n= arg min b\n\nRP +\n\n∈\n\nΣ1/2\n\n(cid:107)\n\n(cid:96)(θ)\n\n2 +\n\n(cid:107)\n\n∇\n\nP (cid:88)\n\ni=1\n\nbi si\n\n(cid:112)\n\ns\n\n=\n\ng2 =\n\n·\n\n√s,\n\ng\n\n|\n\n| ·\n\n(22)\n\nwhere θ\n\n(θ\n\n|\n\n∼ N\n\nm, diag(s)−\n\n1) and g =\n\n(cid:96)(θ).\n\n∇\n\nE.2 SIMPLIFYING THE NATURAL-GRADIENT UPDATES\n\nHaving a way to approximate the biconjugate-gradient, we now rewrite and simplify the BLR update (14). The derivations are largely similar to the ones used to derive the variational online Newton approaches, see Khan et al. (2017; 2018); Khan & Rue (2021).\n\nInserting the parametrization (19) and our expression update (14), we get the following updates\n\n1 f ∗∗(μ) = (Σ− ∗\n\n,\n\nm\n\n∗\n\n∇\n\n−\n\n1\n\n2 b\n\n∗\n\n) into the BLR\n\ns\n\n·\n\nω s\n\n← ←\n\n(1 (1 1\n\n− −\n\n1 ω + αΣ− ∗\n\nα)s ·\nα)s + α(b ∗\n\n+ δ0),\n\nm\n\n,\n\n∗\n\nwhere we also used the prior λ0 = (0, them in an equivalent form that resembles an adaptive gradient method: (1\n\n+ δ0),\n\n−\n\ns\n\nα)s + α(b ∗\nαV (cid:2)Σ− (ω\n\n1\n\nω\n\nω\n\n←\n\n←\n\n−\n\n−\n\n) + δω(cid:3) .\n\nm\n\n∗\n\n−\n\n2 δ0I). By changing the order of the updates, we can write\n\n(23)\n\n(24)\n\n∗\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n(25)\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\nNow, inserting the solutions Σ− ∗\nand 22 into these updates, we arrive at\n\n(ω\n\n−\n\n1\n\n(cid:96)(θ + (cid:15)) and b ∗\n\n=\n\n(cid:96)(θ)\n\n| ·\n\n|∇\n\n√s from Eqs. 21\n\nm\n\n) =\n\n∗\n\n∇ α)s + α (cid:0) αV (\n\n(1\n\nω\n\n−\n\ns\n\nω\n\n←\n\n← (θ\n\n− ω, V).\n\n∇\n\n√s + δ0\n\n(cid:1) ,\n\n(cid:96)(θ)\n\n|∇\n\n| ·\n\n(cid:96)(ω + (cid:15)) + δ0ω) ,\n\nwhere (cid:15) = V\n\n(cid:96)(θ) and θ\n\n∇\n\n∼ N\n\n|\n\nE.3 FURTHER MODIFICATIONS FOR LARGE-SCALE DEEP LEARNING\n\nNow we will consider further modifications to the updates (25) – (26) to arrive at our bSAM method shown in Alg. 1. These are similar to the ones used for VOGN (Osawa et al., 2019).\n\nFirst, we consider a stochastic approximation to deal with larger problems. To that end, we now briefly describe the general learning setup. The loss is then written as a sum of individual losses, (cid:96)(θ) = (cid:80)N i=1 (cid:96)i(θ) where N is the number of datapoints. The N datapoints are disjointly partitioned into K minibatches\n\nBi\n\n{\n\n}\n\nK i=1 of B examples each. (cid:80)K\n\nWe then apply a stochastic variant to the bound N 1 (cid:80) Eθ functions are fi(μ) = 1 j\nan incrementally sampled fi∗∗ rather than considering the full dataset.\n\nf ∗∗(μ), where the individual (cid:96)j(θ)]. Essentially, the variant computes the gradient on\n\ni=1 f ∗∗i (μ)\n\nqμ[\n\n≤\n\n−\n\nBi\n\nK\n\n∼\n\nB\n\n∈\n\nWith s = Nˆs, δ0 = N δ this turns (25) – (26) into the following updates: (cid:105)\n\n(cid:104)\n\nNˆs\n\nω\n\n←\n\n← (cid:96)j(θ), θ\n\n(1\n\nω\n\n−\n\n−\n\nα)Nˆs + α\n\nN δ + N √Nˆs\n\ng\n\n· |\n\n|\n\n,\n\nα [N δω + N g(cid:15)] /(Nˆs),\n\n(cid:80)\n\nj\n\nB\n\nwhere g = 1 denotes a randomly drawn minibatch. Moreover, we have g(cid:15) = 1 (cid:96)j(ω + (cid:15)) with (cid:15) = g/(Nˆs) = ρ g/ˆs where we introduce the step-size ρ > 0 in the adversarial step to absorb the 1/N factor and to gain additional control over the perturbation strength.\n\nω, diag(σ2)), σ2 = (Nˆs)−\n\n1 and\n\n∈M ∇\n\n∈M ∇\n\n(cid:80) j\n\n∼ N\n\nM\n\n(θ\n\nB\n\n|\n\nFollowing the practical improvements of Osawa et al. (2019), we introduce a damping parameter γ > 0 and seperate step-size β2 > 0 on the precision estimate, as well as an exponential moving average for the gradient β1 > 0. Under these changes, and dividing out the N -factors in (27) – (28) we arrive at the following method:\n\ngm ←\n\nβ1gm + (1\n\nˆs\n\nω\n\nβ2ˆs + (1\n\n−\n\n←\n\nω\n\n←\n\n−\n\nαgm/ˆs,\n\n− β2)\n\nβ1) (δω + g(cid:15)) ,\n\n(cid:104)\n\nδ + γ + √Nˆs\n\n(cid:105)\n\n,\n\ng\n\n|\n\n· |\n\nwith g and g(cid:15) defined as above. In practice, the method performed better without the √N -factor in the ˆs-update. Renaming ˆs as s, the updates (29) – (31) are equivalent to bSAM (Alg. 1).\n\nE.4 BSAM ALGORITHM WITH m-SHARPNESS\n\nThe bSAM algorithm parallelizes well onto multiple accelerators. This is done by splitting up a minibatch into m parts and computing independent perturbations for each part in parallel. The algorithm remains exactly the same as Alg. 1, with lines 3 – 6 replaced by the following lines 1 – 8:\n\n1: Equally partition {M 2: for k = 1 . . . m in parallel do\n\ninto\n\nM\n\n1, . . . ,\n\nof B examples each\n\nM\n\nm\n\n}\n\n3:\n\n4:\n\n5:\n\n6:\n\nθk\n\n←\n\ngk ←\n\n(cid:15)k\n\n←\n\ng(cid:15),k ←\n\nω + ek, ek (1/B) (cid:80) gk/s\n\ni\n\n(1/B) (cid:80)\n\n7: end for\n\n0, σ2), σ2\n\n1/(N\n\ns)\n\n·\n\n←\n\n(ek |\n(cid:96)i(θk)\n\n∼ N\n\n∈M\n\nk ∇\n\n(cid:96)i(ω + ek)\n\ni\n\n∈M\n\nk ∇\n\n8: g(cid:15) ←\n\n(1/m) (cid:80)m\n\nk=1 g(cid:15),k, g\n\n(1/m) (cid:80)m\n\nk=1 gk\n\n←\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nF ADDITIONAL DETAILS AND EXPERIMENTS\n\nF.1 DETAILS ON THE LOGISTIC REGRESSION EXPERIMENT\n\nFig. 5(a) shows the binary classification data-set (red vs blue circles) we adopted from (Murphy, 2012, Ch. 8.4). The data can be linearly seperated by a linear classifier with two parameters whose decision boundary passes through zero. We show the decision boundaries obtained from the MAP solution and from samples of the exact Bayesian posterior in Fig. 5(a).\n\nFig. 5(b) shows the 2D probability density function of the exact Bayesian posterior over the two parameters, where the black cross indicates the mode (MAP solution), and the orange triangles the weight-samples corresponding to the classifiers shown in the left figure. In Fig. 5(c), we show the solutions found by SAM, bSAM and an approximate Bayesian posterior. On this example, the posterior approximation found by bSAM is similar to the Bayesian one. Performing a Laplace approximation at the SAM solution leads to the covariance shown by the green dashed ellipse. It overestimates the extend of the posterior as it is based only on local information.\n\n(a)\n\n(b)\n\n(c)\n\nFigure 5: Posterior shapes for the logistic regression problem shown in Sec. 4.1.\n\nF.2 COMPARISON ON A LOW-DIMENSIONAL CLASSIFICATION PROBLEM\n\nIn Fig. 6, we compare predictive uncertainties of SAM and bSAM on the two-moons classification problem. We fit a small network (2 1). For SAM+Laplace, we consider a diagonal GGN approximation to the Hessian (which was indefinite at the SAM solution) and use the linearized model for predictions, as suggested by Immer et al. (2021).\n\n24\n\n12\n\n12\n\n−\n\n−\n\n−\n\n−\n\n(a) bSAM\n\n(b) SAM\n\n(c) SAM + Laplace\n\nFigure 6: bSAM improves SAM’s predictive uncertainty. Using a Laplace approximation at the SAM solution can lead to an overestimation of the predictive uncertainty.\n\nF.3 EFFECT OF USING f ∗∗ INSTEAD OF f\n\nIn this section, we empirically check the effect of using f ∗∗ in the Bayesian objective instead of f . To that end, we compute an exact solution to the relaxed problem as well as the original problem (2) for a full Gaussian variational family.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFor small problems, gradients of the biconjugate function can be easily approximated using convex optimization. To see this, notice that:\n\nf ∗∗(μ) = arg min\n\n∇\n\nf ∗(λ(cid:48))\n\nλ(cid:48), μ (cid:105)\n\n− (cid:104)\n\nλ(cid:48)\n\nΩ\n\n∈\n\n= arg min R\n\nΩ,c\n\nλ(cid:48)\n\n∈\n\n∈\n\nc\n\nλ(cid:48), μ (cid:105)\n\n− (cid:104)\n\ns.t. c\n\n≥\n\nf ∗(λ(cid:48)).\n\n(32)\n\nThis is a optimization problem with linear cost function and convex constraints. To implement the constraints, we approximate the loss by a maximum over linear functions, which is always possible qμ for convex losses. The linear functions are first-order approximations of the loss at L points θi drawn from the current variational distribution. Under this approximation, the constraint in (32) can 1, . . . , L be written as L seperate constraints for i\n\nas follows:\n\n∼\n\n∈ { θ(cid:62)λ(cid:48)1 + θ(cid:62)λ(cid:48)2θ + (cid:96)(θi) +\n\n}\n\n(cid:96)(θi), θ\n\n(cid:104)∇\n\nθi\n\n−\n\n(cid:105) ≤\n\nc\n\nsup Θ\nθ\n\n∈\n\n1 4\n\n(λ(cid:48)1 +\n\n∇\n\n(cid:96)(θi))(cid:62)λ(cid:48)2−\n\n1\n\n(λ(cid:48)1 +\n\n(cid:96)(θi))\n\nc\n\n−\n\n∇\n\n(cid:96)(θi), θi\n\n≤ (cid:104)∇\n\n(cid:96)(θi).\n\n(cid:105) −\n\n(33)\n\n⇔ −\n\nThe convex program (32) with convex constraints (33) is then solved using the software package CVXPY (Diamond & Boyd, 2016) to obtain the gradient of the biconjugate function.\n\nHaving the gradients of the biconjugate available, our posterior is obtained by the Bayesian learning rule of Khan & Rue (2021),\n\nλ\n\n(1\n\n−\n\n←\n\nα)λ + α\n\n(cid:26)λ0 + λ0 +\n\nf (μ), f ∗∗(μ),\n\n∇ ∇\n\nfor Bayes (2), for relaxed-Bayes (11).\n\n(34)\n\nFig. 7 compares both solutions obtained by iterating (34) on the logistic regression problem described in App. F.1. We can see that the relaxation induces a lower-bound as expected, but the relaxed solution is still a reasonable approximation to the true posterior.\n\n(a)\n\n(b)\n\nFigure 7: Fig. 7(a) compares the posterior approximations obtained by solving (2) and our lowerbound (11) for a full Gaussian variational family. Fig. 7(b) shows the gap in the evidence-lower bound (ELBO) induced by the relaxation.\n\nF.4 SENSITIVITY OF BSAM TO THE CHOICE OF ρ\n\nWe show the sensitivity of all considered SAM variants to the hyperparameter ρ in Fig. 8. As the bSAM algorithm learns the variance vector, the method is expected to be less sensitive to the choice of ρ. We confirm this here for a ResNet–20 on the CIFAR-100 dataset without data augmentation.\n\nF.5 ABLATION STUDY: EFFECT OF GAUSSIAN SAMPLING (LINE 3 IN ALG. 1)\n\nIn the derivation of the bSAM algorithm in Sec. 3 we performed a local linearization in order to approximate the inner problem. There, we claim that performing this local linearization not at the\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: Sensitivity of SAM-variants to ρ. For bSAM, we plot the markers at 100 ρ to roughly align the minima/maxima of the curves. The proposed bSAM method adapts to shape of the loss and is overall more robust to misspecified parameter ρ while also giving the overall best performance for , AUROC all four metrics (Accuracy\n\n, NLL\n\n, ECE\n\n).\n\n·\n\n↑\n\n↓\n\n↓\n\n↑\n\nmean but rather at a sampled point is preferable from a Bayesian viewpoint. A concurrent work by Liu et al. (2022) also shows that combining SAM with random sampling improves the performance. The following Table 3 confirms that the noisy linearization helps in practice.\n\nModel / Data\n\nMLP / MNIST\n\nLeNet-5 / FMNIST\n\nResNet-20-FRN / CIFAR-10\n\nResNet-20-FRN / CIFAR-100\n\nMethod\n\nbSAM w/o Noise bSAM (Alg. 1) Improvement:\n\nbSAM w/o Noise bSAM (Alg. 1) Improvement:\n\nbSAM w/o Noise bSAM (Alg. 1) Improvement:\n\nbSAM w/o Noise bSAM (Alg. 1) Improvement:\n\nAccuracy ↑ (higher is better)\n\nNLL ↓ (lower is better)\n\nECE ↓ (lower is better)\n\nAUROC ↑ (higher is better)\n\n98.67(0.04) 98.78(0.06)\n\n0.041(0.0011) 0.038(0.0012)\n\n0.0030(0.0007) 0.0024(0.0004)\n\n0.981(0.001) 0.982(0.001)\n\n+00.11\n\n+0.003\n\n+0.0006\n\n+0.001\n\n92.01(0.11) 92.08(0.15)\n\n0.23(0.002) 0.22(0.005)\n\n0.0128(0.0021) 0.0066(0.0022)\n\n0.918(0.002) 0.920(0.003)\n\n+00.07\n\n+0.01\n\n+0.0062\n\n+0.002\n\n87.74(0.28) 88.72(0.24)\n\n0.38(0.009) 0.34(0.005)\n\n0.0251(0.006) 0.0163(0.002)\n\n0.896(0.004) 0.903(0.003)\n\n+00.98\n\n+0.04\n\n+0.0088\n\n+0.007\n\n60.30(0.33) 62.64(0.34)\n\n1.44(0.009) 1.32(0.015)\n\n0.0226(0.003) 0.0311(0.002)\n\n0.832(0.002) 0.841(0.003)\n\n+02.34\n\n+0.12\n\n−0.0085\n\n+0.008\n\nTable 3: Performing the loss-linearization at the noisy point rather than at the mean (“bSAM w/o Noise”) typically improves the results with respect to all metrics.\n\nF.6 ABLATION STUDY: EFFECT OF BAYESIAN MODEL AVERAGING\n\nIn Table 4 we show how the predictive performance and uncertainty of bSAM depends on the number of MC samples used to approximate the integral in the Bayesian marginalization.\n\n21\n\n10−410−310−210−1100ρ203040506070Accuracy(higherisbetter)ResNet20-FRN/CIFAR-100SAM-SGDSAM-AdambSAM10−410−310−210−1100ρ123456NLL(lowerisbetter)ResNet20-FRN/CIFAR-100SAM-SGDSAM-AdambSAM10−410−310−210−1100ρ0.00.10.20.30.4ECE(lowerisbetter)ResNet20-FRN/CIFAR-100SAM-SGDSAM-AdambSAM10−410−310−210−1100ρ0.600.650.700.750.800.850.90AUROC(higherisbetter)ResNet20-FRN/CIFAR-100SAM-SGDSAM-AdambSAMPublished as a conference paper at ICLR 2023\n\nModel / Data\n\nSamples\n\nAccuracy ↑ (higher is better)\n\nNLL ↓ (lower is better)\n\nECE ↓ (lower is better)\n\nAUROC ↑ (higher is better)\n\nResNet-20-FRN / CIFAR-10\n\n0 (mean) 8\n64 512 2048\n\n88.34(0.22) 88.43(0.21) 88.59(0.20) 88.69(0.20) 88.63(0.20)\n\n0.392(0.007) 0.349(0.006) 0.339(0.005) 0.338(0.005) 0.338(0.005)\n\n0.0538(0.002) 0.0212(0.002) 0.0186(0.001) 0.0173(0.002) 0.0173(0.001)\n\n0.9025(0.005) 0.9038(0.005) 0.9056(0.005) 0.9047(0.005) 0.9058(0.005)\n\nTable 4: Increasing the number of MC samples to approximate the marginal Bayesian predictive distribution tends to improve the performance in terms of NLL and ECE.\n\nF.7 ABLATION STUDY: DEPENDANCE OF SAM AND BSAM ON “M-SHARPNESS”\n\nIt has been observed in Foret et al. (2021) that the performance of SAM depends on the number m of subbatches the minibatch is split into. This phenomenon is referred to as m-sharpness. From a theoretical viewpoint, a larger value of m corresponds to a looser lower-bound for bSAM, as the sum of biconjugates is always smaller than the biconjugate of the sum.\n\nHere we investigate the performance of SAM and bSAM on the parameter m for a ResNet-20-FRN on CIFAR-100 without data augmentation. The other parameters are chosen as before, and the batch-size is set to 128. The results are summarized in the following Table 5.\n\nAlgorithm m Accuracy ↑\n\n(higher is better)\n\nNLL ↓ (lower is better)\n\nECE ↓ (lower is better)\n\nAUROC ↑ (higher is better)\n\nbSAM\n\nSAM-SGD\n\n1 2\n4 8\n16\n\n1 2\n4 8\n16\n\n61.38(0.27) 61.47(0.46) 62.35(0.32) 62.81(0.57) 63.31(0.32)\n\n55.88(0.81) 56.66(0.74) 56.53(1.09) 58.42(0.66) 59.51(0.67)\n\n0.0264(0.002) 1.39(0.007) 1.38(0.002) 0.0233(0.003) 1.34(0.014) 0.0208(0.003) 0.0327(0.004) 1.31(0.017) 0.0592(0.003) 1.30(0.011)\n\n0.1501(0.004) 1.82(0.032) 0.1421(0.006) 1.76(0.041) 0.1289(0.006) 1.73(0.058) 1.60(0.030) 0.1017(0.005) 1.51(0.040) 0.0681(0.006)\n\n0.836(0.003) 0.837(0.003) 0.837(0.003) 0.842(0.003) 0.840(0.003)\n\n0.820(0.005) 0.819(0.005) 0.810(0.003) 0.825(0.003) 0.829(0.004)\n\nTable 5: Effect of “m-sharpness” mini-batch size for SAM-SGD and bSAM for a ResNet-20-FRN on CIFAR-100 without data augmentation. As the effective minibatch size decreases, all performance metrics (accuracy and uncertainty) tend to improve. This boost in performance is not captured by our theory, and understanding it is an interesting direction for future work.\n\nF.8 EXPERIMENTS ON MNIST\n\nIn Table 6 we compare bSAM to several baselines for smaller neural networks on MNIST and FashionMNIST. Also in these smaller settings, bSAM performs competitively to state-of-the-art.\n\nG CHOICE OF HYPERPARAMETERS AND OTHER DETAILS\n\nG.1 EXPERIMENTS IN TABLE 1 AND TABLE 6\n\nIn the following, we list the details of the experimental setup. First, we provide some general remarks, followed by tables of the detailed parameters used on each dataset.\n\nFor all experiments, the hyperparameters are selected using a grid-search over a moderate amount of configurations to find the best validation accuracy. Our neural network outputs the natural param-\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nModel / Data\n\nMethod\n\nAccuracy ↑ (higher is better)\n\nNLL ↓ (lower is better)\n\nECE ↓ (lower is better)\n\nAUROC ↑ (higher is better)\n\nMLP MNIST\n\nLeNet-5 FMNIST\n\nSGD SAM-SGD SWAG VOGN Adam SAM-Adam bSAM (ours)\n\nSGD SAM-SGD SWAG VOGN Adam SAM-Adam bSAM (ours)\n\n0.044(0.0012) 0.0028(0.0007) 98.63(0.06) 0.0031(0.0002) 98.82(0.03) 0.039(0.0005) 0.044(0.0002) 0.0028(0.0005) 98.65(0.04) 0.046(0.0008) 0.0033(0.0007) 98.54(0.03) 0.0036(0.0007) 0.050(0.0012) 98.41(0.06) 0.0044(0.0005) 0.046(0.0009) 98.58(0.03) 98.78(0.06) 0.038(0.0011) 0.0024(0.0004)\n\n91.37(0.27) 91.95(0.17) 91.38(0.27) 91.24(0.25) 91.14(0.25) 91.66(0.19) 92.10(0.26)\n\n0.32(0.007) 0.0429(0.0015) 0.22(0.004) 0.0062(0.0007) 0.31(0.005) 0.0397(0.0026) 0.24(0.004) 0.0071(0.0012) 0.0450(0.0008) 0.33(0.005) 0.25(0.004) 0.0225(0.0026) 0.22(0.005) 0.0066(0.0022)\n\n0.979(0.002) 0.978(0.003) 0.976(0.003) 0.980(0.002) 0.979(0.002) 0.980(0.002) 0.982(0.001)\n\n0.897(0.004) 0.917(0.002) 0.901(0.005) 0.916(0.002) 0.897(0.005) 0.913(0.002) 0.920(0.002)\n\nTable 6: Comparison on small datasets.\n\neters of the categorical distribution as a minimal exponential family (number of classes minus one output neurons). The loss function is the negative log-likelihood.\n\nWe always use a batch-size of B = 128. For SAM-SGD, SAM-Adam and bSAM we split each minibatch into m = 8 subbatches, for VOGN we set m = 16 and consider independently computed perturbations for each subbatch. Finally, to demonstrate that bSAM does not require an excessive amount of tuning and parameters transfer well, we use the same hyper-parameters found on CIFAR10 for the CIFAR-100 experiments.\n\nWe summarize all hyperparameters in Table 7.\n\nMNIST. The architecture is a 784 – 500 – 300 – 9 fully-connected multilayer perceptron (MLP) with ReLU activation functions. All methods are trained for 75 epochs. We use a cosine learning rate decay scheme, annealing the learning rate to zero. The hyperparameters of the individual methods are shown in Table 7. For SWAG, we run SGD for 60 epochs (with the same parameters). Then collect SWAG statistics with fixed α = 0.01, β1 = 0.95. For all SWAG runs, we set the rank to 20, and collect a sample every 100 steps for 15 epochs.\n\nFashionMNIST. The architecture is a LeNet–5 with ReLU activations. We use a cosine learning rate decay scheme, annealing the learning rate to zero. We train all methods for 120 epochs. For SWAG, we run SGD for 105 epochs and collect 15 epochs with α = 0.001 and β1 = 0.8.\n\nCIFAR-10/100. The architecture is a ResNet-20 with filter response normalization nonlinearity adopted from Izmailov et al. (2021). The ResNet-20 has the same number of parameters as the one used in the original publication (He et al., 2016). We train for 180 epochs and decay the learning rate by factor 10 at epoch 100 and 120. For SWAG, we run SGD for 165 epochs and collect for 15 epochs with α = 0.0001, β1 = 0.9.\n\nG.2 EXPERIMENTS IN TABLE 2\n\nThe bSAM method depends on the number of data-samples N . To account for the random cropping and horizontal flipping, we rescale this number by a factor 4 which improved the performance. This corresponds to a tempered posterior, as also suggested in Osawa et al. (2019).\n\nIn all experiments, all methods use cosine learning rate schedule which anneals the learning rate across 180 epochs to zero. The batch size is again chosen as B = 128.\n\nFor an overview over all the hyperparameters, please see Table 8.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nConfiguration\n\nMethod\n\nMNIST / MLP\n\nFashionMNIST / LeNet-5\n\nCIFAR-10 / ResNet-20-FRN\n\nCIFAR-100 / ResNet-20-FRN\n\nSGD VOGN Adam SAM-SGD SAM-Adam bSAM\n\nSGD VOGN Adam SAM-SGD SAM-Adam bSAM\n\nSGD VOGN Adam SAM-SGD SAM-Adam bSAM\n\nSGD VOGN Adam SAM-SGD SAM-Adam bSAM\n\nα\n\n0.1 0.0005 0.0005 0.1 0.0005 0.1\n\n0.01 0.0001 0.001 0.05 0.001 0.1\n\n0.1 0.001 0.001 0.05 0.0003 0.1\n\n0.1 0.001 0.001 0.05 0.0003 0.1\n\nβ1\n\n0.95 0.95 0.8 0.95 0.8 0.8\n\n0.8 0.95 0.9 0.8 0.9 0.95\n\n0.9 0.9 0.9 0.9 0.9 0.9\n\n0.9 0.9 0.9 0.9 0.9 0.9\n\nN · δ\n\nρ\n\n24 25 24 30 24 15\n\n60 25 60 60 60 50\n\n25 25 250 25 25 25\n\n30 25 250 25 25 25\n\n× ×\n× 0.05 0.02 0.01\n\n× ×\n× 0.05 0.02 0.002\n\n× ×\n× 0.05 0.1 0.001\n\n× ×\n× 0.05 0.1 0.001\n\nβ2\n\n× 0.999 (cid:13) ×\n(cid:13) (cid:13)\n\n× 0.99 (cid:13) ×\n(cid:13) (cid:13)\n\n× 0.999 (cid:13) ×\n(cid:13) (cid:13)\n\n× 0.999 (cid:13) ×\n(cid:13) (cid:13)\n\nγ\n\n× 0.05 (cid:13) ×\n(cid:13) (cid:13)\n\n× 0.05 (cid:13) ×\n(cid:13) (cid:13)\n\n× 0.01 (cid:13) ×\n(cid:13) (cid:13)\n\n× 0.01 (cid:13) ×\n(cid:13) (cid:13)\n\nTable 7: Hyperparameters used in the experiments without data augmentation. “ method does not use that hyperparameter. “\n\n” indicates fixed hyperparameter across all datasets.\n\n” denotes that this\n\n×\n\n(cid:13)\n\nDataset\n\nMethod\n\nCIFAR-10\n\nCIFAR-100\n\nTinyImageNet\n\nSGD Adam SAM-SGD SAM-Adam bSAM\n\nSGD Adam SAM-SGD SAM-Adam bSAM\n\nSGD Adam SAM-SGD SAM-Adam bSAM\n\nα\n\n0.1 0.005 0.03 0.001 1.0\n\n0.03 0.005 0.2 0.005 1.0\n\n0.1 0.002 0.1 0.001 0.1\n\nβ1\n\n0.95 0.7 0.95 0.8 0.95\n\n0.95 0.9 0.8 0.7 0.95\n\n0.95 0.9 0.95 0.95 0.9\n\nN · δ\n\nρ\n\n10 2.5 25 10 2\n\n25 1\n10 1\n2\n\n20 20 50 10 25\n\n× ×\n0.01 0.1 0.01\n\n× ×\n0.02 0.2 0.01\n\n× ×\n0.01 0.1 0.00005\n\nβ2\n\n× (cid:13) ×\n(cid:13) (cid:13)\n\n× (cid:13) ×\n(cid:13) (cid:13)\n\n× (cid:13) ×\n(cid:13) (cid:13)\n\nγ\n\n× (cid:13) ×\n(cid:13) (cid:13)\n\n× (cid:13) ×\n(cid:13) (cid:13)\n\n× (cid:13) ×\n(cid:13) (cid:13)\n\nTable 8: Hyperparameters used in the experiments with data augmentation shown in Table 2. “ denotes that this method does not use that hyperparameter. “ across all datasets.\n\n” ” indicates fixed hyperparameter\n\n(cid:13)\n\n×\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nG.3 RESNET–18 EXPERIMENTS IN TABLE 2\n\nWe trained all methods over 180 epochs with a cosine learning rate scheduler, annealing the learning rate to zero. The batch-size is set to 200, and both SAM and bSAM use m-sharpness with m = 8.\n\nDataset\n\nMethod\n\nCIFAR-10\n\nCIFAR-100\n\nSGD SAM-SGD bSAM\n\nSGD SAM-SGD bSAM\n\nα\n\n0.03 0.03 0.5\n\n0.03 0.03 0.5\n\nβ1\n\n0.9 0.9 0.9\n\n0.9 0.9 0.9\n\nN · δ\n\nρ\n\n25 25 10\n\n25 25 10\n\n× 0.05 0.01\n\n× 0.05 0.01\n\nβ2\n\n× ×\n(cid:13)\n\n× ×\n(cid:13)\n\nγ\n\n× ×\n(cid:13)\n\n× ×\n(cid:13)\n\nTable 9: Hyperparameters used in the ResNet–18 experiments of Table 2. “ method does not use that hyperparameter. “\n\n” indicates fixed hyperparameter across all datasets.\n\n” denotes that this\n\n×\n\n(cid:13)\n\nH TABLE OF NOTATIONS\n\nThe following Table 10 provides an overview of the notations used throughout the paper. Multidimensional quantities are written in bold-face, scalars and scalar-valued functions are regular face.\n\nSymbol\n\n, (cid:104)·\n\n·(cid:105)\n\n(cid:107) · (cid:107) f ∗, f ∗∗ θ\nT(θ)\n\nμ, μ(cid:48), μ(cid:48)(cid:48) λ, λ(cid:48), λ(cid:48)(cid:48)\n\nΩ\n\nM qμ qλ A(λ)\n\nA∗(μ) ω, v m, σ2 (e |\n\nN δ\n\nm, σ2I)\n\nDescription\n\nAn inner-product on a finite-dimensional vector-space. The (cid:96)2-norm. Fenchel conjugate and biconjugate of the function f .\n\nParameter vector of the model, e.g., the neural network.\n\nSufficient statistics of an exponential family distribution over the parameters.\n\nExpectation (or mean/moment) parameters of exponential family.\n\nNatural parameters of an exponential family.\n\nSpace of valid natural parameters.\n\nSpace of valid expectation parameters. Exponential family distribution with moments μ. Exponential family distribution with natural parameters λ.\n\nLog-partition function of the exponential family in natural parameters. Entropy of qμ, the convex conjugate of the log-partition function. Mean and variance of a normal distribution, corresponding to μ and λ. Mean and variance of another normal, corresponding to μ(cid:48) and λ(cid:48). e follows a normal distribution with mean m and covariance σ2I. Precision of an isotropic Gaussian prior.\n\nTable 10: Notation.\n\n25",
    "reference": "# Summary Of The Paper\n\nThe authors establish a connection between sharpness-aware minimization (SAM) and a Bayes objective by using a relaxation of the latter, such that the expected negative-loss is replaced by a tight convex lower bound, which is shown to be equivalent to SAM. Moreover, they propose a Bayesian extension of SAM that improves both generalization and uncertainty estimation.\n\n# Strength And Weaknesses\n\nStrength\n\nUsing the Fenchel biconjugate to bridge SAM and the Bayes objective is an interesting and insightful idea, and the resulting relaxation of the Bayes objective has shown good empirical performance. Moreover, the authors have conducted a variety of experiments and ablation studies on both toy and real datasets, providing deeper understanding of the proposed method.\n\nWeaknesses\n1. While the equivalence between SAM and the relaxed Bayes objective is interesting, it doesn't explain why the relaxed Bayes objective (or SAM) has better generalization performance than the original Bayes objective. In other words, it seems the \"maximum loss\" performs better than the \"expected loss\" despite the established connection, the reason of which is unclear.\n2. The baselines used in the experiments are too weak. For instance, test accuracies around 95% and 75%, respectively, on CIFAR-10 and CIFAR-100 would be more convincing.\n3. The readability of Sec. 2 needs improvement.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is well written in general, but clarity can be improved by simplifying the notation in Sec. 2, or perhaps including a notation table. The proposed method is novel and should be easy to reproduce.\n\nIn Eq. (4), f*(u) -> f(u).\n\n# Summary Of The Review\n\nThis paper provides strong theoretical results, and extensive experimental results. However, some of the main questions remain unanswered, and some of the experimental results are not convincing enough due to weak baselines.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMULTI-EPOCH MATRIX FACTORIZATION MECHANISMS FOR PRIVATE MACHINE LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe introduce new differentially private (DP) mechanisms for gradient-based machine learning (ML) training involving multiple passes (epochs) of a dataset, substantially improving the achievable privacy-utility-computation tradeoffs. Our key contribution is an extension of the online matrix factorization DP mechanism to multiple participations, substantially generalizing the approach of Denisov et al. (2022). We first give conditions under which it is possible to reduce the problem with per-iteration vector contributions to the simpler one of scalar contributions. Using this, we formulate the construction of optimal (in total squared error at each iterate) matrix mechanisms for SGD variants as a convex program. We propose an efficient optimization algorithm via a closed form solution to the dual function. While tractable, both solving the convex problem offline and computing the necessary noise masks during training can become prohibitively expensive when many training steps are necessary. To address this, we design a Fourier-transform-based mechanism with significantly less computation and only a minor utility decrease. Extensive empirical evaluation on two tasks: example-level DP for image classification and user-level DP for language modeling, demonstrate substantial improvements over the previous state-of-the-art. Though our primary application is to ML, we note our main DP results are applicable to arbitrary linear queries and hence may have much broader applicability.\n\n1\n\nINTRODUCTION\n\nDifferentially private stochastic gradient descent (DP-SGD) is the de facto standard algorithm for DP machine learning (ML) (Song et al., 2013; Abadi et al., 2016a). However, obtaining stateof-the-art privacy-utility tradeoffs critically requires use of privacy amplification techniques like shuffling (Erlingsson et al., 2019; Feldman et al., 2022) or (Poisson) subsampling (Bassily et al., 2014; Zhu & Wang, 2019; Wang et al., 2019). These in turn require strong assumptions on the manner in which data is processed that are rarely valid in applications of DP-SGD, as implementing these procedures is often impractical (Kairouz et al., 2021).\n\nKairouz et al. (2021) recently proposed the DP-FTRL framework that avoids reliance on amplification by sampling, through using DP streaming of prefix sums (Dwork et al., 2010; Chan et al., 2011; Honaker, 2015). DP-FTRL can often match (or outperform) DP-SGD in privacy-utility tradeoffs. Indeed, this algorithm enabled McMahan & Thakurta (2022) to train the first known provably DP ML model on user data in a production setting.\n\nSeveral works have since focused on this primitive as an instantiation of the streaming matrix mechanism; in particular, Denisov et al. (2022) showed that leveraging optimal matrix mechanisms led to significant empirical improvements, though their work was restricted to the single-epoch setting. Shown in Figs. 1 and 3, we achieve substantially improved privacy-utility tradeoffs, with comparable computation. Our methods outperform all prior work, including DP-SGD with amplification, to as low as ε ≈ 2. To accomplish this, we propose a formalism for measuring multi-participation sensitivity, given in Section 2, a significant extension to the single-participation sensitivity used in in Denisov et al. (2022). We show in Section 3 how one may compute matrix mechanisms optimized for this multi-participation setting. This generalization enables application of optimized streaming matrix mechanisms to settings where each example (or user) may contribute to multiple elements of the data matrix (the matrix formed by stacking unnoised batch gradients in ML).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Our optimal multi-epoch matrix and FFT-based mechanisms outperform all others, including DP-SGD with amplification, as low as ε ≈ 4. Using our sensitivity calculation of Theorem 2.1 and stamping (Section 5), we optimize a single pass (k = 1) matrix of Denisov et al. (2022) but apply it here with > 1 pass. We use an online Honaker-based decoder equivalent to that of Kairouz et al. (2021) except for a significant improvement to tree-completion in Appendix D.3. Models trained for 20 epochs on CIFAR10 with a batch size of 500. We repeat each setting 12 times and show 95% bootstrapped confidence intervals. Empirical setup is in Section 5.1.\n\nWe also explore the computational tradeoffs of our approaches. In particular, computing optimal matrix factorizations may become intractable when large numbers of training steps n are required , as we discuss in Section 4. While this is uncommon in the federated algorithms for user-level DP, it can be a limitation when training with SGD for example-level privacy. To reduce this cost, we propose and investigate an approach based on the Fast Fourier Transform (FFT) (Nussbaumer, 1981), which is near-optimal for the single-epoch setting and efficiently computable for most, if not all, ML settings. Indeed, we find this approach still outperforms the mechanisms from the extant literature, even under multiple participations.\n\nContributions 1) We provide a framework for computing the sensitivity of matrix mechanisms under general participation schemas. To do this, we prove a new theorem bounding sensitivity for multi-dimensional data contributions. This allows us to reduce the problem to that of measuring sensitivity for scalar contributions alone (Section 2). 2) We extend the results of Denisov et al. (2022) to the optimization problems corresponding to these generalized notions of sensitivity, showing that the algorithms proposed there can be applied in our setting (Section 3). 3) We propose and analyze a computationally-efficient factorization based on the Fourier transform which is near optimal for the single-epoch setting and can be efficiently extended to handle multiple epochs (Section 4). 4) We perform detailed empirical comparisons of our mechanisms with both the prior matrix mechanism approaches and DP-SGD. We show that the methods proposed here outperform all others (in particular, DP-SGD with amplification), to privacy budgets as low as ε ≈ 2, and without any need for privacy amplification (Section 5). 5) We will upload all code used in the final manuscript.\n\nRelated work The core privacy primitive here is the matrix mechanism (Li et al., 2015). Its long history of study and application was mostly in the offline setting (McKenna et al., 2018; Edmonds et al., 2020; Yuan et al., 2016; Hardt & Talwar, 2010). Fichtenberger et al. (2022); Denisov et al. (2022) independently applied it to the adaptive streaming setting, where outputs are released oneby-one and privacy analysis must account for an adversary adaptively defining the inputs. Denisov et al. (2022) connected the matrix mechanism to DP ML, via the DP-FTRL algorithm of Kairouz et al. (2021), and showed that computing optimal factorizations significantly improves the privacyutility-computation tradeoffs when needing only a single pass (epoch) over the training data.\n\nExample- and user-level DP, and the connection to federated learning (FL) In addition to example-level DP, we consider user-level DP. As observed by McMahan et al. (2018), private FL algorithms are well suited to providing user-level DP or other multi-example units of privacy, e.g. document-level, as bounding the sensitivity of a single user’s contribution to an aggregate update is made straightforward by the per-user data processing pattern inherent in FL. However, our primary application is to datacenter training, where user data can be processed in a fixed shuffled order, unlike cross-device FL. We use the term ‘participation’ to denote the event that an example (user or client in FL) contributes to the gradient sum (or a model update in FL) xi for a given step/iteration (round in FL) i. Individual contributions to xi are scaled so their maximum (cid:96)2 norm is ζ. Our\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nmechanisms compute sums over individual clipped contributions, and then post-process by dividing by the batch size (or clients/round) to compute an average gradient (or model update). We assume ζ = 1, applying appropriate scaling as needed. Appendix A summarizes terminology and notation.\n\n2 PRIVACY FOR ADAPTIVE STREAMS WITH MULTIPLE PARTICIPATIONS\n\nWe define and efficiently bound the sensitivity of the multi-participation adaptive streaming (continual release) setting, by generalizing Denisov et al. (2022, Sec. 2). We assume a database of m examples (or users in FL, or records in a general DP application) that is processed as a stream over n steps. A set of B examples is selected on each step i, and processed via an adaptively chosen function (e.g., computing a gradient at the current model), producing a vector of (cid:96)2 norm at most ζ. These vectors are summed and provided to the DP mechanism as xi ∈ Rd, which then releases a privatized function of [x1, . . . , xi], the stream so far. When a particular example contributes to the sum xi, we say it participates on step i. We are primarily interested in the case where m/B < n, and hence each example is used on more than one step. This is the multiple epoch setting of ML.\n\nTwo data streams x and ̃x are said to be neighboring if they differ in the contributions derived from a single example, either by zeroing out all of its contributions, or by replacing them arbitrarily subject to the norm bound ζ. Thus, the participation pattern does not change (all records contribute to the same steps in x and ̃x, with only the vectors associated with one record changing). We define a participation schema Π as the set of possible participation patterns π ∈ Π, with each π ⊆ [n] indicating a set of steps in which a single example might participate. Assuming each record contributes at most once (single-participation, Π = (cid:8){1}, {2}, . . . {n}(cid:9)), recovers the standard streaming setting. This captures, for example, training with minibatch SGD using a single pass (epoch) over a training dataset. At the other extreme, we have every-step participation with Π = {[n]} where each record contributes to every step. This captures learning with full gradient descent, where we compute the gradient on the full training dataset on every iteration.\n\nthe above two,\n\n(k, b)- Fixed-epoch-order participation We focus on generalization of participation, where each example participates at most k times, with any adjacent participations exactly b steps apart: formally, Π is the set of all π such that |π| ≤ k, and if π = {i1, . . . , ik}, we have ∀j ∈ {2, . . . , k}, ij − ij−1 = b. Note (k=1, b=n)-participation recovers the single-epoch setting, and (k=n, b=1)-participation recovers every-step participation, and for example (k=3, b=2)- participation has Π = {{1, 3, 5}, {2, 4, 6}}. We focus on this participation schema because: 1) It encompasses multi-epoch SGD training using a data processing pattern well-supported by modern ML infrastracture.1 The only requirement is that rather than shuffling the dataset for each epoch, the dataset is shuffled once and the same order of minibatches is used for each epoch. With this setup, k epochs of training on a dataset of size m with a batch size B gives n = mk/B total training steps, and satisfies (k, m/B)-participation. 2) We show (e.g., Eq. (3)) in importance cases this participation schema allows for the efficient computation of sensitivity. 3) We will see in Section 3 that the more possible participation patterns π, the more constrained the problem of finding optimal mechanisms becomes. Hence, a relatively restrictive (but practical) schema like (k, b)-participation yields more favorable privacy-utility tradeoffs.\n\nSensitivity of linear queries on multi-participation adaptive streams Consider a full-rank square query (or workload) matrix A ∈ Rn×n; we wish to compute the function x (cid:55)→ Ax in a differentially private manner, where we consider inputs x and outputs Ax to be elements of Rn×d, under geometry inherited from the Frobenius inner product. We utilize the matrix mechanism (Li et al., 2015), which, provided a factorization A = BC, computes the estimate\n\n(cid:100)Ax = B (Cx + z) ,\n\n(1)\n\nwhere z is a sample from appropriately scaled isotropic Gaussian noise. The scale is determined by the sensitivity of the mapping x (cid:55)→ Cx; roughly, how much outputs of this mapping can vary (in (cid:96)2\n\n1This is in contrast to Poisson, or independent fixed-sized batch sampling, with replacement across steps, as is assumed by many works (Abadi et al., 2016a; Bassily et al., 2014; Zhu & Wang, 2019; Wang et al., 2019). Many works in fact process batches in a shuffled order without replacement and then incorrectly apply DP analysis for, e.g., Poisson sampling. Indeed, we perform this same—incorrect—analysis for our DP-SGD baseline because it reproduces the previous state-of-the-art results for DP-SGD.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nnorm) when we swap the input stream x for a neighboring one ̃x. We refer to this matrix C as the “encoder”, as it encodes x as Cx before adding Gaussian noise. Similarly, we call B the “decoder”.\n\nLet N be the set of all pairs of neighboring streams x and D := {x − ̃x | (x, ̃x) ∈ N} represent the set of all possible deltas between neighboring x, ̃x. The definition of D implies it is symmetric (u ∈ D ⇒ −u ∈ D). We will say a D satisfies the participation schema Π if the indices of all nonzero elements in each vector u ∈ D corresponds some π ∈ Π. Critically, for linear queries D fully captures the sensitivity of the query:\n\nDefinition 1. The sensitivity of the matrix factorization mechanism Eq. (1) is defined as\n\nsensD(C) = sup\n\n(x, ̃x)∈N\n\n(cid:107)Cx − C ̃x(cid:107)F = sup u∈D\n\n(cid:107)Cu(cid:107)F .\n\n(2)\n\nConvexity of (cid:107)Cu(cid:107)F in u implies that supu∈D (cid:107)Cu(cid:107)F = supu∈conv(D) (cid:107)Cu(cid:107)F , and hence without loss of generality (wlog), we take D to be convex as needed. It is illustrative to consider some specific Ds for scalar per-step contributions with ζ = d = 1. Single-participation corresponds to D = conv{αei|α ∈ [−1, 1], i ∈ [n]} where ei for i ∈ [n] are the standard basis vectors. Noting (cid:107)Cu(cid:107) = (cid:107) − Cu(cid:107) and convexity of (cid:107)Cu(cid:107), we see the maximum will be achieved at some ei, recovering the ‘max-(cid:96)2-norm-over-columns’ measurement of sensitivity of Li et al. (2015, Proposition 3). Every-step participation corresponds to the (cid:96)∞ ball, D = {x | (cid:107)x(cid:107)∞ ≤ 1}.\n\nConditions allowing the reduction to per-iterate scalar contributions In ML, examples are used to calculate gradients of d > 1 dimensions, and so we wish to consider x ∈ Rn×d, with rows xi ∈ Rd corresponding to the sum of gradients for examples participating in step i. In order to compute sensitivity, one may hope that the sensitivity for each xi ∈ Rd can be bounded by only considering some appropriately worst-case xi ∈ R. More formally, consider a fixed participation schema Π, and further assume (wlog) ζ = 1. Then, for vector-valued contributions we have\n\nDd\n\nΠ = conv (cid:8)G ∈ Rn×d | ∃π ∈ Π s.t. (cid:107)G[i,:](cid:107)2 ≤ 1 for i ∈ π and G[i,:] = 0 for i (cid:54)∈ π(cid:9) .\n\nIn the d = 1 case, we have a much simpler polytope, D1\n\nΠ = conv(D1\n\nΠ) where\n\n(cid:110)\n\nD1\n\nΠ =\n\n(cid:91)\n\nπ∈Π\n\nu ∈ Rn | ui ∈ {−1, 1} if i ∈ π, 0 otherwise\n\n(cid:111) .\n\nOne might hope to show sensDd (C), and the authors in fact initially conjectured this to be true. To our surprise, while this inequality holds under a variety of assumptions, it does not hold in general (Appendix H.2 gives a counterexample).2 Empirically we have observed that for various query matrices A and (k, b)-participation with d = 1, the optimal C satisfy (or almost satisfy) the condition C(cid:62)C ≥ 0 (element-wise non-negativity). In this case, we can show:\n\n(C) ≤ sensD1\n\nΠ\n\nΠ\n\nCorollary 2.1. When per-step contributions bounded by ζ = 1, for any participation schema Π and dimensionality d ≥ 1, when C(cid:62)C ≥ 0 elementwise, we have sensDd\n\n(C) = sensD1\n\n(C).\n\nΠ\n\nΠ\n\nIn particular, this implies that if C is optimal in the d = 1 case and satisfies C(cid:62)C ≥ 0, it is also optimal in the d > 1 case. This result is a corollary of Theorem H.1, which establishes additional conditions under which sensDd\n\n(C) holds. All proofs are in Appendix H onwards.\n\n(C) ≤ sensD1\n\nΠ\n\nΠ\n\nIn general, computing sens(C) is a convex quadratic maximizaDifficulty of computing sens(C) tion problem over a convex set, which can be NP-hard. Even the simple case of computing the sensitivity for an arbitrary matrix C under every-step participation with scalar (d = 1) contributions is NP-hard—it is exactly the problem of computing the (cid:96)∞−(cid:96)2 operator norm (Tropp, 2004). (In fact, it is useful to observe sensD(·) can always be viewed as an operator norm, see Appendix B). This hardness is in stark contrast to the single-participation setting, where calculating sensitivity is trivial. However, we can in some cases compute sensitivity exactly by brute force. Take d = 1. Observe Π| = |Π|2k, D1 and observing the symmetry (cid:107)Cu(cid:107) = (cid:107)C(−u)(cid:107) can reduce the computational cost only by half. In general |Π| may be exponential in k, but in the special case of (k, b)-participation, we have |Π| = b\n\nΠ is a finite set and so a direct calculation by using Eq. (2) is often possible. But, |D1\n\n2We conjecture it is “almost” true; tightly bounding the necessary error term is an interesting open question.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(the number of steps in one epoch). Hence, for modest numbers of epochs k, directly computing sensitivity is possible, e.g., in our StackOverflow experiments in Section 5.2, we can reduce u ∈ D1 Π to only 342 · 25 = 10, 944 vectors. Theorem H.1 can be used to translate bounds from scalar to higher dimensions, though without such a translation it is not clear how to generalize a brute-force method.\n\nComputing sensitivity when C(cid:62)C ≥ 0 Let X = C(cid:62)C. When X has only nonnegative elements, one may reduce the problem of computing sensD1\n\n(C) to\n\nΠ\n\nsensDd\n\nΠ\n\n(C) = max u∈D1 Π\n\n(cid:107)Cu(cid:107)F = max u∈D1 Π\n\n√\n\nu(cid:62)Xu = max π∈Π\n\n(cid:113)\n\n1(cid:62)X[π,π]1,\n\n(3)\n\nwhere X[π,π] ∈ Rk×k is the submatrix of X formed from the rows and columns selected by π, |π| = k and 1 ∈ Rk. The first equality follows from Corollary 2.1, and then the max must be achieved by the maximum-magnitude nonnegative vector u, specifically 1k. As noted above, the matrices we consider satisfy this property, and hence we can compute the exact sensitivity efficiently for (k, b)-participation.\n\nUpper-bounding sensitivity As an alternative to structural conditions on C or X allowing efficient exact computation of sensitivity for d > 1, we can look to (reasonably tight) upper bounds on the sensitivity of C. In the case of (k, b)-participation, one efficient method of computing upper bounds for the multiple-participation sensitivity of C has shown itself to be particularly useful: Theorem 2.1. Let C ∈ Rn×n, and take some participation schema Π, with k = maxπ∈Π |π| the maximum number of participations. With C[:,π] representing selecting the columns of the matrix C indexed by π and (cid:107)·(cid:107)2 the spectral matrix norm, let λ = max k.\n\n(cid:13) (cid:13)2. Then sensD1\n\n(cid:13) (cid:13)C[:,π]\n\n(C) ≤ λ\n\n√\n\nΠ\n\nπ∈Π\n\nIn the (k, b)-participation case, |Π| = b. The complexity of computing the largest eigenvalue of the subselected C matrix is cubic in k. Thus, computing this upper bound is of order bk3, easily computable for the range of k, b considered here (k ≤ 100, b ≤ 500).\n\nDifferential Privacy Guarantee Using our generalization of adaptive streams to multiple participations we obtain the following result (a straightforward generalization of Denisov et al. (2022, Theorem 2.1)). The proof is identical that in Denisov et al. (2022), except we replace the sensitivity bound with that for multiple participations obtained via Corollary 2.1. Theorem 2.2. Let A ∈ Rn×n be a lower-triangular full-rank query matrix, and let A = BC be any factorization with the following property: for any two neighboring streams x, ̃x ∈ Rn×d, we have (cid:107)C(x − ̃x)(cid:107)F ≤ κ. Let Z ∼ N (0, κ2σ2)n×d with σ large enough so that M(x) = Ax + BZ = B(Cx + Z) satisfies (ε, δ)-DP (or ρ-zCDP or μ-Gaussian DP) in the nonadaptive continual release model. Then, M satisfies the same DP guarantee (with the same parameters) even when the rows of the input are chosen adaptively.\n\n3 OPTIMIZING MATRIX MECHANISMS FOR MULTIPLE EPOCHS\n\nWe now present methods for computing optimal matrix mechanisms that are specialized to a specific participation schema Π and query matrix A. For example, Fig. 2 shows the optimal factorization for the query matrix A representing SGD with momentum and coldown under (k=6, b=342)- participation, used in Section 5.2. Specializing the mechanism to both the participation pattern and the specific query workload enables as to obtain state-of-the-art results in ML (Section 5).\n\nWe follow the approach of Denisov et al. (2022) which showed empirical improvements over prior methods in single-epoch settings. We begin by defining the loss of interest, i.e., the total variance of noise added, for the mechanism defined in Eq. (1). Note that this loss characterizes other downstream tasks like DP Mean Estimation. Given D, assume that we may represent D = conv(D) for some finite set D—as we have seen, this is the case, e.g., in (k, b)-participation. Then the loss which corresponds to total squared error of a factorization, at a fixed privacy level, may be expressed as:\n\nL(B, C) = sens2\n\nD(C) (cid:107)B(cid:107)2\n\nF\n\nwhere\n\nsensD(C) = sup u∈D\n\n5\n\n(cid:107)Cu(cid:107)2\n\n2 = sup\n\nu∈D\n\n(cid:107)Cu(cid:107)2 2.\n\n(4)\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The optimal factorization A = BC under (k=6, b=342)-participation, constructed by solving the optimization problem Eq. (5). Matrix A encodes SGD with momentum 0.95 and a learning-rate cooldown schedule for the last 25% of rounds, as used in our StackOverlow experiments (Section 5.2). The constraints on sensitivity imposed by this participation schema are evident in the resulting matrices. For example, the white diagonals with a period of b = 342 in X = C(cid:62)C show that the columns of C that could correspond to a pair of rounds (i, j) where the same user might participate are in fact orthogonal. See Fig. 13 in Appendix F.4 for a larger view.\n\nObserving that ∀α the mechanism A = (cid:0)αB(cid:1)(cid:0) 1 α C(cid:1) has identical loss, we conclude that we may consider the constrained version of the problem of minimizing this loss where sensD(C) ≤ 1. Since for any C, B = AC† produces the minimum-Frobeneous norm B-matrix, it is sufficient to solve:\n\nL (cid:0)AC†, C(cid:1) =\n\nmin C\n\nmin\n\nC:sens2\n\nD(C)=1\n\n(cid:13)AC†(cid:13) (cid:13) 2\n(cid:13)\n\nF .\n\n(5)\n\nWith the change of variables X = C(cid:62)C, equivalently:\n\nD(X)=1\n\ntr(A(cid:62)AX−1) where (cid:100)sens2\n\nmin Xis PD, (cid:100)sens2 i=1 span Rn. Theorem 3.1. Let a finite D = {ui}k Assume that A is full-rank, and for v ∈ Rk define Hv = [u1, . . . , uk] diag(v)1/2, U = HvH(cid:62) v . (cid:0)u(cid:62)Xu − 1(cid:1) . Then, for LaDefine the Lagrangian L as L (X, v) := tr(A(cid:62)AX−1) + (cid:80) grange multipliers v such that the U is full-rank, the minimizer X (v) of L for this fixed v may be 2 A(cid:62)AU 1 represented X (v) = U− 1 Eq. (6) can be expressed in closed form in terms of the dual variables v:\n\ni=1 be given, and assume that the vectors {ui}k\n\n2 . and the Lagrange dual function g for the problem\n\nD(X) ≤ sup\n\nu∈D vu\n\nu(cid:62)Xu.\n\n2 (cid:0)U 1\n\n2 U− 1\n\n2 (cid:1) 1\n\n(6)\n\nu∈D\n\ng(v) := inf\n\nX is PD\n\nL(X, v) = 2 tr\n\n(cid:16)(cid:0)U\n\n1\n\n2 A(cid:62)AU\n\n2 (cid:17)\n\n1\n\n2 (cid:1) 1\n\n(cid:88)\n\n−\n\nvu\n\nu∈D\n\n(7)\n\nRemark. The restriction that v yields a full-rank U serves to restrict to cases where the Lagrangian has a finite, positive-definite minimizer in the primal variable; if the vectors {u} span Rn, the problem Eq. (6) has a finite minimizer by Lemma I.1. Any setting of the dual variables v corresponding to this minimizer is contained in a neighborhood uniformly satisfying this full-rank property, and so it is valid to differentiate our expression for g with respect to such v (as we will do in Appendix I.1). Corollary 3.1. In the same setup as Theorem 3.1, the gradient of the dual function g is: ∂g ∂vi\n\n=\n\nu(cid:62)\n\ni U− 1\n\n2 (cid:0)U 1\n\n2 A(cid:62)AU 1\n\n2 (cid:1) 1\n\n2 U− 1\n\n2 ui − 1. Moreover, a maximizer of the dual v(cid:63) must satisfy:\n\nv(cid:63) = diagpart\n\n(cid:16)(cid:0)H(cid:62)\n\nv(cid:63) A(cid:62)AHv(cid:63)\n\n2 (cid:17)\n\n(cid:1) 1\n\n.\n\n(8)\n\nThe optimal value of the problem defined in Eq. (6) is tr (v(cid:63)).\n\nIn the single-participation case of Denisov et al. (2022), Hv = diag(v) 1 Remark. 2 , and Eq. (8) recovers the fixed point expression of that paper’s Theorem 3.2. Our Corollary 3.1 implies that the optimization methods presented in Denisov et al. (2022) may be applied, with suitable translation, to our setting; we use these methods to generate the optimal matrices studied empirically in Section 5.\n\n4 FAST-FOURIER-TRANSFORM-BASED DP-PREFIX SUM ESTIMATION\n\nOur work has two types of computation costs: optimization costs are those associated with optimizing and generating (or, computing) a mechanism whereas noise generation costs are those\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nassociated with using the mechanism to sample noise for DP prefix sum release. Note that once optimized, a single mechanism can be reused indefinitely to generate noise for other runs by simply resampling new noise and applying the same decoder. The best known methods for computing the optimal factorizations scale as at least O(n3) (Yuan et al., 2016; Denisov et al., 2022). This optimization cost can become intractable when n grows too large. Thus, in this section we focus on reducing optimization computation at a small decrease in the achievable privacy-utility tradeoff.\n\nA prime candidate for this goal is the Discrete Fourier Transform (DFT) because there are known algorithms both for nearly-optimal private convolutions (Fawaz et al., 2013) which are intimately related to the DFT, and for efficient calculation of the DFT using the Fast Fourier Transform (FFT) (Nussbaumer, 1981). We present an FFT-based mechanism that reduces the noise generation costs. We then prove rigorous DP guarantees for it and show that these lead to near-optimal privacy-utility tradeoffs in the single-epoch setting. We provide two improvements over prior work: 1) extending the result to the multi-epoch and multi-dimensional setting and 2) providing explicit non-asymptotic analysis of the algorithm’s utility.\n\nLet A represent the (Toeplitz) matrix of all 1s on or below the main diagonal and 0s elsewhere; i.e., the prefix-sum matrix. In this section, we perform our analysis in the Fourier domain. To release Ax, we define a circulant matrix Acirc ∈ R2n×2n with a corresponding input vector xext = concat(x, 0n), 0n ∈ {0}n so that the first n entries of Acircxext are equal to Ax (see Appendix J.1). Thus, we study the DP release of Acircxext. Note, to be consistent with the literature on FFT, in this section, and in Appendix J, we will index all the vectors and matrices with starting index of zero.\n\nTheorem J.1 of Gray (2006) (restated in Appendix J.1) shows there exists a diagonal Σ such that Acirc = F∗ΣF for diagonal Σ, where F is the DFT matrix. Then, Acirc can then be factorized as Acirc = BcircCcirc where Bcirc = F∗Σ1/2 and Ccirc = Σ1/2F.\n\nThe (complex-valued) matrix mechanism specified by the factorization above (and presented as Algorithm 1 of Appendix C) is nearly optimal in the class of matrix-factorization-based mechanisms, as we show in Appendix J. Though we prove a simple zCDP guarantee for any participation schema having at most k = maxπ∈Π |π| participations in Theorem 4.1, we can instead use our Corollary 2.1 when the participation schema is known in advance (as in our experiments with (k, b)-participation).\n\nTheorem 4.1. Under k-participation, Algorithm 1 satisfies (k2ρ)-zCDP.\n\nThe optimal FFT decoder Observe from Theorems 2.1 and 2.2 that the privacy guarantee of our multi-participation adaptive setting is independent of the choice of decoder, B. Thus, instead of taking Bcirc above, we take the optimal decoder using the Moore-Penrose pseudoinverse of the encoder, i.e., Bcirc = SC† circ. We do so using an equivalent encoder C ∈ Rn×2n as shown in Appendix K. We find that this leads to significant improvements in the privacy-utility tradeoff (see Fig. 9 in Appendix E) at no additional computational overhead.\n\nComputation Costs Though we define the optimal FFT decoder as a pseudoinverse, observe that we do not need to optimize (or even compute) the decoder; by Appendix K, the problem is reduced to that of solving a highly structured linear system. However, we find that even suboptimal implementations using the pseudoinverse can still factorize a mechanism for n = 10, 000 in 146 minutes on a V100 GPU, remaining well within practical requirements since we need only generate a mechanism once, before it can be reused indefinitely for training. In contrast, computing optimal matrices becomes practically difficult near n ≈ 10, 000, taking 24 hours to compute an effective factorization for n = 8, 192 using batch-priority cloud CPU resources. We remark that this regime of n is highly practical, e.g., standard federated benchmarks use n ≈ 2000 (Reddi et al., 2020) and our central image classification use n = 2000. In terms of noise generation, the FFT mechanism shows preferable asymptotic properties, scaling as O(nd log2 n). However, even the optimal matrix mechanism with runtime scaling as O(dn2), noise generation on a GPU (even with significantly suboptimal implementation) takes negligible time. Further, noise from our mechanisms can be pre-generated if needed. We discuss these tradeoffs in Appendix C.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n5 EMPIRICAL EVALUATION\n\nWe compare four main mechanism classes: tree-based mechanisms (Honaker, 2015), including ‘tree-completion’ of Kairouz et al. (2021); our FFT mechanism; our optimal factorizations; and DP-SGD with (though incorrect) amplification via Poisson Subsampling (Abadi et al., 2016b).\n\nWhen computing optimal factorizations by solving Eq. (6), we may vary the sensitivity constraint set D to encode different participation schemas. We compute optimal factorizations for the (k, b)- participation setting. An encoder C factorized for one value of k can be applied for another, by simply computing its new sensitivity (due to our Section 2), though this will alter its privacy-utility tradeoffs. For example, our MF1,6e in Fig. 3 uses this to extend Denisov et al. (2022) to k > 1.\n\nWhen applying a mechanism that is determined independently of the number of participations (e.g., FFT or tree aggregation) or extending an optimal mechanism for k participations to a larger number of participations, the sensitivity may scale poorly in k. In such cases, it may actually have lower sensitivity to reuse a single encoder multiple times over the course of n steps, and hence more favorable privacy-utility tradeoffs. We term this approach encoder stamping. This also provides a straightforward method for extending any factorization to handle more iterations without, e.g., re-optimizing Eq. (6). Combined with our Section 2, stamping lets us apply mechanisms from Denisov et al. (2022), e.g., MF(k=1, n=1000)×2 in Fig. 1; mechanisms with stamping have “×s” appended in this way. Discussion of stamping and its relation to existing literature are in Appendix D.4.\n\nThe manner in which baselines from the extant literature map to this setting can be found in Appendix D.1. Since the matrix mechanism reduces privacy cost of training to that of the release of a single Gaussian mechanism, accounting in our case becomes quite simple; see Appendix D.2.\n\n5.1 EXAMPLE-LEVEL DP FOR AN IMAGE CLASSIFICATION TASK.\n\nWe train image classification models on CIFAR10 (Krizhevsky, 2009) which has become a de facto standard for comparing DP ML algorithms—sufficiently easy for existing DP algorithms to achieve nontrivial accuracy, but not so simple so as to be unable to differentiate approaches. Details on our full setup are in Appendix E; generally, they match those of Kairouz et al. (2021). Notably, we make improvements on their Online Honaker-based approach by not just completing the tree with virtual steps, but also zeroing out noise from virtual steps as detailed in Appendix D.3. We find this led to significant improvements around a few percentage points. For all matrix mechanisms except the Denisov et al. (2022) baseline and our Optimal MF(k = 20, dim = 2000) × 1, we optimize over the stamps s by the losses in Appendix D.5, which we find well match the ordering in ML accuracy.\n\nIn contrast to Section 5.2, in this section we only compare factorizations of the prefix-sum matrix; we do not incorporate momentum or cooldown directly into the mechanisms, though we use both momentum and cooldown as postprocessing for the matrix-factorization-based mechanisms and report results for the best settings we find (with both). For DP-SGD, we report results for the best setting (no momentum, with cooldown). Details are in Appendix E.\n\nMain results (Figure 1) First, we see that the optimal factorization for the target (k, b) = (20, 100) setting outperforms all other mechanisms across (nearly) all privacy levels, only slightly underperforming DP-SGD with amplification at ε = 2. To the best of our knowledge, this represents the first empirical demonstration of an ML algorithm which is competitive with DP-SGD into this high-privacy regime, without any amplification by sampling. The FFT (optimal decoder) mechanism outperforms all baselines, again well toward the high-privacy regime at ε ≈ 4. Though at a worse privacy-utility tradeoff compared with our multi-epoch optimal matrices, this mechanism shows promise for outperforming prior work when n grows too large for generating optimal factorizations.\n\n5.2 USER-LEVEL DP FOR A NEXT WORD PREDICTION TASK\n\nUser-level DP for language models is an important real-word task (McMahan & Thakurta, 2022). There is much history in the DP language modelling literature, which we briefly describe in Appendix F.1. Here, we use the standard benchmark: StackOverflow next-word prediction (Reddi et al., 2020). Our experimental setup fixes the same model and hyperparameters as Kairouz et al. (2021) and Denisov et al. (2022) except notable changes below. Details are in Appendix F.2.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Our MF6, 6e achieves within 2% relative difference in performance from the nonprivate baseline at ε = 8.8, δ = 10−6. Our MF1,6e, though worse, outperforms all baselines from the literature. Select runs were replicated multiple times, with bootstrap 95% intervals shown. DP-GDM, 2052e is the extreme case of every-round participation (342× more computationally expensive than 6 epochs of training), and hence generally infeasible. The server learning rate ηs was optimized over 0.5 and 1.0, and additionally 0.25 for ε = 2. The blue bands give the non-private baseline accuracy for 6 epochs of training with ηs = 1.0 and 0.5.\n\nNotable changes from prior work We use a much higher 1000 clients per round (≈ 100 in prior work)—enabled mainly by our multi-epoch factorizations. We also zero out large updates with (cid:96)∞ norm greater than 100 (rather than scaling down to our clipping norm of ζ = 1) as we found this improved the stability of noisy training (see Table 2 in Appendix F.3). This may have enabled more consistent success of a higher server learning rate ηs = 1.0.\n\nWe conducted initial simulations which verified that two observations from Denisov et al. (2022) for the single-epoch setting extended to our multi-epoch and large-batch (1000clients/round instead of 167) setting. First, linear server learning rate cooldown from 1× to 0.05× over the final 512 rounds offered a small improvement over constant rates (more so in the higher-privacy regime). Second, optimizing a factorizing with momentum and this cooldown schedule, rather than applying both as post-processing, consistently offers a small benefit. See Appendix F.2 for details. Thus, for our primary investigation we fix these preferable design choices and compare the following algorithms.\n\nAlgorithms All algorithms train for 6 epochs 2052 rounds, and a large-batch 1000 clients/round (better for DP training) unless otherwise noted. Honaker,6e is the DP-FTRL algorithm of Kairouz et al. (2021), trained for 2048 rounds (a power of 2). MF1,1e (Denisov et al., 2022) is the stateof-the-art for single-epoch training, using 167 clients/round and k=1. MF1,6e uses our Eq. (3) to take the (non-negative) (k=1)-optimized matrix of the previous approach and compute the sensitivity under (k=6, b=342)-participation, allowing us to train for 6 epochs (2048 rounds) with large batches. MF6,6e is our approach directly optimizing the matrix factorization for (k=6, b=342)- participation via Eq. (6). DP-SGDM,6e is the DP-FedAvg algorithm of McMahan et al. (2018), to 2052 rounds, and accounted with Poisson sampling—an incorrect, though standard, accounting computation, as noted in Section 1. DP-GDM,2052e is full-batch gradient descent for 2052 rounds and 2052 epochs, or 342× the computation cost of our 6 epoch runs. We compute the exact privacy cost for this approach, and estimate the accuracy from experiments with 1000 clients per round for computational efficiency, following the methodology of Kairouz et al. (2021). This is essentially an upper bound on the best privacy-accuracy tradeoffs with unlimited computational resources.\n\nMain results (Figure 3) We find that our MF6,6e, is the best feasible private result at 25.25% accuracy and (17.7, 10−6)-DP. This exceeds the non-private baseline of 25.2% accuracy reported by Kairouz et al. (2021), is within the margins of small hyperparameter tuning differences of our improved non-private baselines (25.43%) and private full-batch gradient descent (25.31%). At (8.84, 10−6)-DP, MF6,6e achieves 24.94% accuracy, substantially improving over the previous state-of-the-art at this privacy level given by MF1,1e at 24.11% accuracy, and considerably improves on both the accuracy and privacy of Honaker,6e (24.86% accuracy at (21.0, 10−6)-DP). In fact, we achieve better accuracy at ε = 8.84 than prior methods achieve at ε = 17.7. DP-SGDM,6e is outperformed by MF6,6e and MF1,6e across all ε values evaluated. Fig. 12 in Appendix F shows results for each learning rate separately, with numeric results in Tables 4 and 5.\n\nDiscussion and conclusions can be found in Appendix G.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and In Proceedings of the 2016 ACM SIGSAC\n\nLi Zhang. Deep learning with differential privacy. conference on computer and communications security, pp. 308–318, 2016a.\n\nMartin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Oct 2016b. doi: 10.1145/2976749.2978318. URL http://dx.doi.org/10.1145/2976749.2978318.\n\nRaef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Proc. of the 2014 IEEE 55th Annual Symp. on Foundations of Computer Science (FOCS), pp. 464–473, 2014.\n\nMark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and\n\nlower bounds. In Theory of Cryptography Conference, pp. 635–658. Springer, 2016.\n\nS. L. Campbell and C. D. Meyer. Generalized inverses of linear transformations / S. L. Campbell,\n\nC. D. Meyer. Pitman London ; San Francisco, 1979. ISBN 0273084224.\n\nLei Cao, Darian McLaren, and Sarah Plosker. Centrosymmetric stochastic matrices. Linear and\n\nMultilinear Algebra, 70(3):449–464, 2022.\n\nNicholas Carlini, Chang Liu, ́Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In Proceedings of the 28th USENIX Conference on Security Symposium, SEC’19, pp. 267–284, USA, 2019. USENIX Association. ISBN 9781939133069.\n\nNicholas Carlini, Florian Tram`er, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, ́Ulfar Erlingsson, Alina Oprea, and Colin RafIn 30th USENIX Security Sympofel. Extracting training data from large language models. sium (USENIX Security 21), pp. 2633–2650. USENIX Association, August 2021. ISBN 978-1939133-24-3. URL https://www.usenix.org/conference/usenixsecurity21/ presentation/carlini-extracting.\n\nT.-H. Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM\n\nTrans. on Information Systems Security, 14(3):26:1–26:24, November 2011.\n\nSoham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. UnlockarXiv preprint\n\ning high-accuracy differentially private image classification through scale. arXiv:2204.13650, 2022.\n\nFrank de Hoog. A new algorithm for solving toeplitz systems of equations.\n\nLinear Algebra and its Applications, 88-89:123–138, 1987. doi: https://doi.org/ 10.1016/0024-3795(87)90107-8. URL https://www.sciencedirect.com/science/ article/pii/0024379587901078.\n\nISSN 0024-3795.\n\nSergey Denisov, Brendan McMahan, Keith Rush, Adam Smith, and Abhradeep Guha Thakurta. Improved differential privacy for sgd via optimal private linear operators on adaptive streams, 2022. URL https://arxiv.org/abs/2202.08312.\n\nCynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N. Rothblum. Differential privacy under continual observation. In Proc. of the Forty-Second ACM Symp. on Theory of Computing (STOC’10), pp. 715–724, 2010.\n\nAlexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman. The Power of Factorization Mechanisms in Local and Central Differential Privacy, pp. 425–438. Association for Computing Machinery, New York, NY, USA, 2020. ISBN 9781450369794. URL https://doi.org/10. 1145/3357713.3384297.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\n ́Ulfar Erlingsson, Vitaly Feldman,\n\nIlya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep Thakurta. Amplification by shuffling: From local to central differential privacy via In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algoanonymity. rithms, pp. 2468–2479. SIAM, 2019.\n\nNadia Fawaz, S Muthukrishnan, and Aleksandar Nikolov. Nearly optimal private convolution. In\n\nEuropean Symposium on Algorithms, pp. 445–456. Springer, 2013.\n\nVitaly Feldman, Audra McMillan, and Kunal Talwar. Hiding among the clones: A simple and nearly optimal analysis of privacy amplification by shuffling. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 954–964. IEEE, 2022.\n\nHendrik Fichtenberger, Monika Henzinger, and Jalaj Upadhyay. Constant matters: Fine-grained complexity of differentially private continual observation, 2022. URL https://arxiv.org/ abs/2202.11205.\n\nRobert M Gray. Toeplitz and circulant matrices: A review. 2006.\n\nMoritz Hardt and Kunal Talwar. On the geometry of differential privacy. In STOC, 2010.\n\nMonika Henzinger, Jalaj Upadhyay, and Sarvagya Upadhyay. Almost tight error bounds on differ-\n\nentially private continual counting. Personal communication, 2022.\n\nJames Honaker. Efficient use of differentially private binary trees. Theory and Practice of Differen-\n\ntial Privacy (TPDP 2015), London, UK, 2015.\n\nPeter Kairouz, Sewoong Oh, and Pramod Viswanath. Extremal mechanisms for local differential privacy. Journal of Machine Learning Research, 17(17):1–51, 2016. URL http://jmlr. org/papers/v17/15-135.html.\n\nPeter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng\n\nXu. Practical and private (deep) learning without sampling or shuffling. In ICML, 2021.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images, 2009.\n\nChao Li, Gerome Miklau, Michael Hay, Andrew Mcgregor, and Vibhor Rastogi. The matrix mechanism: optimizing linear counting queries under differential privacy. The VLDB Journal, 24: 757–781, 2015.\n\nRyan McKenna, Gerome Miklau, Michael Hay, and Ashwin Machanavajjhala. Optimizing error of high-dimensional statistical queries under differential privacy. Proc. VLDB Endow., 11(10): 1206–1219, jun 2018. ISSN 2150-8097. doi: 10.14778/3231751.3231769. URL https:// doi.org/10.14778/3231751.3231769.\n\nBrendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private In International Conference on Learning Representations (ICLR),\n\nrecurrent language models. 2018. URL https://openreview.net/pdf?id=BJ0hF1Z0b.\n\nH. Brendan McMahan and Abhradeep Thakurta. Federated learning with formal differential privacy guarantees. Google AI Blog, 2022. URL https://ai.googleblog.com/2022/ 02/federated-learning-with-formal.html.\n\nHenri J Nussbaumer. The fast fourier transform. In Fast Fourier Transform and Convolution Algo-\n\nrithms, pp. 80–111. Springer, 1981.\n\nSashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn ́y, CoRR,\n\nSanjiv Kumar, and H. Brendan McMahan. abs/2003.00295, 2020. URL https://arxiv.org/abs/2003.00295.\n\nAdaptive federated optimization.\n\nCongzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 196–206, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nShuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE Global Conference on Signal and Information Processing, pp. 245–248. IEEE, 2013.\n\nJoel Tropp. Topics in Sparse Approximation. PhD thesis, The University of Texas at Austin, 2004.\n\nYu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled renyi differential privacy and analytical moments accountant. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pp. 1226– 1235, 2019.\n\nGanzhao Yuan, Yin Yang, Zhenjie Zhang, and Zhifeng Hao. Optimal linear aggregate query processing under approximate differential privacy. CoRR, abs/1602.04302, 2016. URL http: //arxiv.org/abs/1602.04302.\n\nYuqing Zhu and Yu-Xiang Wang. Poisson subsampled r ́enyi differential privacy. In International\n\nConference on Machine Learning, pp. 7634–7642. PMLR, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA SUMMARY OF NOTATION AND TERMINOLOGY\n\nThe following table summarizes the notation used throughout the paper:\n\nn\n\nd xi ∈ R or Rd x ∈ Rn×d\n\nζ\n\nπ\n\nΠ\n\nD\n\nD\n\n(k, b)-participation A ∈ Rn×n T ∈ Rn×n\n\nλmin(A), λmax(A). A∗ X(cid:63) A†\n\nA[i,j] A[i,:] and A[:,j] s\n\nconv (S)\n\n[n]\n\n(cid:107)X(cid:107)F\n\nNumber of steps of the streaming linear query (SGD steps or FL rounds)\n\nDimension of per-step user contributions.\n\nSum of per-example gradients (or per-user model updates) on step i.\n\nStream of inputs xi, equiv. matrix with rows xi (so xi = x[i,:]). Clipping norm that limits the size of per-example contributions to xi.\n\nParticipation pattern, the set of steps that an example could participation in.\n\nParticipation schema, set of sets of steps (set of all π) an example could participate in.\n\n= {x − ̃x | (x, ̃x) ∈ N}, the set of deltas between neighboring input streams x, ̃x.\n\nCorners of D when assumed to be a polytope, D = conv(D).\n\nparticipation schema Π with at most k participations, separated by exactly b.\n\nLower-triangular linear query matrix to be factorized as A = BC. T := A(cid:62)A for convenience.\n\nSmallest and largest eigenvalues of real matrix A.\n\nConjugate (Hermitian) transpose of A.\n\nA matrix X that is “optimal” in a context-dependent sense.\n\nMoore-Penrose pseudoinverse of matrix A. The (i, j)th entry of matrix A. The ith row and jth column.\n\nNumber of encoder C replications (stamps) into a block-diagonal matrix.\n\nConvex hull of the set S.\n\n= {1, . . . , n}\n\nThe Frobenius norm of a matrix X.\n\nWe utilize terminology from federated learning as well as standard centralized training, which generally map as follows:\n\nCentralized\n\nFederated\n\nexample\n\nbatch size\n\nDP-SGD\n\nuser or client\n\nclients-per-round\n\nDP-FedAvg\n\nstep or iteration\n\n(communication) round\n\ngradient\n\nmodel update\n\nB GENERALIZED SENSITIVITY AS AN OPERATOR NORM\n\nEq. (2) shows that our generalized notion of sensitivity can be viewed directly as a particular operator norm. To see this, view C : V1 → V2 as a linear operator from vector space V1 to V2. Then with (cid:107) · (cid:107)(1) the vector norm on V1 and similarly for V2, an operator norm is defined as\n\n(cid:107)C(cid:107)(1),(2) =\n\nmax u∈V1:(cid:107)u(cid:107)(1)≤1\n\n(cid:107)Cu(cid:107)(2).\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nBecause we use the Gaussian mechanism and thus are interested in the (cid:96)2 sensitivity, (cid:107) · (cid:107)(2) = (cid:107) · (cid:107)2, and we define the norm\n\n(cid:107)u(cid:107)(1) = (cid:107)u(cid:107)D := inf\n\n(cid:110)\n\nr > 0 :\n\n(cid:111)\n\n,\n\n∈ D\n\nu r\n\nthe vector norm induced by D (the fact that D is a closed, convex, symmetric set ensures this is a norm). Note u ∈ D ⇔ (cid:107)u(cid:107)D ≤ 1. Thus, we have\n\nsensD(C) = (cid:107)C(cid:107)D,2.\n\n(9)\n\nC THE FFT MECHANISMS AND REDUCING COMPUTATION\n\nAlgorithm 1 DP-Prefix Sum Computation via FFT (with d = 1)\n\nInputs: Data vector x ∈ Rn (with each |xi| ≤ ζ) and zCDP parameter ρ. vDFT ∈ C2n ← the DFT of v (defined in Eq. (34)). Let vDFT F ← DFT matrix in 2n-dimensions, where the k-th row of F is given by\n\n[:n] be the first n coordinates.\n\nF[k,:] =\n\n1 √\n\n2n\n\n(cid:20)\n\n(cid:18)\n\nexp\n\n−\n\n(cid:19)\n\nj2πka 2n\n\n: a ∈ {0, . . . , 2n − 1}\n\n.\n\n(cid:21)\n\n(Σ, w) ← (diag(vDFT), standard complex Normal in 2n-dimensions).\n\n(cid:18)(cid:20)\n\nx0, x0 + x1, . . . ,\n\n(s, (cid:101)z) ← Output s + (cid:101)z.real[0, . . . , n − 1].\n\na=0\n\nxa\n\nn−1 (cid:80)\n\n(cid:21)\n\n(cid:113) κ2(cid:107)vDFT(cid:107)1\n\n4nρ\n\n,\n\n(cid:0)F∗Σ1/2 · w(cid:1)\n\n(cid:19)\n\n.\n\nWe propose two FFT mechanisms. First, we propose the FFT mechanism which is described in Algorithm 1. This mechanism has the same computation complexity—no optimization costs and O(nd log n) noise generation—as the Honaker method used in Kairouz et al. (2021) but at a better privacy-utility tradeoff, as shown in Fig. 9. The privacy and utility analysis can be found in Appendix J.\n\nThe FFT Optimal Decoder (FFT Opt Dec) mechanism presented in Section 4 represents taking (a real-valued translation of) the encoder C from Algorithm 1 and using the optimal decoder, defined in terms of the Moore-Penrose pseudoinverse of C. Similarly to Algorithm 1, there is no need to construct a literal matrix to multiply by in the case of noise defined by the optimal decoder; noise generation time of the mechanism, however, increases by a logarithmic factor to O(nd log2 n) (as discussed in Appendix K). This complexity is still feasible for many steps (which we will discuss below) and comes with significant utility benefits (see Fig. 1).\n\nAll the mechanisms we study scale as either O(n2) or O(n · polylog(n)). For our n = 2000 step environments, and even far beyond to n ≈ 10, 000, our algorithms can be efficiently realized on GPUs with runtime on the order of seconds per step (including computing and applying gradients and noise). The main challenge in these cases are storing the n2 + nd coordinates in GPU memory (the former for the decoder matrix, the latter for the noise samples). Given that each of the d coordinates of noise can be sampled independently, this algorithm is straightforwardly parallelizable and so work may be partitioned across many processors when needed. Noises could instead be pregenerated in an entirely separate process, and stored on disk, to be loaded into memory row-by-row concurrently with training. Outside of computation, both our FFT mechanisms take O(nd) space as all noises for the x ∈ Rn×d must be pre-generated. This is in contrast to all prior work, and even our optimal factorizations, which require only O(d) space to generate the noise at the current step. Again, we note that space is ofter much cheaper so this is typically not the limiting factor.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nD MECHANISMS UNDER CONSIDERATION: BASELINES, SUBTLETIES, AND\n\nLOSSES.\n\nD.1 BASELINES\n\nKairouz et al. (2021) and Denisov et al. (2022) both present approaches for ML model training which can be understood as instances of the matrix mechanism–the former grounded in the binarytree mechanism as refined by Honaker (2015), and the latter explicitly optimizing a factorization under single participation. These two works yield two natural baselines:\n\n• Kairouz et al. (2021) explores ‘tree restarts’ (generalized as our notion of ‘stamps’, s, in Section 5) and the so-called ‘tree-completion trick’ for the Honaker estimator-from-below variant of the binary tree method for computing differentially private prefix sums; the matrix-factorization perspective on this estimator allows us to implement slightly optimized versions of these methods; see Appendices D.3 and D.4.\n\n• Denisov et al. (2022) computes optimal factorizations of various optimization-related matrices, though only for a single epoch. For these matrices, we leverage the results in Section 2 to directly compute the sensitivity of the encoder matrices for multiple participations.\n\nThese two papers can be combined in other ways as well; e.g., the results of Denisov et al. (2022) show that the ‘fully efficient estimator’ of Honaker (2015) may be used as a drop-in replacement for the estimator from below in Kairouz et al. (2021). We focus on the two mechanisms specified above as the natural baselines for the present work.\n\nD.2 PRIVACY ACCOUNTING\n\nThe matrix mechanism Eq. (1) conceptually adds isotropic Gaussian noise in some encoded space. In our case, we encode a matrix of gradients (clipped to (cid:96)2 norm ζ) computed over the course of training, denoted by G, with the matrix C, and add Gaussian noise to each entry in the matrix CG. Under the assumption that the matrix factorization has been appropriately scaled so C has sensitivity 1, this Gaussian noise will have standard deviation σ = ζz in each coordinate, where z is the ‘noise multiplier’ parameter determining the privacy level of the mechanism (see Table 3 for example).\n\nPrivacy costs are computed as a single application of the Gaussian mechanism to GC using the PLDAccountant provided by the Google DP Library3. We also use this accountant to analyze DP-(S)GD baselines (which require more complex accounting), yielding a small improvements in ε over the Renyi-DP accounting used in prior works.\n\nD.3\n\nIMPROVEMENTS TO‘TREE COMPLETION’ BY REMOVING NOISE FROM VIRTUAL STEPS\n\nThe “tree completion” trick of Kairouz et al. (2021) is used on the last step of any restart (in ours, stamp) to reduce the noise added on this step. This is achieved by adding virtual steps (with 0 inputs) until the final step of that level in the tree, because this noise will be the lowest in that level. In this section, we show how to further improve on this trick and that our matrix mechanisms make analyzing such tricks easier. Our implementations of the binary-tree baslines Honaker (2015); Kairouz et al. (2021) utilize these improvements.\n\nFor the online Honaker estimate, Honaker (2015) obtain a DP estimate for the release node i ∈ [n] (representing the prefix sum until i) but summing the corresponding subtrees prior to this node. These are exactly the subtrees corresponding to the binary representation of this node (Honaker, 2015). Then, the variance required to release node i, with subtrees of height 0, 1, ..., li − 1, is\n\n  · σ2 =\n\nj · 2j c2\n\n\n\n\n\nli−1 (cid:88)\n\nj=0\n\n1 2 · (1 − 2−μ)\n\n· σ2\n\nwhere\n\ncj =\n\n1/2j\n\nl1−1 (cid:80)\n\n(1/2j)\n\nj=0\n\n.\n\nNotice that reaching a new height in the tree decreases the variance needed. In Kairouz et al. (2021) and just before terminating on some non-power-of-two step n(cid:48) < n, they run n − n(cid:48) virtual steps\n\n3https://github.com/google/differential-privacy\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\non zero gradients. This enables their mechanism to use the minimal noise for the power-of-two-step for the final real step.\n\nHowever, notice that in both these cases, the methods assume that these virtual steps must be privatized. Indeed, they do not need to be because we know apriori that these steps are virtual, i.e., not corresponding to real gradients. Thus, in our methods we account for this in our mechanism and reduce the noise of the final step accordingly. Importantly, this can be computed without altering the asymptotic runtime and storage complexity of the algorithm: on the last step, the contributions of the virtual steps to the power-of-two noise can be calculated using, e.g., a second binary tree, and removed. This leads to a significant benefit in the loss as we observed in Table 1 in Appendix D.5.\n\nWe believe this oversight of prior works showcases the power of our matrix mechanism approach. Indeed, let Ctree be a matrix representing the (complete) binary tree used in the mechanisms of Honaker (2015); Kairouz et al. (2021) with 2(cid:100)log2(n)(cid:101) leaves; for the sake of concreteness, assume this is the matrix constructed in Appendix C of Denisov et al. (2022). Let BHon represent the Honaker estimator-from-below; in our language, the decoder used by (Kairouz et al., 2021).\n\nThe tree completion trick of (Kairouz et al., 2016) can be understood as follows. The matrix BHonCtree is of size 2(cid:100)log2(n)(cid:101) × 2(cid:100)log2(n)(cid:101). In the case that n is not a power of two, the penultimate rows and columns of this matrix will go unused. However, for this factorization, the variance added by BHonon the final row will be quite small, due to the binary tree’s redundancy in encoding estimates of this sum. The matrix we wish to factorize is a prefix-sum matrix S of size n × n; this matrix can be expressed as any one of a family of transformations of the (potentially larger) product BHonCtree:\n\nS = PjBHonCtreeE,\n\nwhere E embeds a n-dimensional vector into 2(cid:100)log2(n)(cid:101) dimensions by padding with zeros, and Pj projects back down to n dimensions in a similarly axis-aligned way, taking the first n − 1 rows of its right-hand matrix argument, and only one, but any of the jth rows for n ≤ j ≤ 2(cid:100)log2(n)(cid:101). To minimize the Frobenius norm of the constructed decoder in the factorization of S, we may simply pick the row with the lowest (cid:96)2 norm; in the case of BHon, this is the final row.\n\nOne more optimization becomes clear when tree completion is formulated in this manner. Similar to our optimal decoder of Section 4, any decoder can be used without changing the sensitivity of the encoder. Noting that nonzero entries in the decoder increase our loss of Eq. (4), we can simply zero out the columns of the decoder corresponding to these virtual steps—this decreases the loss, preserves the same error in the DP estimate of the prefix sum (the inputs are 0), and maintains the same DP guarantee. In other words, we need not account for the noise, or the error it introduces, of virtual steps. We now provide a more rigorous explanation.\n\nThe image of CtreeE can be contained in an axis-aligned subspace; effectively, the subspace corresponding to elements that may be nonzero in the binary tree when run for n steps. In other words, the columns of the decoder corresponding to rows of the encoder that are removed via the projection need not be included: because the input is not processed. Therefore, denoting the projection onto this subspace by Π, we may write:\n\nS = PjBHonΠCtreeE = (PjBHonΠ) (ΠCtreeE) ,\n\nfurther reducing the variance of the decoder (without increasing the sensitivity of the encoder) in this incomplete binary tree case.\n\nIn our implementations of the online Honaker mechanism, we freely use these tricks, in addition to exact calculations of the sensitivity of the mechanism enabled by noting that the encoder is allnonnegative and the observations of Section 2, leading to some improvements in these mechanisms over those in the existing literature. No changes to accounting are required, as privacy is inherited from the matrix mechanism perspective.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nD.4 STAMPING: REPEATED MECHANISMS IN THE MATRIX-FACTORIZATION SETTING\n\nIn stamping, we define a new encoder matrix as the Kronecker product of some given encoder C with an s × s identity matrix I, creating a new sn-dimensional linear DP query mechanism. Assuming C is of shape n × n, the resulting encoder is an sn × sn block-diagonal encoder matrix, formed by ‘repeating’ the matrix C along the diagonal.\n\nKairouz et al. (2021) explored ‘restarting’ their binary-tree based prefix sum estimation mechanism, treating the number of restarts used as a hyperparameter, and treating the result of a ‘completed’ application of the binary tree as fixed. For linear operators A with constant columns below the diagonal, this approach may be used to construct a new factorization from an existing one. This constant-columns property, or a block-based variant thereof, is required to simply treat the output from a ‘completed’ application of the existing mechanism as fixed; for a general matrix A, there is no clear prefix property which can be leveraged for this purpose.\n\nTaking the matrix view, one may construct a similar encoder/decoder pair for the prefix-sum matrix by reusing the initial decoder B on the block-diagonal and fixing the columns to simply repeat the final row of this decoder below the block-diagonal; notice that the constant-column property of the prefix sum matrix guarantees that this construction appropriately factorizes A. The noise that the matrix mechanism thus constructed adds can be implemented as a ‘restarted’ tree mechanism; however, since we compute sensitivities exactly for decoders of this structure as in Section 2, the privacy properties of these mechanisms we construct to replicate the ‘restarts’ of (Kairouz et al., 2021) may not be identical to those presented there, where accounting is performed by composition.\n\nThe matrix-mechanism perspective additionally allows one to apply the ‘stamping’ construction to any linear operator A (e.g. the momentum matrix), where a reuse of fixed previous outputs is not possible. A ‘stamped’ factorization of any A may be obtained, for example, by matrix pseudoinversion: letting B = A (C ⊗ I)†. This defines a legitimate factorization of any A requiring only suitable non-degeneracy assumptions on C, and indeed represents the optimal decoder for the stamped encoder.\n\nThe pseudoinverse-based construction can, even in the prefix-sum case, be quite different from a construction designed to replicate ‘restarted’ mechanisms. For example, if C is a matrix representation of the binary-tree encoder and I = 1, then the resulting decoder matrix represents the full, rather than online, Honaker decoder (Honaker, 2015); the validity of this mechanism in the adaptive streaming setting was only shown quite recently by Denisov et al. (2022).\n\nFor comparability with existing literature (and to preserve potential for an efficient implementation), however, all of the tree-based mechanisms we explore in the main body have decoders which replicate the setting of composition4. All other stamped mechanisms used the optimal decoder.\n\nOptimizing over s Considering instantiation enables us to directly analyze and minimize (over s) the stamped mechanism’s multi-epoch loss Eq. (4) without running compute intensive ML experiments. Indeed, we observe in Table 1 of Appendix D.5 that for mechanisms which were not explicitly optimized for the (k, b)participation setting, there exist stamped mechanisms (s > 1) with much lower loss that correspondingly led to much more performant ML models (e.g., Figures 7 and 8 of Appendix D).\n\nInterestingly, with our capacity to measure mechanisms at a single shot in the multi-epoch setting, we see a similar trend as was observed for restarts in Kairouz et al. (2021): that ‘stamped’ mechanisms have lower total loss than their non-stamped full-tree counterparts in the 20-epoch, 100 steps / epoch setting (see Table 1); training performance of these ‘stamped’ mechanisms on CIFAR10 can be found in Fig. 7. However, there is a significant improvement in our approach in that we can now directly tune this hyperparameter without the need to actually run ML training. This lets us reduce computation by only analyzing the loss of the generated matrices and then running the mechanism with the lowest loss.\n\n4We show how the optimal binary-tree decoder differs from the online, composition-based decoder in Fig. 6\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Per-iterate variance for prefix-sum factorizations. All mechanisms above yield the same privacy ((ε, δ) = (4.38, 10−5) in the (k, b) = (20, 100) setting), but have different total variances (the integral of the curves above).\n\nD.5 FACTORIZATION LOSSES AND PER-ITERATE VARIANCE\n\nAs a first measure of the privacy-utility tradeoff, we compare the losses of each mechanism from Eq. (4) for factorizations of the matrices under consideration.\n\nWe compare measured losses of several factorizations of the prefix-sum matrix for the (k, b) = (20, 100) setting of the CIFAR experiments in Table 1. We plot the per-iterate variance of the mechanisms in Fig. 1, along with several variations, in Fig. 4. In Fig. 5, we plot the per-iterate variance distribution of factorizations of the momentum and cooldown matrix (described in Section 5) at a fixed variance level, and with various privacies, computed for the (k, b) = (6, 342) participation setting.\n\nFigs. 4 and 5 both demonstrate the effect of (k, b)-participations on the optimization problem. Particularly interesting to consider are the optimally-factorized matrices; in both cases, the epoch structure is clearly visible in the manner in which the mechanisms distribute variance. We see also the effect of ‘stamps’ s in the variance distribution, effectively a proxy for the epoch structure directly accounted for by the optimal mechanisms.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nMechanism\n\n(Online) Honaker(n = 2000) (Online) Honaker(n = 1000)×2 (Online) Honaker(n = 400)×5 (Online) Honaker(n = 200)×10 (Online) Honaker(n = 100)×20\n\nOptimal Decoder Honaker(n = 2000) Optimal Decoder Honaker(n = 1000)×2 Optimal Decoder Honaker(n = 400)×5 Optimal Decoder Honaker(n = 200)×10 Optimal Decoder Honaker(n = 100)×20\n\nMF(k = 20, n = 2000)\n\nMF(k = 10, n = 1000)×2 MF(k = 5, n = 500)×4 MF(k = 1, n = 100)×20 MF(k = 1, n = 2000) MF(k = 1, n = 1000)×2 MF(k = 1, n = 500)×4 MF(k = 1, n = 400)×5 MF(k = 1, n = 200)×10 MF(k = 1, n = 100)×20\n\nFFT(n = 2000) FFT(n = 1000)×2 FFT(n = 400)×5 FFT(n = 200)×10 FFT(n = 100)×20\n\nFFT Optimal Decoder(n = 2000) FFT Optimal Decoder(n = 1000)×2 FFT Optimal Decoder(n = 400)×5 FFT Optimal Decoder(n = 200)×10 FFT Optimal Decoder(n = 100)×20\n\n(k, b) = (20, 100) Prefix Sum Loss\n\n5.8e6 3.3e6 2.1e6 2.0e6 2.1e6\n\n2.4e6 1.6e6 1.2e6 1.4e6 1.8e6\n\n6.5e5\n\n8.8e5 1.2e6 2.5e6 1.6e6 1.37e6 1.4e6 1.5e6 1.8e6 2.5e6\n\n2.3e6 1.8e6 1.6e6 1.9e6 2.5e6\n\n2.2e6 1.5e6 1.1e6 1.2e6 1.7e6\n\nComputation for n Steps, d = 1\n\nO(n log n) Noise Generation\n\nO(n2) Noise Generation\n\nO(n3) Optimization + O(n2) Noise Generation\n\nO(n log n) Noise Generation\n\nO(n log2 n) Noise Generation\n\nTable 1: Loss for various prefix-sum factorizations, computed via Eq. (4), in multiple-participation setting for 20 epochs with 100 steps per epoch. Lowest-loss mechanism in each class bolded. Note that ‘(Online) Honaker’ corresponds to the restarted decoder. By evaluating the dual problem (Section 3), 6.53e5 represents a lower bound on the optimal loss; the optimal matrix factorization is within 0.2% of this optimal value. Though up to On2 noise generation can be tolerated practically for large ML training runs, we find that the stamped FFT optimal decoder obtains the best privacyutility tradeoffs while requiring only O(n log2 (n)) time. Sensitivity is calculated exhaustively with contributions constrained to +1 for all matrices except FFT ones, where sensitivity is calculated using Theorem 2.1.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Per-iterate variance for momentum + cooldown matrix factorizations. Privacy measured in the (k, b) = (6, 342) setting.\n\nFigure 6: Comparing the optimal decoder, i.e., OptDecoderHonaker, with the standard stamping decoder (including fixing the output of each block), i.e., Online Honaker, with the optimal factorization.\n\nE DETAILS AND ADDITIONAL EXPERIMENTS FOR CIFAR10.\n\ntrain\n\nimage-classification models\n\nWe in using tensorflow-datasets, containing 50,000 training and 10,000 test examples. We evaluate and compute test accuracies on the entire test set, following the open-sourced code of Kairouz et al. (2021). We reuse the network architecture, dataset processing and initialization strategies presented in Kairouz et al. (2021); in particular, the architecture we use can be found in their Table 2 (b).\n\nthe CIFAR10\n\ndataset\n\nhosted\n\nas\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nOptimization setup and hyperparameters We train all mechanisms for 20 epochs with batch size of 500, yielding 100 steps per epoch and 2000 total. After performing some small initial grid searches, we settled on using linear learning rate cooldown to 0.05× the initial learning rate over the last 500 steps of training. We found this consistently improved utility for all mechanisms and privacy levels.\n\nAs mentioned in Section 5, for this 20-epoch training setup, we only compare factorizations of the prefix-sum matrix, and do not include any factorizations of matrices which incorporate momentum of learning rate cooldown directly in the mechanism itself (Denisov et al., 2022). We sweep over learning rates of values (1 × 10i, 2 × 10i, 5 × 10i) for i in {−2, −1}; for all mechanisms and noise levels, optimal values were in the interior of this sweep. We sweep over momentum values of 0, 0.85, 0.9, 0.95 though find nonzero momentum works best for all matrix mechanisms, and no momentum works best for DP-SGD at our scale as found previously by Kairouz et al. (2021).\n\nFor Honaker and FFT-based factorizations, there is no known a-priori way to choose the optimal number of s for a given (k, b) setting. Therefore we treat the value s as a hyperparameter, and sweep across it, for s ∈ {1, 2, 5, 10, 20}. As can be seen in Table 1, the optimal s for both of these factorizations was in the interior of this sweep. As shown, e.g., in Fig. 7, the training-time performance of these mechanisms matched the expected order for computed loss. This value s represents an extra hyperarameter which must be set for the Honaker and FFT mechanisms; to the best of our knowledge, computing the loss for various instantiations of these mechanisms via Eq. (4) represents the only known method for setting this parameter other than simply training models.\n\nWe also apply our sensitivity analysis of Section 2 to the matrices of Denisov et al. (2022) which are optimized for k = 1. In doing so, we can also optimize the number of stamps which we do. We report the best results as identified by the losses in Table 1.\n\nFigure 7: DP-FTRL-Honaker baseline ablation with respect to number of ‘stamps’ s.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Ablation of prefix-sum factorizations, optimized for different number of epochs, and ‘stamped’ as appropriate. Performance improves as the geometry used for computing the factorization approaches that used for training.\n\nFigure 9: Our optimal multi-epoch matrix and FFT-based mechanisms outperform all others, including DP-SGD with amplification, as low as ε ≈ 4. Using our sensitivity calculation of Theorem 2.1 and stamping (Section 5), we optimize a single pass (k = 1) matrix of Denisov et al. (2022) but apply it here with > 1 pass. We use an online Honaker-based decoder equivalent to that of Kairouz et al. (2021) except for a significant improvement to tree-completion in Appendix D.3. Models trained for 20 epochs on CIFAR10 with a batch size of 500. We repeat each setting 12 times and show 95% bootstrapped confidence intervals. Empirical setup is in Section 5.1.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nF ADDITIONAL STACKOVERFLOW DETAILS\n\nF.1 PRIVACY AND LANGUAGE MODELLING\n\nLanguage models trained on user data are an important real-world application of DP training, as these models can memorize their training data if appropriate mitigations are not applied (Carlini et al., 2019; Song & Shmatikov, 2019; Carlini et al., 2021). Since one user might contribute 1000s of tokens (training examples) to a dataset, it is particularly important to consider user-level guarantees (McMahan et al., 2018). Building on the approach of Kairouz et al. (2021), Google recently announced the first-ever launch of a language model trained on user data with a formal user-level DP guarantee (ρ = 0.81 zCDP), further demonstrating the importance of this application (McMahan & Thakurta, 2022).\n\nThe StackOverflow next-word prediction task, introduced in (Reddi et al., 2020), has become a benchmark problem for DP training, and our experimental setup here fixes the same model and adapts hyperparmaeters from previous work including Kairouz et al. (2021); Denisov et al. (2022).\n\nF.2 HYPERPARAMETER TUNING AND INITIAL EXPERIMENTS\n\nAll runs use server momentum 0.95 and a learning-rate cooldown schedule for the last 25% of rounds. Zeroing outlier updates and using 1000 clients/round (6 epoch runs) allows the use of the higher server learning rates. MF1,1e replicates the result of single-epoch training from Denisov et al. (2022); note that with 167 clients/round and this mechanism, the higher learning rate does not appear to help. Fig. 10 gives preliminary experimental results which informed the main experiments used in the paper. Note that the y-axis range (Test set accuracy) is highly compressed, and so the primary point of comparison is on epsilons. For example, Denisov et al. (2022) shows that cross-run variation of 0.002 or more is typical.\n\nThe 6 horizontal lines give test-set accuracy for various non-private training mechanisms. The “Unnoised MF” runs correspond to the same code path used for privacy, but without any noise addition. In particular, these use momentum with learning rate cooldown; the other unnoised runs use a standard FL implementation with momentum but a fixed learning rate schedule; “cpr=167“ corresponds to one epoch of training (167 clients/round), and “cpr=50” is 50 clients/rounds (only about 1/3 of an epoch). This last non-private baseline uses the best hyperparameters for FedAvgM from Reddi et al. (2020).\n\nThe two “Unnoised MF” runs with accuracies between 0.246 and 0.248 are functionally identical, and the line near 0.248 accuracy is the same except it does not use learning-rate cooldown. Thus, for the given learning rates, we see the higher-epsilon private runs are adding sufficiently small noise that the accuracy is essentially equivalent to unnoised baselines with the same hyperparameters. However, using larger learning rates can achieve accuracy over 25%, even with privacy as in the case of the MF-6-6 run, hence motivating the inclusion of larger learning rates in the main experiments.\n\nThe MF (Matrix Factorization) runs with “prefix” in the name correspond to computing an optimal factorization of the prefix-sum matrix (lower triangular matrix of ones) and then applying momentum (and possibly learning rate cooldown) as post-processing. The other MF runs directly factor the momentum or momentum+cooldown matrix.\n\nF.3\n\nIMPACT OF ZEROING-OUT LARGE-NORM UPDATES\n\nWe observed that zeroing out updates with an (cid:96)∞ norm greater than 100ζ = 100 greatly stabilized training, allowing larger learning rates, particularly for MF6,6e. We conducted ablation experiments where we turned off this zeroing, which produced a large fraction of unconverged runs as detailed in Table 2. The number of updates zeroed increases significantly with larger amounts of noise and larger learning rates, as shown in Fig. 11.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Preliminary experimental results and non-private (unnoised) baselines. The notation sX, cY indicates a server learning rate X and client learning rate Y .\n\nFigure 11: Number of large-magnitude updates zeroed per training round.\n\nMechanism\n\nDP-SGDM, 6e, ηs = 0.5\n\nDP-SGDM, 6e, ηs = 0.5\n\nMF6,6e, ηs = 0.5\n\nMF6,6e, ηs = 1.0\n\nε\n\n2\n\n9\n\n2\n\n9\n\nUnconverged runs with\n\nZeroing\n\nNo Zeroing\n\n0 of 3\n\n0 of 3\n\n0 of 3\n\n0 of 3\n\n1 of 2\n\n0 of 2\n\n3 of 4\n\n3 of 4\n\nTable 2: Number of divergent training runs with and without zeroing of user updates with (cid:96)∞ norm greater than 100; ηs gives the server learning rate.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Complete data used in Fig. 3 showing results for server learning rates ηs = 0.5 and 1.0, as well as 0.25 when ε=2. All algorithms use momentum 0.95 and a client learning rate of 1.0. For the 1 and 6 epoch runs, we observe MF generally tolerates larger learning rates, though lower learning rates perform better for all algorithms at ε=2.\n\nF.4 COMPLETE RESULTS\n\nIn this section we give additional details on our main grid of experiments. Fig. 12 uses the same data as Fig. 3, but shows results for each learning rate individually. Tables 4 and 5 give the mean, minimum, maximum, and standard deviation of test-set accuracy corresponding to Fig. 12, as well as the number of replicated experiments (‘count’).\n\nTable 3 gives the noise multipliers to achieve our various privacy targets ε. Due to a change in the accountant used, we have slightly different ε targets around 8.8 for the different methods. Note the noise multipliers here are incomparable between the MF and (S)GD mechanisms in terms of the total noise introduced. For matrix factorization, we sample Z ∼ N (0, ζ 2z2) for noise multiplier z; this noise is applied after mapping the raw gradients/updates x through the linear map C which is normalized so the total sensitivity is ζ. For the (S)GD mechanisms, we add noise N (0, ζ 2z2) independently to each model update. In all cases, noise is added to the sum of per-user updates, so the effective noise in the average update scales down with the number of clients per round.\n\ntarget ε\n\nMF DP-SGD (6e) GD (2052e)\n\nNoise multiplier z\n\n17.648 8.841 8.824 2.000\n\n0.341 0.600 -\n2.231\n\n0.402 -\n0.491 0.757\n\n15.518 -\n27.493 106.023\n\nTable 3: Noise multiplier parameters for the StackOverflow experiments to achieve various εs at δ = 10−6. Privacy was computed using the PLD accountant, see Appendix D.2.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nclients/round\n\nε\n\n1000\n\n342477\n\n2.00\n\n8.82\n\n17.65\n\n2.00\n\n8.82\n\n17.65\n\nηs\n\n0.25 0.50 1.00 0.25 0.50 1.00 0.25 0.50 1.00 0.50 1.00 0.50 1.00 0.50 1.00\n\nmean\n\n0.22654 0.22592 0.21869 0.23154 0.23447 0.23135 0.23290 0.23728 0.23571 0.24074 0.24056 0.24640 0.25279 0.24686 0.25370\n\nTest set accuracy std\n\nmin\n\nmax\n\ncount\n\n0.00009 0.00013\n\n0.00015 0.00033 0.00041 0.00002 0.00014 0.00019\n\n0.00097 0.00011 0.00019 0.00025 0.00039\n\n0.22648 0.22581 0.21869 0.23143 0.23419 0.23106 0.23289 0.23710 0.23558 0.24074 0.23886 0.24632 0.25261 0.24668 0.25312\n\n0.22660 0.22606 0.21869 0.23164 0.23495 0.23164 0.23291 0.23741 0.23585 0.24074 0.24125 0.24647 0.25306 0.24704 0.25394\n\n2 3\n1 2\n4 2\n2 4\n2 1\n5 2\n4 2\n4\n\nTable 4: Test set accuracy statistics for DP-SGDM,6e (1000 clients/round) and DP-GDM,2052e (342,477 clients/round). Accuracy for DP-GDM,2052e was estimated with 1000 clients/round with an appropriately scaled noise multiplier. The count columns gives the number of repeated trials of the given configuration, with ηs indicating the server learning rate.\n\nHonaker, 6e (Kairouz’21)\n\n2.00\n\nε\n\nMF1, 1e (Denisov’22)\n\nMF1, 6e (Ours)\n\nMF6, 6e (Ours)\n\n8.84\n\n17.65\n\n2.00\n\n8.84\n\n17.65\n\n2.00\n\n8.84\n\n17.65\n\n2.00\n\n8.84\n\n17.65\n\nηs\n\n0.25 0.50 1.00 0.50 1.00 0.50 1.00 0.25 0.50 0.25 0.50 1.00 0.25 0.50 1.00 0.25 0.50 1.00 0.50 1.00 0.50 1.00 0.25 0.50 1.00 0.50 1.00 0.50 1.00\n\nmean\n\n0.20951 0.19535 0.00011 0.23952 0.23799 0.24422 0.24675 0.19674 0.00093 0.23500 0.24105 0.23909 0.23576 0.24503 0.24522 0.22953 0.22324 0.00010 0.24516 0.24676 0.24628 0.25194 0.22978 0.23237 0.22413 0.24509 0.24897 0.24632 0.25265\n\nTest set accuracy std\n\nmin\n\nmax\n\ncount\n\n0.00134 0.00114\n\n0.00101 0.00212 0.00037 0.00128 0.00057 0.00108 0.00009 0.00083 0.00196 0.00019 0.00056 0.00102 0.00046 0.00017\n\n0.00044\n\n0.00085 0.00038 0.00078 0.00055 0.00010 0.00081 0.00015 0.00025\n\n0.20857 0.19429 0.00011 0.23881 0.23649 0.24383 0.24540 0.19633 0.00017 0.23493 0.24019 0.23767 0.23562 0.24408 0.24424 0.22921 0.22308 0.00010 0.24516 0.24628 0.24628 0.25124 0.22939 0.23147 0.22365 0.24497 0.24804 0.24618 0.25240\n\n0.21046 0.19655 0.00011 0.24023 0.23949 0.24456 0.24810 0.19715 0.00169 0.23506 0.24190 0.24132 0.23590 0.24565 0.24628 0.22985 0.22341 0.00010 0.24516 0.24714 0.24628 0.25288 0.23014 0.23286 0.22472 0.24515 0.24955 0.24648 0.25291\n\n2 3\n1 2\n2 3\n5 2\n2 2\n4 3\n2 6\n3 2\n3 1\n1 3\n1 3\n3 3\n3 3\n3 3\n3\n\nTable 5: Test set accuracy for matrix-factorization based mechanisms. Note: Some 6 epoch runs were conducted with shuffling between epochs, and some were conducted using a fixed order for all epochs (as required by our DP analysis). We saw no impact of reshuffling on the final test set accuracy, and so include all runs in these results regardless of the shuffling setting.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 13: A more detailed view of the matrices shown in Fig. 2.\n\nF.5 OPTIMAL MATRIX MECHANISMS\n\nG DISCUSSIONS AND CONCLUSIONS\n\nOur work significantly improves the privacy-utility tradeoffs in DP ML. Indeed, our work outperforms the state-of-the-art (and, DPSGD with amplification) by ≈ 5 percentage points across many privacy levels—and as low as ε ≈ 2—with practically implementable assumptions. We remark that we compare our mechanisms with DPSGD on a level-ground using well-performing but not state-of-the-art models and training protocols—e.g., very large models, augmentations prior to clipping (De et al., 2022), public data usage, and large batch sizes can all aid training. Many, if not all, of these techniques are applicable to our setting and can thus be used with our mechanisms to realize additional absolute performance gains, again, likely beyond the performances achieved by DPSGD.\n\nThe major limitation of our approach is the computation required to generate the optimal matrices. Though our optimal FFT decoder bridges the gap between the mechanisms without optimizer costs and our optimal mechanism, it still leaves some room for improvement in the privacy-utility tradeoff. We believe this is an important area for future work.\n\nH ANALYSIS FOR SECTION 2\n\nH.1 FROM SCALAR TO VECTOR CONTRIBUTIONS\n\nTheorem H.1. Let C ∈ Rn×k which satisfies\n\n(cid:107)Cu(cid:107)2 ≤ 1\n\n∀u ∈ {−1, 1}k,\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nand let G ∈ Rk×d such that each row G[i,:] for i ∈ [k] satisfies (cid:107)G[i,:](cid:107)2 ≤ 1. Suppose at least one of the following conditions hold:\n\n1. We have k = 1 or k = 2 participations.\n\n2. All the entries of C(cid:62)C are non-negative.\n\n3. The rows of G are all co-linear, G[i,:] = ui · G[1,:] for ui ∈ {−1, 1}, i > 1.\n\n4. The rows of G are all orthogonal, (cid:104)G[i,:], G[j,:](cid:105) = 0, ∀i (cid:54)= j, i, j ∈ [k].\n\nThen,\n\nFurthermore, the following statements are also true without assuming conditions (1)-(3) above.\n\n(cid:107)CG(cid:107)F ≤ 1.\n\n(10)\n\n• (cid:107)CG(cid:107)F = O (log k).\n\n• If we replace the condition on G to ∀i ∈ [k], (cid:107)G[i,:](cid:107)1 ≤ 1, then (cid:107)CG(cid:107)F ≤ 1.\n\nNote Theorem H.1 is generally applied to C[:,π], the sub-matrix of some C ∈ Rn×n formed by keeping only columns selected by a particular participation pattern π.\n\nProof Theorem H.1. Let C = [c1 c2 let we write g[i,j] for the (i, j)-th entry of G. It will also be useful to note F = tr(cid:0)CG(CG)(cid:62)(cid:1) = tr(cid:0)C(cid:62)CGG(cid:62)(cid:1).\n\n(cid:107)CG(cid:107)2\n\n· · · ck] with each ci ∈ Rn being a column vector. Also\n\n(11)\n\nIn the following we prove each of the individual cases of Theorem H.1.\n\nWhen k = 1 or k = 2: For k = 1, we have\n\n(cid:107)CG(cid:107)2\n\nF = (cid:107)c1(cid:107)2\n\n2\n\n\n\n\n\nd (cid:88)\n\nj=1\n\n 2\n\ng[1,j]\n\n\n\n≤ (cid:107)c1(cid:107)2\n\n2 = max\n\nu∈{±1}\n\n(cid:107)u · c1(cid:107)2 2 .\n\nAn equivalent argument is used in Denisov et al. (2022, Thm. 3.1).\n\nFor k = 2, we have the following:\n\n(cid:107)CG(cid:107)2\n\nF =\n\n≤\n\n≤\n\n2 (cid:88)\n\ni=1\n\n2 (cid:88)\n\ni=1\n\n2 (cid:88)\n\ni=1\n\n\n\n\n\nd (cid:88)\n\n  + (cid:0)2g[1,1]g[2,1](cid:104)c1, c2(cid:105)(cid:1) + · · · + (cid:0)2g[1,d]g[2,d](cid:104)c1, c2(cid:105)(cid:1)\n\ng2\n\n[i,j]\n\n(cid:107)ci(cid:107)2\n\n2 ·\n\nj=1\n\n(cid:107)ci(cid:107)2\n\n2 + (cid:0)2|g[1,1]| · |g[2,1]||(cid:104)c1, c2(cid:105)|(cid:1) + · · · + (cid:0)2|g[1,d]| · |g[2,d]||(cid:104)c1, c2(cid:105)|(cid:1)\n\n(cid:16)\n\n(cid:107)ci(cid:107)2\n\n2 +\n\n[1,1] + g2 g2\n\n[2,1]\n\n(cid:17)\n\n|(cid:104)c1, c2(cid:105)| + · · · +\n\n(cid:16)\n\n[1,d] + g2 g2\n\n[2,d]\n\n(cid:17)\n\n|(cid:104)c1, c2(cid:105)|\n\n(12)\n\n= (cid:107)c1(cid:107)2\n\n≤ max\n\nu∈{±1}2\n\n2 + (cid:107)c2(cid:107)2 (cid:107)Cu(cid:107)2\n\n2 ≤ 1,\n\n2 + 2|(cid:104)c1, c2(cid:105)| = max\n\n(cid:110)\n\n(c1 + c2)2 , (c1 − c2)2(cid:111)\n\nwhere Eq. (12) follows from the standard A.M. ≥ G.M. inequality.\n\nAll the entries of C(cid:62)C are non-negative: Let X = C(cid:62)C and ˆG = GG(cid:62). Observe ˆG[i,j] ∈ [−1, 1], and using Eq. (11), when X is elementwise non-negative, tr(cid:0)X ˆG(cid:1) is maximized when ˆG = 1k×k = (cid:98)u(cid:98)u(cid:62) by choosing (cid:98)u = 1k. Hence,\n\n(cid:107)CG(cid:107)F ≤ tr(cid:0)C(cid:62)C(cid:98)u(cid:98)u(cid:62)(cid:1) = (cid:107)C(cid:98)u(cid:107)2 ≤ max\n\nu∈{±1}k\n\n(cid:107)Cu(cid:107)2 2 .\n\n(13)\n\nEq. (13) completes the proof.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nThe rows of G are co-linear: By the convexity of (cid:107)CG(cid:107)2 F with respect to the matrix G, we may assume the rows of G are of (cid:96)2 norm 1. Under the colinearity assumption, this translates to G[i,:] = uiG[1,:], with each ui ∈ {±1}. Let u = [u1, . . . , uk] ∈ {−1, 1}k. Then for the matrix GG(cid:62) we have the following:\n\n[GG(cid:62)][i,j] = (cid:104)G[i,:], G[j,:](cid:105) = uiuj(cid:104)G[1,:], G[1,:](cid:105) = uiuj.\n\nwhich implies\n\nGG(cid:62) = uu(cid:62).\n\n(14)\n\nUsing Eq. (14) with Eq. (11), we have (cid:107)CG(cid:107)2\n\nF = tr(cid:0)CG(CG)(cid:62)(cid:1) = tr(cid:0)C(cid:62)Cuu(cid:62)) ≤ max\n\n(cid:107)Cu(cid:107)2\n\n2 ≤ 1.\n\nu∈{±1}k\n\nThe rows of G are all orthogonal This condition implies ˆG = GG(cid:62) is a diagonal matrix with diagonal entries in [0, 1], and so Eq. (11) implies (cid:107)CG(cid:107)2\n\nF ≤ tr(X). It is thus sufficient to show\n\nWe give a construction for a u that shows this. Observe\n\ntr(X) ≤ max\n\nu∈{−1,1}k\n\ntr(Xuu(cid:62)).\n\ntr(Xuu(cid:62)) = tr(X) + 2\n\nk (cid:88)\n\ni=1\n\nui\n\ni−1 (cid:88)\n\nj=1 (cid:124)\n\nujX[i,j]\n\n.\n\n(cid:123)(cid:122) bi\n\n(cid:125)\n\nObserve we can choose u1 = 1 and then ui = sign(bi) since bi depends only on C and the previously fixed uj for j < i, ensuring the double sum on the right is non-negative, completing the proof.\n\n(cid:107)CG(cid:107)F = O(log k) without assuming conditions (1)-(4): We will prove this claim via probabilistic argument. First notice that due to convexity, we have the following:\n\nmax u∈{±1}k\n\n(cid:107)Cu(cid:107)2\n\n2 ≤ 1\n\n⇒ ∀x ∈ Rk, (cid:107)Cx(cid:107)2\n\n2 ≤ (cid:107)x(cid:107)2\n\n∞ .\n\n(15)\n\nWe now observe the following for Normal distributions:\n\n(cid:107)CG(cid:107)2\n\nF = Ez∼N (0,1)d\n\n≤ Ez∼N (0,1)d\n\n(cid:104)\n\n(cid:104)\n\n(cid:105)\n\n(cid:107)CGz(cid:107)2\n\n2\n\n(cid:105)\n\n(cid:107)Gz(cid:107)2\n\n∞\n\n= O\n\n(cid:18)\n\n(cid:13) (cid:13)G[i,:]\n\n(cid:13) 2\n(cid:13)\n\n2 · log k\n\nmax i∈[k]\n\n(cid:19)\n\n= O (log k) .\n\n(16)\n\nThe first inequality in Eq. (16) follows from Eq. (15), and the first equality in Eq. (16) follows from expectation of the maximum of Gaussian random variables.\n\nReplacing the condition on G to ∀i ∈ [k], (cid:107)G[i,:](cid:107)1 ≤ 1, then (cid:107)CG(cid:107)F ≤ 1: First notice that since (cid:107)CG(cid:107)2 F is a convex function, the maximum happens at the extreme points of the constraint set G = {G | ∀i ∈ [k], (cid:107)G[i,:](cid:107)1 = 1}. We use Claim H.1 to identify the extreme points of G.\n\nClaim H.1 (Theorem 1 in Cao et al. (2022)). The set of extreme points of the set of k × d rowstochastic matrices are precisely the set of row permutation matrices, i.e., set of the matrices with entries in {0, 1}k×d and each row has exactly one non-zero entry.\n\nNotice that the constraint on any matrix G ∈ G is oblivious to the sign, meaning, we can flip the sign of any set of entries in G and the new matrix will still be in G. This along with Claim H.1 immediately implies that the set H = {H ∈ {−1, 0, 1}k×d : ∀i ∈ [k], (cid:107)H[i,:](cid:107)0 = (cid:13) (cid:13) (cid:13)∞ = 1} is the set of extreme points of the set G. (If the set of extreme points of G is larger than H, then the signs of any such extreme point can be flipped to create a new extreme point of row-stochastic matrices, which would violate Claim H.1.)\n\n(cid:13)H[i,:]\n\nIt is not hard to observe that for any H ∈ H, there exists an uH{±1}k s.t. (cid:107)CH(cid:107)F = (cid:107)CuH(cid:107)2. (cid:107)CG(cid:107)F is reached Since, (cid:107)CuH(cid:107)2 ≤ max\n\n(cid:107)Cu(cid:107)2 for any choice of uH, and the fact that max\n\nG\n\nu∈{±1}k\n\nat one of the matrices in H, the claim in Theorem H.1 follows.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nH.2 A COUNTEREXAMPLE FOR GENERAL C\n\nTheorem H.1 indicates the possibility of the following conjecture being true, because of it being true (cid:107)Cu(cid:107)2 ≤ 1, then (cid:107)CG(cid:107)F ≤ 1 for all G with row (cid:96)2-norm in so many special cases: If max\n\nu∈{±1}k\n\nat most one. Unfortunately, we show that the conjecture is not true when k > 2, as shown by the following counterexample with n = k = 3 and d = 2:\n\nC =\n\n1 √\n\n24\n\n(cid:35)\n\n(cid:34)2 1\n1 −1\n\n1 1\n2 −1 2\n\nand G =\n\n1 √\n\n5\n\nDirect calculation shows maxu∈{±1}k (cid:107)Cu(cid:107)2 = 1, but (cid:107)CG(cid:107)F =\n\nH.3 PROOF OF COROLLARY 2.1\n\n(cid:35)\n\n(cid:34)2\n\n1 2 −1 1\n2 √\n\n1.1 ≈ 1.049.\n\nCorollary 2.1 Restated. When per-step contributions are bounded by ζ = 1, for any participation pattern Π and dimensionality d ≥ 1, when C(cid:62)C ≥ 0 elementwise, we have\n\nsensDd\n\nΠ\n\n(C) = sensD1\n\nΠ\n\n(C).\n\nProof. To see sensDd can construct a G such that (cid:107)CG(cid:107)F = (cid:107)Cu(cid:63)(cid:107)2 by taking rows G[i,:] = (u(cid:63) i ∈ [n].\n\n(C), suppose u(cid:63) = arg maxu∈D1\n\n(C) ≥ sensD1\n\nΠ\n\nΠ\n\nΠ\n\n(cid:107)Cu(cid:107)2, and observe we i , 0, . . . , 0) ∈ Rd for\n\nFor the other direction, for each π ∈ Π, we apply Theorem H.1 to the matrix Cπ = C[:,π], and observe C(cid:62)C ≥ 0 is sufficient to imply C(cid:62)\n\nπ Cπ ≥ 0.\n\nNote The condition C(cid:62)C ≥ 0 is sufficient but not in fact necessary for Corollary 2.1 to hold. In π Cπ for π ∈ Π “touch” only k2b entries of particular, for (k, b)-participation Π, the sub-matrices C(cid:62) the n2 = k2b2 entries of C(cid:62)C; the other entries of C(cid:62)C could in fact be negative. However, we did not need to use this observation for any of the matrices in our experiments.\n\nH.4 PROOF OF THEOREM 2.1\n\nTheorem 2.1 Restated. Let C ∈ Rn×n, and take some participation pattern Π, with k = maxπ∈Π |π| the maximum number of participations. With C[:,π] representing to selecting the (cid:13) columns of the matrix C indexed by π and (cid:107)·(cid:107)2 the spectral matrix norm, let λ = max (cid:13)2. Then\n\n(cid:13) (cid:13)C[:,π]\n\nπ∈Π\n\nsensD1\n\nΠ\n\n(C) ≤ λ\n\nk.\n\n√\n\nProof. By assumption we have (cid:107)u(cid:107) ≤\n\n√\n\nk, and so\n\nmax u∈D\n\n(cid:107)Cu(cid:107)2 ≤ max\n\nπ∈Π\n\nmax u∈D\n\nI ANALYSIS FOR SECTION 3\n\nI.1 PROOF OF THEOREM 3.1\n\n(cid:107)C[:, π](cid:107)2 (cid:107)u(cid:107)2 ≤ λ\n\nk.\n\n√\n\nTheorem 3.1 Restated. Let a finite D = {ui}k span Rn. Assume that A is full-rank, and for v ∈ Rk define\n\ni=1 be given, and assume that the vectors {ui}k\n\ni=1\n\nHv = [u1, . . . , uk] diag(v)1/2, U = HvH(cid:62) v . Then, for Lagrange multipliers v such that the U is full-rank, the Lagrange dual function g can be expressed in closed form in terms of the Lagrange multipliers:\n\ng(v) := inf\n\nX is PD\n\nL(X, v) = 2 tr\n\n(cid:16)(cid:0)U\n\n1\n\n2 A(cid:62)AU\n\n30\n\n2 (cid:17)\n\n1\n\n2 (cid:1) 1\n\n(cid:88)\n\n−\n\nvu\n\nu∈D\n\n(17)\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Since there is some finite set of vectors u ∈ Rn specifying D, the supremum in Eq. (5) may be reduced to a maximum over these elements.\n\nOur problem then takes the form:\n\ninf X is PD\n\ntr(A(cid:62)AX−1)\n\ns.t. u(cid:62)Xu ≤ 1, ∀u ∈ D.\n\nRecall that we have defined Hv = [u1, . . . , uk] diag(v) 1\n\n2 , and U = HvH(cid:62)\n\nv . Now, note:\n\nU =\n\n(cid:88)\n\nu\n\nvuuu(cid:62) = H diag(v)H(cid:62) = HvH(cid:62) v .\n\n(18)\n\n(19)\n\nIntroducing Lagrange multipliers vu ≥ 0, for the problem Eq. (18) we form the Lagrangian for positive-definite X:\n\nL(X, v) = tr(A(cid:62)AX−1) +\n\n(cid:0)u(cid:62)Xu − 1(cid:1)\n\nvu\n\n(cid:88)\n\nu\n\n= tr(A(cid:62)AX−1) + tr\n\n(cid:32)\n\n(cid:16) (cid:88)\n\n(cid:33)\n\nvuuu(cid:62)(cid:17)\n\nX\n\nu\n\n(cid:88)\n\n−\n\nvu\n\nu\n\n= tr(A(cid:62)AX−1) + tr (UX) −\n\n(cid:88)\n\nvu.\n\nu\n\n(20)\n\n(21)\n\n(22)\n\nFor fixed v, any finite minimizer of L for positive-definite X must correspond to a zero of this Lagrangian’s gradient. We then compute the gradient\n\n∂L ∂X\n\n= −X−1A(cid:62)AX−1 + U.\n\n(23)\n\nU and A are full-rank by assumption; therefore Lemma I.2 is applicable, and Eq. (23) has a unique positive-definite zero (and indeed, the infimum in Eq. (18) becomes a minimum):\n\nX = U− 1\n\n2 (cid:0)U\n\n1\n\n2 A(cid:62)AU\n\n1\n\n2 (cid:1) 1\n\n2 U− 1 2 .\n\n(24)\n\nNote that Eq. (23) also immediately implies that if U is not full-rank, then there is no finite positivedefinite minimizer of L in X. Letting g(v) = minX L(X, v) be the Lagrange dual function and plugging back into Eq. (22), we have\n\ng(v) = min Xis PD\n\ntr(A(cid:62)AX−1) + tr (UX) −\n\n(cid:88)\n\nvu\n\nu\n\n= min Xis PD\n\ntr(XU) + tr (UX) −\n\n(cid:88)\n\nvu\n\nu\n\nusing Eq. (23)\n\n= min Xis PD\n\n2 tr (UX) −\n\n(cid:88)\n\nvu\n\nu\n\n(cid:16)\n\n= 2 tr\n\nUU− 1\n\n2 (cid:0)U\n\n1\n\n2 A(cid:62)AU\n\n1\n\n2 (cid:1) 1\n\n2 U− 1\n\n2\n\n(cid:17)\n\n−\n\n(cid:88)\n\nvu\n\nu\n\nusing Eq. (24)\n\n= 2 tr\n\n(cid:16)(cid:0)U\n\n1\n\n2 A(cid:62)AU\n\nI.1.1 PROOF OF COROLLARY 3.1\n\n2 (cid:17)\n\n1\n\n2 (cid:1) 1\n\n(cid:88)\n\n−\n\nvu.\n\nu\n\nCorollary 3.1 Restated. In the same setup as Theorem 3.1, a maximizer of the dual v(cid:63) must satisfy: (cid:16)(cid:0)H(cid:62)\n\nv(cid:63) = diagpart\n\n2 (cid:17)\n\n(cid:1) 1\n\n(25)\n\nv(cid:63) A(cid:62)AHv(cid:63)\n\n.\n\nMoreover, the optimal value of the problem defined in 6 is tr (v(cid:63)).\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nProof. As noted in the remark after Theorem 3.1, any optimal setting of the dual variables must be in the interior of a neighborhood in which the representation Eq. (7) is valid. It is therefore permissible to differentiate this representation.\n\nDifferentiating, we find:\n\n(cid:16)(cid:0)U\n\ntr\n\n1\n\n2 A(cid:62)AU\n\n∂ ∂vi\n\n2 (cid:17)\n\n1\n\n2 (cid:1) 1\n\n=\n\n=\n\n=\n\n=\n\n=\n\n1 2\n1 2\n1 2\n1 2\n1 2\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\ntr\n\ntr\n\ntr\n\ntr\n\nA(cid:62)AU\n\n1\n\n2 (U\n\n1\n\n2 A(cid:62)AU\n\n1\n\n2 )− 1\n\n2 U− 1\n\n2 uiu(cid:62)\n\ni\n\n(cid:17)\n\nU− 1\n\n2 (U\n\n1\n\n2 A(cid:62)AU\n\n1\n\n2 )(U\n\n1\n\n2 A(cid:62)AU\n\n1\n\n2 )− 1\n\n2 U− 1\n\n2 uiu(cid:62)\n\ni\n\n(cid:17)\n\nU− 1\n\n2 (U\n\n1\n\n2 A(cid:62)AU\n\n1\n\n2 )\n\n1\n\n2 U− 1\n\n2 uiu(cid:62)\n\ni\n\nu(cid:62)\n\ni U− 1\n\n2 (U\n\n1\n\n2 A(cid:62)AU\n\n1\n\n2 )\n\n1\n\n2 U− 1\n\n2 ui\n\n(cid:17)\n\n(cid:17)\n\nu(cid:62)\n\ni Xui,\n\nby defining X = U− 1\n\n2 (cid:0)U 1\n\n2 A(cid:62)AU 1\n\n2 (cid:1) 1\n\n2 U− 1\n\n2 (recalling the usage of the symbol X in Eq. (24)).\n\nTherefore\n\n∂g(v) ∂vi\n\n= u(cid:62)\n\ni Xui − 1,\n\nand we have the stated expression for the gradient of the dual function.\n\nNow, at a maximizer of the dual function, this derivative must vanish. An equivalent condition is diagpart(H(cid:62)XH) = (cid:126)1, and hence\n\ntr(UX) = tr(H diag(v)H(cid:62)X) = tr(diag(v)H(cid:62)XH) =\n\n(cid:88)\n\nvu,\n\nu\n\n(26)\n\nso at the optimum v(cid:63) in fact g(v(cid:63)) = (cid:80)\n\nu v(cid:63)\n\nu, establishing the second claim of our result.\n\nAgain using the observation that diagpart(H(cid:62)\n\nv XHv) = (cid:126)1 and so v XHv) = diagpart(diag(v)H(cid:62)XH) = v.\n\ndiagpart(H(cid:62)\n\nFurther, using the second claim of Corollary I.1, we can take\n\nX = H−(cid:62)\n\nv\n\n(cid:0)H(cid:62)\n\nv A(cid:62)AHv\n\n(cid:1) 1\n\n2 H−1 v ,\n\nand multiplying this by H(cid:62)\n\nv and Hv on the left and right respectively yields\n\nH(cid:62)\n\nv XHv = H(cid:62)\n\nv H−(cid:62)\n\nv\n\n(cid:0)H(cid:62)\n\nv A(cid:62)AHv\n\n(cid:1) 1\n\n2 H−1\n\nv Hv = (cid:0)H(cid:62)\n\nv VHv\n\n(cid:1) 1\n\n2\n\nand so we conclude for the optimal Lagrange multiplier v(cid:63),\n\nv(cid:63) = diagpart\n\n(cid:16)(cid:0)H(cid:62)\n\nv(cid:63) A(cid:62)AHv(cid:63)\n\n2 (cid:17)\n\n(cid:1) 1\n\n.\n\n(27)\n\nI.2 LEMMAS AND COROLLARIES\n\nLemma I.1. The set of positive-definite X such that supu∈D u(cid:62)Xu ≤ 1 is bounded as a subset of Rn×n if and only if D = {u} spans Rn.\n\nProof. Suppose that D spans Rn. For a PSD matrix, a bound on the trace implies a bound on the elements; therefore it is sufficient to show that supu∈D u(cid:62)Xu ≤ 1 implies that the maximum eigenvalue of X is uniformly bounded for X PSD.\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nTake some set of vectors {ui}n\n\ni=1 ∈ D which span Rn. Fix some representation\n\nwhere ei is the ith standard basis vector.\n\nTake y of (cid:96)2 norm 1, and express:\n\nei =\n\nn (cid:88)\n\nj=1\n\nαijuj,\n\ny =\n\nn (cid:88)\n\ni=1\n\nγiei.\n\nThen for X satisfying our assumptions,\n\n(cid:12)y(cid:62)Xy(cid:12) (cid:12)\n\n(cid:12) =\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\ni,j=1\n\nγiγje(cid:62)\n\ni Xej\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n=≤ n2 max\n\n1≤i,j≤n\n\n|γiγj| (cid:12)\n\n(cid:12)e(cid:62)\n\ni Xej\n\n(cid:12) (cid:12) .\n\n(28)\n\nSimilarly,\n\n(cid:12) (cid:12)e(cid:62)\n\ni Xej\n\n(cid:12) (cid:12) =\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\nk,l=1\n\nαikαjlu(cid:62)\n\nk Xul\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ n2 max\n\n1≤k,l≤n\n\n|αikαjl| (cid:12)\n\n(cid:12)u(cid:62)\n\nk Xul\n\n(cid:12) (cid:12) ≤ n2 max\n\n1≤k,l≤n\n\n|αikαjl|,\n\n(29)\n\nwhere the final inequality follows by the assumptions on X.\n\nNow, since the (cid:96)2 norm of y is 1, the orthogonality of the ei imply that each |γiγj| is at most 1. Therefore:\n\n(cid:12)y(cid:62)Xy(cid:12) (cid:12)\n\n(cid:12) ≤ n4 max\n\n1≤i,j,k,l≤n\n\n|αikαjl|,\n\n(30)\n\nand we have sufficiency of D spanning Rn.\n\nFor necessity, suppose D does not span Rn. Then there is some vector y ∈ span (D)⊥ of norm 1. Take any X such that supu∈D u(cid:62)Xu ≤ 1. Then, since y(cid:62)u = 0 for all u ∈ D, Y := X + αy satisfies the same set of inequalities for any α.\n\nLemma I.2. Let U, V ∈ Sn PSD, and the following equation defines a positive-definite matrix X:\n\n++. Let U = ULUR be a factorization of U such that URVUL is\n\nX = U†\n\nR\n\n(cid:0)URVUL\n\n(cid:1) 1\n\n2 U†\n\nL.\n\n(31)\n\nThen, this X solves the equation\n\nXUX = V or equivalently U = X−1VX−1.\n\nMoreover, this positive-definite solution X is unique.\n\nProof. We will begin by showing that X as defined by Eq. (31) solves the equation XUX = V; then we will show that any two positive-definite representations of the form Eq. (31) are in fact identical.\n\nNotice that the representation U = ULUR implies that rank(UL) ≥ n and rank(UR) ≥ n. Therefore ULU† RUR, as implied by the Moore definition of the Moore-Penrose pseudoinverse. So:\n\nL = I = U†\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nXUX = XULURX (cid:16)\n\n=\n\nU†\n\nR\n\n(cid:0)URVUL\n\n(cid:1) 1\n\n2 U†\n\nL\n\n(cid:17)\n\nULUR\n\n(cid:16)\n\nU†\n\nR\n\n(cid:0)URVUL\n\n(cid:1) 1\n\n2 U†\n\nL\n\n(cid:17)\n\n= U†\n\nR\n\n(cid:0)URVUL\n\n(cid:1) 1\n\n2 PR(U†\n\nL)PR(UR)\n\n(cid:0)URVUL\n\n(cid:1) 1\n\n2 U†\n\nL\n\n(cid:1) 1\n\n(cid:1) 1 We claim that (cid:0)URVUL L) = PR(UR) 2 . this can be seen by multiplying on the left or right as appropriate by (cid:0)URVUL PR(UR)UR = UR, ULPR(U† equality follows by uniqueness of the symmetric matrix square root. Therefore:\n\n2 , and noting L) = UL. Since all the terms here are symmetric, the appropriate\n\nIn both cases, (cid:1) 1\n\n(cid:1) 1 2 = (cid:0)URVUL\n\n(cid:0)URVUL\n\n2 PR(U†\n\nXUX = U† = V.\n\nRURVULU†\n\nL\n\nThe uniqueness of a positive-definite X solving XUX = V follows from the uniqueness of the usual matrix square root. Indeed, assume Y positive-definite satisfies YUY = V. Then:\n\n(cid:16)\n\nU1/2YU1/2(cid:17)2\n\n= U1/2VU1/2\n\nSince the positive-definite square root is uniquely determined, U1/2YU1/2 is uniquely determined. Since U is invertible, Y is uniquely determined as well, and we have Y = X.\n\nCorollary I.1. Two particular instantiations of Lemma I.2 are of interest. X as the matrix geometric mean of U−1 and V (taking UL = UR =\n\nU):\n\n√\n\nand assuming the representation U = HvH(cid:62) v :\n\nX = U− 1\n\n2 (cid:0)U\n\n1\n\n2 VU\n\n1\n\n2 (cid:1) 1\n\n2 U− 1 2 ,\n\nX = H†(cid:62)\n\nv\n\n(cid:0)H(cid:62)\n\nv VHv\n\n(cid:1) 1\n\n2 H† v.\n\n(32)\n\n(33)\n\nProof. By positive-definiteness of U and V, Eq. (32) is clearly positive definite; Eq. (33) may be seen to be positive definite via the SVD of the pseudoinverses involved. Symmetry is again clear. Therefore both representations satisfy the assumptions of Lemma I.2.\n\nJ ANALYSIS FOR SECTION 4\n\nJ.1 ADDITIONAL DETAILS\n\nDefining the circulant matrix We consider the special case where A is the prefix sum linear query matrix (lower-triangle matrix of ones). Then, we define the corresponding circulant matrix\n\nAcirc (cid:44)\n\n\n\n \n\n\nv0 v1 ... v2n−1\n\nv2n−1 v0 ... v2n−2\n\n· · · · · ·\n\n· · · · · ·\n\n\n\n \n\n\nv1 v2 ... v0\n\nwhere\n\nv (cid:44) [1, . . . , 1 (cid:124) (cid:123)(cid:122) (cid:125) n\n\n].\n\n, 0, . . . , 0 (cid:124) (cid:123)(cid:122) (cid:125) n\n\n(34)\n\nIt is straightforward to verify Acirc[:n,:n] = A.\n\nDefining the DFT\n\n∀k ∈ [2n − 1] : vDFT[k] =\n\n2n−1 (cid:88)\n\na=0\n\nv(a) exp\n\n(cid:19)\n\n(cid:18) −j2πka 2n\n\n(35)\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nCirculant matrices expressed using Fourier Transforms Theorem J.1 (Adapted from Gray (2006)). Consider any circulant matrix Acirc ∈ R2n×2n. Let F ∈ (cid:105) C2n×2n, where the k-th row of F is given by F[k, :] = 1√ .\nΣF, where Σ ∈ C2n×2n is a diagonal matrix with the diagonal being the DFT\n\nThen, Acirc = F (defined in Equation 35) of the first column of Acirc. Here, ∗ is the Hermimitian operation.\n\n: a ∈ {0, . . . , 2n − 1}\n\n− j2πka\n\nexp\n\n2n\n\n2n\n\n(cid:16)\n\n(cid:17)\n\n(cid:104)\n\n∗\n\nPrivacy and utility guarantees utility guarantee for the FFT mechanism defined in Algorithm 1.\n\nIn the following we provide the privacy guarantee and the main\n\nTheorem J.2 (DP-Prefix Sum via FFT Privacy Guarantee). Algorithm 1 is ρ-zCDP in the adaptive continuous release model.\n\nNext, we analyze the utility of Algorithm 1 and show that it is nearly optimal in terms of the mean squared error (MSE) in the single-pass setting. First, we express the MSE in Theorem J.3 below.\n\nTheorem J.3 (DP-Prefix Sum via DFT Utility). The MSE achieved by Algorithm 1 using the real and imaginary components of (cid:101)z is\n\nE [MSE] =\n\nκ2 (cid:13)\n\n(cid:13)vDFT(cid:13) 2\n(cid:13) 1\n2ρn2\n\n.\n\nIn the following, we will have an explict expression for (cid:13) Finally, we will argue that Theorem J.3 is nearly optimal.\n\n(cid:13)vDFT(cid:13)\n\n(cid:13)1 in terms of the problem parameters.\n\nCorollary J.1. The expected mean squared error (MSE) is given by the following:\n\nE [MSE] =\n\n\n\nn +\n\nκ2 2ρn2\n\n(cid:99)\n\n(cid:98) 2n−1 (cid:88)\n\n2\n\na=0\n\nsin\n\n1 (cid:16) π(2a+1) 2n\n\n2\n\n\n\n\n\n(cid:17)\n\n.\n\n(cid:16)\n\nNear-optimal utility Here, we show that Theorem J.3 is near-optimal in utility for the single-participation setting. To do this, we compare with a lower bound on the expected MSE of any factorization-based mechanism from Henzinger et al. (2022, Theorem 2):\n\n2 + ln (cid:0) 2n+1\n\n(cid:1) + ln(2n+1)\n\n3\n\n1 2ρπ2 lary J.1 is ≈ 6x worse than the lower bound, the empirical noise added in Algorithm 1 closely tracks the lower bound to within a factor of 1.2x—because it only adds the real part of the noise. Results are in Figure 14 of Appendix J.1.\n\n. We find that the though our analytical upper bound in Corol-\n\n2n\n\n(cid:17)2\n\nShowing near-optimal utility via MSE experiments\n\nFigure 14: Algorithm 1 achieves near-optimal utility as measured by the analytic lower bound from Henzinger et al. (2022, Theorem 2).\n\nJ.2 PROOF OF THEOREM J.2\n\nTheorem J.2 Restated. Algorithm 1 is ρ-zCDP in the adaptive continuous release model.\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nProof. First, consider the non-adaptive setting and the following mechanism, with parameters as defined in Algorithm 1,\n\n(cid:115)\n\n(cid:2)Σ (cid:0)Fxext + Σ−1z(cid:1)(cid:3) ,\n\nκ2 (cid:107)vDFT(cid:107)1 4nρ\n\n(cid:16)√\n\n(cid:17)\n\nΣ · w\n\nwhere z =\n\n(36)\n\n-zCDP. To see this, we proceed by bounding ρi for each coorWe claim that this satisfies dinate i ∈ [2n] defined in Equation 36. For brevity, let b = Fxext. Consider two neighboring data sets g and g(cid:48), correspondingly, (b, xext) and (b(cid:48), x(cid:48)\n\next). Then,\n\nκ2(cid:107)vDFT(cid:107)1 4nσ2\n\n(cid:107)b − b(cid:48)(cid:107)∞ = (cid:107)F(xext − x(cid:48)\n\next)(cid:107)∞ =\n\nκ\n\n√\n\n2n\n\n.\n\n(37)\n\nWe will now prove zCDP guarantee independently for each of the 2n coordinates and then use standard zCDP composition (Bun & Steinke, 2016). For any coordinate a ∈ {0, . . . , 2n − 1},\n\nadding noise\n\n(cid:18)\n\n(cid:19)\n\nσ√\n\n|vDFT[i]|\n\n· Ncomplex (0, 1) to b[i] satisfies ρi-zCDP with ρi = κ2|vDFT[i]|\n\n4nσ2\n\n. Then by\n\ncomposition, we have that\n\nρ =\n\n2n−1 (cid:88)\n\na=0\n\n(ρi) =\n\nκ2 4nσ2\n\n2n−1 (cid:88)\n\na=0\n\n(cid:12)vDFT[i](cid:12) (cid:0)(cid:12) (cid:12)\n\n(cid:1) =\n\nκ2 (cid:13)\n\n(cid:13)vDFT(cid:13) (cid:13)1 4nσ2\n\n.\n\n(38)\n\nTherefore, setting σ2 = adaptive part using the same σ. We have the following from Equation 36.\n\n-satisfies a non-adaptive ρ-zCDP. Using the same σ, we prove the\n\n4nρ\n\nκ2(cid:107)vDFT(cid:107)1\n\n[F∗ (ΣFxext + z)] =\n\n(cid:20)\n\n√\n\nΣ\n\nF∗\n\n(cid:18)√\n\nΣFxext +\n\n(cid:19)(cid:21)\n\nz\n\n=\n\n F∗\n\n√\n\n\n\nΣ\n\n\n\n√\n\nΣFxext +\n\n(cid:115)\n\nκ2 (cid:107)vDFT(cid:107)1 4nρ\n\n\n\n\n\n· w\n\n\n\n\n\n1 √\n\nΣ\n\n(39) Since, w in Equation 39 is spherical Gaussian, and the original query matrix A is lower triangular, by Theorem 2.1 in Denisov et al. (2022), the adaptive privacy guarantee follows.\n\nJ.3 PROOF OF THEOREM J.3\n\nTheorem J.3 Restated. The MSE achieved by Algorithm 1 using the real and imaginary components of (cid:101)z is\n\nE [MSE] =\n\nκ2 (cid:13)\n\n(cid:13)vDFT(cid:13) 2\n(cid:13) 1\n2ρn2\n\n.\n\nProof. The MSE is given by the following:\n\nE[MSE] =\n\nE\n\n(cid:104) (cid:107)(cid:101)z[0, . . . , n − 1](cid:107)2\n\n2\n\n(cid:105)\n\n=\n\n1 n\n\nκ2 (cid:13)\n\n(cid:13)vDFT(cid:13) (cid:13)1 2n2ρ\n\n· tr (|Σ[: n, : n]|) =\n\nκ2 (cid:13)\n\n(cid:13)vDFT(cid:13) 2\n(cid:13) 1\n2n2ρ\n\n.\n\n(40)\n\nIn equation 40, Σ[: n, : n] refers to the top-left n × n submatrix of Σ.\n\nJ.4 PROOF OF COROLLARY J.1\n\nCorollary J.1 Restated. Under the same setting as Theorem J.3, the MSE for Algorithm 1 is the following\n\nE [MSE] =\n\n\n\nn +\n\nκ2 2ρn2\n\n(cid:99)\n\n(cid:98) 2n−1 (cid:88)\n\n2\n\na=0\n\nsin\n\n1 (cid:16) π(2a+1) 2n\n\n2\n\n\n\n\n\n(cid:17)\n\n.\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Recall the definition of DFT from Equation 35 and of v in Equation 34. It is immediate that vDFT[0] = n. For any k (cid:54)= 0, we have,\n\n(cid:16) −j2πkn\n\n(cid:17)\n\n1 − exp\n\nvDFT[k] =\n\n1 − exp\n\n2n\n\n(cid:16) −j2πk\n\n(cid:17) =\n\n2n\n\n1 − exp (−jπk) (cid:17) .\n\n(cid:16) −jπk\n\n1 − exp\n\nn\n\nFrom Equation 41, we have that when k > 0 is even, vDFT[k] = 0. For k odd, we have\n\n(cid:12)vDFT[k](cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:13)vDFT(cid:13) Combining these, the term (cid:13)\n\n(cid:12) =\n\n1 − exp\n\n(cid:13)1 is\n\n2 (cid:16) −jπk\n\nn\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:17)\n\n=\n\n1 |sin(πk/(2n))|\n\n=\n\n1 sin(πk/(2n))\n\n(cid:13)vDFT(cid:13) (cid:13)\n\n(cid:13)1 = n +\n\n2 (cid:99)\n\n(cid:98) n−1 (cid:88)\n\na=0\n\n1 sin(π(2a + 1)/(2n))\n\n(41)\n\n(42)\n\n(43)\n\nJ.5 PROOF OF THEOREM 4.1\n\nTheorem 4.1 Restated. Under k participation, Algorithm 1 satisfies (k2ρ)-zCDP.\n\nProof. The proof goes exactly as Theorem J.2, except equation 37 gets replaced by the following:\n\n(cid:107)b − b(cid:48)(cid:107)∞ = (cid:107)F(xext − x(cid:48)\n\next)(cid:107)∞ =\n\nK TWO RELATED FFT MECHANISMS.\n\nkκ\n\n√\n\n2n\n\n.\n\n(44)\n\nThe FFT mechanism presented in Section 4 can be understood as an application of a complex-valued matrix mechanism factorizing the prefix-sum matrix as √\n\nB = PF∗\n\nΣ,\n\n√\n\nC =\n\nΣFE,\n\nwhere E and P are appropriate embedding and projection matrices, respectively embedding an ndimensional vector in the first n components of R2n, and projecting those same first n components back to Rn, and following this application by ‘chopping off’ the imaginary part of the noise. The entries of Σ may be computed exactly; they contain no purely negative entries, so specifying the principal branch of the square root resolves the implicit ambiguity in the formulation above. This branch corresponds as well to the implementation of the complex square root in major software frameworks (e.g., NumPy).\n\nAll these operations are linear; and since everything begins and ends in the real domain, this mechanism can be expressed as a real-valued mechanism. Therefore identical codepaths can be used for implementing experiments with the FFT, though notably without some special implementation of the mechanism, realizing the potential computational savings will not be immediate. In this small section, we translate this complex-valued mechanism into two real-valued mechanism which can be integrated with the code backing the rest of the paper. These mechanisms differ in their decoding matrix B, and thus achieve different levels of loss. Both have efficient implementations, though with asymptotics differing by a logarithmic factor. We implement and experiment with both of these mechanisms in the main body.\n\nThese two mechanisms share an encoding matrix:\n\nCF = F∗C,\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nwhich is real-valued by Lemma K.1. Note that the sensitivity of CF is identical to that of C for any notion of sensitivity expressible as Definition 1 due to the unitary of the Fourier transform. Since this matrix is of shape [2n, n], there is choice in computing the decoder B such that BCF represents the prefix-sum matrix. The two decoders we present below correspond to two subtly distinct mechanisms.\n\nMechanism 1: A real-valued version of the mechanism presented in Section 4. One natural translation of the analysis in Section 4 (indeed, a real-valued version of the precise operation described in Algorithm 1) may be computed by inserting a Fourier transform to match the inverse transform in CF:\n\nBF = BF,\n\nClearly BFCF = BC, and BF real-valued by Lemma K.1. Proposition K.1. For any D, the mechanism described in Section 4 is distributionally equivalent to an application of the real-valued matrix mechanism with the factorization (BF, CF), and satisfies the same privacy guarantees.\n\nProof. To show this result, by noting that CF and C have the same sensitivity, it suffices to show that it suffices to show:\n\n√\n\n• (cid:60)[F∗\n\nΣz] (for z a sample from an isotropic complex Gaussian) is distributionally equivaΣFb for b a sample from a real (isotropic) Gaussian with the same variance.\n\n√\n\nlent to PF∗\n\nThis is a consequence of the distributional invariance of the Gaussian under unitary transformations:\n\n√\n\n(cid:60)[F∗\n\nΣz] ∼ (cid:60)[F∗ √\n\n√\n\nΣFz]\n\n= F∗\n\nΣF(cid:60)[z]\n\n√\n\nΣFb,\n\nwhere the variances are as desired.\n\n∼ F∗\n\n√\n\n(as F∗\n\nΣF is real)\n\nNote that the efficiency of the mechanism described in Section 4 carries over immediately to this factorization (BF, CF); indeed, the capacity to compute the noise BFb with complexity n log(n) may be reasoned to directly, in a similar manner.\n\nThis mechanism is not, however, the optimal one for the encoder CF, and this subtlety has difficult downstream effects in integrating with real-valued factorization codepaths (e.g., see the discussion in Appendix D.4). We proceed to show that the optimal decoder can be used directly, at only a moderate loss of efficiency with sufficiently careful implementation.\n\nMechanism 2: A real-valued optimal decoder with complexity n log2(n). As noted in the literature (e.g. Section 3 of Denisov et al. (2022)), for a fixed encoder, the optimal decoder may always be computed in terms of an appropriate pseudoinversion of the encoder. Therefore, we may compute the optimal decoder for the encoder CF, defining:\n\nBFopt = SC† F,\n\nwhere S is the prefix-sum matrix. Since CF is real, its pseudoinverse is as well, and BFopt is also real-valued. Since BFopt can have no more variance than BF, all the utility analysis of the DFT mechanism in Section 4 carries through as an upper bound for this factorization. Privacy of this mechanism is ensured by the fact that this mechanism reuses teh encoder CF. The major way in which these mechanisms operationally differ comes down to the cost of computing the noise vector BFoptb, where b represents a sample from an isotropic Gaussian distribution. Though we do not know of a complexity result which matches the decoder BF, we will show that the complexity cost which must be paid is only logarithmically higher.\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nProposition K.2. The mapping b (cid:55)→ BFoptb, where b ∈ Rn, may be evaluated in O(n log2(n)) time.\n\nProof. First, notice that the matrix CF is one-to-one; indeed, this is immediately implied by the factorization S = BFCF. By Theorem 1.2.1 (P6) of (Campbell & Meyer, 1979), any one-to-one matrix T admits the following representation for its pseudoinverse:\n\nWe compute:\n\nT† = (T∗T)−1 T∗.\n\nC†\n\nF = (C∗ (cid:16)(cid:16)\n\nFCF)−1 C∗ F\n(cid:17)∗\n\n√\n\nF∗\n\nΣFE\n\n=\n\n√\n\nF∗\n\n(cid:17)−1 (cid:16)\n\nΣFE\n\n√\n\nF∗\n\n(cid:17)∗\n\nΣFE\n\n(cid:16)\n\n=\n\nPF∗\n\n√\n\n∗\n\nΣ\n\nFF∗\n\n√\n\n(cid:17)−1 (cid:16)\n\n√\n\nF∗\n\n(cid:17)∗\n\nΣFE\n\n= (PF∗|Σ|FE)−1 (cid:16)\n\nF∗\n\n(cid:17)∗\n\nΣFE\n\nΣFE √\n\nNow, the matrix PF∗|Σ|FE is Toeplitz, since F∗|Σ|F is circulant, and P, E combine to select out the top-left n × n square of F∗|Σ|F. Notice that PF∗|Σ|FE is not circulant, and cannot therefore be diagonalized by the n-dimensional Fourier transform.\n\nThe development of Section 4 yield the representation:\n\nS = PF∗ΣFE,\n\n√\n\n(cid:16)\n\nF∗\n\n(cid:17)∗\n\nwhich implies that matrix-vector products with the matrix S may be computed in n log n time by the\n\nuse of the FFT. Similarly, matrix-vector products with\n\nΣFE\n\nmay be computed in n log n\n\ntime. Therefore the computational cost of computing the mapping b (cid:55)→ SC† the maximum of n log n and the cost of computing the mapping v (cid:55)→ (PF∗|Σ|FE)−1 v.\n\nFb can be upper bounded by\n\nThe cost of computing this mapping is, in turn, bounded by the cost of inverting a general (fullrank) Toeplitz system, since (PF∗|Σ|FE)−1 v may be alternatively characterized as the solution x to the equation PF∗|Σ|FEx = v. The computational cost of solving such a system is known to be n log2(n); see, e.g., (de Hoog, 1987).\n\nLemma K.1. For a real-valued vector v, let ˆv represent its discrete Fourier transform. If ˆv as no · denote the (pointwise) principal branch of the square purely real, negative entries, then letting root and F the matrix representation of the Fourier transform, the matrix F∗\n\nˆvF is real-valued.\n\n√\n\n√\n\nProof. Conjugate symmetry of the DFT states that for a j-dimensional real-valued vector x, ˆx[m] = ˆx[j − m], and that the converse also holds–that if ˆx has this symmetry, x is real-valued. This can be seen by examining the action of conjugation of x on the Fourier transform ˆx.\n\nNow, by the assumptions on ˆv and the choice of the principal branch of the square root5, if ˆv ˆv. Therefore there is some real-valued vector y such that has this conjugate symmetry, so does ˆvF represents convolution with y in the standard basis, and hence is ˆy = real-valued.\n\nˆv. The matrix F∗\n\n√\n\n√\n\n√\n\n5These assumptions can be avoided, though at the cost of taking care in choosing the square root of the\n\nnegative elements of ˆv to preserve the appropriate symmetry.\n\n39\n\nUnder review as a conference paper at ICLR 2023\n\nRemark. Lemma K.1 can be understood as a statement about the solvability of a certain repeatedconvolution equation over real-valued functions (the equation g ∗ g = f ). We suspect that this fact has been observed in the harmonic analysis literature as a general property of all Fourier transforms; we could find no reference. The symmetries discussed above take a slightly different form in the continuous and noncompact case (IE, Fourier transform on real-valued function on Rd) and the finite-dimensional Fourier transform here, so we choose to prove this statement in this limited setting.\n\n40",
    "reference": "# Summary Of The Paper\n\nThe paper studies the problem of privately releasing prefix sums in the adaptive continual release model via matrix factorization techniques. Their main results are 1.extension of prior work which was limited to single participation to general participation patterns and 2. a more efficient method based on fast Fourier transform. The authors conduct extensive experiments which show promising results over prior approaches.\n\n# Strength And Weaknesses\n\nStrengths\n\n1. The prefix-sum view of SGD and using matrix factorization techniques to privately release these have been instrumental in improving the utility in private machine learning tasks, especially in large-scale practical settings. Therefore, the problem that the authors study is important and the improvements can have immediate real-world impact in how private machine learning models are being trained.\n\n2. The paper is mostly written-well; it gradually formulates the problem, and builds towards the key ideas and proposed approach. \n\n3. The experimental results showcase improvement over prior approaches; for example they get a 5\\% improvement over DP-SGD with privacy amplification with sub-sampling.\n\nWeakness\n\n1.  The section on FFT seems half baked and not so related with the overall picture. The main goal in the paper to demonstrate empirical improvement but it seems to be that empirical results presented in the main text don't even use this method; correct me if wrong.\n\n2. $(k,b)$ participation - I did not fully understand the choice and use of assumption that a point participates once every $b$ iterations. It seems to me that it is only used to argue that $|\\Pi|=b$; in that case, why not simply explicitly assume this? If not, where is the additional structure useful?\n\n3. At parts, I felt that some details are missing in order to completely understand the arguments, or the authors expect the reader to be familiar with Denisov et al. (2022). For example, in Corollary 3.1, the optimal dual solution computed, but how to get the primal solution from here? It also is the case with some experimental details (see below). It would be helpful if the authors can make the content more self-sufficient.\n\n4. Experiments: I do not understand the description of algorithms MF1,6e and MF6,6e; for MF1,6e the authors say that they use Theorem 2.1 to use \"the single participation optimized matrix\", but Theorem 2.1 is about reducing $d$ dimensional instances to scalar instances; what am I missing here? About MF6,6e, it says, is \"our approach ..\" -- which theorem/procedure description should I be looking at for this?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is mostly-well written but some parts have missing important details or are relegated to prior works. The extension from single participation does not seem particularly challenging and quality mainly hinges on observed empirical improvements.\n\n# Summary Of The Review\n\nI think the topic of the paper is timely and the authors make interesting contributions with promising empirical results. The downsides are missing/unclear details as well as lack of coherence among parts of the paper, like the one based on FFT.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMASTERING SPATIAL GRAPH PREDICTION OF ROAD NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nFigure 1: Our agent interacts with the currently generated spatial graph by proposing new edges to be added. A tree-based search produces a sequence of actions that maximizes a reward function based on complex geometrics priors.\n\nABSTRACT\n\nAccurately predicting road networks from satellite images requires a global understanding of the network topology. We propose to capture such high-level information by introducing a graph-based framework that simulates the addition of sequences of graph edges using a reinforcement learning (RL) approach. In particular, given a partially generated graph associated with a satellite image, an RL agent nominates modifications that maximize a cumulative reward. As opposed to standard supervised techniques that tend to be more restricted to commonly used surrogate losses, these rewards can be based on various complex, potentially noncontinuous, metrics of interest. This yields more power and flexibility to encode problem-dependent knowledge. Empirical results on several benchmark datasets demonstrate enhanced performance and increased high-level reasoning about the graph topology when using a tree-based search. We further highlight the superiority of our approach under substantial occlusions by introducing a new synthetic benchmark dataset for this task.\n\n1\n\nINTRODUCTION\n\nRoad layout modelling from satellite images constitutes an important task of remote sensing, with numerous applications in and navigation. The vast amounts of data available from the commercialization of geospatial data, in addition to the need for accurately establishing the connectivity of roads in remote areas, have led to an increased interest in the precise representation of existing road networks. By nature, these applications require structured data types that provide efficient representations to encode geometry, in this case, graphs, a de facto choice in domains such as computer graphics, virtual reality, gaming, and the film industry. These structured-graph representations are also commonly used to label recent road network datasets (Van Etten et al., 2018) and map repositories (OpenStreetMap contributors, 2017). Based on these observations, we propose a new method for generating predictions directly as spatial graphs, allowing us to explicitly incorporate geometric constraints in the learning process, encouraging predictions that better capture higher-level dataset statistics.\n\nIn contrast, existing methods for road layout detection, mostly rely on pixel-based segmentation models that are trained on masks produced by rasterizing ground truth graphs. Performing pixelwise segmentation, though, ignores structural features and geometric constraints inherent to the\n\n1\n\nPrior policy: Reward estimates: Value estimates Simulate next stepsTextCurrent StatePrior policySimulate next stepsUnder review as a conference paper at ICLR 2023\n\nproblem. As a result, minimum differences in the pixel-level output domain can have significant consequences in the proposed graph, in terms of connectivity and path distances, as manifested by the often fragmented outputs obtained after running inference on these models. In order to address these significant drawbacks, we propose a new paradigm where we: (i) directly generate outputs as spatial graphs and (ii) formalize the problem as a game where we sequentially construct the output by adding edges between key points. These key points can in principle come from any off-the-shelf detector that identifies road pieces with sufficient accuracy. Our generation process avoids having to resort to cumbersome post-processing steps (Batra et al., 2019; Montoya-Zegarra et al., 2015) or optimize some surrogate objectives (M ́attyus & Urtasun, 2018; Mosinska et al., 2018) whose relation to the desired qualities of the final prediction is disputed. Concurrently, the sequential decision-making strategy we propose enables us to focus interactively on different parts of the image, introducing the notion of a current state and producing reward estimates for a succession of actions. In essence, our method can be considered as a generalization of previous refinement techniques (Batra et al., 2019; Li et al., 2019b) with three major advantages: (i) removal of the requirement for greedy decoding, (ii) ability to attend globally to the current prediction and selectively target parts of the image, and (iii) capacity to train based on demanding task-specific metrics.\n\nMore precisely, our contributions are the following:\n\n• We propose a novel generic strategy for training and inference in autoregressive models that removes the requirement of decoding according to a pre-defined order and refines initial sampling probabilities via a tree search.\n\n• We create a new synthetic benchmark dataset of pixel-level accurate labels of overhead satellite images for the task of road network extraction. This gives us the ability to simulate complex scenarios with occluded regions, allowing us to demonstrate the improved robustness of our approach. We plan to release this dataset publicly.\n\n• We confirm the wide applicability of our approach by improving the performance of existing\n\nmethods on the popular SpaceNet and DeepGlobe datasets.\n\n2 RELATED WORK\n\nFigure 2: Typical road network features (SpaceNet dataset). From left to right: (a) the distribution of angles between road segments leading to the same intersection is biased towards 0 and 90 degrees, parallel or perpendicular roads. The same also holds (b) for angles between random road pieces within a ground distance of 400 meters. (c) Most road vertices belong to a single road piece, with a degree of 2. (d) The average number of intersections for areas of 400×400 meters by ground distance.\n\nInitial attempts to extract road networks mainly revolved around handcrafted features and stochastic geometric models of roads (Barzohar & Cooper, 1996). Road layouts have specific characteristics, regarding radiometry and topology e.g. particular junction distribution, certain general orientation, and curvature (see Fig. 2), that enable their detection even in cases with significant occlusion and uncertainty (Hinz & Baumgartner, 2003). Modern approaches mostly formulate the road extraction task as a segmentation prediction task (Lian et al., 2020; Mattyus et al., 2015; Audebert et al., 2017) by applying models such as Hourglass (Newell et al., 2016) or LinkNet (Chaurasia & Culurciello, 2017). This interpretation has significant drawbacks when evaluated against structural losses, because of discontinuities in the predicted masks. Such shortcomings have been addressed by applying some additional post-processing steps, such as high-order conditional random fields (Niemeyer et al., 2011; Wegner et al., 2013) or by training additional models that refine these initial predictions (M ́attyus et al., 2017; Batra et al., 2019). Other common techniques include the optimization of an ensemble of losses. Chu et al. (2019) rely on a directional loss and use non-maximal suppression as a thinning layer, while Batra et al. (2019) calculate orientations of road segments. Although such auxiliary losses somewhat improve the output consistency, the fundamental issue of producing\n\n2\n\n020406080Angle degrees0.000.050.100.150.200.250.30020406080Angle degrees0.000.050.100.150.200.250.301.02.03.04.05.0Vertex degree0.00.10.20.30.40.50.6020406080Num. junctions0.000.020.040.060.080.100.120.140.16Under review as a conference paper at ICLR 2023\n\nFigure 3: (Left) Typical segmentation-based methods generate an output graph by thresholding a segmentation mask, which can however often lead to fragmented outputs. Predicting segmentation masks also makes it impossible (right) to capture complex road interactions, such as overlapping roads at different elevations.\n\npredictions in the pixel space persists. It remains impossible to overcome naturally occurring road network structures, e.g. crossings of roads in different elevations, see Fig. 3.\n\nPrevious failure cases have led to more intuitive conceptualizations of the task. Roadtracer (Bastani et al., 2018), iteratively builds a road network, similar to a depth-first search approach, while Chu et al. (2019) learn a generative model for road layouts and then apply it as a prior on top of a segmentation prediction mask. Proposed graph-based approaches, encode the road network directly as a graph, but either operate based on a constrained step-size (Tan et al., 2020) to generate new vertices or operate on a single step (He et al., 2020; Bandara et al., 2022), involving use-defined thresholding to post-process the final predictions. Most similar to our work, Li et al. (2019b) predict locations of key points and define a specific order traversing them, also similarly Xu et al. (2022). Such autoregressive models have been recently successfully applied with the use of transformers (Vaswani et al., 2017) in a range of applications (Nash et al., 2020; Para et al., 2021a;b; Xu et al., 2022) to model constraints between elements, while their supervised training explicitly requires tokens to be processed in a specific order. This specific order combined with the fact that only a surrogate training objective is used, introduces limitations, discussed further in the next section. In order to eliminate this order requirement and to optimize based on the desired metric, while attending globally to the currently generated graph, we propose to use RL as a suitable alternative.\n\nWhen generating discrete outputs, an unordered set of edges (Zaheer et al., 2017), it is challenging to adapt existing learning frameworks to train generative models (Para et al., 2021b). Instead of optimizing in the image space, however, we are interested in optimizing spatial structured losses by learning program heuristics, i.e. policies. RL has found success in the past in computer vision applications (Le et al., 2021), but mainly as an auxiliary unit with the goal of improving efficiency (Xu et al., 2021) or as a fine-tuning step (Qin et al., 2018). We instead rely on RL to produce the entire graph exploiting the ability of the framework for more high-level reasoning.\n\n3 METHODOLOGY\n\nWe parametrize a road network as a graph G = {V, E} with each vertex vi = [xi, yi]⊤ ∈ V representing a key point on the road surface. The set of edges (vi, vj) ∈ E, corresponds to road segments connecting these key points. We can then generate a probability distribution over roads by following a two-step process: i) generation of a set of vertices and ii) generation of a set of edges connecting them. Formally, for an image I, a road network R is derived as:\n\nR = arg max\n\nP (V, E | I) = P (E | V, I)P (V | I).\n\n(1)\n\nV,E\n\nThe graph nodes typically correspond to local information in an image, and we therefore resort to a CNN-based model to extract key points, providing the set V ′, that sufficiently captures the information in the ground truth graph G. The construction of edges, however, requires higher-level reasoning that can cope with parallel roads, junctions, occlusions, or poor image resolution, among other difficulties.\n\nConsidering probabilistic models over sequences and using the chain rule, we can factorize the joint distribution as the product of a series of conditional distributions\n\nP (E | V, I; σ) =\n\nNE(cid:89)\n\nn=1\n\nP (eσ(n) | e<σ(n), V, I),\n\n(2)\n\n3\n\nSegmentaiton ModelThreshold and extract graphCompare to ground truthUnder review as a conference paper at ICLR 2023\n\n(1) A backbone image model Figure 4: The base autoregressive model with its three main components. (ResNet) that extracts features for each key point at different scales, along with a global image embedding. (2) A key point model embeds visual and location features of distinct key points. (3) An edge embedding model relates the current edge sequence with the respective key points. Each edge token (signalled with the tokens ‘action-i’) corresponds to an index specifying the respective key point. A pair of such tokens designates an edge as its two endpoints. At the end of (3) we obtain a new distribution over key points, that leads to an incremental update to the graph, after sampling.\n\nwhere e<σ(n) represents eσ(1), eσ(2), . . . , eσ(n−1) and σ ∈ SNE denotes the set of all permutations of the integers 1, 2, . . . , NE , with NE the number of edges. For our work, we consider the setting where these sequences are upper bounded in length, i.e. NE ≤ Nmax, a reasonable assumption when dealing with satellite images of fixed size. Autoregressive models (ARMs) have been used to solve similar tasks in the past by defining a fixed order of decoding (Oord et al., 2016; van den Oord et al., 2016; Nash et al., 2020; Para et al., 2021a). In our case, this would correspond to sorting all key points by their x and y locations and generating edges for each of them consecutively. We call this the autoregressive order. There are, however, two major drawbacks.\n\nFirst, the evaluation metrics used for this task define a buffer region in which nodes in the ground truth and the predicted graph are considered to be a match. Therefore, a newly generated edge can be only partially correct, when only partially overlapping with the ground truth graph. This nonsmooth feedback comes in clear contrast to the supervised training scheme of ARMs, minimization of the negative log-likelihood, that assumes perfect information regarding the key points’ locations, i.e. In practice, this condition is rarely met, as the exact spatial graph can be represented in arbitrarily many ways by subdividing long edges into smaller ones or due to small perturbation to key points’ locations. It is thus imperative that our model can estimate the expected improvement of adding selected edges, which implicitly can also signal when to appropriately end the generation process.\n\nthat the sets V and V ′ are the same.\n\nSecond, the requirement to decode according to the autoregressive order introduces a bias and limits the expressiveness of the model (Uria et al., 2014). As a result, it can lead to failures in cases with blurry inputs or occlusions (Li et al., 2019b). Previous solutions include the use of beam search, either deterministic or stochastic (Meister et al., 2021). Beam search does not however eliminate the bias introduced in the selection order of the key points, while suffering from other deficiencies, such as degenerate repetitions (Holtzman et al., 2019; Fan et al., 2018). In order to address these shortcomings, we advocate for a permutation invariant strategy. We present a novel generic strategy, which improves autoregressive models without requiring significantly more computational cost.\n\n3.1 AUTOREGRESSIVE MODEL\n\nWe start by introducing a base autoregressive model, illustrated in Fig. 4. Given an image and a set of key points, our model produces a graph by sequentially predicting a list of indices, corresponding to the graph’s flattened, unweighted edge-list. Each forward pass produces probabilities over the set of key points, which leads to a new action after sampling. A successive pair of indices defines an edge as its two endpoints. A special end-of-sequence token is reserved to designate the end of the generation process.\n\nFollowing Wang et al. (2018); Smith et al. (2019), we begin by extracting visual features per key point, by interpolating intermediate layers of a ResNet backbone to the key points’ locations, which\n\n4\n\nkeypoint-1keypoint-nkeypoint-1 embedding stop-tokenembeddingkeypoint-n embedding selection 1selection mKey-points embedding (2)Multi-Head Attentionstop-tokenLocationencodingAdd & NormFeed ForwardAdd & NormN Global imageembeddingaction-1action-maction-(m + 1)gatherMulti-Head AttentionAdd & NormFeed ForwardAdd & NormM hidden_statenew samplepointer networkImage processing (1)Edges embedding (3)TypeencodingTransformer ITransformer II Under review as a conference paper at ICLR 2023\n\nare further augmented by position encodings of their locations. We then further process these features using two lightweight Transformer modules. The first transformer (Transformer I in Fig. 4) encodes the features of the key points as embeddings. The second transformer (Transformer II in Fig. 4) takes as input the currently generated edge list sequence, corresponding to the currently partially generated graph. Edges are directly mapped to the embeddings of their comprising key points, supplemented by position and type embeddings, to differentiate between them, as shown in Fig. 5 (a). An additional global image embedding, also extracted by the ResNet, is used to initialize the sequence. The Transformer II module produces a single hidden state, which is linked with the NV ′ + 1 (corresponding to the provided key points, supplemented by the special end of the generation token) key points’ embeddings by a pointer network (Vinyals et al., 2015), via a dot-product to generate the final distribution. This allows a variable number of actions that depends on the current environment state, instead of using a fixed action space.\n\n3.2 AUGMENTED SEARCH\n\nIn order to address the problems of greedy decoding (analysed Section 3), we frame our road extraction task as a classical Markov-decision process (MDP). The generation of a graph for every image defines an environment, where the length of the currently generated edge list determines the current step. Let ot, αt and rt correspond to the observation, the action and the observed reward respectively, at time step t. The aim is to search for a policy that maximizes the expected cumulative reward over a horizon T , i.e., maxπ J(π) := Eπ[(cid:80)T −1 t=0 γtrt] where γ ∈ (0, 1] indicates the discount factor and the expectation is with respect to the randomness in the policy and the transition dynamics. We set the discount factor to 1 due to the assumed bounded time horizon, and we note that although the dynamics of the environment are deterministic, optimizing the reward remains challenging.\n\nEach action leads to the selection of a new key point, with new edges being added once every two actions. The addition of a new edge leads to a revision of the predicted graph and triggers an intermediate reward\n\nrt = sc(Ggt, Gpredt\n\n) − sc(Ggt, Gpredt−1\n\n),\n\n(3)\n\n) is a similarity score between the ground truth graph Ggt and the current estimate\n\nwhere sc(Ggt, Gpredt Gpredt. Discussion of the specific similarity scores used in practice is postponed for Section 3.3. A proper spatial graph generation entails (i) correct topology and (ii) accurate location prediction of individual roads. For the latter, intermediate vertices of degree 2 are essential. We call a road segment (RS), an ordered collection of edges, between vertices of degree d(.) two (or a collection of edges forming a circle):\n\nRS = {(vrs1 , vrs2), . . . , (vrsk−1 , vrsk )} s.t (vrsi, vrsi+1) ∈ E for i = 1, . . . , k − 1 (d(vrs1 ) ̸= 2 and d(vrsk ) ̸= 2 or vrs1 = vrsk ).\n\nd(vrsi) = 2, for i = 2, . . . k − 1,\n\nDuring the progression of an episode (i.e. the sequential generation of a graph), the topological nature of the similarity scores in Eq. 3 implies that the effect of each new edge to the reward will be reflected mostly once its whole corresponding road segment has been generated. To resolve the ambiguity in the credit assignment and allow our agent to look ahead into sequences of actions, we rely on Monte Carlo Tree Search (MCTS) to simulate entire sequences of actions. We use a state-of-the-art search-based agent, MuZero (Schrittwieser et al., 2020), that constructs a learnable model of the environment dynamics, simulating transitions in this latent representation and leading to significant computational benefits.\n\nSpecifically, MuZero requires three distinct parts (see also Fig. 5):\n\n1. A representation function f that creates a latent vector of the current state ht = fθ(ot). For this step, we use the autoregressive model, as shown in Fig. 4. Our current latent representation ht contains the graph’s hidden state, along with the key points’ embeddings used to map actions to latent vectors. As key points remain the same throughout the episode, image-based features (Components (1) and (2) in Fig. 4) are only computed once.\n\n2. A dynamics network g, we use a simple LSTM (Hochreiter & Schmidhuber, 1997), that predicts the effect of a new action by predicting the next hidden state and the expected reward: (ˆht, ˆrt) = gθ( ̃ht−1, αt). We can replace ̃ht−1 with the latent representation ht−1, or its previous computed approximation ˆht−1 for tree search of larger depth larger than 1.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: The autoregressive model generates a hidden state corresponding to the graph embedding. (a) When doing so, the edge decoder directly attends to the input key points’ embeddings, augmented by position and type embeddings. (b) The prediction network uses the hidden state to produce value and policy predictions. A pointer network allows an intuitive scale of the action space by the number of key points. (c) The dynamics network simulates trajectories by estimating new hidden states and rewards. For newly generated edges, it takes as input the embeddings of the new key points, but also the degree of the two vertices involved and the length of the newly proposed generated edge.\n\n3. A prediction network ψ, that estimates the policy and the value for the current state (pt+1, vt) = ψθ( ̃ht). We compute the policy via a pointer network, as described in Section 3.1. Value estimates are produced by a simple multi-layer network.\n\nThe dynamics network guides the search and evaluates the expected reward of actions. For every newly generated edge, we also explicitly inform the network regarding the creation of new intersections and the expected relative change in the overall road surface generated via embeddings (see Fig. 5). By using the dynamics network, we bypass the expensive call to the decoder module during the search, and can instead approximate small modifications in the latent representation directly. For our experiments, the dynamics network requires up to 90 times less floating-point operations to simulate trajectories, compared to using the edge embeddings’ decoder. Effectively, our method does not involve significantly more computation budget compared to the base autoregressive model.\n\n3.3 EVALUATION METRICS\n\nWe adopt the same evaluation metrics both as a comparison between different methods but also as the incremental rewards for our agent, by Eq. 3. We use the relaxed versions of precision, recall and intersection over union for pixel-level predictions Correctness/Completeness/Quality (CCQ) (Wiedemann et al., 1998; Wang et al., 2016). As graph-theoretic metrics we use APLS (Van Etten et al., 2018) and additionally include new metrics introduced in Citraro et al. (2020) that compare Paths, Junctions and Sub-graphs of the graphs in question, producing respectively precision, recall and f1 scores. More details can be found in Appendix E.1\n\n4 EXPERIMENTS\n\nImplementation details We resize images to 300 × 300 pixels, standardizing according to the training set statistics. For exploration, we initialize workers using Ray (Moritz et al., 2017) that execute episodes in the environment. For training, we unroll the dynamics function for td = 5 steps and use priority weights for the episode according to the differences between predicted and target values. Our algorithm can be considered as an approximate on-policy TD(λ) (Sutton & Barto, 2018) due to the relatively small replay buffer. We reanalyse older games (Schrittwieser et al., 2020) to provide fresher target estimates. Unvisited graph nodes are selected based on an upper confidence score, balancing exploration with exploitation, similar to Silver et al. (2018). We add exploration noise as Dirichlet noise and select actions based on a temperature-controlled sampling procedure, whose temperature is reduced during training.\n\nGiven the limited high-quality available ground truth labels (Singh et al., 2018) and to accelerate training, we employ modifications introduced in EfficientZero (Ye et al., 2021). We investigate adding supervision to the environment model and better initialize Q-value predictions similar to the implementation of Elf OpenGo (Tian et al., 2019). We further scale values and rewards using an\n\n6\n\nCurrent edge sequenceKey-pointembedding 00 11 20 31 40 51 60 71Hidden statepolicydot-product(a)(b)value estimateposition type Autoregressive ModelCurrent hidden stateEmbedding of selected key pointVertex type embeddingEdge length embeddingDynamicsnetworkreward estimateNext hidden stateCurrent hidden statePredictionnetworkEmbedding(c)Under review as a conference paper at ICLR 2023\n\nFigure 6: (Left) Examples of fragmented outputs from the LinkNet model under cases of significant occlusion and how our approach performs in these demanding circumstances. (Right) The performance gap between our method and the same baseline is wide, based on topological spatial graph metrics, for a wide range of difficulties in the images. More details and examples are given in the supplementary material.\n\ninvertible transform inspired by Pohlen et al. (2018). Here, we predict support, as fully connected networks are biased towards learning low-frequency representations (Jacot et al., 2018). Selecting new actions involves generating simulations that can be done expeditiously given the small dimension of the latent space and the modest size of the dynamics network. Finally, to generate key points, we skeletonize segmentation masks provided by any baseline segmentation model, by thresholding the respective segmentation masks produced and applying RDP-simplification (Douglas & Peucker, 1973; Ramer, 1972). Selecting an appropriate threshold and subdividing larger edges guarantees that the generated set V ′ adequately captures most of the ground truth road network, leaving the complexity of the problem for our model to handle.\n\n4.1 SYNTHETIC DATASET\n\nWe generate a dataset of overhead satellite images of a synthetic town using CityEngine1. We randomly specify vegetation of varying height and width along the side walks of the generated streets, leading inadvertently to occlusions of varying difficulty. The simulated environment allows specifying pixel-perfect masks regarding both roads and trees occluding the road surface based on the provided camera parameters (Kong et al., 2020). We can hence tune the complexity of the task and quantify the benefits of our approach for varying levels of difficulty. We defer more details regarding the generation process and dataset examples to the supplementary material.\n\nWe compare our method by training on our dataset a LinkNet model (Chaurasia & Culurciello, 2017), a popular segmentation model that has been widely used in the remote sensing community (Li et al., 2019a). Even in this synthetic and thus less diverse scenario, the deficiency of segmentation models to rely mostly on local information, with no explicit ability for longer-range interactions, is evident. Fig. 6, illustrates examples of such over-segmented predictions and how our approach can improve on them. We also define a ’difficulty’ attribute per synthetic satellite image, quantifying the occlusions as a percentage of the ground truth road mask covered. We observe a considerable absolute improvement in topological metric scores when training our model on this synthetic dataset, compared to the LinkNet baseline, for varying image difficulty.\n\n4.2 REAL DATASETS\n\nWe evaluate our method on the SpaceNet and DeepGlobe datasets. We use the same train-test splits as in Batra et al. (2019) to promote reproducibility, while results are reported for the final combined graph on the original image scale. No pre-training on the synthetic dataset takes place. Further details regarding pre-processing are available in the Appendix E.2\n\n1https://www.esri.com/en-us/arcgis/products/arcgis-cityengine/overview\n\n7\n\nSample 1Synthetic Satellite ImageLinkNet Model PredictionOur PredictionSample 20.00.10.20.30.40.5Absolute APLS improvementLinear fit0.00.10.20.30.40.50.60.7Absolute TLTS-correct improvement0.00.10.20.30.40.5Percentage of occlusions0.00.51.01.52.02.53.03.5DensityIncreasing difficultyUnder review as a conference paper at ICLR 2023\n\nTable 1: Quantitative results for the SpaceNet and DeepGlobe datasets.\n\nMethod\n\nCCQ corr. ↑ comp. ↑ qual. ↑\n\nTLTS corr. ↑ 2l+2s ↓\n\nAPLS ↑\n\nPath-Based rec. ↑\n\npre. ↑\n\nf1 ↑\n\nJunction-Based rec. ↑\n\npre. ↑\n\nf1 ↑\n\nt DeepRoadMapper (M ́attyus et al., 2017)\n\ne N\ne c\na p\nS\n\nSegmentation (Long et al., 2015; Kaiser et al., 2017) LinkNet (Chaurasia & Culurciello, 2017) Orientation (Batra et al., 2019) Sat2Graph (He et al., 2020)** SPIN road mapper (Bandara et al., 2022) Ours\n\ne LinkNet (Chaurasia & Culurciello, 2017)\n\np e\ne D\n\nb o\nG\n\nl\n\nOrientation (Batra et al., 2019) Ours*\n\n0.6943 0.6838 0.5386 0.7493 0.7094 0.5969 0.8100 0.7449 0.6409 0.8070 0.8001 0.6862 0.6917 0.7351 0.5734 0.7837 0.7988 0.6621 0.7726 0.7852 0.6632\n\n0.8012 0.8676 0.7328 0.8243 0.8857 0.7545 0.8163 0.8929 0.7391\n\n0.4110 0.1012 0.4143 0.0828 0.4894 0.0743 0.5594 0.0884 0.5802 0.1104 0.5922 0.1058 0.5970 0.0878\n\n0.6640 0.0804 0.6866 0.1047 0.7177 0.1058\n\n0.5143 0.5454 0.5743 0.6315 0.5951 0.6422 0.6523\n\n0.6525 0.7012 0.7398\n\n0.5958 0.6400 0.6171 0.6909 0.6863 0.6885 0.6719 0.6460 0.6586 0.7175 0.7280 0.7227 0.5952 0.5416 0.5671 0.7276 0.7265 0.7270 0.7323 0.7543 0.7431\n\n0.6293 0.7443 0.6820 0.7186 0.7710 0.7438 0.6985 0.7809 0.7374 0.7552 0.7591 0.7571 0.7474 0.5951 0.6626 0.7621 0.7827 0.7722 0.7762 0.7824 0.7792\n\n0.6882 0.6920 0.6901 0.6937 0.8082 0.7465 0.7050 0.8181 0.7573\n\n0.7675 0.7444 0.7558 0.7624 0.7939 0.7778 0.7834 0.8253 0.8038\n\nSub-graph-Based f1 ↑\n\n0.6783 0.7117 0.7576 0.7802 0.7180 0.7837 0.7922\n\n0.7879 0.8282 0.8338\n\n* We do not fine-tune our model on the DeepGlobe dataset but instead refine predictions standardizing according to train dataset statistics. ** The authors provided predictions corresponding only to a center crop of the original SpaceNet dataset images. Also note that the test set is different from the one reported on the rest of the methods, see also Appendix. Blue: best score, Green: second best score, Gray: results reported in different test set\n\n4.2.1 COMPARISON TO BASELINES\n\nWe first verify that under the effectiveness of the proposed approach under an ideal scenario where the key points conditioned upon, correspond to the ones from the ground truth. In the interest of space we point the reader to Appendix A and Table 3. Subsequently, we move to the primary task of predicting spatial graphs without the ground-truth graph information but extract key points via the aforementioned process and train using the described topological metrics directly. The previous baselines are not applicable in this case, due to lack of ground truth information, so we instead compare against the following; we explore powerful CNN architectures, by training a Segmentation model with a ResNet backbone. We evaluate DeepRoadMapper (M ́attyus et al., 2017), a model that refines previous segmentation by adding connections along possible identified paths. As done by Batra et al. (2019) we notice that in complex scenarios, the effect of this post-processing step is not always positive. We also evaluate against LinkNet (Chaurasia & Culurciello, 2017), and Orientation (Batra et al., 2019), which is trained to predict simultaneously road surfaces and orientation.\n\nTable 2: Qualitative results of improved connectivity. We recommend zooming in for more details\n\nDeepRoadMapper (M ́attyus et al., 2017)\n\nSegmentation (Kaiser et al., 2017)\n\nOrientation (Batra et al., 2019)\n\nOurs\n\nGround Truth\n\nSatellite Image\n\nQuantitative results in Table 1 and visual inspection in Table 2, affirm that the global context and the gradual generation incite a better understanding of the scene, leading to consistently outperforming topological metric results compared to the baselines. We remark that our predictions are more topologically consistent with fewer shortcomings, such as double roads, fragmented roads, and overconnections. This is further supplemented by comparing the statistics of the predicted spatial graphs in Fig. 7. We further showcase the transferability of our model by employing it with no fine-tuning (apart from dataset-specific image normalization) on the DeepGlobe dataset. We can refine previous predictions by adding missing edges, leading to more accurate spatial graph predictions, as shown in Table 1. This confirms our conjecture that road structures and geometric patterns are repeated across diverse cities’ locations.\n\n4.2.2 ABLATION STUDY\n\nWe experimented attending to image features for the two transformer modules by extracting perpatch visual features from the conditioning image H img = [himg 2 , . . . ], as done in the Vision Transformer (Dosovitskiy et al., 2020). This did not lead to significant improvements, which we attribute to over-fitting. In Fig. 8 we highlight the relative importance of some additional components for the final predictions. As efficiency is also of particular importance to us, we further visualize the effect of varying the simulation depth of the dynamics network during training. Surprisingly\n\n1 , himg\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Comparison of generated graph statistics, following the same post-processing, averaged across regions of 400×400 meters in ground distance. Orientation refers to the method of Batra et al. (2019).\n\nModel\n\nOurs\n\n− autoregressive pretraining − visual features for key-points − tree-search during evaluation + cross attend to image features\n\nAPLS\n\nP-f1\n\nJ-f1\n\nS-f1\n\n0.7431\n\n0.7792\n\n0.7922 0.6523 -15.3% -13.7% -14.1% -13.3% -13.4% -12.7% -12.1% -12.3% -2.1% -1.4% -1.7% -1.1% +0.2% -0.4% -0.7% -0.3%\n\nP: Path-based, J: Junction-based, S: Sub-graph-based\n\n(Left) Ablation study on the SpaceNet dataset.\n\nFigure 8: front obj = avg(APLS + Path-based f1 + Junction-based f1 + Sub-graph-based f1) for the same time budget.\n\n(Right) The training\n\nsimulations\n\nachieved\n\nnumber\n\nrelating\n\nestimated Pareto objective\n\nduring\n\nand\n\nthe\n\nthe\n\nby\n\nof\n\nperhaps, our method performs consistently better than baselines, even for a small overall simulation length, as this already enables better policy approximations.\n\nIn Appendix A we provide incremental results for the task of predicting road networks based on a optimal set of key points. In Appendix B we provide insights concerning interpretability and further comparison to baselines based on the varying difficulty of the predicted underlying road networks. In Appendix C we give more information regarding the generation of the synthetic dataset, while in Appendix D more information regarding the model architecture. Final in Appendix E we provide more implementation decisions, including details on exactly how key points and generated and how individual patch-level predictions are fused together. More examples of full environment trajectories are given in Appendix F. We stress that our method can act on partially initialized predictions, registering it also as a practical refinement approach on top of any baseline. Initializing our model according to the ARM model allows a moderately quick fine-tuning phrase. In combination with the learned environment model, which circumvents expensive calls to the edge embedding model for each simulation step in the MCTS, allows us to train even on a single GPU.\n\n5 CONCLUSIONS\n\nWe presented a novel reinforcement learning framework for generating a graph as a variable-length edge sequence, where a structured-aware decoder selects new edges by simulating action sequences into the future. Importantly, this allows the model to better capture the geometry of the targeted objects. This is confirmed by our experimental results, since our approach consistently produces more faithful graph statistics. One advantage of the proposed method is that the reward function is based on (non-continuous) metrics that are directly connected to the application in question. Our approach does not require significantly more computational resources compared to state-of-the-art supervised approaches, while in addition, it can be used to refine predictions from another given model. We also remark that the direct prediction of a graph enables the concurrent prediction of meta-information about the edges, including, for instance, the type of road (highway, primary or secondary street, biking lane, etc).\n\nOur approach opens the door to several directions for future work. For example, we have assumed that a pre-defined model gives the location of key points, but one could instead augment the action space to propose new key points’ locations. Other promising directions include the direct prediction of input-dependent graph primitives, e.g. T-junctions or roundabouts. Finally, we emphasize that our approach is suitable to a wide variety of applications where autoregressive models are typically used, and it is of special interest when there is a need for complex interactions or constraints between different parts of an object.\n\n9\n\n0100200300400500Distance between Intersections (meters)DensityGround TruthOrientationOurs0246Num. Connected Components020406080Num. Intersections012345Node DegreeUnder review as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY STATEMENT\n\nWe have taken multiple steps to ensure reproducibility of the experiments. We refer the reader to Appendix E for a complete description of the training protocol. We have also released the code as part of the supplementary material, including scripts on how to reproduce our results.\n\nREFERENCES\n\nSamira Abnar and Willem Zuidema. Quantifying attention flow in transformers. arXiv preprint\n\narXiv:2005.00928, 2020.\n\nDavid Acuna, Amlan Kar, and Sanja Fidler. Devil is in the edges: Learning semantic boundaries from noisy annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11075–11083, 2019.\n\nNicolas Audebert, Bertrand Le Saux, and S ́ebastien Lef`evre. Joint learning from earth observation and openstreetmap data to get faster better semantic maps. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 67–75, 2017.\n\nWele Gedara Chaminda Bandara, Jeya Maria Jose Valanarasu, and Vishal M Patel. Spin road mapper: Extracting roads from aerial images via spatial and interaction space graph reasoning for autonomous driving. In 2022 International Conference on Robotics and Automation (ICRA), pp. 343–350. IEEE, 2022.\n\nMeir Barzohar and David B Cooper. Automatic finding of main roads in aerial images by using geometric-stochastic models and estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(7):707–721, 1996.\n\nFavyen Bastani, Songtao He, Sofiane Abbar, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Sam Madden, and David DeWitt. Roadtracer: Automatic extraction of road networks In Proceedings of the IEEE Conference on Computer Vision and Pattern from aerial images. Recognition, pp. 4720–4728, 2018.\n\nAnil Batra, Suriya Singh, Guan Pang, Saikat Basu, CV Jawahar, and Manohar Paluri. Improved road connectivity by joint learning of orientation and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10385–10393, 2019.\n\nCameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1–43, 2012.\n\nAbhishek Chaurasia and Eugenio Culurciello. Linknet: Exploiting encoder representations for efficient semantic segmentation. In 2017 IEEE Visual Communications and Image Processing (VCIP), pp. 1–4. IEEE, 2017.\n\nBenjamin E Childs, James H Brodeur, and Levente Kocsis. Transpositions and move groups in monte carlo tree search. In 2008 IEEE Symposium On Computational Intelligence and Games, pp. 389–395. IEEE, 2008.\n\nHang Chu, Daiqing Li, David Acuna, Amlan Kar, Maria Shugrina, Xinkai Wei, Ming-Yu Liu, Antonio Torralba, and Sanja Fidler. Neural turtle graphics for modeling city road layouts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4522–4530, 2019.\n\nLeonardo Citraro, Mateusz Kozi ́nski, and Pascal Fua. Towards reliable evaluation of algorithms for road network reconstruction from aerial images. In European Conference on Computer Vision, pp. 703–719. Springer, 2020.\n\nIlke Demir, Krzysztof Koperski, David Lindenbaum, Guan Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia, and Ramesh Raskar. Deepglobe 2018: A challenge to parse the earth through satellite images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 172–181, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nDavid H Douglas and Thomas K Peucker. Algorithms for the reduction of the number of points required to represent a digitized line or its caricature. Cartographica: the international journal for geographic information and geovisualization, 10(2):112–122, 1973.\n\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint\n\narXiv:1805.04833, 2018.\n\nSongtao He, Favyen Bastani, Satvat Jagwani, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Mohamed M Elshrif, Samuel Madden, and Mohammad Amin Sadeghi. Sat2graph: Road graph extraction through graph-tensor encoding. In European Conference on Computer Vision, pp. 51–67. Springer, 2020.\n\nStefan Hinz and Albert Baumgartner. Automatic extraction of urban road networks from multi-view aerial imagery. ISPRS journal of photogrammetry and remote sensing, 58(1-2):83–98, 2003.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\n\ndegeneration. arXiv preprint arXiv:1904.09751, 2019.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: Convergence and gen-\n\neralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.\n\nPascal Kaiser, Jan Dirk Wegner, Aur ́elien Lucchi, Martin Jaggi, Thomas Hofmann, and Konrad Schindler. Learning aerial image segmentation from online maps. IEEE Transactions on Geoscience and Remote Sensing, 55(11):6054–6068, 2017.\n\nFanjie Kong, Bohao Huang, Kyle Bradbury, and Jordan Malof. The synthinel-1 dataset: a collection of high resolution synthetic overhead imagery for building segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1814–1823, 2020.\n\nNgan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, and Marios Savvides. Deep reinforcement learning in computer vision: a comprehensive survey. Artificial Intelligence Review, pp. 1–87, 2021.\n\nYuxia Li, Bo Peng, Lei He, Kunlong Fan, and Ling Tong. Road segmentation of unmanned aerial vehicle remote sensing images using adversarial network with multiscale context aggregation. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2279– 2287, 2019a.\n\nZuoyue Li, Jan Dirk Wegner, and Aur ́elien Lucchi. Topological map extraction from overhead In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n\nimages. 1715–1724, 2019b.\n\nRenbao Lian, Weixing Wang, Nadir Mustafa, and Liqin Huang. Road extraction methods in highresolution remote sensing images: A comprehensive review. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 13:5489–5507, 2020.\n\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431–3440, 2015.\n\nGell ́ert M ́attyus and Raquel Urtasun. Matching adversarial networks. In Proceedings of the IEEE\n\nConference on Computer Vision and Pattern Recognition, pp. 8024–8032, 2018.\n\nGellert Mattyus, Shenlong Wang, Sanja Fidler, and Raquel Urtasun. Enhancing road maps by parsIn Proceedings of the IEEE international conference on\n\ning aerial images around the world. computer vision, pp. 1689–1697, 2015.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nGell ́ert M ́attyus, Wenjie Luo, and Raquel Urtasun. Deeproadmapper: Extracting road topology from aerial images. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3438–3446, 2017.\n\nClara Meister, Afra Amini, Tim Viera, and Ryan Cotterell. Conditional poisson stochastic beam\n\nsearch. arXiv preprint arXiv:2109.11034, 2021.\n\nJavier A Montoya-Zegarra, Jan D Wegner, L’ubor Ladick`y, and Konrad Schindler. On the evaluation of higher-order cliques for road network extraction. In 2015 Joint Urban Remote Sensing Event (JURSE), pp. 1–4. IEEE, 2015.\n\nPhilipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, William Paul, Michael I Jordan, and Ion Stoica. Ray: A distributed framework for emerging ai applications. corr abs/1712.05889 (2017). arXiv preprint arXiv:1712.05889, 2017.\n\nAgata Mosinska, Pablo Marquez-Neila, Mateusz Kozi ́nski, and Pascal Fua. Beyond the pixel-wise loss for topology-aware delineation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3136–3145, 2018.\n\nCharlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In International Conference on Machine Learning, pp. 7220– 7229. PMLR, 2020.\n\nAlejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estima-\n\ntion. In European conference on computer vision, pp. 483–499. Springer, 2016.\n\nJoachim Niemeyer, Jan Dirk Wegner, Cl ́ement Mallet, Franz Rottensteiner, and Uwe Soergel. ConIn ISPRS\n\nditional random fields for urban scene classification with full waveform lidar data. Conference on Photogrammetric Image Analysis, pp. 233–244. Springer, 2011.\n\nA ̈aron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with pixelcnn decoders. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 4797–4805, 2016.\n\nOpenStreetMap contributors. Planet dump retrieved from https://planet.osm.org . https://www.\n\nopenstreetmap.org, 2017.\n\nWamiq Para, Paul Guerrero, Tom Kelly, Leonidas J Guibas, and Peter Wonka. Generative layout modeling using constraint graphs. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6690–6700, 2021a.\n\nWamiq Reyaz Para, Shariq Farooq Bhat, Paul Guerrero, Tom Kelly, Niloy Mitra, Leonidas Guibas, and Peter Wonka. Sketchgen: Generating constrained cad sketches. arXiv preprint arXiv:2106.02711, 2021b.\n\nTobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel Barth-Maron, Hado Van Hasselt, John Quan, Mel Veˇcer ́ık, et al. Observe and look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018.\n\nPengda Qin, Weiran Xu, and William Yang Wang. Robust distant supervision relation extraction via\n\ndeep reinforcement learning. arXiv preprint arXiv:1805.09927, 2018.\n\nUrs Ramer. An iterative procedure for the polygonal approximation of plane curves. Computer\n\ngraphics and image processing, 1(3):244–256, 1972.\n\nAbdallah Saffidine, Tristan Cazenave, and Jean M ́ehat. Ucd: Upper confidence bound for rooted\n\ndirected acyclic graphs. Knowledge-Based Systems, 34:26–33, 2012.\n\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140– 1144, 2018.\n\nSuriya Singh, Anil Batra, Guan Pang, Lorenzo Torresani, Saikat Basu, Manohar Paluri, and CV Jawahar. Self-supervised feature learning for semantic segmentation of overhead imagery. In BMVC, volume 1, pp. 4, 2018.\n\nEdward J Smith, Scott Fujimoto, Adriana Romero, and David Meger. Geometrics: Exploiting geo-\n\nmetric structure for graph-encoded objects. arXiv preprint arXiv:1901.11461, 2019.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nYong-Qiang Tan, Shang-Hua Gao, Xuan-Yi Li, Ming-Ming Cheng, and Bo Ren. Vecroad: Pointbased iterative graph exploration for road graphs extraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8910–8918, 2020.\n\nYuandong Tian, Jerry Ma, Qucheng Gong, Shubho Sengupta, Zhuoyuan Chen, James Pinkerton, and Larry Zitnick. Elf opengo: An analysis and open reimplementation of alphazero. In International Conference on Machine Learning, pp. 6244–6253. PMLR, 2019.\n\nBenigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In Inter-\n\nnational Conference on Machine Learning, pp. 467–475. PMLR, 2014.\n\nA ̈aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In 9th ISCA Speech Synthesis Workshop, pp. 125–125, 2016.\n\nAdam Van Etten, Dave Lindenbaum, and Todd M Bacastow. Spacenet: A remote sensing dataset\n\nand challenge series. arXiv preprint arXiv:1807.01232, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017.\n\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n\nPointer networks.\n\narXiv preprint\n\narXiv:1506.03134, 2015.\n\nNanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 52–67, 2018.\n\nShenlong Wang, Min Bai, Gellert Mattyus, Hang Chu, Wenjie Luo, Bin Yang, Justin Liang, Joel Cheverie, Sanja Fidler, and Raquel Urtasun. Torontocity: Seeing the world with a million eyes. arXiv preprint arXiv:1612.00423, 2016.\n\nJan D Wegner, Javier A Montoya-Zegarra, and Konrad Schindler. A higher-order crf model for road network extraction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1698–1705, 2013.\n\nChristian Wiedemann, Christian Heipke, Helmut Mayer, and Olivier Jamet. Empirical evaluation of automatically extracted road axes. Empirical evaluation techniques in computer vision, 12: 172–187, 1998.\n\nJingtao Xu, Yali Li, and Shengjin Wang. Adazoom: Adaptive zoom network for multi-scale object\n\ndetection in large scenes. arXiv preprint arXiv:2106.10409, 2021.\n\nZhenhua Xu, Yuxuan Liu, Lu Gan, Yuxiang Sun, Xinyu Wu, Ming Liu, and Lujia Wang. Rngdet: Road network graph detection by transformer in aerial images. IEEE Transactions on Geoscience and Remote Sensing, 60:1–12, 2022.\n\nWeirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games\n\nwith limited data. arXiv preprint arXiv:2111.00210, 2021.\n\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and\n\nAlexander Smola. Deep sets. arXiv preprint arXiv:1703.06114, 2017.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: APLS metric and perplexity (bits of information per edge) results for the SpaceNet dataset with provided key points that adequately capture the ground truth graph. Autoregressive order is defined in Section 3, while random order entails random permutation between edges and the order of key points within edges. To calculate perplexity for our method we use the initial predicted policy distribution, without any additional search.\n\nMetric\n\nMethod\n\nRandom Cls GCN\n\nARM Ours\n\nAPLS Bits per edge: Autoregressive order Bits per edge: Random order\n\n0.008 8.743 8.743\n\n0.310 0.488\n\n- -\n\n- -\n\n0.743 0.778 0.733 4.636 30.84 4.565\n\nA MORE EXPERIMENTS\n\nWe first assess the performance of our proposed method in an ideal scenario where the key points, correspond to the ones from the ground truth. To hinder training and inference, we insert additional key points as (1) random intermediate points between known edges and (2) randomly sampled locations in the images. Here, our assumption in Section 3 that the set V ′ suffices to generate the ground truth graph, holds by construction. We compare our method against several baselines that learn to connect edges between key points, using the same feature extraction pipeline, described in Section 3.1, as our model. Cls is a classification network that predicts for all pairs of key points a value {0, 1} corresponding to the existence of an edge. GCN implements a graph neural network that predicts directly the adjacency matrix. We also present an autoregressive version of our model ARM, that is trained with cross-entropy loss to predict the pre-defined ordered sequence of key points. We use this model to initialize ours. Results are presented in Table 3.\n\nAs expected, the ARM model achieves a low perplexity score when evaluated against the corresponding sequence, ordered according to the autoregressive order, but suffers in predicting the edges when in random order. The ARM underperforms because of frequent early terminations and the implicit inability to revisit key points, what the desired final metric is concerned, here APLS. Even though our model is developed upon this autoregressive model, it generates tokens in an arbitrary arrangement. Reward and value estimates enable a different training scheme that deeply correlates with the desired objective.\n\nB INTERPRETABILITY\n\nWe visualize attention (of the Transformer II module), using the attention flow proposed in Abnar & Zuidema (2020), in Fig. 9. To create attention scores per edge, we aggregate scores for the pair of tokens that define each edge. New predictions lay increased attention to already generated junctions, parallel road segments, and other edges belonging to the same road segment.\n\nFigure 9: We visualize attention by aggregating attention scores for key points that form the same edge. White denotes the latest edge added, for which the attention scores are calculated. Colours indicate the amount of attention on any current edge in the graph.\n\nWe also compare APLS results achieved by varying the difficulty of the ground truth images in terms of the total number of junctions (vertices with a degree greater than 2) and in terms of the average length of road segments that are present, in Fig. 10. Our method explicitly captures information re-\n\n14\n\n0.000.020.040.060.080.000.010.020.030.040.050.060.070.000.010.020.030.040.050.060.070.000.010.020.030.040.050.060.070.08Under review as a conference paper at ICLR 2023\n\nFigure 10: APLS improvement of our method, compared to the Orientation method Batra et al. (2019), based on images capturing a ground distance area of 400×400 meters of the SpaceNet dataset. (Left) We vary the number of junctions present, and (right) the average road segment length (in meters).\n\ngarding the degree of the key points during the search, while it can encode better global information, even across larger distances. It is not a surprise perhaps then, that it outperforms the baselines more convincingly as the difficulty of the ground truth road network increases.\n\nFinally, we visualize an example of an imagined rollout trajectory at a single step of our algorithm in Fig. 11. During a single inference step, our method uses tree search to look ahead into sequences of actions in the future. For our example, we have chosen a relatively smaller number of simulations (10) for better visual inspection. We also show the corresponding environment states reached, which are, however, not explicitly available to the model, as it is searching and planning using a learned model of the environment.\n\nC DATASET CREATION\n\nWe use CityEngine, a 3D modelling software for creating immersive urban environments. We generate a simple road network and apply a rural city texture on the created city blocks, provided by Kong et al. (2020). We then uniformly generate trees of varying height and size along the side walks of the generated streets. We then iteratively scan the generated city by passing a camera of specific orientation and height. We repeat the same process after suitable modifications to the texture, for the generation of the street masks, as well as the vegetation masks, that correspond to only the plants along the side walks. Some examples of the generated images are provided in Fig. 12. We note that additional occlusion can be caused by the relation of the camera with the 3D meshes corresponding to buildings. These occlusions are, however, not captured by our generated masks, and we can expect them to contribute partially to the fragmented segmentation results.\n\nWe train a segmentation-based model, LinkNet, as our baseline. We rasterize the ground truth graph to create pixel-level labels and train by maximizing the intersection over union, which is commonly done in practice. We note that there is a tradeoff between the nature of the predictions and the choice of the line-width with which the ground truth graph is rasterized. A large width achieves better results in terms of connectivity of the predicted graph but results in poorer accuracy in the final key points’ locations. Furthermore, when providing a large width, areas in the image with more uncertainty, e.g. vegetation that is not above a road segment, are also predicted as road networks with high certainty, leading to spurious, disconnected road segments. To highlight the advantages of our method compared to this baseline and in order to promote more meaningful predictions, we select a relatively smaller width.\n\n15\n\n0 - 11 - 22 - 33 - 44 - 55 - 66 - 77 - 88 - 99 - 1010 - 1212 - 1414 - 1616 - 1818 - 2020 - 2525 - 30>=30Number of junctions in the ground truth graph0.030.020.010.000.010.020.030.040.05Relative APLS improvement of RL compared to Segmentationlinear fit 0 - 3030 - 6060 - 9090 - 120120 - 150150 - 180180 - 210210 - 240240 - 270270 - 300300 - 330330 - 360360 - 390>=390Average path lengths in the ground truth graph0.000.010.020.030.040.05Relative APLS improvement of RL compared to Segmentationlinear fit Under review as a conference paper at ICLR 2023\n\nFigure 11: Example of a ”dreamt” tree search of our model (we recommend zooming in for more details). Here, images correspond to the environment observations by following the respective sequence of actions, which are, however, not given as input to the model but only portrayed for visual purposes. The model has only access to the original observation and ”dreamt” trajectories on the learned latent space. The root of the search tree is indicated by the colour red. Orange nodes correspond to the children that attain the highest estimated value. We also provide reward and value estimates of our model based on the current latent representation. We perform a smaller number of simulations into the future, for visual purposes.\n\nD ARCHITECTURE DETAILS\n\nAs an image backbone model, we use a ResNet-18 for the synthetic dataset and a ResNet-50 for the real dataset experiments. We extract features at four different scales, after each of the 4 ResNet layers. To extract features for each key point, we interpolate the backbone feature maps based on the key points’ locations. We use different learned embeddings based on the actual key points’ locations. For the key points embedding model, we use a transformer encoder with 16 self-attention layers and a dropout rate of 0.15. We use layer normalization and GELU activation functions.\n\nFor the edge-embeddings model, we use the respective key points embedding, along with learned position and type embeddings, which we all sum together. As aforementioned, we can initialize the current edge sequence based on previous predictions, allowing our model to refine any initial prediction provided. Again, we use the same transformer architecture with 16 self-attention layers, and a dropout rate of 0.15.\n\n16\n\nAction: NoneValue: 0.16Visit count: 10Prior: 0.00Reward: 0.00Actions: Action: 5Value: 0.10Visit count: 3Prior: 0.10Reward: 0.00Actions: 5Action: 9Value: 0.18Visit count: 7Prior: 0.68Reward: 0.00Actions: 9Action: 9Value: 0.08Visit count: 2Prior: 0.32Reward: 0.02Actions: 5-9Action: 9Value: 0.08Visit count: 1Prior: 0.24Reward: 0.00Actions: 5-9-9Action: 7Value: 0.15Visit count: 6Prior: 0.60Reward: 0.05Actions: 9-7Action: 8Value: 0.15Visit count: 5Prior: 0.31Reward: 0.00Actions: 9-7-8Action: 6Value: 0.10Visit count: 4Prior: 0.94Reward: 0.07Actions: 9-7-8-6Action: 3Value: 0.10Visit count: 1Prior: 0.14Reward: 0.00Actions: 9-7-8-6-3Action: 5Value: 0.10Visit count: 1Prior: 0.19Reward: 0.00Actions: 9-7-8-6-5Action: 8Value: 0.09Visit count: 1Prior: 0.17Reward: 0.00Actions: 9-7-8-6-8Under review as a conference paper at ICLR 2023\n\nFigure 12: Samples from the synthetic dataset. We generate images and the corresponding street mask, overlaid with the colour red, along with the masks of plants that are occluding the ground truth road network, overlaid with the colour green.\n\nFinally, the architecture of the dynamics network and the value prediction network are shown in Fig. 13. For the value estimation, we also provide the current environment step, as we execute steps in an environment with a bounded time horizon.\n\nFigure 13: Architecture details of the dynamics network and the value prediction network. Reward and value are determined by predicting support of size 601. Final scalar values are calculated by an invertible transform of this support, similar to Pohlen et al. (2018).\n\n17\n\nLinear (512256)GELULinear (256256)GELULinear (256256)GELULinear (256601)Hidden stateCurrent stepLinear (256256)GELULinear (256256)GELULinear (256 256)GELULinear (256 256)Key pointembeddingVertex typeembeddingEdge lengthembeddingLinear (256 256)Linear (256 601)Next hidden stateReward estimateValue estimatePrevious hiddenstateLSTM Input size: 1024 Hidden size: 512 Output_size: 256 Under review as a conference paper at ICLR 2023\n\nFigure 14: Example images sampled from the four cities of the SpaceNet dataset. Images from different cities exhibit different regularity in their road networks. The quality of the overhead satellite images may also vary.\n\nE IMPLEMENTATION DETAILS\n\nE.1 EVALUATION METRICS\n\nAPLS (Van Etten et al., 2018) constitutes a graph theoretic metric that faithfully describes routing properties. APLS is defined as\n\nAPLS = 1 −\n\n1 Np\n\n(cid:88)\n\n(cid:26)\n\nmin\n\n1,\n\npv1v2 <∞\n\n|pv1v2 − pv1\n\n′v′ 2\n\npv1v2\n\n(cid:27)\n\n|\n\n,\n\n(4)\n\nwhere v and v′ denote a source node and its closest point on the predicted graph if such exists within a buffer. Np denotes the number of paths sampled and pv1v2 the length of the shortest path between two nodes. Similarly, the Too Long Too Short (TLTS) metric (Wegner et al., 2013) compares lengths of the shortest paths between randomly chosen points of the two graphs, classifying them as infeasible, correct, or too-long or too-short (2l+2s) if the length of the path on the predicted graph does not differ by more than a threshold (5%) compared to the ground truth path. Since small perturbations to the predicted graph can have larger implications to pixel-level predictions, the definitions of precision, recall and intersection over union were relaxed in Wiedemann et al. (1998); Wang et al. (2016) leading to the metrics Correctness/Completeness/Quality (CCQ).\n\nStill, some types of errors, such as double roads or over-connections, are not penalized from the above metrics (Citraro et al., 2020). We therefore additionally include new metrics introduced in Citraro et al. (2020) that compare Paths, Junctions and Sub-graphs of the graphs in question, producing respectively precision, recall and f1 scores. For the final similarity score used in Eq. 3, we use a linear combination of the aforementioned metrics, more details are available in the supplementary material.\n\nE.2 DATASET INFORMATION\n\nWe use the following datasets to train our models, i.e. baselines and our newly proposed RL agent.\n\nSpaceNet (Van Etten et al., 2018) includes a road network of over 8000 Km over four different cities: Vegas, Paris, Shanghai, and Khartoum, where the complexity and quality, and regularity of the road network depend on the city of origin. Satellite images are provided at a pixel resolution of 1300 × 1300, corresponding to a ground resolution of 30cm per pixel. We split the 2780 total images into crops of size 400×400 with an overlap of 100 pixels for training. To better highlight the diversity of the satellite images from these four different locations, we have included some randomly sampled examples in Fig. 14.\n\nDeepGlobe (Demir et al., 2018) contains satellite images from 3 different locations with pixellevel annotations. Images have a resolution of 1024 × 1024, with a ground resolution of 50cm per pixel. We crop the 6226 images into tiles, leading to a similar ground truth resolution per pixel compared to SpaceNet.\n\n18\n\nVegasParisShanghaiKhartoumUnder review as a conference paper at ICLR 2023\n\nFigure 15: Example of how key points are generated. We start by evaluating a segmentation model (left) and extracting the predicted graph (middle). We then over-sample vertices along edges to enlarge the action space (right).\n\nE.3 TRAINING DETAILS\n\nAt each MCTS search step, we perform several simulations from the root state s0 for a number of steps k = 1, . . . and select an action that maximizes the upper confidence bound (Silver et al., 2018),\n\n(cid:34)\n\nak = arg max\n\nQ(s, a) + P (s, a)\n\na\n\n(cid:112)(cid:80)\n\nb N (s, b) 1 + N (s, a)\n\n(cid:18)\n\nc1 + log\n\n(cid:18) (cid:80)\n\nb N (s, b) + c2 + 1\n\nc2\n\n(cid:19)(cid:19)(cid:35)\n\n,\n\nwhere N (s, a), Q(s, a), P (s, a) corresponds to the visit counts, mean values and policies, as calculated by the current search statistics. Constants c1, c2 balance exploration and exploitation. Based on a state sk−1 and a selected action ak, a new state sk and reward ˆrk are estimated through the dynamics network. We update the mean values based on bootstrapped values of the estimated value functions and rewards. We experimented with training the reward and value support predictions with both mean squared error (MSE) and cross-entropy loss. We opted for MSE because of its stability. For a more in depth description of the training scheme of MuZero we recommend Schrittwieser et al. (2020) and Ye et al. (2021).\n\nAs hinted in the main text, we train using intermediate rewards, a linear combination of topological metrics. We experimented using a variety of different scores and metrics, but ended up using APLS, Path-based f1, Junction-based f1 and Sub-graph-based f1 at a relative scale of (0.35, 0.25, 0.25, 0.15). We found the Sub-graph-based f1 to be more sensitive to small perturbations and therefore weighted it less in the final combination. The metrics mentioned above are highly correlated, as examined in Batra et al. (2019). This correlation, though, holds when comparing the final predictions. Intermediate incremental rewards are more independent, so we still found it useful to use a mixture of them. Initially, to let our network learn basic stable rewards, we use the segmentation prediction mask as target. That means that we train our model to predict the graph that can be extracted after post-processing the segmentation model’s prediction.\n\nAfter pre-training the autoregressive model, we experimented with fine-tuning using RL with two different learning rates, where a slower by a factor (0 − 1] rate was chosen for the pre-trained modules. Here, we noticed that the model still performed better than the ARM baseline. As it has trouble though to escape the autoregressive order, compared to the single learning rate model, results are less optimal.\n\nWe finally note that by avoiding type and position encoding in the Transformer II module, we can ensure the embedded graph is permutation invariant regarding the sequence of edges and the order of key points within an edge. Our search graph can then be formulated into a directed acyclic graph, circumventing unnecessary division of the search space (Browne et al., 2012; Childs et al., 2008), enabling more efficient sampling (Saffidine et al., 2012). These updated search statistics are cumbersome to compute, though, and we found no significant efficiency improvement. They do, however, confirm our model’s potential ability to handle the input graph as an unordered set, as the problem suggests.\n\nE.4 PRODUCING KEY POINTS\n\nWe initially train a segmentation model for predicting pixel-level accurate masks of the road network. For this step, we can use any model from the literature. We extract the predicted graph by\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nskeletonizing the predicted mask and simplifying the graph by a smoothing threshold. We then sample intermediate vertices along the largest in terms of ground length edges, to enlarge the action space. We illustrate a toy example of such a process in Fig. 15. To accelerate inference, we can also initialize our prediction graph based on the provided segmentation mask. In such a case, our method closer resembles previous refinement approaches. We additionally remove edges of connected components with small overall size and edges belonging to roads segments leading to dead ends (that means vertices of degree one), keeping though the corresponding key points in the environment state. Thus, if our model deems the existence of the respective edges necessary, it can add them once more. We plan to further investigate augmenting the action space with the ability to remove edges in future work, that would not require such a pre-processing strategy.\n\nE.5 COMBINING PREDICTIONS\n\nWhen creating the final per image prediction, we initially simply generated predictions on nonoverlapping patches and fused them together. To overcome small pixel location differences in the predicted graphs, we fuse by rasterizing the individual graphs in the pixel domain with a line width larger than 1. What we found more successful was to perform inference on overlapping patches and to initialize the currently predicted graph based on the predictions made so far. This is particularly useful, as road segments are often close to the boundaries of our cropped image. Individual inference and simple fuse can often lead to over-connected predictions. We visualize a toy example of such a process in Fig. 16.\n\nFigure 16: Toy example of how inference is performed in larger images. We start by cropping the image to overlapping patches. (Top) Naive fusion leads to over-connections because of perturbation in the key points’ locations. (Bottom) We initialize subsequent graph predictions based on the key points and edge predictions of the so far generated output from previous patches if such exists.\n\nFor the segmentation baselines, unless specified in their respective documentation, we perform inference by cropping images to overlapping patches and normalizing the final predicted mask based on the number of overlapping predictions per pixel location. We also pad images around their boundary, as done in Acuna et al. (2019). We note some small differences in the final scores for the Orientation model (Batra et al., 2019) and the SpaceNet dataset, compared to the ones in Citraro et al. (2020). We assume these are an outcome of different chosen parameters for the calculation of metrics. We keep these parameters fixed when calculating scores for all methods.\n\n20\n\nInferenceSimple FusionRasterizeExtract GraphUnder review as a conference paper at ICLR 2023\n\nE.6 MORE COMPARISONS WITH BASELINES\n\nWe elaborate more on the evaluation method on Sat2Graph. The authors provided predictions corresponding only to a center crop of the original SpaceNet dataset images. For each 400 × 400 pixel image, predictions are made for the center 352 × 352 area of the image. One could expect slightly better results if trained in the same conditions but that the gap does still seem large enough to show the merits of our approach.\n\nOther baselines like Neural turtle graphics (Chu et al., 2019) and Topological Map Extraction (Li et al., 2019b) do not have an implementation available. We do not compare against VecRoad (Tan et al., 2020) or RoadTracer (Bastani et al., 2018), as different datasets were used for the current evaluations. These baselines have been already shown to underperform though in the literature, by methods that we are comparing against.\n\nF MORE EXAMPLES\n\nWe showcase in Fig. 17 and Fig. 18 more examples of the environment state progression, for the synthetic dataset.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 17: Example of an environment progression for the synthetic dataset. Key points’ locations are shown in blue. By over-sampling initial segmentation predictions as shown in Fig. 15, we can generate key points in possibly occluded areas of the image.\n\n22\n\nStarting state12345678910111213141516Last action: 112345678910111213141516Last action: 212345678910111213141516Last action: 1612345678910111213141516Last action: 1112345678910111213141516Last action: 1512345678910111213141516Last action: 1412345678910111213141516Last action: 412345678910111213141516Last action: 312345678910111213141516Last action: 412345678910111213141516Last action: 512345678910111213141516Last action: 412345678910111213141516Last action: 612345678910111213141516Last action: 612345678910111213141516Last action: 812345678910111213141516Last action: 512345678910111213141516Last action: 712345678910111213141516Last action: 1412345678910111213141516Last action: 1312345678910111213141516Last action: 812345678910111213141516Last action: 1312345678910111213141516Last action: 912345678910111213141516Last action: 1112345678910111213141516Last action: 912345678910111213141516Last action: 712345678910111213141516Last action: 1212345678910111213141516Last action: 1312345678910111213141516Last action: 1012345678910111213141516Last action: 1212345678910111213141516Last action: 212345678910111213141516Last action: 312345678910111213141516Last action: 1012345678910111213141516Last action: 912345678910111213141516Last action: 0, terminated12345678910111213141516Under review as a conference paper at ICLR 2023\n\nFigure 18: Example of an environment progression for the synthetic dataset. Generating the same edge twice (between key points 6 and 7), although unintuitive, does not lead to a different final predicted graph.\n\n23\n\nStarting state1234567891011121314151617Last action: 161234567891011121314151617Last action: 141234567891011121314151617Last action: 111234567891011121314151617Last action: 91234567891011121314151617Last action: 131234567891011121314151617Last action: 141234567891011121314151617Last action: 11234567891011121314151617Last action: 41234567891011121314151617Last action: 21234567891011121314151617Last action: 31234567891011121314151617Last action: 171234567891011121314151617Last action: 151234567891011121314151617Last action: 61234567891011121314151617Last action: 71234567891011121314151617Last action: 61234567891011121314151617Last action: 41234567891011121314151617Last action: 61234567891011121314151617Last action: 71234567891011121314151617Last action: 31234567891011121314151617Last action: 51234567891011121314151617Last action: 81234567891011121314151617Last action: 71234567891011121314151617Last action: 81234567891011121314151617Last action: 91234567891011121314151617Last action: 61234567891011121314151617Last action: 51234567891011121314151617Last action: 61234567891011121314151617Last action: 101234567891011121314151617Last action: 121234567891011121314151617Last action: 141234567891011121314151617Last action: 101234567891011121314151617Last action: 121234567891011121314151617Last action: 151234567891011121314151617Last action: 141234567891011121314151617Last action: 0, terminated1234567891011121314151617",
    "reference": "# Summary Of The Paper\n\nThe authors propose a reinforcement learning approach to road vectorization. In contrast with previous works, the task is modeled as generating a graph as a variable-length edge sequence. \n\nThe pipeline consists of multiple stages: semantic segmentation from RGB images (this results in binary maps), a transformer-based autoregression model (results in a collection of road graphs) and the RL part- muZero adapted for road graph extraction. It achieves competitive performance on DeepGlobe and SpaceNet datasets.\n\nThe authors also introduce a synthetic dataset that helps pretraining the autoregression model (need to clarify with the authors on that).\n\n# Strength And Weaknesses\n\n### Strengths\n- competitive performance on two public datasets\n- interesting synthetic dataset with path occlusion difficulty analysis\n\n### Weaknesses\n- more recent [relevant] related work that yields much improved results such as RNGDet (ignored) or VecRoad (different split, mentioned but ignored) is not compared against; what do you mean by \"these baselines have been already shown to underperform though in the literature\"? Who has shown that and where?\n- small performance improvements on both datasets, seems to struggle at times, sometimes beaten by a 2017 paper\n- convoluted pipeline, RGB segmentation >> point extraction from sampling >> auto-regression model >> muZero; feels like the whole system was glued in place to fix the problems from the previous step (e.g., the sampled keypoints are not perfect, throw in the auto-regression transformer model; its graphs are not that great, throw in the RL to figure out which graphs are usable); why not try predicting the graph directly?\n- using RL for navigation / computing routes is not novel; for example, [1*] presented a similar concept, but with street views instead of satellite images.\n\n___\n[1*]Mirowski, P., Grimes, M., Malinowski, M., Hermann, K. M., Anderson, K., Teplyashin, D., ... & Hadsell, R. (2018). Learning to navigate in cities without a map. Advances in neural information processing systems, 31.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity/quality: \n\nAs previously stated, the pipeline consist of a number of methods mashed together. Even an overview is missing - Fig 1 describes muZero modeling, which is a tiny bit of the whole pipeline. IMHO the authors need to test a modern architecture, such as RNGDet and check if RL improves on top of that. If not, well, RL is not the way to go for this task. Especially looking at the RNGNet results, I do not see compelling evidence the authors would be able to come close to those numbers, even with the complex system they built on top of the (already existing) graphs.\n\nThe paper has only minor text issues - e.g., Fig 6. \"imporovement\" x2, it is well written and structured. \n\nNovelty:\n\nAll in all, I do not think this paper in its current form is ICLR-worthy. It is more of an engineering work that struggles to marginally improve the results. Judging from the ablation study, they start from a terrible baseline, which again, in 2022, it is unacceptable. If my calculator is right, the 0.455 ALPS is the method without any improvements. Why? If the focus is the RL agent, why not start with the best graphs possible and improve over them? Is it something here that I am missing?\n\nThe synthetic dataset could be used to improve the results and show a compelling advantage for RL, but it's not. In fact, it's not even clear on how it's used, they claim the generation of a 'difficulty' score and training with LinkNet [a 2017 method], but only Figure 6 supports this claim, and no other attempts to find a meaningful use are described (e.g., is the autoregressive pretrained done on this dataset? What about other ideas, such as style transfer, or use to train the RL agent? Or is it this already done but not mentioned?).\n\nFew recent approaches have attempted to upgrade the algorithms to modern pipelines (e.g., directly output vertices similar to [1*,2*], without intermediate keypoints/road segmentation, as in RNGDet/RNGDet++[3*- this paper is here only for future reference]), it would have been nice to adapt the framework, but no efforts were made.\n\nReproducibility:\n\nThe code is released in the supplementary material, it should be easy to reproduce the experiments.\n\n___\n[1*]Lazarow, J., Xu, W., & Tu, Z. (2022). Instance Segmentation With Mask-Supervised Polygonal Boundary Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4382-4391).\n\n[2*]Liang, J., Homayounfar, N., Ma, W. C., Xiong, Y., Hu, R., & Urtasun, R. (2020). Polytransform: Deep polygon transformer for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9131-9140). \n\n[3*]Xu, Z., Liu, Y., Sun, Y., Liu, M., & Wang, L. (2022). RNGDet++: Road Network Graph Detection by Transformer with Instance Segmentation and Multi-scale Features Enhancement. arXiv preprint arXiv:2209.10150.\n\n# Summary Of The Review\n\nThe paper provides an interesting alternative to all-in-one transformer-based architectures, but the latter are not compared and show significant improvements compared to (Batra et al., 2019), even though the split is different - RNGDet ( Xu et al., 2022)] or at least VectorNet (Tan et al., 2020). The ablation study is worrying to say the least - for an unknown reason, they start with a very poor baseline, despite the focus of the paper being the novel RL graph fixing method. I am waiting for the authors to clarify on this, I hope I haven't understood something right.\n\nThat being said, it proposes an interesting synthetic datasets approach for modelling occlusions/ambiguity. Nevertheless, it is underexploited - not clear why an old road detection was trained on it and how it helps the RL. It was only used in Figure 6 to check that LinkNet (a 5 year old method) is worse?\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nWHAT DO VISION TRANSFORMERS LEARN? A VISUAL EXPLORATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nVision transformers (ViTs) are quickly becoming the de-facto architecture for computer vision, yet we understand very little about why they work and what they learn. While existing studies visually analyze the mechanisms of convolutional neural networks, an analogous exploration of ViTs remains challenging. In this paper, we first address the obstacles to performing visualizations on ViTs. Assisted by these solutions, we observe that neurons in ViTs trained with language model supervision (e.g., CLIP) are activated by semantic concepts rather than visual features. We also explore the underlying differences between ViTs and CNNs, and we find that transformers detect image background features, just like their convolutional counterparts, but their predictions depend far less on high-frequency information. On the other hand, both architecture types behave similarly in the way features progress from abstract patterns in early layers to concrete objects in late layers. In addition, we show that ViTs maintain spatial information in all layers except the final layer. In contrast to previous works, we show that the last layer most likely discards the spatial information and behaves as a learned global pooling operation. Finally, we conduct large-scale visualizations on a wide range of ViT variants, including DeiT, CoaT, ConViT, PiT, Swin, and Twin, to validate the effectiveness of our method.\n\n1\n\nINTRODUCTION\n\nRecent years have seen the rapid proliferation of vision transformers (ViTs) across a diverse range of tasks from image classification to semantic segmentation to object detection (Dosovitskiy et al., 2020; He et al., 2021; Dong et al., 2021; Liu et al., 2021; Zhai et al., 2021; Dai et al., 2021). Despite their enthusiastic adoption and the constant introduction of architectural innovations, little is known about the inductive biases or features they tend to learn. While feature visualizations and image reconstructions have provided a looking glass into the workings of CNNs (Olah et al., 2017; Zeiler & Fergus, 2014; Dosovitskiy & Brox, 2016), these methods have shown less success for understanding ViT representations, which are difficult to visualize. In this work we show that, if properly applied to the correct representations, feature visualizations can indeed succeed on VITs. This insight allows us to visually explore ViTs and the information they glean from images.\n\nEdges\n\nTextures\n\nPatterns\n\nParts\n\nObjects\n\nFigure 1: The progression for visualized features of ViT B-32. Features from early layers capture general edges and textures. Moving into deeper layers, features evolve to capture more specialized image components and finally concrete objects.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nIn order to investigate the behaviors of vision transformers, we first establish a visualization framework that incorporates improved techniques for synthesizing images that maximally activate neurons. Through dissecting and visualizing the internal representations in the transformer architecture, we find that patch tokens preserve spatial information, even in individual channels, throughout all layers except the last attention block. The last layer of ViTs learns a token-mixing operation akin to average pooling, such that the classification head exhibits comparable accuracy when ingesting a random token instead of the CLS token.\n\nAfter probing the role of spatial information, we delve into the behavioral differences between ViTs and CNNs. When performing activation maximizing visualizations, we notice that ViTs consistently generate higher quality image backgrounds than CNNs. Thus, we try masking out image foregrounds during inference, and find that ViTs consistently outperform CNNs when exposed only to image backgrounds. These findings bolster the observation that transformer models extract information from many sources in an image to exhibit superior performance on out-of-distribution generalization (Paul & Chen, 2021) as well as adversarial robustness (Shao et al., 2021). Additionally, convolutional neural networks are known to rely heavily on high-frequency texture information in images (Geirhos et al., 2018). In contrast, we find that ViTs perform well even when high-frequency content is removed from their inputs.\n\nWe further visualize the effects of language model supervision, i.e. CLIP (Radford et al., 2021), on the features extracted by vision transformers. While both ImageNet-trained ViTs and CLIP-trained vision transformers possess neurons that are activated by visual features (e.g. shapes and colors) and distinct classes, the neurons of CLIP-trained vision transformers are also activated by features that do not represent physical objects, such as visual characteristics relating to parts of speech (e.g. epithets, adjectives, and prepositions) or broader concepts such as morbidity.\n\nOur contributions are summarized as follows:\n\nI. We observe that uninterpretable and adversarial behavior occurs when applying standard methods of feature visualization to the relatively low-dimensional components of transformer-based models, such as keys, queries, or values. However, applying these tools to the relatively high-dimensional features of the position-wise feedforward layer results in successful and informative visualizations. We conduct large-scale visualizations on a wide range of transformer-based vision models, including ViTs, DeiT, CoaT, ConViT, PiT, Swin, and Twin, to validate the effectiveness of our method.\n\nII. We show that patch-wise image activation patterns for ViT features essentially behave like saliency maps, highlighting the regions of the image a given feature attends to. This behavior persists even for relatively deep layers, showing the model preserves the positional relationship between patches instead of using them as global information stores.\n\nIII. We compare the behavior of ViTs and CNNs, finding that ViTs make better use of background information and rely less on high-frequency, textural attributes. Both types of networks build progressively more complex representations in deeper layers and eventually contain features responsible for detecting distinct objects.\n\nIV. We investigate the effect of natural language supervision with CLIP on the types of features extracted by ViTs. We find CLIP-trained models include various features clearly catered to detecting components of images corresponding to caption text, such as prepositions, adjectives, and conceptual categories.\n\n2 RELATED WORK\n\n2.1 OPTIMIZATION-BASED VISUALIZATION\n\nOne approach to understanding what models learn during training is using gradient descent to produce an image which conveys information about the inner workings of the model. This has proven to be a fruitful line of work in the case of understanding CNNs specifically. The basic strategy underlying this approach is to optimize over input space to find an image which maximizes a particular attribute of the model. For example, Erhan et al. (2009) use this approach to visualize images which maximally activate specific neurons in early layers of a network, and Olah et al. (2017) extend this to neurons, channels, and layers throughout a network. Simonyan et al. (2014); Yin et al. (2020) produce images\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nwhich maximize the score a model assigns to a particular class. Mahendran & Vedaldi (2015) apply a similar method to invert the feature representations of particular image examples.\n\nRecent work Ghiasi et al. (2021) has studied techniques for extending optimization-based class visualization to ViTs. We incorporate and adapt some of these proposed techniques into our scheme for feature visualization.\n\n2.2 OTHER VISUALIZATION APPROACHES\n\nAside from optimization-based methods, many other ways to visualize CNNs have been proposed. Dosovitskiy & Brox (2016) train an auxiliary model to invert the feature representations of a CNN. Zeiler & Fergus (2014) use ‘deconvnets’ to visualize patches which strongly activate features in various layers. Simonyan et al. (2014) introduce saliency maps, which use gradient information to identify what parts of an image are important to the model’s classification output. Zimmermann et al. (2021) demonstrate that natural image samples which maximally activate a feature in a CNN may be more informative than generated images which optimize that feature. We draw on some aspects of these approaches and find that they are useful for visualizing ViTs as well.\n\n2.3 UNDERSTANDING VITS\n\nGiven their rapid proliferation, there is naturally great interest in how ViTs work and how they may differ from CNNs. Although direct visualization of their features has not previously been explored, there has been recent progress in analyzing the behavior of ViTs. Paul & Chen (2021); Naseer et al. (2021); Shao et al. (2021) demonstrate that ViTs are inherently robust to many kinds of adversarial perturbations and corruptions. Raghu et al. (2021) compare how the internal representation structure and use of spatial information differs between ViTs and CNNs. Chefer et al. (2021) produce ‘image relevance maps’ (which resemble saliency maps) to promote interpretability of ViTs. Park & Kim (2022) demonstrates that multi-head self-attention are low-pass filters, but Convs are high-pass filters. They also examined the effect of removing a specific frequency and concluded that ViTs are resistant to high-frequency removal and CNNs are resistant to low-frequency removal.\n\n3 VIT FEATURE VISUALIZATION\n\nLike many visualization techniques, we take gradient steps to maximize feature activations starting from random noise (Olah et al., 2017). To improve the quality of our images, we penalize total variation (Mahendran & Vedaldi, 2015), and also employ the Jitter augmentation (Yin et al., 2020), the ColorShift augmentation, and augmentation ensembling (Ghiasi et al., 2021). Finally, we find that Gaussian smoothing facilitates better visualization in our experiments as is common in feature visualization (Smilkov et al., 2017; Cohen et al., 2019).\n\nEach of the above techniques can be formalized as follows. A ViT represents each patch p (of an input x) at layer l by an array Al,p with d entries. We define a feature vector f to be a stack composed of one entry from each of these arrays. Let fl,i be formed by concatenating the ith entry in Al,p for all patches p. This vector f will have dimension equal to the number of patches. The optimization objective starts by maximizing the sum of the entries of f over inputs x. The main loss is then\n\nLmain(x, l, i) =\n\n(fl,i)p.\n\n(1)\n\np (cid:88)\n\nWe employ total variation regularization by adding the term λT V (x) to the objective. T V represents the total variation, and λ is the hyperparameter controlling the strength of its regularization effect. We can ensemble augmentations of the input to further improve results. Let A define a distribution of augmentations to be applied to the input image x, and let a be a sample from A. To create a minibatch of inputs from a single image, we sample several augmentations {ak} from A. Finally, the optimization problem is:\n\nx∗ = arg max\n\nx\n\nk (cid:88)\n\nLmain(ak(x), l, i) + λT V (ak(x)).\n\n(2)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 2: (a): Example feature visualization from ViT feed forward layer. Left: Image optimized to maximally activate a feature from layer 5. Center: Corresponding maximally activating ImageNet example. Right: The image’s patch-wise activation map. (b): A feature from the last layer most activated by shopping carts.\n\nWe achieve the best visualizations when A is GS(CS(Jitter(x))), where GS denotes Gaussian smoothing and CS denotes ColorShift, whose formulas are:\n\nGS(x) = x + ε; ε ∼ N (0, 1)\n\nCS(x) = σx + μ; μ ∼ U(−1, 1); σ ∼ eU (−1,1).\n\nNote that even though ε and μ are both additive noise, they act on the input differently since μ is applied per channel (i.e. has dimension three), and ε is applied per pixel. For more details on hyperparameters, refer to Appendix B.\n\nTo better understand the content of a visualized feature, we pair every visualization with images from the ImageNet validation/train set that most strongly activate the relevant feature. Moreover, we plot the feature’s activation pattern by passing the most activating images through the network and showing the resulting pattern of feature activations. Figure 2(a) is an example of such a visualization. From the leftmost panel, we hypothesize that this feature corresponds to gravel. The most activating image from the validation set (middle) contains a lizard on a pebbly gravel road. Interestingly, the gravel background lights up in the activation pattern (right), while the lizard does not. The activation pattern in this example behaves like a saliency map (Simonyan et al., 2014), and we explore this phenomenon across different layers of the network further in Section 4.\n\nkey\n\nquery\n\nL1 F0 L1 F1 L1 F2 L11 F0 L11 F1 L11 F2\n\nThe model we adopt for the majority of our demonstrations throughout the paper is ViT-B16, implemented based on the work of Dosovitskiy et al. (2020). In addition, in the Appendix, we conduct large-scale visualizations on a wide range of ViT variants, including DeiT Touvron et al. (2021a), CoaT Xu et al. (2021), ConViT d’Ascoli et al. (2021), PiT Heo et al. (2021), Swin Liu et al. (2021), and Twin Chu et al. (2021), 38 models in total, to validate the effectiveness of our method. ViT-B16 is composed of 12 blocks, each consisting of multi-headed attention layers, followed by a projection layer for mixing attention heads, and finally followed by a positionwise-feed-forward layer. For brevity, we henceforth refer to the position-wise-feedforward layer simply as the feed-forward layer. In this model, every patch is always represented by a vector of size 768 except in the feed-forward layer which has a size of 3072 (4 times larger than other layers).\n\nFigure 3: Left: Visualization of key, query, and value. The visualization both fails to extract interpretable features and to distinguish between early and deep layers. High-frequency patterns and adversarial behavior dominate. Right: ViT feed forward layer. The first linear layer increases the dimension of the feature space, and the second one brings it back to its initial dimension.\n\nvalue\n\nWe first attempt to visualize features of the multi-headed attention layer, including visualization of the keys, queries, and values, by performing activation maximization. We find that the visualized feed-forward features are significantly more interpretable than other layers. We attribute this difficulty\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nL1 F31\n\nL4 F9\n\nL6 F2\n\nL7 F37\n\nL8 F19\n\nL11 F16\n\ns r\nu O\n\nl a\nV\n\nFigure 5: Feature activation maps in internal layers can effectively segment the contents of an image with respect to a semantic concept. For each image triple, the visualization on top shows the result of our method, the image on the bottom left is the most activating image from the validation set and the image on the bottom right shows the activation pattern.\n\nof visualizing other layers to the property that ViTs pack a tremendous amount of information into only 768 features, (e.g. in keys, queries, and values) which then behave similar to multi-modal neurons, as discussed by Goh et al. (2021), due to many semantic concepts being encoded in a low dimensional space. Furthermore, we find that this behaviour is more extreme in deeper layers. See Figure 3 for examples of visualizations of keys, queries and values in both early and deep layers of the ViT. Inspired by these observations, we visualize the features within the feed-forward layer across all 12 blocks of the ViT. We refer to these blocks interchangeably as layers.\n\nThe feed-forward layer depicted in Figure 4 takes an input of size d = 768, projects it into a t = 4 times higher dimensional space, applies the non-linearity GELU, and then projects back to d dimensional space. Unless otherwise stated, we always visualize the output of the GELU layers in our experiments. We hypothesize that the network exploits these high-dimensional spaces to store relatively disentangled representations. On the other hand, compressing the features into a lower dimensional space may result in the jumbling of features, yielding uninterpretable visualizations.\n\n4 LAST-LAYER TOKEN MIXING\n\nIn this section, we investigate the preservation of patch-wise spatial information observed in the visualizations of patch-wise feature activation levels which, as noted before, bear some similarity to saliency maps. Figure 2(a) demonstrates this phenomenon in layer 5, where the visualized feature is strongly activated for almost all rocky patches but not for patches that include the lizard. Additional examples can be seen in Figure 5 and the Appendix, where the activation maps approximately segment the image with respect to some relevant aspect of the image. We find it surprising that even though every patch can influence the representation of every other patch, these representations remain local, even for individual channels in deep layers in the network. While a similar finding for CNNs, whose neurons may have a limited receptive field, would be unsurprising, even neurons in the first layer of a ViT have a complete receptive field. In other words, ViTs learn to preserve spatial information, despite lacking the inductive bias of CNNs. Spatial information in patches of deep layers has been explored in Raghu et al. (2021) through the CKA similarity measure, and we further show that spatial information is in fact present in individual channels.\n\nFigure 4: ViT feed forward layer. The first linear layer increases the dimension of the feature space, and the second one brings it back to its initial dimension.\n\nThe last layer of the network, however, departs from this behavior and instead appears to serve a role similar to average pooling. Figure 2(b) shows one example of our visualizations for a feature from the last layer that is activated by shopping carts. The activation pattern is fairly uniform across the image. For classification purposes, ViTs use a fully connected layer applied only on the class token\n\n5\n\nFC1Input Shape: (b, p, d) Output Shape: (b, p, t x d) GELUFC2Input Shape: (b, p, t x d) Output Shape: (b, p, d) +Under review as a conference paper at ICLR 2023\n\n(the CLS token). It is possible that the network globalizes information in the last layer to ensure that the CLS token has access to the entire image, but because the CLS token is treated the same as every other patch by the transformer, this seems to be achieved by globalizing across all tokens.\n\nTable 1: After the last layer, every patch contains the same information. “Isolating CLS” denotes the experiment where attention is only performed between patches before the final attention block, while “Patch Average” and “Patch Maximum” refer to the experiment in which the classification head is placed on top of individual patches without fine-tuning. Experiments conducted on ViT-B16.\n\nAccuracy Natural Accuracy\n\nIsolating CLS\n\nPatch Average\n\nPatch Maximum\n\nTop 1 Top 5\n\n84.20 97.16\n\n78.61 94.18\n\n75.75 90.99\n\n80.16 95.65\n\nBased on the preservation of spatial information in patches, we hypothesize that the CLS token plays a relatively minor role throughout the network and is not used for globalization until the last layer. To demonstrate this, we perform inference on images without using the CLS token in layers 1-11, meaning that in these layers, each patch only attends to other patches and not to the CLS token. At layer 12, we then insert a value for the CLS token so that other patches can attend to it and vice versa. This value is obtained by running a forward pass using only the CLS token and no image patches; this value is constant across all input images.\n\nThe resulting hacked network that only has CLS access in the last layer can still successfully classify 78.61% of the ImageNet validation set as shown in Table 1. From this result, we conclude that the CLS token captures global information mostly at the last layer, rather than building a global representation throughout the network.\n\nWe perform a second experiment to show this lastlayer globalization behaviour is not exclusive to the CLS token, but actually occurs across every patch in the last layer. We take the fully connected layer trained to classify images on top of the CLS token, and without any fine-tuning or adaptation, we apply it to each patch, one at a time. This setup still successfully classifies 75.75% of the validation set, on average across individual patches, and the patch with the maximum performance achieves 80.16% accuracy (see Table 1), further confirming that the last layer performs a token mixing operation so that all tokens contain roughly identical information. Figure 6 contains a heat-map depicting the performance of this setup across spatial patches. This observation stands in stark contrast to the suggestions of Raghu et al. (2021) that ViTs possess strong localization throughout the entire network, and their further hypothesis that the addition of global pooling is required for mixing tokens at the end of the network.\n\nFigure 6: Heat map of classification accuracy on the validation set when we apply the classification head trained to classify images on the top of the CLS token to the other patches.\n\nWe conclude by noting that the information structure of a ViT is remarkably similar to a CNN, in the sense that the information is positionally encoded and preserved until the final layer. Furthermore, the final layer in ViTs appears to behave as a learned global pooling operation that aggregates information from all patches, which is similar to its explicit average-pooling counterpart in CNNs.\n\n5 COMPARISON OF VITS AND CNNS\n\nAs extensive work has been done to understand the workings of convolutional networks, including similar feature visualization and image reconstruction techniques to those used here, we may be able to learn more about ViT behavior via direct comparison to CNNs. An important observation is that\n\n6\n\n61218246121824ViT-B16304050607080Under review as a conference paper at ICLR 2023\n\nTexture\n\nParts\n\nObject\n\nL5 F32\n\nL6 F58\n\nL7 F10\n\nL7 F44\n\nL9 F34\n\nL10 F11\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 7: Complexity of features vs depth in ViT B-32. Visualizations suggest that ViTs are similar to CNNs in that they show a feature progression from textures to parts to objects as we progress from shallow to deep features.\n\nin CNNs, early layers recognize color, edges, and texture, while deeper layers pick out increasingly complex structures eventually leading to entire objects (Olah et al., 2017). Visualization of features from different layers in a ViT, such as those in Figures 1 and 7, reveal that ViTs exhibit this kind of progressive specialization as well.\n\nOn the other hand, we observe that there are also important differences between the ways CNNs and ViTs recognize images. In particular, we examine the reliance of ViTs and CNNs on background and foreground image features using the bounding boxes provided by ImageNet Deng et al. (2009). We filter the ImageNet-1k training images and only use those which are accompanied by bounding boxes. If several objects are present in an image, we only take the bounding boxes corresponding to the true class label and ignore the additional bounding boxes. Figure 8(b) shows an example of an image and variants in which the background and foreground, respectively, are masked.\n\n(a)\n\n(b)\n\nFigure 8: (a): ViT-B16 detects background features. Left: Image optimized to maximally activate a feature from layer 6. Center: Corresponding maximally activating example from ImageNet. Right: The image’s patch-wise activation map. (b): An example of an original image and masked-out foreground and background.\n\nFigure 8(a) displays an example of ViTs’ ability to detect background information present in the ImageNet dataset. This particular feature appears responsible for recognizing the pairing of grass and snow. The rightmost panel indicates that this feature is solely activated by the background, and not at all by the patches of the image containing parts of the wolf.\n\nTo quantitatively assess each architecture’s dependence on different parts of the image on the dataset level, we mask out the foreground or background on a set of evaluation images using the aforementioned ImageNet bounding boxes, and we measure the resulting change in top-5 accuracy. These tests are performed across a number of pretrained ViT models, and we compared to a set of common CNNs in Table 2. Further results can be found in Table 3.\n\nWe observe that ViTs are significantly better than CNNs at using the background information in an image to identify the correct class. At the same time, ViTs also suffer noticeably less from the\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nremoval of the background, and thus seem to depend less on the background information to make their classification. A possible, and likely, confounding variable here is the imperfect separation of the background from the foreground in the ImageNet bounding box data set. A rectangle containing the wolf in Figure 8(a), for example, would also contain a small amount of the grass and snow at the wolf’s feet. However, the foreground is typically contained entirely in a bounding box, so masking out the bounding box interiors is highly effective at removing the foreground. Because ViTs are better equipped to make sense of background information, the leaked background may be useful for maintaining superior performance. Nonetheless, these results suggest that ViTs consistently outperform CNNs when information, either foreground or background, is missing.\n\nNext, we study the role of texture in ViT predictions. To this end, we filter out high-frequency components from ImageNet test images via lowpass filtering. While the predictions of ResNets suffer greatly when highfrequency texture information is removed from their inputs, ViTs are seemingly resilient. See Figure 15 for the decay in accuracy of ViT and ResNet models as textural information is removed.\n\n6 VITS WITH LANGUAGE MODEL SUPERVISION\n\nTable 2: ViTs more effectively correlate background information with correct class. Both foreground and background data are normalized by full image top-5 accuracy.\n\nNormalized Top-5 ImageNet Accuracy\n\nArchitecture\n\nFull Image\n\nForeground Background\n\nViT-B32 ViT-L16 ViT-L32 ViT-B16\n\nResNet-50 ResNet-152 MobileNetv2 DenseNet121\n\n98.44 99.57 99.32 99.22\n\n98.00 98.85 96.09 96.55\n\n93.91 96.18 93.89 95.64\n\n89.69 90.74 86.84 89.58\n\n28.10 33.69 31.07 31.59\n\n18.69 19.68 15.94 17.53\n\nRecently, ViTs have been used as a backbone to develop image classifiers trained with natural language supervision and contrastive learning techniques (Radford et al., 2021). These CLIP models are state-of-the-art in transfer learning to unseen datasets. The zero-shot ImageNet accuracy of these models is even competitive with traditionally trained ResNet-50 competitors. We compare the feature visualizations for ViT models with and without CLIP training to study the effect of natural language supervision on the behavior of the transformer-based backbone.\n\nThe training objective for CLIP models consists of matching the correct caption from a list of options with an input image (in feature space). Intuitively, this procedure would require the network to extract features not only suitable for detecting nouns (e.g. simple class labels like ‘bird’), but also modifying phrases like prepositions and epithets. Indeed, we observe several such features that are not present in ViTs trained solely as image classifiers.\n\n(a) Before and after/Step-by-step\n\n(b) From above\n\n(c) Many\n\nFigure 9: Left: Feature optimization shows sharp boundaries, and maximally activating ImageNet examples contain distinct, adjacent images. Middle: Feature optimization and maximally activating ImageNet photos all show images from an elevated vantage point. Right: Feature optimization shows a crowd of people, but maximally activating images indicate that the repetition of objects is more relevant than the type of object.\n\nFigure 9(a) shows the image optimized to maximally activate a feature in the fifth layer of a ViT CLIP model alongside its two highest activating examples from the ImageNet dataset. The fact that all three images share sharp boundaries indicates this feature might be responsible for detecting caption texts relating to a progression of images. Examples could include “before and after,\" as in the airport images or the adjective “step-by-step\" for the iPod teardown. Similarly, Figure 9(b) and 9(c)\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Category of morbidity\n\n(b) Category of music\n\nFigure 10: Features from ViT trained with CLIP that relates to the category of morbidity and music. Top-left image in each category: Image optimized to maximally activate a feature from layer 10. Rest: Seven of the ten ImageNet images that most activate the feature.\n\ndepict visualizations from features which seem to detect the preposition “from above\", and adjectives relating to a multitude of the same object, respectively.\n\nThe presence of features that represent conceptual categories is another consequence of CLIP training. Unlike ViTs trained as classifiers, in which features detect single objects or common background information, CLIP-trained ViTs produce features in deeper layers activated by objects in clearly discernible conceptual categories. For example, the top left panel of Figure 10(a) shows a feature activated by what resembles skulls alongside tombstones. The corresponding seven highly activating images from the dataset include other distinct objects such as bloody weapons, zombies, and skeletons. From a strictly visual point of view, these classes have very dissimilar attributes, indicating this feature might be responsible for detecting components of an image relating broadly to morbidity. In Figure 10(b), we see that the top leftmost panel shows a disco ball, and the corresponding images from the dataset contain boomboxes, speakers, a record player, audio recording equipment, and a performer. Again, these are visually distinct classes, yet they are all united by the concept of music.\n\nGiven that the space of possible captions for images is substantially larger than the mere one thousand classes in the ImageNet dataset, high performing CLIP models understandably require higher level organization for the objects they recognize. Moreover, the CLIP dataset is scraped from the internet, where captions are often more descriptive than simple class labels.\n\n7 DISCUSSION\n\nIn order to dissect the inner workings of vision transformers, we introduce a framework for optimization-based feature visualization. We then identify which components of a ViT are most amenable to producing interpretable images, finding that the high-dimensional inner projection of the feed-forward layer is suitable while the key, query, and value features of self-attention are not.\n\nApplying this framework to said features, we observe that ViTs preserve spatial information of the patches even for individual channels across all layers with the exception of the last layer, indicating that the networks learn spatial relationships from scratch. We further show that the sudden disappearance of localization information in the last attention layer results from a learned token mixing behavior that resembles average pooling.\n\nIn comparing CNNs and ViTs, we find that ViTs make better use of background information and are able to make vastly superior predictions relative to CNNs when exposed only to image backgrounds despite the seemingly counter-intuitive property that ViTs are not as sensitive as CNNs to the loss of high-frequency information, which one might expect to be critical for making effective use of background. We also conclude that the two architectures share a common property whereby earlier layers learn textural attributes, whereas deeper layers learn high level object features or abstract concepts. Finally, we show that ViTs trained with language model supervision learn more semantic and conceptual features, rather than object-specific visual features as is typical of classifiers.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nWe make our code repository available at: https://github.com/anonymous2023iclr/ ViTVis\n\nREFERENCES\n\nHila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 782–791, 2021.\n\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 1(2):3, 2021.\n\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019.\n\nZihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and\n\nattention for all data sizes. arXiv preprint arXiv:2106.04803, 2021.\n\nStéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697, 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.\n\nAlexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4829–4837, 2016.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n\nDumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer\n\nfeatures of a deep network. University of Montreal, 1341(3):1, 2009.\n\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\n\nAmin Ghiasi, Hamid Kazemi, Steven Reich, Chen Zhu, Micah Goldblum, and Tom Goldstein. Plug-in\n\ninversion: Model-agnostic inversion for vision with data augmentations. 2021.\n\nGabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 6(3):e30, 2021.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked\n\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nByeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. arXiv preprint arXiv:2103.16302, 2021.\n\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. arXiv preprint arXiv:2111.09883, 2021.\n\nAravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5188–5196, 2015.\n\nMuzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. arXiv preprint arXiv:2105.10497, 2021.\n\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017. doi:\n\n10.23915/distill.00007. https://distill.pub/2017/feature-visualization.\n\nNamuk Park and Songkuk Kim. How do vision transformers work? arXiv preprint arXiv:2202.06709,\n\n2022.\n\nSayak Paul and Pin-Yu Chen. Vision transformers are robust\n\nlearners.\n\narXiv preprint\n\narXiv:2105.07581, 2021.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n\nMaithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? Advances in Neural Information Processing Systems, 34, 2021.\n\nRulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robustness\n\nof visual transformers. arXiv preprint arXiv:2103.15670, 2021.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: In In Workshop at International\n\nVisualising image classification models and saliency maps. Conference on Learning Representations, 2014.\n\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smoothgrad:\n\nremoving noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, volume 139, pp. 10347–10357, July 2021a.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021b.\n\nRoss Wightman.\n\nPytorch image models.\n\nhttps://github.com/rwightman/\n\npytorch-image-models, 2019.\n\nWeijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers.\n\narXiv preprint arXiv:2104.06399, 2021.\n\nHongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8715–8724, 2020.\n\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.\n\nIn\n\nEuropean conference on computer vision, pp. 818–833. Springer, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\n\narXiv preprint arXiv:2106.04560, 2021.\n\nRoland Zimmermann, Judy Borowski, Robert Geirhos, Matthias Bethge, Thomas Wallis, and Wieland Brendel. How well do feature visualizations support causal understanding of cnn activations? Advances in Neural Information Processing Systems, 34, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA FAILED EXAMPLES\n\nFigure 11 shows few examples of our visualization method failing when applied on low dimensional spaces. We attribute this to entanglement of more than 768 features when represented by vectors of size 768. We note that, due to skip connections, activation in previous layers can cause activation in the next layer for the same feature, consequently, the visualizations of the same features in different layers can share visual similarities.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFeature 0\n\nFeature 1\n\nFeature 2\n\nFeature 3\n\nLayer 0\n\nLayer 1\n\nLayer 2\n\nLayer 3\n\nLayer 4\n\nLayer 5\n\nLayer 6\n\nLayer 7\n\nLayer 8\n\nLayer 9\n\nLayer 10\n\nLayer 11\n\nFigure 11: Some examples of failed visualizations on the input of the attention layers. Same visualization technique fails when applied on low dimensional (e.g. on key, query, value, etc) spaces. We believe that the visualization shows roughly meaningful and interpretable visualizations in early layers, since there are not many different features to be embedded. However, in deeper layers the features are entangled, so it is more difficult to visualize them. For every example, the picture on the left shows the results of optimization and the picture on the right shows the most activating image from ImageNet1k validation set.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nB EXPERIMENTAL SETUP AND HYPERPARAMETERS\n\nAs mentioned before, we ensemble augmentations to the input. More specifically, we use A is GS(CS(Jitter(x))) as our augmentation. The bound for Jitter is (−32, 32) for both directions vertical and horizontal. The hyper parameters for CS are always mean = 1.0 and std = 1.0 in all of the experiments. For GS, the mean is always 0, however, for the std we have a linear scheduling, where at the beginning of the optimization the std = 0.5 and at the end of the optimization std = 0.0. We use a batch-size n = 8 for all of our experiments. We use ADAM as our choice of optimizer with β = (0.5, 0.99). Optimization is done in 400 steps and at every step, we re-sample the augmentations Jitter, GS and CS. We also use a CosineAnealing learning rate scheduling, starting from lr = 0.1 at the beginning and l = 0 the end. The hyper-parameter used for total variation λtv = 0.00005.\n\nFor all ViT experiments, we use the pretrained models from https://github.com/lukemelas/PyTorchPretrained-ViT. For clip models, we use pretrained models by Wightman (2019). The rest of models we use are from https://github.com/pytorch/vision.\n\nFor all of our experiments, we use GeForce RTX 2080 Ti GPUs with 12GB of memory. All inferences on ImageNet are done under 20 minutes on validation set and under 1 hour on training set using only 1 GPU. All visualization experiments take at most 90 seconds to complete.\n\nC MODELS\n\nIn our experiments, we use publicly available pre-trained models from various sources. The following tables list the models used from each source, along with references to where they are introduced in the literature.\n\nName B_16_imagenet1k B_32_imagenet1k\n\nPaper Dosovitskiy et al. (2021) Dosovitskiy et al. (2021)\n\nFigure 12: Pre-trained models used from : https://github.com/lukemelas/PyTorch-Pretrained-ViT.\n\nName deit_base_patch16_224 deit_base_distilled_patch16_384 deit_base_patch16_384 deit_tiny_distilled_patch16_224 deit_small_distilled_patch16_224 deit_base_distilled_patch16_224\n\nPaper Touvron et al. (2021b) Touvron et al. (2021b) Touvron et al. (2021b) Touvron et al. (2021b) Touvron et al. (2021b) Touvron et al. (2021b)\n\nFigure 13: Pre-trained models from Touvron et al. (2021a) .\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nName coat_lite_mini coat_lite_small coat_lite_tiny coat_mini coat_tiny convit_base convit_small convit_tiny pit_b_224 pit_b_distilled_224 pit_s_224 pit_s_distilled_224 pit_ti_224 pit_ti_distilled_224 swin_base_patch4_window7_224 swin_base_patch4_window7_224_in22k swin_base_patch4_window12_384 swin_base_patch4_window12_384_in22k swin_large_patch4_window7_224 swin_large_patch4_window7_224_in22k swin_large_patch4_window12_384 swin_large_patch4_window12_384_in22k swin_small_patch4_window7_224 swin_tiny_patch4_window7_224 twins_pcpvt_base twins_pcpvt_large twins_pcpvt_small twins_svt_base twins_svt_large twins_svt_small\n\nPaper Xu et al. (2021) Xu et al. (2021) Xu et al. (2021) Xu et al. (2021) Xu et al. (2021) d’Ascoli et al. (2021) d’Ascoli et al. (2021) d’Ascoli et al. (2021) Heo et al. (2021) Heo et al. (2021) Heo et al. (2021) Heo et al. (2021) Heo et al. (2021) Heo et al. (2021) Liu et al. (2021) Liu et al. (2021) Liu et al. (2021) Liu et al. (2021) Liu et al. (2021) Liu et al. (2021) Liu et al. (2021) Liu et al. (2021) Liu et al. (2021) Liu et al. (2021) Chu et al. (2021) Chu et al. (2021) Chu et al. (2021) Chu et al. (2021) Chu et al. (2021) Chu et al. (2021)\n\nFigure 14: Pre-trained models used from: Wightman (2019)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nD EFFECT OF LOW-PASS FILTERING\n\nFigure 15: Effect of low-pass filtering on top-1 ImageNet accuracy. CNNs are more dependent on high frequency textural image information than ViTs.\n\nE ADDITIONAL VISUALIZATIONS\n\n17\n\n0.50.40.30.20.1Low-pass threshold020406080100Top-1 accuracy on ImageNet (%)ViT-B/16ViT-L/16ResNet-50ResNet152-dUnder review as a conference paper at ICLR 2023\n\nL0 F29\n\nL0 F30\n\nL0 F22\n\nL1 F31\n\nL1 F29\n\nL2 F5\n\nL2 F3\n\nL2 F36\n\nL3 F35\n\nL3 F27\n\nL3 F37\n\nL4 F5\n\nL4 F9\n\nL4 F12\n\nL5 F5\n\nL5 F10\n\nL5 F24\n\nL6 F1\n\nL6 F2\n\nL6 F6\n\nL6 F9\n\nL6 F20\n\nL6 F27\n\nL7 F1\n\nL7 F5\n\nL7 F4\n\nL7 F8\n\nL7 F14\n\nL7 F16\n\nL7 F17\n\nL7 F24\n\nL7 F37\n\nL8 F0\n\nL8 F3\n\nL8 F5\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\nFigure 16: Visualization of ViT-base-patch16\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nL8 F19\n\nL8 F22\n\nL8 F28\n\nL9 F0\n\nL9 F25\n\nL10 F0\n\nL10 F9\n\nL10 F22\n\nL10 F20\n\nL10 F27\n\nL10 F37\n\nL11 F0\n\nL11 F16\n\nL11 F33\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\nFigure 17: (Cont.) Visualization of ViT-base-patch16\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nL0 F17\n\nL0 F22\n\nL0 F7\n\nL1 F33\n\nL1 F36\n\nL1 F8\n\nL2 F0\n\nL2 F1\n\nL2 F4\n\nL2 F11\n\nL2 F22\n\nL2 F50\n\nL3 F6\n\nL3 F38\n\nL3 F44\n\nL4 F4\n\nL4 F11\n\nL4 F10\n\nL4 F22\n\nL4 F33\n\nL4 F59\n\nL5 F12\n\nL5 F17\n\nL5 F50\n\nL6 F12\n\nL6 F31\n\nL6 F43\n\nL6 F42\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 18: Visualization of a CLIP model with ViT-base-patch16 as its visual part.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nL6 F40\n\nL6 F50\n\nL7 F7\n\nL7 F6\n\nL7 F5\n\nL7 F16\n\nL7 F37\n\nL7 F41\n\nL7 F45\n\nL7 F55\n\nL7 F54\n\nL8 F3\n\nL8 F2\n\nL8 F0\n\nL8 F1\n\nL8 F5\n\nL8 F6\n\nL8 F19\n\nL8 F22\n\nL8 F24\n\nL8 F34\n\nL8 F33\n\nL8 F32\n\nL8 F36\n\nL8 F39\n\nL8 F50\n\nL9 F0\n\nL9 F1\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 19: (Cont.) Visualization of a CLIP model with ViT-base-patch16 as its visual part.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nL9 F2\n\nL9 F11\n\nL9 F19\n\nL9 F21\n\nL9 F24\n\nL9 F25\n\nL9 F31\n\nL9 F33\n\nL9 F40\n\nL9 F47\n\nL9 F44\n\nL9 F48\n\nL9 F52\n\nL9 F56\n\nL10 F20\n\nL10 F33\n\nL10 F51\n\nL10 F55\n\nL10 F53\n\nL11 F2\n\nL11 F0\n\nL11 F4\n\nL11 F10\n\nL11 F9\n\nL11 F13\n\nL11 F25\n\nL11 F32\n\nL11 F41\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 20: (Cont.) Visualization of a CLIP model with ViT-base-patch16 as its visual part.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nL5 F32\n\nL5 F35\n\nL5 F25\n\nL6 F58\n\nL6 F45\n\nL6 F46\n\nL6 F42\n\nL6 F40\n\nL6 F32\n\nL6 F34\n\nL6 F17\n\nL6 F9\n\nL7 F55\n\nL7 F49\n\nL7 F44\n\nL7 F39\n\nL7 F25\n\nL7 F27\n\nL7 F17\n\nL7 F10\n\nL7 F0\n\nL8 F58\n\nL8 F52\n\nL8 F43\n\nL8 F27\n\nL8 F23\n\nL8 F18\n\nL8 F14\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 21: Visualization of ViT-base-patch32\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nL8 F15\n\nL8 F10\n\nL8 F9\n\nL8 F5\n\nL8 F2\n\nL9 F52\n\nL9 F48\n\nL9 F50\n\nL9 F47\n\nL9 F44\n\nL9 F40\n\nL9 F33\n\nL9 F34\n\nL9 F35\n\nL9 F29\n\nL9 F28\n\nL9 F24\n\nL9 F25\n\nL9 F16\n\nL10 F56\n\nL10 F51\n\nL10 F48\n\nL10 F45\n\nL10 F43\n\nL10 F42\n\nL10 F41\n\nL10 F40\n\nL10 F36\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 22: (Cont.) Visualization of ViT-base-patch32\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nL10 F35\n\nL10 F34\n\nL10 F31\n\nL10 F26\n\nL10 F22\n\nL10 F23\n\nL10 F18\n\nL10 F14\n\nL10 F13\n\nL10 F9\n\nL10 F10\n\nL10 F11\n\nL10 F6\n\nL10 F5\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 23: (Cont.) Visualization of ViT-base-patch32\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nL5 F58\n\nL5 F47\n\nL5 F39\n\nL5 F32\n\nL5 F31\n\nL5 F9\n\nL5 F11\n\nL5 F3\n\nL6 F50\n\nL6 F51\n\nL6 F45\n\nL6 F33\n\nL6 F32\n\nL6 F31\n\nL7 F57\n\nL7 F53\n\nL7 F40\n\nL7 F42\n\nL7 F39\n\nL7 F38\n\nL8 F54\n\nL8 F47\n\nL8 F34\n\nL8 F16\n\nL8 F12\n\nL8 F13\n\nL9 F59\n\nL9 F47\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 24: Visualization of a CLIP model with ViT-base-patch32 as its visual part.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nL9 F41\n\nL9 F27\n\nL9 F15\n\nL9 F14\n\nL10 F43\n\nL10 F39\n\nL10 F24\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 25: (Cont.) Visualization of a CLIP model with ViT-base-patch32 as its visual part.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nL11 F3\n\nL10 F2\n\nL10 F3\n\nL10 F10\n\nL10 F12\n\nL10 F14\n\nL10 F16\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nL10 F19\n\nL9 F0\n\nL9 F1\n\nL9 F3\n\nL9 F11\n\nL9 F15\n\nL9 F19\n\nL8 F3\n\nL8 F6\n\nL8 F15\n\nL8 F16\n\nL8 F19\n\nL7 F0\n\nL7 F2\n\nFigure 26: Visualization of features in Deit base p-16 im-224\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nL7 F3\n\nL7 F13\n\nL6 F6\n\nL6 F16\n\nL5 F1\n\nL4 F9\n\nL1 F17\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 27: (Cont.) Visualization of features in Deit base p-16 im-224\n\nL10 F8\n\nL9 F0\n\nL9 F2\n\nL9 F3\n\nL9 F11\n\nL9 F12\n\nL9 F13\n\nL8 F2\n\nL8 F3\n\nL8 F5\n\nL8 F6\n\nL8 F7\n\nL8 F12\n\nL8 F15\n\nFigure 28: Visualization of features in DeiT base p-16 im-384\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nL8 F18\n\nL7 F0\n\nL7 F1\n\nL7 F8\n\nL7 F10\n\nL7 F12\n\nL7 F13\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 29: (Cont.) Visualization of features in DeiT base p-16 im-384\n\nL11 F9\n\nL10 F5\n\nL10 F11\n\nL10 F12\n\nL10 F14\n\nL10 F18\n\nL10 F19\n\nL9 F0\n\nL9 F2\n\nL9 F3\n\nL9 F8\n\nL9 F9\n\nL9 F12\n\nL9 F18\n\nFigure 30: Visualization of features in DeiT base p-16 im-384\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nL9 F0\n\nL9 F2\n\nL9 F4\n\nL9 F8\n\nL9 F12\n\nL9 F13\n\nL9 F15\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nL8 F0\n\nL8 F1\n\nL8 F12\n\nL8 F18\n\nL5 F17\n\nL4 F8\n\nL3 F18\n\nFigure 31: Visualization of features in DeiT tiny distilled p-16 im-224\n\nL10 F0\n\nL10 F2\n\nL9 F10\n\nL9 F11\n\nL9 F12\n\nL8 F2\n\nL8 F3\n\nFigure 32: Visualization of features in DeiT small distilled p-16 im-224\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nL8 F8\n\nL8 F9\n\nL8 F13\n\nL8 F16\n\nL8 F19\n\nL7 F6\n\nL7 F8\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nL7 F9\n\nL7 F10\n\nL7 F12\n\nL7 F13\n\nL6 F10\n\nL6 F16\n\nL5 F3\n\nFigure 33: Visualization of features in DeiT small distilled p-16 im-224\n\nL10 F1\n\nL10 F8\n\nL9 F2\n\nL9 F3\n\nL9 F11\n\nL9 F15\n\nL9 F19\n\nFigure 34: Visualization of features in DeiT base distilled p-16 im-224\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nL8 F0\n\nL8 F7\n\nL8 F12\n\nL8 F14\n\nL8 F15\n\nL8 F17\n\nL8 F19\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 35: (Cont.) Visualization of features in DeiT base distilled p-16 im-224\n\nL6 F8\n\nL6 F10\n\nL6 F11\n\nL6 F12\n\nL6 F17\n\nL5 F10\n\nL5 F12\n\nFigure 36: Visualization of features in Coat lite mini\n\nL14 F10\n\nL14 F16\n\nL14 F18\n\nL13 F11\n\nL12 F11\n\nL8 F6\n\nL7 F16\n\nFigure 37: Visualization of features in Coat lite small\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nL9 F2\n\nL9 F4\n\nL9 F5\n\nL9 F7\n\nL9 F8\n\nL9 F11\n\nL9 F13\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nL9 F15\n\nL9 F19\n\nL8 F1\n\nL8 F11\n\nL8 F12\n\nL8 F13\n\nL8 F17\n\nFigure 38: Visualization of features in ConViT base\n\nL11 F13\n\nL10 F14\n\nL9 F0\n\nL9 F12\n\nL9 F13\n\nL9 F16\n\nL9 F17\n\nFigure 39: Visualization of features in ConViT small.\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nL9 F19\n\nL8 F0\n\nL8 F1\n\nL8 F3\n\nL8 F7\n\nL8 F8\n\nL8 F14\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nL8 F18\n\nL7 F4\n\nL7 F6\n\nL6 F10\n\nL6 F17\n\nL5 F8\n\nL4 F1\n\nFigure 40: (Cont.) Visualization of features in ConViT small.\n\nL10 F0\n\nL10 F5\n\nL10 F6\n\nL10 F11\n\nL10 F13\n\nL10 F17\n\nL9 F3\n\nFigure 41: Visualization of features in ConViT tiny.\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nL9 F8\n\nL9 F9\n\nL9 F12\n\nL9 F16\n\nL9 F18\n\nL8 F9\n\nL7 F6\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 42: (Cont.) Visualization of features in ConViT tiny.\n\nL9 F10\n\nL8 F3\n\nL8 F12\n\nL8 F14\n\nL8 F15\n\nL7 F5\n\nL7 F9\n\nFigure 43: Visualization of features in Pit base im-224\n\nL9 F19\n\nL8 F0\n\nL8 F1\n\nL8 F4\n\nL8 F9\n\nL8 F13\n\nL6 F7\n\nFigure 44: Visualization of features in Pit base distilled im-224.\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nL10 F0\n\nL9 F14\n\nL8 F12\n\nL7 F9\n\nL6 F18\n\nL5 F12\n\nL4 F14\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\nFigure 45: Visualization of features in Pit small im-224.\n\nL10 F8\n\nL10 F19\n\nL8 F4\n\nL8 F17\n\nL7 F2\n\nL7 F17\n\nL6 F7\n\nFigure 46: Visualization of features in Pit small distilled im-224.\n\nL9 F9\n\nL8 F12\n\nL8 F14\n\nL7 F13\n\nL7 F16\n\nL5 F3\n\nL5 F13\n\nFigure 47: Visualization of features in Pit tiny im-224.\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nL9 F15\n\nL8 F3\n\nL8 F4\n\nL8 F18\n\nL7 F3\n\nL7 F11\n\nL7 F15\n\ns r\nu O\n\nl a\nV\n\nn\n\ni a\nr\n\nT\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\nFigure 48: Visualization of features in Pit tiny distilled im-224.\n\nL21 F5\n\nL21 F8\n\nL20 F0\n\nL20 F18\n\nL19 F17\n\nL18 F4\n\nL16 F0\n\nFigure 49: Visualization of features in Swin base p-4 w-7 im-224.\n\nL21 F0\n\nL21 F8\n\nL21 F18\n\nL20 F18\n\nL19 F17\n\nL18 F4\n\nL10 F17\n\nFigure 50: Visualization of features in Swing base base p-4 w-7 im-224 imagenet 22k.\n\nL21 F5\n\nL20 F0\n\nL20 F18\n\nL18 F4\n\nL18 F12\n\nL17 F10\n\nL10 F11\n\ns r\nu O\n\nl a\nV\n\nFigure 51: Visualization of features in Swin base p-4 w-12 im-384.\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nL21 F8\n\nL20 F5\n\nL20 F8\n\nL19 F3\n\nL19 F7\n\nL18 F1\n\nL18 F5\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\nL17 F8\n\nL17 F11\n\nL17 F15\n\nL17 F16\n\nL16 F5\n\nL16 F7\n\nL16 F10\n\nFigure 52: Visualization of features in Swin large p-4 w-7 im-224.\n\nL22 F19\n\nL20 F4\n\nL20 F5\n\nL20 F11\n\nL20 F14\n\nL19 F3\n\nL17 F8\n\nFigure 53: Visualization of features in Swin large p-4 w-7 im-224 imagenet 22k.\n\nL21 F8\n\nL20 F11\n\nL19 F12\n\nL18 F18\n\nL16 F0\n\nL16 F12\n\nL11 F1\n\nFigure 54: Visualization of features in Swin large p-4 w-12 im-384.\n\n39\n\nUnder review as a conference paper at ICLR 2023\n\nL20 F4\n\nL19 F9\n\nL19 F13\n\nL19 F14\n\nL18 F0\n\nL18 F4\n\nL15 F8\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\nFigure 55: Visualization of features in Swin small p-4 w-7 im-224.\n\nL22 F14\n\nL20 F2\n\nL20 F12\n\nL18 F16\n\nL13 F12\n\nL11 F15\n\nL6 F16\n\nFigure 56: Visualization of features in Twins pcpvt base.\n\nL22 F16\n\nL20 F2\n\nL19 F13\n\nL16 F19\n\nL15 F19\n\nL9 F5\n\nL6 F2\n\nFigure 57: Visualization of features in Twins pcpvt large.\n\nL15 F2\n\nL15 F4\n\nL15 F15\n\nL15 F18\n\nL13 F11\n\nL10 F6\n\nL0 F13\n\nFigure 58: Visualization of features in Twins pcpvt small.\n\n40\n\nUnder review as a conference paper at ICLR 2023\n\nL23 F5\n\nL21 F4\n\nL21 F6\n\nL21 F13\n\nL18 F13\n\nL17 F18\n\nL16 F3\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\ns r\nu O\n\nl a\nV\n\nFigure 59: Visualization of features in Twins svt base.\n\nL19 F8\n\nL19 F13\n\nL18 F15\n\nL17 F5\n\nL16 F3\n\nL16 F11\n\nL14 F18\n\nFigure 60: Visualization of features in Twins svt large.\n\nL17 F2\n\nL13 F5\n\nL10 F10\n\nL9 F3\n\nL9 F5\n\nL8 F7\n\nL0 F10\n\nFigure 61: Visualization of features in Twins svt small.\n\n41\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: ViTs more effectively correlate background information with correct class. Both foreground and background data are normalized by full image top-1 accuracy.\n\nNormalized Top-1 ImageNet Accuracy\n\nArchitecture\n\nFull Image\n\nForeground\n\nBackground\n\nViT-B32 ViT-L16 ViT-L32 ViT-B16\n\nResNet-50 ResNet-152 MobileNetv2 DenseNet121\n\n89.25 95.00 94.64 92.37\n\n87.67 82.92 83.77 90.58\n\n91.53 93.88 90.63 93.70\n\n85.59 82.03 85.58 86.53\n\n15.04 19.08 17.67 16.98\n\n9.25 8.24 8.75 9.72\n\n42\n\nUnder review as a conference paper at ICLR 2023\n\nF SPATIAL INFORMATION PRESENCE - QUANTITATIVE EVALUATION\n\nIn the following experiments, we find the most activating images for each feature. Then, we forward these images to the network. We call a patch active if the activation for this patch is higher than 0.5. First we mask out all the inactive patches meaning that we replace them with black patches. Then, we mask out x percent of the active patches in the current image. Then, we forward this new image to the network. Finally, we plot the number/sum of active patches of the modified image divided by the number/sum of the active patches in the initial image for different percentages. If the spatial information is present in a layer, we expect this number to have a linear trend. As we see in figures 62, and 63, all the layers except for the last one, have a linear trend, indicating that the loss of spatial information in individual channels is mostly made in the last layer.\n\nFigure 62: Number of active patches after drop divided by number of active patches before drop\n\n43\n\n20406080100PercentofUnmaskedOriginally-ActivePatches(%)020406080100PercentofActivePatchesAfterMasking(%)Layer7Layer8Layer9Layer10Layer11x=yUnder review as a conference paper at ICLR 2023\n\nFigure 63: Sum of active patches after drop divided by number of active patches before drop\n\n44\n\n20406080100PercentofUnmaskedOriginally-ActivePatches(%)20406080100Ratio=PActivations(BeforeMasking)PActivations(AfterMasking)Layer7Layer8Layer9Layer10Layer11x=y",
    "reference": "# Summary Of The Paper\n\nThis work adapts visualization techniques originally proposed for CNNs to ViT and observe the similarities and difference between CNN and ViT in terms of these visualization. For example, CNNs and ViT explores low-level features in the earlier layers and more semantic features in the higher layers. Some discoveries are interesting. For example, they observe that CLlP-based ViT image encoders can be activated by semantic concepts in the text information, e.g., such as prepositions, adjectives, and conceptual categories.\n\n# Strength And Weaknesses\n\n(+) They perform visualization on multiple ViT variants.\n\n(+) Experiments related to ViTs with text information (CLIP models) are interesting. \n\nWeakness:\n(-) The paper is not well-organized and the writing needs to be improved.\n\n(-) The visualization is corresponding to different layers displayed in different figures without justifications. Therefore, it is hard to tell if these results are cherry-picked.\n\n(-) Most conclusions drawn from this work are based on qualitative visualization without quantitative results, which makes these conclusions less convincing.\n\n(-) Some of the results are in the expectation without significant contributions to the communities. For example, the spatial information is lost in the last layer since the classification is performed based on the class-token. In addition, ViT relies more on background information has also been explored in previous works, e.g, patch-based infill transformation in Qin et al \"Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation\" (NeurIPS-2022) and Naseer et al \"Intriguing Properties of Vision Transformers\" (NeurIPS-2021).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is not well-organized and the writing needs to be improved. In addition, the novelty is not strong enough to support ICLR.\n\n# Summary Of The Review\n\nThis work does not have strong novelty and lack of quantitative analysis for most of the conclusions presented in the paper. In all, I lean toward a bordline reject.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSPARSE RANDOM NETWORKS FOR COMMUNICATION-EFFICIENT FEDERATED LEARNING\n\nBerivan Isik¶∗, Francesco Pase§∗, Deniz Gunduz‡, Tsachy Weissman¶, Michele Zorzi§ ¶Stanford University, §University of Padova, ‡Imperial College London berivan.isik@stanford.edu, pasefrance@dei.unipd.it\n\nABSTRACT\n\nOne main challenge in federated learning is the large communication cost of exchanging weight updates from clients to the server at each round. While prior work has made great progress in compressing the weight updates through gradient compression methods, we propose a radically different approach that does not update the weights at all. Instead, our method freezes the weights at their initial random values and learns how to sparsify the random network for the best performance. To this end, the clients collaborate in training a stochastic binary mask to find the optimal sparse random network within the original one. At the end of the training, the final model is a sparse network with random weights – or a subnetwork inside the dense random network. We show improvements in accuracy, communication (less than 1 bit per parameter (bpp)), convergence speed, and final model size (less than 1 bpp) over relevant baselines on MNIST, EMNIST, CIFAR10, and CIFAR-100 datasets, in the low bitrate regime.\n\n1\n\nINTRODUCTION\n\nFederated learning (FL) is a distributed learning framework where clients collaboratively train a model by performing local training on their data and by sharing their local updates with a server every few iterations, which in turn aggregates the local updates to create a global model, that is then transmitted to the clients for the next round of training. While being an appealing approach for enabling model training without the need to collect client data at the server, uplink communication of local updates is a significant bottleneck in FL (Kairouz et al., 2021). This has motivated research in communication-efficient FL strategies (McMahan et al., 2017a) and various gradient compression schemes via sparsification (Lin et al., 2018; Wang et al., 2018; Barnes et al., 2020; Ozfatura et al., 2021; Isik et al., 2022), quantization (Alistarh et al., 2017; Wen et al., 2017; Bernstein et al., 2018; Mitchell et al., 2022), and low-rank approximation (Koneˇcn`y et al., 2016; Vargaftik et al., 2021; 2022; Basat et al., 2022). In this work, while aiming for communication efficiency in FL, we take a radically different approach from prior work, and propose a strategy that does not require communication of weight updates. To be more precise, instead of training the weights,\n\n(1) the server initializes a dense random network with d weights, denoted by the weight vector winit = (winit d ), using a random seed SEED, and broadcasts SEED to the clients enabling them to reproduce the same winit locally,\n\n2 , . . . , winit\n\n1 , winit\n\n(2) both the server and the clients keep the weights frozen at their initial values winit at all times,\n\n(3) clients collaboratively train a probability mask of d parameters θ = (θ1, θ2, . . . , θd) ∈ [0, 1]d,\n\n(4) the server samples a binary mask from the trained probability mask and generates a sparse network with random weights – or a subnetwork inside the initial dense random network as follows\n\nwfinal = Bern(θ) ⊙ winit,\n\n(1)\n\nwhere Bern(·) is the Bernoulli sampling operation and ⊙ the element-wise multiplication.\n\nWe call the proposed framework Federated Probabilistic Mask Training (FedPM) and summarize it in Figure 1. At first glance, it may seem surprising that there exist subnetworks inside randomly\n\n∗First two authors contributed equally. Work done while F.P. was visiting Imperial College London.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ninitialized networks that could perform well without ever modifying the weight values. This phenomenon has been explored to some extent in prior work (Zhou et al., 2019; Ramanujan et al., 2020; Pensia et al., 2020; Diffenderfer & Kailkhura, 2020; Aladago & Torresani, 2021) with different strategies for finding the subnetworks. However, how to find these subnetworks in a FL setting has not attracted much attention so far. Some exceptions to this are works by Li et al. (2021); Vallapuram et al. (2022); Mozaffari et al. (2021), which provide improvements in other FL challenges, such as personalization and poisoning attacks, while not being competitive with existing (dense) compression methods such as QSGD (Alistarh et al., 2017), DRIVE (Vargaftik et al., 2021), and SignSGD (Bernstein et al., 2018) in terms of accuracy under the same communication budget. In this work, we propose a stochastic way of finding such subnetworks while reaching higher accuracy at a reduced communication cost – less than 1 bit per parameter (bpp).\n\nFigure 1: Extracting a randomly weighted sparse network using the trainable probability mask θt in the forward-pass of round t (for clients and the server). In practice, clients collaboratively train continuous scores s ∈ Rd, and then at inference time, the clients (or the server) find θt = Sigmoid(st) ∈ [0, 1]d. We skip this step in the figure for the sake of simplicity.\n\nIn addition to the accuracy and communication gains, our framework also provides an efficient representation of the final model post-training by requiring less than 1 bpp to represent (i) the random seed that generates the initial weights winit, and (ii) a sampled binary vector Bern(θ) (computed with the trained θ). Therefore, the final model enjoys a memory-efficient deployment – a crucial feature for machine learning at power-constrained edge devices. Another advantage our framework brings is the privacy amplification under some settings, thanks to the stochastic nature of our training strategy.\n\nOur contributions can be summarized as follows:\n\n(1) We propose a FL framework, in which the clients do not train the model weights, but instead a stochastic binary mask to be used in sparsifying the dense network with random weights. This differs from the standard training approaches in the literature.\n\n(2) Our framework provides efficient communication from clients to the server by requiring (less than) 1 bpp per client while yielding faster convergence and higher accuracy than the baselines.\n\n(3) We propose a Bayesian aggregation strategy at the server side to better deal with partial client participation and non-IID data splits.\n\n(4) The final model (a sparse network with random weights) can be efficiently represented with a random seed and a binary mask which requires (less than) 1 bpp – at least 32× more efficient storage and communication of the final model with respect to standard FL strategies.\n\n(5) We demonstrate the efficacy of our strategy on MNIST, EMNSIT, CIFAR-10, and CIFAR-100 datasets under both IID and non-IID data splits; and show improvements in accuracy, bitrate, convergence speed, and final model size over relevant baselines, under various system configurations.\n\n2 RELATED WORK\n\nIn this section, we briefly discuss the related work in (1) communication-efficient FL, (2) pruning for FL, and (3) finding subnetworks in a random network.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nCommunication-Efficient FL. One way of improving communication efficiency in FL is to compress the model updates using gradient compression methods like sparsification (Aji & Heafield, 2017; Lin et al., 2018; Wang et al., 2018; Barnes et al., 2020; Ozfatura et al., 2021; Isik et al., 2022), quantization (Alistarh et al., 2017; Wen et al., 2017; Suresh et al., 2017; Bernstein et al., 2018; Mitchell et al., 2022), and low-rank approximation (Wang et al., 2018; Vogels et al., 2019; Vargaftik et al., 2021; 2022; Mohtashami et al., 2022; Basat et al., 2022); or more FL-oriented compression schemes such as (Koneˇcn`y et al., 2016; McMahan et al., 2017a; Sattler et al., 2019; Rothchild et al., 2020; Reisizadeh et al., 2020; Haddadpour et al., 2020; 2021), while training a dense network. Our framework differs from these dense compression methods substantially due to the unconventional stochastic mask training strategy; however, we take SignSGD (Bernstein et al., 2018), TernGrad (Wen et al., 2017), QSGD (Alistarh et al., 2017), DRIVE (Vargaftik et al., 2021), and EDEN (Vargaftik et al., 2022) as our baselines since they work in the same bitrate regime as ours (≈1 bpp).\n\nPruning for FL. Since the introduction of the Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2018), there has been growing interest in finding sparse and trainable networks at initialization. The main hypothesis in this line of work is that there exist sparse networks (lottery tickets) inside randomly initialized dense networks such that those sparse networks can be trained to a surprisingly good performance. In the original paper, the strategy for finding these lottery tickets is to iteratively train the dense network, i.e., finding the lottery tickets is expensive. We distinguish our approach from the FL papers that utilize the LTH (Li et al., 2020; Ji et al., 2020; Seo et al., 2021) and pruning (Lin et al., 2020; Munir et al., 2021; Yu et al., 2021; Liu et al., 2021; Jiang et al., 2022; Babakniya et al., 2022; Dai et al., 2022; Lin et al., 2020; Bibikar et al., 2022) for three mains reasons: (i) These methods require training the weight values, and thus cannot provide an efficient representation of the final model as FedPM does. Recall that we achieve at least 32× more efficient storage and communication of the final model by representing it with just a random seed and a binary mask. (ii) Some of these works require finding the lottery tickets prior to FL training (Li et al., 2020). While this could improve the communication cost during the FL training since they communicate sparse networks, it increases the computation cost significantly due to the burden of finding lottery tickets via training. (iii) As opposed to the LTH- or pruning-based FL works, our framework learns with what probability a particular weight should stay in the final model, i.e., the final sparsity level is also a learned parameter optimized for the best performance. Overall, since FedPM is not in the same bitrate regime as these works (they require higher bitrates to communicate continuous weight values), we do not compare against them.\n\nFinding Subnetworks Inside a Random Network. Our work is closest to recent works of Zhou et al. (2019); Ramanujan et al. (2020); Pensia et al. (2020); Aladago & Torresani (2021), which find subnetworks (or supermasks) inside a dense network with random weights that perform surprisingly well without ever training the weights, but in a centralized scenario. In this work, we take advantage of the existence of such subnetworks to reduce the communication budget in FL to less than 1 bpp with faster convergence and higher accuracy than our relevant baselines in the same bitrate regime, while further compressing the final model, all simultaneously. Prior works (Li et al., 2021; Vallapuram et al., 2022; Mozaffari et al., 2021) also consider finding subnetworks inside a dense random network in a FL setting, but they differ from our approach on several levels. For instance, they focus on different challenges in FL, such as personalization and poisoning attacks, which limits their ability to improve over existing compression methods in accuracy-communication bitrate tradeoff. One fundamental reason for this is their deterministic mask training strategy, which involves hard thresholding or sign operations. On the other hand, the stochasticity in FedPM allows us to (i) enjoy a better accuracy-communication cost tradeoff, (ii) have an unbiased estimate of the true aggregate of the local masks with a provable upper bound on the error, (iii) design an improved aggregation strategy with a Bayesian approach so that the previous masks at the server are not hard replaced – a useful strategy specifically in unbalanced non-IID splits, and (iv) gain privacy benefits via amplification in the Bernoulli sampling step. To demonstrate these benefits over deterministic schemes, we compare our method against FedMask (Li et al., 2021) by adapting it slightly to mainly focus on communication efficiency, rather than personalization, and to improve its accuracy-communication efficiency performance. More specifically, we discard the initial pruning stage that was deployed for personalization. This change was necessary because (a) this paper does not study personalization, so this pruning step would put FedMask at a disadvantage in our experimental setup, and (b) the initial pruning step requires extra training at client devices, which is computationally more expensive than FedPM and the dense baselines.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 FEDERATED PROBABILISTIC MASK TRAINING (FedPM)\n\nWe first describe the simpler version of the FedPM framework in Section 3.1, which provides an unbiased estimation of the mean of the learned probability masks at the server with bounded error. Next, we propose a modification in our aggregation strategy by exploiting the underlying Bernoulli mechanism in Section 3.2. This helps boost the performance of FedPM in the case of partial client participation. We then discuss the details of the distribution of the initial weights in Section 3.3, and finally describe the privacy benefits of FedPM in Section 3.4. We use capital letters for random variables, small letters for their realization and deterministic quantities, and bold letters for vectors. Moreover, we indicate with xu,t the state of the local vector x (e.g., the local mask) at client u during round t, and with xu,t , and sets are indicated with calligraphic fonts. We denote a neural network with weight vector p as fp.\n\nits i-th component. Global values are denoted with xg,t and xg,t\n\ni\n\ni\n\n3.1 FedPM\n\n1 , winit\n\n2 , . . . , winit\n\nIn this section, we present the general FedPM training pipeline. First, the server randomly initializes d ) ∈ Rd, a neural network fwinit, parameterized by the weight vector winit = (winit whose components are sampled IID according to a distribution Pw using a randomly generated seed SEED. The random SEED value is then communicated to all the clients, which can locally sample the same pseudo-random vector winit, which is kept fixed and never modified during training. The goal for the clients is to collaboratively train a probability mask θ ∈ [0, 1]d, which indicates the Bernoulli parameters for the global stochastic binary mask M ∼ Bern(θ) ∈ {0, 1}d, such that the function f ̇W maximizes its performance on a given task, where ̇W = M ⊙ winit. Specifically, FedPM learns the probabilities for the weights of being active, which are given by the probability mask θ = (θ1, θ2, . . . , θd) ∈ [0, 1]d. To achieve this, at every round t, the server samples a set Kt of |Kt| = K participants (out of the total N clients), which individually train their local probability masks θk,t, k ∈ Kt, by using their local datasets Dk, each composed of Dk = |Dk| samples. These local masks are then aggregated by the server in a communication-efficient way to estimate the optimal θ. At test time, at the server, the initial random network fwinit is sparsified using the global probability mask θg,t, following the stochastic approach in Figure 1. In the following sections, we provide more details on each step of each round. We give the pseudocode for FedPM in Appendix A.\n\n3.1.1 LOCAL TRAINING OF PROBABILITY MASKS\n\nUpon receiving a global probability mask θg,t−1 from the server at the beginning of round t, the client k performs local training and updates the mask via back-propagation. First, however, we have to guarantee that the updated probability mask satisfies θk,t ∈ [0, 1]d. While this can be achieved with a regularization term in the loss, this may require clipping θk,t ∈ [0, 1]d before taking a Bernoulli sample, especially in the early training stages. Clipping would then make the estimate at the server biased and hence lead to a slower convergence and lower accuracy. Therefore, similarly to the work of Zhou et al. (2019), we introduce another mask, called score mask s = (s1, s2, . . . , sd) ∈ Rd, that has unbounded support and can be used to generate the probability masks through the oneto-one sigmoid function by setting θ = Sigmoid(s). Then, the procedure for local training of the probability mask at round t is as follows (here, the steps from Step 2 to 4 describe one local iteration, which is repeated a number τ of times as standard in FL (McMahan et al., 2017a)):\n\n(1) The server sends the global probability mask θg,t−1 to K chosen clients, and the clients set sk,t = Sigmoid−1(θg,t−1), where Sigmoid−1(·) is the inverse of the sigmoid function.\n\n(2) Then, the clients generate a binary mask by first transforming back θk,t = Sigmoid(sk,t), and then sampling a binary mask M k,t from θk,t as shown in Figure 1: mk,t ∼ Bern(θk,t).\n\n(3) The sampled binary mask then sparsifies the initial weight vector winit:\n\n ̇wk,t = mk,t ⊙ winit.\n\n(4) ̇wk,t is then used for forward pass, and the loss L(f ̇wk,t , Dk) on the local task is backpropagated to update the score mask as sk,t = sk,t − η∇L(f ̇wk,t , Dk) (η is the local learning rate).\n\nAll the local operations from Step 2 to Step 4 are differentiable, except for the Bernoulli sampling. We backpropagate the gradients through the Bernoulli sampling operation with a straight-through estimator Bengio et al. (2013), using the first-order gradient of the Bernoulli function, which is simply equal to the probability mask θk,t.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Communication-efficient estimation of the mean of the probability masks ̄θg,t. Each client communicates a stochastic binary mask mk,t sampled from the local probabillity mask θk,t. We reduce the bitrate to less than 1 bit per parameter by using arithmetic coding to encode mk,t. When the frequency of 1’s is far from 0.5 (which is usually the case with FedPM), the number of bits per parameter to communicate mk,t is less than 1. See Figure 3 for more details.\n\n3.1.2 COMMUNICATION STRATEGY\n\nOnce the local training at round t is completed, the server needs to distill the global probability mask θg,t, by taking the empirical average of the local probability masks ̄θg,t = 1 θk,t collected from the clients. However, since we aim for communication efficiency, the clients do not send their local probability masks directly. Instead, they communicate a stochastic binary sample M k,t from their probability masks sampled as mk,t ∼ Bern(θk,t), and then the server estimates the global aggregate ̄θg,t as ˆ ̄θg,t = 1 (cid:80) mk,t. This distributed mean estimation problem with communication constraints is summarized in Figure 2. Our estimator ˆ ̄θg,t = 1 mk,t is an unbiased estimate of the true aggregate, in that\n\nk∈Kt\n\nk∈Kt\n\nk∈Kt\n\n(cid:80)\n\n(cid:80)\n\nK\n\nK\n\nK\n\nEM k,t∼Bern(θk,t) ∀k∈Kt[ˆ ̄θg,t] = EM k,t∼Bern(θk,t) ∀k∈Kt\n\n(cid:34)\n\n1 K\n\n(cid:35)\n\n(cid:88)\n\nM k,t\n\nk∈Kt\n\nEM k,t∼Bern(θk,t)[M k,t]\n\nθk,t\n\n=\n\n=\n\n1 K\n\n1 K\n\n(cid:88)\n\nk∈Kt (cid:88)\n\nk∈Kt\n\n= ̄θg,t.\n\nMoreover, the estimation error is upper bounded as (the proof is given in Appendix B)\n\nEM k,t∼Bern(θk,t) ∀k∈Kt\n\n(cid:2)||ˆ ̄θg,t − ̄θg,t||2\n\n2\n\n(cid:3) ≤\n\nd 4K\n\n.\n\n(2)\n\nSince each client communicates a stochastic binary mask M k,t, 1 bpp is the worst-case bitrate for FedPM. We can further reduce the bitrate to less than 1 by using arithmetic coding (Rissanen & Langdon, 1979) or universal coding (Krichevsky & Trofimov, 1981; Barron et al., 1998) to encode mk,t, and achieve the empirical entropy since d is large. This gives us smaller bitrates whenever the frequency of 1’s in mk,t is far from 0.5 – which is usually the case for our method (see Figure 3 and Appendix E.2 for results). We note that, with a deterministic mask training approach as in FedMask (Li et al., 2021), arithmetic coding of mk,ts does not provide any further gain in bitrate, as we have empirically observed that the frequency of 1’s is always around 0.5 (see Figure 3 and Appendix E.2) – here we apply arithmetic coding for FedMask to improve our baseline although it was not proposed in the original paper. Moreover, FedMask (Li et al., 2021) and HideNSeek (Vallapuram et al., 2022) do not enjoy the guarantees we have as their estimator (i) is not unbiased and (ii) does not have an upper bound on the estimation error due to hard thresholding (Li et al., 2021) and sign operations (Vallapuram et al., 2022). This is another benefit of our stochastic sampling approach.\n\n5\n\n......Server EstimateTrue MeanPublished as a conference paper at ICLR 2023\n\n3.2 FedPM WITH BAYESIAN AGGREGATION\n\nAnother important aspect that differentiates our work from existing masking methods such as FedMask (Li et al., 2021) and HideNSeek (Vallapuram et al., 2022) is the Bayesian aggregation strategy, which exploits the underlying stochastic mask to synthesize a global model, boosting the performance in scenarios where only a fraction of the clients participate in each round. Given the probabilistic interpretation of the FedPM mask’s values, at the server side we further model the probability mask θg,t with a Beta distribution Beta(αg,t, βg,t), parameterized by the round-dependent parameters αg,t and βg,t, which are initialized to αg,0 = βg,0 = λ0. At the beginning of the training process, there is no prior knowledge indicating which network weight should be more important than the others, and so each entry in the probability mask is uniformly distributed in [0, 1] – which is the prior distribution. Consequently, the clients’ local binary masks M k,ts are the data the server uses to update its belief on each weight score, and so the aggregation strategy corresponds now to a posterior update. Specifically, given the conjugate relation between the Beta-Bernoulli distributions, the new posteriors are still Beta distributions with parameters\n\nαg,t = αg,t−1 + M agg,t\n\nand βg,t = βg,t−1 + K · 1 − M agg,t ∀t ≥ 1,\n\n(3)\n\nwhere M agg,t = (cid:80) casts to the clients the mode of the Bernoulli distributions, as suggested by Ferreira et al. (2021),\n\nM k,t, and 1 is the d-dimensional all-ones vector. Then, the server broad-\n\nk∈Kt\n\nθg,t =\n\nαg,t − 1 αg,t + βg,t − 2\n\n,\n\n(4)\n\nwhere the division operation is applied element-wise. However, to obtain the best performance out of this method, the Beta parameters should be re-initialized to their original values λ0 with some regularity. We present an ablation study to demonstrate the improvements gained by the Bayesian aggregation strategy and the reasonable choices for the resetting frequency in Section 4.3. Notice that if λ0 = 1, and if α and β are re-initialized at the beginning of each round, the method is equivalent to the aggregation strategy detailed in Section 3.1.2.\n\n3.3 WEIGHT DISTRIBUTION\n\nAs mentioned in Section 3.1, the fixed weight vector winit is initialized by sampling from the distribution Pw using the randomly generated SEED. We note that the choice of this distribution impacts two important aspects of FedPM: (i) the values of winit highly influence the final accuracy achieved by the model, as they represent the building blocks to extract a subnetwork f ̇w (see Figure 1), which should be rich enough to solve the learning task, and (ii) the size of the sample space of Pw affects the number of bits needed to store the model during the inference process (this is different from the 1 bpp model storage when the model is not in use). Regarding (i), as also proposed in Ramanujan et al. (2020), we sample weights from a uniform distribution, whose domain is {−σ, +σ}, where σ is the standard deviation of the Kaiming Normal distribution (He et al., 2015). In this way, we control the variance of the neurons’ output to be ∼ 1, which avoids the vanishing or the explosion of activation values. Previous experiments in (Zhou et al., 2019; Ramanujan et al., 2020) also demonstrate the superior performance achieved by binary weights distributions when compared to standard continuous counterparts, e.g., Gaussian. Regarding (ii), even if knowing the value of SEED is enough to perfectly reconstruct the vector winit, one would have to generate the entire vector at every inference step. Consequently, to achieve fast inference, the actual values of the weights need to be stored in the memory of the devices during the inference process. Fortunately, our initialization allows for efficient storage even during inference since (after reconstructing wfinal using SEED and mfinal ∈ {0, 1}d) we only need to indicate whether the weight values in wfinal are −σ, 0, or +σ, with a ternary representation that can be efficiently deployed on hardware (Alemdar et al., 2017).\n\n3.4 PRIVACY\n\nPrivacy is another challenge in FL as the model updates (in our case, M k,ts) may leak information about the client data. In Appendix C, we analyze the differential privacy guarantees of FedPM and give an initial foray into how FedPM can be helpful in amplifying privacy.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nIn this section, we empirically show the performance of FedPM in terms of accuracy, bitrate, converge speed, and the final model size. We consider four datasets: CIFAR-10 with 10 classes, CIFAR-100 (Krizhevsky et al., 2009) with 100 classes, MNIST (Deng, 2012) with 10 classes, and EMNIST (Cohen et al., 2017) with 47 classes. For CIFAR-100, we use a 10-layer convolutional network (CNN) CONV-10 and ResNet-18 He et al. (2016); for CIFAR-10, a 6-layer CNN CONV-6 and ResNet-18 He et al. (2016); and for MNIST and EMNIST, a 4-layer CNN CONV-4. A detailed description of the architectures can be found in Appendix D. Due to limited space, we provide the results on ResNet-18 in Appendix E.1. We first compare FedPM with SignSGD (Bernstein et al., 2018), TernGrad (Wen et al., 2017), QSGD (Alistarh et al., 2017), DRIVE (Vargaftik et al., 2021), EDEN (Vargaftik et al., 2022), and FedMask (Li et al., 2021) on IID data split and full client participation in Section 4.1. We then extend our experiments to non-IID data splits and partial participation in Section 4.2. Finally, in Section 4.3, we present a key ablation study to justify why the Bayesian aggregation strategy is necessary for partial participation and to demonstrate how the resetting frequency affects the convergence rate and the final accuracy. Clients perform 3 local epochs in all experiments. We provide additional details on the experimental setup in Appendix D. We present results averaged over 3 runs.\n\n4.1\n\nIID DATA SPLIT AND FULL PARTICIPATION (K = N )\n\nIn this section, we focus on IID data distribution and the case when all the clients participate in the training at each round. We set the number of clients to N = K = 10. We report the estimated bitrate for the arithmetic code that uses the empirical frequency of the symbols (for our method FedPM, this corresponds to the frequency of 1’s in mk,t) – which is equal to the empirical entropy for blocklength d as large as the model size. In Figure 3, we compare the accuracy, bitrate, and convergence speed of FedPM with relevant baselines. As can be seen in the figure, FedPM converges to the highest accuracy on all four datasets. DRIVE, EDEN, and QSGD (they mostly overlap in the accuracy plots) seem to be the three baselines that perform the best after FedPM; however, their convergence speed is significantly lower than FedPM. In terms of convergence speed, FedMask is the fastest among the baselines – in fact, at the beginning of the training, FedMask is faster than FedPM as well. However, its final accuracy is lower than the others. We also would like to highlight that while some of our baselines, such as FedMask and TernGrad, have a visibly high variance in accuracy, FedPM shows stable training behavior across all experiments.\n\nFigure 3: Accuracy and bitrate comparison of FedPM with SignSGD (Bernstein et al., 2018), TernGrad (Wen et al., 2017), QSGD (Alistarh et al., 2017), DRIVE (Vargaftik et al., 2021), EDEN (Vargaftik et al., 2022), and FedMask (Li et al., 2021), all performing in the same bitrate regime.\n\nIn terms of bitrate, SignSGD and FedMask consistently spend 1 bpp, which is the default number when a binary mask or sign mask is communicated. This means binary values (1’s and 0’s) are almost equally distributed in their masks, which prevents them from enjoying additional bitrate gains. Across all experiments, TernGrad has the highest bitrate. We would like to leave a note about the bitrate of QSGD. Unlike other baselines, including our work, QSGD can go down to very low bitrates by adjusting the number of levels in quantization. We have observed that in the extreme quantization case, QSGD underperforms FedPM. Then, we have decided to increase the number of quantization levels in QSGD to see if it improves the accuracy. However, as can be seen from the plots, even with bitrate larger than 1, QSGD still underperforms FedPM. The only two\n\n7\n\n050100150200250300350400Rounds0.10.20.30.40.50.60.70.8AccuracyServer Test Accuracy (conv6-cifar10)050100150200250300350400Rounds0.00.10.20.30.4Server Test Accuracy (conv10-cifar100)020406080100Rounds0.900.920.940.960.981.00Server Test Accuracy (conv4-mnist)050100150200250300350400Rounds0.80.91.01.11.2Bitrate (bits per parameter)Average Bitrate (conv6-cifar10)050100150200250300350400Rounds0.50.60.70.80.91.01.1Average Bitrate (conv10-cifar100)020406080100Rounds0.91.01.11.21.3Average Bitrate (conv4-mnist)Published as a conference paper at ICLR 2023\n\nbaselines that challenge FedPM in terms of bitrate are DRIVE and EDEN. While FedPM has lower bitrates on CIFAR-10 and EMNIST; DRIVE and EDEN have better bitrates on CIFAR-100 and MNIST. However, the accuracy of DRIVE and EDEN on these datasets (specifically CIFAR-100) is significantly lower than that of FedPM, with slower convergence.\n\nAs for the final model size, FedPM needs only 0.8 bpp for the CONV-6 model trained on CIFAR-10, 0.85 bpp for the CONV-10 model trained on CIFAR-100, 0.96 bpp for the CONV-4 model trained on MNIST, and 0.83 bpp for the CONV-4 model trained on EMNIST. On the other hand, other baselines that train a dense model, namely SignSGD, TernGrad, QSGD, DRIVE, and EDEN, would need to represent each weight with their full precision value, i.e., 32 bpp. This implies that FedPM provides around 38.6× improvement in the storage or the communication of the final model. Since FedMask also trains a sparse model, it enjoys a similar gain in the final model size requiring 1 bpp across all the models. Due to the stochastic masking procedure and uneven distribution of 1’s and 0’s in the binary masks, FedPM has up to 0.17 bpp improvement over the deterministic procedure in FedMask, which adds up to a large gain due to the huge model size.\n\nWe provide additional experimental results with ResNet-18 model on CIFAR-10 and CIFAR-100 datasets in Appendix E.1; and observe similar improvements over the baselines.\n\n4.2 NON-IID DATA SPLIT AND PARTIAL PARTICIPATION (K < N )\n\n(cid:80)\n\nn jj\n\nThis section considers more realistic scenarios, in which the local clients’ datasets are generated from slightly different data distributions. We focus on CIFAR-10 with CONV-6, and we compare FedPM against (i) the most promising baselines, which, based on the results of Section 4.1, are DRIVE, EDEN, and QSGD, and (ii) FedMask, as it is the only sparse baseline. To choose the size of each dataset |Dn| = Dn, for each client n ∈ {1, . . . , N }, an integer jn is sampled uniformly from {10, 11, . . . , 100}. Then, a coefficient pn = jn is computed, which represents the size of the local dataset Dn as a fraction of the size of the full dataset, i.e., the training set of CIFAR-10. In this way, highly unbalanced datasets can be generated from the central one. Moreover, since the task is a classification problem, we impose a maximum number of different labels, or classes, cmax, that one client can see. Consequently, clients need cooperation to learn the statistics of other classes’ distributions, as the test dataset contains samples from all classes. In addition, partial participation is also considered, meaning that at each round, the server uniformly samples a fraction ρ = K N of the clients to participate in the training round. This is motivated in real-world scenarios by the scarcity of physical communication network resources, which may limit the availability of part of the clients during one round. The maximum number of classes per local dataset is set to cmax ∈ {2, 4}, and the participation ratio is set to ρ ∈ {0.1, 0.2, 0.5, 1}. For ρ = 1 and ρ = 0.5, the total number of clients is set to N = 10 (and so K is equal to 10 and 5, respectively). For ρ = 0.2, we set N = 100 (and so K = 20), and for ρ = 0.1, we set N = 50 (and so K = 5), which is the worst scenario among all combinations, given the small amount of information the server can collect at the end of each round. When ρ = 1, for the FedPM algorithm, we keep the same aggregation strategy exposed in Section 3.1.2 and Figure 2; and we switch to the Bayesian aggregation method (see Section 3.2) when there is partial participation, i.e., when ρ < 1. Indeed, applying the Bayesian aggregation method is revealed to be crucial for achieving good accuracy when ρ < 1 and data are non-IID, obtaining a large gain with respect to the simpler version in Section 3.1.2, which resets the Beta priors at each round (or takes the average of the samples, as explained in Section 3.2). We elaborate\n\nTable 1: Average final accuracy ±σ in non-IID data split with cmax = 4 and 2, and client participation ratios ρ = {0.1, 0.2, 0.5, 1}, for FedPM, FedMask, and the strongest baselines in the IID experiments: EDEN, DRIVE, and QSGD. The training duration was set to tmax = 200 rounds.\n\nAlgorithm\n\nρ = 1\n\nρ = 0.5\n\nρ = 0.2\n\nρ = 0.1\n\ncmax = 4\n\ncmax = 2\n\nDRIVE (Vargaftik et al., 2021) EDEN (Vargaftik et al., 2022) QSGD (Alistarh et al., 2017) FedMask (Li et al., 2021) FedPM (Ours)\n\nDRIVE (Vargaftik et al., 2021) EDEN (Vargaftik et al., 2022) QSGD (Alistarh et al., 2017) FedMask (Li et al., 2021) FedPM (Ours)\n\n0.739 ± 0.005 0.717 ± 0.006 0.709 ± 0.006 0.531 ± 0.044 0.748 ± 0.003\n\n0.434 ± 0.025 0.535 ± 0.050 0.476 ± 0.033 0.420 ± 0.028 0.643 ± 0.016\n\n0.632 ± 0.010 0.665 ± 0.012 0.644 ± 0.014 0.435 ± 0.057 0.720 ± 0.007\n\n0.376 ± 0.014 0.461 ± 0.016 0.464 ± 0.002 0.387 ± 0.062 0.556 ± 0.031\n\n0.563 ± 0.005 0.565 ± 0.009 0.567 ± 0.010 0.434 ± 0.036 0.617 ± 0.021\n\n0.375 ± 0.015 0.380 ± 0.015 0.375 ± 0.026 0.285 ± 0.040 0.372 ± 0.004\n\n0.405 ± 0.018 0.360 ± 0.016 0.399 ± 0.020 0.362 ± 0.024 0.496 ± 0.007\n\n0.221 ± 0.003 0.219 ± 0.005 0.243 ± 0.014 0.197 ± 0.030 0.277 ± 0.003\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nmore on this observation with an ablation study in Section 4.3. We adopt a simple heuristic schedule to reset the priors: Reset every 3 rounds when ρ = 0.5 and ρ = 0.2, and every 10 rounds when ρ = 0.1. As expected, the smaller the ratio ρ, the larger the number of rounds we should wait before resetting the priors to collect more information from a much more diverse pool of clients (see Section 4.3 for a rule of thumb on the resetting frequency).\n\nTable 1 reports the results with cmax = 4 and 2. FedPM seems to outperform all the baselines in every configuration, as the Bayesian aggregation allows the server to collect more data before resetting the priors, which is important when clients’ data distributions are non-IID, and only a fraction of the clients participate in each round. This strategy can be seen as the FedPM counterpart of decreasing the learning rate (which we applied in the other dense compression-based baselines, like DRIVE, EDEN, and QSGD). It is seen from Table 1 that FedMask (Li et al., 2021) is struggling in the nonIID case, as applying a hard threshold on the scores to binarize the mask does not provide a proper way to implement multiple-rounds aggregation, emphasizing the benefit of the stochastic process in FedPM. It is interesting to notice that, especially when cmax = 4, the lower the value of ρ, the larger the gap between FedPM and the baselines, corroborating the fact that the Bayesian strategy can better deal with partial participation. Analysis of the communication bitrate is provided in Appendix. E.2.\n\n4.3 ABLATION STUDY ON THE BAYESIAN AGGREGATION STRATEGY\n\nIn this section, we try to answer two questions: (1) Is Bayesian aggregation really necessary? and (2) What is the effect of resetting frequency on the convergence rate and the final accuracy? We do this by analyzing the effect of different resetting frequencies of the Beta priors on the training behavior of FedPM with non-IID data split and partial client participation; and report the results in Figure 4. Hereafter, we denote with γ the number of aggregation rounds before resetting the priors. For instance, γ = 1 corresponds to resetting the priors at every iteration, which is equivalent to the aggregation method presented in Section 3.1.2. On the other extreme, γ = 200 indicates that the priors are never reset. It is seen that γ = 1 curves fluctuate significantly and never converge to the best accuracy in any setting, while γ = 200 curves look smoother but converge to the lowest accuracy in all settings. This intuitively makes sense because, as already mentioned in Section 3.2, by increasing the value of γ, we allow the server to consider the information coming from multiple rounds while updating the global parameters. Indeed, with partial participation and non-IID data, a single round’s updates may convey skewed information, depending on the level of data heterogeneity cmax, and client participation ratio ρ. As a rule of thumb for the resetting frequency value, we suggest tuning γ around the value 1 ρ . The rationale behind this is that with uniform client sampling, at least 1 ρ\nrounds are needed to have the non-zero probability to sample from each client once before resetting the prior. In practice, we do not need to sample exactly from every client, as enough information is contained in the updates of the other sampled ones.\n\nFigure 4: Accuracy for different values of γ – the number of rounds before resetting the priors.\n\n5 CONCLUSION\n\nIn this work, we introduced Federated Probabilistic Mask Training (FedPM) – a communicationefficient FL strategy. FedPM relies on the idea of finding a sparse network in a randomly initialized dense network, which is then sparsified by a collaboratively trained stochastic binary mask. In addition to reducing the communication cost to less than 1 bit per parameter (bpp), FedPM also reaches higher accuracy with faster convergence than the relevant baselines, and can potentially amplify privacy while additionally outputting a compressed final model with a size less than 1 bpp. Throughout the manuscript, we highlighted the advantages of having a stochastic mask training approach rather than a deterministic one in terms of accuracy, bitrate, and privacy.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n6 ETHICS STATEMENT\n\nAll the experiments in the paper were performed on publicly available datasets. When we evaluated our strategy, we only considered accuracy as a measure of performance. However, as pointed out by Hooker et al. (2020), compression methods may disproportionately impact different subgroups of the data. We agree that this may potentially create a fairness issue in all communication-efficient federated learning frameworks and deserves more attention from the community.\n\n7 REPRODUCTION STATEMENT\n\nThe codebase for this work is open-sourced at https://github.com/BerivanIsik/ sparse-random-networks. All the hyperparameters necessary to reproduce the results in the paper can be found in Appendix D. We only used publicly available standard datasets and included links to them in the manuscript.\n\n8 ACKNOWLEDGEMENT\n\nThe authors would like to thank the anonymous reviewers and area chairs who provided valuable feedback; and Zachary Charles, Mahdi Haghifam, Peter Kairouz, and Nicole Mitchell for inspiring discussions. This work was supported in part by a Sony Stanford Graduate Fellowship, a National Science Foundation (NSF) award, a Meta research grant, and the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on “Telecommunications of the Future” (PE0000001 - program “RESTART”).\n\nREFERENCES\n\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the ACM SIGSAC conference on computer and communications security, pp. 308–318, 2016.\n\nNaman Agarwal, Peter Kairouz, and Ziyu Liu. The skellam mechanism for differentially private federated learning. Advances in Neural Information Processing Systems, 34:5052–5064, 2021.\n\nAlham Aji and Kenneth Heafield. Sparse communication for distributed gradient descent.\n\nIn EMNLP 2017: Conference on Empirical Methods in Natural Language Processing, pp. 440–445. Association for Computational Linguistics (ACL), 2017.\n\nMaxwell M Aladago and Lorenzo Torresani. Slot machines: Discovering winning combinations of random weights in neural networks. In International Conference on Machine Learning, pp. 163–174. PMLR, 2021.\n\nHande Alemdar, Vincent Leroy, Adrien Prost-Boucle, and Fr ́ed ́eric P ́etrot. Ternary neural networks for resource-efficient AI applications. In International Joint conference on Neural Networks (IJCNN), pp. 2547–2554, 2017.\n\nDan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.\n\nQSGD: Communication-efficient SGD via gradient quantization and encoding. Advances in Neural Information Processing Systems, 30, 2017.\n\nGalen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially private learning with adaptive clipping. Advances in Neural Information Processing Systems, 34:17455– 17466, 2021.\n\nSara Babakniya, Souvik Kundu, Saurav Prakash, Yue Niu, and Salman Avestimehr. Federated sparse training: Lottery aware model compression for resource constrained edge. arXiv preprint arXiv:2208.13092, 2022.\n\nB. Balle, G Barthe, and M. Gaboardi. Privacy amplification by subsampling: tight analyses via\n\ncouplings and divergences. Advances in neural information processing systems, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBorja Balle, Peter Kairouz, Brendan McMahan, Om Thakkar, and Abhradeep Guha Thakurta. Privacy amplification via random check-ins. In Advances in Neural Information Processing Systems, 2020.\n\nLeighton Pate Barnes, Huseyin A Inan, Berivan Isik, and Ayfer ̈Ozg ̈ur. rtop-k: A statistical estimation approach to distributed SGD. IEEE Journal on Selected Areas in Information Theory, 1(3): 897–907, 2020.\n\nAndrew Barron, Jorma Rissanen, and Bin Yu. The minimum description length principle in coding\n\nand modeling. IEEE transactions on information theory, 44(6):2743–2760, 1998.\n\nRan Ben Basat, Shay Vargaftik, Amit Portnoy, Gil Einziger, Yaniv Ben-Itzhak, and Michael Mitzenmacher. Quick-fl: Quick unbiased compression for federated learning. arXiv preprint arXiv:2205.13341, 2022.\n\nYoshua Bengio, Nicholas L ́eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n\nJeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. In International Conference on\n\nsignsgd: Compressed optimisation for non-convex problems. Machine Learning, pp. 560–569. PMLR, 2018.\n\nSameer Bibikar, Haris Vikalo, Zhangyang Wang, and Xiaohan Chen. Federated dynamic sparse training: Computing less, communicating less, yet learning better. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 6080–6088, 2022.\n\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending MNIST to handwritten letters. In International Joint Conference on Neural Networks (IJCNN), pp. 2921–2926, 2017.\n\nRong Dai, Li Shen, Fengxiang He, Xinmei Tian, and Dacheng Tao. Dispfl: Towards communicationarXiv preprint\n\nefficient personalized federated learning via decentralized sparse training. arXiv:2206.00187, 2022.\n\nLi Deng. The MNIST database of handwritten digit images for machine learning research. IEEE\n\nSignal Processing Magazine, 29(6):141–142, 2012.\n\nJames Diffenderfer and Bhavya Kailkhura. Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning a randomly weighted network. In International Conference on Learning Representations, 2020.\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pp. 265–284. Springer, 2006.\n\n ́Ulfar Erlingsson, Vitaly Feldman,\n\nIlya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep Thakurta. Amplification by shuffling: From local to central differential privacy via In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algoanonymity. rithms, pp. 2468–2479, 2019.\n\nVitaly Feldman, Audra McMillan, and Kunal Talwar. Hiding among the clones: A simple and nearly optimal analysis of privacy amplification by shuffling. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 954–964, 2022. doi: 10.1109/FOCS52979.2021. 00096.\n\nPaulo Abelha Ferreira, Pablo Nascimento da Silva, Vinicius Gottin, Roberto Stelling, and Tiago Calmon. Bayesian signSGD optimizer for federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n\nnetworks. In International Conference on Learning Representations, 2018.\n\nAntonious M. Girgis, Deepesh Data, and Suhas Diggavi. Differentially private federated learning with shuffling and client self-sampling. In 2021 IEEE International Symposium on Information Theory (ISIT), pp. 338–343, 2021. doi: 10.1109/ISIT45174.2021.9517906.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nFarzin Haddadpour, Belhal Karimi, Ping Li, and Xiaoyun Li. Fedsketch: Communication-efficient\n\nand private federated learning via sketching. arXiv preprint arXiv:2008.04975, 2020.\n\nFarzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated learning with compression: Unified analysis and sharp guarantees. In International Conference on Artificial Intelligence and Statistics, pp. 2350–2358. PMLR, 2021.\n\nBurak Hasircioglu and Deniz Gunduz. Privacy amplification via random participation in federated learning. 2022. doi: 10.48550/ARXIV.2205.01556. URL https://arxiv.org/abs/ 2205.01556.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In IEEE International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015. doi: 10.1109/ICCV.2015.123.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. Characterising\n\nbias in compressed models. arXiv preprint arXiv:2010.03058, 2020.\n\nJacob Imola and Kamalika Chaudhuri. Privacy amplification via bernoulli sampling. arXiv preprint\n\narXiv:2105.10594, 2021.\n\nBerivan Isik, Tsachy Weissman, and Albert No. An information-theoretic justification for model In International Conference on Artificial Intelligence and Statistics, pp. 3821–3846.\n\npruning. PMLR, 2022.\n\nShaoxiong Ji, Wenqi Jiang, Anwar Walid, and Xue Li. Dynamic sampling and selective masking for\n\ncommunication-efficient federated learning. arXiv preprint arXiv:2003.09603, 2020.\n\nYuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K Leung, and Leandros Tassiulas. Model pruning enables efficient federated learning on edge devices. IEEE Transactions on Neural Networks and Learning Systems, April 2022.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\nJakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt ́arik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.\n\nRaphail Krichevsky and Victor Trofimov. The performance of universal encoding. IEEE Transac-\n\ntions on Information Theory, 27(2):199–207, 1981.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nAng Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryfl: Personalized and communication-efficient federated learning with lottery ticket hypothesis on non-iid datasets. arXiv preprint arXiv:2008.03371, 2020.\n\nAng Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. Fedmask: Joint computation and communication-efficient personalized federated learning via heterogeneous masking. In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems, pp. 42–55, 2021.\n\nSheng Lin, Chenghong Wang, Hongjia Li, Jieren Deng, Yanzhi Wang, and Caiwen Ding. Esmfl:\n\nEfficient and secure models for federated learning. arXiv preprint arXiv:2009.01867, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nYujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In International Conference on Learning Representations, 2018.\n\nYang Liu, Yi Zhao, Guangmeng Zhou, and Ke Xu. Fedprune: Personalized and communicationefficient federated learning on non-iid data. In International Conference on Neural Information Processing, pp. 430–437. Springer, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017a.\n\nH Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private\n\nrecurrent language models. arXiv preprint arXiv:1710.06963, 2017b.\n\nIlya Mironov. R ́enyi differential privacy. In IEEE 30th computer security foundations symposium\n\n(CSF), pp. 263–275. IEEE, 2017.\n\nNicole Mitchell, Johannes Ball ́e, Zachary Charles, and Jakub Koneˇcn`y.\n\nOptimizing the communication-accuracy trade-off in federated learning with rate-distortion theory. arXiv preprint arXiv:2201.02664, 2022.\n\nAmirkeivan Mohtashami, Martin Jaggi, and Sebastian Stich. Masked training of neural networks with partial gradients. In International Conference on Artificial Intelligence and Statistics, pp. 5876–5890. PMLR, 2022.\n\nHamid Mozaffari, Virat Shejwalkar, and Amir Houmansadr. Frl: Federated rank learning. arXiv\n\npreprint arXiv:2110.04350, 2021.\n\nMuhammad Tahir Munir, Muhammad Mustansar Saeed, Mahad Ali, Zafar Ayyub Qazi, and arXiv preprint\n\nFedprune: Towards inclusive federated learning.\n\nIhsan Ayyub Qazi. arXiv:2110.14205, 2021.\n\nEmre Ozfatura, Kerem Ozfatura, and Deniz G ̈und ̈uz.\n\ncommunication-efficient federated learning. Theory (ISIT), pp. 461–466. IEEE, 2021.\n\nTime-correlated sparsification for In IEEE International Symposium on Information\n\nAnkit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopoulos. Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient. Advances in Neural Information Processing Systems, 33:2599–2610, 2020.\n\nVivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. What’s hidden in a randomly weighted neural network? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11893–11902, 2020.\n\nAmirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-efficient federated learning method with periodic averaging and quanIn International Conference on Artificial Intelligence and Statistics, pp. 2021–2031. tization. PMLR, 2020.\n\nJorma Rissanen and Glen G Langdon. Arithmetic coding. IBM Journal of research and development,\n\n23(2):149–162, 1979.\n\nDaniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman, Joseph Gonzalez, and Raman Arora. Fetchsgd: Communication-efficient federated learning with sketching. In International Conference on Machine Learning, pp. 8253–8265. PMLR, 2020.\n\nFelix Sattler, Simon Wiedemann, Klaus-Robert M ̈uller, and Wojciech Samek. Robust and communication-efficient federated learning from non-iid data. IEEE transactions on neural networks and learning systems, 31(9):3400–3413, 2019.\n\nSejin Seo, Seung-Woo Ko, Jihong Park, Seong-Lyun Kim, and Mehdi Bennis. Communicationefficient and personalized federated lottery ticket learning. In IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), pp. 581–585. IEEE, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAnanda Theertha Suresh, X Yu Felix, Sanjiv Kumar, and H Brendan McMahan. Distributed mean estimation with limited communication. In International conference on machine learning, pp. 3329–3337. PMLR, 2017.\n\nAnish K Vallapuram, Pengyuan Zhou, Young D Kwon, Lik Hang Lee, Hengwei Xu, and Pan Hui. Hidenseek: Federated lottery ticket via server-side pruning and sign supermask. arXiv preprint arXiv:2206.04385, 2022.\n\nShay Vargaftik, Ran Ben-Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben-Itzhak, and Michael Mitzenmacher. Drive: one-bit distributed mean estimation. Advances in Neural Information Processing Systems, 34:362–377, 2021.\n\nShay Vargaftik, Ran Ben Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben Itzhak, and Michael Mitzenmacher. Eden: Communication-efficient and robust distributed mean estimation for federated learning. In International Conference on Machine Learning, pp. 21984–22014. PMLR, 2022.\n\nThijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank gradient compression for distributed optimization. Advances in Neural Information Processing Systems, 32, 2019.\n\nHongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen Wright. Atomo: Communication-efficient learning via atomic sparsification. Advances in Neural Information Processing Systems, 31, 2018.\n\nYu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled renyi differential privacy and analytical moments accountant. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp. 1226–1235. PMLR, 16–18 Apr 2019.\n\nWei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. Advances in neural information processing systems, 30, 2017.\n\nSixing Yu, Phuong Nguyen, Ali Anwar, and Ali Jannesari. Adaptive dynamic pruning for non-iid\n\nfederated learning. arXiv preprint arXiv:2106.06921, 2021.\n\nHattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros,\n\nsigns, and the supermask. Advances in neural information processing systems, 32, 2019.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA FedPM ALGORITHM\n\nWe provide the pseudocode for FedPM in Algorithms 1 and 2. In Algorithm 2, the prior resetting scheduling policy is controlled by the procedure ResPrior(t), which may depend on quantities other than the round number t, such as loss.\n\nAlgorithm 1 FedPM. Hyperparameters: learning rate η, minibatch size B, number of local iterations τ . Inputs: local datasets Di, i = 1, . . . , N Output: random seed SEED and binary mask parameters mk,T\n\nAt the server, initialize a random network with weight vector winit ∈ Rd using a random seed SEED, and broadcast it to the clients. At the server, initialize the random score vector sg,0 ∈ Rd, and compute θg,0 ← Sigmoid(sg,0). At the server, initialize Beta priors αg,0 = βg,0 = λ0. for t = 1, . . . , T do\n\nSample a subset Kt ⊂ {1, . . . , N } of |Kt| = K clients without replacement. On Client Nodes: for k ∈ Kt do\n\nReceive θg,t−1 from the server and set sk,t = Sigmoid−1(θg,t−1). for l = 1, . . . , τ do\n\nθk,t ← Sigmoid(sk,t) Sample binary mask mk,t ∼ Bern(θk,t). ̇wk,t ← mk,t ⊙ winit gradsk,t ← 1 sk,t ← sk,t − η · gradsk,t\n\nb=1 ∇l( ̇wk,t; Bk\n\nj ); {Bk\n\n(cid:80)B\n\nB\n\nend for θk,t ← Sigmoid(sk,t) Sample a binary mask mk,t ∼ Bern(θk,t). Send the arithmetic coded binary mask mk,t to the server.\n\nj }B\n\nj=1 is uniformly chosen from Dk\n\nend for\n\nOn the Server Node: Receive mk,t’s from K client nodes. θg,t = BayesAgg( {mk,t}k∈Kt, t) Broadcast θg,t to all client nodes.\n\n// See Algorithm 2.\n\nend for Sample the final binary mask mfinal ∼ Bern(θg,T ). Generate the final model:\n\n ̇wfinal ← mfinal ⊙ winit.\n\nAlgorithm 2 BayesAgg. Inputs: clients’ updates {mk,t}k∈Kt, and round number t Output: global probability mask θg,t\n\nif ResPriors(t) then\n\nαg,t−1 = βg,t−1 = λ0\n\nmk,t.\n\nk∈Kt\n\nend if Compute magg,t = (cid:80) αg,t = αg,t−1 + magg,t βg,t = βg,t−1 + K · 1 − magg,t θg,t = αg,t−1 Return θg,t\n\nαg,t+βg,t−2\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB PROOF OF THE UPPER BOUND ON THE ESTIMATION ERROR\n\nWe now provide proof of the upper bound on the estimation error in Eq. 2. Recall that our true mean is ̄θg,t = 1 mk,t, where mk,t ∼ Bern(θk,t). Then we can compute the error as\n\nθk,t, whereas our estimate is ̄θg,t = 1\n\nk∈Kt\n\nk∈Kt\n\n(cid:80)\n\n(cid:80)\n\nK\n\nK\n\nEM k,t∼Bern(θk,t) ∀k∈Kt\n\ng,t\n\n(cid:2)||ˆ ̄θ\n\n− ̄θg,t||2\n\n2\n\n(cid:3) =\n\nd (cid:88)\n\ni=1\n\nE\n\nM k,t\n\ni ∼Bern(θk,t\n\ni\n\nE\n\nM k,t\n\ni ∼Bern(θk,t\n\ni\n\n) ∀k∈Kt\n\n(cid:20)(cid:16)ˆ ̄θg,t\n\ni − ̄θg,t\n\ni\n\n(cid:17)2(cid:21)\n\n(5)\n\n(cid:33)2\n\n(M k,t\n\ni − θk,t i )\n\n (6)\n\n(cid:88)\n\nk∈Kt\n\n) ∀k∈Kt\n\n\n\n(cid:32)\n\n\n\n1 K\n\n\n\n(cid:32)\n\n(cid:88)\n\n(cid:33)2\n\n(M k,t\n\ni − θk,t i )\n\n (7)\n\nE\n\nM k,t\n\ni ∼Bern(θk,t\n\ni\n\n) ∀k∈Kt\n\n\n\n(cid:34)\n\nE\n\nM k,t\n\ni ∼Bern(θk,t\n\ni\n\nk∈Kt\n\n(cid:88)\n\n(cid:16)\n\n) ∀k∈Kt\n\nk∈Kt\n\n(cid:17)2(cid:35)\n\nM k,t\n\ni − θk,t\n\ni\n\nM k,t\n\ni ∼Bern(θk,t\n\ni\n\n(cid:104)\n\n)\n\n(M k,t\n\ni − θk,t\n\ni )2(cid:105)\n\nE\n\nM k,t\n\ni ∼Bern(θk,t\n\ni\n\n)[(M k,t\n\ni\n\n)2] − (θk,t\n\ni )2(cid:17)\n\n(cid:16)\n\ni − (θk,t θk,t\n\ni )2(cid:17)\n\n(8)\n\n(9)\n\n(10)\n\n(11)\n\n(12)\n\nd (cid:88)\n\ni=1\n\nd (cid:88)\n\ni=1\n\nd (cid:88)\n\nd (cid:88)\n\ni=1\n\n1 K 2\n\n1 K 2\n\n1 K 2\n\n1 K 2\n\n1 K 2\n\n=\n\n=\n\n=\n\n=\n\n=\n\n=\n\n≤\n\nd 4K\n\n.\n\n(cid:88)\n\nE\n\ni=1\n\nk∈Kt\n\nd (cid:88)\n\n(cid:88)\n\n(cid:16)\n\ni=1\n\nk∈Kt\n\nd (cid:88)\n\n(cid:88)\n\ni=1\n\nk∈Kt\n\nFrom (5) to (6), we use the definition of ˆ ̄θg,t (cid:80)K i − θk,t to (8), we use the fact that E are independent for l ̸= k ∈ [K]. Finally, the inequality in (8) follows from θk,t k ∈ [K].\n\ni = 1 ] = 0; and M k,t\n\n(cid:80)K i − θk,t\n\nk=1 mk,t\n\ni ∼Bern(θk,t\n\ni = 1\n\nand ̄θg,t\n\n[M k,t\n\n) ∀k∈Kt\n\nM k,t\n\nK\n\nK\n\ni\n\ni\n\ni\n\ni\n\ni\n\nk=1 θk,t\n\nand M l,t\n\n. From (7) i − θl,t i ∈ [0, 1] for all\n\ni\n\nC PRIVACY AMPLIFICATION AND BIAS CORRECTION\n\nDifferential privacy (DP) guarantees that the probability of an outcome of an algorithm that runs on client data does not change much by a single client’s data. This is typically ensured via injecting noise to a function of the client data at a particular step in the algorithm with some utility loss in the application. While there have been many DP strategies developed for FL and deep learning (Abadi et al., 2016; McMahan et al., 2017b; Agarwal et al., 2021; Andrew et al., 2021), these strategies typically suffer from severe performance degradation due to noise injection. To make DP practical, researchers have explored certain randomization mechanisms that amplify the privacy guarantee. When these mechanisms are parts of the FL framework, such as sampling (data (Balle et al., 2018; Wang et al., 2019) or device (Balle et al., 2020; Girgis et al., 2021; Hasircioglu & Gunduz, 2022)) and shuffling (Erlingsson et al., 2019; Feldman et al., 2022), the amplification comes for free. This is helpful because the overall process can meet a stronger privacy guarantee without increasing the noise level. FedPM promises one such amplification due to the stochastic Bernoulli sampling step. We first revisit the definitions of differential privacy (Dwork et al., 2006), R ́enyi divergence, and R ́enyi differential privacy (Mironov, 2017); and then present the amplification result. Definition 1. [Adjacent Datasets] Two datasets D, D′ ∈ D are called adjacent if they differ in at most one data sample.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nDefinition 2. [(ε, δ)-DP] A randomized mechanism f : D → R offers (ε, δ)-differential privacy if for any adjacent D, D′ ∈ D and S ⊂ R\n\nPr[f (D) ∈ S] ≤ eεPr[f (D′ ∈ S)] + δ.\n\nDefinition 3. [R ́enyi Divergence] For two probability distributions P and Q defined over R, the R ́enyi divergence of order α > 1 is\n\nDα(P ||Q) =\n\n1 α − 1\n\nlog Ex∼Q\n\n(cid:18) P (x) Q(x)\n\n(cid:19)α\n\n.\n\nDefinition 4. [(α, ε)-RDP] A randomized mechanism f : D → R offers ε-R ́enyi differential privacy of order α (or in short (α, ε)-RDP) if for any adjacent D, D′ ∈ D, it holds that\n\nDα(f (D)||f (D′)) ≤ ε.\n\nIn particular, Imola & Chaudhuri (2021) have shown that when a sample M ∈ {0, 1}d from an already privatized vector θ ∈ [c, 1 − c]d, where 0 < c < 0.5, is released to a third party (instead of θ itself), the privacy is amplified under some conditions. More precisely, when there is an (α, ε)- R ́enyi Differential Privacy mechanism (Mironov, 2017) that privatizes θ ∈ [c, 1 − c]d, releasing a sample from Bern(θ) yields an improved privacy budget (the smaller ε, the better the privacy): εamp ≤ min {ε, d · rα(c)}. Here, rα(p) is the binary symmetric R ́enyi divergence function defined α−1 log (cid:0)pα(1 − p)1−α + (1 − p)αp1−α(cid:1). Notice that FedPM already involves this as rα(p) = 1 Bernoulli sampling step in the communication protocol and in the forward pass mk,t ∼ Bern(θk,t). However, the d term in the upper bound limits the amplification for large model sizes. We believe it is worth exploring a tighter upper bound on εamp to enjoy privacy amplification in FedPM with practical models. Nonetheless, in Appendix C, we demonstrate the impact of this amplification on a distributed mean estimation problem, described in Figure 2, where the goal is to estimate the true mean of the probability masks ̄θ = 1 k=1 θk under communication and privacy constraints. We also provide a bias correction mechanism, specific to our scheme in Figure 5 in Appendix C, that mitigates the bias due to the DP mechanism and reduces the estimation error.\n\n(cid:80)K\n\nK\n\nNow, suppose that we have an (α, ε)-RDP algorithm f that outputs privatized θk ∈ [c, 1 − c]d with 0 < c < 0.5, using local client data Dk. As summarized in Figure 5, we are interested in what happens when instead of releasing θk = f (Dk), the client k releases a Bernoulli sample from it: mk ∈ {0, 1}d ∼ Bern(θk). We already explained the advantages in terms of communication bitrate, estimation error, unbiasedness throughout the manuscript; however, this approach also amplifies the privacy guarantees, meaning that it makes the overall privacy budget smaller εamp ≤ ε. Quantitatively, Imola & Chaudhuri (2021) showed that after the Bernoulli sampling, the privacy budget of the overall process is\n\nεamp ≤ min {ε, drα(c)},\n\nwhere rα(·) is the R ́enyi divergence of the binary symmetric function. More precisely, consider P, Q random variables with support on {x1, x2} ⊂ Θ and let p = Pr[P = x1], 1 − p = Pr(Q = x1). Then the R ́enyi divergence is defined as\n\nrα(p) = Rα(P, Q) =\n\n1 α − 1\n\nlog (pα(1 − p)1−α + (1 − p)αp1−α).\n\nNotice that FedPM already involves this Bernoulli sampling step in the communication protocol and in the forward pass mk,t ∼ Bern(θk,t). This implies that FedPM improves the privacy guarantee without changing the privacy mechanism – e.g. without increasing the injected noise level. However, the d term in the upper bound limits the amplification for large model sizes. We believe it is worth exploring a tighter upper bound on εamp to enjoy privacy amplification in FedPM with practical models. Nonetheless, we demonstrate the impact of this amplification on a distributed mean estimation problem, described in Figure 5, where the probability masks θk ∈ [c, 1 − c]d are a function of client data Dk; and are first corrupted by Gaussian noise, and then clipped to the range\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n[c, 1 − c]d. Our goal is, as before, to estimate the true mean ̄θ = 1 θk by averaging the sampled binary masks, i.e., ˆ ̄θ = 1 mk. Differently from our previous experiments, we have privacy constraints now, meaning that we want to guarantee (ε, δ)-DP by injecting a Gaussian noise with variance σ2 = 2 ln (1.25/δ)∆2 N 2 and ∆2 is the l2-sensitivity d). We transfer the above amplification results of the probability masks (in our case ∆2 = (1 − 2c) in RDP to DP using the well-known relation:\n\nwith a small ε, where δ ≈ 1\n\nk∈Kt\n\nk∈Kt\n\n(cid:80)\n\n√\n\nε2\n\nK\n\nK\n\n2\n\n(cid:80)\n\nRemark C.1. Mironov (2017) showed that if f is an (α, ε)-RDP mechanism, it also satisfies (ε + log 1/δ\n\nα−1 , δ)-DP for any 0 < δ < 1.\n\nFigure 5: Distributed mean estimation scheme in FedPM, modified for differential privacy.\n\nSince clipping after the noise addition step would lead to bias in the estimated mean, we work out a bias correction mechanism. We denote with θ one general parameter at client k for one parameter, with ̃θ its noisy version, and with ˆθ = clip( ̃θ) its clipped version. Specifically, if ̃θ = θ + η is the noisy version of the parameter, where η ∼ N (0, σ2), then\n\nclip( ̃θ) =\n\n \n\n\n\n ̃θ, 1 − c, c,\n\nif if if\n\nc ≤ θ + η ≤ 1 − c θ + η > 1 − c θ + η < c.\n\n(13)\n\nWe now compute E where M ∼ Bern(θ). First of all, notice that\n\n(cid:105)\n\n(cid:104) ˆM\n\n, where ˆM ∼ Bern(ˆθ), to analyze the bias E\n\n(cid:105)\n\n(cid:104) ˆM\n\n− E [M ] = E\n\n(cid:105)\n\n(cid:104) ˆM\n\n− θ,\n\n(cid:105)\n\n(cid:104) ˆM\n\nE\n\n=\n\n(cid:90) 1\n\n0\n\n(cid:104) ˆM |ˆθ = ρ\n\n(cid:105)\n\nE\n\nf (ρ)dρ =\n\n(cid:90) 1\n\n0\n\nρf (ρ)dρ = E[ˆθ].\n\n18\n\n...Server EstimateTrue MeanPublished as a conference paper at ICLR 2023\n\nAnd we now compute the mean of the clipped parameter\n\n(cid:90) 1\n\n(cid:105)\n\n(cid:104)ˆθ\n\nE\n\n=\n\nρf (ρ)dρ\n\n0\n\n(cid:90) +∞\n\n−∞ (cid:90) c−θ\n\n−∞\n\n=\n\n=\n\nclip(θ + η)f (η)dη\n\n(cid:90) 1−c−θ\n\nc · f (η)dη +\n\n(θ + η) · f (η)dη +\n\nc−θ (cid:90) 1−c−θ\n\n(cid:90) 1−c−θ\n\nf (η)dη +\n\nc−θ\n\nc−θ\n\n= cΦσ(c − θ) + θ [Φσ(1 − c − θ) − Φσ(c − θ)] +\n\n+ (1 − c) (1 − Φσ (1 − c − θ))\n\n= cΦσ(c − θ) + θ\n\nηf (η)dη + (1 − c) (1 − Φσ (1 − c − θ))\n\n(cid:90) +∞\n\n1−c−θ\n\n(1 − c) · f (η)dη\n\n(cid:20)\n\ne\n\n−σ √\n\n2π\n\n−(1−c−θ)2 2σ2\n\n−(c−θ)2 2σ2\n\n− e\n\n(cid:21)\n\n+\n\n= 1 − c + [θ − 1 + c]Φσ(1 − c − θ) + [c − θ]Φσ(c − θ) +\n\n−(c−θ)2 2σ2\n\n−σe\n\n√\n\n2π\n\n(cid:104)\n\ne−2(c−θ)−1 − 1\n\n(cid:105)\n\n,\n\nwhere Φσ (·) is the cumulative distribution function of a Gaussian random variable with standard deviation σ, and zero mean. We use this relation to correct the bias in ˆ ̄θ. In practice, to adopt the bias-correction strategy, we sample the function E , which is a function of the true parameter θ, noise standard deviation σ, and clipping parameter c, at Q different points x1, . . . , xQ, i.e., different values for the uncorrupted θ, and we store the values in a table. Indeed, the values σ and c are set at the beginning of the training process, secretly shared among the participants, and never modified. (cid:105) Then, once the server computes an estimate for ˆθ, it corrects it by finding the closest outputs of E in the stored table, and it inverts the map by choosing the corresponding xi, i.e., the original θ.\n\n(cid:104)ˆθ\n\n(cid:104)ˆθ\n\n(cid:105)\n\ng,t\n\n− ̄θg,t||2\n\nWe conduct our experiments with N = 100 clients, each having independent probability masks with dimension d = 5 and range [0.2, 0.8], i.e., θ ∈ [0.2, 0.8]5. Figure 6 shows the estimation error ||ˆ ̄θ 2 under no noise injection case (i.e. no DP) with the black line. Recall that we want to reach a smaller estimation error and smaller ε (i.e., a stronger privacy guarantee). The red curve corresponds to the ε vs. estimation error behavior if Bernoulli sampling did not amplify the privacy. The blue curve shows the amplified ε (i.e. εamp ≤ ε) vs. estimation error behavior, and it overlaps with the red curve for ε values smaller than d · rα(c) = 8.96, where there is no privacy amplification, i.e., εamp = ε. However, notice that the blue line never reaches ε’s higher than this value due to amplification, while enjoying smaller estimation errors that the red curve can only achieve with very large ε. This shows the promise of FedPM in having a better privacy-accuracy performance than most baselines that do not have amplification. Finally, the green curve shows that bias correction improves this performance further even with ε < d · rα(c) = 8.96 by achieving lower estimation errors with the same ε.\n\nD ADDITIONAL EXPERIMENTAL DETAILS\n\nIn Table 2, we provide the architectures for all the models used in our experiments. Clients performed 3 local epochs with a batch size of 128 and a local learning rate of 0.1 in all the experiments. Notice that there is no server learning rate in FedPM; instead, we tune the prior resetting schedule in Bayesian aggregation for the experiments in Section 4.2. We conducted our experiments on NVIDIA Titan X GPUs on an internal cluster server, using 1 GPU per one run.\n\nIn the non-IID and partial participation experiments in Section 4.2, to distill the final model, we may apply both stochastic sampling, as during training, or a hard-threshold method, similar to the one adopted in FedMask (Li et al., 2021). In the latter, a binary mask coefficient mi is set to 1 if θi > αths, and 0 otherwise. For all experiments but one, when αths ∈ [0.4, 0.6], the thresholding test accuracy is always higher than the sampling method, and so we use the threshold method. However, in the extreme case cmax = 2 and ρ = 0.1, the optimal values for αmax were in [0.2, 0.4] and [0.6, 0.8]\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: The effect of privacy amplification and bias correction in the privacy budget (ε) vs. estimation error behavior. Comparing red and blue curves, we see that we can reach small estimation errors without increasing ε thanks to the amplification (see the vertical blue line at low estimation error.). While the red curve and blue curve overlap for ε < d · rα(c) = 8.96, in that regime, we benefit from our bias correction strategy to reach a lower error.\n\nTable 2: Architectures for CONV-4, CONV-6, and CONV-10 models used in the experiments.\n\nModel\n\nCONV-4\n\nCONV-6\n\nCONV-10\n\nConvolutional Layers\n\n64, 64, pool 128, 128, pool\n\n64, 64, pool 128, 128, pool 256, 256, pool\n\n64, 64, pool 128, 128, pool 256, 256, pool 512, 512, pool 1024, 1024, pool\n\nFully-Connected Layers\n\n256, 256, 10\n\n256, 256, 10\n\n256, 256, 100\n\nin all experiments, probably due to the high randomness given by the highly heterogeneous scenario. Consequently, for the last experiment, we just adopt the stochastic sampling strategy to evaluate the model, as further optimizing the αths means adapting to the test dataset, which may corrupt the ability of the model to generalize.\n\nE ADDITIONAL EXPERIMENTAL RESULTS\n\nE.1 ADDITIONAL EXPERIMENTS ON RESNET ARCHITECTURES\n\nIn this section, we provide additional experimental results with ResNet-18 (He et al., 2016) on CIFAR-10 and CIFAR-100 datasets. For these experiments, we focus on IID data distribution and the case when all the clients participate in the training at each round. We use the same hyperparameters from Section 4.1. We provide the details of the ResNet-18 architecture in Table 3 below.\n\nFigures 7 and 8 show the results on CIFAR-10 and CIFAR-100 datasets, respectively. It is seen that FedPM outperforms all the baselines in terms of accuracy. Although DRIVE and EDEN require\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: ResNet-18 architecture.\n\nName conv1\n\nResidual Block 1\n\nResidual Block 2\n\nResidual Block 3\n\nResidual Block 4\n\nComponent 3 × 3 conv, 64 filters. stride 1, BatchNorm\n\n(cid:21)\n\n(cid:21) (cid:20)3 × 3 conv, 64 filters 3 × 3 conv, 64 filters (cid:20)3 × 3 conv, 128 filters 3 × 3 conv, 128 filters (cid:20)3 × 3 conv, 256 filters 3 × 3 conv, 256 filters (cid:20)3 × 3 conv, 512 filters 3 × 3 conv, 512 filters\n\n(cid:21)\n\n(cid:21)\n\n× 2\n\n× 2\n\n× 2\n\n× 2\n\nOutput Layer\n\n4 × 4 average pool stride 1, fully-connected, softmax\n\nFigure 7: Accuracy and bitrate comparison of FedPM with baselines SignSGD (Bernstein et al., 2018), TernGrad (Wen et al., 2017), QSGD (Alistarh et al., 2017), DRIVE (Vargaftik et al., 2021), EDEN (Vargaftik et al., 2022), and FedMask (Li et al., 2021), with ResNet-18 on CIFAR-10.\n\nFigure 8: Accuracy and bitrate comparison of FedPM with baselines SignSGD (Bernstein et al., 2018), TernGrad (Wen et al., 2017), QSGD (Alistarh et al., 2017), DRIVE (Vargaftik et al., 2021), EDEN (Vargaftik et al., 2022), and FedMask (Li et al., 2021), with ResNet-18 on CIFAR-100.\n\napproximately 0.1 smaller bitrates than FedPM, they also reach lower accuracy. In summary, the advantages of FedPM discussed in the main manuscript carry over to ResNet-18 model as well.\n\nE.2 BITRATE CONSIDERATIONS ON NON-IID DATA\n\nWe now report the communication bitrate considerations on the non-IID data split experiments described in Section 4.2. Table 4 reports the average bitrate needed by different algorithms over the whole training process when cmax = 4 and cmax = 2. By simply multiplying the obtained average bitrate by the total number of rounds tmax = 200, we obtain the total number of bits one element in the global probability mask needs to converge to its final value, indicating the total amount of information communicated during the training process.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nWe first observe that both DRIVE and EDEN consume almost the same amount of bits no matter the system configuration and round number (very small variance), and it is instead model dependent (see Figure 3). On the contrary, FedPM and QSGD report higher bitrate variability, as it depends on both the training phase and system setting. As already observed in Section 4.1, FedMask balances almost uniformly the binary updates, leading to a bitrate that is basically fixed to 1. For both cmax = 4 and cmax = 2, FedPM yields the smallest bitrate when ρ = 1, whereas for the other scenarios, EDEN and DRIVE are slightly more efficient. We argue that this is motivated by the fact that, as the learning task becomes harder due to the high system heterogeneity, all the models struggle to converge to good and stable solutions, which means that FedPM is still uncertain about the weights’ importance probabilities θ, setting many of them close to 0.5. However, we think that this may be a useful feature of FedPM to quantify its internal uncertainty, which we will further analyze.\n\nAlgorithm\n\nρ = 1\n\nρ = 0.5\n\nρ = 0.2\n\nρ = 0.1\n\nDRIVE (Vargaftik et al., 2021) 0.885 ± 9 · 10−5 0.885 ± 1 · 10−4 0.885 ± 6 · 10−5 0.885 ± 1 · 10−4 EDEN (Vargaftik et al., 2022) 0.885 ± 1 · 10−4 0.885 ± 1 · 10−4 0.885 ± 8 · 10−5 0.885 ± 1 · 10−4\n\ncmax = 4 QSGD (Alistarh et al., 2017)\n\nFedMask (Li et al., 2021) FedPM (Ours)\n\n0.982 ± 0.027 1.000 ± 3 · 10−6 0.863 ± 0.077\n\n0.923 ± 0.029 1.000 ± 8 · 10−8 0.912 ± 0.056\n\n1.188 ± 0.034 1.000 ± 2 · 10−6 0.965 ± 1 · 0.01812\n\n0.910 ± 0.05 1.000 ± 6 · 10−7 0.996 ± 0.003\n\nDRIVE (Vargaftik et al., 2021) 0.885 ± 7 · 10−5 0.885 ± 2 · 10−4 0.885 ± 7 · 10−5 0.885 ± 2 · 10−4 EDEN (Vargaftik et al., 2022) 0.885 ± 1 · 10−4 0.885 ± 7 · 10−5 0.885 ± 6 · 10−5 0.885 ± 7 · 10−5\n\ncmax = 2 QSGD (Alistarh et al., 2017)\n\nFedMask (Li et al., 2021) FedPM (Ours)\n\n1.230 ± 0.043 1.000 ± 2 · 10−6 0.868 ± 0.076\n\n1.234 ± 0.038 1.000 ± 2 · 10−6 0.904 ± 0.063\n\n1.100 ± 0.01 1.000 ± 1 · 10−5 0.980 ± 0.014\n\n1.082 ± 0.01 1.000 ± 2 · 10−7 0.997 ± 0.01\n\nTable 4: Average bitrate ±σ over the whole training process in non-IID data split with cmax = 4 and cmax = 2, and partial participation with ratios ρ = {0.1, 0.5, 1}, for FedPM, FedMask, and the strongest baselines in the IID experiments: EDEN, DRIVE, and QSGD. The training duration was set to tmax = 200 rounds.\n\nTo conclude the analysis, we also report the FedPM bpp for the final model, which is an indication of the average number of bits needed per one parameter of the model. In the case of cmax = 4, the final model sizes are 0.79 bpp, 0.834 bpp, and 0.99 bpp, when ρ = {0.1, 0.5, 1}, respectively. When cmax = 2, the final model sizes are 0.8 bpp, 0.817 bpp, and 0.992 bpp. Consequently, at the end of the training process, FedPM remains the most efficient option, as already observed in Section 4.1.\n\n22",
    "reference": "# Summary Of The Paper\n\n**UPDATE**  \nI thank the authors for providing an extensive response. I carefully read it and discussed with the other reviewers. I'm glad the authors clarified their major design choices and supported them with new experiments. One issue that remains is that of limited evaluations, but with the extra support added in the new version of the paper, I feel more confident that the proposed methodology will generalize to other setups.\n\n---------\nThis paper studies communication-efficient federated learning with random sparsification of the network weights. The main idea of the work is to use the lottery ticket hypothesis and try to find a good subnetwork by estimating a sparsifying mask. The mask parameterized by a vector of probabilities $\\theta$, which, in turn, are estimated using a score mask $s$, whose softmax gives us $\\theta$. Moreover, the authors use multiple clients that run gradient descent to estimate the best mask and then communicate only the binary vector produced randomly from the local estimate. By receiving multiple random masks, the authors build a Bayesian estimator to produce a global model.\n\n# Strength And Weaknesses\n\nThe main strength of this work is in the presented empirical improvements on the considered benchmarks. The proposed method is shown to outperform all competitors both in terms of test accuracy and communication efficiency.\n\nWhat makes me concerned is the way the method was derived and studied. Most of the ideas do not look well-motivated. For instance, the authors explain that the mask $\\theta$ cannot be used with regularization because it may lead to a performance drop, so they instead use softmax. However, it is not apparent to me why there would be a performance drop with regularization but no performance drop with score mask. In the majority of cases, I think that an ablation study is required to show that the made choices were the right ones. As of now, the paper just selects a set of ideas that are not justified using theory and then proceed to study the empirical behaviour of the resulting method. I am left with the feeling that I do not understand why the method performs well and if it will generalize to other problems. This is especially troubling since the evaluations are limited to simple problems (CIFAR-10/100, EMNIST, small networks).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI do not see major issues with paper writing. The novelty aspect is explained quite well.\n\nSome minor issues:  \nPages 4-5: I do not understand why the authors sometimes write $M^{k,t}$ and sometimes $m^{k,t}$  \nPage 4: there seems to be a typo in the update rule for $s^{k,t}$ because both sides have the same indices  \nCan the authors use a higher resolution for Figures 2, 3, and 4? It'd be even better if the figures were produced from pdf files\n\n# Summary Of The Review\n\nWhile the work clearly explains the points that are novel, they do not seem justified or derived in an intuitive manner. The proposed modifications to prior work are somewhat arbitrary and do not follow a rigorous mathematical derivation or empirical evaluation of different options. While the experiments show that the resulting method performs better on a number of problems, it is not apparent that the same improvement would hold universally and where exactly the improvement comes from. For instance, the method converged to better test accuracy on conv10-cifar100, but why it is not explained why it is so.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nPOST-HOC CONCEPT BOTTLENECK MODELS\n\nMert Yuksekgonul, Maggie Wang, James Zou Stanford University {merty,maggiewang,jamesz}@stanford.edu\n\nABSTRACT\n\nConcept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (“the bottleneck”) and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model ”sees” in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via conceptlevel feedback can provide significant performance gains without using data from the target domain or model retraining. The code for our paper can be found in https://github.com/mertyg/post-hoc-cbm.\n\n1\n\nINTRODUCTION\n\nThere is growing interest in developing deep learning models that are interpretable and yet still flexible. One approach is concept analysis (Kim et al., 2018), where the goal is to understand if and how high-level human-understandable features are “engineered” and used by neural networks. For instance, we may like to probe a skin lesion classifier to understand if the Irregular Streaks concept is encoded in the embedding space of the classifier and used later to make the prediction.\n\nOur work builds on the earlier idea of concept bottlenecks, specifically Concept Bottleneck Models (CBMs) (Koh et al., 2020). Concept bottlenecks are inspired by the idea that we can solve the task of interest by applying a function to an underlying set of human-interpretable concepts. For instance, when trying to classify whether a skin tumor is malignant, dermatologists look for different visual patterns, e.g. existence of Blue-Whitish Veils can be a useful indicator of melanoma (Menzies et al., 1996; Lucieri et al., 2020). CBMs train an entire model in an end-to-end fashion by first predicting concepts (e.g. the presence of Blue-Whitish Veils), then using these concepts to predict the label.\n\nBy constraining the model to only rely on a set of concepts and an interpretable predictor, we can: explain what information the model is using when classifying an input by looking at the weights/rules in the interpretable predictor and understand when the model made a particular mistake due to incorrect concept predictions.\n\nWhile CBMs provide several of the benefits mentioned above, they have several key limitations:\n\n1. Data: CBMs require access to concept labels during model training, i.e. training data should be annotated with which concepts are present. Even though there are a number of densely annotated datasets such as CUB (Wah et al., 2011), this is particularly restrictive for real-world use cases, where training datasets rarely have concept annotations.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n2. Performance: CBMs often do not match the accuracy of an unrestricted model, potentially reducing the incentive to use them in practice. When the concepts are not enough to solve the desired task, it is not clear how to improve the CBM and match the original model performance, while retaining the interpretability benefits.\n\n3. Model editing: Koh et al. (2020) discuss intervening on the model to fix the prediction for a single input, yet it is not shown how to holistically edit and improve the model itself. Intervening only changes the model behavior for a single sample, but global editing changes the model behavior completely. When the model picks up an unintended cue, or learns spurious associations, using the latter approach and editing the concept bottleneck can improve the model performance more generally than an intervention tailored toward one specific input. Prior work on CBMs does not discuss how to globally edit a model’s behavior. Ideally, we would like to edit models with the help of human input in order to lower computational costs and remove assumptions about data access.\n\nOur contributions. In this work, we propose the Post-hoc Concept Bottleneck Model (PCBM) to address these important challenges. PCBMs can convert any pre-trained model into a concept bottleneck model in a data-efficient manner, and enhance the model with the desired interpretability benefits. When the training data does not have concept annotations, which is often the case, PCBM can flexibly leverage concepts annotated in other datasets and natural language descriptions of concepts. When applicable, PCBMs can remove the laborious concept annotation process by leveraging multimodal models to obtain concept representations; this results in richer and more expressive bottlenecks using natural language descriptions of a concept, making PCBMs more accessible in various settings. Furthermore, when the available concepts are not sufficiently rich, we introduce a residual modeling step to the PCBM to recover the original blackbox model’s performance. In experiments across several tasks, we show that PCBMs can be used with comparable performance compared to black-box models. While prior work (Koh et al., 2020) demonstrated the possibility of performing local model interventions to change individual predictions, here we propose interventions for changing global model behavior. Through user studies, we show that PCBM enables efficient global model edits without retraining or access to data from the target domain and that users can improve PCBM performance by using concept-level feedback to drive editing decisions.\n\n2 POST-HOC CONCEPT BOTTLENECK MODELS\n\nFigure 1: Post-hoc Concept Bottleneck Models. First, we learn the vectors in our concept bank. With the CAV approach, for each concept, e.g. stripes, we train a linear SVM to distinguish the embeddings of examples that contain the concept and use the vector normal to the boundary (CAV). When annotations are hard to obtain, we can leverage multimodal models and use the text encoder to map each concept to a vector. Next, we project the embeddings produced by the backbone onto the concept subspace defined by the set of vectors. We then train an interpretable predictor to classify the examples from their projections. When the concept library is incomplete, we can construct a PCBM-h by sequentially introducing a residual predictor that maps the embeddings to the target space.\n\n2\n\nProjection onto the Concept SubspaceEmbeddingsPredictionsBackboneLearning the Concept Bank...Stripes: 0.8Horse: 1.2Hands: -0.5Black: 0.7Mane: 0.9..Fitting the Concept BottleneckPost-hoc CBM (PCBM)Hybrid Post-hoc CBM (PCBM-h)Residual FittingPCBM predictionp(Zebra): 0.8PCBM-h predictionp(Zebra): 0.95“stripes”“horse”“mane”with Concept Activation Vectors (CAVs)with Multimodal ModelsText EncoderorInterpretable Predictor(e.g. Sparse Linear Layer)Published as a conference paper at ICLR 2023\n\nThere are two main steps to building a PCBM. We let f : X → Rd be any pretrained backbone model, where d is the size of the corresponding embedding space and X is the input space. For instance, f can be the image encoder of CLIP (Radford et al., 2021) or the model layers up to the penultimate layer of a ResNet (He et al., 2016). An overview of PCBMs can be found in 1, and we describe the steps in detail below.\n\n)}, that exhibit the concept, and negative examples Ni = {f (xn1), ..., f (xpNn\n\nLearning the Concept Subspace (C): To learn concept representations, we make use of CAVs (Concept Activation Vectors) (Kim et al., 2018). In particular, we first define a concept library I = {i1, i2, ..., iNc}, where Nc denotes the number of concepts. The concepts in the library can be selected by a domain expert or learned automatically from the data (Ghorbani et al., 2019; Yeh et al., 2020). For each concept i, we collect embeddings for the positive examples, denoted by the set Pi = {f (xp1 ), ..., f (xpNp )} that do not contain the concept. In practice, densely annotated datasets (Caesar et al., 2018; Fong & Vedaldi, 2018) are used to collect points that are positive/negative for the concept. Importantly, unlike CBMs, these samples can be different from the data used to train the backbone model. Following (Kim et al., 2018), we train a linear SVM using Pi and Ni to learn the corresponding CAV, that is, the vector normal to the linear classification boundary. We denote the CAV for concept i as ci. Let C ∈ RNc×d denote the matrix of concept vectors, where each row ci represents the corresponding concept i. Given an input, we project the embedding of the input onto the subspace spanned by concept vectors (the concept subspace). Particularly, we let fC(x) = projCf (x) ∈ RNc, where the ith entry is f (i) ∈ R. It is important to observe that we do not need to annotate the training data with concepts. Namely, the dataset used to learn concepts can be different from the original training data. In several of our experiments, we use held-out datasets to learn the concept subspace.\n\nC (x) = ⟨f (x),ci⟩\n\n||ci||2\n\n2\n\nLeveraging multimodal models to learn concepts: Annotating images with concepts is a laborious process. In practice, this can be an important roadblock before using CBMs. To address this, here we show that we can also leverage natural language concept descriptions and multimodal models to implement concept bottlenecks. This approach alleviates the need of collecting labeled data to construct the concept subspace. Multimodal models such as CLIP (Radford et al., 2021) have a text encoder ftext along with an image encoder, which maps a description to the shared embedding space. We leverage the text encoder to augment the process of learning concept vectors. For instance, we can have ctext stripes = ftext(“stripes”) where the concept vector for stripes is obtained by mapping the prompt using the text encoder. For each concept description, we can collect the text embeddings and construct our multimodal concept bank Ctext as our subspace.\n\nFor a given classification task, we can use ConceptNet (Speer et al., 2017) to obtain concepts that are relevant to these classes. ConceptNet is an open knowledge graph, where we can find concepts that have particular relations to a query concept. For instance, we can find relations of the form “A Cat has {whiskers, four legs, sharp claws, ..}”. Similarly, we can find ”parts” of a given class (e.g. ”bumper”, ”roof” for ”truck”), or the superclass of a given class (e.g. ”animal”, ”canine” for ”dog”). We restrict ourselves to five sets of relations for each class: the hasA, isA, partOf, HasProperty, MadeOf relations in ConceptNet. We collect all concepts that have these relations with classes in each classification task to build the concept subspace.\n\nLearning the Interpretable Predictor: Next, we define an interpretable predictor that maps the concept subspace to the model prediction. Concretely, let g : RNc → Y be an interpretable predictor, such as a sparse linear model or a decision tree, where Y = {1, 2, ..., K} denotes the label space. An interpretable predictor is desirable because it provides insight into which concepts the model is relying on when making a decision. If a domain expert observes a counter-intuitive phenomenon in the predictor, they can edit the predictor to improve the model. To learn the PCBM, we solve the following problem:\n\nmin g\n\nE (x,y)∼D\n\n[L(g(fC(x)), y)] +\n\nλ NcK\n\nΩ(g)\n\n(1)\n\nwhere fC = projCf (x) is the projection onto the concept subspace, L(ˆy, y) is a loss function such as cross-entropy loss, Ω(g) is a complexity measure to regularize the model, and λ is the regularization strength. Note that the concept subspace is fixed during PCBM training. In this work, we use sparse linear models to learn the interpretable predictor, where g(fC(x)) = wT fC(x) + b. We apply the softmax function to g if the problem is a classification problem. Similarly, we define\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nΩ(g) = α||w||1 + (1 − α)||w||2 the number of classes and concepts.\n\n2 to be the elastic-net penalty parameterized by α, then normalize by\n\nThe expressiveness of the concept subspace is crucial to PCBM performance. However, even when we have a rich concept subspace, concepts may not be enough to solve a task of interest. When the performance of the PCBM does not match the performance of the original model, users have less incentive to use interpretable models like concept bottlenecks. Next, we aim to address this limitation.\n\nRecovering the original model performance with residual modeling: What happens when the concept bank is not sufficiently expressive, and PCBM performs worse than the original model? For instance, there may be skin lesion descriptors that are not available in the concept library. Ideally, we would like to preserve the original model’s accuracy while retaining the interpretability benefits. Drawing inspiration from the semiparametric model literature on fitting residuals (H ̈ardle et al., 2004), we introduce Hybrid Post-hoc CBMs (PCBM-h). After fixing the concept bottleneck and the interpretable predictor, we re-introduce the embeddings to ‘fit the residuals’. In particular, we solve the following:\n\nE(x,y)∼D[L(g(fC(x)) + r(f (x)), y)]\n\nmin r\n\n(2)\n\nwhere r : Rd → Y is the residual predictor. Note that in Equation 2, while training the residual predictor, the trained concept bottleneck (the concept subspace (fC) and the interpretable predictor(g)) is kept fixed, i.e. fitting PCBM-hs is a sequential procedure. We hypothesize that the residual predictor will compensate for what is missing from the concept bank and recover the original model’s accuracy. We implement the residual predictor as a linear model, i.e. r(f (x)) = wT r f (x) + br. Given a trained PCBM-h, if we would like to observe model performance in the absence of the residual predictor, we can simply drop r (in other words, concept contributions do not depend on the residual step).\n\n3 EXPERIMENTS\n\nWe evaluate the PCBM and PCBM-h in challenging image classification and medical settings, demonstrating several use cases for PCBMs. We further address practical concerns and show that PCBMs can be used without a loss in the original model performance. We used the following datasets to systematically evaluate the PCBM and PCBM-h:\n\nCIFAR10, CIFAR100 (Krizhevsky et al., 2009) We use CLIP-ResNet50 (Radford et al., 2021) as the backbone model. For the concept bottleneck, we use 170 concepts introduced in (Abid et al., 2022) which are extracted from the BRODEN visual concepts dataset (Fong & Vedaldi, 2018). These include objects (e.g. dog), settings, (e.g. snow) textures (e.g. stripes), and image qualities (e.g. blurriness). The full list of concepts can be found in (Abid et al., 2022).\n\nCOCO-Stuff (Caesar et al., 2018) is a dataset derived from MS-COCO (Lin et al., 2014), consisting of scenes with various object annotations. Previous work (Singh et al., 2020) has shown that there are severe co-occurrence biases in this dataset. For instance, images from the wine glass often also have a dining table in the image. Singh et al. (2020) identifies these 20 classes with heavy co-occurrence biases and we train PCBMs to recognize these 20 objects, where we minimize the binary cross-entropy loss individually for each class. In this scenario, we again use CLIP-ResNet50 (Radford et al., 2021) as the backbone and BRODEN visual concepts as the set of concepts.\n\nCUB (Wah et al., 2011) In the 200-way bird identification task, we use a ResNet18 (He et al., 2016) trained on the CUB dataset1. We use the same training/validation splits and 112 concepts as in (Koh et al., 2020). These concepts include wing shape, back pattern, and eye color.\n\nHAM10000 (Tschandl et al., 2018) is a dataset of dermoscopic images, which contain skin lesions from a representative set of diagnostic categories. The task is detecting whether a skin lesion is benign or malignant. We use the Inception (Szegedy et al., 2015) model trained on this dataset, which is available from (Daneshjou et al., 2021). Following the setting in (Lucieri et al., 2020), we collect concepts from the Derm7pt (Kawahara et al., 2018) dataset. The 8 concepts obtained from this dataset include Blue Whitish Veil, Pigmented Networks, Regression Structures, which are reportedly associated with the malignancy of a lesion.\n\n1The CUB pretrained model is obtained from https://github.com/osmr/imgclsmob\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nSIIM-ISIC (Rotemberg et al., 2021) To test a real-world transfer learning use case, we evaluate the model trained on HAM10000 on a subset of the SIIM-ISIC Melanoma Classification dataset. We use the same concepts described in the HAM10000 dataset.\n\nWe use linear probing on top of the backbones for CIFAR, COCO, and ISIC to obtain the baseline model performance. For CUB and HAM10000, we use the out-of-the-box models trained on the respective datasets without any additional training. We emphasize that in all of our experiments except CUB, we learn concepts using a held-out dataset. Namely, while the original CBMs need to have concept annotations for training images, we remove this limitation by post-hoc learning of concepts with any dataset. To learn the concept subspace, we use 50 pairs of images for each concept to train a Linear SVM. We refer the reader to the Appendix for further details on datasets and training.\n\nPCBMs achieve comparable performance to the original model: In Table 1, we report results over these five datasets. PCBM achieves comparable performance to the original model in all datasets except CIFAR100, and PCBM-h matches the performance in all scenarios. Strikingly, PCBMs match the performance of the original model in HAM10000 and ISIC, with as few as 8 human-interpretable concepts. In CIFAR100, we hypothesize that the concept bank available is not sufficient to classify finer-grained classes, and hence there is a performance gap between the PCBM and the original model. When the concept bank is not sufficient to solve the task, PCBM-h can be introduced to recover the original model performance while retaining the benefits of PCBM (see next sections). Comparing to CBM: A comparison to CBM was not possible in most datasets, as CBMs cannot be trained due to the lack of dense annotations. To compare the data efficiency of both methods, we report a comparison of CBM to PCBM when trained on CUB in Appendix C, finding that CBM can only reach a similar performance to PCBM with 20x more annotations. Analyzing the residual predictor: To understand whether PCBM-h overrides PCBM predictions, in Appendix B, we look at the consistency between PCBM and PCBM-h predictions. We show that the residual component in PCBM-h intervenes only when the prediction is wrong, and fixes mistakes. When PCBM is confident, PCBM-h does not modify the prediction or significantly increase the confidence. In general, the residual component may dominate when the bottleneck is insufficient and future work can aim to explicitly limit the residual component, e.g. PIE (Wang et al., 2021) regularizer.\n\nTable 1: PCBMs achieve comparable performance to the original model. We report performance over different scenarios for the original model and PCBMs with concept datasets. In CIFAR100, PCBM performs poorly since the concept bank is not expressive enough to solve a finer-grained task; however, PCBM-h recovers the original model’s accuracy. Strikingly, PCBMs match the performance of the original model in HAM10000 and ISIC, with as few as 8 human-interpretable concepts. Original CBMs cannot be trained on CIFAR/HAM10000/ISIC/COCO-Stuff, as they do not have concept labels in the training dataset. The mean and standard errors are reported over 10 random seeds. We report AUROC for HAM10000 and ISIC, mAP for COCO-Stuff, and accuracy for CIFAR and CUB.\n\nCIFAR10\n\nCIFAR100\n\nCOCO-Stuff\n\nCUB\n\nHAM10000\n\nISIC\n\nOriginal Model PCBM PCBM-h\n\n0.888 0.777 ± 0.003 0.871 ± 0.001\n\n0.701 0.520 ± 0.005 0.680 ± 0.001\n\n0.770 0.741 ± 0.002 0.768 ± 0.01\n\n0.612 0.588 ± 0.008 0.610 ± 0.010\n\n0.963 0.947 ± 0.001 0.962 ± 0.002\n\n0.821 0.736 ± 0.012 0.801 ± 0.056\n\nTable 2: Concept Bottlenecks with CLIP concepts. When a concept bank is not available or is insufficient, we can use natural language descriptions of concepts with CLIP to implement CBMs.\n\nPCBM using CLIP concepts: Here, we show that when labeled examples to learn concepts are not available, we can use multimodal representations such as CLIP to generate concepts without the laborious annotation cost. Using ConceptNet, we obtain concept descriptions for CIFAR10, CIFAR100, and COCOStuff tasks (206, 527, and 822 concepts, respectively), using the relations described in Section 2. The concept subspace is obtained by using text embeddings for the concept descriptions generated by the ResNet50 variant of CLIP. In Table 2, we give an overview of the results. We observe that with a more expressive multimodal\n\nOriginal Model (CLIP) PCBM & labeled concepts PCBM & CLIP concepts PCBM-h & CLIP concepts\n\nCOCO-Stuff 0.770 0.741 ± 0.002 0.755 ± 0.001 0.769 ± 0.001\n\nCIFAR100 0.701 0.520 ± 0.005 0.600 ± 0.003 0.691 ± 0.006\n\nCIFAR10 0.888 0.777 ± 0.003 0.833 ± 0.003 0.874 ± 0.001\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nbottleneck, we can almost recover the original model accuracy in CIFAR10, and we reduce the performance gap in CIFAR100. CLIP contains extensive knowledge of natural images, and thus captures natural image concepts (e.g. red, thorns). However, CLIP is less reliable at representing more granular or domain-specific concepts (e.g. blue whitish veils) and hence this methodology is less applicable to more domain-specific applications. While there is a recent trend in training multimodal models for biomedical applications (Zhang et al., 2020; EleutherAI), these models are trained with smaller datasets and are less competitive. We hope that as these models improve, PCBMs will make it easier to deploy concept bottlenecks in more specialized domains.\n\nExplanations in PCBMs: In Figure 2, we provide sample concept weights in the corresponding PCBMs. For instance, in HAM10000, PCBMs use Blue Whitish Veils, Atypical Pigment Networks, and Irregular Streaks to identify malignant lesions, whereas Typical Pigment Networks, Regular Streaks, and Regular Dots and Globules are used to identify benign lesions. These associations are consistent with medical knowledge (Menzies et al., 1996; Lucieri et al., 2020). We observe similar cases in CIFAR-10 and CIFAR-100, where the class Rose is associated with the concepts Flower, Thorns and Red. In Appendix Section F, we further analyze the global explanations of COCO-Stuff and show that PCBM reveals co-occurrence biases in the training data. Note that the concept weights for PCBM-h are exactly the same as for PCBM since the residual model is fit after the concept weights are fixed. However, given the addition of the residual component, the net effect of concepts may be different. We also note that the concept weights discussed may not reflect the causal effect. Estimating the causal effect of a concept requires careful treatment (Goyal et al., 2019), e.g. accounting for the interactions between concepts, and we leave this to future work.\n\nFigure 2: Explaining Post-hoc CBMs. We report the top 3 largest weights in the linear layer for the shown classes. For instance, Blue Whitish Veils, Atypical Pigment Networks, and Irregular Streaks have large weights for classifying whether a skin lesion is malignant. These are consistent with dermatologists’ findings (Menzies et al., 1996).\n\n4 MODEL EDITING WITH POST-HOC CONCEPT BOTTLENECKS\n\nWhen we observe a counter-intuitive phenomenon or a spurious correlation in the concept bottleneck, can we make the model perform better by simple edit operations? In this section, we show that PCBMs come with the benefit of easy and global concept-level model feedback. Koh et al. (2020) demonstrate the ability of CBMs to incorporate local interventions. Namely, if a user identifies a misprediction in the concept bottleneck, they can intervene on the concept prediction, and change the prediction for the particular instance, we call this a local intervention or edit. Here, we are interested in editing models to perform ‘global’ edits. Namely, with simple concept-level feedback, we would like to edit a PCBM and change the entire model behavior. For instance, in our experiments, we investigate the use case of adapting a PCBM to a new distribution. Unlike most existing model editing approaches, we do not need any data from either the training or target domains to perform the model edit, which can be a significant advantage in practice when data is inaccessible. Given a trained PCBM, we edit our concept bottleneck by manipulating the concept weights. For simplicity, in the experiments below we only focus on positive weights in the linear model.\n\n6\n\nInterpretable Predictor(Sparse Linear Layer)HAM10000 CIFAR100Blue Whitish Veils0.459Atypical Pigment Nets 0.158Irregular Streaks0.131Class: MalignantRegular Streaks0.323Regular Dots and Globules0.151Typical Pigment Nets0.091Class: BenignCanine4.972Fur3.860Long Ears3.641Class: WolfFlower3.249Thorns2.214Red2.109Class: RoseAnalyzing Concept ImportancesPCBMPCBM with CLIP ConceptsPublished as a conference paper at ICLR 2023\n\n4.1 CONTROLLED EXPERIMENT: EDITING PCBM WHEN THE SPURIOUS CONCEPT IS KNOWN\n\nFor our editing experiments, we use the Metashift (Liang & Zou, 2022) dataset to simulate distribution shifts. We use 10 different scenarios where there is a distribution shift for a particular class between the training and test datasets. For instance, during training, we only use table images that also contain a dog in the image and test the model with table images where there is not a dog in the image. We denote the training domain as table(dog). We give more details and the results of all domains in the Appendix. We use an ImageNet pretrained ResNet50 variant as a backbone and the visual concept bank described in the Experiments section. Given a PCBM, we evaluate three editing strategies:\n\n1. Prune: We set the weight of a particular concept on a particular class prediction to 0, i.e. for a\n\nconcept indexed by i, we let ̃wi = 0.\n\n2. Prune+Normalize: After applying pruning, we rescale the concept weights. Let P denote the indices of positive weights that are pruned, ̃P denote the indices of weights that remain, and wP , w ̃P be corresponding weight vectors for the particular class. We rescale each weight to match the original norm of the vector by letting ∀i ∈ ̃P , ̃wi = wi(1+ ||wP ||1 ), leading to || ̃w||1 = ||w||1. ||w ̃P ||1 The normalization step alleviates the imbalance between class weights upon pruning concepts with large weights for a particular class.\n\n3. Fine-tune (Oracle): We compare our simple editing strategies to fine-tuning on the test domain, which can be considered an oracle. Particularly, we fine-tune the PCBM using samples from the test domain and then test the fine-tuned model with a set of held-out samples.\n\nIn the context of Metashift experiments, we simply edit the concept spuriously correlated with a particular class. Concretely, for the domain table(dog), we prune the weight of the dog concept for the class table. In Table 3, we report the result of our editing experiments over 10 scenarios. We observe that for PCBMs, the Prune+Normalize strategy can recover almost half of the accuracy gains achieved by fine-tuning on the original test domain. Further, we observe that PCBM-h obtains lower accuracy gains with model editing. In PCBM, we can remove the concept from the model, but in PCBM-h, there may still be leftover information about the spurious concept in the residual part of the model. This is in line with our expectations, i.e. PCBM-h is a ‘less’ interpretable but more powerful variant of PCBM. While it still offers partial editing improvements that would not be possible with the vanilla model, it does not bring as much improvement as the PCBM.\n\nTable 3: Model edits with Post-Hoc CBMs. We report results over 10 distribution shift experiments generated using Metashift. Accuracy and edit gains are averaged over 10 scenarios and are reported as mean ± standard error. We observe that very simple editing strategies in the concept subspace provide almost 50% of the gains made by fine-tuning on the test domain.\n\nUnedited\n\nPrune\n\nPrune + Normalize\n\nFine-tune (Oracle)\n\nPCBM Accuracy PCBM Edit Gain\n\n0.656 ± 0.025 —\n\n0.686 ± 0.026 0.029\n\nPCBM-h Accuracy PCBM-h Edit Gain\n\n0.657 ± 0.039 —\n\n0.672 ± 0.033 0.017\n\n0.750 ± 0.019 0.093\n\n0.713 ± 0.027 0.058\n\n0.859 ± 0.028 0.202\n\n0.861 ± 0.032 0.190\n\nEven though our edit strategy is extremely simple, we can recover almost half of the gains made by fine-tuning the model. It is particularly easy to use since it can be applied without fine-tuning or using any knowledge or data from the target domain. However, this methodology requires knowledge of the spurious concept in the training domain. This may not be realistic in practice, and in the next section, we turn to human users for guidance.\n\n4.2 USER STUDY: EDITING PCBM WITH HUMAN GUIDANCE\n\nOne of the advantages of pruning is that it is naturally amenable to bringing humans into the loop. Rather than attempting to automate the edits, we can rely on a human to pick concepts that make logical sense to prune. We show that users make fast and insightful pruning decisions that improve model performance on test data when there is a distribution shift. Notably, these decisions can be made even when the user knows little about the model architecture and has no knowledge of the true\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nunderlying spurious correlation in the training data. We conduct a user study where we explore the benefits of human-guided pruning on PCBM and PCBM-h performance.\n\nStudy Design: We construct 9 MetaShift scenarios with a distribution shift for one of the classes between the training and testing datasets, which we refer to as the ”shifted class”. For instance, all of the training images for the class Keyboard also contain a Cat in the image, whereas test images do not have a cat in the image. The list of the 9 scenarios can be found in the Appendix. For each scenario, we train a PCBM and a PCBM-h with CLIP concepts on the training dataset and task the users with making edits to the trained models by selecting concepts to prune. We use CLIP’s ResNet50 variant as our backbone, and leverage ConceptNet to construct the concept subspace.\n\nFigure 3 shows the concept-selection interface. We refer to the Appendix for the full set of instructions given to the user. The display consists of the classification task the model was trained on and the concepts with the ten most positive weights for the shifted class. The user selects a subset of the concepts to prune. Neither the original model accuracy nor the accuracy after pruning is revealed to the user until all scenarios are completed. The target audience for the study was machine learning practitioners and researchers; 30 volunteers participated for a total of 30 × 9 = 270 experiments. The IRB has determined that this does not require review.\n\nFigure 3: User Study Interface. We train PCBMs on MetaShift scenarios, each with a distribution shift between the training and test datasets. The user selects concepts to prune from the model.\n\nHuman-guided editing is fast and improves model accuracy: On average, users prune 3.65 ± 0.39 concepts in 34.3 ± 6.4s per scenario. We evaluate the effectiveness of user pruning by comparing the model accuracy after user pruning to the accuracy of the unedited model on a held-out dataset from the test domain. All 30/30 users improve the model accuracy when averaged over scenarios. 8/30 users improve model accuracy in all scenarios and 26/30 users improve model accuracy in at least 6 scenarios. We compare human-guided editing to three baselines: 1. Random Pruning: We uniformly select without replacement a subset of the top ten concepts to\n\nprune, matching the number of concepts pruned by the user for a fair comparison.\n\n2. Greedy Pruning (Oracle): We greedily select one of the top 10 concepts that, when pruned, improves the model accuracy the most. We match the number of concepts pruned by the user.\n\n3. Fine-tune (Oracle): The model is fine-tuned on samples from the test domain.\n\nTable 4: Human-guided editing improves model accuracy for PCBMs with CLIP concepts (N=30). We show the test accuracy on the shifted class averaged across 9 MetaShift scenarios, before and after the model is edited. Accuracies and edit gains are reported as mean ± standard error. (For the pruning strategies, we first compute the mean test accuracy across users within each scenario, then calculate the overall mean and standard error from the within-scenario means). We see that user pruning surpasses random pruning and can achieve nearly 50% of the accuracy gains made by fine-tuning and over 80% of the accuracy gains made by greedy pruning.\n\nUnedited\n\nRandom Prune\n\nUser Prune\n\nGreedy Prune (Oracle)\n\nFine-tune (Oracle)\n\nPCBM Accuracy PCBM Edit Gain\n\n0.620 ± 0.035 —\n\n0.604 ± 0.039 -0.016\n\n0.719 ± 0.042 0.099\n\nPCBM-h Accuracy PCBM-h Edit Gain\n\n0.642 ± 0.034 —\n\n0.622 ± 0.037 -0.020\n\n0.736 ± 0.034 0.094\n\n0.740 ± 0.041 0.120\n\n0.766 ± 0.034 0.124\n\n0.824 ± 0.049 0.204\n\n0.856 ± 0.018 0.224\n\nWe consider greedy pruning and retraining to be oracles since they require ”leaked” knowledge of the test domain. Table 4 reports the model performance improvements for PCBM and PCBM-h, averaged over all 270 experiments in the user study. Improvements within each separate scenario can be found in the Appendix. We note that even with the residual component in PCBM-h, we can still improve model performance by editing the concept bottleneck. We observe that user pruning achieves marked improvement in model performance compared to the unedited model, attaining around 50% of the accuracy gains from fine-tuning and 80% of the gains from greedy pruning.\n\n8\n\nRemove concepts from the model that you think may hinder generalization for classifying Keyboard.The model was trained to distinguish Beach, Bus, Airplane, Keyboard and BirdPublished as a conference paper at ICLR 2023\n\n5 RELATED WORKS\n\nConcepts Using human concepts to interpret model behavior has been drawing increasing interest (Kim et al., 2018; Bau et al., 2017; 2020). Related work focuses on understanding if neural networks encode and use concepts (Lucieri et al., 2020; Kim et al., 2018; McGrath et al., 2021), or generate counterfactual explanations to understand model behavior (Ghandeharioun et al., 2021; Abid et al., 2022; Akula et al., 2020). Recent works evaluated the causal validity of explanations (Feder et al., 2021; Elazar et al., 2021; Goyal et al., 2019), e.g. to eliminate potential confounding effects. There is further increasing interest in automatically discovering the concepts that are used by a model (Yeh et al., 2020; Ghorbani et al., 2019; Lang et al., 2021). Concept-based Models: Concept bottleneck models (CBMs) (Koh et al., 2020) extend the earlier idea (Lampert et al., 2009; Kumar et al., 2009) of first predicting the concepts, then using concepts to predict the target. CBMs bring about interpretability benefits but require training the model using concept labels for the entire training dataset, which is a key limitation. CBMs have not been analyzed in terms of model edits. Recent work reveals that end-to-end learned CBMs encode information beyond the desired concept (Mahinpei et al., 2021; Margeloiu et al., 2021). Concept Whitening (Chen et al., 2020) aims to align each concept with an individual dimension in a layer. While a subset of dimensions is aligned, this approach lacks a bottleneck since there are dimensions that are potentially not aligned with a concept (comparable to PCBM-h). Barnett et al. (2021) learns prototypes and uses them as concepts, where the distance to prototypes determines the model prediction. PCBM-h is inspired by semiparametric models on fitting residuals (H ̈ardle et al., 2004). PIE (Wang et al., 2021) has a similar approach, where they combine individual features with more complicated interaction terms for tabular data.\n\nModel Editing Model editing aims to achieve the removal or modification of information in a given neural network. Several models investigated editing factual knowledge in language models: Zhu et al. (2020) use variants of fine-tuning to achieve this objective while retaining performance on unmodified factual knowledge and Mitchell et al. (2021); De Cao et al. (2021); Hase et al. (2021) update the model by training a separate network to modify model parameters to achieve the desired edit. Similarly, Sotoudeh & Thakur (2021) proposes ”repairing” models by finding minimal parameter updates that satisfy a given specification. One thread of work focuses on intervening on the latent space of neural networks to alter the generated output towards the desired state, e.g. removal of artifacts or manipulation of object positions (Sinitsin et al., 2020; Bau et al., 2020; Santurkar et al., 2021). For instance, Bau et al. (2020) edits generative models. Santurkar et al. (2021) edits classifiers by modifying ‘rules’, such as making a model perceive the concept of a ‘snowy road‘ as the concept of a ‘road’, and they achieve this by modifying minimal updates to specific feature extractors. FIND (Lertvittayakumjorn et al., 2020) prunes individual neurons chosen by users and later fine-tunes the model. Right for the Right Concepts (Stammer et al., 2021) similarly fine-tune a neuro-symbolic model with user-provided feedback maps. Bontempelli et al. (2021) give an overview of concept-based models and a discussion on debugging strategies while concurrent work ProtoPDebug (Bontempelli et al., 2022) proposes an efficient debugging strategy.\n\n6 LIMITATIONS AND CONCLUSION\n\nIn this work, we presented Post-hoc CBMs as a way of converting any model into a CBM, retaining the original model performance without losing the interpretability benefits. We leveraged multimodal models as an interface to use concepts, without the laborious concept annotation steps. Further, in addition to the local intervention benefits of CBM, we demonstrated that PCBMs can be leveraged to perform global model interventions. Many benefits of CBMs depend on the quality of the concept library. The concept set should be expressive enough to solve the task of interest. Users should be careful about the concept dataset used to learn concepts, which can reflect various biases. While there are several such real-world tasks, it is an open question if human-constructed concept bottlenecks can solve larger-scale tasks(e.g. ImageNet level). Hence, finding concept subspaces for models in an unsupervised fashion is an active area of research that will help with the usability and expressivity of concept bottlenecks. Multimodal models provide an effective interface for concept-level reasoning, yet it is unclear what is the optimal way to have humans in the loop. Here, we presented a simple way of taking human input via concept pruning; how to incorporate richer feedback is an interesting direction for future work.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nWe would like to thank Duygu Yilmaz, Adarsh Jeewajee, Carlos Guestrin, Edward Chen, Kyle Swanson, Omer Faruk Akgun, and Roxana Daneshjou for their support and comments on the manuscript, and all members of the Zou Lab and Guestrin Lab for helpful discussions. We thank the anonymous reviewers for their suggestions to improve this manuscript. J.Z. is supported by NSF CAREER 1942926 and the Chan-Zuckerberg Biohub.\n\nETHICAL STATEMENT\n\nIn our user study, we did not collect any user information. We applied to the IRB within our institution and “IRB has determined that this research does not involve human subjects as defined in 45 CFR 46.102(f) and therefore does not require review by the IRB”.\n\nREPRODUCIBILITY STATEMENT\n\nThe code to reproduce all experiments will be released at https://github.com/mertyg/ post-hoc-cbm. All of the backbones we used to construct concept bottlenecks are released public checkpoints, and we stated all of these in the manuscript.\n\nREFERENCES\n\nAbubakar Abid, Mert Yuksekgonul, and James Zou. Meaningfully debugging model mistakes using conceptual counterfactual explanations. In International Conference on Machine Learning, pp. 66–88. PMLR, 2022.\n\nArjun Akula, Shuai Wang, and Song-Chun Zhu. Cocox: Generating conceptual and counterfactual explanations via fault-lines. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.\n\nAlina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan Chen, Yinhao Ren, Joseph Y Lo, and Cynthia Rudin. A case-based interpretable deep learning model for classification of mass lesions in digital mammography. Nature Machine Intelligence, 3(12):1061–1070, 2021.\n\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6541–6549, 2017.\n\nDavid Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba. Rewriting a deep generative model. In European Conference on Computer Vision, pp. 351–369. Springer, 2020.\n\nAndrea Bontempelli, Fausto Giunchiglia, Andrea Passerini, and Stefano Teso. Toward a unified\n\nframework for debugging gray-box models. arXiv preprint arXiv:2109.11160, 2021.\n\nAndrea Bontempelli, Stefano Teso, Fausto Giunchiglia, and Andrea Passerini. Concept-level debug-\n\nging of part-prototype networks. arXiv preprint arXiv:2205.15769, 2022.\n\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1209–1218, 2018.\n\nZhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening for interpretable image recognition.\n\nNature Machine Intelligence, 2(12):772–782, 2020.\n\nRoxana Daneshjou, Kailas Vodrahalli, Weixin Liang, Roberto A Novoa, Melissa Jenkins, Veronica Rotemberg, Justin Ko, Susan M Swetter, Elizabeth E Bailey, Olivier Gevaert, et al. Disparities in dermatology ai: Assessments using diverse clinical images. arXiv preprint arXiv:2111.08006, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nNicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. arXiv\n\npreprint arXiv:2104.08164, 2021.\n\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160–175, 2021.\n\nEleutherAI. CLASP: Contrastive Language Aminoacid Sequence Pretraining, 2021. URL https:\n\n//github.com/MicPie/clasp.\n\nAmir Feder, Nadav Oved, Uri Shalit, and Roi Reichart. Causalm: Causal model explanation through\n\ncounterfactual language models. Computational Linguistics, 47(2):333–386, 2021.\n\nRuth Fong and Andrea Vedaldi. Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8730–8738, 2018.\n\nAsma Ghandeharioun, Been Kim, Chun-Liang Li, Brendan Jou, Brian Eoff, and Rosalind W Picard. Dissect: Disentangled simultaneous explanations via concept traversals. arXiv preprint arXiv:2105.15164, 2021.\n\nAmirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based\n\nexplanations. Advances in Neural Information Processing Systems, 32, 2019.\n\nYash Goyal, Amir Feder, Uri Shalit, and Been Kim. Explaining classifiers with causal concept effect\n\n(cace). arXiv preprint arXiv:1907.07165, 2019.\n\nWolfgang H ̈ardle, Marlene M ̈uller, Stefan Sperlich, and Axel Werwatz. Nonparametric and semi-\n\nparametric models, volume 1. Springer, 2004.\n\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. arXiv preprint arXiv:2111.13654, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nJeremy Kawahara, Sara Daneshvar, Giuseppe Argenziano, and Ghassan Hamarneh. Seven-point checklist and skin lesion classification using multitask multimodal neural nets. IEEE journal of biomedical and health informatics, 23(2):538–546, 2018.\n\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pp. 2668–2677. PMLR, 2018.\n\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International Conference on Machirane Learning, pp. 5338–5348. PMLR, 2020.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nNeeraj Kumar, Alexander C Berg, Peter N Belhumeur, and Shree K Nayar. Attribute and simile classifiers for face verification. In 2009 IEEE 12th international conference on computer vision, pp. 365–372. IEEE, 2009.\n\nChristoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE conference on computer vision and pattern recognition, pp. 951–958. IEEE, 2009.\n\nOran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, William T Freeman, Phillip Isola, Amir Globerson, Michal Irani, et al. Explaining in style: Training a gan to explain a classifier in stylespace. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 693–702, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nPiyawat Lertvittayakumjorn, Lucia Specia, and Francesca Toni. Find: Human-in-the-loop debugging\n\ndeep text classifiers. arXiv preprint arXiv:2010.04987, 2020.\n\nWeixin Liang and James Zou. Metashift: A dataset of datasets for evaluating contextual distribution\n\nshifts and training conflicts. arXiv preprint arXiv:2202.06523, 2022.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014.\n\nAdriano Lucieri, Muhammad Naseer Bajwa, Stephan Alexander Braun, Muhammad Imran Malik, Andreas Dengel, and Sheraz Ahmed. On interpretability of deep learning based skin lesion classifiers using concept activation vectors. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–10. IEEE, 2020.\n\nAnita Mahinpei, Justin Clark, Isaac Lage, Finale Doshi-Velez, and Weiwei Pan. Promises and pitfalls\n\nof black-box concept learning models. arXiv preprint arXiv:2106.13314, 2021.\n\nAndrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja Jamnik, and Adrian Weller.\n\nDo concept bottleneck models learn as intended? arXiv preprint arXiv:2105.04289, 2021.\n\nThomas McGrath, Andrei Kapishnikov, Nenad Tomaˇsev, Adam Pearce, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik. Acquisition of chess knowledge in alphazero. arXiv preprint arXiv:2111.09259, 2021.\n\nSW Menzies, C Ingvar, and WH McCarthy. A sensitivity and specificity analysis of the surface\n\nmicroscopy features of invasive melanoma. Melanoma research, 6(1):55–62, 1996.\n\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model\n\nediting at scale. arXiv preprint arXiv:2110.11309, 2021.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nVeronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein, Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza, Pascale Guitera, David Gutman, et al. A patient-centric dataset of images and metadata for identifying melanomas using clinical context. Scientific data, 8(1):1–8, 2021.\n\nShibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry. Editing a classifier by rewriting its prediction rules. Advances in Neural Information Processing Systems, 34, 2021.\n\nKrishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram. Don’t judge an object by its context: learning to overcome contextual bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11070–11078, 2020.\n\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and Artem Babenko. Editable neural networks. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=HJedXaEtvS.\n\nMatthew Sotoudeh and Aditya V Thakur. Provable repair of deep neural networks. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, pp. 588–603, 2021.\n\nRobyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of\n\ngeneral knowledge. In Thirty-first AAAI conference on artificial intelligence, 2017.\n\nWolfgang Stammer, Patrick Schramowski, and Kristian Kersting. Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3619–3629, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.\n\nPhilipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, 5(1):1–9, 2018.\n\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\n\nbirds-200-2011 dataset. 2011.\n\nTong Wang, Jingyi Yang, Yunyi Li, and Boxiang Wang. Partially interpretable estimators (pie):\n\nblack-box-refined interpretable machine learning. arXiv preprint arXiv:2105.02410, 2021.\n\nChih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. Advances in Neural Information Processing Systems, 33:20554–20565, 2020.\n\nYuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive learning of medical visual representations from paired images and text. arXiv preprint arXiv:2010.00747, 2020.\n\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363, 2020.\n\nA TRAINING DETAILS\n\nIn all of our experiments ElasticNet sparsity ratio parameter was α = 0.99. Hyperparameters: We trained all our models on a single NVIDIA-Titan Xp gpu. All of the models were trained for a total number of 10 epochs. We tune the regularization strength on a subset of the training set, that is kept as a validation set. PCBMs are fitted using scikit-learn’s SGDClassifier class, with 5000 maximum steps. Hybrid parts are trained with PyTorch, where we used Adam as the optimizer with 0.01 learning rate, with 0.01 L2 regularization on the residual classifier weights, and trained for 10 epochs.\n\nDataset Details:\n\n1. Metashift: For all metashift experiments, we have 5 class classification problems. We have 50 images for each class as the training set, and 50 images for each class in the test dataset, which is different from the training images. The regularization strength for Metashift experiments is 0.002.\n\n2. CIFAR: We use the original training and test splits of CIFAR datasets. The regularization . We use linear probing to evaluate the original\n\nstrength for CIFAR10 and CIFAR100 is 2.0 KNc model\n\n3. COCO-Stuff: We use the original training and test splits of COCO dataset. We sample 500 training and 250 test images from the dataset for each class, and we upsample the images whenever there are not enough images. Recognizing each of the 20 biased classes itepsingh2020don is treated as a binary classification task where we minimize binary cross entropy loss separately for 20 classes, and compute the mean average precision metric. The regularization strength for COCO-Stuff is 0.001. We evaluate the original model performance using linear probes with CLIP.\n\n4. Ham10k: We split 80% of the HAM10k dataset and use it as the training set, and use the\n\nremaining 20% as the test data. The regularization strength for Ham10k is 2.0 KNc\n\n.\n\n5. ISIC: We use 2000 images for training (400 malignant, 1600 benign) and evaluate the model on a held-out set of 500 images (100 malignant, 400 benign). The regularization strength for ISIC is 0.001 KNc\n\n. We evaluate the original model performance using a linear probe.\n\n6. CUB: We use the training and test splits provided in Koh et al. (Koh et al., 2020). The\n\nregularization strength for CIFAR10 and CUB is 0.01 KNc\n\n.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nVisual Concepts: We used Broden Visual concept bank for CIFAR and controlled editing experiments, CUB’s training data for CUB concepts, and derm7pt dataset for dermatology concepts. For each of these, we use 50 pairs of positive and negative images, and learn a linear SVM. We use the vector normal to the decision boundary as the concept vector.\n\nMultimodal concepts: For concept learning, we leveraged the ConceptNet hierarchy. For each classification task, we searched concept net for the class name and obtained concepts that have the following relation with the query concept: hasA, isA, partOf, HasProperty, MadeOf. We share the code for obtaining natural language concepts using ConceptNet. For each of these concepts, we obtain the text embedding using CLIP and use those as the concept vector.\n\nB RESIDUAL COMPONENT INTERVENES ONLY WHEN NECESSARY\n\nDoes PCBM-h alter predictions made by the PCBM? We hypothesized that the residual component would intervene only when the concept bottleneck is not sufficient. To better understand the effect of residual predictor, we analyzed the prediction consistency between the two models for CIFAR10 and CIFAR100, where we looked at the models with labeled concepts (i.e. 170 concepts). In Figure 4 and 5, x-axis denotes the confidence of PCBM, the orange line gives the accuracy for samples with the given confidence, and the blue line gives the consistency between PCBM and PCBM-h for the same samples, i.e. whether they make the same prediction. In CIFAR10 and CIFAR100, we show that PCBM and PCBM-h consistency is high when the model confidence is high, and the model accuracy is high. Consequently, PCBM-h changes the model prediction mostly when the PCBM prediction is likely a mistake, and the confidence is low. In Figure 5, we show that all of the predictions changed by PCBM-h are to fix model mistakes. Further, in Figure 6 we show the PCBM confidence and the mean absolute deviation between the PCBM confidence and the PCBM-h confidence. We observe that PCBM-h has less effect on model confidence as the model gets more confident.\n\nFigure 4: Residual component intervenes mostly when the confidence is low. Here, we look at the consistency between PCBM and PCBM-h predictions (i.e. whether both models make the same prediction). Namely, at each confidence level for the PCBM, we report the accuracy and consistency with the PCBM-h predictions. Overall, we see that PCBM-h is most likely to change the model prediction when the model is making a mistake, and otherwise, predictions are consistent.\n\nC COMPARISON TO CBM\n\nIn most of our experiments, there is no clear way to run CBM as a benchmark, as none of the datasets have dense concept annotations - except CUB. In CUB, we run CBM as a baseline. Particularly, we try the joint training strategy from Koh et al. (Koh et al., 2020), where it was reported to\n\n14\n\n0.0-0.20.2-0.40.4-0.60.6-0.80.8-1.0PCBM Confidence0.20.40.60.81.0MetricCIFAR10PCBM-h ConsistencyAccuracy0.0-0.20.2-0.40.4-0.60.6-0.80.8-1.0PCBM Confidence0.40.60.81.0MetricCIFAR100PCBM-h ConsistencyAccuracyPublished as a conference paper at ICLR 2023\n\nFigure 5: Residual component intervenes only to fix mistakes. Here we show the number of mistakes, the number of predictions changed by PCBM-h, and the number of mistakes fixed by PCBM-h. We see that PCBM-h only changes the model predictions to fix model mistakes.\n\nFigure 6: Effect of the residual predictor on the confidence. Here we show the PCBM confidence and mean absolute deviation between the PCBM confidence and the PCBM-h confidence. We observe that PCBM-h has less effect on model confidence as the model gets more confident.\n\ngive the best results in the CUB dataset. To make the comparison equal, we use the same set of concepts in both models, and use a linear predictor layer. Further, in both cases, we use the same frozen ResNet18 backbone. We searched over a grid of learning rates from {0.01, 0.01, 0.1, 1.0} and λ ∈ {0.001, 0.01, 0.1, 1.0} where λ is the coefficient of the concept predictors in the joint training objective (see the CBM paper, Section 3), and the ElasticNet regularization strength in {0.001, 0.01, 0.1, 1.0}. In Table 5, we report the model performance. Overall, we observe some benefit from using dense concept annotations over the entire training dataset, where CBMs achieve a slightly better performance than PCBMs, and the original backbone. We note that this was at the cost of 112 concept annotations for each of the training samples.\n\nWe further analyze the behavior of CBM under different number of annotations. In Figure 7 we train the CBM with varying number of annotations. CBMs require dense annotations, e.g. 11200 annotations would mean 11200/112 = 100 images. On the x-axis, we give the number of annotations used to train the CBM. On the y-axis, we give the accuracy on the test set. We observe that CBMs require a much larger amount of annotations to achieve the same accuracy as PCBMs.\n\n15\n\n0.0-0.20.2-0.40.4-0.60.6-0.80.8-1.0PCBM Confidence0200400600800MetricCIFAR100.0-0.20.2-0.40.4-0.60.6-0.80.8-1.0PCBM Confidence050010001500200025003000MetricCIFAR100# of PCBM mistakes# of Mistakes fixed by PCBM-h# of Predictions Changed0.0-0.20.2-0.40.4-0.60.6-0.80.8-1.0PCBM Confidence0.20.40.60.8MetricCIFAR100.0-0.20.2-0.40.4-0.60.6-0.80.8-1.0PCBM Confidence0.20.40.60.8MetricCIFAR100Mean PCBM ConfidenceMean Abs. Dev. in ConfidencePublished as a conference paper at ICLR 2023\n\nTable 5: Comparison to CBM. We compare PCBM to CBM in the CUB dataset. The accuracy over the test set is reported.\n\nResNet18(Backbone)\n\nPCBM PCBM-h CBM(Backbone Fixed)\n\nCUB Accuracy\n\n0.612\n\n0.588\n\n0.610\n\n0.629\n\nFigure 7: Effect of the number of annotations in CBM. Here we train the CBM with varying number of annotations. CBMs require dense annotations, e.g. 11200 annotations would mean 11200/112 = 100 images. On the x-axis, we give the number of annotations used to train the CBM. On the y-axis, we give the accuracy on the test set. We observe that CBMs require a much larger amount of annotations to achieve the same accuracy as PCBMs.\n\nD CONTROLLED METASHIFT EXPERIMENTS FOR MODEL EDITING\n\nFor Metashift, we have 2 tasks. Both tasks are 5-class object recognition tasks, where in the first one classes are airplane, bed, car, cow, keyboard, and for the second one we have beach, computer, motorcycle, stove, table. For each of these, we use a ResNet18 pretrained on ImageNet as the backbone of the P-CBM, and then use 100 images per class to train the concept bottleneck. For all experiments, we use the Adam Optimizer with a learning rate of 0.05, the regularization parameters λ = 0.05, α = 0.99. Similar to CIFAR experiments, we use the Broden Concept dataset. Below we give the entire set of results.\n\n16\n\n1120050176188160313600# of Annotations0.20.30.40.50.6AccuracyCBMPCBMPublished as a conference paper at ICLR 2023\n\nTrain bed(dog) bed(cat) table(dog) table(cat) table(books) table(books) car(dog) car(cat) cow(dog) keyboard(dog) bed(dog) bed(cat) table(dog) table(cat) table(books) table(books) car(dog) car(cat) cow(dog) keyboard(dog)\n\nOriginal Model Test 0.760 P-CBM bed(cat) 0.680 P-CBM bed(dog) 0.520 P-CBM table(cat) 0.660 P-CBM table(dog) 0.600 P-CBM table(dog) 0.620 P-CBM table(cat) 0.718 P-CBM car(cat) 0.620 P-CBM car(dog) 0.778 P-CBM cow(cat) P-CBM 0.620 keyboard(cat) HP-CBM 0.760 bed(cat) HP-CBM 0.760 bed(dog) HP-CBM 0.600 table(cat) HP-CBM 0.540 table(dog) HP-CBM 0.660 table(dog) HP-CBM 0.760 table(cat) HP-CBM 0.795 car(cat) HP-CBM 0.640 car(dog) cow(cat) HP-CBM 0.639 keyboard(cat) HP-CBM 0.400\n\nPrune 0.760 0.700 0.540 0.700 0.580 0.680 0.718 0.760 0.750 0.580 0.760 0.740 0.620 0.580 0.700 0.800 0.769 0.660 0.639 0.460\n\nPrune+Normalize 0.760 0.720 0.620 0.740 0.780 0.800 0.744 0.840 0.778 0.720 0.780 0.760 0.640 0.640 0.760 0.820 0.795 0.740 0.639 0.560\n\nFine-Tune 0.920 0.940 0.760 0.760 0.720 0.820 0.949 0.840 0.944 0.940 0.900 0.940 0.780 0.820 0.680 0.780 0.974 0.720 0.917 0.940\n\nTable 6: Results of the Metashift editing experiments.\n\nE USER STUDY\n\nE.1 USER INTERFACE\n\nBelow, we provide screenshots of the initial launch page and the concluding summary page from the user study. For the concept-selection interface, refer to the main paper.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: Launch page for user study. Before starting the editing tasks, participants are shown a launch page containing a brief background section on PCBMs and a set of instructions.\n\nFigure 9: Summary page for user study. After completing all 9 scenarios, participants are shown a summary page that includes the user’s choice of concepts, the accuracy of the unedited model, and the accuracy achieved by the edited model.\n\nE.2 HUMAN-GUIDED PCBM EDITING: DETAILED PERFORMANCE RESULTS\n\nIn each of the 9 scenarios in the user study, the underlying classification task is a 5-class object recognition task, and one of the classes has a spurious correlation in the training set. For instance, all\n\n18\n\nPublished as a conference paper at ICLR 2023\n\ntraining images for the class keyboard also contain a cat in the image. In Table 7, we list the classes in the underlying classification task and the spurious correlation for each scenario.\n\nTable 7: Classification tasks and spurious correlations in user study scenarios.\n\nClassification Task\n\nairplane, bed, car, cow, keyboard beach, bus, airplane, keyboard, bird beach, car, airplane, bed, bird beach, motorcycle, airplane, couch, bird bus, painting, cat, computer, snowboard bus, pillow, cat, computer, snowboard bus, television, cat, computer, snowboard car, fork, table, bed, computer dog, car, airplane, couch, bird\n\nSpurious Correlation\n\nbed(dog) keyboard(cat) bed(cat) couch(cat) painting(lamp) pillow(clock) television(fireplace) fork(tomato) car(snow)\n\nBelow, we report the test accuracies for each of the 9 scenarios in our user study, both for the shifted class and for the overall classification task.\n\nTable 8: Model accuracy of PCBM-h with CLIP concepts after editing (N=30) We report the test accuracy in the shifted class and the overall test accuracy for each individual scenario in the user study. Accuracy for the pruning strategies is averaged over users and is shown as mean ± standard error.\n\nScenario\n\nUnedited Random Prune User Prune\n\nGreedy Prune\n\nFine-tune\n\nShifted Class Test Accuracy\n\nbed(dog) keyboard(cat) bed(cat) couch(cat) painting(lamp) pillow(clock) television(fireplace) fork(tomato) car(snow)\n\nbed(dog) keyboard(cat) bed(cat) couch(cat) painting(lamp) pillow(clock) television(fireplace) fork(tomato) car(snow)\n\n0.720 0.520 0.700 0.700 0.640 0.800 0.480 0.580 0.640\n\n0.864 0.764 0.820 0.840 0.852 0.888 0.816 0.700 0.784\n\n0.651 ± 0.023 0.425 ± 0.040 0.663 ± 0.016 0.725 ± 0.033 0.617 ± 0.006 0.776 ± 0.008 0.485 ± 0.004 0.583 ± 0.012 0.676 ± 0.014\n\n0.832 ± 0.008 0.788 ± 0.021 0.789 ± 0.006 0.763 ± 0.011 0.640 ± 0.006 0.815 ± 0.003 0.535 ± 0.008 0.655 ± 0.010 0.807 ± 0.013 Overall Test Accuracy\n\n0.838 ± 0.008 0.803 ± 0.015 0.813 ± 0.008 0.883 ± 0.010 0.659 ± 0.002 0.815 ± 0.003 0.564 ± 0.009 0.707 ± 0.009 0.817 ± 0.011\n\n0.850 ± 0.005 0.741 ± 0.008 0.808 ± 0.004 0.839 ± 0.005 0.841 ± 0.001 0.882 ± 0.002 0.817 ± 0.001 0.695 ± 0.002 0.791 ± 0.003\n\n0.882 ± 0.001 0.811 ± 0.004 0.834 ± 0.001 0.851 ± 0.002 0.845 ± 0.001 0.891 ± 0.001 0.827 ± 0.002 0.706 ± 0.001 0.816 ± 0.002\n\n0.882 ± 0.001 0.814 ± 0.003 0.841 ± 0.002 0.858 ± 0.002 0.854 ± 0.001 0.891 ± 0.001 0.832 ± 0.002 0.711 ± 0.001 0.817 ± 0.002\n\n0.940 0.940 0.860 0.960 0.840 0.820 0.760 0.840 0.840\n\n0.900 0.860 0.832 0.904 0.884 0.888 0.856 0.732 0.848\n\nF ANALYSIS ON THE COCO-STUFF BIASES\n\nSingh et al (Singh et al., 2020) identifies co-occurence biases in the COCO-Stuff dataset, where 20 categories frequently co-occur with other identified categories. In Table 10, reader can find the concepts, taken from Singh et al. In Table 10, we report the Top-5 concepts for the PCBM trained with CLIP concepts. In the Biased Context column, we give the category that co-occurs frequently with the given context. In PCBM Top-5 Concepts column, we give the concepts that have the highest\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Model accuracy of PCBM with CLIP concepts after editing (N=30) We report the test accuracy in the shifted class and the overall test accuracy for each individual scenario in the user study. Accuracy for the pruning strategies is averaged over users and is shown as mean ± standard error.\n\nScenario\n\nUnedited Random Prune User Prune\n\nGreedy Prune\n\nFine-tune\n\nShifted Class Test Accuracy\n\nbed(dog) keyboard(cat) bed(cat) couch(cat) painting(lamp) pillow(clock) television(fireplace) fork(tomato) car(snow)\n\nbed(dog) keyboard(cat) bed(cat) couch(cat) painting(lamp) pillow(clock) television(fireplace) fork(tomato) car(snow)\n\n0.720 0.520 0.700 0.680 0.580 0.740 0.420 0.600 0.620\n\n0.860 0.748 0.804 0.828 0.836 0.876 0.800 0.704 0.768\n\n0.647 ± 0.024 0.412 ± 0.040 0.677 ± 0.017 0.701 ± 0.034 0.579 ± 0.004 0.760 ± 0.008 0.428 ± 0.004 0.581 ± 0.012 0.651 ± 0.014\n\n0.830 ± 0.008 0.787 ± 0.021 0.813 ± 0.008 0.751 ± 0.012 0.607 ± 0.007 0.811 ± 0.005 0.459 ± 0.006 0.642 ± 0.007 0.777 ± 0.011 Overall Test Accuracy\n\n0.832 ± 0.008 0.799 ± 0.016 0.804 ± 0.007 0.882 ± 0.011 0.635 ± 0.005 0.799 ± 0.006 0.483 ± 0.007 0.685 ± 0.006 0.737 ± 0.008\n\n0.844 ± 0.005 0.727 ± 0.008 0.801 ± 0.004 0.826 ± 0.006 0.835 ± 0.001 0.879 ± 0.002 0.802 ± 0.001 0.699 ± 0.002 0.776 ± 0.003\n\n0.878 ± 0.001 0.802 ± 0.004 0.830 ± 0.002 0.841 ± 0.002 0.841 ± 0.001 0.890 ± 0.001 0.808 ± 0.001 0.710 ± 0.001 0.799 ± 0.002\n\n0.877 ± 0.001 0.804 ± 0.003 0.829 ± 0.001 0.850 ± 0.001 0.847 ± 0.001 0.888 ± 0.001 0.813 ± 0.001 0.705 ± 0.002 0.792 ± 0.002\n\n0.900 0.980 0.820 0.920 0.520 0.720 0.880 0.960 0.720\n\n0.880 0.800 0.668 0.892 0.840 0.848 0.812 0.720 0.732\n\nBiased Context\n\ndining table person fruit road road\n\nBiased Category cup handbag apple car bus potted plant spoon bowl microwave oven keyboard mouse clock hair drier skateboard person\n\nvase\n\nbuilding-other towel\n\nPCBM Top-5 Concepts\n\ntable, coaster brake, crockery, dining table, column bag, platform, woman, person, toiletry apple tree, fruit, edible fruit, citrus fruit, produce traffic circle, intersection, car window, car mirror, trunk public transport, transportation, tube, carriages, traffic circle\n\ntall plant, patio, vase, plant, plant organ\n\neating utensil, utensil, cutlery, crockery, flatware kitchen, washing machine, kitchen appliance, oven, countertop computer, computer system, portable computer, computer brand, mouse clock face, timepiece, the dial, plank, chimney shower, bathroom, toiletry, plumbing fixture, toilet paved surface, person, outsole, instep, footwear\n\nTable 10: COCO-Stuff Concepts\n\nweight for the given category. Looking at PCBM categories, we can see that we can identify the biased contexts. For instance, cup category has table, dining table, apple has fruit, fresh vegetables, car has traffic circle, intersection in the concepts identified as important, which are parallel to the biased context. In other cases, our model surfaced different potential co-occurence biases, such as chimney for clock, and toilet for hair drier. One important limitation for this analysis is that the desired concepts should exist in the concept bank. To further improve the pipeline, automatic concept discovery approaches shall be a fruitful research direction (Ghorbani et al., 2019; Yeh et al., 2020).\n\n20",
    "reference": "# Summary Of The Paper\n\nThe authors present Post-hoc Concept Bottleneck Models (PCBMs) as a method for converting any model into a CBM with its interpretability benefits while retaining the original model performance ( by introducing a residual layer that circumvents the bottleneck and helps with the concept set isn’t rich enough ).   When there isn’t concept supervision for a task, they use CLIP and ConceptNet as a method to learn concepts based on task classes.  They also show that PCBMs can be leveraged to perform global model interventions, specifically to reduce spurious correlations in a contrived dataset.  Through a model-editing user study, they show that editing PCBMs via concept-level feedback can provide significant performance gains without using any data from a target domain or model retraining for tasks in new data distributions.\n\n# Strength And Weaknesses\n\n**Strength:**  \nThe authors demonstrate the utility of their method for constructing CBMs in a post-hoc fashion using Concept net & CLIP if need be along with their model variant which uses a residual layer PCBM-h to get near same accuracy as the original model  ( at the expense of interpretability, but still a novel and simple addition to the architecture that is quite useful.\n\n**Weaknesses:**  \nThe paper had a few areas that could have been clearer (1) how to select negatives when given concept supervision and (2) it took me until after reading section 3 to understand when Visual or Multimodal concepts are used ( maybe that could be added to Figure 1?) You all mention it in your limitations section, but the use of Concept Net will not translate well to domain specific tasks or tasks with classes not in ConceptNet.\n\nFinally on Page 6 before Figure 2, you mention PCBM-h and PCBM provide the same concept interpretations, but this is not exactly the same interpretation since the final prediction in PCBM-h is no longer a linear combination of concept weights, but rather concept weights and residual weights so  although the concept bottleneck learns the  same weights, the overall model behavior is different and the concepts influence in each model differ.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work is clear and novel, and if the code is provided seems like it should be reproducible ( though i couldn’t check repo as it was redacted and not included as supplemental )\n\n# Summary Of The Review\n\nThe authors demonstrate the utility of their method for constructing CBMs in a post-hoc fashion using Concept net & CLIP if need be along with their model variant which uses a residual layer PCBM-h to get near same accuracy as the original model  ( at the expense of intepretability, but still a novel and simple addition to the architecture that is quite useful.\n\nThe paper mostly clearly written, though it took me a bit to understand certain parts such as when ConceptNet and Clip were used ( ie, when labeled concept examples don’t exist ).  The experiments well done and model editing was explained\nwell for the use case of datashift ( which is relatively simplified ), but still has utility.  The human user study was well done and very convincing.  It’d be interesting to explain the differences between the fine-tune oracle and user pruned concept weights in that section to see if fine tuning is actually picking up spurious correlations which are useful for task accuracy or as a way to suggest concepts for a human to consider?\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nRELATIVE REPRESENTATIONS ENABLE ZERO-SHOT LATENT SPACE COMMUNICATION\n\nLuca Moschella1,∗ Valentino Maiorca1,∗\n\nMarco Fumero1 Antonio Norelli1\n\nFrancesco Locatello2,† Emanuele Rodol`a1\n\n1Sapienza University of Rome\n\n2Amazon Web Services\n\nABSTRACT\n\nNeural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, the angles between the encodings within distinct latent spaces do not change. In this work, we propose the latent similarity between each sample and a fixed set of anchors as an alternative data representation, demonstrating that it can enforce the desired invariances without any additional training. We show how neural architectures can leverage these relative representations to guarantee, in practice, invariance to latent isometries and rescalings, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings. We extensively validate the generalization capability of our approach on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).\n\n1\n\nINTRODUCTION\n\nNeural Networks (NN) learn to transform high dimensional data into meaningful representations that are helpful for solving downstream tasks. Typically, these representations are seen as elements of a vector space, denoted as latent space, which corresponds to the constrained output (explicitly or implicitly) of a key component of the NN, e.g., the bottleneck in an Autoencoder (AE), or the word embedding space in an NLP task. The underlying assumption is that the learned latent spaces should be an optimal encoding given the data distribution, the downstream task, and the network constraints.\n\nIn practice, however, the learned latent spaces are subject to changes even when the above assumptions remain fixed. We illustrate this phenomenon in Figure 1, where we show the latent spaces produced by an AE with a two-dimensional bottleneck, trained on the MNIST dataset several times from scratch. These spaces differ from one another, breaking the fundamental assumptions made above. The distribution of the latent embeddings is affected by several factors, such as the random initialization of the network weights, the data shuffling, hyperparameters, and other stochastic processes in the training phase. Although different, the learned representations in Figure 1 are intrinsically similar: the distances between the embedded representations are approximately the same across all spaces, even if their absolute coordinates differ. Indeed, the learned latent spaces are the same up to a nearly isometric transformation.1\n\nThis symmetry is a consequence of the implicit biases underlying the optimization process (Soudry et al., 2018) forcing the model to generalize and, therefore, to give similar representations to similar samples with respect to the task. There exist infinitely many spatial arrangements complying with these similarity constraints, each associated with a different isometry in the example of Figure 1.\n\n∗Equal contribution. 1To the best of our knowledge, the first to acknowledge this behavior was Olah (2015) in a blogpost.\n\n† Work done outside of Amazon.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nTrain 1\n\nTrain 2\n\nTrain 3\n\nTrain 4\n\nFigure 1: Latent spaces learned by distinct trainings of the same AE on the MNIST dataset. The bottleneck has size 2, thus there is no dimensionality reduction involved in the visualization of the latent space. The stochasticity in the training phase induces intrinsically similar representations. As we show in Figure 5, this property holds even for high-dimensional latent spaces.\n\nBut while the resulting models will be equally good in terms of the task, one still encounters several practical problems. For example, it is notoriously challenging to compare latent spaces across different trainings or across different NNs; perhaps more importantly, re-using neural components trained on different embeddings of the same data becomes impossible, since they are incompatible. To overcome this, we propose adopting a local coordinate system defined by the data itself. Each data point becomes a set of coefficients that encode the point as a function of other data samples, instead of an independent point in Rd. The proposed relative representation directly encodes the intrinsic information underlying the data, and only depends on the angles between embeddings by construction. Remarkably, this enables a form of compositionality between learning models; it allows, for instance, to stitch together an encoder trained on ImageNet with a decoder trained on CIFAR, as we showcase in our experiments.\n\nOur main contributions can be summarized as follows:\n\n• We show that the representations learned by NNs are subject to change due to several\n\ntraining factors; nonetheless, the angles between latent embeddings are preserved.\n\n• We introduce a novel relative representation for latent embeddings, that is invariant by construction to the transformations induced by stochastic factors in the training process. • For the first time, we successfully demonstrate zero-shot stitching of neural components produced by distinct training regimens, e.g., due to different seeds or different neural architectures; we validate our findings on different data modalities (e.g. images, text).\n\n• Our framework also provides a quantitative measure of performance while training neural models, which is differentiable, does not need any labeled data, and is correlated with standard performance measures such as accuracy.\n\n2 RELATED WORK\n\nRepresentation similarity. Recently, there has been growing agreement that good networks learn similar representations across a variety of architectures, tasks and domains (Morcos et al., 2018; Li et al., 2016; Kornblith et al., 2019; Bonheme & Grzes, 2022; Tsitsulin et al., 2020; Barannikov et al., 2022; Vuli ́c et al., 2020; Lample et al., 2018; Lenc & Vedaldi, 2015; Mikolov et al., 2013b; Antonello et al., 2021), although this is still debated (Wang et al., 2018) and missing strong theoretical justifications. Similar observations have been made in the context of biological models Laakso & Cottrell (2000); Kriegeskorte et al. (2008); Raizada & Connolly (2012). Supported by the empirical evidence widely reported in these works, our method assumes that well-performing neural networks trained on similar tasks and data produce similar latent spaces, which allows us to define a representation that unifies all these spaces.\n\nModel stitching. Lenc & Vedaldi (2015) introduced trainable stitching layers that allow swapping parts of different networks, while Bansal et al. (2021); Csisz ́arik et al. (2021) employed stitching to quantitatively verify statements such as “good networks learn similar representations” and “more data, width or time is better”. Other works, such as Gygli et al. (2021); Biondi et al. (2021); Yaman et al. (2022); Bianchi et al. (2020), tried to directly produce compatible and reusable network\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ncomponents without stitching layers; more generally, stitching has been adopted in the literature to analyze neural networks. In our work, we sidestep the need for trainable stitching layers and propose zero-shot model stitching to effectively reuse models.\n\nRelative information. The attention mechanism (Vaswani et al., 2017) and its variants (Kossen et al., 2021) exploit the relationship between features to extract meaningful representations. Prototypical Networks (Snell et al., 2017) learn a metric space where the classification can be performed by measuring the distances to prototype representations. Shalam & Korman (2022) proposed the Self Optimal Transport feature transform to enrich the sample representations with higher order relations between the instance features, while Alvarez-Melis et al. (2019) proposed a general formulation of the optimal transport that accounts for global invariances in the underlying feature spaces. Mathematically, our method bears resemblance to a kernel method (Hofmann et al., 2008) as it employs inner products of embedded features as a core ingredient. However, differently from kernel methods, we do not introduce learnable parameters and, crucially, we compute the representations explicitly without resorting to a kernel trick.\n\n3 METHOD\n\nGiven a training set X, standard NNs learn an embedding function Eθ : X → Rd, parametrized by θ, which maps each sample x(i) ∈ X to its latent representation, or absolute representation, ex(i) = Eθ(x(i)). This representation is then exploited to solve downstream tasks, such as classification, reconstruction or generation, optimizing over some objective function of the general form:\n\nEx∈X[L(Eθ(x)) + Reg(θ)] .\n\nmin θ\n\n(1)\n\nHere, EX denotes the expectation over the training distribution, and Reg(θ) encodes additional constraints on the weights θ. As previously discussed, we argue that the learned weights θ∗ are not only a function of X and of the specific loss appearing in Equation 1, but in practice they are also affected by the optimization process used to train the network due to weight initialization, data shuffling, hyperparameters, and other stochastic factors. We denote these factors collectively by φ. In particular, as shown in Figure 1, changing these factors induces a transformation T over the latent space, i.e., φ → φ′ implies Eθ(x(i)) → T Eθ(x(i)). We make the core assumption that T preserves the angles between elements of the latent space, namely ∠(ex(i) , ex(j)) = ∠(T ex(i), T ex(j)) for every (x(i), x(j)) ∈ X. While this assumption might seem too restrictive, in practice it arises in several real scenarios as we show in the following sections.\n\n3.1 RELATIVE REPRESENTATIONS\n\nTo build our representation, we start by selecting a subset A of the training data X, which we denote as anchor samples. Every sample in the training distribution will be represented with respect to the embedded anchors ea(j) = E(a(j)) with a(j) ∈ A. As a measure capturing the relation between the anchors and the other samples, we consider a generic similarity function sim : Rd ×Rd → R, yielding a scalar score r between two absolute representations r = sim(ex(i) , ex(j) ). Given the anchors A in an arbitrary ordering a(1), . . . , a(|A|), we define the relative representation of x(i) ∈ X as:\n\nrx(i) = (sim(ex(i) , ea(1)), sim(ex(i) , ea(2)), . . . , sim(ex(i) , ea(|A|) )) . Figure 2 illustrates the key differences between absolute and relative representations.\n\n(2)\n\nChoice of the anchors. Anchors directly affect the expressivity of the relative representation space, and are related to the task at hand. For example, in a classification task, we should sample anchors from each class in the training set, in order to well represent each data sample in X. One case of interest arises when the data comes from different domains or modalities X, Y, and we are given a partial correspondence Γ : PX (cid:55)→ PY mapping from a subset of X to a subset of Y. In this case, we can sample anchors AX ⊆ PX and obtain corresponding anchors on the other domain directly as Γ(A) (Norelli et al., 2022). We refer to these as parallel anchors. We show an example of parallel anchors in Section 5.2, where X and Y are Amazon reviews in two different languages.\n\nThe choice of the anchors is not restricted to elements in the training distribution. Given an encoder pre-trained on a fixed training distribution, we can pick elements from a set ̃A that is out-of-domain\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n⇒\n\n⇒\n\nFigure 2: Left: Three anchors (colored stars) are selected on the data manifold; given a point on the three anchors, yielding a vector of the manifold (blue dot), we compute its similarity w.r.t. dimensionality 3 (middle). Right: Each dimension is treated as coefficients in a coordinate system defined by the anchors. Anchors are orthogonal in this example only for visualization purposes.\n\nw.r.t. X, and build the relative representations on top of ̃A. We refer to these as OOD anchors and exploit them, e.g., to solve domain adaptation tasks where we do not have access to a correspondence, and have scarce data labels. We refer again to the Sections 5.2 and 5.3 for real-world examples and to Appendix A.2 for a preliminary analysis of different selection strategies.\n\nAchieving latent invariance. In this work, we choose the cosine similarity as the similarity function due to the properties it induces on the relative representation. The cosine similarity SC is the dot product of unit vectors, corresponding to the cosine of the angle θ between the two:\n\nSC(a, b) =\n\nab ||a||||b||\n\n= cos θ .\n\n(3)\n\nImportantly, cos θ does not change if we apply the same angle-preserving transformation T to two vectors a and b, i.e., the cosine similarity is invariant to rotations, reflections, and rescaling. While this is not true for translations, NNs commonly employ normalization techniques (e.g., InstanceNorm (Ulyanov et al., 2016)) to center the latent spaces. Under this assumption, cosine similarity guarantees a relative representation rx(i) invariant to angle-preserving transformations. This means we have the freedom to change the embedding function Eθ with any other function ̃E that produces different representations with same angles, i.e.:\n\n[SC(ex(i), ea(1)), . . . , SC(ex(i) , ea(|A|) ] = [SC( ̃ex(i) , ̃ea(1)), . . . , SC( ̃ex(i), ̃ea(|A|))] , where ̃ex(i) = ̃E(x(i)) = T E(x(i)) and T is an arbitrary angle-preserving transformation. A practical application of this invariance is the possibility of comparing latent spaces across multiple trainings, and re-using models as demonstrated in Sections 4 and 5.\n\n(4)\n\nWe remark that other choices of similarity function can be made to enforce different invariances into the representation. For example, one may impose invariance to non-isometric deformations with bounded distortion. We did not find this to be necessary in our experiments, as typically NNs that generalize sufficiently well can handle small perturbations. Nevertheless, this invariance can be enforced by design with vector quantization algorithms. Figure 7 shows a preliminary investigation, leaving further exploration to future work.\n\n4 LATENT SPACE COMMUNICATION\n\nIn this section, we demonstrate how our relative representations can effectively be used to produce latent spaces that are stable under a variety of factors. Our main question is the following: Given two different learning models that are trained separately on different data, can we compare their latent embeddings? In asking this, we assume that the two models are trained on a similar phenomenon, e.g., on two different samplings of the English language or on two different modalities.\n\nWe answer in the positive, showing the gained invariance enables effective communication between different, but semantically equivalent latent spaces. In particular, we analyze how different word embedding spaces, once projected onto relative representations, are intrinsically the same (Section 4.1); we then show how the similarity between the relative counterparts of two or more embedding spaces is a surprisingly good predictor of model performance (Section 4.2); finally, we confirm that relative representations in the training phase are not detrimental to performance (Section 4.3).\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n4.1 WORD EMBEDDINGS\n\nExperimental setting. We select two different word embeddings on the English language, namely FastText (Bojanowski et al., 2017) and Word2Vec (Mikolov et al., 2013a). Both models are pre-trained on different data, but partly share a vocabulary from which we extract ≈ 20K words. Using 300 randomly drawn parallel anchor, we convert each embedding space to a relative one. In Table 1 (left), we show the original and the relative embeddings. For each word w, we consider its corresponding encodings x and y in the source and target space. We apply three different metrics to measure their similarity (in a setting similar to Vuli ́c et al. (2020)): (i) Jaccard: the discrete Jaccard similarity between the set of word neighbors of x in source and target; (ii) Mean Reciprocal Rank: measures the (reciprocal) ranking of w among the top-k neighbors of x in the target space; (iii) Cosine: measures the cosine similarity between x and y. Additional details in Appendix A.5.1\n\nFastText Word2Vec\n\nSource\n\nTarget\n\nJaccard ↑\n\nMRR ↑\n\nCosine ↑\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\ne FT\n\nt\n\nu\n\nl\n\no s\nb A\n\nW2V\n\ne FT\n\nv\n\ni t\na l\ne R\n\nW2V\n\nFT W2V FT W2V\n\nFT W2V FT W2V\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.34 ± 0.01 0.39 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.94 ± 0.00 0.98 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.01 ± 0.00 0.01 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.86 ± 0.00 0.86 ± 0.00 1.00 ± 0.00\n\nTable 1: Qualitative (left) and quantitative (right) comparisons of English word embeddings using absolute and relative representations. PCA is applied only for visualization. All metrics are calculated with K = 10 averaged over 20k words and across 10 different random seeds. See Figure 10 for other dimensionality reductions and Table 8 and fig. 11 for the same experiment on CIFAR-10.\n\nResult analysis. Table 1 (left) highlights clusters of semantically similar words and shows that the absolute representations are incoherent across the two latent spaces, while the relative embeddings are highly similar. The average Jaccard distance reported in Table 1 (right), says that the word neighborhoods of the relative representations are matched exactly 34% of the time in one direction, and 39% of the time in the other one (the missing 61% is due to semantic differences, that are not taken into account by the discrete nature of the Jaccard metric). By contrast, the absolute embeddings are never matched exactly (Jaccard score equal to zero); for a match to happen, it would mean that the FastText and Word2Vec embeddings of a given English word are almost the same, which is highly unlikely. MRR, close to a perfect score for the relative representations, shows that the most-similar word to a given one is usually itself, even if their cosine similarity doesn’t reach 1.\n\nOverall, these results show that relative representations are preserved across different word embedding models, validating our assumptions.\n\n4.2 LATENT DISTANCE AS A PERFORMANCE PROXY\n\nExperimental setting. In this experiment, we consider a node classification task on the Cora graph dataset (Sen et al., 2008). We first train a reference model that achieves good accuracy on a validation set. Then, we train ≈ 2000 models with various combinations of seed, number of epochs, number of layers, dropout probability, activation functions, optimizer type, learning rate or type of graph embedder (Table 10). All the models are classically trained using absolute representations, which are converted to relative post-training by projecting the embeddings onto 300 randomly drawn but fixed anchors. For each model, we measure its classification accuracy and compute the similarity of its space with the reference one. This similarity is computed as the average cosine similarity between the node embeddings produced by a given model and the corresponding embeddings in the reference.\n\nResult analysis. The scatter plot in Figure 3 (left) shows that better-performing models tend to be the ones with the latent spaces most similar to the reference. The performance-similarity correlation\n\n5\n\nPublished as a conference paper at ICLR 2023\n\ne c\nn a\n\nm\n\nr o\nf r\ne P\n\ne c\nn a\n\nm\n\nr o\nf r\ne P\n\ny\n\nt i\nr a\nl i\n\nm S\n\ni\n\nSimilarity\n\nEpochs\n\nFigure 3: Graph node classification task on Cora. Left: Correlation between the performance of ≈ 2000 models and the similarity of their latent spaces with respect to a well-performing reference model. Right: The same correlation plotted over time. The mean Pearson correlation over all models is 0.955, after filtering out the models having best validation accuracy below 0.5.\n\nalso holds over time, as shown in Figure 3 (right). Additional correlation examples are in Figure 9. Interestingly, this metric is differentiable, enabling an explicit supervision signal on the latent space, which does not require labeled data and could be readily exploited in a teacher-student framework.\n\nOverall, these results suggest that the similarity between the relative representations of latent spaces is a remarkably good proxy to evaluate model performance.\n\n4.3 TRAINING WITH ABSOLUTE VS. RELATIVE REPRESENTATIONS\n\nExperimental setting. Finally, we compare architectures that do or do not employ the relative representation while training. In these experiments, the models vary slightly according to the dataset; however, the relative and absolute versions are always comparable in terms of architecture, number of learnable parameters and hyperparameters. We refer to the supplementary material and the open-source code for further details on their implementation. In this section we consider classification tasks on several datasets, spanning the image domain (Lecun et al., 1998; Xiao et al., 2017; Krizhevsky, 2009) and the graph domain (Yang et al., 2016).\n\nTable 2: Performance comparison between relative and absolute representations on several image and graph datasets. The metric is the classification weighted F1 score (± std), over 6 seeds.\n\nImage Classification\n\nGraph Node Classification\n\nMNIST\n\nF-MNIST\n\nCIFAR-10\n\nCIFAR-100\n\nCora\n\nCiteSeer\n\nPubMed\n\nRelative Absolute\n\n97.91 ± 0.07 97.95 ± 0.10\n\n90.19 ± 0.27 90.32 ± 0.21\n\n87.70 ± 0.09 87.85 ± 0.06\n\n66.72 ± 0.35 68.88 ± 0.14\n\n0.89 ± 0.02 0.90 ± 0.01\n\n0.77 ± 0.03 0.78 ± 0.03\n\n0.91 ± 0.01 0.91 ± 0.01\n\nResult analysis. The results, reported in Table 2, show that relative representations, when used at training time, are not detrimental to performance in general. This is further shown in Tables 3 to 6 and 15 to 18, where a subset of the results compares the absolute and relative representations on a variety of domains, datasets and tasks.\n\nOverall, these results show that relative representations are effective when involved in end-to-end training, without significant performance drops.\n\n5 ZERO-SHOT MODEL STITCHING\n\nIn this section, we illustrate how the latent space communication demonstrated in Section 4 enables zero-shot interoperability of pre-trained neural components. In previous works, such as Lenc & Vedaldi (2015); Bansal et al. (2021), stitching layers are trainable linear projections that allow swapping parts of different networks. Instead, on relative representations unlocks the possibility of zero-shot stitching different neural components, treating them as frozen black-box modules.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nWe define a generic stitched model as the composition of an encoder, that embeds data, plus a relative decoder specialized in a downstream task (classification, reconstruction). The stitching operation is always performed without training or fine-tuning, in a zero-shot fashion. Hereafter, we showcase stitching capabilities across combinations of different stochasticity sources (Figure 4 and table 3), neural architectures (Tables 4 and 5) or datasets (Table 6). Finally, we present strong real-world applications in NLP (Section 5.2) and CV (Section 5.3), e.g. zero-shot predictions on novel languages. Additional implementation details are given in the supplementary materials.\n\nS\n\n. s\nb A\n\n. l\ne R\n\n. s\nb A\n\n. l\ne R\n\nE A\n\nE A\nV\n\nFigure 4: Reconstruction examples. Each column is a different image, row pairs are different architectures. In each pair, we first report the non-stitched reconstructions, then the stitched ones.\n\n5.1\n\nIMAGE RECONSTRUCTION\n\nExperimental setting. We perform zero-shot stitching with AEs and VAEs trained with relative representations end-to-end on several datasets. For each combination of model and dataset, we perform 5 trainings with different seeds, and zero-shot stitch together the resulting encoders and decoders.\n\nResult analysis. In Figure 4 the stitched models that employ absolute representations (Abs.) produce erroneous predictions, since the latent spaces obtained from distinct trainings are incompatible. Interestingly, although the absolute VAE does not produce compatible latent spaces, it is regularized, thus all embeddings produced by the encoders correspond to wrong but semantically meaningful reconstructions. Relative representations (Rel.) exhibit almost indistinguishable reconstructions between the models trained end-to-end and the stitched ones. Quantitative results are in Table 3.\n\nThese results support our claim that relative representations are empirically invariant to training stochasticity.\n\nTable 3: Stitching performance. The MSE (± std) between the ground truth X and the reconstructions is computed over 5 different seeds. Stitching with our relative representations yields an error up to two orders of magnitude less than the absolute counterpart.\n\nE A A\n\n. Non-Stitch.\n\ns b\n\nStitch.\n\n. Non-Stitch. l\ne R\n\nStitch.\n\ns b\nE A A\nV\n\n. Non-Stitch.\n\nStitch.\n\n. Non-Stitch. l\ne R\n\nStitch.\n\nMNIST\n\nF-MNIST\n\nCIFAR-10\n\nCIFAR-100\n\nMSE ↓\n\n0.66 ± 0.02 97.79 ± 2.48\n\n1.57 ± 0.03 120.54 ± 6.81\n\n1.94 ± 0.08 86.74 ± 4.37\n\n2.13 ± 0.08 97.17 ± 3.50\n\n1.58 ± 0.05 100.56 ± 4.29\n\n1.18 ± 0.02 2.83 ± 0.20\n\n3.59 ± 0.04 6.37 ± 0.29\n\n2.83 ± 0.13 5.39 ± 1.18\n\n3.50 ± 0.08 18.03 ± 12.46\n\n2.78 ± 0.07 8.16 ± 3.53\n\n1.31 ± 0.04 98.51 ± 1.49\n\n2.97 ± 0.14 13.43 ± 6.79\n\n4.38 ± 0.03 118.96 ± 2.96\n\n6.81 ± 0.06 24.03 ± 13.15\n\n2.68 ± 0.06 69.02 ± 1.54\n\n5.18 ± 0.22 11.20 ± 3.15\n\n3.00 ± 0.03 78.57 ± 1.88\n\n5.93 ± 0.14 11.23 ± 2.38\n\n2.84 ± 0.04 91.27 ± 1.97\n\n5.22 ± 0.14 14.97 ± 6.37\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n5.2 TEXT CLASSIFICATION\n\nIn this Section, we show practical examples of the use of parallel anchors (Sec 3.1).\n\nTable 4: Cross-lingual stitching performance comparison. The table reports the mean weighted F1 (± std) and MAE on Amazon Reviews coarse-grained, across 5 seeds.\n\nAbsolute\n\nRelative\n\nTranslated\n\nWikipedia\n\nDecoder\n\nEncoder\n\nFScore\n\nMAE\n\nFScore\n\nMAE\n\nFScore\n\nMAE\n\nen\n\nen es fr ja\n\n91.54 ± 0.58 43.67 ± 1.09 54.41 ± 1.61 48.72 ± 0.90\n\n0.08 ± 0.01 0.56 ± 0.01 0.45 ± 0.02 0.51 ± 0.01\n\n90.06 ± 0.60 82.78 ± 0.81 78.49 ± 0.66 65.72 ± 0.55\n\n0.10 ± 0.01 0.17 ± 0.01 0.21 ± 0.01 0.34 ± 0.01\n\n90.45 ± 0.52 78.53 ± 0.30 70.41 ± 0.57 66.31 ± 0.80\n\n0.10 ± 0.01 0.21 ± 0.00 0.29 ± 0.01 0.34 ± 0.01\n\nTable 5: Cross-architecture stitching performance comparison. The table reports the mean weighted F1 (± std) for each dataset, across 5 different seeds.\n\nTREC\n\nDBpedia\n\nAmazon Reviews\n\nCoarse\n\nFine\n\n. Non-Stitch s\nb Stitch A\n\n91.70 ± 1.39 21.49 ± 3.64\n\n98.62 ± 0.58 6.96 ± 1.46\n\n87.81 ± 1.58 49.58 ± 2.95\n\n55.35 ± 3.19 19.01 ± 2.04\n\n. Non-Stitch l\ne Stitch R\n\n88.08 ± 1.37 75.89 ± 5.38\n\n97.42 ± 2.05 80.47 ± 21.14\n\n85.08 ± 1.93 72.37 ± 7.32\n\n48.92 ± 3.57 33.24 ± 7.21\n\nExperimental setting. We consider two different text classification settings.\n\nCross-lingual: given a review predict the associated star rating, done on multi-lingual data from the Amazon Reviews dataset (Keung et al., 2020). Following the original paper, we work on a binarized version of the task, with FScore and MAE as metrics. In the supplementary material, we report results on the fine-grained formulation. We adopt four different pre-trained languagespecific RoBERTa transformers (Liu et al., 2019) and evaluate their zero-shot stitching performance on languages never seen by the classifier. We use parallel anchors in two modalities: i) Translated: consider English reviews translated2 into the other languages; ii) Wikipedia: adopt an external corpus, WikiMatrix (Schwenk et al., 2021), providing parallel sentences extracted from Wikipedia.\n\nCross-architecture: assessed on three different datasets: TREC (coarse) (Hovy et al., 2001), DBpedia (Zhang et al., 2015), Amazon Reviews (English split). We adopt two different pretrained BERT (Devlin et al., 2019) transformers (cased and uncased version), ELECTRA (Clark et al., 2020) and RoBERTa.\n\nResult analysis. Tables 4 and 5 show for the first time that it is possible to learn to solve a downstream task on a specific language or transformer and perform predictions on another.\n\nStitching with absolute representations yields performances comparable to random guessing across the board, proving that relative representations are a key element for the success of this kind of zero-shot stitching. Moreover, Table 4 highlights the robustness that relative representations have on the choice of anchors, even when they are noisy (Translated case), or their distribution differs from the one of the downstream task (Wikipedia case), as long as their encoding can be handled correctly by the encoder. In our case, the encoder is pre-trained to represent a variety of texts in a specific language, thus, even if WikiMatrix has a completely different domain from Amazon Reviews, the transformer still computes a meaningful and comparable representation with those of the reviews. We report in Tables 15 and 16 complete results on all languages combination, and in Table 17 the performance obtained by a multi-lingual transformer. To the best of our knowledge, it is the only alternative for obtainining compatible representations across languages.\n\nAccording to these results, relative representations show invariance to different architectures and data distribution shifts (e.g., different train languages).\n\n2We used the =GOOGLETRANSLATE function available in Google Sheets.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n5.3\n\nIMAGE CLASSIFICATION\n\nIn this Section, we show practical examples of the use of OOD anchors (Sec 3.1).\n\nTable 6: Stitching performance comparison with different encoding techniques. The table reports the mean weighted F1 (± std) on CIFAR-100 coarse-grained and ImageNet1k, across 5 seeds.\n\nCIFAR-100\n\nImageNet1k\n\nDecoder\n\nEncoder\n\nAbsolute\n\nRelative\n\nAbsolute\n\nRelative\n\nrexnet-100\n\nvit-base-patch16-224\n\nvit-base-resnet50-384\n\nvit-small-patch16-224\n\nrexnet-100 vit-base-patch16-224 vit-base-resnet50-384 vit-small-patch16-224\n\nrexnet-100 vit-base-patch16-224 vit-base-resnet50-384 vit-small-patch16-224\n\nrexnet-100 vit-base-patch16-224 vit-base-resnet50-384 vit-small-patch16-224\n\nrexnet-100 vit-base-patch16-224 vit-base-resnet50-384 vit-small-patch16-224\n\n82.06 ± 0.15 -\n- -\n\n- 93.15 ± 0.05 6.21 ± 0.33 -\n\n- 4.69 ± 0.07 91.41 ± 0.09 -\n\n- -\n- 90.07 ± 0.19\n\n80.22 ± 0.28 54.98 ± 0.44 53.33 ± 0.37 59.82 ± 0.32\n\n76.81 ± 0.49 91.94 ± 0.10 81.42 ± 0.38 84.29 ± 0.86\n\n79.79 ± 0.43 84.46 ± 0.19 90.77 ± 0.16 84.66 ± 0.16\n\n75.35 ± 0.41 81.23 ± 0.31 78.35 ± 0.69 88.85 ± 0.44\n\n73.78 ± 0.29 -\n- -\n\n- 80.91 ± 0.29 0.07 ± 0.05 -\n\n- 0.08 ± 0.04 82.55 ± 0.30 -\n\n- -\n- 77.73 ± 0.41\n\n72.61 ± 0.16 37.39 ± 0.36 42.36 ± 0.36 43.75 ± 0.27\n\n30.78 ± 0.81 78.86 ± 0.33 44.72 ± 0.57 48.31 ± 0.72\n\n53.46 ± 0.68 62.21 ± 0.54 81.88 ± 0.16 61.32 ± 0.36\n\n37.58 ± 0.44 50.08 ± 0.63 45.45 ± 1.41 76.36 ± 0.40\n\nExperimental setting. We consider a classification task on ImageNet1k and CIFAR-100 with coarse labels (20), and 4 different pre-trained image encoders: three variants of the ViT transformer (Dosovitskiy et al., 2021) and RexNet (Han et al., 2020).\n\nResult analysis. The results in Table 6 highlight how the relative representations allow stitching modules with different encoding dimensionality, since the decoder receives a relative representation with guaranteed equal size. Further, the results demonstrate the ability to generalize and perform zero-shot stitching on CIFAR-100, although that data was never seen by the encoder since it is a frozen transformer trained on ImageNet1k. Interestingly, rexnet-100 is the only transformer whose latent dimensionality is higher than the number of anchors, and the biggest drop in stitching performance happens when the decoder is trained on it. This suggests the number of anchors is an important hyperparameter; we refer to Figure 6 for a deeper analysis.\n\nOverall, these results prove that relative representations can bridge general-purpose encoders and pre-trained task-specific decoders.\n\n6 CONCLUSION\n\nIn this work, we introduced the concept of relative representations to enable zero-shot latent space communication, with several practical consequences as showcased in our discussion and experiments. Our work proves that a latent semantic correspondence between data domains, when present, can be exploited through a simple representation shift, without resorting to sophisticated processing or heavy training.\n\nLimitations and future work. Our work is open to several follow-up directions. While in this paper we considered the cosine similarity, different functions can enforce additional invariances in the relative representation. The study of invariant latent spaces as a general direction has the potential to lead to further impact; in Figure 7 we showed preliminary results of this possibility. Another interesting line of research to improve the representation expressivity would be to estimate geodesic distances over the data manifold instead of adopting Euclidean approximations. Similarly, we believe that the connections between the composition of the anchors set A and the expressivity of relative representations demands additional research. For example, the training cost is directly affected by the number and update frequency of the anchors. Finally, the stitching procedure may be extended to multiple layers, promoting reusable network components.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe authors gratefully acknowledge the anonymous reviewers for the thoughtful remarks, and Luigi Gresele for the insightful discussions. This work is supported by the ERC Starting Grant No. 802554 (SPECGEO).\n\nREPRODUCIBILITY STATEMENT\n\nWe describe in detail the relative representation computation in Section 3.1. We describe the experimental settings for the various scenarios, and refer to the supplementary material for further implementation details (Appendix A.5). Moreover, we release a well-documented and modular codebase, with the relative representation layer being implemented as a stand-alone PyTorch module. All the checkpoints used in the experiments are versioned with DVC (Kuprieiev et al., 2023) to easily reproduce all the figures and tables. The stand-alone module allows the integration of the relative representations in any existing neural network effortlessly.\n\nREFERENCES\n\nDavid Alvarez-Melis, Stefanie Jegelka, and Tommi S. Jaakkola. Towards optimal transport with global invariances. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pp. 1870–1879. PMLR, 2019. URL http://proceedings.mlr.press/v89/alvarez-melis19a.html.\n\nRichard Antonello, Javier S Turek, Vy Vo, and Alexander Huth.\n\nLow-dimensional structure in the space of language representations is reflected in brain responses. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 8332–8344. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 464074179972cbbd75a39abc6954cd12-Paper.pdf.\n\nYamini Bansal, Preetum Nakkiran, and Boaz Barak. Revisiting model stitching to compare neural representations. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 225–236, 2021. URL https://proceedings.neurips.cc/ paper/2021/hash/01ded4259d101feb739b06c399e9cd9c-Abstract.html.\n\nSerguei Barannikov, Ilya Trofimov, Nikita Balabin, and Evgeny Burnaev. Representation topology divergence: A method for comparing neural network representations. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 1607–1626. PMLR, 2022. URL https://proceedings. mlr.press/v162/barannikov22a.html.\n\nFederico Bianchi, Jacopo Tagliabue, Bingqing Yu, Luca Bigon, and Ciro Greco. Fantastic embeddings and how to align them: Zero-shot inference in a multi-shop scenario. ArXiv preprint, abs/2007.14906, 2020. URL https://arxiv.org/abs/2007.14906.\n\nLukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.\n\nwandb.com/. Software available from wandb.com.\n\nNiccolo Biondi, Federico Pernici, Matteo Bruni, and Alberto Del Bimbo. Cores: Compatible representations via stationarity. ArXiv preprint, abs/2111.07632, 2021. URL https: //arxiv.org/abs/2111.07632.\n\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135–146, 2017. doi: 10.1162/tacl a 00051. URL https://aclanthology.org/Q17-1010.\n\nLisa Bonheme and Marek Grzes. How do variational autoencoders learn? insights from representa-\n\ntional similarity, 2022. URL https://arxiv.org/abs/2205.08399.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.\n\nAdri ́an Csisz ́arik, P ́eter K ̋or ̈osi-Szab ́o, ́Akos K. Matszangosz, Gergely Papp, and D ́aniel Varga. Similarity and matching of neural network representations, 2021. URL https://arxiv. org/abs/2110.14633.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=YicbFdNTTy.\n\nWilliam Falcon and The PyTorch Lightning team. PyTorch Lightning, 2019. URL https://\n\ngithub.com/Lightning-AI/lightning.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nGrokAI. nn-template bootstraps pytorch projects by advocating reproducibility & best practices in deep learning, 2021. URL https://github.com/grok-ai/nn-template. Software available from https://github.com/grok-ai/.\n\nMichael Gygli, Jasper Uijlings, and Vittorio Ferrari. Towards reusable network components by\n\nlearning compatible representations. AAAI, 35(9):7620–7629, 2021.\n\nDongyoon Han, Sangdoo Yun, Byeongho Heo, and YoungJoon Yoo. Rethinking channel dimensions for efficient model design. ArXiv preprint, abs/2007.00992, 2020. URL https://arxiv. org/abs/2007.00992.\n\nThomas Hofmann, Bernhard Sch ̈olkopf, and Alexander J Smola. Kernel methods in machine learn-\n\ning. The annals of statistics, 36(3):1171–1220, 2008.\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. URL https://aclanthology.org/ H01-1069.\n\nPhillip Keung, Yichao Lu, Gy ̈orgy Szarvas, and Noah A. Smith. The multilingual Amazon reviews In Proceedings of the 2020 Conference on Empirical Methods in Natural Language corpus. Processing (EMNLP), pp. 4563–4568, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.369. URL https://aclanthology.org/2020. emnlp-main.369.\n\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural network representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 915 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 3519–3529. PMLR, 2019. URL http://proceedings.mlr.press/v97/ kornblith19a.html.\n\nJannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Tom Rainforth, and Yarin Gal. Selfattention between datapoints: Going beyond individual input-output pairs in deep learning, 2021. URL https://arxiv.org/abs/2106.02584.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nNikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. Representational similarity analysis - connecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2, 2008. ISSN 1662-5137. doi: 10.3389/neuro.06.004.2008.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. pp. 32–33, 2009. URL https://www.cs.toronto.edu/ ̃kriz/learning-features-2009-TR.pdf.\n\nRuslan Kuprieiev, skshetry, Dmitry Petrov, Peter Rowlands, Paweł Redzy ́nski, Casper da CostaLuis, Alexander Schepanovski, Gao, David de la Iglesia Castro, Ivan Shcheklein, Batuhan Taskaya, Jorge Orpinel, F ́abio Santos, Dave Berenbaum, daniele, Ronan Lamy, Aman Sharma, Zhanibek Kaimuldenov, Dani Hodovic, Nikita Kodenko, Andrew Grigorev, Earl, Nabanita Dash, George Vyshnya, maykulkarni, Max Hora, Vera, and Sanidhya Mangal. Dvc: Data version control - git for data & models, 2023. URL https://doi.org/10.5281/zenodo.7646429.\n\nAarre Laakso and G. Cottrell. Content and cluster analysis: Assessing representational similarity in\n\nneural systems. Philosophical Psychology, 13:47 – 76, 2000.\n\nGuillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv ́e J ́egou. Word translation without parallel data. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=H196sainb.\n\nY Lecun, L Bottou, Y Bengio, and P Haffner. Gradient-based learning applied to document recog-\n\nnition. Proc. IEEE, 86(11):2278–2324, 1998.\n\nKarel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance and equivalence. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 991–999. IEEE Computer Society, 2015. doi: 10. 1109/CVPR.2015.7298701. URL https://doi.org/10.1109/CVPR.2015.7298701.\n\nQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario ˇSaˇsko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl ́ement Delangue, Th ́eo Matussi`ere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Franc ̧ois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175–184, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-demo.21. URL https://aclanthology.org/2021.emnlp-demo.21.\n\nYixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John E. Hopcroft. Convergent learning: Do different neural networks learn the same representations? In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/ 1511.07543.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv preprint, abs/1907.11692, 2019. URL https://arxiv.org/abs/1907. 11692.\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ArXiv preprint, abs/1301.3781, 2013a. URL https://arxiv.org/ abs/1301.3781.\n\nTom ́as Mikolov, Quoc V. Le, and Ilya Sutskever. Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168, 2013b. URL http://arxiv.org/abs/1309. 4168.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nInsights on representational similarAri S. Morcos, Maithra Raghu, and Samy Bengio. ity in neural networks with canonical correlation. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr ́eal, Canada, pp. 5732–5741, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ a7a3d70c6d17a73140918996d03c014f-Abstract.html.\n\nAntonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodol`a, and Francesco Locatello. Asif: Coupled data turns unimodal models to multimodal without training, 2022. URL https://arxiv.org/abs/2210.01738.\n\nChristopher Olah. Visualizing representations: Deep learning and human beings.\n\nhttp:\n\n//colah.github.io/posts/2015-01-Visualizing-Representations/, 2015. Accessed: 2022-8-2.\n\nRajeev DS Raizada and Andrew C Connolly. What makes different people’s representations alike: neural similarity space solves the problem of across-subject fmri decoding. Journal of cognitive neuroscience, 24(4):868–877, 2012.\n\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzm ́an. WikiMatrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1351–1361, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.115. URL https://aclanthology.org/2021. eacl-main.115.\n\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective Classification in Network Data. AIMag., 29(3):93, 2008. ISSN 2371-9621. doi: 10.1609/aimag.v29i3.2157.\n\nDaniel Shalam and Simon Korman. The self-optimal-transport feature transform. ArXiv preprint,\n\nabs/2204.03065, 2022. URL https://arxiv.org/abs/2204.03065.\n\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4077–4087, 2017. URL https://proceedings.neurips.cc/ paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html.\n\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, and Nathan Srebro. The implicit bias of gradient descent on separable data. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=r1q7n9gAb.\n\nAnton Tsitsulin, Marina Munkhoeva, Davide Mottin, Panagiotis Karras, Alexander M. Bronstein, Ivan V. Oseledets, and Emmanuel M ̈uller. The shape of data: Intrinsic distance for data distributions. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/ forum?id=HyebplHYwB.\n\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. ArXiv preprint, abs/1607.08022, 2016. URL https://arxiv. org/abs/1607.08022.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nIvan Vuli ́c, Sebastian Ruder, and Anders Søgaard. Are all good word vector spaces isomorIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language phic? Processing (EMNLP), pp. 3178–3192, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.257. URL https://aclanthology.org/2020. emnlp-main.257.\n\nLiwei Wang, Lunjia Hu,\n\nJiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, and John E. Hopcroft. Towards understanding learning representations: To what extent do different neural networks learn the same representation. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr ́eal, Canada, pp. 9607–9616, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 5fc34ed307aac159a30d81181c99847e-Abstract.html.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art In Proceedings of the 2020 Conference on Empirical Methods natural language processing. in Natural Language Processing: System Demonstrations, pp. 38–45, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. URL https://arxiv.org/abs/1708.07747.\n\nMuammer Y. Yaman, Sergei V. Kalinin, Kathryn N. Guye, David Ginger, and Maxim Ziatdinov. Learning and predicting photonic responses of plasmonic nanoparticle assemblies via dual variational autoencoders, 2022. URL https://arxiv.org/abs/2208.03861.\n\nZhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 40–48. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/yanga16.html.\n\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 649–657, 2015. URL https://proceedings.neurips.cc/paper/ 2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 HIGH-DIMENSIONAL LATENT SPACES\n\nIn Figure 1, multiple trainings of the same two-dimensional AE produce intrinsically similar latent spaces; in Figure 5 we show this property also holds on AEs with a high-dimensional bottleneck. In the first row, PCA is fitted indipendently in each column, and since the PCA transformation produces the same output everywhere the latent spaces are intrinsically the same. In the second row, PCA is fitted only on the first latent space; since in this case the PCA transformation produces different outputs, the latent spaces, although intrinsically similar, are extrinsically different.\n\nFigure 5: Latent spaces learned by distinct trainings of the same high-dimensional AE on the MNIST dataset. Each column is the latent space obtained by the AE with a different seed. On the first row, the dimensionality reduction is performed through PCAs fitted independently on each latent space, meanwhile, on the second row PCA is fitted on the leftmost latent space and then applied to all of them.\n\nA.2 ANCHORS ANALYSIS\n\nThe cardinality of the anchors set A and the choice of specific anchors is crucial to the quality of the relative representations. At the extreme, selecting one single anchor or the same repeated data points for all anchors, will produce collapsed relative representations. We believe that additional research is required to obtain a better understanding on the optimal choice for A. Questions like “Are anchors set composed only by stopwords worse than the ones composed by meaningful and diverse words?” require empirical evidence and could help revealing the semantics of the latent space. Indeed, each anchor is associated with a dimension in a relative representation; one could inspect the anchor data point to get a sense of the meaning of that latent dimension.\n\nAnchor number. Below, we report a preliminary study on the performance sensitivity against the cardinality of the anchors set. In Figure 6 we report the performance on the node classification task on Cora, with a model trained end-to-end adopting the relative representations while training, and on image classification tasks on CIFAR-100, with a frozen encoder. The performance improves monotonically as the number of anchors increase when the absolute representations are frozen (right). Differently, training models end-to-end proves to be more susceptible to model collapse and instabilities, as increasing the number of anchors does not always improve the performance (left). Further research on the relation between the absolute latent space dimensionality and the relative representation dimensionality (i.e., the number of anchors) is needed to clarify how the two quantities impact the performance, when training end-to-end or not.\n\nAnchor selection. experimental setting analogous to the one described in Section 4.1:\n\nIn Tables 7 and 8, we analyze different anchor selection strategies under an\n\n15\n\nPublished as a conference paper at ICLR 2023\n\ne c\nn a\n\nm\n\nr o\nf r\ne P\n\nNumber of anchors\n\nNumber of anchors\n\nFigure 6: Accuracy vs Number of anchors. Each point is a trained model. Left: Trained embedder on Cora, node classification. Right: Frozen transformer on Cifar100 coarse-grained, image classification. Left is less stable because the absolute embeddings are trained, and we are working on a domain that is less stable (graphs). Some collapsed examples are not visualized.\n\n• uniform The first selection strategy is the same adopted in the main manuscript. We randomly select the anchors with a uniform probability distribution over all the available samples;\n\n• fps We select the anchors according to a farthest point sampling strategy;\n\n• kmeans We select the anchors as the words more close to the centroids of K-means clus-\n\ntering with K = number of anchors;\n\n• top{k} We select the anchors as the k most frequent words, after skipping the first 400\n\nwhich are mostly stopwords.\n\nWe expect strategies that better cover the absolute space with anchors to be the most effective ones. Indeed, the results are comparable across selection strategies, but fps reaches everywhere the best Jaccard and MRR scores while k-means the best Cosine ones. We attribute this behavior to their different nature: they both rely on the geometry of the latent spaces they are applied to, but kmeans also favors high-density regions, and this can become a negative bias for the task at hand. In general, the uniform sampling is the most straightforward to apply, since it does not require additional computation for the selection process, and still achieves good performances.\n\nA.3\n\nINVARIANCE WITH GUARANTEED BOUNDS\n\nIn this section, we explore a slightly modified version of the similarity function adopted in the main paper. The experimental setting is the same as in Section 4.1. We want to measure the similarity between pairs of absolute embeddings and their relative counterparts. To get some kind of quantitative measure, we add a similarity score calculated as the pairwise cosine distance between the two embedding types, averaged. Therefore, a lower score indicates the spaces are more similar. On top of the standard relative representations, the ones computed with sim = SC, here we try to improve the similarity measure with guaranteed robustness to bounded distortion. In Figure 7 we report preliminary results that adopt this technique: a vector-quantized similarity function produces relative representations which are more similar (they have a lower score). The vector-quantization is done through agglomerative clustering on the absolute embeddings at various thresholds t. We leave to future works the study of the trade-off between guaranteed invariance to arbitrary bounded distorsion and the expressiveness of the resulting representations.\n\nA.4 DATASET INFORMATION\n\nIn Table 9 we summarize the datasets utilized in our work, and for each one, we specify the number of classes, to give an idea about the classification difficulty.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nAbs.\n\nRel.\n\nRel. Qnt. t = 1\n\nRel. Qnt. t = 1.5\n\nRel. Qnt. t = 2\n\nt x\ne T\nt s\na F\n\nc e\nV 2\nd r\no W\n\n3.30\n\n1.24\n\nScore ↓ 1.19 Figure 7: The FastText and Word2Vec embeddings of a subset of the English dictionary. The score is the pairwise distance average between the two embedding types, thus a lower score indicates the spaces are more similar. The absolute representations appear very dissimilar meanwhile the relative representations yield almost identical spaces. Quantizing the absolute representations by performing agglomerative clustering with distance threshold t produces even more similar spaces.\n\n1.02\n\n1.09\n\nA.5\n\nIMPLEMENTATION DETAILS\n\nIn this Section, following the corresponing sections in the main paper, we report implementation details for all the experimental settings considered.\n\nTools & Technologies\n\nIn all the experiments presented in this work, the following tools were used:\n\n• NN-Template GrokAI (2021), to easily bootstrap the project and enforce best practices;\n\n• PyTorch Lightning (Falcon & The PyTorch Lightning team, 2019), to ensure reproducible\n\nresults while also getting a clean and modular codebase;\n\n• Weights and Biases (Biewald, 2020), to log experiments and compare runs across huge\n\nsweeps;\n\n• Transformers by HuggingFace (Wolf et al., 2020), to get ready-to-use transformers for both\n\ntext and images;\n\n• Datasets by HuggingFace (Lhoest et al., 2021), to access most of the NLP datasets and\n\nImageNet for CV;\n\n• DVC (Kuprieiev et al., 2023), for data versioning;\n\n• PyTorch Geometric (Fey & Lenssen, 2019), to handle graph datasets and get ready-to-use\n\nGNN architectures.\n\nA.5.1 WORD EMBEDDINGS\n\nFor both the Figure and the Table in Section 4.1, the number of anchors is set to 300 for a fair comparison with the dimensionality of the original spaces. For visualization purposes, we needed the figure to both show an easy clusterable and restricted set of word embeddings. They are obtained by subsampling the shared vocabulary with the following procedure: we select 4 random pivot words, and for each of them we consider the top-200 words in their neighborhood. This results in a total of 800 points divided in 4 clusters, the ones used only for the visualization part. For the quantitative part (table results), we select 20K random words from the shared vocabulary with a fixed seed for reproducibility purposes.\n\nFor the computer vision counterpart (Figure 11 and table 8), the procedure is similar but with the following differences: i) the number of anchors is set to 500 to balance between the different encoding dimensions of the two transformers (384 for ViT-small and 768 for ViT-base); ii) the subsampling for visualization purposes is done by selecting 4 classes and randomly picking 200 samples for each of them;\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nEvaluation metrics Consider the set of ≈ 20k samples S (words for the NLP test, images for the CV one) and the source space X and target space Y and any sample s ∈ S, we compute its representation in X and Y through the functions fX : S → X and fY : S → Y and define the metrics as follows:\n\nJaccard(s) =\n\n| KNNX | KNNX\n\nk (fX(s)) ∩ KNNY k (fX(s)) ∪ KNNY\n\nk (fX(s))| k (fX(s))|\n\nMRR(s) =\n\n1 RankY(fX(s), fY(s))\n\nCosine(s) =\n\nfX(s) · fY(s) ∥fX(s)∥∥fY(s)∥\n\nwhere KNNA k (v) is a function that returns the k-top similar samples (according to cosine similarity) to v in the space A, and RankA(v, u) is a function that returns the index at which u is found in the ordered KNNA\n\nk (v). The final score for each metric is the mean over each s ∈ S.\n\nA.5.2 RELATIVE REPRESENTATION SPACE CORRELATIONS\n\nIn this section, we analyze how similarities in absolute and relative spaces are correlated. Let us consider two spaces alignable in the relative space. We denote elements of the spaces with A ∈ Rm1×n1 and B ∈ Rm2×n2 and corresponding relative embeddings with C ∈ Rm1×d, D ∈ Rm2×d. Examples of A and B can be the FastText and Word2Vec word embedding spaces. We already observed in Table 1 how the spaces A and B are well aligned in the relative space. We can go further and analyze how self similarities in each space are preserved by the relative transform. In Figure 8, we show that relative representations not only facilitate latent space communication, but also preserve the underlying (absolute) latent space metric up to a certain degree.\n\nFigure 8: Self similiarities correlations between each space, measured with the Pearson correlation coefficient. In blue, we denote the self similarities in the absolute spaces A, B of FastText and Word2Vec; in green we depict the relative spaces C, D. The correlation in the vertical arrows indicate how much the underlying metric in the abolute space is preserved by the relative coordinate transformation.\n\nA.5.3 LATENT DISTANCE AS A PERFORMANCE PROXY\n\nThe hyperperameters used in Section 4.2 are summarized in Table 10.\n\nA.5.4 TRAINING WITH ABSOLUTE VS. RELATIVE REPRESENTATIONS\n\nThe models trained on relative representations do not backpropagate through the anchors, which encourages a smoother optimization of the anchors’ representations.\n\nImage Classification The architecture is a standard deep CNN. We run a sweep for each dataset where we vary only the random seed (over 10 possible in total). We then aggregate by dataset and encoding type to obtain the final results with their standard deviation.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nGraph Classification We run a sweep identical to the one in Table 10 for the reference model, except that we sweep on the “Number of layers” with two values: 32 and 64. Each configuration is repeated with 10 different seeds, then we aggregate by dataset and encoding type to obtain the final results with their standard deviation.\n\nA.5.5\n\nIMAGE RECONSTRUCTION\n\nThe relative and absolute models appearing in Figure 4 are vanilla AEs and VAEs, the same for all the datasets, and have a comparable number of trainable parameters. Their architecture is composed by simple convolutions, deconvolutions and mean squared error as reconstruction loss. The number of anchors is 500 and the latent dimensionality of the absolute representations is 500.\n\nA.5.6 TEXT CLASSIFICATION\n\nWe report in Tables 11 to 13 details on the transformers and anchors adopted in Section 5.2.\n\nPreprocessing Following the original work in which the Amazon Reviews dataset was proposed (Keung et al., 2020), we utilize both the title and body of each review. We differ in not using the category and in how we merge them; namely, we add the title as prefix for the body and add a full stop as separator when needed (avoiding duplicates). To obtain a single latent encoding for each sample, with fixed shape, we take the last hidden state and select the representation corresponding to the [CLS] token.\n\nWikipedia anchors We use WikiMatrix, a corpus of sentences extracted from Wikipedia. The sentences are parallel between pairs of languages (i.e., same sentences translated in two languages), and since we are looking for a collection of parallel anchors between all 4 languages, we decided to use the English language as a pivot to compute the intersection. To get the final results, we considered only the sentences with margin score ≥ 1.06, getting high-quality sentence alignments. In Table 13 we show the total number of parallel sentences when computing the intersections. We randomly selected 768 samples to use as anchors.\n\nA.5.7\n\nIMAGE CLASSIFICATION\n\nThe details of the transformers used in Section 5.3 are summarized in Table 14.\n\nA.6 ADDITIONAL RESULTS\n\nIn this section we report additional results on the correlation between latent similarity and performance in Figure 9, results on the multilingual stitching both with Amazon coarse-grained in Table 15 and fine-grained in Table 16, results on the image classification stitching on CIFAR-100 fine-grained in Table 18. Moreover, we evaluate the stitching performance of a multilingual transformer in Table 17.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: Correlation plot between performance and latent similarity with the reference model for multiple different models, over time.\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\nFigure 10: Same encodings as in Table 1 (left) but with tSNE (left) dimensionality reduction or visualizing only their first two dimensions (right).\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTable 7: Extended results from Section 4.1 with different anchor selection strategies. The table reports the mean score for each metric and its std across 10 different seeds.\n\nMode Type\n\nSource\n\nTarget\n\nJaccard ↑\n\nMRR ↑\n\nCosine ↑\n\nm\n\nr o\nf i\n\nn u\n\ns p\n\nf\n\ns n\na e\n\nm k\n\n0 0\n0 1\np o\nt\n\n0 0\n0 5\np o\nt\n\n0 0\n0 0\n1 p\no t\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText\n\nWord2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\nFastText Word2Vec FastText Word2Vec\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.34 ± 0.01 0.39 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.34 ± 0.01 0.41 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.35 ± 0.00 0.39 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.27 ± 0.01 0.35 ± 0.01 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.32 ± 0.00 0.38 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.34 ± 0.00 0.39 ± 0.01 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.94 ± 0.00 0.98 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.94 ± 0.00 0.98 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.94 ± 0.00 0.97 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.84 ± 0.01 0.97 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.92 ± 0.00 0.97 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.93 ± 0.00 0.97 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.01 ± 0.00 0.01 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.86 ± 0.00 0.86 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.01 ± 0.00 0.01 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.81 ± 0.00 0.83 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.01 ± 0.00 0.01 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.87 ± 0.00 0.87 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.01 ± 0.00 0.01 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.85 ± 0.00 0.85 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.01 ± 0.00 0.01 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.86 ± 0.00 0.86 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.01 ± 0.00 0.01 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.86 ± 0.00 0.86 ± 0.00 1.00 ± 0.00\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Generalization of the results from Section 4.1 on word embeddings to a different data modality, with different anchor selection strategies (See Appendix A.2 for their description). The dataset considered is CIFAR-10, and the table reports the mean score for each metric and its std across 10 different seeds.\n\nMode Type\n\nSource\n\nTarget\n\nJaccard ↑ MRR ↑\n\nCosine ↑\n\nm\n\nr o\nf i\n\nn u\n\ns p\n\nf\n\ns n\na e\n\nm k\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\nViT-base\n\nViT-small\n\nViT-base\n\nViT-small\n\nViT-base\n\nViT-small\n\nViT-base\n\nViT-small\n\nViT-base\n\nViT-small\n\nViT-base\n\nViT-small\n\nViT-base ViT-small ViT-base ViT-small\n\nViT-base ViT-small ViT-base ViT-small\n\nViT-base ViT-small ViT-base ViT-small\n\nViT-base ViT-small ViT-base ViT-small\n\nViT-base ViT-small ViT-base ViT-small\n\nViT-base ViT-small ViT-base ViT-small\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.11 ± 0.00 0.11 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.12 ± 0.00 0.12 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.11 ± 0.00 0.10 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.27 ± 0.01 0.30 ± 0.01 1.00 ± 0.00\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.37 ± 0.01 0.39 ± 0.01 1.00 ± 0.00\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.25 ± 0.01 0.27 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.97 ± 0.00 0.97 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.96 ± 0.00 0.96 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 -\n- 1.00 ± 0.00\n\n1.00 ± 0.00 0.97 ± 0.00 0.97 ± 0.00 1.00 ± 0.00\n\nTable 9: All the datasets utilized in our work with their number of classes.\n\nDataset\n\nNumber of Classes\n\ne g\na m\n\nI\n\nMNIST Fashion MNIST CIFAR-10 CIFAR-100 ImageNet1k\n\n10 10 10 20 (coarse) — 100 (fine) 1000\n\n7 6\n3\n\n6 (coarse) — 50 (fine) 14 2 (coarse) — 5 (fine)\n\nh Cora\n\nCiteSeer PubMed\n\nt TREC\n\np a\nr\n\nG\n\nx e\nT\n\nDBpedia Amazon Reviews\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTable 10: The reference model and exhaustive hyperparameter combinations pertaining Section 4.2.\n\nHyperparameter\n\nReference Model\n\nSweep\n\n1 Seed 500 Epochs 32 Number of layers 0.5 Dropout Probability ReLU Hidden Activations Convolution Activation ReLU Adam Optimizer 0.02 Learning Rate GCNConv Graph Embedder\n\n0, 1, 2, 3, 4 10, 30, 50 32, 64 0.1, 0.5 ReLU, Tanh ReLU, Tanh Adam, SGD 0.01, 0.02 GCNConv, GINConv\n\nTable 11: The HuggingFace transformers employed in Section 5.2 to tackle the Cross-lingual setting.\n\nLanguage HuggingFace transformers name\n\nEncoding Dim\n\nEnglish Spanish French Japanese\n\nroberta-base PlanTL-GOB-ES/roberta-base-bne ClassCat/roberta-base-french nlp-waseda/roberta-base-japanese\n\n768 768 768 768\n\nTable 12: The HuggingFace transformers employed in Section 5.2 to tackle the Cross-architecture setting.\n\nHuggingFace transformers name\n\nEncoding Dim\n\nbert-base-cased bert-base-uncased google/electra-base-discriminator roberta-base\n\n768 768 768 768\n\nTable 13: WikiMatrix analysis. Each row shows the number of parallel sentences having a translation available in all the languages of that row. Since we consider all four languages, we have 3338 parallel sentences available.\n\nLanguages\n\nNumber of Sentences\n\nen, es en, ja en, fr en, es, fr en, es, ja en, fr, ja en, es, fr, ja\n\n2302527 264259 1682477 23200 147665 20990 3338\n\nTable 14: Timm transformers used in Section 5.3.\n\nVersion\n\nTimm model name\n\nEncoding Dim\n\nTraining data\n\nViT ViT ViT RexNet\n\nvit base patch16 224 vit small patch16 224 vit base resnet50 384 rexnet 100\n\n768 384 768 1280\n\nJFT-300M, ImageNet ImageNet ImageNet ImageNet\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nTable 15: Stitching performance comparison with different encodings techniques. The table reports the mean weighted F1 (± std) and MAE classification performance on Amazon Reviews coarsegrained, across 5 different seeds. All the language pairs are shown.\n\nAbsolute\n\nRelative\n\nTranslated\n\nWikipedia\n\nDecoder\n\nEncoder\n\nFScore\n\nMAE\n\nFScore\n\nMAE\n\nFScore\n\nMAE\n\nen\n\nes\n\nfr\n\nja\n\nen es fr ja\n\nen es fr ja\n\nen es fr ja\n\nen es fr ja\n\n91.54 ± 0.58 43.67 ± 1.09 54.41 ± 1.61 48.72 ± 0.90\n\n33.23 ± 1.00 91.64 ± 1.02 47.66 ± 0.70 53.10 ± 2.27\n\n51.00 ± 2.63 51.96 ± 2.81 88.22 ± 0.75 50.32 ± 4.16\n\n53.82 ± 2.62 44.91 ± 2.21 66.46 ± 1.30 83.30 ± 0.67\n\n0.08 ± 0.01 0.56 ± 0.01 0.45 ± 0.02 0.51 ± 0.01\n\n0.66 ± 0.01 0.08 ± 0.01 0.52 ± 0.01 0.46 ± 0.02\n\n0.49 ± 0.03 0.48 ± 0.03 0.12 ± 0.01 0.50 ± 0.04\n\n0.46 ± 0.03 0.55 ± 0.02 0.34 ± 0.01 0.17 ± 0.01\n\n90.06 ± 0.60 82.78 ± 0.81 78.49 ± 0.66 65.72 ± 0.55\n\n78.68 ± 2.74 89.96 ± 0.77 78.57 ± 1.80 67.69 ± 0.24\n\n83.32 ± 1.80 82.50 ± 0.83 85.68 ± 1.37 69.38 ± 0.73\n\n68.66 ± 3.62 70.37 ± 6.94 76.49 ± 1.13 81.04 ± 0.82\n\n0.10 ± 0.01 0.17 ± 0.01 0.21 ± 0.01 0.34 ± 0.01\n\n0.21 ± 0.03 0.10 ± 0.01 0.21 ± 0.02 0.32 ± 0.00\n\n0.17 ± 0.02 0.17 ± 0.01 0.14 ± 0.01 0.31 ± 0.01\n\n0.31 ± 0.04 0.29 ± 0.06 0.23 ± 0.01 0.19 ± 0.01\n\n90.45 ± 0.52 78.53 ± 0.30 70.41 ± 0.57 66.31 ± 0.80\n\n76.65 ± 3.23 89.62 ± 0.94 75.25 ± 0.76 61.84 ± 0.61\n\n75.55 ± 0.37 77.12 ± 0.88 86.45 ± 0.96 62.79 ± 0.27\n\n70.26 ± 3.16 58.54 ± 1.21 63.94 ± 2.70 80.80 ± 1.25\n\n0.10 ± 0.01 0.21 ± 0.00 0.29 ± 0.01 0.34 ± 0.01\n\n0.23 ± 0.03 0.10 ± 0.01 0.25 ± 0.01 0.38 ± 0.01\n\n0.24 ± 0.00 0.23 ± 0.01 0.13 ± 0.01 0.37 ± 0.00\n\n0.29 ± 0.03 0.41 ± 0.01 0.36 ± 0.02 0.19 ± 0.01\n\nTable 16: Stitching performance comparison with different encodings techniques. The table reports the mean weighted F1 (± std) and MAE classification performance on Amazon Reviews finegrained, across 5 different seeds. All the language pairs are shown.\n\nAbsolute\n\nRelative\n\nTranslated\n\nWikipedia\n\nDecoder\n\nEncoder\n\nFScore\n\nMAE\n\nFScore\n\nMAE\n\nFScore\n\nMAE\n\nen\n\nes\n\nfr\n\nja\n\nen es fr ja\n\nen es fr ja\n\nen es fr ja\n\nen es fr ja\n\n65.46 ± 2.89 22.70 ± 0.41 30.75 ± 0.67 24.85 ± 0.91\n\n21.24 ± 0.81 61.29 ± 3.04 29.02 ± 0.85 29.23 ± 1.32\n\n27.39 ± 1.22 29.47 ± 3.68 56.40 ± 1.89 25.92 ± 1.31\n\n29.36 ± 0.59 25.64 ± 1.77 31.79 ± 1.91 54.09 ± 1.35\n\n0.38 ± 0.02 1.39 ± 0.03 1.19 ± 0.02 1.37 ± 0.07\n\n1.43 ± 0.07 0.43 ± 0.02 1.26 ± 0.05 1.22 ± 0.02\n\n1.23 ± 0.06 1.18 ± 0.07 0.51 ± 0.01 1.25 ± 0.05\n\n1.17 ± 0.04 1.28 ± 0.04 1.06 ± 0.02 0.60 ± 0.02\n\n61.18 ± 1.92 51.67 ± 1.20 49.18 ± 0.83 37.34 ± 1.49\n\n51.02 ± 2.54 57.89 ± 3.80 48.40 ± 1.02 37.22 ± 1.56\n\n45.55 ± 3.55 40.29 ± 1.72 53.58 ± 0.70 38.60 ± 1.03\n\n38.19 ± 2.28 34.23 ± 2.62 38.50 ± 2.46 50.89 ± 1.70\n\n0.44 ± 0.02 0.62 ± 0.01 0.69 ± 0.02 0.99 ± 0.02\n\n0.68 ± 0.05 0.48 ± 0.03 0.71 ± 0.02 1.03 ± 0.04\n\n0.76 ± 0.09 0.90 ± 0.04 0.57 ± 0.01 0.96 ± 0.02\n\n0.88 ± 0.03 1.00 ± 0.05 0.89 ± 0.02 0.65 ± 0.02\n\n62.36 ± 2.23 45.40 ± 0.68 40.29 ± 0.90 37.73 ± 0.70\n\n47.70 ± 5.08 57.96 ± 4.40 44.92 ± 1.83 34.56 ± 0.87\n\n39.01 ± 1.25 41.29 ± 2.01 54.23 ± 0.95 35.22 ± 0.56\n\n36.57 ± 1.72 33.16 ± 2.28 36.68 ± 3.14 51.64 ± 1.47\n\n0.43 ± 0.02 0.76 ± 0.01 0.91 ± 0.02 1.01 ± 0.02\n\n0.73 ± 0.10 0.48 ± 0.03 0.77 ± 0.01 1.08 ± 0.04\n\n0.88 ± 0.06 0.83 ± 0.04 0.56 ± 0.01 1.08 ± 0.02\n\n0.98 ± 0.02 1.06 ± 0.03 1.00 ± 0.05 0.65 ± 0.02\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nTable 17: Stitching performance comparison on XLM-R, a multilingual model by design. The table reports the mean weighted F1 (± std) and MAE classification performance on Amazon Reviews fine-grained, across 5 different seeds.\n\nDecoder Encoder\n\nFScore\n\nMAE\n\nFScore\n\nMAE\n\nAbsolute\n\nRelative\n\nen\n\nes\n\nfr\n\nja\n\nen es fr ja\n\nen es fr ja\n\nen es fr ja\n\nen es fr ja\n\n65.27 ± 0.94 59.55 ± 0.76 58.58 ± 1.04 57.98 ± 0.77\n\n60.32 ± 1.50 61.25 ± 1.74 59.50 ± 1.41 58.24 ± 1.31\n\n58.00 ± 4.21 56.87 ± 3.79 57.99 ± 3.88 55.83 ± 3.32\n\n59.53 ± 1.73 57.02 ± 1.36 57.48 ± 1.06 61.43 ± 0.97\n\n0.41 ± 0.01 0.48 ± 0.01 0.49 ± 0.01 0.52 ± 0.01\n\n0.47 ± 0.01 0.44 ± 0.01 0.47 ± 0.01 0.51 ± 0.02\n\n0.49 ± 0.03 0.49 ± 0.03 0.47 ± 0.02 0.53 ± 0.03\n\n0.48 ± 0.01 0.51 ± 0.00 0.51 ± 0.01 0.45 ± 0.01\n\n58.24 ± 1.92 52.81 ± 1.57 54.01 ± 1.34 48.47 ± 2.67\n\n45.69 ± 2.19 57.61 ± 0.73 45.16 ± 3.30 41.14 ± 1.76\n\n52.37 ± 1.66 54.99 ± 0.46 57.00 ± 0.90 39.15 ± 1.21\n\n39.46 ± 2.34 40.74 ± 2.75 43.36 ± 3.70 57.67 ± 1.17\n\n0.51 ± 0.03 0.62 ± 0.02 0.59 ± 0.02 0.71 ± 0.04\n\n0.87 ± 0.07 0.51 ± 0.01 0.83 ± 0.09 0.99 ± 0.05\n\n0.66 ± 0.03 0.57 ± 0.01 0.52 ± 0.01 1.02 ± 0.03\n\n1.04 ± 0.07 0.97 ± 0.09 0.89 ± 0.11 0.51 ± 0.01\n\nTable 18: Stitching performance comparison with different encodings techniques. The table reports the mean weighted F1 (± std) classification performance on CIFAR-100 fine-grained, across 5 different seeds.\n\nDecoder\n\nEncoder\n\nAbsolute\n\nRelative\n\nrexnet-100\n\nvit-base-patch16-224\n\nvit-base-resnet50-384\n\nvit-small-patch16-224\n\nrexnet-100 vit-base-patch16-224 vit-base-resnet50-384 vit-small-patch16-224\n\nrexnet-100 vit-base-patch16-224 vit-base-resnet50-384 vit-small-patch16-224\n\nrexnet-100 vit-base-patch16-224 vit-base-resnet50-384 vit-small-patch16-224\n\nrexnet-100 vit-base-patch16-224 vit-base-resnet50-384 vit-small-patch16-224\n\n72.77 ± 0.19 -\n- -\n\n- 88.69 ± 0.14 1.08 ± 0.19 -\n\n- 1.10 ± 0.09 85.85 ± 0.18 -\n\n- -\n- 84.11 ± 0.14\n\n71.39 ± 0.18 40.68 ± 0.50 38.18 ± 0.24 44.11 ± 0.84\n\n57.81 ± 0.39 87.05 ± 0.34 66.65 ± 1.79 73.73 ± 0.60\n\n66.91 ± 0.79 75.70 ± 0.68 85.04 ± 0.38 75.52 ± 0.36\n\n56.60 ± 0.39 70.14 ± 0.46 62.85 ± 1.22 83.24 ± 0.13\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nViT-base ViT-small\n\nViT-base ViT-small\n\nViT-base ViT-small\n\ne t\n\nu\n\nl\n\no s\nb A\n\ne v\n\ni t\na l\ne R\n\nFigure 11: Different dimensionality reduction techniques applied to absolute and relative spaces on CIFAR-10. From left to right: PCA (Principal Component Analysis), tSNE, and visualizing only their first two dimensions. Only 800 randomly sampled points are shown, belonging to the classes ”bird”, ”ship”, ”cat”, and ”frog”.\n\n26",
    "reference": "# Summary Of The Paper\n\nThis paper proposes the opinion that the relative representation, the representation of data described by a fixed set of anchor representations, is invariant among different randomized training factors like seeds, optimization strategies, training steps, and even architectures. To prove their opinion, the authors then prevent extensive studies across a broad range of domains in modern machine learning. The results are impressive and very interesting. The authors show that 1) model trained on datasets sampled from similar domains (like English language) with different distributions may share the same relative representations; 2) the similarity of the relative representations of networks has a positive correlation with their performance; 3) training with relative representations shows comparable performance against the absolute representations; 4) relative representations can be shared by networks of different architectures, training distributions, and seeds, to achieve a valid effect.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. The empirical findings in this paper are very impressive and interesting.\n\n2. The paper is extremely well-written and easy to follow.\n\n3. The experiments involve broad domains of machine learning and are of interest to a general audience.\n\n4. The experiment arguments involve rich aspects of analysis and thus are convincing and impressive.\n\nWeakness:\n\n1. The experiments in sec. 4.1 are somewhat weak compared with those in the remained sections; only one case is considered. Adding an extra case of computer vision task may further enhance it.\n\n2. The existence of this phenomenon is well proven, but the source and deep reason for it to happen are not presented as strongly as the former.\n\n3. In Sec.4.1 and Tab. 1, it will be better if the authors can add references and more detailed explanations for the two metrics Jaccard and Mrr. It is a bit hard for me to understand how to calculate them and their roles directly from the current context.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee my comments above.\n\n# Summary Of The Review\n\nVery impressive results in the general area of machine learning, combined with very extensive and strong empirical arguments. Deeper analysis of the sources and reasons of this phenomenon will be appreciated.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTOWARDS RELIABLE LINK PREDICTION WITH ROBUST GRAPH INFORMATION BOTTLENECK\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLink prediction on graphs has achieved great success with the rise of deep graph learning. However, the potential robustness under the edge noise is less investigated. We reveal that the inherent edge noise that naturally perturbs both input topology and target label leads to severe performance degradation and representation collapse. In this work, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the general information bottleneck, RGIB decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives toward robust representation. We also provide two instantiations, RGIB-SSL and RGIB-REP, which benefit from different methodologies, i.e., self-supervised learning and data reparametrization, for implicit and explicit data denoising, respectively. Extensive experiments on 6 benchmarks of various scenarios verify the effectiveness of the proposed RGIB.\n\n1\n\nINTRODUCTION\n\nAs a fundamental problem in graph learning, link prediction (Liben-Nowell & Kleinberg, 2007) has attracted growing interest in real-world applications like drug discovery (Ioannidis et al., 2020), knowledge graph completion (Bordes et al., 2013), and question answering (Huang et al., 2019). Recent advances from heuristic designs (Katz, 1953; Page et al., 1999) to graph neural networks (GNNs) (Kipf & Welling, 2016a; Gilmer et al., 2017; Kipf & Welling, 2016b; Zhang & Chen, 2018; Zhu et al., 2021) have achieved superior performances. Nevertheless, the poor robustness in imperfect scenarios with the inherent edge noise is still a practical bottleneck to the current deep graph models (Gallagher et al., 2008; Ferrara et al., 2016; Wu et al., 2022a; Dai et al., 2022).\n\nEarly explorations improve the robustness of GNNs for node classification under label noise (Dai et al., 2021; Li et al., 2021) through the smoothing effect of neighboring nodes. Other methods achieve a similar goal via randomly removing edges (Rong et al., 2020) or actively selecting the informative nodes or edges and pruning the task-irrelevant ones (Zheng et al., 2020; Luo et al., 2021). However, when applying these noise-robust methods to the link prediction with noise, only marginal improvements are achieved (see Section 5). The attribution is that the edge noise can naturally deteriorate both the input topology and the target labels (Figure 1(a)). Previous works that consider the noise either in input space or label space cannot effectively deal with such a coupled scenario. Therefore, it raises a new challenge to understand and tackle the edge noise for robust link prediction.\n\nIn this paper, we dive into the inherent edge noise and empirically show the significantly degraded performances it leads to (Section 3.1). Then, we reveal the negative effect of the edge noise through carefully inspecting the distribution of learned representations, and discover that graph representation is severely collapsed, which is reflected by much lower alignment and poorer uniformity (Section 3.2). To solve this challenging problem, we propose the Robust Graph Information Bottleneck (RGIB) principle based on the basic GIB for adversarial robustness (Wu et al., 2020) (Section 4.1). Conceptually, the RGIB principle is with new learning objectives that decouple the mutual information (MI) among noisy inputs ̃A, noisy labels ̃Y , and the representation H. As illustrated in Figure 1(b), RGIB generalizes the basic GIB to learn a robust representation that is resistant to the edge noise.\n\nTechnically, we provide two instantiations of RGIB based on different methodologies, i.e., RGIBSSL and RGIB-REP: (1) the former utilizes contrastive pairs with automatically augmented views\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Illustration of inherent edge noise. The GNN takes the graph topology A as inputs, and predicts the logits of unseen edges with labels Y . The noisy ̃A and ̃Y are added with random edges for simulating the inherent edge noise as in Def. 3.1.\n\n(b) The basic GIB (left) and the proposed RGIB (right). I(·; ·) here indicates the mutual information. To solve the intrinsic deficiency of basic GIB in tackling the edge noise, the RGIB learns the graph represenation H via a further balance of informative signals 1(cid:13), 3(cid:13), 4(cid:13) regarding the H.\n\nFigure 1: Link prediction with inherent edge noise (a) and the proposed RGIB principle (b).\n\nto form the informative regularization in a self-supervised learning manner (Section 4.2); and (2) the latter explicitly purifies the graph topology and supervision targets with the reparameterization mechanism (Section 4.3). Both instantiations are equipped with adaptive designs, aiming to effectively estimate and balance the corresponding informative terms in a tractable manner. For example, the hybrid augmentation algorithm and self-adversarial alignment loss for RGIB-SSL, and the relaxed information constraints on topology space as well as label space for RGIB-REP. Empirically, we show that these two instantiations work effectively under extensive noisy scenarios and can be seamlessly integrated with various existing GNNs (Section 5). Our main contributions are summarized as follows.\n\n• To our best knowledge, we are the first to study the robustness problem of link prediction under the inherent edge noise. We reveal that the inherent noise can bring a severe representation collapse and performance degradation, and such negative impacts are general to common datasets and GNNs.\n\n• We propose a general learning framework, RGIB, with refined representation learning objectives to promote the robustness of GNNs. Two instantiations, RGIB-SSL and RGIB-REP, are proposed upon different methodologies that are equipped with adaptive designs and theoretical guarantees.\n\n• Without modifying the GNN architectures, the RGIB achieves state-of-the-art results on 3 GNNs and 6 datasets under various noisy scenarios, obtaining up to 12.9% AUC promotion. The distribution of learned representations is notably recovered and more robust to the inherent noise.\n\n2 PRELIMINARIES\n\nNotation. We denote V = {vi}N ij=1 as the set of edges. With adjacent matrix A and node features X, an undirected graph is denoted as G = (A, X), where Aij = 1 means there is an edge eij between vi and vj. X[i,:] ∈ RD is the D-dimension node feature of vi. Link prediction is to indicate the existence of query edges with labels Y that are not observed in A.\n\ni=1 as the set of nodes and E = {eij}M\n\nGNNs for Link Prediction. We follow the common link prediction framework, i.e., graph autoencoders (Kipf & Welling, 2016b), where the GNN architecture can be GCN (Kipf & Welling, 2016a), GAT (Veliˇckovi ́c et al., 2018), or SAGE (Hamilton et al., 2017). Given a L-layer GNN, the graph representations H ∈ R|V|×D(cid:48) for each node vi ∈ V are obtained by a L-layer message propagation as the encoding process. For decoding, logits φeij of each query edge eij are computed with a readout function, e.g., the dot product φeij = h(cid:62) i hj. Finally, the optimization objective is to minimize the eij ∈E train −yijlog(cid:0)σ(φeij )(cid:1)−(1−yij)log(cid:0)1−σ(φeij )(cid:1), binary classification loss, i.e., min Lcls = (cid:80) where σ(·) is the sigmoid function, and yij = 1 for positive edges while yij = 0 for negative ones.\n\nTopological denoising approaches. A natural way to tackle the input edge noise is to directly clean the noisy graph. Sampling-based methods, such as DropEdge (Rong et al., 2020), NeuralSparse (Zheng et al., 2020), and PTDNet (Luo et al., 2021), are proposed to remove the taskirrelevant edges. Besides, as GNNs can be easily fooled by adversarial network with only a few perturbed edges (Chen et al., 2018; Zhu et al., 2019; Entezari et al., 2020), defending methods like GCN-jaccard (Wu et al., 2019) and GIB (Wu et al., 2020) are designed for pruning adversarial edges.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nLabel-noise-resistant techniques. For tackling the general problem of noisy label, Co-teaching (Han et al., 2018) lets two neural networks teach each other with small-loss samples based on the memorization effect (Arpit et al., 2017). Besides, peer loss function (Liu & Guo, 2020) pairs independent peer examples for supervision and works within the standard empirical risk minimization framework. As for the graph domain, label propagation techniques proposed by pioneer works (Dai et al., 2021; Li et al., 2021) propagate the reliable signals from clean nodes to noisy ones, which are nonetheless entangled with the node annotations and node classification task that cannot be directly applied here.\n\n3 AN EMPIRICAL STUDY OF THE INHERENT EDGE NOISE\n\nIn this section, we attempt to figure out how GNNs behave when learning with the edge noise and what are the latent mechanisms behind it. We first present an empirical study in Section 3.1 and then investigate the negative impact of noise through the lens of representation distribution in Section 3.2.\n\n3.1 HOW DO GNNS PERFORM UNDER THE INHERENT EDGE NOISE?\n\nSince existing benchmarks are usually well-annotated and clean, there is a need to simulate the inherent edge noise properly to investigate the impact of noise. Note the data split manner adopt by most relevant works (Kipf & Welling, 2016b; Zhang & Chen, 2018; Zhu et al., 2021) randomly divides partial edges as observations and the others as prediction targets. The inherent edge noise, if exists, should be false positive samples and uniformly distributed to both input observations and output labels. Thus, the training data can be with noisy adjacence ̃A and noisy labels ̃Y , i.e., the input noise and label noise. We elaborate the formal simulation of such an additive edge noise as follows.\n\nDefinition 3.1 (Additive edge noise). Given a clean training data, i.e., observed graph G = (A, X) and labels Y ∈ {0, 1} of query edges, the noisy adjacence ̃A is generated by only adding edges to the original adjacent matrix A while keeping the node features X unchanged. The noisy labels ̃Y are generated by only adding false-positive edges to the labels Y . Specifically, given a noise ratio εa, the added noisy edges A(cid:48) ( ̃A = A+A(cid:48)) are randomly generated with the zero elements in A as candidates. It satisfies that A(cid:48) (cid:12) A = O and εa = |nonzero( ̃A)|−|nonzero(A)|/|nonzero(A)|. Similarly, noisy labels are generated and added to original labels, where εy = |nonzero( ̃Y )|−|nonzero(Y )|/|nonzero(Y )|.\n\nWith the simulated noise, an empirical study is then performed with various GNNs and datasets. As shown in Figure 2, the edge noise causes a significant drop in performance, and a larger noise ratio generally leads to greater degradation. It means that these common GNNs normally trained with the stochastic gradient descent are vulnerable to the inherent edge noise, yielding a severe robustness problem. 1 However, none of the existing defending methods are designed for such a coupled noise, which is practical for real-world graph data that can be extremely noisy (Gallagher et al., 2008; Wu et al., 2022a). Thus, it is urgent to understand the noise effect and devise the corresponding robust method.\n\nFigure 2: Performances (test AUC) with simulated edge noise. For fair evaluations, results are obtained by repeating 10 times with 4-layer GNNs on each dataset.\n\n3.2 UNDERSTANDING THE IMPACT OF NOISE VIA REPRESENTATION DISTRIBUTION\n\nDenote a GNN as fw(·) with learnable weights w. Node representations are extracted by a forward pass as H = fw(A, X) while the backward propagation with stochastic gradient descent optimizes the GNN as by contrast When encountering noise within ̃A and ̃Y , the representation H can be directly influenced, since the training neglects the adverse effect of data corruption. Besides, the GNN readouts the edge logit φeij based on top-layer node representations hi and hj, which are possibly degraded or even collapsed under the edge noise (Graf et al., 2021; Nguyen et al., 2022).\n\nFor quantitive and qualitative analysis of H under edge noise, we introduce two concepts (Wang & Isola, 2020) here, i.e., alignment and uniformity. Specifically, alignment is computed as the\n\n1For generality, we provide the full empirical study in Appendix D.\n\n3\n\nclean=20%=40%=60%0.700.750.800.850.90Test AUCGCN-CoraGAT-CoraSAGE-CoraGCN-CiteseerGAT-CiteseerSAGE-CiteseerUnder review as a conference paper at ICLR 2023\n\ndistance of representations between two randomly augmented graphs. It quantifies the stability of GNN when encountering edge noise in the testing phase. A higher alignment means the GNN is more resistant and invariant to input perturbations. Uniformity, on the other hand, qualitatively measures the denoising effects of GNN when learning with edge noise in the training phase. A greater uniformity implies that the learned representations of various samples are more uniformly distributed on the unit hypersphere, preserving as much information about the original data as possible.\n\nAs shown in Table 1 and Figure 3, a poorer alignment and a worse uniformity are brought by a severer edge noise. The learned GNN fw(·) is more sensitive to input perturbations as the alignment values are sharply increased, and the learned edge representations tend to be less uniformly distributed and gradually collapse to individual points as the noise ratio increases. Thereby, it is discovered that the graph representation is severely collapsed under the inherent edge noise, resulting in the nondiscriminative property. This is usually undesirable and potentially risky for downstream applications.\n\nTable 1: Mean values of alignment on Cora dataset, that are calculated as the L2 distance of representations of two randomly perturbed graphs i.e., 1 i=1 ||H i 2||2. N\nwhere H i 1 = fw(Ai 1, X) and H i\n\n2 = fw(Ai\n\n1−H i\n\n2, X).\n\n(cid:80)N\n\ndataset Cora Citeseer\n\nclean\n\n.616 ε = 20% .687 ε = 40% .695 ε = 60% .732\n\n.445 .586 .689 .696\n\n(a) Clean data\n\n(b) ε = 20%\n\n(c) ε = 40%\n\n(d) ε = 60%\n\nFigure 3: Uniformity distribution on Cora dataset. Here, we map the representations of query edges in test set to unit circle of R2 followed by the Gaussian kernel density estimation, as in (Wang & Isola, 2020).\n\n4 ON TACKLING THE REPRESENTATION COLLAPSE VIA ROBUST GIB\n\nAs aforementioned, the inherent edge noise brings unique challenges to link-level graph learning. Without prior knowledge like the noise ratio, without the assistance of auxiliary datasets, and even without modifying GNN architectures, how can learned representations be resistant to the inherent noise? Here, we formally build a method to address this problem from the perspective of graph information bottleneck (Section 4.1) and design its two practical instantiations (Section 4.2 and 4.3).\n\n4.1 THE PRINCIPLE OF ROBUST GIB\n\nRecall in Section 3.2, the graph representation H is severely degraded due to incorporating noisy signals in ̃A and ̃Y . To robustify H, one can naturally utilize the information constraint based on the basic graph information bottleneck (GIB) principle (Wu et al., 2020; Yu et al., 2020), i.e., solving\n\nmin GIB (cid:44) −I(H; ̃Y ), s.t. I(H; ̃A) < γ.\n\n(1)\n\nwhere the hyper-parameter γ constrains the MI I(H; ̃A) to avoid H from capturing excess taskirrelevant features from ̃A. The basic GIB (Eqn. 1) can effectively withstand the input perturbation (Wu et al., 2020). However, it is intrinsically susceptive to label noise since it entirely preserves the label supervision with maximizing I(H; ̃Y ). Empirical results in Section 5 show that it becomes ineffective when learning with inherent edge noise simulated as Definition 3.1.\n\nThe basic GIB decreases I(H; ̃A| ̃Y ) by directly constraining I(H; ̃A) to handle the input noise, as illustrated in Figure 1(b). Symmetrically, the label noise can be hidden in I(H; ̃Y | ̃A), but trivially constraining I(H; ̃Y ) to regularize I(H; ̃Y | ̃A) is not ideal, since it will conflict with Eqn. 1 and also cannot tackle the noise within I( ̃A; ̃Y ). Thus, it is crucial to further decouple the dependences among ̃A, ̃Y , and H, while the noise can exist in areas of I(H; ̃Y | ̃A), I(H; ̃A| ̃Y ), and I( ̃A; ̃Y |H).\n\n4\n\n03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samplesUnder review as a conference paper at ICLR 2023\n\n(a) RGIB (Eqn. 2)\n\n(b) RGIB-SSL (Eqn. 3)\n\n(c) RGIB-REP (Eqn. 4)\n\nFigure 4: Digrams of the proposed RGIB and its two instantiations (best viewed in color).\n\nAnalytically, I( ̃A; ̃Y |H) = I( ̃A; ̃Y )+I(H; ̃Y | ̃A)+I(H; ̃A| ̃Y )−H(H)+H(H| ̃A, ̃Y ), where I( ̃A; ̃Y ) is a constant and redundancy H(H| ̃A, ̃Y ) can be easily minimized. Thus, the I( ̃A; ̃Y |H) can be approximated by the other three terms, i.e., H(H), I(H; ̃Y | ̃A) and I(H; ̃A| ̃Y ). Since the two later terms are also with noise, a balance of these three informative terms can be a solution to the problem.\n\nBased on the above analysis, we propose the Robust Graph Information Bottleneck (RGIB), a new learning objective to balance informative signals regarding H, as illustrated in Figure 4(a), i.e.,\n\nmin RGIB (cid:44) −I(H; ̃Y ), s.t. γ−\n\nH , I(H; ̃Y | ̃A) < γY , I(H; ̃A| ̃Y ) < γA. where constraints on H(H) encourage a diverse H to prevent representation collapse (> γ− H ) and H ) to avoid over-fitting. Another two symmetric terms, I(H; ̃Y | ̃A) and also limit its capacity (< γ+ I(H; ̃A| ̃Y ), mutually regularize posteriors to mitigate the negative impact of inherent noise on H.\n\nH < H(H) < γ+\n\n(2)\n\nNote that MI terms like I(H; ̃A| ̃Y ) are usually intractable. Therefore, we introduce two practical implementations of RGIB, i.e., RGIB-SSL and RGIB-REP, based on different methodologies. The former explicitly optimizes the representation H with the self-supervised regularization, while the latter implicitly optimizes H by purifying the noisy ̃A and ̃Y with the reparameterization mechanism.\n\n4.2\n\nINSTANTIATING ROBUST GIB VIA SELF-SUPERVISED LEARNING\n\nRecall that the graph representation is deteriorated with the supervised learning paradigm. Naturally, we modify it into a self-supervised counterpart by explicitly regularizing the representation H (shown in Figure 4(b)) to avoid collapse and to implicitly capture reliable relations among noisy edges, i.e.,\n\nmin RGIB-SSL (cid:44) − λs(I(H1; ̃Y )+I(H2; ̃Y )) (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) supervision\n\n− λu(H(H1)+H(H2)) (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) uniformity\n\n− λaI(H1; H2) (cid:125)\n\n(cid:123)(cid:122) alignment\n\n(cid:124)\n\n.\n\n(3)\n\nwhere margins λs, λu, λa balance one supervised and two self-supervised regularization terms. When λs ≡ 1 and λu ≡ 0, the RGIB-SSL can be degenerated to the basic GIB.\n\nNote that contrastive learning is a prevalent technique in the self-supervised area to learn robust representation (Chen et al., 2020), where contrasting pair of samples with data augmentation plays an essential role. In practice, we follow such a manner and calculate the supervision term by E[I(H; ̃Y | ̃A)] ≤ E[I(H; ̃Y )] = E ̃As∼P( ̃A)[I(Hs; ̃Y )] ≈ 1/2(I(H1; ̃Y )+I(H2; ̃Y )) = 1/2(Lcls(H1; ̃Y )+Lcls(H2; ̃Y )), where ̃A1, ̃A2 are sampled from P( ̃A). Similarly, we have E[H(H)] ≈ 1/2(H(H1) + H(H2)), and a higher uniformity leads to a high entropy H(H) as proved in Proposition 4.1. As for I(H; ̃A| ̃Y ), we approximate it by the alignment term I(H1; H2) with the derived upper bound in Proposition 4.2. Proposition 4.1. A higher entropy H(H) leads to a higher uniformity. Proof. See Appendix C.1. Proposition 4.2. The alignment I(H1; H2) is the upper bound of I(H; ̃A| ̃Y ). Since I(H; ̃A| ̃Y ) ≤ I(H; ̃A) ≤ 1/2(cid:0)I(H1; H2)+I( ̃A1; ̃A2)(cid:1) = 1/2(cid:0)I(H1; H2)+c(cid:1), a constrained alignment estimated by MI I(H1; H2) is capable to bound a lower I(H; ̃A| ̃Y ) and I(H; ̃A). Proof. See Appendix C.2.\n\nHowever, directly applying existing contrastive methods like (Chen et al., 2020; Khosla et al., 2020) can be suboptimal, since they are not originally designed for graph data and neglect the internal correlation of topology ̃A and target ̃Y . Two following designs are proposed to avoid trivial solutions.\n\nHybrid graph augmentation. To encourage more diverse views with lower I(A1; A2) and to avoid manual selection of augmentation operations, we propose a hybrid augmentation method with four augmentation operations as predefined candidates and ranges of their corresponding hyper-parameters.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nIn each training iteration, two augmentation operators, T 1(·) and T 2(·), and their hyper-parameters θ1 and θ2 are automatically sampled from the search space. Then, two augmented graphs are obtained by applying the two operators on the original graph G. Namely, G1 = T 1(G|θ1) and G2 = T 2(G|θ2).\n\nij, h2\n\nSelf-adversarial alignment & uniformity loss. With representations H1 and H2 from two augmented views, we build the alignment objective by minimizing the representation similarity of the ij) and maximizing that of the randomly-sampled negative pairs (h1 positive pairs (h1 ij, h2 mn), eij (cid:54)= emn. The proposed self-adversarial alignment loss is Ralign = (cid:80)N i +Rneg i=1 Rpos , where i\n(cid:13) mn) · (γ − (cid:13) 2\nRpos ij − h2 (cid:13)h1 ). 2 Impor- (cid:13) 2\ntantly, softmax functions ppos(·) and pneg(·) aim to mitigate the inefficiency problem (Sun et al., 2019) that aligned pairs are not informative. Besides, with Gaussian potential kernel, the uniformity loss is as Runif = (cid:80)K\n\n2, where eij,emn are randomly sampled.\n\ni = pneg(h1\n\ni = ppos(h1\n\n2 +e−(cid:107)h2\n\nand Rneg\n\nij − h2\n\nij) · (cid:13)\n\nij, h2\n\nij, h2\n\nmn(cid:107)2\n\nmn(cid:107)2\n\nij −h1\n\nij −h2\n\n(cid:13)h1\n\nij,mn e−(cid:107)h1\n\n(cid:13) 2\n(cid:13) 2\n\nmn\n\nij\n\nOptimization. As Eqn. 3, the overall loss function of RGIB-SSL is L = λsLcls+λaRalign+λuRunif . Remark 4.1. The inherent noise leads to class collapse, i.e., samples from the same class have the same representation. It comes from trivially minimizing the noisy supervision I(H; ̃Y ) and results in degraded representations shown in Section 3.2. Fortunately, the alignment and uniformity terms regularizing representations can alleviate such noise effects and avoid collapse (see Section 5).\n\n4.3\n\nINSTANTIATING ROBUST GIB VIA DATA REPARAMETERIZATION\n\nAnother realization is by reparameterizing the graph data on both topology space and label space jointly to preserve clean information and discard noise. We propose RGIB-REP that explicitly models the reliability of ̃A and ̃Y via latent variables Z to learn a noise-resistant H (as Figure 4(c)), namely,\n\nmin RGIB-REP (cid:44) − λsI(H; ZY ) (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) supervision\n\n+ λY I(ZY ; ̃Y ) +λAI(ZA; ̃A) (cid:125) (cid:125) (cid:123)(cid:122) (cid:124) topology constraint\n\n(cid:123)(cid:122) label constraint\n\n(cid:124)\n\n.\n\n(4)\n\nwhere latent variables ZY and ZA are clean signals extracted from noisy ̃Y and ̃A. Their complementary parts ZY (cid:48) and ZA(cid:48) are considered as noise, satisfying ̃Y = ZY +ZY (cid:48) and ̃A = ZA +ZA(cid:48). When ZY ≡ ̃Y and ZA ≡ ̃A, the RGIB-REP can be degenerated to the basic GIB. Here, the I(H; ZY ) measures the supervised signals with selected samples ZY , where the classifier takes ZA (a subgraph of ̃A) as input instead of the original ̃A, i.e., H = fw(ZA, X). Constraints I(ZA; ̃A) and I(ZY ; ̃Y ) aim to select the cleanest and most task-relevant information from ̃A and ̃Y .\n\nInstantiation. For deriving a tractable objective regarding ZA and ZY , a parameterized sampler fφ(·) sharing the same architecture and weights as fw(·) is adopted here. fφ(·) generates the probabilistic distribution of edges that include both ̃A and ̃Y by P = σ(HφH (cid:62) φ ) ∈ (0, 1)|V|×|V|, where hidden representations Hφ = fφ( ̃A, X). Then, the Bernoulli sampling is used to obtain edges of high confidences, i.e., ZA = SAMP(P | ̃A), ZY = SAMP(P | ̃Y ) where |ZA| ≤ | ̃A| and |ZY | ≤ | ̃Y |.\n\nn\n\nPφ(P |n)P( ̃A = n) = P(n) (cid:81)n\n\nOptimization. A relaxation is then conducted on the three MI terms in Eqn. 4. With derived bounds in Proposition 4.3 (i.e., RA and RY ) and Proposition 4.4 (i.e., Lcls), the final objective is formed as L = λsLcls + λARA + λY RY , and the corresponding guarantee is discussed in Theorem 4.5. Proposition 4.3. Given the edge number n of ̃A, the marginal distribution of ZA is Q(ZA) = (cid:80) ̃Aij=1 Pij. Then, we derive the upper bound I(ZA; ̃A) ≤ 1−τ = RA, where τ is a con-\n\nE[KL(Pφ(ZA|A)||Q(ZA))] = (cid:80) stant. Similarly, the label constraint is bounded as I(ZY ; ̃Y ) ≤ RY , Proof. See Appendix C.3. Proposition 4.4. Since I(H; ZY ) ≥ EZY ,ZA [log Pw(ZY |ZA)] ≈ −Lcls(fw(ZA), ZY ), the supervision term in Eqn. 4 can be empirically reduced to the classification loss. Proof. See Appendix C.4. Theorem 4.5. Assume the noisy training data Dtrain = ( ̃A, X, ̃Y ) contains a potentially clean subset Dsub = (Z∗ A, X, Z∗ Y ≈ Y , based on which a trained GNN predictor fw(·) satisfies fw(Z∗ A, X) = Z∗ Y +(cid:15). The error (cid:15) is independent from Dsub and (cid:15) → 0. Then, for any λs, λA, λY ∈ [0, 1], ZA = Z∗\n\nτ +(1 − Pij) log 1−Pij\n\neij ∈ ̃A Pij log Pij\n\nY ) and Z∗\n\nY minimizes the RGIB-REP (Eqn. 4). Proof. See Appendix C.5. ij −h2\n\nA and ZY = Z∗\n\nmn(cid:107)2\n\nij −h2\n\n)\n\n)\n\n2Specifically, ppos(h1\n\nij, h2\n\nij) =\n\n, and pneg(h1\n\nij, h2\n\nmn) =\n\nexp(α−(cid:107)h1 i=1 exp(α−(cid:107)h1\n\n(cid:80)N\n\n2\n\nij −h2\n\nmn(cid:107)2\n\n2\n\n.\n\n)\n\nexp((cid:107)h1 i=1 exp((cid:107)h1\n\nij(cid:107)2 ij −h2\n\n2\n\n(cid:80)N\n\nij(cid:107)2\n\n2\n\n)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Method comparison with GCN under inherent noise, i.e., both the input and label noise exist. The boldface numbers mean the best results, while the underlines indicate the second-best results.\n\nmethod\n\nCora\n\nCiteseer\n\nPubmed\n\nFacebook\n\nChameleon\n\nSquirrel\n\n20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60%\n\nStandard DropEdge\n\n.8111 .7419 .6970 .7864 .7380 .7085 .8870 .8748 .8641 .9829 .9520 .9438 .9616 .9496 .9274 .9432 .9406 .9386 .8017 .7423 .7303 .7635 .7393 .7094 .8711 .8482 .8354 .9811 .9682 .9473 .9568 .9548 .9407 .9439 .9377 .9365 NeuralSparse .8190 .7318 .7293 .7765 .7397 .7148 .8908 .8733 .8630 .9825 .9638 .9456 .9599 .9497 .9402 .9494 .9309 .9297 .8047 .7559 .7388 .7795 .7423 .7283 .8872 .8733 .8623 .9725 .9674 .9485 .9607 .9514 .9424 .9485 .9326 .9304 .8197 .7479 .7030 .7533 .7238 .7131 .8943 .8760 .8638 .9820 .9526 .9480 .9595 .9516 .9483 .9461 .9352 .9374 .8185 .7468 .7018 .7423 .7345 .7104 .8961 .8815 .8566 .9807 .9536 .9430 .9543 .9533 .9267 .9457 .9345 .9286 .8143 .7498 .7024 .7473 .7324 .7107 .8872 .8803 .8512 .9794 .9579 .9428 .9503 .9538 .9344 .9443 .9327 .9244 .8198 .7485 .7148 .7509 .7388 .7121 .8899 .8729 .8544 .9773 .9608 .9417 .9554 .9561 .9321 .9472 .9329 .9302 .8240 .7819 .7490 .7554 .7458 .7299 .8853 .8718 .8525 .9588 .9508 .9297 .9561 .9531 .9467 .9473 .9348 .9301 .7872 .6940 .6929 .7632 .7242 .6844 .8922 .8749 .8588 .8899 .8865 .8315 .8978 .8987 .8949 .9394 .9380 .9363 RGIB-REP .8313 .7966 .7591 .7875 .7519 .7312 .9017 .8834 .8652 .9832 .9770 .9519 .9723 .9621 .9519 .9509 .9455 .9434 RGIB-SSL .8930 .8554 .8339 .8694 .8427 .8137 .9225 .8918 .8697 .9829 .9711 .9643 .9655 .9592 .9500 .9499 .9426 .9425\n\nPTDNet Co-teaching Peer loss Jaccard GIB SupCon GRACE\n\n5 EXPERIMENTS\n\nSetup. In this section, we empirically verify the effectiveness of the proposed RGIB framework. 6 popular datasets and 3 GNNs are taken in the experiments. The inherent edge noise is generated based on Definition. 3.1 after the commonly used data split where 85% edges are randomly selected for training, 5% as the validation set, and 10% for testing. The AUC is used as the evaluation metric as in (Zhang & Chen, 2018; Zhu et al., 2021). The software framework is the Pytorch (Paszke et al., 2017), while the hardware platform is a single NVIDIA RTX 3090 GPU. We repeat all the experiments 10 times to obtain evaluation results with mean values and standard deviations. 3\n\nBaselines. As existing robust methods separately deal with input noise or label noise, both kinds of methods should be considered as baselines. For input noise, three sampling-based approaches are used for comparison, i.e., DropEdge (Rong et al., 2020), NeuralSparse (Zheng et al., 2020), and PTDNet (Luo et al., 2021). Jaccard (Wu et al., 2019), GIB (Wu et al., 2020), VIB (Sun et al., 2022), and PRI (Yu et al., 2022), which are designed for pruning adversarial edges, are also included. Two generic methods are selected for label noise, i.e., Co-teaching (Han et al., 2018) and Peer loss (Liu & Guo, 2020). In addition, two contrastive learning methods are taken into comparison with adaptation to the link prediction task, including SupCon (Khosla et al., 2020) utilizing the full labels for supervision and GRACE (Zhu et al., 2020) that are optimized in a self-supervised manner without labels. All the above baseline methods are evaluated w.r.t. their original implementations\n\n5.1 MAIN RESULTS\n\nPerformance comparison. As shown in Table 2, the RGIB achieves the best results in all 6 datasets under the inherent edge noise with various noise ratios, especially on the more challenging datasets, i.e., Cora and Citeseer, where a 12.9% AUC promotion can be gained compared with the second-best methods. When it comes to decoupled noise settings shown in Table 3, RGIB also surpasses all the baselines ad hoc for input noise or label noise by a large margin.\n\nRemark 5.1. The two instantiations of RGIB can be generalized to different scenarios with their own priority w.r.t. intrinsic graph properties that can be complementary to each other with flexible options in practice. Basically, the RGIB-SSL is more adaptive to sparser graphs, e.g., Cora and Citeseer, where the inherent edge noise presents a considerable challenge and results in greater performance degradation. The RGIB-REP can be more suitable for denser graphs, e.g., Facebook and Chameleon. Meanwhile, two instantiations also work effectively on heterogeneous graphs with low homophily.\n\nThe learned representation distribution. Next, we justify that the proposed method can effectively alleviate the representation collapse. Compared with the standard training, both RGIB-REP and RGIB-SSL bring significant improvements for the alignment with much lower values, as in Table 4. At the same time, the uniformity of learned representation is also enhanced: it can be seen from Figure 5 that the various query edges tend to be more uniformly distributed on the unit circle. As RGIB-SSL explicitly constrains the representation, its recovery power on representation distribution is naturally stronger than RGIB-REP, resulting in comparably better alignment and uniformity measures.\n\n3Full evaluation results can be found in Appendix F.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Method comparison with GCN under decoupled input noise (upper) or label noise (lower).\n\ninput noise\n\nCora\n\nCiteseer\n\nPubmed\n\nFacebook\n\nChameleon\n\nSquirrel\n\n20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60%\n\nStandard DropEdge\n\n.8027 .7856 .7490 .8054 .7708 .7583 .8854 .8759 .8651 .9819 .9668 .9622 .9608 .9433 .9368 .9416 .9395 .9411 .8338 .7826 .7454 .8025 .7730 .7473 .8682 .8456 .8376 .9803 .9685 .9531 .9567 .9433 .9432 .9426 .9376 .9358 NeuralSparse .8534 .7794 .7637 .8093 .7809 .7468 .8931 .8720 .8649 .9712 .9691 .9583 .9609 .9540 .9348 .9469 .9403 .9417 .8433 .8214 .7770 .8119 .7811 .7638 .8903 .8776 .8609 .9725 .9668 .9493 .9610 .9457 .9360 .9469 .9400 .9379 .8045 .7871 .7530 .8059 .7753 .7668 .8931 .8792 .8606 .9712 .9707 .9714 .9524 .9446 .9447 .9462 .9425 .9306 .8051 .7866 .7517 .8106 .7767 .7653 .8917 .8811 .8643 .9758 .9703 .9622 .9558 .9482 .9412 .9362 .9386 .9336 .8200 .7838 .7617 .8176 .7776 .7725 .8987 .8764 .8639 .9784 .9702 .9638 .9507 .9436 .9364 .9388 .9345 .9240 .8002 .8099 .7741 .8070 .7717 .7798 .8932 .8808 .8618 .9796 .9647 .9650 .9605 .9521 .9416 .9390 .9406 .9397 .8349 .8301 .8025 .8076 .7767 .7655 .8867 .8739 .8558 .9647 .9517 .9401 .9606 .9536 .9468 .9372 .9343 .9305 .7877 .7107 .6975 .7615 .7151 .6830 .8810 .8795 .8593 .9015 .8833 .8395 .8994 .9007 .8964 .9392 .9378 .9363 RGIB-REP .8624 .8313 .8158 .8299 .7996 .7771 .9008 .8822 .8687 .9833 .9723 .9682 .9705 .9604 .9480 .9495 .9432 .9405 RGIB-SSL .9024 .8577 .8421 .8747 .8461 .8245 .9126 .8889 .8693 .9821 .9707 .9668 .9658 .9570 .9486 .9479 .9429 .9429\n\nPTDNet Co-teaching Peer loss Jaccard GIB SupCon GRACE\n\nlabel noise\n\nCora\n\nCiteseer\n\nPubmed\n\nFacebook\n\nChameleon\n\nSquirrel\n\n20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60% 20% 40% 60%\n\nStandard DropEdge\n\n.8281 .8054 .8060 .7965 .7850 .7659 .9030 .9039 .9070 .9882 .9880 .9886 .9686 .9580 .9362 .9720 .9720 .9710 .8363 .8273 .8148 .7937 .7853 .7632 .9313 .9201 .9240 .9673 .9771 .9776 .9580 .9579 .9578 .9608 .9603 .9698 NeuralSparse .8524 .8246 .8211 .7968 .7921 .7752 .9272 .9136 .9089 .9781 .9781 .9784 .9583 .9583 .9571 .9633 .9626 .9625 .8460 .8214 .8138 .7968 .7765 .7622 .9219 .9099 .9093 .9879 .9880 .9783 .9585 .9576 .9665 .9633 .9623 .9626 .8446 .8209 .8157 .7974 .7877 .7913 .9315 .9291 .9319 .9762 .9797 .9638 .9642 .9650 .9533 .9675 .9641 .9655 .8325 .8036 .8069 .7991 .7990 .7751 .9126 .9101 .9210 .9769 .9750 .9734 .9621 .9501 .9569 .9636 .9694 .9696 .8289 .8064 .8148 .8061 .7887 .7689 .9098 .9135 .9096 .9702 .9725 .9758 .9603 .9659 .9557 .9529 .9512 .9501 .8337 .8137 .8157 .7986 .7852 .7649 .9037 .9114 .9064 .9742 .9703 .9771 .9651 .9582 .9489 .9641 .9628 .9601 .8491 .8275 .8256 .8024 .7983 .7807 .9131 .9108 .9162 .9647 .9567 .9553 .9584 .9580 .9477 .9516 .9595 .9511 .8531 .8237 .8193 .7909 .7630 .7737 .9234 .9252 .9255 .8913 .8972 .8887 .9053 .9074 .9075 .9171 .9174 .9166 RGIB-REP .8554 .8318 .8297 .8083 .7846 .7945 .9357 .9343 .9332 .9884 .9883 .9889 .9785 .9797 .9785 .9735 .9733 .9737 RGIB-SSL .9314 .9224 .9241 .9204 .9218 .9250 .9594 .9604 .9613 .9857 .9881 .9857 .9730 .9752 .9744 .9727 .9729 .9726\n\nPTDNet Co-teaching Peer loss Jaccard GIB SupCon GRACE\n\nTable 5: Comparison on different schedulers. Here, SSL/REP are short for RGIB-SSL/RGIB-REP. Experiments are performed with a 4-layer GAT and (cid:15) = 40% inherent edge noise.\n\ndataset method\n\nCora\n\nCiteseer\n\nPubmed\n\nSSL\n\nREP\n\nSSL\n\nREP\n\nSSL\n\nREP\n\nFacebook\n\nSSL\n\nREP\n\nChameleon REP SSL\n\nSquirrel\n\nSSL\n\nREP\n\nconstant linear(·) sin(·) cos(·) exp(·)\n\n.8398 .8427 .8436 .8334 .8381\n\n.7927 .7653 .7924 .7833 .7815\n\n.8227 .8167 .8132 .8088 .8085\n\n.7742 .7559 .7680 .7647 .7569\n\n.8596 .8645 .8637 .8579 .8617\n\n.8416 .8239 .8275 .8372 .8177\n\n.9694 .9669 .9685 .9477 .9471\n\n.9778 .9529 .9594 .9629 .9613\n\n.9384 .9434 .9447 .9338 .9402\n\n.9498 .9369 .9434 .9444 .9418\n\n.9293 .9316 .9325 .9251 .9316\n\n.9320 .9265 .9282 .9303 .9299\n\nTable 4: Alignment comparison. Here, is short for standard training, std. and SSL/REP are short for RGIBSSL/RGIB-REP, respectively.\n\ndataset method std. REP SSL std. REP SSL\n\nCiteseer\n\nCora\n\nclean .616 .524 .475 .445 .439 .418 ε = 20% .687 .642 .543 .586 .533 .505 ε = 40% .695 .679 .578 .689 .623 .533 ε = 60% .732 .704 .615 .696 .647 .542\n\n(a) Standard\n\n(b) RGIB-REP\n\n(c) RGIB-SSL\n\nFigure 5: Uniformity distribution on Citeseer with ε = 40%.\n\nOptimization schedulers. To reduce the search cost of coefficients in objectives of RGIB-SSL and RGIB-REP, we setup a unified optimization framework that can be formed as L = αLcls + (1 − α)R1 + (1 − α)R2. Here, we attempt 5 different schedulers to tune the only hyper-parameter α ∈ [0, 1], including (1) constant, α ≡ c; (2) linear, αt = k · t, where t is the normalized time step; (3) sine, αt = sin(t · π/2); (4) cosine, αt = cos(t · π/2); and (5) exponential, αt = ek·t. As empirical results summarized in Table 5, the selection of optimization schedulers greatly influences the final results. Although there is no gold scheduler that consistently performs the best, the constant and sine schedulers are generally better than others among the 5 above candidates. A further hyper-parameter study with grid search of the λs in RGIB-SSL and RGIB-REP can be found in Appendix F.2.\n\n8\n\n03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samplesUnder review as a conference paper at ICLR 2023\n\nTable 6: Ablation study for RGIB-SSL and RGIB-REP with a 4-layer SAGE. Here, (cid:15) = 60% indicates the 60% coupled inherent noise, while the (cid:15)a/(cid:15)y represent ratios of decoupled input/label noise.\n\nvariant\n\n(cid:15) = 60%\n\nCora (cid:15)a = 60%\n\n(cid:15)y = 60%\n\n(cid:15) = 60%\n\nChameleon (cid:15)a = 60%\n\n(cid:15)y = 60%\n\nRGIB-SSL (full) - w/o hybrid augmentation - w/o self-adversarial - w/o supervision (λs = 0) - w/o alignment (λa = 0) - w/o uniformity (λu = 0)\n\n.8730 .8604 (1.4%↓) .8705 (0.2%↓)\n\n.9762 .8596 .9695 (0.6%↓) .8150 (5.1%↓) .8410 (2.1%↓) .9746 (0.1%↓) .7480 (12.9%↓) .7810 (10.5%↓) .7820 (13.0%↓) .8626 (10.7%↓) .8628 (11.5%↓) .8512 (12.8%↓) .9722 (0.4%↓) .8194 (4.6%↓) .9751 (0.1%↓) .8355 (2.8%↓)\n\n.9663 .9528 (1.3%↓) .9655 (0.1%↓)\n\n.8994 .8757 (2.6%↓) .8927 (0.7%↓)\n\n.9758 .9746 (0.1%↓) .9732 (0.2%↓)\n\n.9613 (0.5%↓) .9652 (0.1%↓)\n\n.8461 (5.9%↓) .8878 (1.3%↓)\n\n.9749 (0.1%↓) .9710 (0.4%↓)\n\n.8510 (2.5%↓) .8621 (1.2%↓)\n\nRGIB-REP (full) - w/o edge selection (ZA ≡ ̃A) - w/o label selection (ZY ≡ ̃Y ) - w/o topology constraint (λA = 0) - w/o label constraint (λY = 0)\n\n.7611 .7515 (1.2%↓) .7533 (1.0%↓) .7355 (3.3%↓) .7381 (3.0%↓)\n\n.8487 .8199 (3.3%↓) .8373 (1.3%↓) .7699 (9.2%↓) .8106 (4.4%↓)\n\n.8095 .7890 (2.5%↓) .7847 (3.0%↓) .7969 (1.5%↓) .8032 (0.7%↓)\n\n.9567 .9554(0.1%↓) .9484(0.8%↓) .9503(0.6%↓) .9443(1.2%↓)\n\n.9706 .9704 (0.1%↓) .9666 (0.4%↓) .9658 (0.4%↓) .9665 (0.4%↓)\n\n.9676 .9661 (0.1%↓) .9594 (0.8%↓) .9635 (0.4%↓) .9669 (0.1%↓)\n\n5.2 ABLATION STUDY\n\nIn this part, we conduct a thorough ablation study for the RGIB framework. As shown in Table 6, each component contributes to the final performance. For RGIB-SSL, we have the following analysis:\n\n• Hybrid augmentation. RGIB-SSL benefits from the hybrid augmentation algorithm that automatically generates graphs of high diversity for contrast. Compared with fixed augmentation, the hybrid augmentation brings consistent improvements with a 3.0% average AUC promotion on Cora.\n\n• Self-adversarial alignment loss. Randomly-sampled pairs are with hierarchical information to be learned from, and the proposed re-weighting technique further enhances high-quality pairs and decreases low-quality counterparts. It enables to discriminate the more informative contrasting pairs and thus refines the alignment signal for optimization, bringing to up 2.1% AUC promotion.\n\n• Information constraints. Label supervision contributes the most among the three informative terms, even with label noise. Degenerating RGIB-SSL to a pure self-supervised manner without supervision (i.e., λs = 0) leads to an average 11.9% AUC drop. Meanwhile, we show that three regularization terms can be jointly optimized while another two terms are also of significant values.\n\nAs for RGIB-REP, its sample selection mechanism and corresponding constraints are also essential:\n\n• Edge / label selection. Two sample selection methods are near equally important for learning with the coupled inherent noise that both informative sources are required to be purified. Besides, the edge selection is more important for tackling the decoupled input noise, as a greater drop will come when removed. Similarly, the label selection plays a dominant role in handling the label noise.\n\n• Topological / label constraint. Table 6 also shows that the selection mechanism should be regularized by the related constraint; and otherwise, sub-optimal solutions will be achieved. Besides, the topological constraint is generally more sensitive than the label constraint for RGIB-REP.\n\n6 CONCLUSION\n\nIn this work, we study the problem of link prediction with the inherent edge noise and reveal that the graph representation is severely collapsed under such a coupled noise. Based on the observation, we introduce the Robust Graph Information Bottleneck (RGIB) principle, aiming to extract reliable signals via decoupling and balancing the mutual information among inputs, labels, and representation to enhance the robustness and avoid collapse. Regarding the instantiation of RGIB, the self-supervised learning technique and data reparametrization mechanism are utilized to establish the RGIB-SSL and RGIB-REP, respectively. Empirical studies on 6 datasets and 3 GNNs verify the denoising effect of the proposed RGIB under different noisy scenarios. In future work, we will generalize RGIB to more scenarios in the link prediction domain, e.g., multi-hop reasoning on knowledge graphs with multi-relational edges that characterize more diverse and complex patterns.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nTo ensure the reproducibility of the empirical result in this work, we have stated all the details about our experiments in corresponding contents of Section 3 and Section 5. The implementation details are further elaborated in Appendix E. Besides, we will provide an anonymous Github repository for the source codes during the discussion phase for the reviewers of ICLR 2023.\n\nREFERENCES\n\nIbrahim Ahmad and Pi-Erh Lin. A nonparametric estimation of the entropy for absolutely continuous\n\ndistributions (corresp.). IEEE Transactions on Information Theory, 22(3):372–375, 1976.\n\nD. Arpit, S. Jastrz ̨ebski, N. Ballas, D. Krueger, E. Bengio, M. S Kanwal, T. Maharaj, A. Fischer, A. Courville, Y. Bengio, et al. A closer look at memorization in deep networks. In ICML, 2017.\n\nA. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddings for\n\nmodeling multi-relational data. In NeurIPS, 2013.\n\nJinyin Chen, Ziqiang Shi, Yangyang Wu, Xuanheng Xu, and Haibin Zheng. Link prediction adversar-\n\nial attack. arXiv preprint arXiv:1810.01110, 2018.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, 2020.\n\nE. Dai, C. Aggarwal, and S. Wang. Nrgnn: Learning a label noise resistant graph neural network on\n\nsparsely and noisily labeled graphs. In SIGKDD, 2021.\n\nEnyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu, Jiliang Tang, and Suhang Wang. A comprehensive survey on trustworthy graph neural networks: Privacy, robustness, fairness, and explainability. arXiv preprint arXiv:2204.08570, 2022.\n\nNegin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you\n\nneed is low (rank) defending against adversarial attacks on graphs. In WSDM, 2020.\n\nEmilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, and Alessandro Flammini. The rise of\n\nsocial bots. Communications of the ACM, 59(7):96–104, 2016.\n\nBrian Gallagher, Hanghang Tong, Tina Eliassi-Rad, and Christos Faloutsos. Using ghost edges for\n\nclassification in sparsely labeled networks. In SIGKDD, 2008.\n\nJ. Gilmer, S. S Schoenholz, P. F Riley, O. Vinyals, and G. E Dahl. Neural message passing for\n\nquantum chemistry. In ICML, 2017.\n\nFlorian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised con-\n\nstrastive learning. In ICML, 2021.\n\nW. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In NeurIPS,\n\n2017.\n\nB. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama. Co-teaching: Robust\n\ntraining of deep neural networks with extremely noisy labels. In NeurIPS, 2018.\n\nX. Huang, J. Zhang, D. Li, and P. Li. Knowledge graph embedding based question answering. In\n\nWSDM, pp. 105–113, 2019.\n\nV. N Ioannidis, D. Zheng, and G. Karypis. Few-shot link prediction via graph neural networks for\n\ncovid-19 drug-repurposing. arXiv preprint arXiv:2007.10261, 2020.\n\nHidetaka Kamigaito and Katsuhiko Hayashi. Comprehensive analysis of negative sampling in\n\nknowledge graph representation learning. In ICML, 2022.\n\nL. Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39–43, 1953.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nP. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan.\n\nSupervised contrastive learning. 33:18661–18673, 2020.\n\nT. N Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In\n\nICLR, 2016a.\n\nT. N Kipf and M. Welling. Variational graph auto-encoders. NIPS Workshop on Bayesian Deep\n\nLearning, 2016b.\n\nY. Li, J. Yin, and L. Chen. Unified robust training for graph neural networks against label noise. In\n\nPAKDD, 2021.\n\nD. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the\n\nAmerican society for information science and technology, 58(7):1019–1031, 2007.\n\nY. Liu and H. Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In\n\nICML, 2020.\n\nD. Luo, W. Cheng, W. Yu, C. Zong, J. Ni, H. Chen, and X. Zhang. Learning to drop: Robust graph\n\nneural network via topological denoising. In WSDM, 2021.\n\nDuc Anh Nguyen, Ron Levie, Julian Lienen, Gitta Kutyniok, and Eyke Hüllermeier. Memorization-\n\ndilation: Modeling neural collapse under noise. arXiv preprint arXiv:2206.05530, 2022.\n\nL. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the\n\nweb. Technical report, Stanford InfoLab, 1999.\n\nA. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,\n\nand A. Lerer. Automatic differentiation in PyTorch. In ICLR, 2017.\n\nY. Rong, W. Huang, T. Xu, and J. Huang. Dropedge: Towards deep graph convolutional networks on\n\nnode classification. In ICLR, 2020.\n\nQingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Xingcheng Fu, Cheng Ji, and S Yu Philip. Graph\n\nstructure learning with variational information bottleneck. In AAAI, 2022.\n\nZ. Sun, Z. Deng, J. Nie, and J. Tang. Rotate: Knowledge graph embedding by relational rotation in\n\ncomplex space. In ICLR, 2019.\n\nP. Veliˇckovi ́c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph Attention\n\nNetworks. ICLR, 2018.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-\n\nment and uniformity on the hypersphere. In ICML, 2020.\n\nB. Wu, J. Li, C. Hou, G. Fu, Y. Bian, L. Chen, and J. Huang. Recent advances in reliable deep graph\n\nlearning: Adversarial attack, inherent noise, and distribution shift. In arXiv, 2022a.\n\nBingzhe Wu, Jintang Li, Junchi Yu, Yatao Bian, Hengtong Zhang, CHaochao Chen, Chengbin Hou, Guoji Fu, Liang Chen, Tingyang Xu, et al. A survey of trustworthy graph learning: Reliability, explainability, and privacy protection. In arXiv, 2022b.\n\nHuijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial examples on graph data: Deep insights into attack and defense. arXiv preprint arXiv:1903.01610, 2019.\n\nTailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. NeurIPS, 33:\n\n20437–20448, 2020.\n\nZhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang. Understanding\n\nnegative sampling in graph representation learning. In SIGKDD, 2020.\n\nJunchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, and Ran He. Graph information\n\nbottleneck for subgraph recognition. arXiv preprint arXiv:2010.05563, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nShujian Yu, Francesco Alesiani, Wenzhe Yin, Robert Jenssen, and Jose C Principe. Principle of\n\nrelevant information for graph sparsification. In UAI, 2022.\n\nM. Zhang and Y. Chen. Link prediction based on graph neural networks. In NeurIPS, 2018.\n\nC. Zheng, B. Zong, W. Cheng, D. Song, J. Ni, W. Yu, H. Chen, and W. Wang. Robust graph\n\nrepresentation learning via neural sparsification. In ICML, 2020.\n\nDingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks\n\nagainst adversarial attacks. In SIGKDD, 2019.\n\nY. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang. Deep Graph Contrastive Representation Learning.\n\nIn ICML Workshop on Graph Representation Learning and Beyond, 2020.\n\nZ. Zhu, Z. Zhang, L. Xhonneux, and J. Tang. Neural bellman-ford networks: A general graph neural\n\nnetwork framework for link prediction. In NeurIPS, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAppendix\n\nTable of Contents\n\nA Broader impact\n\nB Notations\n\nC Theoretical justification\n\nC.1 Proof for Proposition 4.1 . C.2 Proof for Proposition 4.2 . C.3 Proof for Proposition 4.3 . C.4 Proof for Proposition 4.4 . .\nC.5 Proof for Theorem 4.5 .\n\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\nD A Further Empirical Study of Noise Effects .\n\n. D.1 Full evaluation results . .\n. D.2 Further investigation on the decoupled noise . .\n. D.3 Decoupling input noise and label noise . D.4 A further discussion on the inherent edge noise .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nE Implementation details\n\n. E.1 GNNs for link prediction . E.2 Implementation details of RGIB-SSL . .\nE.3 Baseline implementations .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\nF Full empirical results\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. F.1 Robust methods comparison with clean data F.2 Further ablation studies on the trade-off parameters λ . .\nF.3 Further case studies . .\n. .\nF.4 Full baseline comparison with decoupled noise .\n\n. .\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n14\n\n14\n\n15 15 15 16 16 16\n\n17 17 18 18 19\n\n20 20 21 22\n\n22 22 22 24 28\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA BROADER IMPACT\n\nIn this work, we propose the RGIB framework for denoising the inherent edge noise in graphs and for improving the robustness of link prediction. We conceptually derive the RGIB and empirically justify that it enables the various GNNs to learn the input-invariant and label-invariant graph representations, preventing representation collapse and obtaining superior performances against edge noise. Our objective is to protect and enhance the current graph models, and we do not think our work would have negative societal impacts.\n\nA similar problem has been noticed in an early work (Gallagher et al., 2008), where robots tend to build connections with normal users to spread misinformation on social networks, yielding the degeneration of GNNs for robot detection. In addition, as pointed out by a recent survey (Wu et al., 2022b), the inherent noise can be produced in the data generation process that requires manually annotating the data. As introduced in (Wu et al., 2022b), the inherent noise here refers to irreducible noises in graph structures, attributes, and labels. In short, the studied edge noise in our work is in line with the generic concept of graph inherent noise in the literature, which is recognized as a common problem in practice but also an under-explored challenge in both academic and industrial scenes.\n\nHowever, several existing benchmarks, e.g., Cora and Citeseer, are generally clean and without annotated noisy edges. Inevitably, there is a gap when one would like to study the influence of inherent noise on these common benchmarks. Thus, to fill this gap, it is necessary to simulate the inherent edge noise properly to investigate the impact of noise. If the inherent edge noise exists, it should be false positive samples, while the false negative samples are often intractable to be collected. Thus, we focus on the investigation of the false positive edges as the inherent noise and design the simulation approach in Definition 3.1 as it is more close to the real-world scenarios.\n\nBesides, to our best knowledge, we are the first to study the robustness problem of link prediction under the inherent edge noise. One of our major contributions to this research problem is that we reveal that the inherent noise can bring a severe representation collapse and performance degradation, and such negative impacts are general to common datasets and GNNs.\n\nWhat’s more, it is also possible that new instantiations based on other kinds of methodology are inspired by the robust GIB principle. We believe that such a bidirectional information bottleneck that strictly treats the information source on both input side and label side is helpful in practice, especially for extremely noisy scenarios.\n\nB NOTATIONS\n\nWe summarize the frequently used notations in Table 7.\n\nTable 7: The most frequently used notations in this paper.\n\nnotations\n\nV = {vi}N E = {eij}M\n\ni=1\n\nij=1 A ∈ {0, 1}N ×N\n\nX ∈ RN ×D\n\nG = (A, X)\n\nEquery = {eij}K\n\nij=1\n\nY\n\nhij\n\nH\n\nI(X; Y )\n\nmeanings\n\nthe set of nodes\n\nthe set of edges\n\nthe adjacent matrix with binary elements\n\nthe node features\n\nthe input graph of a GNN\n\nthe query edges to be predicted\n\nthe labels of the query edges\n\nrepresentation of a query edge eij\n\nrepresentation of all the query edges\n\nthe mutual information of X and Y\n\nI(X; Y |Z)\n\nthe conditional mutual information of X and Y when observing Z\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC THEORETICAL JUSTIFICATION\n\nC.1 PROOF FOR PROPOSITION 4.1\n\nProof. As introduced in Section 4.2, the uniformity loss on two augmented graphs is formed as ij,mn e−(cid:107)h1 Runif = (cid:80)K Runif = Eij,mn∼pE [e−(cid:107)hij −hmn(cid:107)2\n\n2]. Next, we obtain its upper bound by the following derivation.\n\n2 . For simplicity, we can reduce it to one graph, i.e.,\n\n2 +e−(cid:107)h2\n\nmn(cid:107)2\n\nmn(cid:107)2\n\nij −h1\n\nij −h2\n\nlog Runif = log Eij,mn∼pE [e−(cid:107)hij −hmn(cid:107)2 2] = log Eij,mn∼pE [e2t·hT ij hmn−2t] ≤Eij∼pE\n\n(cid:2) log Emn∼pE [e2t·hT\n\nij hmn ](cid:3)\n\n=\n\n=\n\n1 N\n\n1 N\n\nN (cid:88)\n\nij\n\nN (cid:88)\n\nij\n\n(cid:2) log\n\nN (cid:88)\n\n[e2t·hT\n\nij hmn ](cid:3)\n\nmn\n\nlog ˆpvMF-KDE(hij) + log ZvMF\n\n(5)\n\n(cid:44) − ˆH(H) + log ZvMF\n\nHere, ˆpvMF-KDE(·) is a von Mises-Fisher (vMF) kernel density estimation based on N training samples, and ZvMF is the vMF normalization constant. ˆH(·) is the resubstitution entropy estimator of H = fw(·) (Ahmad & Lin, 1976). As the uniformity loss Lunif can be approximated by entropy ˆH(H), a higher entropy H(H) indicates a lower uniformity loss, i.e., a higher uniformity.\n\nC.2 PROOF FOR PROPOSITION 4.2\n\nProof. Recall the hybrid graph augmentation technique is adopted by RGIB-SSL for contrasting, we can approximate the expectation E[I(H; ̃A| ̃Y )] by 1/N (cid:80)N i=1 I(Hi; ̃Ai| ̃Y ) with N augmented graphs. Based on this, we drive the upper bound of I(H; ̃A| ̃Y ) as follows.\n\nI(H; ̃A| ̃Y ) ≤ I(H; ̃A) =1/N\n\nN (cid:88)\n\ni=1\n\n(cid:0)I(Hi; ̃Ai)(cid:1)\n\n≈1/2(cid:0)I(H1; ̃A1) + I(H2; ̃A2)(cid:1) =1/2(cid:0)H(H1) + H( ̃A1) + H(H2) + H( ̃A2) − H(H1, ̃A1) − H(H2, ̃A2)(cid:1) ≤1/2(cid:0)H(H1) + H( ̃A1) + H(H2) + H( ̃A2) − H(H1, H2) − H( ̃A1, ̃A2)(cid:1) =1/2(cid:0)I(H1; H2) + I( ̃A1; ̃A2)(cid:1) =1/2(cid:0)I(H1; H2) + c(cid:1)\n\n(6)\n\nThus, a lower I(H1; H2) can upper bounded as lower I(H; ̃A) and I(H; ̃A| ̃Y ).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nC.3 PROOF FOR PROPOSITION 4.3\n\nProof. With the marginal distribution Q(ZA) = (cid:80) the upper bound of MI I(ZA; ̃A) as:\n\nn\n\nPφ(P |n)P( ̃A = n) = P(n) (cid:81)n\n\n ̃Aij=1 Pij, we drive\n\nI(ZA; ̃A) =E\n\nZA, ̃A[log(\n\n=E\n\nZA, ̃A[log(\n\n)]\n\nP(ZA| ̃A) P(ZA) Pφ(ZA| ̃A) Q(ZA)\n\n)] − KL(P(ZA)||Q(ZA))\n\n≤E[KL(Pφ(ZA|A)||Q(ZA))]\n\n(cid:88)\n\n=\n\nPij log\n\neij ∈ ̃A\n\nPij τ\n\n+ (1 − Pij) log\n\n1 − Pij 1 − τ\n\n= RA\n\n(7)\n\nwhere the KL divergence on two given distribution P(x) and Q(x) is defined as KL(P(x)||Q(x)) = (cid:80) P(x) log P(x)/Q(x). Thus, we obtain the upper bound of I(ZA; ̃A) as Eqn. 7. Similarly, the label constraint is bounded as I(ZY ; ̃Y ) ≤ (cid:80)\n\nτ +(1 − Pij) log 1−Pij\n\neij ∈ ̃Y Pij log Pij\n\n1−τ = RY .\n\nx\n\nC.4 PROOF FOR PROPOSITION 4.4\n\nProof. We derive the lower bound of I(H; ZY ) as follows.\n\nI(H; ZY ) =I(fw(ZA); ZY )\n\n]\n\n≥EZY ,ZA [log\n\nPw(ZY |ZA) P(ZY ) =EZY ,ZA [log Pw(ZY |ZA)−log(P(ZY ))] =EZY ,ZA [log Pw(ZY |ZA) + H(ZY )] ≥EZY ,ZA [log Pw(ZY |ZA)] (cid:88)\n\nLcls(fw(ZA), ZY )\n\n≈ −\n\n1 |ZY |\n\neij ∈ZY\n\n(8)\n\nThus, the MI I(H; ZY ) can be lower bounded by the classification loss, and min −λsI(H; ZY ) in RGIB-REP (Eqn. 4) is upper bounded by min λs/|ZY | (cid:80)\n\nLcls(fw(ZA), ZY ) as Eqn. 8.\n\neij ∈ZY\n\nC.5 PROOF FOR THEOREM 4.5\n\nProof. With the relaxation via parametrization in C.4, we first relax the standard RGIB-REP to its upper bound as follows.\n\nmin RGIB-REP (cid:44) − λsI(H; ZY ) + λAI(ZA; ̃A) + λY I(ZY ; ̃Y ) ≤ − λsI(ZA; ZY ) + λAI(ZA; ̃A) + λY I(ZY ; ̃Y )\n\n(9)\n\nY ≈ Y , Eqn. 9 can be reduced to min −λsI(ZA; Y ) + λAI(ZA; ̃A) + λY H(Y ), where As Z∗ I(Y ; ̃Y ) = H(Y ) as Y ⊆ ̃Y . Removing the final term with constant H(Y ), it can be further reduced to min −I(ZA; Y ) + λI(ZA; ̃A), where λ = λA/λs. Since to minimize the −I(ZA; Y ) + λI(ZA; ̃A)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nis equal to maximize the I(ZA; Y ) − λI(ZA; ̃A), next, we conduct the following derivation:\n\nmax I(ZA; Y ) − λI(ZA; ̃A)\n\n= max (cid:0)I(Y ; ZA, ̃A) − I( ̃A; Y |ZA)(cid:1) − λI(ZA; ̃A) = max I(Y ; ZA, ̃A) − I( ̃A; Y |ZA) − λ(cid:0)I(ZA; ̃A, Y ) − I( ̃A; Y |ZA)(cid:1) = max I(Y ; ZA, ̃A) − (1 − λ)I( ̃A; Y |ZA) − λI(ZA; ̃A, Y ) = max I(Y ; ̃A) − (1 − λ)I( ̃A; Y |ZA) − λI(ZA; ̃A, Y ) = max(1 − λ)I( ̃A; Y ) − (1 − λ)I( ̃A; Y |ZA) − λI(ZA; ̃A|Y ) = max(1 − λ)c − (1 − λ)I( ̃A; Y |ZA) − λI(ZA; ̃A|Y ) =(1 − λ)c\n\n(10)\n\nSince MI I( ̃A; Y |ZA) ≥ 0 and I(ZA; ̃A|Y ) ≥ 0 are always true, the optimal Z∗ A should makes −(1 − λ)I( ̃A; Y |ZA) − λI(ZA; ̃A|Y ) = 0 to reach the optimal case. Thus, it should satisfy I( ̃A; Y |ZA) = 0 and I(ZA; ̃A|Y ) = 0 simultaneously. Therefore, ZA = Z∗ A maximizes I(ZA; Y ) − λI(ZA; ̃A), which is equal to minimize −I(ZA; Y ) + λI(ZA; ̃A), i.e., the Y maximizes I(A; ZY ) − λI(ZY ; ̃Y ) as RGIB-REP. Symmetrically, when ZA ≡ A, ZY = Z∗ I(A; ZY ) − λI(ZY ; ̃Y ) = (1 − λ)I( ̃Y ; A) − (1 − λ)I( ̃Y ; A|ZY ) − λI(ZY ; ̃Y |A).\n\nD A FURTHER EMPIRICAL STUDY OF NOISE EFFECTS\n\nThis section is the extension of Section 3 in the main content.\n\nD.1 FULL EVALUATION RESULTS\n\nEvaluation settings. In this part, we provide a thorough empirical study traversing all combinations of following settings from 5 different aspects.\n\n• 3 GNN architectures: GCN, GAT, SAGE.\n\n• 3 numbers of layers: 2, 4, 6.\n\n• 4 noise types: clean, mixed noise, input noise, label noise.\n\n• 3 noise ratios: 20%, 40%, 60%.\n\n• 6 datasets: Cora, CiteSeer, PubMed, Facebook, Chameleon, Squirrel.\n\nThen, we summarize the entire evaluation results of three kinds of GNNs as follows. As can be seen, all three common GNNs, including GCN, GAT, and SAGE, are vulnerable to the inherent edge noise.\n\n• Table 13: full evaluation results with GCN.\n\n• Table 14: full evaluation results with GAT.\n\n• Table 15: full evaluation results with SAGE.\n\nData statistics. The statistics of the 6 datasets in our experiments are shown in Table 8. As can be seen, 4 homogeneous graphs (Cora, CiteSeer, PubMed, and Facebook) are with the much higher homophily values than the other 2 heterogeneous graphs (Chameleon and Squirrel).\n\nTable 8: Dataset statistics.\n\ndataset\n\nCora CiteSeer PubMed Facebook Chameleon Squirrel\n\n# Nodes # Edges\n\n2,708 5,278 Homophily 0.81\n\n3,327 4,676 0.74\n\n19,717 88,648 0.80\n\n4.039 88,234 0.98\n\n2,277 31,421 0.23\n\n5,201 198,493 0.22\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nD.2 FURTHER INVESTIGATION ON THE DECOUPLED NOISE\n\nLoss distribution We visualize the loss distribution under different scenarios to further investigate the memorization effect of GNNs. As shown in Figure 6, two clusters are gradually separated apart with clean data, but such a learning process can be slowed down when training with the noisy-input data, which confuses the model and leads to overlapped distributions. As for the label noise, the model cannot distinguish the noisy samples with a clear decision boundary to separate the clean and noisy samples apart. Besides, it is found that the model can gradually memorize the noisy edges according to the decreasing trend of the corresponding losses, where the loss distribution of noisy edges is minimized and progressively moving towards that of the clean ones.\n\n(a) Training loss of positive/nagative samples with clean data (left) and 40% input-noisy data (right).\n\n(b) Training loss of clean/noisy samples with clean data (left) and 40% label-noisy data (right).\n\nFigure 6: Loss distribution of the standard GCN with 40% input noise (a) and 40% label noise (b).\n\nVisualization of edge representations As shown in Figure 7, the dimensionality reduction technique, T-SNE, is utilized here to measure the similarity of the learned edge representations hij = [hi, hj] with edges eij in the test set E test. Compared with the representation distribution of clean data (Figure 7(a)), which present comparatively distinct cluster boundaries, both the input noise (Figure 7(b)) and the label noise (Figure 7(c)) are with overlapped clusters of the positive and negative samples. Namely, the GCN model is confused by the two patterns of noise, and fails to give a discriminate the true or false edges with their representations.\n\n(a) Clean data\n\n(b) Input noise\n\n(c) Label noise\n\nFigure 7: Visualization of the representations of predictive edges through T-SNE. The edge representations are learned on the Cora dataset by a 4-layer GCN with the standard supervised learning.\n\nD.3 DECOUPLING INPUT NOISE AND LABEL NOISE\n\nFor a further and deeper study of the coupled noise, we use the edge homophily metric to quantify the distribution of edges. Specifically, the homophily value hhomo of the edge eij is computed as the ij cosine similarity of the node feature xi and xj, i.e., hhomo = cos(xi, xj). As shown in figure 8, the distributions of edge homophily are nearly the same for label noise and structure noise, where the\n\nij\n\n18\n\n012010DensityClean, epoch=100positivenegative012010DensityRatio=0.4, epoch=100positivenegative012010DensityClean, epoch=500positivenegative012010DensityRatio=0.4, epoch=500positivenegative012Training Loss010DensityClean, epoch=1000positivenegative012Training Loss010DensityRatio=0.4, epoch=1000positivenegative012010DensityClean, epoch=100clean012010DensityRatio=0.4, epoch=100cleannoisy012010DensityClean, epoch=500clean012010DensityRatio=0.4, epoch=500cleannoisy012Training Loss010DensityClean, epoch=1000clean012Training Loss010DensityRatio=0.4, epoch=1000cleannoisy3020100102030Component 13020100102030Component 2positivenegative3020100102030Component 13020100102030Component 2positivenegative3020100102030Component 13020100102030Component 2positivenegativeUnder review as a conference paper at ICLR 2023\n\nenvelope of the two distributions are almost overlapping. Here, we justify that the randomly split label noise and structure noise are indeed coupled together.\n\n(a) Homophily distributions of the input edges\n\n(b) Homophily distributions of the output edges\n\n(c) Homophily distributions of the noisy edges\n\nFigure 8: Distributions of edge homophily with random split manner.\n\nBased on the measurement of edge homophily, we then decouple these two kinds of noise in simulation. Here, we consider two cases that separate these two kinds of noise apart, i.e., (case 1) input noise with high homophily and label noise with low homophily shown in Figure 9, and (case 2) input noise with low homophily and label noise with high homophily shown in Figure 10.\n\n(a) Homophily distributions of the input edges\n\n(b) Homophily distributions of the output edges\n\n(c) Homophily distributions of the noisy edges\n\nFigure 9: Distributions of edge homophily for decoupled noise (case 1).\n\n(a) Homophily distributions of the input edges\n\n(b) Homophily distributions of the output edges\n\n(c) Homophily distributions of the noisy edges\n\nFigure 10: Distributions of edge homophily for decoupled noise (case 2).\n\nD.4 A FURTHER DISCUSSION ON THE INHERENT EDGE NOISE\n\nBasically, the edge noise can exist in the form of false positive edges or false negative noise. Specifically, the false positive edges are treated as existing edges with label 1, but in fact, such edges do not exist. On the other hand, the false negative edges are treated as non-existing edges with label 0 that the predictive probabilities of such edges will be minimized.\n\nHere, we would highlight that our work focuses on the false positive edges as it is more practical and common in real-world scenarios since the data annotating procedure can produce such a kind of noise (Wu et al., 2022b). Thus, if the inherent edge noise exists, it is more likely to be false positive samples, while the false negative samples are often intractable to be collected and annotated\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nin practice.\n\nActually, investigating the influences of the false negative samples is another line of research, such as (Yang et al., 2020; Kamigaito & Hayashi, 2022), which is orthogonal and complementary to our work. Learning from a clean graph can also encounter the problem of false negative samples, which is usually due to the random sampling of negative nodes/edges. Note that tackling the false negative samples is a well-known problem in the area of link prediction while handling the false positive samples is also valuable but still remains under-explored.\n\nThe input noise and label noise does look quite similar and coupled. From the perspective of data processing, the collected noisy edges in the training set can be randomly split into the observed graph (i.e., the input of GNN) or the predictive graph (i.e., the query edges for the GNN to predict). Considering the noisy edges might come from similar sources, e.g., biases from human annotation, the corresponding noise patterns can also be similar between these two kinds of noise that are naturally coupled together to some extent. However, we would claim and justify that the two kinds of noise are fundamentally different.\n\nAlthough both noise can bring severe degradation on empirical performances, they actually act in different places from the model perspective. As the learnable weights are updated as w := w − η∇wL(H, ̃Y), the label noise ̃Y acting on the backward propagation can directly influence the model. By contrast, the input noise indirectly acts on the learned weights as it appears in the front end of the forward inference, i.e., H = fw(A, X).\n\nEmpirically, as results shown in Table 3 and Table 4, the standard GNN (without any defenses) performs quite differently under the same proportion of input noise and label noise. Such a phenomenon can inspire one to understand the intrinsic denoising mechanism or memorization effects of the GNN, and we would leave that as further work.\n\nMore importantly, from the perspective of defending, it could be easy and trivial to defend if these two kinds of noise are the same. However, none of the existing robust methods can effectively defend such an inherently coupled noise. As can be seen from Table 2, only marginal improvements are achieved when applying the existing robust methods to the coupled noise. While in Table 3 and Table 4, these robust methods work effectively in handling the decoupled noise, i.e., only the structure noise or label noise exists. The reason is that, the properties of coupled noise are much more complex than the single decoupled noise. Both sides of the information sources, i.e., ̃A and ̃Y , should be considered noisy, based on which the defending mechanism could be devised.\n\nE IMPLEMENTATION DETAILS\n\nE.1 GNNS FOR LINK PREDICTION\n\nWe provide a detailed introduction forward propagation and backward update of GNNs in this part.\n\nFormally, let (cid:96) = 1 . . . L denote the layer index, h(cid:96) i is the representation of the node i, MESS(·) is a learnable mapping function to transformer the input feature, AGGREGATE(·) captures the 1-hop information from neighborhood N (v) in the graph, and COMBINE(·) is final combination between neighbour features and the node itself. Then, the l-layer operation of GNNs can be formulated as m(cid:96) , euv) : u ∈ N (v)}), where the representation of node v is h(cid:96)\n\nv = AGGREGATE(cid:96)({MESS(h(cid:96)−1\n\nv = COMBINE(cid:96)(h(cid:96)−1\n\nu , h(cid:96)−1\n\n, m(cid:96)\n\nv).\n\nv\n\nv\n\nAfter L-layer propagation, the final node representations hL e of each e ∈ V are obtained. Then, for each query edge eij ∈ E train unseen from the input graph, the logit φeij is computed with the node representations hL\n\n4 with the readout function, i.e., φeij = READOUT(hL\n\ni and hL\n\nj ) → R.\n\ni , hL\n\nj\n\nFinally, the optimization objective can be defined as minimizing the binary cross-entropy loss, i.e., min Lcls = (cid:80) eij ∈E train −yijlog(σ(φeij )) − (1 − yij)log(1 − σ(φeij )) where σ(·) is the sigmoid function, and yij = 1 for positive edges while yij = 0 for negative ones.\n\nIn addtion, we summarize the detailed architectures of different GNNs in the following Table 9.\n\n4To avoid abusing notations, we use the hi to stand for the final representation hL\n\ni in later contents.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Detailed architectures of different GNNs.\n\nGNN\n\nGCN ml\n\nGAT SAGE ml\n\nMESS(·) & AGGREGATE(·) i = W l (cid:80) hl−1 i = (cid:80) ml i = W l\n\nj∈N (i) αijW lhl−1\n\nj∈N (i) hl−1\n\n1 |N (i)|\n\nj∈N (i)\n\n1√\n\nˆdi ˆdj\n\n(cid:80)\n\nj\n\nj\n\nj\n\nCOMBINE(·) i + W l 1 ˆdi\n\nhl\n\ni = σ(ml\n\nREADOUT(·) φeij = h(cid:62)\n\ni hj\n\nhl−1\n\ni\n\n)\n\nhl\n\ni = σ(ml hl\n\ni = σ(ml\n\ni + W lαiihl−1 i + W lhl−1 )\n\ni\n\ni\n\n) φeij = h(cid:62) φeij = h(cid:62)\n\ni hj i hj\n\nE.2\n\nIMPLEMENTATION DETAILS OF RGIB-SSL\n\nAs introduced in Section 4.2, the graph augmentation technique is adopted here to generate the perpetuated graphs of various views.\n\nTo avoid manually selecting and tuning the augmentation operations, we propose the hybrid graph augmentation method with the four augmentation operations as predefined candidates and the ranges of their corresponding hyper-parameters. The search space is summarized in Table 10, where the candidate operations cover most augmentation approaches except for those operations modifying the number of nodes that are unsuitable for the link prediction task.\n\nIn each training iteration, two augmentation operators T 1(·) and T 2(·) and their hyper-parameters θ1 and θ2 are randomly sampled from the search space as elaborated in Algorithm. 1. The two operators will be performed on the observed graph G, obtaining the two augmented graphs, namely, G1 = T 1(G|θ1) and G2 = T 2(G|θ2). The edge representations are gained by h1 ij = f (G1, eij|w) = h1 i are generated by the GNN model f (·|w) with learnable weights w, and so it is for h2\n\nj , where the node representations h1\n\ni and h2\n\ni (cid:12) h1\n\nij with G2.\n\nTable 10: Search space HT of the hybrid graph augmentation.\n\nOperator Ti(·) edge removing feature masking feature dropping identity\n\nhyper-parameter θTi θer ∈ (0.0, 0.5) θf m ∈ (0.0, 0.3) θf d ∈ (0.0, 0.3) NA.\n\ndescription Randomly remove the observed edges with prob. θer. Masking the columns of node features with prob. θf m. Dropping the elements of node features with prob. θf d. Do not modify anything.\n\nAlgorithm 1 Hybrid graph augmentation.\n\nRequire: number of augmentation operators n;\n\n1: initialize ∅ → T (·) 2: for i = 1 . . . n do 3: 4: 5:\n\nrandomly select one augmentation operator from operator space Ti(·) ∈ HT ; uniformly sample the corresponding hyper-parameter θi ∼ U (θTi); store the newly sampled operator and combine it with the existing ones T (·) ∪ {Ti(·|θi)} → T (·); 6: end for 7: return the hybrid augmentation operators T (·)\n\nThen, we learn the self-supervised edge representations by maximizing the edge-level agreement between the same query edge of different augmented graphs (positive pairs) and minimizing the agreement among different edges (negative pairs) with their representations as shown in Figure 11. Note the hij here is the edge representation. Specifically, we minimize the representation similarity of the positive pairs (h1 ij) and maximize the representation similarity of the randomly-sampled negative pairs (h1\n\nmn), where eij (cid:54)= emn.\n\nij, h2\n\nij, h2\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 11: Illustration of the RGIB-SSL model.\n\nE.3 BASELINE IMPLEMENTATIONS\n\nAll baselines compared in this paper are based on their own original implementations. We list their source links here.\n\n• DropEdge, https://github.com/DropEdge/DropEdge.\n\n• NeuralSparse, https://github.com/flyingdoog/PTDNet.\n\n• PTDNet, https://github.com/flyingdoog/PTDNet.\n\n• GCN Jaccard, https://github.com/DSE-MSU/DeepRobust.\n\n• GIB, https://github.com/snap-stanford/GIB.\n\n• VIB, https://github.com/RingBDStack/VIB-GSL.\n\n• PRI, https://github.com/SJYuCNEL/PRI-Graphs.\n\n• Co-teaching, https://github.com/bhanML/Co-teaching.\n\n• Peer loss functions,\n\nhttps://github.com/weijiaheng/Multi-class-Peer-Loss-functions.\n\n• SupCon, https://github.com/HobbitLong/SupContrast.\n\n• GRACE, https://github.com/CRIPAC-DIG/GRACE.\n\nF FULL EMPIRICAL RESULTS\n\nIn this section, we elaborate the full empirical study on inherent edge noise with various robust methods and GNN architectures.\n\nF.1 ROBUST METHODS COMPARISON WITH CLEAN DATA\n\nHere, we would like to figure out how the robust methods introduced in Section 5 behave when learning with clean data, i.e., no edge noise exists. As shown in Table 11, the proposed two instantiations of RGIB can also boost the predicting performance when learning on clean graphs, and outperforms other baselines in most cases.\n\nF.2 FURTHER ABLATION STUDIES ON THE TRADE-OFF PARAMETERS λ\n\nWe conduct an ablation study with the grid search of several hyper-parameters λ in RGIB. For simplicity, we fix the weight of the supervision signal as one, i.e., λs = 1. Then, the objective of RGIB can be formed as L = Lcls + λ1R1 + λ2R2, where the information regularization terms R1/R2 are alignment and uniformity for RGIB-SSL, while topology constraint and label constraint for RGIB-REP, respectively. As the heatmaps illustrated in Figure 12 and Figure 13, the λ1, λ2 are\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Method comparison with a 4-layer GCN trained on the clean data. The boldface numbers mean the best results, while the underlines indicate the second-best results.\n\nmethod\n\nCora\n\nCiteseer\n\nPubmed\n\nFacebook Chameleon\n\nSquirrel\n\nStandard DropEdge NeuralSparse PTDNet Co-teaching Peer loss Jaccard GIB SupCon GRACE RGIB-REP RGIB-SSL\n\n.8686 .8684 .8715 .8577 .8684 .8313 .8413 .8582 .8529 .8329 .8758 .9260\n\n.8317 .8344 .8405 .8398 .8387 .7742 .8005 .8327 .8003 .8236 .8415 .9148\n\n.9178 .9344 .9366 .9315 .9192 .9085 .8831 .9019 .9131 .9358 .9408 .9593\n\n.9870 .9869 .9865 .9868 .9771 .8951 .9792 .9691 .9692 .8953 .9875 .9845\n\n.9788 .9700 .9803 .9696 .9698 .9374 .9703 .9628 .9717 .8999 .9792 .9740\n\n.9725 .9629 .9635 .9640 .9626 .9422 .9610 .9635 .9619 .9165 .9680 .9646\n\nbetter in certain ranges. Neither too large nor too small value is not guaranteed to find a good solution.\n\n(a) Grid search in {0, 10−2, 10−1, 100, 101, 102}\n\n(b) Grid search in {0, 0.1, 0.2, ..., 0.9, 1.0}\n\nFigure 12: Grid search of hyper-parameters with RGIB-SSL on Cora dataset ((cid:15) = 40%).\n\n(a) Grid search in {0, 10−2, 10−1, 100, 101, 102}\n\n(b) Grid search in {0, 0.1, 0.2, ..., 0.9, 1.0}\n\nFigure 13: Grid search of hyper-parameters with RGIB-REP on Cora dataset ((cid:15) = 40%).\n\n23\n\n00.010.11.010100u00.010.11.010100a0.700.720.740.760.780.800.820.840.860.00.10.20.30.40.50.60.70.80.91.0u0.00.10.20.30.40.50.60.70.80.91.0a0.830.840.850.8600.010.11.010100Y00.010.11.010100A0.640.660.680.700.720.740.760.780.800.00.10.20.30.40.50.60.70.80.91.0Y0.00.10.20.30.40.50.60.70.80.91.0A0.660.680.700.720.740.760.780.80Under review as a conference paper at ICLR 2023\n\nF.3 FURTHER CASE STUDIES\n\nAlignment and uniformity of baseline methods. The alignment of other methods is summarized in Table 12, while the uniformity is visualized in Figure 14 and Figure 15. We have the following three observations.\n\nFirst, the sampling-based methods, e.g., DropEdge, PTDNet, and NeuralSparse, can also promote alignment and uniformity due to their sampling mechanisms to defend the structural perturbations. Second, the contrastive methods, e.g., SupCon and GRACE, are with much better alignment but much worse uniformity. The reason is that the learned representations are severely collapsed, which can be degenerated to single points seen from the uniformity plots but stay nearly unchanged when encountering structural perturbations. Third, the remaining methods are not observed with significant improvements in alignment or uniformity.\n\nWhen connecting the above observations with their empirical performances, we can draw a conclusion. That is, both alignment and uniformity are important to evaluate the robust methods from the perspective of representation learning. Besides, such a conclusion is in line with the previous study (Wang & Isola, 2020).\n\nTable 12: Alignment comparison.\n\nmethod\n\nStandard DropEdge NeuralSparse PTDNet Co-teaching Peer loss Jaccard GIB SupCon GRACE RGIB-REP RGIB-SSL\n\nclean\n\nε = 20% ε = 40% ε = 60% clean\n\nε = 20% ε = 40% ε = 60%\n\nCora\n\nCiteseer\n\n.616 .606 .561 .335 .570 .424 .608 .592 .060 .466 .524 .475\n\n.687 .670 .620 .427 .691 .690 .627 .635 .045 .548 .642 .543\n\n.695 .712 .691 .490 .670 .722 .658 .652 .033 .582 .679 .578\n\n.732 .740 .692 .522 .693 .629 .703 .692 .024 .556 .704 .615\n\n.445 .463 .455 .330 .464 .529 .471 .439 .086 .454 .439 .418\n\n.586 .557 .469 .397 .581 .576 .596 .524 .077 .551 .533 .505\n\n.689 .603 .583 .459 .683 .580 .674 .591 .060 .589 .623 .533\n\n.696 .637 .594 .499 .706 .598 .703 .623 .065 .609 .647 .542\n\nLearning curves of RGIB. We draw the learning curves of RGIB with constant schedulers in Figure 16, Figure 17, Figure 18, and Figure 19. We normalize the values of each plotted line to (0, 1) for better visualization.\n\nFor RGIB-SSL, the uniformity term, i.e, H(H), converges quickly and remains low after 200 epochs. Similarly, the alignment term, i.e, I(H1; H2) also converges in the early stages and keeps stable in the rest. At the same time, the supervised signal, i.e, I(H; Y) gradually and steadily decreases as the training time moves forward. The learning processes are generally stable across different datasets.\n\nAs for RGIB-REP, we observe that the topology I(ZA; ̃A) and label constraints I(ZY; ̃Y) can indeed adapt to noisy scenarios with different noise ratios. As can be seen, these two regularizations converge more significantly when learning on a more noisy case. That is, when noise ratio (cid:15) increases from 0 to 60%, these two regularizations react adaptively to the noisy data ̃A, ̃Y . Such a phenomenon shows that RGIB-REP with these two information constraints works as an effective information bottleneck to filter out the noisy signals.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\n(a) standard\n\n(b) Dropedge\n\n(c) Neural Sparse\n\n(d) PTDNet\n\n(e) co-teaching\n\n(f) peer loss\n\n(g) Jaccard\n\n(h) GIB\n\n(i) SupCon\n\n(j) GRACE\n\n(k) RGIB-REP\n\n(l) RGIB-SSL\n\nFigure 14: Uniformity on Cora with (cid:15) = 40% inherent noise.\n\n25\n\n03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samplesUnder review as a conference paper at ICLR 2023\n\n(a) standard\n\n(b) Dropedge\n\n(c) Neural Sparse\n\n(d) PTDNet\n\n(e) co-teaching\n\n(f) peer loss\n\n(g) Jaccard\n\n(h) GIB\n\n(i) SupCon\n\n(j) GRACE\n\n(k) RGIB-REP\n\n(l) RGIB-SSL\n\nFigure 15: Uniformity on Citeseer with (cid:15) = 40% inherent noise.\n\n26\n\n03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samples03264961280326496128positive samples03264961280326496128negative samplesUnder review as a conference paper at ICLR 2023\n\n(a) clean\n\n(b) ε = 20%\n\n(c) ε = 40%\n\n(d) ε = 60%\n\nFigure 16: Learning curves of RGIB-SSL on Cora dataset.\n\n(a) clean\n\n(b) ε = 20%\n\n(c) ε = 40%\n\n(d) ε = 60%\n\nFigure 17: Learning curves of RGIB-REP on Cora dataset.\n\n(a) clean\n\n(b) ε = 20%\n\n(c) ε = 40%\n\n(d) ε = 60%\n\nFigure 18: Learning curves of RGIB-SSL on Citeseer dataset.\n\n(a) clean\n\n(b) ε = 20%\n\n(c) ε = 40%\n\n(d) ε = 60%\n\nFigure 19: Learning curves of RGIB-REP on Citeseer dataset.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nF.4 FULL BASELINE COMPARISON WITH DECOUPLED NOISE\n\nThe entire evaluation with 10 baselines and two proposed methods are conducted keeping the same settings as in D.1.\n\nResults on each dataset are summarized as follows.\n\n• Table 16, Table 17, Table 18 : full results of GCN/GAT/SAGE on Cora dataset.\n\n• Table 19, Table 20, Table 21 : full results of GCN/GAT/SAGE on CiteSeer dataset.\n\n• Table 22, Table 23, Table 24 : full results of GCN/GAT/SAGE on PubMed dataset.\n\n• Table 25, Table 26, Table 27 : full results of GCN/GAT/SAGE on Facebook dataset.\n\n• Table 28, Table 29, Table 30 : full results of GCN/GAT/SAGE on Chameleon dataset.\n\n• Table 31, Table 32, Table 33 : full results of GCN/GAT/SAGE on Squirrel dataset.\n\nTable 13: Full results of GCN with edge noise.\n\nlayers\n\nclean\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nCora\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n.9183±.0071 .8870±.0108 .8430±.0145 .7959±.0190 .9091±.0085 .8943±.0085 .8905±.0092 .9020±.0041 .8793±.0099 .8723±.0103 .8686±.0102 .8111±.0213 .7419±.0325 .6970±.0377 .8027±.0580 .7856±.0526 .7490±.0498 .8281±.0133 .8054±.0213 .8060±.0172 .8256±.0494 .7940±.0133 .7429±.0226 .7177±.0207 .8035±.0370 .7973±.0166 .7546±.0270 .8249±.0112 .7925±.0137 .7901±.0145\n\nCiteseer\n\n.8968±.0092 .8651±.0084 .8355±.0154 .8254±.0066 .8767±.0097 .8615±.0090 .8585±.0085 .8834±.0155 .8649±.0104 .8674±.0122 .8317±.0217 .7864±.0145 .7380±.0201 .7085±.0146 .8054±.0240 .7708±.0325 .7583±.0258 .7965±.0124 .7850±.0174 .7659±.0240 .8161±.0163 .7355±.0387 .7110±.0222 .7106±.0168 .7720±.0161 .7460±.0401 .7212±.0470 .7900±.0131 .7741±.0157 .7648±.0229\n\nPubmed\n\n.9737±.0011 .9473±.0019 .9271±.0027 .9141±.0041 .9590±.0022 .9468±.0022 .9337±.0016 .9646±.0024 .9637±.0021 .9597±.0022 .9178±.0084 .8870±.0041 .8748±.0031 .8641±.0041 .8854±.0051 .8759±.0031 .8651±.0040 .9030±.0082 .9039±.0029 .9070±.0062 .9081±.0056 .8870±.0056 .8731±.0032 .8640±.0036 .8855±.0025 .8742±.0029 .8652±.0041 .9050±.0083 .9112±.0059 .9063±.0036\n\nFacebook\n\n.9887±.0008 .9880±.0007 .9866±.0007 .9843±.0010 .9878±.0008 .9852±.0006 .9834±.0011 .9892±.0006 .9888±.0008 .9888±.0007 .9870±.0009 .9829±.0020 .9520±.0424 .9438±.0402 .9819±.0015 .9668±.0147 .9622±.0154 .9882±.0007 .9880±.0007 .9886±.0006 .9849±.0009 .9798±.0013 .9609±.0138 .9368±.0179 .9764±.0034 .9502±.0096 .9469±.0160 .9863±.0013 .9865±.0012 .9876±.0010\n\n.9823±.0027 .9753±.0025 .9696±.0022 .9657±.0029 .9784±.0017 .9762±.0016 .9754±.0023 .9775±.0018 .9769±.0018 .9755±.0036 .9797±.0021 .9616±.0033 .9496±.0190 .9274±.0276 .9608±.0038 .9433±.0261 .9368±.0271 .9686±.0020 .9580±.0021 .9362±.0035 .9752±.0036 .9662±.0042 .9511±.0079 .9286±.0067 .9656±.0045 .9450±.0177 .9276±.0229 .9752±.0027 .9766±.0035 .9745±.0040\n\nChameleon\n\n.9761±.0011 .9621±.0018 .9519±.0020 .9444±.0024 .9610±.0028 .9490±.0031 .9401±.0036 .9744±.0013 .9731±.0010 .9722±.0011 .9725±.0011 .9432±.0036 .9406±.0031 .9386±.0025 .9416±.0042 .9395±.0011 .9411±.0040 .9720±.0016 .9720±.0016 .9710±.0021 .9694±.0028 .9484±.0049 .9429±.0038 .9408±.0039 .9489±.0057 .9408±.0021 .9386±.0022 .9688±.0028 .9675±.0027 .9656±.0034\n\nSquirrel\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nTable 14: Full results of GAT with edge noise.\n\nlayers\n\nclean\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nCora\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n.9076±.0070 .8786±.0155 .8489±.0131 .8286±.0120 .9014±.0070 .8825±.0097 .8880±.0077 .8991±.0088 .8841±.0097 .8830±.0101 .8783±.0103 .8323±.0181 .8005±.0212 .7841±.0095 .8616±.0107 .8454±.0118 .8377±.0146 .8530±.0121 .8357±.0082 .8281±.0114 .8650±.0157 .8067±.0302 .7514±.0389 .7032±.0452 .8414±.0127 .7891±.0537 .7678±.0641 .8376±.0097 .8154±.0069 .8024±.0169\n\nCiteseer\n\n.8911±.0090 .8586±.0125 .8338±.0127 .8207±.0121 .8689±.0096 .8526±.0130 .8512±.0174 .8762±.0076 .8650±.0102 .8648±.0166 .8386±.0138 .8026±.0157 .7775±.0248 .7518±.0183 .8191±.0092 .8043±.0105 .7912±.0073 .8174±.0172 .7998±.0143 .7934±.0156 .8299±.0098 .7807±.0117 .7373±.0270 .7139±.0251 .7970±.0134 .7860±.0107 .7741±.0126 .7963±.0129 .7883±.0162 .7801±.0161\n\nPubmed\n\n.9406±.0032 .9173±.0028 .8984±.0030 .8884±.0033 .9255±.0024 .9176±.0035 .9102±.0030 .9306±.0038 .9271±.0030 .9232±.0034 .8960±.0068 .8610±.0045 .8434±.0042 .8339±.0048 .8668±.0040 .8547±.0050 .8476±.0037 .8817±.0042 .8772±.0040 .8696±.0062 .8631±.0072 .8315±.0059 .8116±.0073 .8040±.0092 .8374±.0036 .8201±.0321 .8067±.0306 .8480±.0086 .8414±.0116 .8313±.0071\n\nFacebook\n\n.9874±.0008 .9869±.0005 .9856±.0008 .9836±.0007 .9878±.0006 .9861±.0006 .9858±.0008 .9872±.0006 .9864±.0009 .9857±.0008 .9875±.0007 .9857±.0015 .9850±.0012 .9820±.0014 .9855±.0011 .9827±.0019 .9773±.0046 .9873±.0010 .9874±.0010 .9874±.0005 .9860±.0007 .9805±.0025 .9658±.0321 .9577±.0314 .9804±.0018 .9738±.0044 .9710±.0036 .9854±.0015 .9860±.0007 .9867±.0012\n\n.9770±.0044 .9725±.0027 .9650±.0018 .9625±.0018 .9767±.0026 .9747±.0020 .9759±.0018 .9746±.0023 .9743±.0017 .9711±.0041 .9734±.0047 .9721±.0035 .9652±.0023 .9605±.0031 .9741±.0028 .9686±.0030 .9674±.0027 .9740±.0037 .9738±.0027 .9712±.0047 .9742±.0052 .9659±.0029 .9573±.0036 .9482±.0054 .9644±.0033 .9543±.0075 .9474±.0074 .9722±.0043 .9688±.0055 .9698±.0065\n\nChameleon\n\n.9740±.0011 .9680±.0007 .9635±.0017 .9588±.0025 .9702±.0008 .9690±.0010 .9659±.0014 .9719±.0018 .9701±.0017 .9686±.0012 .9720±.0023 .9581±.0046 .9436±.0063 .9335±.0062 .9592±.0047 .9455±.0075 .9415±.0061 .9682±.0030 .9690±.0028 .9686±.0021 .9578±.0067 .9507±.0050 .9309±.0164 .9254±.0089 .9487±.0065 .9419±.0041 .9255±.0073 .9585±.0097 .9520±.0070 .9507±.0162\n\nSquirrel\n\nTable 15: Full results of SAGE with edge noise.\n\nlayers\n\nclean\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nCora\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n2 4\n6\n\n.9045±.0066 .8733±.0101 .8520±.0137 .8469±.0056 .9006±.0074 .8857±.0093 .8917±.0063 .8868±.0070 .8695±.0106 .8564±.0091 .8664±.0109 .8225±.0079 .7833±.0093 .7595±.0259 .8607±.0110 .8437±.0099 .8387±.0183 .8309±.0090 .8046±.0074 .7920±.0191 .8426±.0207 .7787±.0423 .7420±.0251 .7180±.0248 .8256±.0222 .7947±.0561 .8005±.0421 .8158±.0168 .7707±.0235 .7660±.0134\n\nCiteseer\n\n.8648±.0098 .8438±.0143 .8404±.0113 .8279±.0098 .8644±.0109 .8608±.0064 .8647±.0161 .8560±.0120 .8533±.0112 .8412±.0122 .8329±.0093 .7914±.0101 .7686±.0213 .7539±.0149 .8171±.0173 .8226±.0141 .8157±.0203 .8068±.0116 .7891±.0136 .7705±.0172 .8390±.0187 .7708±.0168 .7223±.0614 .7204±.0236 .8086±.0145 .7997±.0183 .7872±.0145 .7903±.0219 .7690±.0201 .7564±.0096\n\nPubmed\n\n.8995±.0044 .9136±.0032 .9094±.0035 .9035±.0040 .9295±.0025 .9378±.0022 .9410±.0023 .8817±.0034 .8793±.0043 .8757±.0041 .8446±.0058 .8627±.0056 .8663±.0061 .8619±.0073 .8715±.0080 .8901±.0082 .9033±.0057 .8386±.0085 .8330±.0104 .8268±.0092 .8360±.0224 .8314±.0081 .8105±.0279 .8333±.0089 .8224±.0335 .8538±.0105 .8566±.0199 .8242±.0104 .8161±.0071 .8090±.0129\n\nFacebook\n\n.9882±.0008 .9858±.0008 .9827±.0008 .9788±.0012 .9881±.0008 .9862±.0007 .9862±.0008 .9867±.0006 .9840±.0006 .9825±.0014 .9859±.0013 .9824±.0015 .9783±.0025 .9698±.0040 .9849±.0015 .9815±.0024 .9815±.0025 .9844±.0015 .9817±.0007 .9809±.0009 .9828±.0024 .9751±.0055 .9603±.0365 .9616±.0091 .9715±.0077 .9672±.0057 .9517±.0228 .9761±.0136 .9788±.0045 .9749±.0151\n\n.9786±.0030 .9683±.0036 .9605±.0038 .9536±.0028 .9754±.0019 .9734±.0015 .9762±.0018 .9700±.0019 .9670±.0026 .9603±.0049 .9743±.0025 .9645±.0035 .9580±.0035 .9505±.0010 .9721±.0017 .9721±.0021 .9721±.0016 .9678±.0026 .9641±.0028 .9605±.0051 .9729±.0022 .9606±.0046 .9541±.0058 .9470±.0035 .9679±.0031 .9658±.0041 .9640±.0059 .9673±.0026 .9641±.0038 .9625±.0042\n\nChameleon\n\n.9745±.0015 .9680±.0015 .9626±.0015 .9570±.0016 .9737±.0010 .9736±.0010 .9721±.0011 .9692±.0015 .9643±.0012 .9606±.0017 .9689±.0052 .9584±.0107 .9577±.0076 .9541±.0021 .9637±.0092 .9630±.0079 .9607±.0090 .9663±.0020 .9612±.0049 .9612±.0020 .9682±.0045 .9555±.0065 .9528±.0038 .9461±.0054 .9592±.0053 .9600±.0036 .9551±.0042 .9574±.0192 .9540±.0207 .9583±.0023\n\nSquirrel\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nTable 16: Full results on Cora dataset with GCN.\n\nGCN\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.8870±.0108 .8430±.0145 .7959±.0190 .9091±.0085 .8943±.0085 .8905±.0092 .9020±.0041 .8793±.0099 .8723±.0103 .8773±.0128 .8305±.0175 .8091±.0104 .9016±.0086 .8692±.0120 .8635±.0099 .9016±.0059 .8791±.0089 .8660±.0095 NeuralSparse .8869±.0096 .8497±.0103 .8215±.0158 .9118±.0063 .8903±.0113 .8849±.0086 .8995±.0069 .8797±.0073 .8743±.0088 .8853±.0127 .8451±.0168 .8180±.0106 .9095±.0108 .8926±.0106 .8825±.0096 .9030±.0080 .8801±.0067 .8702±.0147 Co-teaching .8857±.0202 .8419±.0198 .8026±.0237 .9084±.0132 .8959±.0107 .8901±.0101 .9021±.0203 .8929±.0126 .8699±.0115 .8867±.0115 .8472±.0237 .7970±.0182 .9134±.0147 .8993±.0171 .8806±.0138 .9015±.0021 .8874±.0163 .8813±.0291 .8912±.0190 .8461±.0168 .7964±.0182 .9107±.0210 .9016±.0270 .8996±.0238 .9094±.0107 .8856±.0093 .8721±.0135 .8857±.0296 .8464±.0326 .8037±.0309 .9126±.0065 .8973±.0141 .8937±.0150 .9083±.0136 .8828±.0136 .8807±.0146 .8827±.0125 .8451±.0157 .8125±.0119 .9006±.0074 .8877±.0073 .8846±.0124 .8876±.0082 .8668±.0088 .8721±.0092 .8588±.0158 .8220±.0204 .7987±.0076 .8671±.0121 .8102±.0126 .8048±.0119 .8990±.0092 .8887±.0109 .8896±.0074 RGIB-REP .8915±.0091 .8516±.0120 .8358±.0077 .9137±.0060 .8958±.0048 .8911±.0102 .9046±.0107 .8958±.0120 .8935±.0087 RGIB-SSL .9272±.0091 .9001±.0145 .8892±.0107 .9342±.0049 .9108±.0063 .9014±.0107 .9473±.0054 .9427±.0051 .9380±.0084\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8111±.0213 .7419±.0325 .6970±.0377 .8027±.0580 .7856±.0526 .7490±.0498 .8281±.0133 .8054±.0213 .8060±.0172 .8017±.0187 .7423±.0335 .7303±.0235 .8338±.0132 .7826±.0377 .7454±.0425 .8363±.0110 .8273±.0106 .8148±.0141 NeuralSparse .8190±.0170 .7318±.0379 .7293±.0393 .8534±.0250 .7794±.0285 .7637±.0529 .8524±.0094 .8246±.0111 .8211±.0106 .8047±.0460 .7559±.0246 .7388±.0216 .8433±.0443 .8214±.0122 .7770±.0624 .8460±.0128 .8214±.0166 .8138±.0151 Co-teaching .8197±.0236 .7479±.0372 .7030±.0475 .8045±.0609 .7871±.0564 .7530±.0500 .8446±.0219 .8209±.0481 .8157±.0246 .8185±.0226 .7468±.0388 .7018±.0473 .8051±.0664 .7866±.0623 .7517±.0492 .8325±.0201 .8036±.0227 .8069±.0193 .8143±.0218 .7498±.0418 .7024±.0403 .8200±.0772 .7838±.0558 .7617±.0546 .8289±.0177 .8064±.0229 .8148±.0227 .8198±.0331 .7485±.0518 .7148±.0455 .8002±.0607 .8099±.0566 .7741±.0584 .8337±.0133 .8137±.0279 .8157±.0270 .8240±.0147 .7819±.0261 .7490±.0230 .8349±.0124 .8301±.0218 .8025±.0210 .8491±.0120 .8275±.0115 .8256±.0108 .7872±.0207 .6940±.0248 .6929±.0140 .7877±.0211 .7107±.0188 .6975±.0124 .8531±.0166 .8237±.0252 .8193±.0246 RGIB-REP .8313±.0098 .7966±.0110 .7591±.0142 .8624±.0071 .8313±.0136 .8158±.0193 .8554±.0149 .8318±.0151 .8297±.0150 RGIB-SSL .8930±.0072 .8554±.0167 .8339±.0100 .9024±.0097 .8577±.0152 .8421±.0156 .9314±.0066 .9224±.0100 .9241±.0049\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.7940±.0133 .7429±.0226 .7177±.0207 .8035±.0370 .7973±.0166 .7546±.0270 .8249±.0112 .7925±.0137 .7901±.0145 .7941±.0172 .7353±.0130 .6909±.0208 .7900±.0560 .7488±.0187 .7170±.0187 .8288±.0148 .8187±.0076 .7990±.0513 NeuralSparse .7931±.0201 .7324±.0434 .7018±.0330 .8082±.0418 .7432±.0386 .7393±.0243 .8415±.0167 .8160±.0181 .8081±.0105 .8037±.0168 .7601±.0225 .7167±.0574 .8129±.0158 .7873±.0199 .7570±.0511 .8348±.0144 .7933±.0538 .7899±.0466 Co-teaching .7930±.0132 .7387±.0244 .7151±.0262 .8079±.0469 .8058±.0170 .7623±.0310 .8298±.0365 .8020±.0172 .7976±.0375 .8015±.0211 .7466±.0320 .7196±.0286 .8068±.0361 .8031±.0169 .7607±.0272 .8409±.0146 .7999±.0289 .7918±.0240 .8002±.0159 .7492±.0246 .7211±.0250 .8199±.0368 .8041±.0204 .7690±.0364 .8277±.0138 .8005±.0229 .7904±.0230 .7961±.0163 .7584±.0313 .7201±.0300 .8036±.0451 .8067±.0331 .7639±.0402 .8347±.0111 .8016±.0156 .7947±.0147 .8092±.0242 .7365±.0227 .6920±.0415 .8021±.0251 .7845±.0280 .7434±.0257 .8273±.0202 .8181±.0234 .8157±.0135 .7576±.0148 .7187±.0243 .6860±.0213 .7693±.0161 .7171±.0223 .6886±.0272 .8209±.0347 .8134±.0310 .8102±.0234 RGIB-REP .8103±.0137 .7439±.0221 .7040±.0192 .8282±.0123 .7857±.0142 .7623±.0144 .8365±.0163 .8247±.0142 .8240±.0119 RGIB-SSL .8623±.0126 .8080±.0240 .7357±.0342 .8632±.0187 .7878±.0368 .7310±.0483 .9184±.0070 .9120±.0108 .9126±.0081\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 17: Full results on Cora dataset with GAT.\n\nGAT\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.8786±.0155 .8489±.0131 .8286±.0120 .9014±.0070 .8825±.0097 .8880±.0077 .8991±.0088 .8841±.0097 .8830±.0101 .8741±.0114 .8279±.0172 .8101±.0141 .8930±.0056 .8720±.0063 .8586±.0110 .8954±.0093 .8788±.0083 .8717±.0106 NeuralSparse .8820±.0134 .8447±.0151 .8248±.0112 .9051±.0075 .8884±.0121 .8828±.0067 .8982±.0060 .8855±.0110 .8761±.0129 .8799±.0152 .8487±.0101 .8314±.0075 .9023±.0105 .8838±.0116 .8827±.0090 .9019±.0086 .8772±.0082 .8726±.0060 Co-teaching .8883±.0148 .8571±.0192 .8378±.0169 .9069±.0134 .8896±.0173 .8939±.0125 .9289±.0281 .9100±.0358 .8969±.0243 .8867±.0189 .8562±.0206 .8343±.0140 .9026±.0135 .8908±.0169 .8937±.0073 .9057±.0212 .8848±.0194 .8918±.0217 .8809±.0180 .8492±.0161 .8334±.0124 .9066±.0142 .8823±.0081 .9065±.0215 .9010±.0092 .8919±.0186 .8929±.0114 .8826±.0192 .8564±.0218 .8375±.0294 .9260±.0330 .9092±.0361 .9162±.0213 .9007±.0128 .8928±.0195 .8915±.0098 .8709±.0119 .8462±.0121 .8300±.0132 .8957±.0115 .8827±.0131 .8805±.0116 .8881±.0065 .8730±.0116 .8652±.0143 .8286±.0224 .7564±.0229 .7328±.0213 .8238±.0215 .7615±.0379 .7309±.0297 .8833±.0100 .8805±.0125 .8807±.0119 RGIB-REP .8759±.0132 .8374±.0104 .8269±.0134 .9006±.0118 .8833±.0079 .8798±.0131 .8993±.0086 .8838±.0115 .8810±.0125 RGIB-SSL .9142±.0092 .8878±.0135 .8777±.0118 .9234±.0053 .8973±.0067 .8866±.0148 .9389±.0053 .9347±.0088 .9311±.0057\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8323±.0181 .8005±.0212 .7841±.0095 .8616±.0107 .8454±.0118 .8377±.0146 .8530±.0121 .8357±.0082 .8281±.0114 .8237±.0157 .7782±.0072 .7515±.0050 .8548±.0144 .8205±.0090 .8000±.0151 .8516±.0100 .8373±.0142 .8374±.0157 NeuralSparse .8309±.0154 .7954±.0118 .7769±.0135 .8575±.0174 .8450±.0110 .8277±.0148 .8503±.0123 .8395±.0122 .8348±.0121 .8364±.0147 .8045±.0096 .7890±.0081 .8669±.0132 .8445±.0155 .8331±.0146 .8507±.0113 .8399±.0096 .8370±.0117 Co-teaching .8294±.0222 .8001±.0300 .7895±.0171 .8696±.0196 .8344±.0163 .8423±.0180 .8534±.0145 .8374±.0313 .8544±.0169 .8344±.0276 .8067±.0254 .7933±.0142 .8520±.0202 .8353±.0145 .8376±.0216 .8533±.0121 .8527±.0075 .8444±.0203 .8319±.0237 .8001±.0235 .7932±.0107 .8634±.0130 .8449±.0272 .8406±.0141 .8537±.0171 .8402±.0134 .8354±.0152 .8352±.0367 .8111±.0244 .7945±.0279 .8860±.0099 .8579±.0163 .8493±.0404 .8604±.0166 .8434±.0094 .8340±.0130 .8324±.0102 .8033±.0099 .7776±.0145 .8620±.0162 .8441±.0098 .8337±.0386 .8514±.0138 .8381±.0092 .8318±.0121 .7403±.0347 .6711±.0695 .6656±.0578 .7707±.0267 .7154±.0366 .7146±.0237 .8040±.0447 .7988±.0292 .8321±.0230 RGIB-REP .8274±.0153 .7882±.0134 .7552±.0657 .8652±.0138 .8370±.0118 .8154±.0147 .8480±.0181 .8332±.0129 .8259±.0163 RGIB-SSL .8760±.0112 .8469±.0101 .8304±.0176 .8865±.0125 .8553±.0127 .8349±.0130 .9163±.0090 .9075±.0087 .9036±.0087\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8067±.0302 .7514±.0389 .7032±.0452 .8414±.0127 .7891±.0537 .7678±.0641 .8376±.0097 .8154±.0069 .8024±.0169 .8051±.0111 .7375±.0354 .7110±.0388 .8198±.0132 .7514±.0615 .7248±.0511 .8499±.0135 .8312±.0163 .8112±.0079 NeuralSparse .8169±.0130 .7726±.0165 .7149±.0596 .8443±.0183 .7997±.0149 .7273±.0592 .8460±.0113 .8257±.0123 .8149±.0224 .8207±.0166 .7460±.0714 .7145±.0621 .8253±.0514 .8209±.0231 .7759±.0339 .8464±.0106 .8234±.0168 .8159±.0148 Co-teaching .8059±.0312 .7576±.0386 .7070±.0455 .8496±.0165 .7969±.0542 .7717±.0691 .8549±.0298 .8204±.0284 .8215±.0369 .8133±.0336 .7572±.0411 .7070±.0521 .8455±.0214 .7938±.0547 .7679±.0737 .8405±.0229 .8318±.0197 .8049±.0224 .8155±.0400 .7537±.0466 .7123±.0454 .8495±.0142 .7947±.0561 .7762±.0771 .8374±.0160 .8245±.0067 .8108±.0180 .8188±.0386 .7509±.0439 .7014±.0442 .8452±.0392 .8039±.0706 .7923±.0718 .8366±.0137 .8220±.0108 .8090±.0222 .7586±.0629 .6434±.0457 .6115±.0607 .7535±.0856 .7102±.0655 .6241±.0433 .8088±.0568 .8040±.0384 .7869±.0392 .5748±.0659 .5949±.0650 .5611±.0608 .5675±.0788 .6125±.0782 .5537±.0607 .5632±.0583 .5588±.0692 .6176±.0961 RGIB-REP .8148±.0158 .7553±.0179 .6842±.0264 .8404±.0129 .8001±.0178 .7433±.0663 .8366±.0107 .8274±.0151 .8192±.0179 RGIB-SSL .8613±.0107 .8194±.0158 .7858±.0133 .8657±.0118 .8213±.0189 .8045±.0153 .9049±.0059 .8960±.0141 .8985±.0117\n\nPeer loss Jaccard GIB SupCon GRACE\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nTable 18: Full results on Cora dataset with SAGE.\n\nSAGE\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.8733±.0101 .8520±.0137 .8469±.0056 .9006±.0074 .8857±.0093 .8917±.0063 .8868±.0070 .8695±.0106 .8564±.0091 .8944±.0105 .8600±.0083 .8478±.0095 .9013±.0080 .9026±.0086 .9050±.0084 .8985±.0079 .8725±.0107 .8699±.0091 NeuralSparse .8821±.0123 .8570±.0090 .8491±.0105 .9020±.0090 .8880±.0087 .8949±.0098 .8885±.0111 .8681±.0110 .8638±.0075 .8860±.0109 .8536±.0101 .8474±.0099 .9040±.0101 .8925±.0085 .8947±.0070 .8902±.0096 .8721±.0068 .8586±.0075 Co-teaching .8794±.0097 .8569±.0141 .8557±.0080 .8910±.0099 .8891±.0100 .8935±.0123 .9071±.0216 .8759±.0291 .8632±.0156 .8817±.0102 .8559±.0132 .8480±.0058 .9042±.0099 .8866±.0174 .8960±.0064 .8876±.0111 .8768±.0251 .8739±.0149 .8828±.0170 .8513±.0185 .8474±.0081 .9080±.0209 .8975±.0210 .8897±.0045 .8922±.0112 .8695±.0183 .8596±.0133 .8765±.0196 .8679±.0125 .8546±.0246 .9089±.0232 .9004±.0098 .8998±.0347 .8938±.0082 .8727±.0101 .8614±.0125 .8844±.0117 .8507±.0135 .8499±.0076 .8974±.0075 .8904±.0076 .8943±.0122 .8916±.0094 .8742±.0092 .8601±.0093 .8123±.0113 .7978±.0090 .7944±.0137 .8082±.0199 .8010±.0160 .7962±.0120 .8280±.0092 .8228±.0170 .8261±.0102 RGIB-REP .8748±.0094 .8484±.0149 .8380±.0102 .9016±.0074 .8876±.0075 .8914±.0093 .8863±.0078 .8628±.0089 .8449±.0077 RGIB-SSL .9102±.0074 .8967±.0114 .8993±.0076 .9196±.0072 .9059±.0090 .9082±.0084 .9278±.0073 .9174±.0089 .9163±.0076\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8225±.0079 .7833±.0093 .7595±.0259 .8607±.0110 .8437±.0099 .8387±.0183 .8309±.0090 .8046±.0074 .7920±.0191 .8323±.0146 .7944±.0130 .7842±.0149 .8734±.0082 .8635±.0120 .8628±.0146 .8466±.0111 .8199±.0155 .8125±.0059 NeuralSparse .8292±.0157 .7930±.0108 .7573±.0198 .8703±.0108 .8549±.0145 .8596±.0061 .8418±.0148 .8025±.0148 .8069±.0106 .8310±.0149 .7847±.0174 .7690±.0177 .8626±.0141 .8607±.0090 .8630±.0154 .8435±.0112 .8125±.0117 .7961±.0231 Co-teaching .8237±.0139 .7855±.0103 .7658±.0343 .8660±.0168 .8533±.0149 .8424±.0196 .8580±.0345 .8117±.0187 .8088±.0275 .8230±.0107 .7863±.0174 .7626±.0294 .8669±.0175 .8534±.0179 .8451±.0234 .8363±.0142 .8124±.0191 .7904±.0231 .8261±.0153 .7850±.0115 .7602±.0348 .8619±.0253 .8465±.0227 .8496±.0233 .8378±.0106 .8046±.0084 .7976±.0288 .8286±.0059 .7947±.0240 .7729±.0378 .8844±.0280 .8437±.0158 .8574±.0207 .8374±.0185 .8052±.0103 .7947±.0229 .8295±.0143 .7809±.0176 .7383±.0218 .8568±.0115 .8450±.0153 .8445±.0187 .8426±.0105 .8150±.0170 .7943±.0129 .6242±.0245 .6424±.0290 .6711±.0452 .6465±.0381 .6172±.0320 .6496±.0544 .6434±.0384 .6376±.0251 .6438±.0449 RGIB-REP .8274±.0112 .7822±.0143 .7692±.0202 .8634±.0121 .8470±.0144 .8528±.0131 .8367±.0149 .8087±.0187 .7991±.0120 RGIB-SSL .8837±.0065 .8728±.0116 .8613±.0148 .8960±.0109 .8817±.0119 .8825±.0113 .9130±.0038 .9041±.0075 .9023±.0072\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.7787±.0423 .7420±.0251 .7180±.0248 .8256±.0222 .7947±.0561 .8005±.0421 .8158±.0168 .7707±.0235 .7660±.0134 .8035±.0228 .7398±.0560 .7176±.0389 .8262±.0153 .8193±.0679 .8089±.0260 .8340±.0161 .7993±.0091 .7897±.0144 NeuralSparse .7953±.0177 .7378±.0180 .7292±.0238 .8384±.0120 .8234±.0288 .7980±.0701 .8214±.0107 .7908±.0136 .7622±.0160 .7999±.0151 .7604±.0169 .7352±.0202 .8311±.0143 .8267±.0078 .8109±.0140 .8222±.0121 .7823±.0078 .7745±.0231 Co-teaching .7817±.0477 .7445±.0312 .7212±.0332 .8306±.0256 .7991±.0595 .8007±.0439 .8324±.0256 .7720±.0263 .7687±.0266 .7781±.0451 .7445±.0286 .7192±.0277 .8300±.0234 .8020±.0624 .8043±.0449 .8309±.0149 .7734±.0388 .7652±.0262 .7779±.0437 .7493±.0245 .7277±.0238 .8333±.0323 .8075±.0605 .8037±.0494 .8148±.0186 .7707±.0243 .7709±.0204 .7814±.0493 .7473±.0442 .7349±.0437 .8366±.0194 .8106±.0689 .8040±.0617 .8172±.0258 .7806±.0265 .7689±.0180 .7879±.0356 .7019±.0285 .6673±.0317 .8219±.0469 .7648±.0666 .7159±.0717 .8242±.0159 .7880±.0152 .7686±.0148 .6866±.0160 .6437±.0455 .5967±.0248 .6949±.0181 .6536±.0365 .6114±.0394 .7239±.0231 .7035±.0160 .7014±.0111 RGIB-REP .8049±.0146 .7157±.0725 .7099±.0473 .8391±.0215 .8149±.0234 .7927±.0171 .8358±.0100 .7974±.0140 .8046±.0135 RGIB-SSL .8662±.0130 .8430±.0178 .8306±.0108 .8746±.0091 .8634±.0099 .8603±.0156 .8982±.0089 .8930±.0108 .8940±.0076\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 19: Full results on Citeseer dataset with GCN.\n\nGCN\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.8651±.0084 .8355±.0154 .8254±.0066 .8767±.0097 .8615±.0090 .8585±.0085 .8834±.0155 .8649±.0104 .8674±.0122 .8613±.0112 .8317±.0168 .8112±.0158 .8755±.0117 .8557±.0122 .8483±.0164 .8862±.0084 .8695±.0133 .8688±.0143 NeuralSparse .8605±.0119 .8402±.0138 .8239±.0069 .8801±.0085 .8634±.0109 .8614±.0134 .8827±.0094 .8753±.0111 .8720±.0156 .8646±.0155 .8404±.0111 .8223±.0142 .8805±.0095 .8647±.0112 .8571±.0116 .8813±.0074 .8734±.0126 .8708±.0097 Co-teaching .8689±.0100 .8416±.0215 .8278±.0061 .8849±.0167 .8659±.0180 .8659±.0090 .8833±.0231 .8744±.0281 .8847±.0094 .8728±.0159 .8374±.0182 .8308±.0062 .8783±.0089 .8665±.0152 .8638±.0131 .8909±.0144 .8653±.0276 .8808±.0212 .8682±.0164 .8406±.0230 .8292±.0137 .8725±.0090 .8743±.0186 .8696±.0158 .8849±.0231 .8718±.0105 .8680±.0154 .8825±.0119 .8476±.0331 .8306±.0156 .8819±.0381 .8660±.0318 .8688±.0136 .8914±.0253 .8681±.0128 .8707±.0144 .8344±.0101 .8173±.0155 .8140±.0121 .8501±.0100 .8325±.0129 .8387±.0186 .8436±.0128 .8337±.0139 .8269±.0131 .8450±.0123 .8225±.0169 .7898±.0081 .8529±.0074 .8209±.0137 .7967±.0171 .8697±.0110 .8646±.0122 .8699±.0156 RGIB-REP .8585±.0088 .8347±.0213 .8167±.0113 .8751±.0074 .8637±.0122 .8600±.0119 .8795±.0101 .8638±.0175 .8644±.0095 RGIB-SSL .9199±.0091 .8957±.0099 .8759±.0087 .9271±.0055 .9018±.0090 .8942±.0145 .9495±.0059 .9515±.0049 .9480±.0062\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.7864±.0145 .7380±.0201 .7085±.0146 .8054±.0240 .7708±.0325 .7583±.0258 .7965±.0124 .7850±.0174 .7659±.0240 .7635±.0106 .7393±.0170 .7094±.0190 .8025±.0106 .7730±.0147 .7473±.0161 .7937±.0175 .7853±.0109 .7632±.0142 NeuralSparse .7765±.0123 .7397±.0168 .7148±.0237 .8093±.0129 .7809±.0160 .7468±.0289 .7968±.0198 .7921±.0129 .7752±.0209 .7795±.0131 .7423±.0200 .7283±.0130 .8119±.0103 .7811±.0191 .7638±.0167 .7968±.0141 .7765±.0124 .7622±.0215 Co-teaching .7533±.0181 .7238±.0245 .7131±.0157 .8059±.0263 .7753±.0408 .7668±.0272 .7974±.0381 .7877±.0291 .7913±.0343 .7423±.0215 .7345±.0213 .7104±.0242 .8106±.0250 .7767±.0400 .7653±.0315 .7991±.0241 .7990±.0289 .7751±.0245 .7473±.0160 .7324±.0204 .7107±.0172 .8176±.0283 .7776±.0471 .7725±.0432 .8061±.0187 .7887±.0196 .7689±.0310 .7509±.0336 .7388±.0240 .7121±.0210 .8070±.0398 .7717±.0612 .7798±.0421 .7986±.0120 .7852±.0186 .7649±.0275 .7554±.0196 .7458±.0176 .7299±.0218 .8076±.0099 .7767±.0111 .7655±.0164 .8024±.0108 .7983±.0123 .7807±.0166 .7632±.0224 .7242±.0219 .6844±.0226 .7615±.0152 .7151±.0193 .6830±.0232 .7909±.0211 .7630±.0196 .7737±.0307 RGIB-REP .7875±.0131 .7519±.0181 .7312±.0227 .8299±.0134 .7996±.0130 .7771±.0178 .8083±.0152 .7846±.0234 .7945±.0203 RGIB-SSL .8694±.0108 .8427±.0174 .8137±.0170 .8747±.0144 .8461±.0109 .8245±.0142 .9204±.0085 .9218±.0098 .9250±.0071\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.7355±.0387 .7110±.0222 .7106±.0168 .7720±.0161 .7460±.0401 .7212±.0470 .7900±.0131 .7741±.0157 .7648±.0229 .7653±.0139 .7191±.0159 .7006±.0304 .7645±.0441 .7442±.0160 .7209±.0182 .8007±.0152 .7823±.0141 .7656±.0196 NeuralSparse .7644±.0180 .7361±.0191 .7036±.0387 .7913±.0152 .7476±.0279 .7391±.0235 .7793±.0391 .7739±.0188 .7688±.0256 .7661±.0153 .7401±.0134 .7072±.0385 .7882±.0134 .7728±.0164 .7464±.0188 .7913±.0122 .7742±.0155 .7582±.0151 Co-teaching .7375±.0468 .7171±.0299 .7188±.0248 .7813±.0185 .7550±.0475 .7310±.0496 .8034±.0117 .7885±.0275 .7701±.0245 .7398±.0387 .7105±.0288 .7166±.0250 .7783±.0158 .7532±.0488 .7301±.0509 .7915±.0169 .7748±.0299 .7695±.0239 .7415±.0444 .7162±.0307 .7135±.0171 .7793±.0263 .7459±.0416 .7369±.0473 .7925±.0176 .7825±.0245 .7710±.0298 .7370±.0531 .7226±.0268 .7175±.0234 .7743±.0435 .7722±.0697 .7276±.0534 .7948±.0178 .7803±.0221 .7681±.0251 .7714±.0122 .7413±.0166 .7205±.0231 .7907±.0161 .7842±.0109 .7670±.0183 .7745±.0231 .7659±.0114 .7573±.0180 .6995±.0221 .6901±.0210 .6946±.0152 .7041±.0240 .7037±.0247 .6818±.0169 .7752±.0230 .7739±.0151 .7773±.0351 RGIB-REP .7725±.0077 .7429±.0202 .7232±.0110 .7781±.0218 .7559±.0130 .7415±.0221 .7900±.0252 .7797±.0181 .7886±.0194 RGIB-SSL .8417±.0169 .7995±.0148 .7673±.0137 .8379±.0065 .8026±.0173 .7793±.0207 .8984±.0144 .9062±.0066 .9060±.0098\n\nPeer loss Jaccard GIB SupCon GRACE\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nTable 20: Full results on Citeseer dataset with GAT.\n\nGAT\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.8586±.0125 .8338±.0127 .8207±.0121 .8689±.0096 .8526±.0130 .8512±.0174 .8762±.0076 .8650±.0102 .8648±.0166 .8566±.0113 .8333±.0183 .8100±.0098 .8750±.0079 .8496±.0101 .8512±.0121 .8820±.0086 .8679±.0112 .8673±.0114 NeuralSparse .8573±.0101 .8431±.0151 .8222±.0092 .8743±.0117 .8577±.0067 .8580±.0135 .8826±.0080 .8724±.0076 .8657±.0089 .8602±.0107 .8381±.0137 .8157±.0075 .8755±.0090 .8560±.0084 .8574±.0154 .8784±.0120 .8693±.0098 .8669±.0142 Co-teaching .8628±.0220 .8366±.0124 .8199±.0194 .8720±.0128 .8521±.0139 .8510±.0224 .8924±.0122 .8888±.0365 .8919±.0305 .8637±.0125 .8378±.0170 .8235±.0120 .8721±.0172 .8529±.0173 .8559±.0216 .8878±.0185 .8653±.0288 .8631±.0258 .8615±.0197 .8379±.0222 .8223±.0124 .8841±.0079 .8556±.0119 .8498±.0309 .8843±.0143 .8676±.0195 .8661±.0256 .8610±.0230 .8462±.0114 .8324±.0316 .8909±.0091 .8823±.0188 .8488±.0276 .8781±.0135 .8739±.0144 .8741±.0156 .8495±.0100 .8138±.0174 .8155±.0099 .8611±.0086 .8454±.0111 .8393±.0172 .8558±.0137 .8459±.0170 .8379±.0185 .8092±.0221 .7564±.0264 .7479±.0278 .8014±.0370 .7628±.0240 .7433±.0245 .8788±.0146 .8768±.0073 .8654±.0172 RGIB-REP .8545±.0108 .8310±.0127 .8137±.0091 .8736±.0107 .8566±.0097 .8503±.0159 .8778±.0093 .8696±.0081 .8614±.0084 RGIB-SSL .9106±.0102 .8829±.0058 .8677±.0095 .9172±.0072 .8909±.0086 .8785±.0121 .9419±.0071 .9410±.0047 .9410±.0090\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8026±.0157 .7775±.0248 .7518±.0183 .8191±.0092 .8043±.0105 .7912±.0073 .8174±.0172 .7998±.0143 .7934±.0156 .8063±.0079 .7624±.0211 .7434±.0124 .8171±.0132 .7977±.0178 .7814±.0162 .8262±.0148 .8103±.0178 .8057±.0148 NeuralSparse .7958±.0142 .7761±.0172 .7550±.0129 .8282±.0130 .8088±.0088 .7911±.0174 .8259±.0119 .8135±.0092 .7986±.0109 .8000±.0113 .7734±.0198 .7597±.0185 .8254±.0105 .8132±.0089 .7950±.0143 .8137±.0243 .8082±.0094 .8036±.0139 Co-teaching .8016±.0184 .7807±.0315 .7521±.0267 .8213±.0173 .8068±.0156 .7903±.0105 .8402±.0220 .8109±.0316 .7947±.0350 .8064±.0178 .7802±.0253 .7544±.0191 .8246±.0145 .8108±.0122 .7945±.0113 .8160±.0329 .8045±.0185 .7925±.0207 .8098±.0222 .7771±.0273 .7517±.0186 .8258±.0124 .8083±.0138 .7901±.0073 .8206±.0168 .8036±.0176 .7999±.0215 .8170±.0230 .7884±.0341 .7645±.0247 .8422±.0365 .8112±.0212 .7972±.0305 .8192±.0249 .8080±.0155 .8010±.0177 .7940±.0114 .7728±.0125 .7478±.0145 .8137±.0115 .8003±.0116 .7777±.0409 .8038±.0114 .7972±.0198 .7852±.0201 .7319±.0433 .6611±.0395 .6449±.0579 .7216±.0261 .5947±.0660 .6060±.0507 .7775±.1040 .7739±.0475 .7882±.0328 RGIB-REP .7991±.0107 .7743±.0164 .7418±.0121 .8155±.0156 .7905±.0157 .7372±.0908 .8108±.0118 .7946±.0180 .7935±.0131 RGIB-SSL .8520±.0145 .8306±.0149 .8029±.0098 .8592±.0120 .8251±.0132 .8145±.0110 .9084±.0091 .9101±.0076 .9102±.0117\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.7807±.0117 .7373±.0270 .7139±.0251 .7970±.0134 .7860±.0107 .7741±.0126 .7963±.0129 .7883±.0162 .7801±.0161 .7768±.0088 .7477±.0195 .7116±.0119 .7854±.0232 .7640±.0188 .7425±.0362 .8114±.0132 .7840±.0217 .7826±.0186 NeuralSparse .7704±.0099 .7462±.0170 .7242±.0138 .8047±.0101 .7647±.0372 .7248±.0596 .8087±.0235 .7855±.0176 .7880±.0148 .7805±.0193 .7503±.0223 .7286±.0237 .7927±.0287 .7822±.0132 .7579±.0355 .8002±.0085 .7977±.0134 .7890±.0145 Co-teaching .7819±.0141 .7399±.0335 .7236±.0292 .7964±.0189 .7809±.0183 .7740±.0185 .7933±.0406 .7918±.0348 .7979±.0245 .7846±.0214 .7459±.0294 .7187±.0259 .7979±.0172 .7955±.0168 .7796±.0218 .7957±.0273 .7865±.0285 .7912±.0148 .7902±.0117 .7365±.0332 .7157±.0248 .8056±.0264 .8038±.0226 .7733±.0245 .7964±.0226 .7936±.0255 .7847±.0218 .7818±.0230 .7378±.0285 .7137±.0416 .8161±.0267 .7995±.0183 .7762±.0176 .8002±.0155 .7955±.0166 .7794±.0244 .7370±.0524 .7160±.0462 .6670±.0442 .7667±.0402 .7729±.0356 .6999±.0597 .7810±.0219 .7752±.0119 .7591±.0362 .5068±.0128 .5034±.0106 .5108±.0319 .5058±.0096 .4956±.0069 .5379±.0427 .5181±.0547 .5288±.0467 .5068±.0178 RGIB-REP .7817±.0129 .7062±.0681 .7254±.0188 .7883±.0160 .7769±.0168 .7620±.0176 .7981±.0092 .7711±.0487 .7817±.0164 RGIB-SSL .8275±.0148 .7989±.0136 .7681±.0140 .8261±.0096 .8024±.0087 .7806±.0174 .8855±.0103 .8918±.0143 .8940±.0119\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 21: Full results on Citeseer dataset with SAGE.\n\nSAGE\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.8438±.0143 .8404±.0113 .8279±.0098 .8644±.0109 .8608±.0064 .8647±.0161 .8560±.0120 .8533±.0112 .8412±.0122 .8654±.0118 .8593±.0129 .8503±.0125 .8834±.0059 .8791±.0108 .8806±.0101 .8809±.0085 .8734±.0064 .8710±.0170 NeuralSparse .8658±.0079 .8478±.0109 .8437±.0069 .8746±.0102 .8674±.0086 .8722±.0183 .8670±.0120 .8645±.0136 .8485±.0105 .8620±.0125 .8488±.0113 .8477±.0078 .8740±.0094 .8675±.0120 .8702±.0134 .8657±.0138 .8576±.0147 .8530±.0073 Co-teaching .8505±.0143 .8436±.0179 .8323±.0144 .8652±.0133 .8606±.0067 .8650±.0247 .8787±.0269 .8660±.0101 .8392±.0363 .8525±.0157 .8440±.0120 .8319±.0186 .8661±.0199 .8693±.0113 .8735±.0151 .8563±.0278 .8535±.0107 .8563±.0241 .8514±.0208 .8492±.0143 .8291±.0183 .8691±.0282 .8805±.0205 .8715±.0152 .8616±.0120 .8629±.0210 .8413±.0167 .8574±.0192 .8577±.0134 .8323±.0125 .8693±.0186 .8632±.0310 .8739±.0405 .8600±.0197 .8576±.0182 .8403±.0157 .8344±.0106 .8241±.0123 .8168±.0111 .8485±.0133 .8493±.0147 .8484±.0161 .8487±.0109 .8394±.0096 .8323±.0113 .8283±.0295 .8319±.0192 .8253±.0208 .8361±.0190 .8273±.0222 .8351±.0134 .8434±.0193 .8422±.0119 .8400±.0233 RGIB-REP .8514±.0120 .8359±.0093 .8213±.0112 .8614±.0118 .8537±.0105 .8664±.0153 .8592±.0111 .8533±.0134 .8455±.0123 RGIB-SSL .9003±.0104 .8894±.0119 .8916±.0078 .9045±.0076 .8945±.0097 .8992±.0141 .9143±.0082 .9075±.0080 .9087±.0108\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.7914±.0101 .7686±.0213 .7539±.0149 .8171±.0173 .8226±.0141 .8157±.0203 .8068±.0116 .7891±.0136 .7705±.0172 .7889±.0182 .7850±.0117 .7678±.0206 .8300±.0130 .8032±.0953 .8271±.0115 .8058±.0118 .7899±.0173 .7826±.0282 NeuralSparse .7934±.0115 .7746±.0195 .7639±.0175 .8331±.0119 .8242±.0176 .8324±.0144 .8111±.0146 .7904±.0229 .7747±.0242 .7972±.0105 .7804±.0201 .7563±.0230 .8299±.0177 .8259±.0119 .8374±.0110 .8121±.0168 .7892±.0103 .7845±.0166 Co-teaching .7928±.0189 .7679±.0230 .7557±.0214 .8226±.0172 .8297±.0202 .8256±.0263 .8233±.0203 .8083±.0218 .7836±.0347 .7913±.0116 .7710±.0307 .7556±.0238 .8197±.0264 .8321±.0155 .8239±.0275 .8135±.0131 .7940±.0194 .7885±.0195 .7904±.0108 .7691±.0269 .7626±.0183 .8359±.0251 .8244±.0189 .8290±.0264 .8167±.0119 .7972±.0157 .7744±.0227 .7931±.0134 .7739±.0254 .7691±.0291 .8199±.0278 .8306±.0209 .8217±.0273 .8151±.0152 .7947±.0170 .7737±.0266 .7870±.0130 .7672±.0145 .7641±.0125 .8191±.0197 .8081±.0138 .8036±.0211 .7982±.0137 .7898±.0121 .7783±.0172 .6196±.0253 .6404±.0258 .6308±.0323 .6286±.0287 .6365±.0223 .6198±.0157 .6495±.0202 .6431±.0210 .6411±.0191 RGIB-REP .7854±.0123 .7703±.0123 .7562±.0200 .8195±.0140 .8151±.0118 .8134±.0221 .8038±.0158 .7863±.0140 .7799±.0212 RGIB-SSL .8545±.0157 .8482±.0147 .8352±.0127 .8706±.0074 .8525±.0118 .8564±.0141 .8867±.0088 .8866±.0161 .8903±.0124\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.7708±.0168 .7223±.0614 .7204±.0236 .8086±.0145 .7997±.0183 .7872±.0145 .7903±.0219 .7690±.0201 .7564±.0096 .7756±.0143 .7485±.0104 .7290±.0261 .8070±.0125 .8060±.0165 .8026±.0195 .7861±.0174 .7769±.0149 .7534±.0212 NeuralSparse .7757±.0175 .7564±.0205 .7306±.0173 .8039±.0144 .8008±.0157 .8016±.0223 .7902±.0189 .7762±.0177 .7588±.0160 .7844±.0116 .7525±.0270 .7435±.0108 .8070±.0130 .8128±.0108 .8085±.0135 .7948±.0165 .7830±.0182 .7634±.0210 Co-teaching .7792±.0246 .7248±.0642 .7268±.0297 .8087±.0136 .8076±.0234 .7864±.0147 .8011±.0394 .7671±.0406 .7626±.0232 .7796±.0158 .7213±.0628 .7294±.0233 .8135±.0225 .8012±.0206 .7914±.0208 .8005±.0409 .7710±.0316 .7672±.0098 .7800±.0257 .7267±.0606 .7248±.0330 .8166±.0328 .8058±.0177 .7980±.0250 .7906±.0274 .7749±.0269 .7617±.0147 .7823±.0358 .7352±.0808 .7247±.0332 .8180±.0388 .8073±.0241 .8049±.0378 .7967±.0223 .7710±.0267 .7576±.0097 .7649±.0176 .7193±.0426 .7040±.0231 .7864±.0187 .7802±.0219 .7529±.0419 .7775±.0174 .7715±.0206 .7517±.0235 .6608±.0371 .6767±.0441 .6433±.0494 .6591±.0344 .6660±.0366 .6670±.0292 .6966±.0367 .6678±.0682 .6979±.0476 RGIB-REP .7766±.0070 .7479±.0150 .7427±.0127 .7981±.0104 .7939±.0149 .7894±.0238 .7959±.0137 .7865±.0163 .7695±.0169 RGIB-SSL .8372±.0169 .8226±.0189 .8184±.0135 .8441±.0078 .8252±.0129 .8288±.0114 .8795±.0112 .8802±.0100 .8749±.0178\n\nPeer loss Jaccard GIB SupCon GRACE\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nTable 22: Full results on Pubmed dataset with GCN.\n\nGCN\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9473±.0019 .9271±.0027 .9141±.0041 .9590±.0022 .9468±.0022 .9337±.0016 .9646±.0024 .9637±.0021 .9597±.0022 .9394±.0025 .9155±.0027 .8994±.0036 .9467±.0022 .9302±.0021 .9146±.0022 .9594±.0026 .9558±.0027 .9519±.0019 NeuralSparse .9479±.0021 .9251±.0039 .9120±.0029 .9558±.0019 .9315±.0033 .9269±.0014 .9654±.0015 .9525±.0023 .9588±.0017 .9467±.0018 .9264±.0025 .9111±.0032 .9554±.0026 .9320±.0035 .9272±.0027 .9651±.0018 .9616±.0021 .9584±.0021 Co-teaching .9502±.0085 .9335±.0104 .9160±.0096 .9510±.0098 .9331±.0054 .9255±.0113 .9676±.0018 .9560±.0127 .9608±.0098 .9500±.0034 .9339±.0044 .9140±.0138 .9558±.0068 .9397±.0019 .9283±.0090 .9615±.0175 .9521±.0178 .9545±.0144 .9496±.0039 .9325±.0087 .9235±.0081 .9554±.0019 .9327±.0067 .9230±.0159 .9580±.0032 .9532±.0086 .9591±.0054 .9509±.0205 .9268±.0074 .9131±.0066 .9599±.0303 .9303±.0047 .9228±.0234 .9559±.0076 .9573±.0040 .9597±.0099 .9345±.0020 .9257±.0017 .9118±.0031 .9583±.0015 .9345±.0030 .9214±.0023 .9625±.0021 .9522±.0018 .9506±.0010 .9341±.0032 .9319±.0027 .9154±.0049 .9409±.0046 .9321±.0032 .9225±.0078 .9489±.0034 .9516±.0027 .9511±.0022 RGIB-REP .9537±.0013 .9368±.0018 .9270±.0036 .9579±.0022 .9467±.0023 .9365±.0016 .9696±.0033 .9690±.0023 .9671±.0015 RGIB-SSL .9585±.0022 .9471±.0032 .9301±.0021 .9425±.0088 .9305±.0098 .9121±.0125 .9719±.0020 .9724±.0015 .9711±.0012\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8870±.0041 .8748±.0031 .8641±.0041 .8854±.0051 .8759±.0031 .8651±.0040 .9030±.0082 .9039±.0029 .9070±.0062 .8711±.0149 .8482±.0045 .8354±.0062 .8682±.0158 .8456±.0036 .8376±.0046 .9313±.0071 .9201±.0091 .9240±.0077 NeuralSparse .8908±.0080 .8733±.0022 .8630±.0049 .8931±.0090 .8720±.0043 .8649±.0041 .9272±.0108 .9136±.0117 .9089±.0084 .8872±.0071 .8733±.0036 .8623±.0050 .8903±.0087 .8776±.0078 .8609±.0055 .9219±.0122 .9099±.0104 .9093±.0101 Co-teaching .8943±.0090 .8760±.0117 .8638±.0093 .8931±.0045 .8792±.0036 .8606±.0083 .9315±.0075 .9291±.0327 .9319±.0324 .8961±.0130 .8815±.0099 .8566±.0057 .8917±.0076 .8811±.0127 .8643±.0129 .9126±.0116 .9101±.0046 .9210±.0095 .8872±.0036 .8803±.0060 .8512±.0136 .8987±.0221 .8764±.0099 .8639±.0073 .9098±.0110 .9135±.0116 .9096±.0132 .8899±.0239 .8729±.0205 .8544±.0051 .8932±.0256 .8808±.0053 .8618±.0317 .9037±.0089 .9114±.0065 .9064±.0059 .8853±.0061 .8718±.0110 .8525±.0108 .8867±.0080 .8739±.0033 .8558±.0042 .9131±.0068 .9108±.0095 .9162±.0125 .8922±.0034 .8749±.0098 .8588±.0042 .8810±.0034 .8795±.0099 .8593±.0040 .9234±.0088 .9252±.0052 .9255±.0043 RGIB-REP .9017±.0044 .8834±.0082 .8652±.0038 .9008±.0033 .8822±.0054 .8687±.0056 .9357±.0028 .9343±.0062 .9332±.0062 RGIB-SSL .9225±.0125 .8918±.0065 .8697±.0053 .9126±.0046 .8889±.0052 .8693±.0049 .9594±.0026 .9604±.0028 .9613±.0023\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8870±.0056 .8731±.0032 .8640±.0036 .8855±.0025 .8742±.0029 .8652±.0041 .9050±.0083 .9112±.0059 .9063±.0036 .8623±.0039 .8421±.0044 .8342±.0058 .8623±.0054 .8407±.0051 .8328±.0058 .9140±.0054 .9102±.0082 .9092±.0072 NeuralSparse .8814±.0049 .8586±.0029 .8603±.0060 .8792±.0072 .8695±.0031 .8612±.0046 .9140±.0065 .9130±.0047 .9080±.0077 .8807±.0053 .8610±.0026 .8518±.0040 .8791±.0040 .8708±.0032 .8619±.0040 .9123±.0074 .9086±.0079 .9105±.0072 Co-teaching .8850±.0126 .8698±.0101 .8568±.0050 .8773±.0023 .8767±.0113 .8691±.0095 .9103±.0372 .9311±.0290 .9204±.0162 .8860±.0125 .8633±.0079 .8534±.0048 .8734±.0023 .8775±.0107 .8668±.0042 .9155±.0070 .9172±.0089 .9251±.0107 .8852±.0092 .8658±.0085 .8555±.0055 .8711±.0031 .8794±.0126 .8670±.0200 .9045±.0137 .9118±.0108 .9065±.0059 .8807±.0046 .8738±.0140 .8605±.0193 .8718±.0070 .8767±.0110 .8623±.0114 .9081±.0131 .9136±.0147 .9155±.0106 .8716±.0090 .8627±.0043 .8533±.0045 .8705±.0108 .8733±.0026 .8643±.0047 .9232±.0081 .9294±.0075 .9218±.0092 .8798±.0079 .8664±.0031 .8584±.0051 .8806±.0043 .8675±.0035 .8591±.0045 .9296±.0064 .9190±.0052 .9110±.0049 RGIB-REP .8846±.0071 .8715±.0035 .8640±.0043 .8818±.0054 .8716±.0021 .8646±.0047 .9161±.0081 .9156±.0075 .9129±.0095 RGIB-SSL .8915±.0062 .8737±.0029 .8633±.0036 .8891±.0057 .8732±.0034 .8639±.0051 .9450±.0042 .9488±.0065 .9484±.0027\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 23: Full results on Pubmed dataset with GAT.\n\nGAT\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9173±.0028 .8984±.0030 .8884±.0033 .9255±.0024 .9176±.0035 .9102±.0030 .9306±.0038 .9271±.0030 .9232±.0034 .9102±.0023 .8970±.0032 .8837±.0034 .9208±.0030 .9177±.0028 .9087±.0037 .9374±.0031 .9309±.0020 .9267±.0019 NeuralSparse .9106±.0021 .8952±.0031 .8840±.0029 .9179±.0029 .9210±.0032 .9141±.0038 .9310±.0039 .9297±.0026 .9252±.0021 .9119±.0015 .8943±.0030 .8844±.0037 .9068±.0027 .9210±.0026 .9143±.0025 .9311±.0031 .9280±.0034 .9267±.0024 Co-teaching .9163±.0044 .8924±.0050 .8881±.0100 .9058±.0089 .9180±.0089 .9136±.0068 .9556±.0098 .9364±.0207 .9376±.0249 .9156±.0115 .8968±.0083 .8823±.0041 .9147±.0055 .9171±.0089 .9173±.0070 .9407±.0225 .9296±.0084 .9219±.0229 .9128±.0057 .8940±.0094 .8831±.0086 .9042±.0092 .9181±.0175 .9228±.0214 .9304±.0039 .9355±.0127 .9282±.0109 .9136±.0061 .8965±.0110 .8881±.0115 .9052±.0313 .9208±.0097 .9383±.0268 .9366±.0069 .9291±.0065 .9277±.0089 .9072±.0036 .8881±.0037 .8763±.0027 .9076±.0025 .9080±.0033 .9002±.0037 .9202±.0032 .9123±.0040 .9059±.0027 .8230±.0512 .7882±.0693 .7914±.0767 .8273±.0671 .8053±.0498 .7993±.0695 .8792±.0342 .8926±.0257 .8946±.0211 RGIB-REP .9190±.0025 .9034±.0017 .8939±.0056 .9250±.0033 .9164±.0024 .9099±.0029 .9311±.0036 .9306±.0025 .9276±.0022 RGIB-SSL .9223±.0027 .9054±.0032 .8960±.0027 .9183±.0021 .9071±.0023 .8977±.0029 .9405±.0027 .9407±.0022 .9378±.0018\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8610±.0045 .8434±.0042 .8339±.0048 .8668±.0040 .8547±.0050 .8476±.0037 .8817±.0042 .8772±.0040 .8696±.0062 .8600±.0034 .8462±.0028 .8325±.0044 .8605±.0041 .8557±.0057 .8454±.0029 .8852±.0052 .8851±.0055 .8871±.0034 NeuralSparse .8691±.0034 .8505±.0025 .8402±.0055 .8612±.0042 .8572±.0048 .8459±.0038 .8906±.0059 .8806±.0076 .8796±.0075 .8614±.0038 .8568±.0026 .8372±.0338 .8640±.0035 .8541±.0050 .8562±.0027 .8909±.0052 .8855±.0054 .8785±.0064 Co-teaching .8688±.0125 .8450±.0065 .8413±.0081 .8666±.0066 .8556±.0103 .8468±.0118 .8792±.0107 .8946±.0335 .8968±.0132 .8613±.0040 .8493±.0105 .8342±.0108 .8652±.0061 .8504±.0143 .8471±.0059 .8851±.0110 .8839±.0085 .8735±.0069 .8630±.0089 .8502±.0082 .8365±.0136 .8677±.0198 .8495±.0065 .8485±.0116 .8863±.0078 .8846±.0060 .8750±.0119 .8616±.0036 .8534±.0143 .8370±.0116 .8814±.0189 .8486±.0059 .8421±.0220 .8828±.0111 .8827±.0066 .8700±.0076 .8534±.0061 .8292±.0128 .8048±.0164 .8625±.0085 .8447±.0069 .8360±.0141 .8768±.0057 .8686±.0062 .8629±.0065 .8355±.0477 .8202±.0387 .8046±.0409 .8439±.0344 .8078±.0649 .7822±.0686 .7878±.0839 .8185±.0702 .8008±.0794 RGIB-REP .8656±.0043 .8443±.0057 .8339±.0033 .8674±.0058 .8528±.0043 .8392±.0036 .8909±.0084 .8925±.0051 .8833±.0062 RGIB-SSL .8891±.0037 .8651±.0036 .8480±.0068 .8878±.0039 .8650±.0044 .8489±.0055 .9276±.0042 .9277±.0022 .9264±.0024\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8315±.0059 .8116±.0073 .8040±.0092 .8374±.0036 .8201±.0321 .8067±.0306 .8480±.0086 .8414±.0116 .8313±.0071 .8468±.0407 .8236±.0050 .8004±.0071 .8655±.0037 .8303±.0044 .7915±.0334 .8501±.0058 .8568±.0068 .8523±.0381 NeuralSparse .8426±.0035 .8234±.0032 .8098±.0067 .8323±.0342 .8258±.0090 .7977±.0295 .8622±.0062 .8563±.0091 .8577±.0075 .8384±.0319 .8117±.0384 .8031±.0380 .8508±.0044 .8251±.0284 .8204±.0034 .8635±.0064 .8498±.0251 .8452±.0065 Co-teaching .8360±.0122 .8111±.0122 .8081±.0113 .8391±.0101 .8246±.0398 .8071±.0312 .8729±.0090 .8647±.0107 .8424±.0079 .8354±.0128 .8186±.0111 .8044±.0104 .8416±.0122 .8277±.0359 .8133±.0336 .8504±.0132 .8563±.0215 .8310±.0089 .8333±.0140 .8191±.0073 .8044±.0082 .8408±.0221 .8340±.0424 .8216±.0497 .8528±.0100 .8507±.0210 .8304±.0108 .8316±.0125 .8266±.0055 .8064±.0241 .8359±.0136 .8205±.0345 .8274±.0430 .8551±.0149 .8493±.0134 .8361±.0072 .8218±.0236 .7763±.0306 .7113±.0241 .8137±.0821 .8025±.0286 .7555±.0297 .8543±.0139 .8390±.0370 .8540±.0071 .7773±.0737 .7729±.0344 .6821±.1217 .7015±.1340 .6953±.0805 .6643±.1009 .5212±.0262 .5455±.0459 .5743±.0549 RGIB-REP .8324±.0039 .8162±.0088 .7934±.0068 .8248±.0070 .8145±.0088 .7892±.0107 .8518±.0056 .8496±.0083 .8492±.0113 RGIB-SSL .8738±.0051 .8443±.0033 .8124±.0084 .8709±.0023 .8432±.0046 .8187±.0050 .9208±.0043 .9250±.0029 .9235±.0031\n\nPeer loss Jaccard GIB SupCon GRACE\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nTable 24: Full results on Pubmed dataset with SAGE.\n\nSAGE\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9136±.0032 .9094±.0035 .9035±.0040 .9295±.0025 .9378±.0022 .9410±.0023 .8817±.0034 .8793±.0043 .8757±.0041 .9142±.0026 .9188±.0029 .9014±.0038 .9179±.0020 .9178±.0013 .9165±.0013 .9021±.0021 .9003±.0018 .9011±.0020 NeuralSparse .9118±.0030 .9137±.0031 .9094±.0034 .9271±.0019 .9209±.0026 .9234±.0017 .9033±.0028 .8925±.0030 .8864±.0032 .9114±.0026 .9138±.0030 .9104±.0037 .9267±.0020 .9211±.0027 .9229±.0024 .9030±.0028 .8941±.0040 .8868±.0033 Co-teaching .9164±.0031 .9160±.0027 .9092±.0132 .9362±.0061 .9258±.0072 .9228±.0017 .8827±.0026 .9054±.0236 .8856±.0113 .9103±.0104 .9174±.0094 .9044±.0087 .9308±.0079 .9238±.0051 .9307±.0076 .9005±.0118 .8833±.0066 .8909±.0058 .9108±.0123 .9167±.0105 .9109±.0031 .9236±.0098 .9259±.0175 .9308±.0141 .8861±.0052 .8785±.0092 .8842±.0048 .9173±.0049 .9125±.0211 .9096±.0020 .9338±.0152 .9294±.0125 .9299±.0194 .8880±.0125 .8865±.0129 .8853±.0058 .9167±.0026 .9148±.0031 .9106±.0035 .9259±.0034 .9305±.0031 .9230±.0033 .9033±.0055 .8929±.0051 .8888±.0034 .8317±.0063 .8291±.0059 .8336±.0068 .8288±.0071 .8344±.0074 .8305±.0052 .8320±.0046 .8344±.0078 .8328±.0075 RGIB-REP .9192±.0022 .9136±.0027 .9119±.0037 .9278±.0031 .9322±.0031 .9331±.0034 .9055±.0033 .9015±.0051 .9000±.0043 RGIB-SSL .9276±.0052 .9285±.0030 .9313±.0031 .9273±.0056 .9305±.0040 .9334±.0025 .9304±.0017 .9294±.0049 .9274±.0066\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8627±.0056 .8663±.0061 .8619±.0073 .8715±.0080 .8901±.0082 .9033±.0057 .8386±.0085 .8330±.0104 .8268±.0092 .8737±.0060 .8740±.0065 .8816±.0077 .9072±.0120 .9016±.0078 .9087±.0019 .9037±.0024 .9036±.0048 .9048±.0059 NeuralSparse .8888±.0037 .8825±.0046 .8822±.0051 .8982±.0067 .9146±.0029 .9190±.0041 .8647±.0075 .8535±.0091 .8472±.0079 .8842±.0045 .8853±.0059 .8846±.0052 .8885±.0055 .9057±.0055 .9127±.0058 .8464±.0105 .8458±.0066 .8408±.0065 Co-teaching .8678±.0104 .8752±.0090 .8667±.0122 .8727±.0079 .8971±.0159 .9061±.0098 .8601±.0240 .8498±.0368 .8514±.0388 .8683±.0136 .8717±.0100 .8665±.0117 .8736±.0075 .8997±.0079 .9095±.0119 .8510±.0151 .8514±.0115 .8423±.0128 .8646±.0124 .8725±.0141 .8625±.0140 .8773±.0125 .9017±.0205 .9155±.0164 .8378±.0137 .8383±.0118 .8275±.0083 .8816±.0203 .8768±.0098 .8641±.0238 .8824±.0102 .8918±.0095 .9221±.0243 .8479±.0103 .8410±.0151 .8330±.0120 .8765±.0086 .8682±.0100 .8518±.0198 .8907±.0054 .8957±.0104 .8879±.0113 .8865±.0046 .8811±.0052 .8810±.0044 .7142±.0451 .6741±.0484 .7227±.0446 .6578±.0701 .6816±.0536 .7012±.0516 .6498±.0896 .6647±.0535 .6540±.0424 RGIB-REP .8746±.0049 .8708±.0064 .8758±.0071 .8800±.0045 .8904±.0048 .8941±.0051 .8504±.0078 .8466±.0070 .8533±.0118 RGIB-SSL .9315±.0036 .9237±.0035 .9214±.0033 .9276±.0046 .9310±.0039 .9286±.0035 .9463±.0030 .9448±.0037 .9430±.0021\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.8314±.0081 .8105±.0279 .8333±.0089 .8224±.0335 .8538±.0105 .8566±.0199 .8242±.0104 .8161±.0071 .8090±.0129 .8781±.0041 .8665±.0076 .8556±.0081 .8800±.0050 .8731±.0122 .8628±.0152 .8503±.0070 .8698±.0074 .8426±.0096 NeuralSparse .8611±.0101 .8589±.0081 .8482±.0182 .8694±.0083 .8858±.0068 .8889±.0057 .8368±.0074 .8344±.0090 .8353±.0081 .8425±.0073 .8511±.0067 .8525±.0064 .8430±.0107 .8627±.0115 .8739±.0103 .8232±.0076 .8195±.0071 .8198±.0060 Co-teaching .8385±.0114 .8202±.0303 .8349±.0147 .8265±.0385 .8607±.0164 .8612±.0190 .8299±.0352 .8243±.0074 .8143±.0409 .8339±.0162 .8111±.0285 .8335±.0084 .8317±.0412 .8610±.0135 .8605±.0223 .8277±.0224 .8260±.0197 .8204±.0316 .8355±.0110 .8180±.0272 .8347±.0124 .8298±.0436 .8564±.0107 .8607±.0394 .8260±.0138 .8207±.0130 .8140±.0192 .8309±.0062 .8195±.0329 .8336±.0206 .8226±.0355 .8527±.0208 .8647±.0344 .8310±.0142 .8237±.0128 .8141±.0165 .8288±.0109 .8276±.0351 .8174±.0070 .8617±.0122 .8443±.0528 .8365±.0350 .8745±.0071 .8692±.0132 .8661±.0076 .7293±.0133 .7151±.0368 .6737±.0522 .7102±.0296 .7392±.0257 .6751±.0444 .7120±.0166 .7042±.0588 .7230±.0286 RGIB-REP .8493±.0057 .8424±.0083 .8373±.0079 .8439±.0063 .8527±.0092 .8536±.0112 .8275±.0218 .8148±.0098 .8211±.0145 RGIB-SSL .8915±.0060 .8971±.0051 .8956±.0046 .8948±.0079 .9026±.0042 .8995±.0040 .9313±.0042 .9311±.0036 .9294±.0053\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 25: Full results on Facebook dataset with GCN.\n\nGCN\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9880±.0007 .9866±.0007 .9843±.0010 .9878±.0008 .9852±.0006 .9834±.0011 .9892±.0006 .9888±.0008 .9888±.0007 .9865±.0007 .9845±.0008 .9821±.0009 .9764±.0010 .9835±.0007 .9708±.0012 .9783±.0006 .9879±.0008 .9881±.0007 NeuralSparse .9877±.0007 .9861±.0008 .9837±.0009 .9772±.0007 .9852±.0007 .9732±.0010 .9788±.0006 .9885±.0007 .9888±.0005 .9876±.0008 .9862±.0008 .9838±.0008 .9776±.0007 .9848±.0008 .9728±.0011 .9859±.0006 .9786±.0009 .9886±.0006 Co-teaching .9871±.0052 .9722±.0104 .9876±.0009 .9738±.0018 .9848±.0077 .9750±.0002 .9716±.0266 .9873±.0121 .9825±.0009 .9755±.0095 .9737±.0033 .9895±.0096 .9797±.0024 .9855±.0051 .9720±.0019 .9763±.0001 .9732±.0110 .9774±.0034 .9759±.0043 .9796±.0056 .9707±.0006 .9739±.0098 .9807±.0040 .9793±.0150 .9729±.0030 .9773±.0044 .9729±.0042 .9752±.0009 .9735±0.021 .9720±.0154 .9796±.0245 .9820±.0173 .9707±.0024 .9724±.0017 .9774±.1404 .9717±.0086 .9779±.0008 .9748±.0014 .9665±.0022 .9724±.0009 .9803±.0011 .9783±.0010 .9713±.0060 .9569±.0160 .9701±.0024 .8883±.0120 .8811±.0077 .8412±.0224 .8950±.0081 .8792±.0119 .8530±.0221 .8864±.0077 .8826±.0051 .8784±.0104 RGIB-REP .9881±.0007 .9866±.0008 .9846±.0009 .9880±.0007 .9854±.0005 .9841±.0011 .9895±.0007 .9891±.0008 .9894±.0008 RGIB-SSL .9845±.0005 .9810±.0005 .9765±.0014 .9840±.0011 .9808±.0012 .9787±.0009 .9862±.0005 .9859±.0010 .9866±.0007\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9829±.0020 .9520±.0424 .9438±.0402 .9819±.0015 .9668±.0147 .9622±.0154 .9882±.0007 .9880±.0007 .9886±.0006 .9811±.0028 .9682±.0096 .9473±.0120 .9803±.0016 .9685±.0033 .9531±.0112 .9673±.0008 .9771±.0008 .9776±.0009 NeuralSparse .9825±.0020 .9638±.0039 .9456±.0067 .9712±.0022 .9691±.0045 .9583±.0071 .9781±.0007 .9781±.0008 .9784±.0008 .9725±.0017 .9674±.0131 .9485±.0094 .9725±.0014 .9668±.0089 .9493±.0267 .9879±.0003 .9880±.0008 .9783±.0008 Co-teaching .9820±.0113 .9526±.0439 .9480±.0450 .9712±.0084 .9707±.0141 .9714±.0146 .9762±.0070 .9797±.0167 .9638±.0154 .9807±.0016 .9536±.0503 .9430±.0437 .9758±.0011 .9703±.0204 .9622±.0176 .9769±.0085 .9750±.0128 .9734±.0137 .9794±.0031 .9579±.0471 .9428±.0452 .9784±.0127 .9702±.0295 .9638±.0340 .9702±.0016 .9725±.0064 .9758±.0035 .9773±.0182 .9608±.0548 .9417±.0441 .9796±.0260 .9647±.0200 .9650±.0373 .9742±.0073 .9703±.0023 .9771±.0030 .9588±.0067 .9508±.0100 .9297±.0121 .9647±.0088 .9517±.0113 .9401±.0135 .9647±.0164 .9567±.0149 .9553±.0133 .8899±.0100 .8865±.0226 .8315±.0108 .9015±.0049 .8833±.0285 .8395±.0157 .8913±.0036 .8972±.0046 .8887±.0123 RGIB-REP .9832±.0026 .9770±.0032 .9519±.0063 .9833±.0014 .9723±.0062 .9682±.0087 .9884±.0006 .9883±.0007 .9889±.0007 RGIB-SSL .9829±.0023 .9711±.0025 .9643±.0029 .9821±.0019 .9707±.0021 .9668±.0040 .9857±.0010 .9881±.0008 .9857±.0007\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9798±.0013 .9609±.0138 .9368±.0179 .9764±.0034 .9502±.0096 .9469±.0160 .9863±.0013 .9865±.0012 .9876±.0010 .9774±.0030 .9635±.0063 .9500±.0103 .9540±.0034 .9579±.0083 .9411±.0084 .9556±.0014 .9642±.0006 .9559±.0012 NeuralSparse .9774±.0020 .9596±.0067 .9435±.0114 .9449±.0029 .9496±.0073 .9490±.0087 .9564±.0012 .9562±.0011 .9465±.0016 .9767±.0066 .9606±.0131 .9449±.0175 .9555±.0034 .9568±.0064 .9416±.0145 .9560±.0011 .9460±.0014 .9569±.0009 Co-teaching .9638±.0051 .9565±.0235 .9426±.0261 .9520±.0051 .9568±.0148 .9489±.0228 .9474±.0068 .9458±.0143 .9561±.0169 .9699±.0045 .9557±.0141 .9362±.0221 .9554±.0053 .9494±.0135 .9445±.0234 .9498±.0063 .9581±.0200 .9650±.0021 .9641±.0032 .9553±.0137 .9425±.0178 .9507±.0064 .9616±.0125 .9480±.0170 .9528±.0060 .9525±.0075 .9423±.0057 .9649±.0145 .9602±.0198 .9416±.0236 .9511±.0333 .9552±.0261 .9555±.0346 .9556±.0099 .9430±.0106 .9578±.0052 .9462±.0050 .9377±.0054 .9282±.0113 .9460±.0073 .9355±.0118 .9253±.0110 .9411±.0088 .9550±.0087 .9510±.0122 .8939±.0118 .8563±.0206 .8259±.0019 .9071±.0077 .8582±.0228 .8274±.0029 .8962±.0093 .9026±.0035 .8977±.0100 RGIB-REP .9774±.0031 .9680±.0097 .9501±.0134 .9780±.0024 .9619±.0069 .9423±.0085 .9868±.0008 .9863±.0010 .9872±.0011 RGIB-SSL .9751±.0023 .9641±.0037 .9395±.0122 .9763±.0034 .9633±.0040 .9451±.0084 .9845±.0008 .9843±.0012 .9847±.0011\n\nPeer loss Jaccard GIB SupCon GRACE\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nTable 26: Full results on Facebook dataset with GAT.\n\nGAT\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9869±.0005 .9856±.0008 .9836±.0007 .9878±.0006 .9861±.0006 .9858±.0008 .9872±.0006 .9864±.0009 .9857±.0008 .9645±.0007 .9532±.0010 .9517±.0009 .9652±.0007 .9636±.0007 .9733±.0008 .9755±.0007 .9743±.0010 .9739±.0012 NeuralSparse .9661±.0007 .9651±.0009 .9638±.0006 .9765±.0008 .9652±.0007 .9744±.0006 .9662±.0010 .9657±.0006 .9656±.0007 .9762±.0007 .9551±.0008 .9735±.0007 .9766±.0007 .9650±.0007 .9645±.0009 .9766±.0006 .9759±.0009 .9757±.0008 Co-teaching .9723±.0006 .9685±.0054 .9762±.0039 .9632±.0057 .9720±.0001 .9741±.0046 .9703±0.001 .9789±.0232 .9792±.0288 .9762±.0032 .9644±.0020 .9725±.0076 .9699±.0020 .9720±.0104 .9708±.0103 .9632±.0194 .9807±.0207 .9713±.0078 .9771±.0038 .9661±.0057 .9736±.0078 .9556±.0064 .9758±.0180 .9780±.0012 .9720±.0100 .9749±.0064 .9701±.0087 .9879±.0075 .9616±.0007 .9709±.0199 .9564±.0122 .9746±.0004 .9645±.0225 .9758±.0101 .9738±.0019 .9799±.0094 .9718±.0028 .9646±.0085 .9659±.0087 .9753±.0008 .9737±.0009 .9629±.0009 .9720±.0038 .9456±.0038 .9419±.0029 .8377±.0309 .8716±.0263 .8519±.0264 .8343±.0341 .8502±.0271 .8487±.0407 .8768±.0457 .8370±.0416 .8794±.0319 RGIB-REP .9865±.0006 .9854±.0009 .9837±.0006 .9872±.0007 .9855±.0007 .9855±.0008 .9871±.0007 .9861±.0009 .9859±.0006 RGIB-SSL .9788±.0011 .9764±.0017 .9717±.0021 .9784±.0012 .9764±.0015 .9734±.0021 .9778±.0008 .9784±.0009 .9791±.0009\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9857±.0015 .9850±.0012 .9820±.0014 .9855±.0011 .9827±.0019 .9773±.0046 .9873±.0010 .9874±.0010 .9874±.0005 .9839±.0009 .9791±.0032 .9767±.0024 .9836±.0009 .9772±.0022 .9705±.0030 .9852±.0005 .9845±.0009 .9844±.0007 NeuralSparse .9854±.0009 .9812±.0019 .9768±.0027 .9839±.0012 .9790±.0028 .9736±.0030 .9872±.0009 .9869±.0009 .9868±.0007 .9843±.0039 .9830±.0018 .9745±.0076 .9844±.0012 .9787±.0039 .9752±.0027 .9872±.0005 .9868±.0010 .9872±.0007 Co-teaching .9789±.0039 .9777±.0086 .9717±.0111 .9703±.0050 .9718±.0071 .9782±.0049 .9645±.0105 .9730±.0267 .9646±.0014 .9944±.0029 .9630±.0105 .9687±.0046 .9610±.0068 .9610±.0064 .9783±.0073 .9677±.0053 .9638±.0050 .9691±.0203 .9757±.0079 .9863±.0087 .9883±.0015 .9607±.0182 .9616±.0184 .9655±.0151 .9689±.0025 .9625±.0009 .9604±.0036 .9809±.0134 .9747±.0077 .9709±.0204 .9721±.0297 .9741±.0199 .9740±.0111 .9747±.0038 .9768±.0093 .9799±.0016 .9709±.0156 .9464±.0254 .9134±.0309 .9824±.0008 .9745±.0062 .9677±.0113 .9600±.0154 .9580±.0183 .9482±.0174 .7803±.0649 .7947±.0600 .7618±.1058 .8259±.0669 .7588±.0963 .7260±.1331 .8440±.0370 .8448±.0337 .8122±.0403 RGIB-REP .9855±.0014 .9839±.0016 .9783±.0034 .9838±.0019 .9806±.0013 .9765±.0037 .9872±.0008 .9871±.0008 .9867±.0007 RGIB-SSL .9751±.0033 .9707±.0061 .9602±.0043 .9767±.0030 .9683±.0053 .9603±.0062 .9755±.0028 .9772±.0032 .9762±.0047\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9805±.0025 .9658±.0321 .9577±.0314 .9804±.0018 .9738±.0044 .9710±.0036 .9854±.0015 .9860±.0007 .9867±.0012 .9643±.0041 .9638±.0054 .9500±.0274 .9736±.0042 .9589±.0089 .9606±.0055 .9735±.0008 .9726±.0012 .9736±.0009 NeuralSparse .9746±.0077 .9667±.0092 .9560±.0122 .9740±.0047 .9668±.0056 .9652±.0043 .9735±.0017 .9742±.0018 .9733±.0030 .9769±.0037 .9679±.0071 .9582±.0296 .9754±.0027 .9688±.0045 .9645±.0039 .9738±.0041 .9741±.0022 .9756±.0012 Co-teaching .9745±.0034 .9686±.0316 .9625±.0408 .9747±.0084 .9833±.0108 .9738±.0035 .9622±.0024 .9676±.0072 .9721±.0215 .9690±.0114 .9728±.0334 .9588±.0309 .9716±.0028 .9776±.0075 .9766±.0074 .9871±.0207 .9605±.0202 .9768±.0063 .9789±.0084 .9651±.0360 .9620±.0337 .9673±.0007 .9809±.0110 .9731±.0021 .9652±.0064 .9607±.0004 .9751±.0030 .9839±.0037 .9777±.0420 .9584±.0374 .9868±.0290 .9760±.0135 .9791±.0249 .9648±.0054 .9640±.0046 .9774±.0029 .9428±.0194 .9097±.0361 .8779±.0347 .9653±.0171 .9459±.0378 .9312±.0344 .9465±.0173 .9484±.0194 .9023±.0333 .7234±.0687 .6604±.1398 .6075±.1089 .7415±.1242 .6176±.1345 .6216±.1341 .7613±.1157 .7256±.1351 .7062±.1042 RGIB-REP .9779±.0031 .9711±.0042 .9433±.0392 .9753±.0041 .9666±.0087 .9614±.0057 .9847±.0017 .9848±.0013 .9856±.0009 RGIB-SSL .9653±.0107 .9491±.0105 .9254±.0206 .9676±.0057 .9502±.0087 .9384±.0067 .9737±.0065 .9675±.0052 .9713±.0103\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 27: Full results on Facebook dataset with SAGE.\n\nSAGE\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9858±.0008 .9827±.0008 .9788±.0012 .9881±.0008 .9862±.0007 .9862±.0008 .9867±.0006 .9840±.0006 .9825±.0014 .9745±.0008 .9807±.0011 .9765±.0011 .9766±.0007 .9749±.0011 .9752±.0008 .9761±.0008 .9739±.0010 .9719±.0011 NeuralSparse .9753±.0008 .9619±.0011 .9683±.0007 .9675±.0008 .9660±.0006 .9659±.0011 .9667±.0007 .9640±.0007 .9627±.0011 .9755±.0007 .9723±.0009 .9779±.0012 .9775±.0007 .9761±.0008 .9754±.0007 .9766±.0007 .9743±.0007 .9726±.0014 Co-teaching .9759±.0101 .9722±.0004 .9728±.0100 .9702±.0050 .9742±.0023 .9739±.0081 .9764±.0125 .9788±.0205 .9700±.0282 .9765±.0052 .9720±.0072 .9790±.0103 .9787±.0005 .9781±.0105 .9787±.0075 .9721±.0019 .9842±.0136 .9793±.0031 .9747±.0018 .9720±.0009 .9721±.0096 .9729±.0166 .9760±.0139 .9752±.0102 .9733±.0053 .9706±.0059 .9716±.0098 .9798±.0089 .9780±.0040 .9765±.0020 .9774±.0139 .9767±.0287 .9778±.0006 .9780±.0012 .9713±.0067 .9772±.0050 .9482±.0166 .9288±.0046 .9141±.0139 .9793±.0043 .9782±.0072 .9786±.0078 .9278±.0048 .9252±.0074 .9156±.0041 .9113±.0116 .8907±.0295 .9077±.0089 .9012±.0113 .8885±.0177 .9031±.0104 .9053±.0126 .9050±.0173 .9067±.0137 RGIB-REP .9860±.0009 .9834±.0008 .9802±.0014 .9878±.0009 .9863±.0007 .9863±.0011 .9875±.0006 .9860±.0007 .9855±.0011 RGIB-SSL .9840±.0007 .9829±.0010 .9811±.0006 .9838±.0007 .9821±.0008 .9823±.0004 .9841±.0008 .9839±.0007 .9844±.0010\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9824±.0015 .9783±.0025 .9698±.0040 .9849±.0015 .9815±.0024 .9815±.0025 .9844±.0015 .9817±.0007 .9809±.0009 .9818±.0026 .9764±.0034 .9722±.0022 .9625±.0015 .9601±.0043 .9600±.0023 .9649±.0008 .9627±.0017 .9615±.0012 NeuralSparse .9811±.0031 .9776±.0021 .9740±.0031 .9636±.0023 .9600±.0026 .9615±.0016 .9648±.0008 .9628±.0014 .9613±.0025 .9823±.0012 .9757±.0041 .9735±.0032 .9623±.0041 .9609±.0031 .9607±.0022 .9652±.0007 .9632±.0009 .9623±.0024 Co-teaching .9660±.0009 .9653±.0075 .9659±.0114 .9600±.0098 .9634±.0091 .9626±.0079 .9654±.0253 .9706±.0136 .9628±.0263 .9668±.0105 .9676±.0064 .9716±.0030 .9681±.0079 .9641±.0044 .9603±.0112 .9671±.0204 .9609±.0088 .9660±.0032 .9621±.0019 .9715±.0056 .9772±.0068 .9662±.0143 .9675±.0094 .9782±.0056 .9725±.0107 .9726±.0063 .9705±9.276 .9774±0.000 .9758±.0200 .9732±.0025 .9708±.0195 .9626±.0286 .9679±.0171 .9765±.0101 .9714±.0018 .9710±.0107 .9169±.0144 .9096±.0191 .9001±.0143 .9324±.0208 .9188±.0100 .9185±.0192 .9295±.0114 .9328±.0158 .9325±.0148 .8683±.0292 .8355±.0696 .8545±.0415 .8585±.0424 .8721±.0261 .8413±.0364 .8108±.0213 .8527±.0375 .8376±.0330 RGIB-REP .9828±.0016 .9804±.0020 .9750±.0024 .9843±.0014 .9811±.0030 .9808±.0030 .9857±.0004 .9833±.0025 .9830±.0019 RGIB-SSL .9817±.0016 .9797±.0014 .9768±.0009 .9828±.0018 .9798±.0014 .9797±.0013 .9821±.0009 .9820±.0011 .9826±.0010\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9751±.0055 .9603±.0365 .9616±.0091 .9715±.0077 .9672±.0057 .9517±.0228 .9761±.0136 .9788±.0045 .9749±.0151 .9652±.0039 .9676±.0088 .9578±.0149 .9734±.0050 .9616±.0126 .9665±.0073 .9631±.0022 .9606±.0046 .9651±.0145 NeuralSparse .9640±.0075 .9577±.0222 .9672±.0061 .9647±.0048 .9694±.0156 .9673±.0036 .9632±.0017 .9618±.0037 .9615±.0051 .9668±.0029 .9633±.0142 .9617±.0111 .9774±.0028 .9693±.0053 .9642±.0055 .9629±.0035 .9736±.0010 .9708±.0037 Co-teaching .9644±.0116 .9699±.0463 .9658±.0182 .9730±.0107 .9738±.0076 .9583±.0239 .9742±.0138 .9744±.0106 .9720±.0202 .9644±.0141 .9602±.0369 .9681±.0085 .9613±.0167 .9728±.0053 .9524±.0288 .9655±.0220 .9769±.0028 .9650±.0256 .9750±.0119 .9666±.0453 .9686±.0094 .9666±.0261 .9612±.0056 .9683±.0383 .9634±.0170 .9784±.0110 .9781±.0178 .9739±.0197 .9656±.0564 .9749±.0125 .9743±.0351 .9717±.0034 .9615±.0291 .9607±.0153 .9779±.0059 .9743±.0222 .9007±.0112 .8859±.0189 .8730±.0140 .8997±.0142 .8940±.0104 .9006±.0232 .9146±.0114 .9166±.0143 .9185±.0122 .8202±.0365 .7225±.0840 .7901±.1065 .8018±.0575 .8224±.0547 .7960±.0915 .8061±.0367 .8262±.0306 .8103±.0351 RGIB-REP .9770±.0040 .9557±.0119 .9574±.0149 .9749±.0046 .9683±.0072 .9584±.0093 .9838±.0020 .9829±.0015 .9814±.0029 RGIB-SSL .9773±.0032 .9720±.0031 .9678±.0030 .9781±.0027 .9739±.0023 .9692±.0050 .9774±.0016 .9786±.0012 .9786±.0017\n\nPeer loss Jaccard GIB SupCon GRACE\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nTable 28: Full results on Chameleon dataset with GCN.\n\nGCN\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9753±.0025 .9696±.0022 .9657±.0029 .9784±.0017 .9762±.0016 .9754±.0023 .9775±.0018 .9769±.0018 .9755±.0036 .9716±.0030 .9640±.0025 .9579±.0029 .9646±.0026 .9602±.0024 .9661±.0024 .9686±.0018 .9664±.0029 .9651±.0031 NeuralSparse .9741±.0032 .9692±.0030 .9633±.0030 .9682±.0026 .9646±.0023 .9627±.0022 .9679±.0017 .9669±.0029 .9651±.0039 .9748±.0026 .9682±.0032 .9637±.0031 .9679±.0021 .9649±.0019 .9627±.0025 .9690±.0019 .9674±.0021 .9648±.0033 Co-teaching .9679±.0059 .9641±.0086 .9676±.0088 .9767±.0096 .9712±.0084 .9709±.0106 .9790±.0218 .9729±.0015 .9745±.0272 .9647±.0057 .9662±.0106 .9658±.0059 .9781±.0043 .9784±.0014 .9759±.0095 .9762±.0159 .9695±.0090 .9736±.0036 .9684±.0103 .9657±.0118 .9660±.0030 .9611±.0190 .9799±.0005 .9774±.0128 .9711±.0010 .9754±.0091 .9750±.0086 .9649±.0052 .9711±.0050 .9634±.0177 .9709±.0106 .9714±.0257 .9786±.0012 .9674±.0066 .9660±.0070 .9778±.0089 .9625±.0024 .9677±.0024 .9613±.0032 .9757±.0018 .9718±.0023 .9697±.0034 .9749±.0013 .9755±.0024 .9750±.0030 .9145±.0105 .8978±.0110 .8915±.0033 .9081±.0121 .9005±.0062 .8931±.0042 .9171±.0042 .9184±.0054 .9196±.0058 RGIB-REP .9754±.0026 .9698±.0026 .9652±.0027 .9797±.0023 .9757±.0016 .9744±.0028 .9804±.0023 .9798±.0016 .9782±.0028 RGIB-SSL .9719±.0023 .9658±.0025 .9595±.0026 .9709±.0017 .9664±.0024 .9638±.0025 .9786±.0016 .9797±.0022 .9789±.0034\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9616±.0033 .9496±.0190 .9274±.0276 .9608±.0038 .9433±.0261 .9368±.0271 .9686±.0020 .9580±.0021 .9362±.0035 .9568±.0044 .9548±.0058 .9407±.0050 .9567±.0039 .9433±.0055 .9432±.0088 .9580±.0020 .9579±.0033 .9578±.0033 NeuralSparse .9599±.0026 .9497±.0032 .9402±.0057 .9609±.0027 .9540±.0178 .9348±.0169 .9583±.0017 .9583±.0030 .9571±.0031 .9607±.0030 .9514±.0036 .9424±.0097 .9610±.0024 .9457±.0194 .9360±.0090 .9585±.0022 .9576±.0030 .9665±.0035 Co-teaching .9595±.0112 .9516±.0199 .9483±.0374 .9524±.0123 .9446±.0288 .9447±.0286 .9642±.0079 .9650±.0058 .9533±.0050 .9543±.0090 .9533±.0192 .9267±.0320 .9558±.0037 .9482±.0280 .9412±.0269 .9621±.0161 .9501±.0055 .9569±.0227 .9503±.0061 .9538±.0223 .9344±.0364 .9507±.0056 .9436±.0286 .9364±.0385 .9603±.0051 .9659±.0020 .9557±.0070 .9554±.0036 .9561±.0292 .9321±.0267 .9605±.0069 .9521±.0231 .9416±.0512 .9651±.0061 .9582±.0100 .9489±.0029 .9561±.0046 .9531±.0043 .9467±.0045 .9606±.0031 .9536±.0031 .9468±.0075 .9584±.0015 .9580±.0051 .9477±.0038 .8978±.0145 .8987±.0050 .8949±.0030 .8994±.0141 .9007±.0045 .8964±.0043 .9053±.0110 .9074±.0136 .9075±.0113 RGIB-REP .9723±.0035 .9621±.0036 .9519±.0052 .9705±.0027 .9604±.0052 .9480±.0053 .9785±.0031 .9797±.0017 .9785±.0032 RGIB-SSL .9655±.0022 .9592±.0043 .9500±.0053 .9658±.0023 .9570±.0047 .9486±.0029 .9730±.0017 .9752±.0030 .9744±.0032\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9662±.0042 .9511±.0079 .9286±.0067 .9656±.0045 .9450±.0177 .9276±.0229 .9752±.0027 .9766±.0035 .9745±.0040 .9601±.0032 .9484±.0053 .9331±.0066 .9608±.0020 .9481±.0129 .9331±.0073 .9537±.0022 .9559±.0027 .9547±.0024 NeuralSparse .9644±.0035 .9550±.0033 .9411±.0046 .9538±.0035 .9468±.0038 .9387±.0053 .9525±.0033 .9541±.0031 .9535±.0037 .9655±.0043 .9585±.0033 .9384±.0088 .9657±.0024 .9553±.0051 .9391±.0044 .9537±.0037 .9559±.0025 .9543±.0032 Co-teaching .9654±.0058 .9582±.0139 .9319±.0163 .9505±.0121 .9534±.0222 .9271±.0308 .9679±.0198 .9557±.0112 .9557±.0282 .9658±.0120 .9551±.0174 .9308±.0099 .9552±.0040 .9441±.0226 .9358±.0328 .9575±.0164 .9667±.0230 .9593±.0236 .9754±.0063 .9549±.0088 .9292±.0104 .9637±.0035 .9495±.0213 .9457±.0350 .9500±.0040 .9550±.0128 .9433±.0066 .9800±.0203 .9621±.0246 .9329±.0238 .9544±.0212 .9542±.0395 .9456±.0219 .9619±.0072 .9505±.0112 .9536±.0124 .9537±.0054 .9452±.0073 .9352±.0046 .9534±.0068 .9454±.0068 .9375±.0023 .9618±.0036 .9627±.0032 .9557±.0067 .9024±.0098 .8995±.0030 .8936±.0029 .9025±.0097 .8983±.0055 .8952±.0042 .9154±.0092 .9166±.0092 .9151±.0082 RGIB-REP .9666±.0028 .9580±.0074 .9467±.0067 .9662±.0027 .9606±.0041 .9453±.0049 .9740±.0033 .9776±.0021 .9753±.0031 RGIB-SSL .9633±.0032 .9416±.0036 .9177±.0082 .9627±.0037 .9482±.0037 .9276±.0093 .9712±.0017 .9742±.0025 .9726±.0032\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 29: Full results on Chameleon dataset with GAT.\n\nGAT\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9725±.0027 .9650±.0018 .9625±.0018 .9767±.0026 .9747±.0020 .9759±.0018 .9746±.0023 .9743±.0017 .9711±.0041 .9716±.0023 .9630±.0036 .9563±.0023 .9747±.0026 .9717±.0030 .9708±.0025 .9752±.0018 .9726±.0026 .9698±.0035 NeuralSparse .9733±.0029 .9667±.0028 .9615±.0028 .9762±.0028 .9748±.0028 .9741±.0015 .9762±.0022 .9742±.0033 .9704±.0042 .9736±.0028 .9657±.0024 .9620±.0027 .9768±.0023 .9735±.0021 .9739±.0022 .9670±.0021 .9631±.0028 .9611±.0058 Co-teaching .9617±.0071 .9629±.0047 .9626±.0051 .9624±.0084 .9645±.0027 .9647±.0104 .9684±.0279 .9610±.0139 .9688±.0092 .9675±.0064 .9646±.0101 .9683±.0037 .9602±.0046 .9621±.0112 .9611±.0078 .9638±.0045 .9626±.0074 .9620±.0089 .9662±.0063 .9665±.0062 .9697±.0073 .9652±.0130 .9649±.0051 .9616±.0123 .9608±.0076 .9612±.0047 .9603±.0134 .9628±.0018 .9639±.0115 .9625±.0208 .9681±.0091 .9643±.0257 .9601±.0285 .9644±.0119 .9688±.0096 .9631±.0113 .9709±.0059 .9611±.0195 .9607±.0032 .9752±.0011 .9721±.0026 .9706±.0022 .9692±.0035 .9701±.0030 .9689±.0057 .7908±.0348 .7829±.0494 .7598±.0693 .7992±.0371 .7960±.0371 .7913±.0418 .7905±.0261 .7985±.0322 .7894±.0340 RGIB-REP .9736±.0031 .9668±.0021 .9629±.0026 .9769±.0018 .9752±.0016 .9752±.0018 .9766±.0021 .9753±.0029 .9732±.0041 RGIB-SSL .9635±.0037 .9593±.0037 .9536±.0041 .9640±.0062 .9587±.0048 .9545±.0035 .9709±.0015 .9728±.0028 .9719±.0046\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9721±.0035 .9652±.0023 .9605±.0031 .9741±.0028 .9686±.0030 .9674±.0027 .9740±.0037 .9738±.0027 .9712±.0047 .9666±.0031 .9572±.0034 .9469±.0043 .9659±.0028 .9598±.0024 .9496±.0040 .9749±.0024 .9747±.0016 .9718±.0031 NeuralSparse .9687±.0027 .9603±.0026 .9536±.0033 .9699±.0025 .9600±.0036 .9502±.0037 .9748±.0022 .9756±.0022 .9727±.0035 .9707±.0041 .9640±.0031 .9549±.0034 .9708±.0027 .9602±.0025 .9548±.0058 .9754±.0025 .9761±.0021 .9709±.0041 Co-teaching .9726±.0096 .9648±.0030 .9639±.0088 .9551±.0037 .9503±.0027 .9566±.0028 .9691±.0039 .9664±.7565 .9622±.0242 .9745±.0092 .9714±.0088 .9668±.0103 .9637±.0041 .9602±.0088 .9658±.0064 .9549±.0073 .9531±.0096 .9587±.0068 .9776±.0089 .9692±.0098 .9663±.0112 .9729±.0024 .9677±.0214 .9744±.0012 .9587±.0078 .9519±.0092 .9573±.0137 .9600±.0148 .9646±.0041 .9641±.0089 .9639±.0197 .9642±.0189 .9636±.0047 .9646±.0044 .9624±.0103 .9609±.0064 .9644±.0033 .9537±.0106 .9381±.0125 .9675±.0028 .9631±.0042 .9555±.0068 .9594±.0131 .9442±.0136 .9386±.0102 .5429±.0465 .6085±.1132 .6273±.1475 .6178±.0940 .5839±.1022 .6513±.1544 .5534±.0439 .6195±.1055 .6333±.0818 RGIB-REP .9704±.0035 .9645±.0027 .9591±.0036 .9713±.0023 .9654±.0046 .9572±.0047 .9741±.0040 .9745±.0028 .9718±.0039 RGIB-SSL .9507±.0034 .9432±.0034 .9389±.0059 .9526±.0037 .9451±.0045 .9360±.0083 .9619±.0037 .9645±.0042 .9643±.0038\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9659±.0029 .9573±.0036 .9482±.0054 .9644±.0033 .9543±.0075 .9474±.0074 .9722±.0043 .9688±.0055 .9698±.0065 .9461±.0082 .9295±.0063 .9062±.0218 .9469±.0059 .9293±.0087 .9182±.0063 .9698±.0036 .9689±.0046 .9694±.0037 NeuralSparse .9563±.0079 .9391±.0057 .9185±.0157 .9473±.0149 .9338±.0043 .9250±.0037 .9688±.0037 .9705±.0026 .9696±.0063 .9508±.0075 .9359±.0222 .9288±.0153 .9535±.0082 .9424±.0053 .9332±.0043 .9723±.0033 .9717±.0036 .9697±.0046 Co-teaching .9699±.0033 .9627±.0129 .9501±.0141 .9709±.0129 .9569±.0115 .9482±.0155 .9673±.0285 .9698±.0260 .9631±.0176 .9736±.0116 .9570±.0133 .9485±.0076 .9659±.0060 .9555±.0157 .9469±.0086 .9704±.0136 .9701±.0196 .9682±.0078 .9680±.0057 .9607±.0055 .9511±.0106 .9815±.0185 .9563±.0145 .9468±.0100 .9750±.0124 .9732±.0093 .9772±.0120 .9652±.0138 .9666±.0113 .9612±.0165 .9776±.0272 .9610±.0322 .9624±.0365 .9587±.0079 .9514±.0078 .9585±.0138 .9481±.0185 .9310±.0171 .9015±.0169 .9455±.0267 .9490±.0157 .9194±.0239 .9413±.0206 .9370±.0174 .9241±.0144 .5681±.0785 .5107±.0287 .5147±.0361 .5599±.0776 .5046±.0124 .5167±.0502 .5850±.0827 .5864±.0795 .5518±.0404 RGIB-REP .9612±.0042 .9410±.0069 .9258±.0169 .9531±.0054 .9417±.0058 .9313±.0092 .9716±.0049 .9747±.0036 .9707±.0056 RGIB-SSL .9367±.0058 .9233±.0046 .9199±.0053 .9418±.0085 .9236±.0056 .9205±.0050 .9576±.0066 .9562±.0079 .9557±.0083\n\nPeer loss Jaccard GIB SupCon GRACE\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nTable 30: Full results on Chameleon dataset with SAGE.\n\nSAGE\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9683±.0036 .9605±.0038 .9536±.0028 .9754±.0019 .9734±.0015 .9762±.0018 .9700±.0019 .9670±.0026 .9603±.0049 .9604±.0025 .9614±.0028 .9556±.0022 .9677±.0022 .9659±.0030 .9673±.0026 .9630±.0016 .9678±.0033 .9625±.0047 NeuralSparse .9669±.0031 .9584±.0029 .9550±.0029 .9751±.0025 .9735±.0022 .9745±.0021 .9705±.0014 .9661±.0031 .9606±.0051 .9674±.0035 .9594±.0028 .9538±.0019 .9752±.0022 .9637±.0033 .9651±.0029 .9603±.0016 .9658±.0036 .9598±.0042 Co-teaching .9581±.0040 .9549±.0110 .9543±.0093 .9453±.0044 .9425±.0083 .9469±.0089 .9585±.0006 .9595±.0039 .9525±.0309 .9638±.0096 .9581±.0098 .9512±.0021 .9541±.0091 .9561±.0075 .9557±.0104 .9625±.0146 .9506±.0155 .9587±.0227 .9672±.0074 .9650±.0123 .9589±.0056 .9503±.0007 .9540±.0190 .9517±.0015 .9592±.0010 .9522±.0027 .9545±.0118 .9650±.0066 .9593±.0214 .9530±.0021 .9671±.0007 .9606±0.000 .9639±.0275 .9537±.0055 .9523±.0048 .9586±.0068 .9636±.0031 .9656±.0017 .9599±.0041 .9506±.0022 .9585±.0020 .9593±.0019 .9567±.0024 .9530±.0027 .9516±.0037 .8215±.0332 .8327±.0178 .8372±.0245 .8369±.0146 .8251±.0441 .8295±.0422 .8610±.0055 .8583±.0114 .8641±.0137 RGIB-REP .9699±.0028 .9626±.0031 .9577±.0034 .9765±.0022 .9741±.0018 .9770±.0014 .9733±.0017 .9715±.0025 .9670±.0037 RGIB-SSL .9740±.0030 .9709±.0027 .9686±.0025 .9752±.0020 .9737±.0031 .9741±.0035 .9789±.0016 .9791±.0028 .9781±.0029\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9645±.0035 .9580±.0035 .9505±.0010 .9721±.0017 .9721±.0021 .9721±.0016 .9678±.0026 .9641±.0028 .9605±.0051 .9674±.0041 .9585±.0026 .9516±.0024 .9652±.0029 .9638±.0039 .9652±.0019 .9602±.0021 .9663±.0030 .9614±.0038 NeuralSparse .9649±.0022 .9576±.0035 .9495±.0041 .9619±.0032 .9602±.0022 .9620±.0031 .9675±.0017 .9642±.0031 .9603±.0046 .9651±.0036 .9574±.0034 .9506±.0037 .9624±.0027 .9622±.0015 .9630±.0035 .9687±.0024 .9646±.0025 .9596±.0056 Co-teaching .9722±.0090 .9665±.0040 .9582±.0007 .9683±.0083 .9633±.0101 .9613±.0067 .9617±.0255 .9638±.0070 .9637±.0066 .9543±.0099 .9663±.0044 .9496±.0096 .9610±.0110 .9667±.0086 .9601±.0060 .9657±.0035 .9679±.0026 .9661±.0180 .9438±.0076 .9485±.0047 .9444±.0069 .9653±.0115 .9683±.0218 .9697±.0157 .9696±.0043 .9537±.0048 .9685±.0088 .9557±.0177 .9574±.0043 .9538±.0117 .9644±.0214 .9614±.0204 .9559±.0038 .9576±.0026 .9464±.0080 .9303±.0065 .9494±.0130 .9351±.0093 .9263±.0065 .9592±.0122 .9538±.0122 .9512±.0125 .9478±.0099 .9501±.0126 .9449±.0099 .8040±.0364 .8542±.0415 .8266±.0443 .8362±.0272 .8251±.0484 .8642±.0383 .7462±.0412 .7970±.0306 .7892±.0341 RGIB-REP .9665±.0031 .9582±.0035 .9520±.0035 .9740±.0018 .9719±.0028 .9722±.0022 .9690±.0021 .9696±.0023 .9671±.0058 RGIB-SSL .9725±.0027 .9682±.0028 .9655±.0049 .9740±.0021 .9707±.0040 .9721±.0044 .9748±.0023 .9762±.0031 .9754±.0040\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9606±.0046 .9541±.0058 .9470±.0035 .9679±.0031 .9658±.0041 .9640±.0059 .9673±.0026 .9641±.0038 .9625±.0042 .9532±.0028 .9540±.0024 .9487±.0043 .9592±.0028 .9582±.0035 .9580±.0299 .9500±.0014 .9561±.0030 .9537±.0024 NeuralSparse .9525±.0043 .9383±.0365 .9480±.0036 .9597±.0027 .9569±.0037 .9530±.0057 .9589±.0024 .9552±.0025 .9515±.0034 .9536±.0042 .9557±.0038 .9483±.0047 .9706±.0035 .9581±.0040 .9459±.0074 .9580±.0024 .9559±.0021 .9429±.0045 Co-teaching .9503±.0070 .9466±.0113 .9355±.0039 .9492±.0031 .9457±.0059 .9403±.0118 .9445±.0009 .9465±.0055 .9497±.0115 .9553±.0064 .9409±.0086 .9330±.0076 .9697±.0084 .9656±.0110 .9652±.0066 .9521±.0210 .9595±.0195 .9589±.0047 .9416±.0113 .9535±.0116 .9535±.0111 .9610±.0042 .9568±.0210 .9463±.0085 .9518±.0057 .9421±.0032 .9433±.0137 .9609±.0173 .9579±.0050 .9568±.0070 .9691±.0315 .9698±.0037 .9592±.0279 .9433±.0065 .9366±.0074 .9347±.0115 .9310±.0107 .9134±.0087 .9178±.0101 .9291±.0114 .9244±.0271 .9181±.0281 .9342±.0132 .9373±.0116 .9313±.0087 .7479±.0742 .7756±.0708 .7718±.0880 .7275±.0906 .7914±.0755 .8109±.0655 .7754±.0258 .7629±.0508 .7686±.0437 RGIB-REP .9636±.0022 .9558±.0032 .9455±.0038 .9703±.0022 .9641±.0075 .9607±.0050 .9683±.0013 .9681±.0033 .9664±.0042 RGIB-SSL .9665±.0030 .9601±.0045 .9563±.0036 .9662±.0052 .9654±.0040 .9610±.0032 .9699±.0022 .9716±.0033 .9698±.0031\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 31: Full results on Squirrel dataset with GCN.\n\nGCN\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9621±.0018 .9519±.0020 .9444±.0024 .9610±.0028 .9490±.0031 .9401±.0036 .9744±.0013 .9731±.0010 .9722±.0011 .9571±.0015 .9480±.0032 .9370±.0028 .9587±.0011 .9486±.0026 .9349±.0026 .9623±.0015 .9612±.0008 .9604±.0014 NeuralSparse .9611±.0019 .9528±.0027 .9409±.0023 .9621±.0017 .9495±.0016 .9364±.0022 .9741±.0013 .9724±.0018 .9722±.0010 .9612±.0018 .9513±.0026 .9424±.0024 .9619±.0016 .9488±.0035 .9363±.0022 .9744±.0011 .9723±.0015 .9718±.0009 Co-teaching .9701±.0076 .9535±.0044 .9465±.0059 .9599±.0097 .9525±.0120 .9454±.0109 .9629±.0047 .9666±.0304 .9631±.0004 .9610±.0015 .9608±.0057 .9451±.0097 .9636±.0107 .9491±.0125 .9436±.0091 .9694±.0012 .9628±.0134 .9676±.0149 .9613±.0027 .9568±.0046 .9509±.0014 .9622±.0159 .9594±.0111 .9397±.0119 .9528±.0017 .9405±.0089 .9401±.0010 .9522±.0195 .9478±.0094 .9489±.0159 .9494±.0260 .9461±.0091 .9432±.0068 .9457±.0099 .9462±.0017 .9492±.0060 .9494±.0013 .9439±.0009 .9458±.0024 .9421±.0008 .9549±.0013 .9489±.0030 .9444±.0011 .9423±.0015 .9318±.0009 .9292±.0011 .9316±.0010 .9330±.0013 .9290±.0012 .9317±.0010 .9327±.0016 .9054±.0018 .9058±.0015 .9052±.0018 RGIB-REP .9614±.0019 .9554±.0026 .9471±.0035 .9611±.0021 .9512±.0032 .9392±.0043 .9761±.0015 .9753±.0011 .9745±.0015 RGIB-SSL .9571±.0019 .9476±.0029 .9402±.0020 .9534±.0016 .9486±.0027 .9451±.0031 .9752±.0023 .9749±.0019 .9738±.0023\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9432±.0036 .9406±.0031 .9386±.0025 .9416±.0042 .9395±.0011 .9411±.0040 .9720±.0016 .9720±.0016 .9710±.0021 .9439±.0063 .9377±.0024 .9365±.0045 .9426±.0042 .9376±.0033 .9358±.0023 .9608±.0008 .9603±.0013 .9698±.0016 NeuralSparse .9494±.0065 .9309±.0028 .9297±.0033 .9469±.0047 .9403±.0028 .9417±.0062 .9633±.0009 .9626±.0016 .9625±.0016 .9485±.0056 .9326±.0046 .9304±.0048 .9469±.0048 .9400±.0030 .9379±.0026 .9633±.0018 .9623±.0012 .9626±.0012 Co-teaching .9461±.0099 .9352±.0064 .9374±.0051 .9462±.0101 .9425±.0032 .9306±.0053 .9675±.0292 .9641±.0115 .9655±.0264 .9457±.0117 .9345±.0117 .9286±.0044 .9362±.0038 .9386±.0058 .9336±.0043 .9636±.0012 .9694±.0033 .9696±.0006 .9443±.0047 .9327±.0079 .9244±.0054 .9388±.0179 .9345±.0146 .9240±.0055 .9529±.0029 .9512±.0089 .9501±.0120 .9472±.0127 .9329±.0155 .9302±.0202 .9390±.0045 .9406±.0196 .9397±.0249 .9641±.0025 .9628±.0032 .9601±.0042 .9473±.0010 .9348±.0017 .9301±.0029 .9372±.0010 .9343±.0037 .9305±.0033 .9516±.0042 .9595±.0011 .9511±.0040 .9394±.0008 .9380±.0007 .9363±.0016 .9392±.0011 .9378±.0008 .9363±.0017 .9171±.0019 .9174±.0014 .9166±.0013 RGIB-REP .9509±.0046 .9455±.0071 .9434±.0059 .9495±.0053 .9432±.0059 .9405±.0050 .9735±.0013 .9733±.0015 .9737±.0009 RGIB-SSL .9499±.0040 .9426±.0019 .9425±.0021 .9479±.0051 .9429±.0017 .9429±.0024 .9727±.0015 .9729±.0019 .9726±.0011\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9484±.0049 .9429±.0038 .9408±.0039 .9489±.0057 .9408±.0021 .9386±.0022 .9688±.0028 .9675±.0027 .9656±.0034 .9460±.0063 .9375±.0028 .9349±.0017 .9474±.0057 .9409±.0030 .9365±.0022 .9669±.0022 .9652±.0031 .9638±.0038 NeuralSparse .9508±.0049 .9429±.0047 .9391±.0029 .9532±.0060 .9412±.0044 .9388±.0036 .9681±.0020 .9647±.0018 .9671±.0019 .9535±.0047 .9451±.0050 .9395±.0033 .9514±.0041 .9432±.0068 .9380±.0020 .9690±.0023 .9667±.0035 .9649±.0025 Co-teaching .9480±.0144 .9454±.0074 .9463±.0076 .9538±.0051 .9424±.0048 .9379±.0012 .9672±.0179 .9694±.0283 .9654±.0195 .9539±.0051 .9420±.0127 .9405±.0060 .9445±.0139 .9446±.0012 .9462±.0081 .9684±.0182 .9639±.0188 .9624±.0189 .9519±.0138 .9459±.0100 .9398±.0115 .9483±.0070 .9429±.0203 .9428±.0105 .9697±.0074 .9687±.0062 .9673±.0077 .9447±.0103 .9459±.0026 .9410±.0108 .9431±.0289 .9413±.0301 .9386±.0238 .9633±.0041 .9619±.0114 .9612±.0064 .9590±.0023 .9507±.0019 .9463±.0028 .9580±.0024 .9519±.0018 .9464±.0026 .9638±.0051 .9524±.0048 .9419±.0012 .9399±.0007 .9372±.0006 .9348±.0017 .9397±.0012 .9370±.0009 .9349±.0017 .9213±.0017 .9218±.0013 .9211±.0014 RGIB-REP .9556±.0028 .9492±.0060 .9463±.0063 .9570±.0018 .9496±.0060 .9420±.0056 .9677±.0029 .9676±.0031 .9662±.0018 RGIB-SSL .9462±.0029 .9471±.0020 .9448±.0032 .9509±.0032 .9474±.0017 .9449±.0030 .9642±.0012 .9659±.0027 .9647±.0010\n\nPeer loss Jaccard GIB SupCon GRACE\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nTable 32: Full results on Squirrel dataset with GAT.\n\nGAT\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9680±.0007 .9635±.0017 .9588±.0025 .9702±.0008 .9690±.0010 .9659±.0014 .9719±.0018 .9701±.0017 .9686±.0012 .9619±.0014 .9567±.0020 .9505±.0022 .9627±.0008 .9593±.0014 .9551±.0012 .9684±.0015 .9662±.0026 .9661±.0018 NeuralSparse .9656±.0009 .9602±.0012 .9533±.0019 .9662±.0011 .9629±.0010 .9584±.0017 .9709±.0012 .9692±.0014 .9685±.0009 .9655±.0011 .9602±.0012 .9546±.0024 .9668±.0010 .9630±.0015 .9603±.0021 .9610±.0015 .9698±.0012 .9689±.0016 Co-teaching .9566±.0034 .9562±.0066 .9516±.0026 .9505±.0045 .9509±.0092 .9509±.0046 .9591±.0293 .9880±.0174 .9360±.0089 .9720±.0097 .9726±.0039 .9682±.0106 .9706±6.649 .9715±.0064 .9704±.0107 .9681±.0121 .9635±.0069 .9683±.0116 .9715±.0098 .9651±.0053 .9648±.0094 .9807±.0202 .9768±.0041 .9645±.0174 .9637±.0081 .9668±.0104 .9627±.0002 .9727±.0019 .9820±.0058 .9610±.0069 .9780±0.001 .9965±0.000 .9838±.0123 .9660±.0067 .9611±.0022 .9679±.0061 .9471±.0019 .9386±.0022 .9353±.0026 .9539±.0032 .9463±.0026 .9411±.0034 .9629±.0028 .9631±.0019 .9636±.0021 .7300±.1193 .7583±.1135 .7015±.1383 .7494±.1101 .6903±.1807 .7032±.1659 .8109±.0382 .7780±.1044 .7608±.0620 RGIB-REP .9675±.0014 .9631±.0012 .9587±.0023 .9692±.0007 .9660±.0013 .9632±.0016 .9735±.0012 .9719±.0013 .9715±.0010 RGIB-SSL .9453±.0020 .9421±.0017 .9385±.0022 .9418±.0024 .9394±.0033 .9362±.0023 .9726±.0007 .9705±.0037 .9695±.0024\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9581±.0046 .9436±.0063 .9335±.0062 .9592±.0047 .9455±.0075 .9415±.0061 .9682±.0030 .9690±.0028 .9686±.0021 .9501±.0041 .9340±.0046 .9234±.0059 .9476±.0054 .9354±.0067 .9292±.0039 .9529±.0047 .9513±.0054 .9510±.0037 NeuralSparse .9524±.0041 .9383±.0046 .9320±.0044 .9401±.0071 .9377±.0045 .9334±.0041 .9559±.0031 .9555±.0032 .9556±.0022 .9416±.0054 .9310±.0059 .9302±.0032 .9447±.0031 .9393±.0053 .9305±.0131 .9574±.0039 .9557±.0025 .9552±.0042 Co-teaching .9324±.0109 .9024±.0120 .9029±.0126 .9421±.0074 .9345±.0126 .9371±.0091 .9576±.0124 .9592±.0064 .9551±.0087 .9520±.0071 .9506±.0145 .9370±.0140 .9560±.0063 .9451±.0086 .9448±.0142 .9548±.0128 .9500±.0088 .9593±.0134 .9561±.0133 .9423±.0160 .9396±.0117 .9578±.0185 .9583±.0256 .9558±.0050 .9572±.0074 .9572±.0083 .9554±.0089 .9353±.0080 .9271±.0093 .9098±.0079 .9545±.0156 .9504±.0240 .9530±.0348 .9544±.0023 .9549±.0073 .9524±.0060 .9397±.0044 .9322±.0038 .9266±.0047 .9365±.0039 .9340±.0021 .9314±.0029 .9574±.0019 .9557±.0065 .9556±.0049 .5710±.1110 .5658±.1143 .6203±.1324 .5694±.1202 .5771±.0945 .6021±.1203 .5291±.0694 .5097±.0139 .5029±.0072 RGIB-REP .9541±.0059 .9432±.0059 .9316±.0063 .9526±.0037 .9401±.0072 .9389±.0056 .9681±.0028 .9672±.0029 .9660±.0048 RGIB-SSL .9347±.0038 .9321±.0031 .9260±.0032 .9321±.0022 .9327±.0020 .9270±.0028 .9592±.0031 .9651±.0014 .9641±.0021\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9507±.0050 .9309±.0164 .9254±.0089 .9487±.0065 .9419±.0041 .9255±.0073 .9585±.0097 .9520±.0070 .9507±.0162 .9347±.0049 .9279±.0118 .9181±.0078 .9368±.0082 .9262±.0076 .9175±.0127 .9412±.0092 .9431±.0076 .9444±.0090 NeuralSparse .9399±.0088 .9279±.0167 .9224±.0051 .9423±.0063 .9342±.0023 .9213±.0115 .9468±.0118 .9474±.0102 .9504±.0051 .9433±.0105 .9348±.0072 .9213±.0100 .9460±.0048 .9340±.0052 .9273±.0094 .9482±.0088 .9541±.0092 .9453±.0077 Co-teaching .9219±.0059 .9329±.0231 .9229±.0185 .9392±.0093 .9352±.0098 .9261±.0100 .9496±.0383 .9445±.0272 .9490±.0284 .9300±.0136 .9316±.0220 .9349±.0147 .9271±.0119 .9170±.0117 .9057±.0127 .9230±.0221 .9341±.0113 .9187±.0325 .9577±.0112 .9374±.0225 .9297±.0158 .9469±.0111 .9406±.0111 .9309±.0168 .9393±.0153 .9390±.0151 .9314±.0223 .9504±.0239 .9475±.0221 .9445±.0127 .9484±.0211 .9487±.0139 .9438±.0307 .9421±.0133 .9484±.0101 .9486±.0206 .9269±.0089 .9174±.0092 .9107±.0052 .9278±.0057 .9223±.0054 .9210±.0038 .9140±.0072 .9173±.0085 .9182±.0081 .5003±.0009 .5427±.0961 .6004±.1210 .5657±.1221 .6023±.0931 .5342±.0684 .5064±.0145 .5017±.0031 .5074±.0190 RGIB-REP .9404±.0067 .9248±.0161 .9218±.0051 .9437±.0084 .9299±.0070 .9238±.0063 .9473±.0071 .9486±.0073 .9397±.0045 RGIB-SSL .9286±.0033 .9241±.0055 .9165±.0055 .9276±.0023 .9245±.0015 .9257±.0037 .9593±.0043 .9586±.0027 .9499 ±.0067\n\nPeer loss Jaccard GIB SupCon GRACE\n\nTable 33: Full results on Squirrel dataset with SAGE.\n\nSAGE\n\nlayers\n\nmethod\n\n20%\n\nmixed noise 40%\n\n60%\n\n20%\n\ninput noise 40%\n\n60%\n\n20%\n\nlabel noise 40%\n\n60%\n\nL=2\n\nL=4\n\nL=6\n\nPTDNet\n\nStandard DropEdge\n\n.9680±.0015 .9626±.0015 .9570±.0016 .9737±.0010 .9736±.0010 .9721±.0011 .9692±.0015 .9643±.0012 .9606±.0017 .9657±.0011 .9602±.0015 .9535±.0020 .9622±.0010 .9625±.0009 .9609±.0020 .9593±.0009 .9530±.0009 .9593±.0007 NeuralSparse .9572±.0008 .9517±.0012 .9562±.0019 .9534±.0007 .9534±.0010 .9521±.0016 .9589±.0012 .9534±.0009 .9591±.0010 .9578±.0009 .9513±.0013 .9559±.0021 .9630±.0010 .9504±.0007 .9524±.0010 .9596±.0014 .9529±.0010 .9596±.0019 Co-teaching .9526±.0031 .9560±.0071 .9526±.0029 .9403±.0050 .9585±.0061 .9585±.0101 .9533±.0022 .9533±.0306 .9539±.0026 .9552±.0079 .9507±.0050 .9588±.0066 .9512±.0052 .9534±.0019 .9599±.0013 .9525±.0105 .9555±.0178 .9510±.0187 .9658±.0111 .9619±.0068 .9630±.0027 .9600±.0189 .9649±.0173 .9617±.0171 .9620±.0105 .9608±.0068 .9603±.0044 .9656±.0079 .9660±.0077 .9661±.0060 .9632±.0260 .9517±.0213 .9565±.0010 .9565±.0009 .9593±.0002 .9548±.0111 .9628±.0015 .9541±.0019 .9409±.0040 .9585±.0061 .9585±.0101 .9533±.0022 .9533±.0306 .9539±.0026 .9400±.0021 .8543±.0076 .8572±.0094 .8528±.0166 .8563±.0068 .8601±.0087 .8528±.0075 .8477±.0119 .8459±.0105 .8452±.0141 RGIB-REP .9701±.0010 .9647±.0014 .9597±.0018 .9757±.0009 .9747±.0007 .9737±.0011 .9735±.0010 .9707±.0007 .9682±.0016 RGIB-SSL .9536±.0016 .9480±.0027 .9448±.0030 .9734±.0014 .9739±.0015 .9803±.0024 .9765±.0021 .9702±.0015 .9694±.0005\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9584±.0107 .9577±.0076 .9541±.0021 .9637±.0092 .9630±.0079 .9607±.0090 .9663±.0020 .9612±.0049 .9612±.0020 .9518±.0030 .9577±.0021 .9515±.0033 .9656±.0051 .9612±.0093 .9573±.0111 .9683±.0020 .9628±.0014 .9608±.0012 NeuralSparse .9604±.0083 .9570±.0040 .9513±.0040 .9644±.0066 .9599±.0084 .9605±.0061 .9652±.0035 .9632±.0023 .9605±.0021 .9624±.0050 .9581±.0039 .9536±.0033 .9667±.0045 .9668±.0057 .9649±.0052 .9675±.0043 .9635±.0040 .9607±.0026 Co-teaching .9518±.0181 .9468±.0093 .9479±.0024 .9525±.0086 .9577±.0159 .9528±.0137 .9625±.0244 .9610±.0220 .9675±.0097 .9632±.0196 .9662±.0085 .9602±.0088 .9674±.0145 .9626±.0120 .9643±.0145 .9637±.0004 .9617±.0170 .9691±.0016 .9622±.0135 .9606±.0157 .9587±.0105 .9647±.0084 .9630±.0253 .9640±.0278 .9505±.0026 .9508±.0063 .9599±.0039 .9527±.0122 .9538±.0275 .9531±.0174 .9603±.0081 .9620±.0115 .9580±.0330 .9598±.0031 .9523±.0064 .9559±.0091 .9478±.0042 .9412±.0059 .9342±.0039 .9267±.0045 .9368±.0028 .9249±.0026 .9375±.0034 .9335±.0012 .9317±.0021 .8859±.0115 .8751±.0085 .8609±.0212 .8824±.0113 .8805±.0261 .8633±.0178 .8386±.0130 .8466±.0103 .8347±.0180 RGIB-REP .9611±.0058 .9578±.0035 .9536±.0037 .9682±.0049 .9667±.0051 .9584±.0074 .9689±.0014 .9664±.0027 .9635±.0022 RGIB-SSL .9498±.0032 .9443±.0047 .9422±.0044 .9531±.0071 .9599±.0023 .9551±.0019 .9623±.0021 .9603±.0087 .9601±.0091\n\nPeer loss Jaccard GIB SupCon GRACE\n\nPTDNet\n\nStandard DropEdge\n\n.9555±.0065 .9528±.0038 .9461±.0054 .9592±.0053 .9600±.0036 .9551±.0042 .9574±.0192 .9540±.0207 .9583±.0023 .9544±.0069 .9411±.0200 .9463±.0041 .9566±.0070 .9473±.0172 .9519±.0052 .9464±.0023 .9402±.0084 .9396±.0025 NeuralSparse .9563±.0060 .9515±.0036 .9457±.0054 .9591±.0046 .9460±.0052 .9415±.0061 .9371±.0025 .9344±.0017 .9319±.0033 .9565±.0060 .9524±.0050 .9463±.0056 .9593±.0045 .9584±.0044 .9492±.0038 .9384±.0023 .9342±.0018 .9236±.0018 Co-teaching .9423±.0071 .9396±.0126 .9251±.0137 .9382±.0063 .9433±.0054 .9388±.0088 .9344±.0232 .9496±.0204 .9374±.0083 .9473±.0108 .9371±.0040 .9333±.0084 .9339±.0053 .9312±.0058 .9331±.0066 .9396±.0211 .9375±.0195 .9335±.0035 .9335±.0058 .9305±.0057 .9225±.0146 .9309±.0059 .9386±.0084 .9433±.0147 .9348±.0200 .9210±.0285 .9273±.0029 .9437±.0258 .9323±.0155 .9204±.0182 .9492±.0239 .9406±.0137 .9322±.0335 .9407±.0269 .9364±.0274 .9394±.0079 .9392±.0049 .9330±.0098 .9221±.0138 .9266±.0024 .9313±.0102 .9309±.0052 .9264±.0021 .9202±.0084 .9196±.0021 .8418±.0983 .8470±.0823 .8732±.0151 .8747±.0151 .8126±.1348 .8716±.0148 .8385±.0097 .8370±.0295 .8487±.0121 RGIB-REP .9558±.0047 .9474±.0066 .9455±.0052 .9583±.0044 .9566±.0035 .9447±.0030 .9662±.0057 .9646±.0021 .9634±.0030 RGIB-SSL .9437±.0026 .9391±.0036 .9366±.0042 .9441±.0064 .9549±.0012 .9469±.0012 .9678±.0031 .9613±.0076 .9601±.0041\n\nPeer loss Jaccard GIB SupCon GRACE\n\n38",
    "reference": "# Summary Of The Paper\n\nA method to handle graph-edge noise is introduced based on an information-bottleneck theory. It is shown that adding edge noise to real data will increase the link prediction error. The introduced method is shown to reduce this error.\n\n# Strength And Weaknesses\n\nAddressing noisy edges in a graph is a sensible and practical direction of research, as it is not often considered, is likely to be present in real data and will reduce model performance error. This paper demonstrates that errors increase for the link prediciotn task with synthetically added edges to public datasets.\n\nHowever, it is many parts of this paper are unclear, how noise is added, what a graph illustration is trying to show, what the x and y values represetn, what values in a table represent, how well this approach works on the original data without synthetically added edges and more. This paper needs to explain more clearly many parts of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nCL.\n\nFig 1. a shows two illustrations of the same set of nodes; on the left the solid lines (clean edges) are connecting one set of the nodes, and on the right they are connecting another set; the same with the blue dashed line (noisy edges). The left graph has a title noisy input A_tilde and the right noisy labels Y_tilde. I do not understand what is the input and what are the labels here? An input graph is in my understanding a set of nodes and edges, this can also include labels (class labels) for each node but why the edges on the right are changed completely I do not follow.\n\nV = {v i } but i is from 1 to ?\n\nTab. 1 has some interesting plots, which seem to indicate that the signal is lost as more randomly generated edges are added to the graph. However, it is unclear what the x and y axis are, it's unclear what dimension H_1 and H_2 are, it's unclear why both A_1 and A_2 are perturbed, I would imagine one would be clean and the other has the noise added? In the table with the values .616, .445 etc.. are representing what exactly, alignment maybe, but how is this quantified?\n\nQU.\n\n\"Other methods (Rong et al., 2020; Zheng et al., 2020; Luo et al., 2021) achieve a similar goal by actively selecting the informative nodes or edges and pruning the task-irrelevant ones.\" I am familiar with Rong et al. and they do not actively select the informative edges, they do random sampling of the edges at each epoch so this sentence is inaccurate. Also this reference is incomplete and has a typo: \"Y. Rong, W. Huang, T. Xu, and J. Huang. Dropedge: Towards volutional networks on node classification. In ICLR, 2020.\".\n\n\"Since existing benchmarks are usually well-annotated and clean, there is a need to simulate the inherent edge noise properly to investigate the impact of noise.\" Can you substantiate this claim?\n\nIn Table 2 including the clean graph results will give a reference to how well the performance can be with no randomly added edges and can also show if the proposed method can improve on the results of 'clean' data, which is likely to have false positive edges even without artificially added ones.\n\nNO.\n\nIt's not clear what additions are made to GIB to improve it's robustness.\n\nRE.\n\nIt's not clear how the edges are added, I assume the additional synthetically added noisy edges simply sampled uniformly over all candidate edges. Also, the updates to the labels is not clear in this experiment?\n\n# Summary Of The Review\n\nSadly this paper is too unclear to follow and understand. As well as addressing the above pointed out issues, I recommend focusing on explaining more clearly the edge data generation steps, the contribution added starting from the GIB work, and explaining why these additions improve on the existing work. I look forward to see the revised version as this is a very intersting research area.\n\n[After Review Period]\n\nThe paper has a very nice contribution with respect to the motivation [identify and remove noise from graphs for link prediction] and the intuition behind the idea [extend GIB to separate the label and feature noise], and the paper is clearly structured. Many of the concerns have been addressed and modified in the paper and so I feel that the paper is closer to being ready for publication. It is still unclear how the claim that the public datasets either do not have noise, or that this noise is always FPs. Another reviewer commented that typically the noise in graphs are missing edges, which contradicts the authors comment. The experiments on the public dataset without added noise also show improvements with their method, implying that there must be some noise. This leaves a number of unanswered questions about the noise in the data. Maybe the authors can inspect samples of the edges being updated by RGIB in the clean datasets to see if qualitatively they can start to investigate these questions. Also, could the synthetically generate data to analyse the behaviour of their method? Due to this and the number of initially found issues I believe this is still below the acceptance level, although it has improved during the review period.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPARTIAL TRANSPORTABILITY FOR DOMAIN GENERALIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLearning prediction models that generalize to related domains is one of the most fundamental challenges in artificial intelligence. There exists a growing literature that argues for learning invariant associations using data from multiple source domains. However, whether invariant predictors generalize to a given target domain depends crucially on the assumed structural changes between domains. Using the perspective of transportability theory, we show that invariance learning, and the settings in which invariant predictors are optimal in terms of worst-case losses, is a special case of a more general partial transportability task. Specifically, the partial transportability task seeks to identify / bound a conditional expectation EP ̊rY | xs in an unseen domain π ̊ using knowledge of qualitative changes across domains in the form of causal graphs and data from source domains π1, . . . , πk. We show that solutions to this problem have a much wider generalization guarantee that subsumes those of invariance learning and other robust optimization methods that are inspired by causality. For computations in practice, we develop an algorithm that provably provides tight bounds asymptotically in the number of data samples from source domains for any partial transportability problem with discrete observables and illustrate its use on synthetic datasets.\n\n1\n\nINTRODUCTION\n\nGeneralization guarantees are central to the design of reliable machine learning models as the predictions and conclusions obtained in one or several source domains π1, . . . , πk (e.g. in controlled laboratory circumstances, from a specific study or population, etc.) are transported and applied elsewhere, in a domain π ̊ that may differ in several aspects from that of source domains. It is apparent that what structure and what assumptions are imposed on the relationship between domains determines whether a model will generalize as intended. For example, if the target environment is arbitrary, or substantially different from the study environment, transporting predictions is difficult or even impossible.\n\nA structural account of causation provides suitable semantics for reasoning about the structural invariances across different domains, and has been studied under the umbrella of transportability theory (Pearl & Bareinboim, 2011; Bareinboim et al., 2013; Bareinboim & Pearl, 2016). Each domain πi is associated with a different structural causal model (SCM) M i that differs in one or more of its component parts with respect to other domains and defines different distributions over the observed variables. In practice, the SCMs are usually not fully observable, which leads to the transportability challenge of using data from one (or more) SCMs to make inference about distributions from another SCM. A query, e.g. EP ̊rY | xs, is said to be point identified if it can be uniquely computed given available data (from one or more domains) and qualitative knowledge about the causal changes between domains in the form of selection diagrams. However, in problems of transportability, especially when no data in the target domain can be collected, the combination of qualitative assumptions and data often does not permit one to uniquely determine a given query, which is said to be non-identifiable. In such cases, partial identification methods deal with bounding a given query e.g. l ă EP ̊rY | xs ă u in non-identifiable problems and may still serve an informative purpose for decision-making if 0 ă l ă u ă 1. Both settings have been studied in the literature. In particular, there exists an extensive set of graphical conditions and algorithms for the identifiability of observational, interventional, and counterfactuals distributions across domains from a combination of\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ndatasets in various settings (Pearl & Bareinboim, 2011; Bareinboim et al., 2013; Bareinboim & Pearl, 2014; 2016; Lee et al., 2020; Correa & Bareinboim, 2019). For example, Lee et al. (2020) investigate the transportability of conditional causal effects, while Correa & Bareinboim (2020) investigate the transportability of soft interventions or policies, from an arbitrary combination of datasets collected under different conditions. Several methods exist also for partial identification of causal effects and counterfactuals (Balke & Pearl, 1997; Chickering & Pearl, 1996; Zhang et al., 2021) that aim at bounding insead of point-identifying a particular causal effect. Despite the generality of these results, there is still no treatment or algorithms for the partial identification of transportability queries.\n\nIn the machine learning literature, notably, a version of the transportability task is also widely studied as the problem of domain generalization (Wang et al., 2022). The objective is to learn a prediction function with a minimum performance guarantee on any distribution in some uncertainty set that includes potential test / target distributions (Ben-Tal et al., 2009; Gulrajani & Lopez-Paz, 2020). This problem has implicit connections to causality and SCMs if uncertainty sets of distributions are defined on the basis of \"invariant correlations\", such as stable conditional expectations EP 1rY | xs “ ̈ ̈ ̈ “ EP k rY | xs across training domains π1, . . . , πk, to be used for prediction in a target domain π ̊ and that may be learned from data sampled across sufficiently many different domains with statistical tests (Peters et al., 2016; Subbaswamy et al., 2019; Subbaswamy & Saria, 2020) or custom loss functions (Magliacane et al., 2018; Arjovsky et al., 2019; Rojas-Carulla et al., 2018; Bellot & van der Schaar, 2020). For instance, Arjovsky et al. (2019) argue for learning representations that define an invariant optimal classifier across several training datasets. Subbaswamy et al. (2019); Subbaswamy & Saria (2020) use causal graphs and identifiable interventional distributions to define invariant prediction rules across domains. Notwithstanding their wide applicability, there is little theoretical understanding of the extrapolation guarantees that can be expected from invariant prediction rules given a finite set of domains. Correlations invariant across source domains need not be invariant in a target domain; and performance guarantees, in general, depend on the structural invariances assumed for their respective SCMs.\n\nIn this paper, we start by describing the conditions under which invariant prediction rules can be expected to perform well in an arbitrary target domain from first principles using the semantics of structural causal models (Pearl, 2009; Pearl & Bareinboim, 2011). We then introduce a broader optimization problem – the task of partial transportability – whose objective is to bound, instead of point estimate, a query in an arbitrary target domain of interest, such as EP ̊rY | xs, given data from one or more source domains and qualitative knowledge about the causal changes between domains in the form of selection diagrams. We demonstrate that solutions to this problem subsume various instantiations of invariant predictors (in the conditions where these are adequate) and have a wider distributional robustness guarantee to any distribution in the target domain that is compatible with the assumed selection diagrams. For computations in practice, we show that the partial transportability task can be solved approximately for systems of variables with finite domains with a Markov Chain Monte Carlo sampling approach. The resulting bounds are sound and tight, and provide the most informative inference on a target query given the available information.\n\n1.1 PRELIMINARIES\n\nWe introduce in this section some basic notations and definitions that will be used throughout the paper. We use capital letters to denote variables (X), small letters for their values (x), bold letters for sets of variables (X) and their values (x), and use Ω to denote their domains of definition (x P ΩX ). A conditional independence statement in distribution P is written as pX KK Y | ZqP . A d-separation statement in some graph G is written as pX KK Y | ZqG. For convenience, we denote by P pxq probabilities P pX “ xq, and 1t ̈u for the indicator function equal to 1 if the statement in t ̈u evaluates to true, and equal to 0 otherwise. All proofs are given in the Appendix.\n\nWe use the language of structural causal models (SCMs) (Definition 7.1.1 (Pearl, 2009)) to define the semantics of causality. An SCM M is a tuple M “ xV, U, F, P y where V is a set of endogenous variables and U is a set of exogenous variables. Each exogeneous variable U P U is distributed according to a probability measure P puq. F is a set of functions where each fV P F determines the deterministic dependencies of V on other parts of the system. That is, v :“ fV ppaV , uV q, with PaV Ă V, and UV Ă U, the exogeneous sources of variation that influence V . With this construction, we define the potential response Vpuq to be the solution of V in the model M given U “ u. Moreover, drawing values of exogenous variables U following the probability measure P\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nX2\n\nX1 W\n\nY\n\n(a) G ̊\n\nX2\n\nSX1\n\nX2\n\nX1 W SW\n\nY\n\n(b) G ̊,a\n\nY\n\nX1 W SW\n\n(c) G ̊,b\n\nSX1\n\nX2\n\nX1 W\n\nY\n\n(d) Ga,b\n\nFigure 1: Example of graphs: (a) Causal graph of target domain π ̊, (b) selection diagram that compares domains π ̊ with πa, (c) selection diagram that compares domains π ̊ with πb, (d) selection diagram that compares domains πa with πb.\n\ninduces a joint distribution over observables given by,\n\nż\n\nź\n\nP pvq “\n\nΩU\n\nV PV\n\n1tfV ppaV , uV q “ vudP puq.\n\n(1)\n\nAn SCM induces a causal graph G in which each variable in V is associated with a node; we draw a directed edge between two variables X Ñ Y if X P PaY appears as an argument of fY in the SCM, and a bidirected arrow X Ø Y if UX X UY ‰ H, that is X and Y share an unobserved confounder. The set of parent nodes of X in G is denoted by papXqG “ XPX papXqG. Its capitalized version Pa includes the argument as well, e.g. PapXqG “ papXqG Y X. Similar definitions are used for children ch, descendants de, etc.\n\nŤ\n\n2 DOMAIN GENERALIZATION THROUGH THE LENS OF TRANSPORTABILITY\n\nWe adopt the setting of domain generalization. We assume access to k source domains π1, π2, . . . , πk with associated data distributions over a common set of variables V denoted P 1pvq, P 2pvq, . . . , P kpvq. Our focus is on a query, such as EP ̊rY | xs, to be evaluated in a target domain π ̊ (potentially) different from source domains, where typically Y is an outcome variable, X is a set of covariates, and Y Y X “ V.\n\nFor concreteness, consider a medical study where patient data was collected under different treatment protocols in an attempt to assess, in a target hospital π ̊, the prognosis of neurodegenerative diseases such as Alzheimer’s in patients with a number of existing conditions. In the causal graph in Fig. 1a, X1 and X2 are treatments for hypertension and clinical depression, respectively, both known to be causes of neurodegenerative diseases Y . In the case of hypertension, the effect is mediated by blood pressure W , whose effect on neurodegenerative diseases is confounded, since both conditions share important confounding factors such as physical activity levels and diet patterns (Skoog & Gustafson, 2006) (graphically encoded through the bidirected arrows). Hypertension and clinical depression are not known to affect each other (no direct link between them), although it’s common for patients with clinical depression to simultaneously be at risk of hypertension (Meng et al., 2012). In this example, we have access to an observational study conducted in a hospital πa, and to hospital πb following different guidelines for the administration of X1, both of which however are known to have a different high blood pressure W incidence than that in π ̊. These differences are called domain discrepancies (Pearl & Bareinboim, 2011).\n\nDefinition 1 (Domain Discrepancy, (Pearl & Bareinboim, 2011)). Let πa and πb be domains associated, respectively, with SCMs M a and M b and causal diagrams Ga and Gb. We denote by ∆a,b Ă V a set of variables such that, for every Vi P ∆a,b, there might exist a discrepancy if f a or P apUiq ‰ P bpUiq.\n\nVi ‰ f b\n\nVi\n\nDefinition 2 (Selection diagram, (Pearl & Bareinboim, 2011)). Given domain discrepancies ∆a,b between two domains πa and πb and a causal graph Ga “ pV, Eq, let S “ tSV : V P ∆a,bu be called selection nodes. Then, a selection diagram Ga,b is defined as a graph pV Y S, E Y tSV Ñ V uSV PSq.\n\nSelection nodes locate the mechanisms where structural discrepancies between the two domains are suspected to take place. The absence of a selection node pointing to a variable represents the assumption that the mechanism responsible for assigning value to that variable is identical in both domains. In the medical example above, Fig. 1b shows a selection diagram comparing domains πa and π ̊ in which the SW node indicates a structural difference in the assignment of W , either\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nX2\n\nX3\n\nSX1\n\nX1\n\nY\n\nSX1\n\nX4\n\nX5\n\n(a) Ga,b\n\nX1\n\nX2\n\nY\n\nSX1\n\nSX2\n\nX1\n\nX2\n\nY\n\nSX1\n\nSX2\n\nX1\n\nX2\n\nY\n\n(b) G1,2\n\n(c) G ̊,1\n\n(d) G ̊,2\n\nFigure 2: Graphs used in Sec. 2.1.\n\nW and / or P ̊puW q ‰ P apuW q, but not in the assignment of other variables, for instance Y and P ̊puY q “ P apuY q. Fig. 1c and Fig. 1d are selection diagrams that compare domains\n\nW ‰ f a f ̊ Y “ f a f ̊ pπb, π ̊q and pπa, πbq respectively.\n\n2.1\n\nINVARIANCE LEARNING FOR DOMAIN GENERALIZATION\n\nIt is apparent that there is a degree of unidentifiability in optimal prediction rules in a target domain depending on the structural differences between it and the available data. A natural objective for a chosen prediction function is to minimize worst-case losses over an uncertainty set of potential target distributions that are compatible with a set of selection diagrams tGi, ̊ : i “ 1, . . . , ku\n\narg min f\n\nmax M PMpG ̊q\n\nEP M rpY ́ f pXqq2s,\n\n(2)\n\nwhere MpG ̊q is the family of SCMs compatible with the causal graph G ̊. In the literature on domain generalization, selection diagrams tGi, ̊ : i “ 1, . . . , ku are mostly implicit, and it is common to define predictors agnostic of assumptions on the underlying causal structure of the target domain, and instead exploit invariances with respect to source domains, see e.g. the proposals of (Arjovsky et al., 2019; Peters et al., 2016; Lu et al., 2021; Rojas-Carulla et al., 2018; Magliacane et al., 2018). This section studies the generalization guarantees of a common class of invariant predictors in the language of selection diagrams. Definition 3 (Invariant predictor). Given selection diagrams tGi,j : i, j “ 1, . . . , ku, an invariant predictor is given by EP rY | zs where pY KK S | ZqGi,j for i, j “ 1, . . . , k and the expectation is taken with respect to any P among source domain distributions.1\n\nInvariant predictors define stable conditional expectations, i.e. EP 1rY | zs “ ̈ ̈ ̈ “ EP k rY | zs. We use the notion of domain-independent Markov blankets to define optimal invariant predictors. Definition 4 (Domain-independent Markov blankets). Given a set of selection diagrams tGi,j : i, j “ 1, . . . , ku, the set of domain-independent Markov blankets for Y P V is given by the set of Z Ă V such that (1) pY KK S | ZqGi,j for i, j “ 1, . . . , k and (2) pW ­KK Y | ZzW qGi,j for i, j “ 1, . . . , k and all W P Z.\n\nDomain-independent Markov blankets are designed to be minimal, in the sense that no proper subset of them satisfies conditions (1) and (2), and informative for predicting Y while defining stable conditional distributions across source domains. In general, such a set is not guaranteed to exist. For example, in Fig. 1b there is no set (and by implication no invariant predictor) that separates Y from all selection nodes, i.e. condition (1) in Def. 4 is violated for any subset of V. Moreover, contrary to the conventional Markov blanket (Pearl & Paz, 1985), it is not guaranteed to be unique. For example, in Fig. 2a both tX1, X2, X5u and tX1, X3, X4u are domain invariant Markov blankets. Which one is most informative to predict Y is undecidable from the graph structure alone, i.e. it depends the exact functional associations between variables. Proposition 1 (Optimal invariant predictor). Given selection diagrams tGi,j : i, j “ 1, . . . , ku, the optimal invariant predictor is defined as the minimizer of EP irpY ́ f pZqq2s across all i “ 1, . . . , k,\n\n1Other invariance assumptions have also been made, e.g. pZ KK SqGi,j and pY KK S | XqGi,j for problems where only the distribution of some covariates is expected to change across domains (Muandet et al., 2013), or such as pZ KK S | Y qGi,j (Li et al., 2018). Problems where the magnitude of changes is assumed to be bounded, i.e. | P ̊py | xq ́ P ipy | xq |ď c, instead of restricting the d-separation statements involving S have been studied by (Rothenhäusler et al., 2021).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nand belongs to the set of invariant predictors for which Z is a domain-independent Markov blanket for Y P V.\n\nInvariant predictors may be desirable due to their stability, although, it is apparent that the extent to which predictors will generalize outside of source domains depends on the structure of MpG ̊q and, in particular, differences in structure with respect to source domains. In general, structural invariances across source domains need not hold outside of source domains. For example, given two source domains π1, π2 described by G1,2 in Fig. 2b, it holds that EP 1 rY | x1, x2s “ EP 2rY | x1, x2s is the invariant predictor, which may not be optimal in a target domain π ̊ if the same invariance doesn’t hold. For example, for the selection diagrams G ̊,1 and G ̊,2 given in Fig. 2c and Fig. 2d EP 1 rY | x1, x2s ‰ EP ̊rY | x1, x2s. In fact, the generalization error of the optimal invariant predictor, here denoted EP 1rY | zs, can be written as\n\nmax M PMpG ̊q\n\nmax M PMpG ̊q\n\nEP M rpY ́ EP 1rY | Zsq2s “ `\n\n ̆ EP M rpY ́ EP M rY | Xsq2s ` EP M rpEP M rY | Xs ́ EP 1rY | Zsq2s\n\n,\n\n(3)\n\nwhere the first term on the RHS is the expected conditional variance and is in general irreducible, and the second term on the RHS quantifies the difference between the invariant predictor and the optimal prediction rule. This second term may be arbitrarily large for a general class of SCMs MpG ̊q with arbitrary differences with source domains. As a consequence, optimality of invariant predictors as solutions to Eq. (2) is limited in general to specific scenarios.\n\nProposition 2 (Generalization guarantees of optimal invariant predictors). Given a set of selection diagrams tGi,j : i, j “ 1, . . . , ku, let ∆ “ i,j ∆i,j be the set of variables in V whose causal mechanisms differ between any two source domains, and let S “ tSV : V P ∆u. Consider the robust optimization problem in Eq. (2). The optimal invariant predictor is a solution if selection nodes in all selection diagrams tGi, ̊ : i “ 1, . . . , ku are given by S with edges tSV Ñ V uSV PS.\n\nŤ\n\nIn words, an optimal invariant predictor has lowest generalization error in the sense of Eq. (2) only in the space of target SCMs MpG ̊q with the same structural invariances observed across source domains. Otherwise, in general better predictors are achievable. This observation includes predictors using causal parents as a conditioning set (often understood as desirable for domain generalization) which, similarly, define robust predictors for a target domain if invariance in the association between causal parents and outcomes is assumed. For example, in Fig. 1, EP a rY | paY s ‰ EP b rY | paY s, EP a rY | paY s ‰ EP ̊rY | paY s, and EP b rY | paY s ‰ EP ̊rY | paY s,and thus predictors based on causal parents may not be robust or optimal, in general. In particular, a prediction function of the form EP 1rY | paY s is a solution to the robust optimization problem in Eq. (2) if and only if it is the optimal invariant predictor and tGi, ̊ : i “ 1, . . . , ku is defined as in Prop. 2.\n\nMoreover, independently of whether solutions to a worst-case optimization problem can be found, they say nothing about the range of values optimal prediction functions EP ̊rY | xs may take in other distributions P ̊ away from the worst-case. In the following section, we attempt to define predictors and ranges of predictors with guarantees to arbitrary sets MpG ̊q.\n\n3 PARTIAL TRANSPORTABILITY OF STATISTICAL RELATIONS\n\nThe uncertainty and inherent under-identifiability of solutions to domain generalization problems motivates us to define the task of partial transportability, that extends the literature on domain generalization by considering bounds on the value of arbitrary queries EP ̊rY | xs in arbitrary target domains π ̊ defined by a set of selection diagrams tGi, ̊ : i “ 1, . . . , ku. Task (Partial Transportability). Derive a tight bound rl, us over a query of the form EP ̊rY | xs with knowledge of selection diagrams tG ̊,i : i “ 1, . . . , ku, a corresponding collection of data distributions tP ipvq : i “ 1, . . . , ku, and set of intervals tIj : Vj P i ∆ ̊,iu that define potential constraints on probabilities in the target domain. Algorithmically, this may be written as a solution\n\nŤ\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nto the following optimization problem,\n\nmin / max M PMpG ̊q\n\nEP M rY | xs,\n\nsuch that @V R ∆ ̊,i : f ̊ ď\n\nV “ f i\n\nV , P ̊puV q “ P ipuV q,\n\nand @V P\n\n∆ ̊,i, P ̊pv | paV q P IV .\n\n(4)\n\ni In words, the task is to evaluate the minimum and maximum values over all possible SCMs M compatible with tG ̊,i : i “ 1, . . . , ku that define the structurally invariant mechanisms in the system and (potentially uninformative, i.e., IV P r0, 1s) assumptions about target-specific probabilities.\n\nFor example, given the causal description of the protocols presented in the introductory medical example and Fig. 1, the question might be how to combine these various datasets to predict an individual’s risk of developing neurodegenerative diseases in π ̊? The optimal prediction function is given by EP ̊rY | w, x1, x2s (under mean squared error losses), which may be written as,\n\nÿ\n\nÿ\n\nyP ̊py, w, x1, x2q{\n\nP ̊py, w, x1, x2q,\n\n(5)\n\nyPΩY\n\nyPΩY\n\nEP ̊rY | w, x1, x2s “\n\nwhere P ̊py, w, x1, x2q is equal to,\n\nż\n\n1tf ̊ Y pw, x2, uwyq “ yu looooooooooooomooooooooooooon\n\n1tf ̊ W px1, uwyq “ wu loooooooooooomoooooooooooon\n\n1tf ̊ X1,X2pux1x2q “ x1, x2u loooooooooooooooomoooooooooooooooon\n\ndP ̊puq.\n\n(6)\n\nmatches RCT πb\n\nspecific to π ̊\n\nmatches hospital πa\n\nΩU\n\nThis is a mixture of terms for which data from source domains can be leveraged, for example Y pw, x2, uwyq “ f a f ̊ Y pw, x2, uwyq (superscripts denote domain), but also involves unobserved confounders uwy and ux1x2 that cannot be marginalized out, and terms that are specific to π ̊. In addition, although P ̊pw | x1q is known to differ in our target medical study it may be the case that we have some domain knowledge that constrains it, e.g. P ̊pw | x1q P r0.2, 0.7s “: Iw (if left undetermined, Iw :“ r0, 1s), and can be used to further inform a target query.\n\nThe following proposition show that the solution of the partial transportability task defines an interval that contains the invariant predictor and, by definition, also the optimal \"worst-case\" predictor across MpG ̊q. Proposition 3. For a given set of selection diagrams, let rlpxq, upxqs denote the solution of the partial transportability task for the query EP M rY | xs, M P MpG ̊q and EP 1rY | zs, Z Ď X be the invariant predictor. Then, EP 1 rY | zs P rlpxq, upxqs. Moreover, by definition EP M rY | xs P rlpxq, upxqs for a particular \"worst-case\" member M P MpG ̊q.\n\nIn general, there is no reason to believe that the invariant predictor has any special performance guarantee among other solutions in rlpxq, upxqs. For example, the worst-case loss in Eq. (3) is not, in general, smallest when EP M rY | xs ‰ EP 1rY | zs. An alternative is to exploit the solutions to the partial transportability task to define the median of rlpxq, upxqs as a general predictor for domain generalization problems. Proposition 4. For a given set of selection diagrams and data, let rlpxq, upxqs denote the solution of the partial transportability task for the query EP M rY | xs, M P MpG ̊q. Then, EP M rY | Xsq2s\n\nEP M rpY ́ med\n\nmax M PMpG ̊q\n\nM PMpG ̊q\n\nˆ\n\nď max\n\nM PMpG ̊q\n\nEP M rpY ́ EP M rY | Xsq2s `\n\nEP M rpupXq ́ lpXqq2s\n\n.\n\n ̇\n\n1 4\n\nUnder the condition that the irreducible error EP M rpY ́ EP M rY | Xsq2s is constant across EP M rY | Xs provably solves the robust optimization problem Eq. (2). M P MpG ̊q, medM PMpG ̊q\n\nThis proposition says that the error of the median is, at most, off from the optimal predictor by \"half the range of possible values of EP M rY | xs compatible with the data and assumptions\" and that this error is optimal in the worst case (under some assumptions on how the expected conditional variance is allowed to vary). This result is important because it applies to any set of target causal graph, source domains, and selection diagrams. Note, however, that this does not mean that the median is always superior to the optimal invariant predictor: in selected settings where the expected conditional variance changes across domains we may still have the optimal invariant predictor being a better worst-case solution.\n\n6\n\nż\n\nź\n\nΩUC\n\nV PC\n\nUnder review as a conference paper at ICLR 2023\n\n4 ALGORITHMS FOR PARTIAL TRANSPORTABILITY\n\nThis section presents algorithms to solve the partial transportability task for SCMs with discrete observables, that is each V P V taking values in a finite space of outcomes, while each U P U associated with an arbitrary probability density function P puq.\n\nA first step in our argument will be to decompose a chosen query into smaller factors so as to infer which factors can be matched across domains and point-identified from data, and subsequently re-parameterize unmatched factors by a special family of SCMs to make the bounding problem tractable. We use the concept of c-components and C-factors developed by Tian & Pearl (2002). The set V can be partitioned into c-components such that two variables are assigned to the same set C Ă V if and only if they are connected by a bi-directed path in G. In addition let UC “ ViPC Ui denote the set of exogeneous variables that are parents of any V P C. For example, the graph in Fig. 1a induces c-components tX1, X2u and tW, Y u.\n\nŤ\n\nFor any set C Ď V, let QirCsppapCqq denote the C-factor of C in domain πi which is defined by,\n\nQirCsppacq “\n\n1tf i\n\nV ppaV , uV q “ vudP ipuCq.\n\n(7)\n\nMoreover, let C denote the collection of c-components, then P pvq “ CPC QrCs and QrCs “ P pc | paCq (we omit the dependence of each C-factor on papCq for readability). This construction is useful because the joint distribution may be factorized according to the c-components of G and its factors matched across domains (Tian & Pearl, 2002; Correa & Bareinboim, 2019). Lemma 1. Let Ga,b be a selection diagram for the SCMs M a and M b, then QarCs “ QbrCs if Ga,b does not contain selection nodes SV pointing to any variable in V P C.\n\nś\n\nFor example, for the selection diagram in Fig. 1b, P ̊pvq “ Q ̊rX1, X2sQ ̊rW, Y s where by Lem. 1 Q ̊rX1, X2s “ QarX1, X2s “ P apx1, x2q, since the is no S-node pointing to X1 or X2. In turn, Q ̊rW, Y s ‰ QarW, Y s because of the selection node pointing to W . Note, however, that Q ̊rW, Y s defined as in Eq. (7) involves terms, e.g. 1tfY pw, x1, uwyq “ yu, that are invariant across domains since the absence of an S-node into Y denotes invariance in causal mechanisms, and for which P apvq may be used for estimation. We discuss next a re-parameterization of C-factors Q ̊rCs that cannot be matched across domains with the goal of defining a tractable constrained optimization problem to bound Q ̊rCs. Proposition 5. Let M be an arbitrary SCM with graph G and let C be any c-component. Then, there exists a corresponding SCM N with finite exogeneous domain compatible with G such that QM rCs “ QN rCs, where for every exogenous variable U P UC, its cardinality dU “\n\nˇ ˇΩPapCq\n\nˇ ˇ.\n\nThis proposition shows that SCMs with discretely-valued exogeneous variables are expressive enough to represent C-factors QrCs irrespective of the true underlying data generating mechanism. From an optimization perspective, this is useful because it allows us to consistently parameterize C-factors and make inference on its distribution in a well-defined latent variable model (Rosset et al., 2017; Zhang et al., 2021). As an example, consider the introductory example with tX1, X2, Y, W u binary and causal graphs in Fig. 1. Q ̊rW, Y s defined using Eq. (7) can also be written as:\n\nÿ\n\n1tf a\n\nY pw, x2, uwy, uyq “ yu1tf ̊\n\nW px1, uwy, uwq “ wuP apuwy, uyqP ̊puwq,\n\n(8)\n\nuwy,uy,uw\n\nwhere |ΩUwy | “ |ΩUw | “ |ΩUy | “ |ΩX1| ̈ |ΩX2| ̈ |ΩW | ̈ |ΩY | “ 16; the function fV is a mapping between finite domains ΩPaV ˆ ΩUV ÞÑ ΩV for V P tW, Y u. Moreover, we have used the structural invariances encoded by the selection diagrams in Fig. 1 to match causal mechanisms and exogeneous probabilities between domains. In particular, P apuyq “ P bpuyq “ P ̊puyq by definition of the selection diagrams Ga, ̊ and Gb, ̊. Although discretely-valued causal mechanisms and exogeneous probabilities imply well-defined parameters to optimize over, the partial transportability task remains a difficult constrained optimization problem.\n\n4.1 APPROXIMATIONS VIA GIBBS SAMPLING\n\nWe follow (Chickering & Pearl, 1996; Zhang et al., 2021; Bellot et al., 2022) and take a Bayesian perspective to approximating bounds rlpxq, upxqs. We evaluating credible intervals P plpxq ă\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nEP ̊rY | xs ă upxq | ̄vq “ 1 ́ α on the posterior of EP ̊rY | xs by approximating the expectation, Er1tlpxq ă EP ̊rY | xs ă upxqu | ̄vs “ P plpxq ă EP ̊rY | xs ă upxq | ̄vq\n\n(9)\n\nprovided with finite samples ̄v :“ p ̄vπ1, . . . , ̄vπk q, where ̄vπi “ tvpjq : j “ 1, . . . , niu are ni independent sampled collected in domain πi and a set of selection diagrams tG ̊,i : i “ 1, 2, . . . , ku using Gibbs sampling. Following the arguments in the previous section the query may be reduced to bounding a C-factor of the form,\n\nπi\n\nÿ\n\nÿ\n\nź\n\nź\n\nωobj :“ Q ̊rCs “\n\nU PUC\n\nu“1,...,dU\n\nV PC\n\n1tξppaV ,uV q\n\nV\n\n“ vu\n\nθu.\n\nU PUC\n\n(10)\n\nthat are parameterized by ξ “ tξppaV ,uV q : V P C, PaV Ă V, UV Ă UCu and θ “ tθu : U P UCu that represent causal functional assignments and exogeneous probabilities, respectively. We have dropped the domain indicator ” ̊” from the definition of parameters for readability.\n\nV\n\nFor every V P V, @paV , uV , the functional assignment parameters ξppaV ,uV q are drawn uniformly in the discrete domain ΩV . For every U P U, exogenous probabilities θU with dimension dU “ ˇ\nˇΩPapCq\n\nˇ ˇ are drawn from a prior Dirichlet distribution,\n\nV\n\nθU “ pθ1, . . . , θdU q „ Dirichlet pα1, . . . , αdU q,\n\n(11)\n\nwith hyperparameters α1, . . . , αdU . The Gibbs sampler starts with some initial value for all latent quantities pu, ξ, θq in the expression of ωobj, and iterates over the following sampling steps, each parameter conditioned on the current values of the remaining terms in the parameter vector.\n\n1. Sample u. Let u P ΩU , U P UC. For each observed data example across all domains vpnq P ̄v, i ni, we sample corresponding exogeneous variables U P UC from the conditional\n\nř\n\nn “ 1, . . . , distribution,\n\nP pupnq | vpnq, ξ, θq 9 P pupnq, vpnq | ξ, θq “\n\nź\n\nV PC\n\n1tξppapnq\n\nV ,upnq V q\n\nV\n\n“ vpnqu\n\nź\n\nU PUC\n\nθu.\n\n(12)\n\nV\n\nP ξ its conditional distribution is given by P pξppaV ,uV q\n\n2. Sample ξ. Parameters ξ define deterministic causal mechanisms. For a given parameter ξppaV ,uV q “ v | ̄v, ̄uq “ 1 if there exists a sample pvpnq, papnq V , upnqq for some n, where n iterates over the samples of u from step 1 and v associated with the subset of domains in which exogeneous probabilities match the target domain, such that ξppapnq uniform discrete distribution over its domain ΩV .\n\n“ vpnq. Otherwise, P pξppaV ,uV q\n\n“ v | ̄v, ̄uq is given by a\n\nV ,upnq V q\n\nV\n\nV\n\nV\n\n3. Sample θ. Let θU “ pθ1, . . . , θdU q P θ be the parameters that define the probability vector of\n\npossible values of variables U P UC. Its conditional distribution is given by,\n\nθ1, . . . , θdU | ̄v, ̄u „ Dirichlet\n\nα1 `\n\n ̃\n\nÿ\n\nn\n\n1tupnq “ u1u, . . . , αdU `\n\nÿ\n\nn\n\n ̧\n\n1tupnq “ udU u\n\n,\n\n(13)\n\nwhere, similarly, n iterates over the samples of u from step 1 associated with the subset of domains in which exogeneous probabilities match the target domain.\n\nIn the above, we have described the conditional distributions of parameters that can be matched across domains, and therefore estimated from the subset of relevant available data. By the definition of the partial transportability task, parameters that are specific to the target domain π ̊ are constrained to ř\nlie in an assumed interval, e.g. P ̊pv | paV q “ θu P IV Ď r0, 1s, or else left unspecified. In the first case, parameters are sampled independently and uniformly in the space defined by the constraints and in the second case, they are sampled independently and uniformly in their domain of definition, i.e. ξppaV ,uV q P ΩV , θu P ΩU , in every step of the sampler.\n\n1tξppaV ,uV q\n\n“ viu\n\nU PUV\n\nś\n\nuV\n\nV\n\nV\n\nIterating this procedure forms a Markov chain with the invariant distribution to be the target posterior distribution P pu, ξ, θ | ̄vq. P pωobj | ̄vq is then approximated by plugging the MCMC samples into Eq. (10). The upper and lower α quantile among T samples of P pωobj | ̄vq, when combined with the\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nSW\n\nSW\n\nSS\n\nS\n\nW\n\nT\n\nSS\n\nC\n\nS\n\nW\n\nT\n\nW\n\nT\n\nC\n\nC\n\nS\n\nEP UK rc|t, ws EP UK rc|t, w, ss medpˆl0, ˆu0q\n\nπFR\n\n.1502 (.005)\n\n.1448 (.004)\n\n.1361 (.005)\n\n(a) GFR,UK\n\n(b) GFR,SW\n\n(c) GSW,UK\n\n(d) Performance comparisons.\n\nFigure 3: (a-c) selection diagrams that compare domain πFR with πUK, πFR with πSW, and πSW with πUK, respectively. (d) gives mean squared error for cancer prediction on a sample of data from P FR.\n\nidentified C-factors that form EP ̊ry | xs, gives us a p1 ́ αq credible interval ˆlα ă EP ̊ry | xs ă ˆuα defined by,\n\nÿ\n\nˆlαpxq “ suptx :\n\n1tEP ̊rY | xsptq ď xu “ α{2u,\n\nˆuαpxq “ inftx :\n\nt ÿ\n\nt\n\n1tEP ̊rY | xsptq ď xu “ 1 ́ α{2u.\n\n(14)\n\n(15)\n\nThe following Theorem shows that credible intervals rˆl0pxq, ˆu0pxqs converge to the true bounds rlpxq, upxqs for the unknown query EP ̊rY | xs and are, moreover, maximally informative, in the sense that we can always construct two data generating mechanisms M 1, M 2 for the target domain that are compatible with our current knowledge of the world such that E P M 1 rY | xs “ l and E\n\nP M 2 ry | xs “ u.\n\nTheorem 1. The solution rlpxq, upxqs to the partial transportability task defined over discrete SCMs is a tight bound over a target query EP ̊rY | xs. The credible interval rˆl0pxq, ˆu0pxqs coincides with rlpxq, upxqs as ni Ñ 8 in all observable domains πi, i “ 1, . . . , k.\n\n5 EXPERIMENTS\n\n5.1 SMOKING AND LUNG CANCER\n\nOur first experiment is inspired by the debate around the relationship between smoking and lung cancer in the 1950’s (US Department of Health and Human Services, 2014). We use a scientificallygrounded variation of the front-door graph that includes an individual’s smoking status S, presence of tar in the lungs T , wealth W , and lung cancer status C, using the fact that smoking and lung cancer may be confounded by an individual’s unobserved genetic profile. In this example, the objective is to make inference on cancer probability distributions in the French population πFR from corresponding data in πUK where the prevalence of smoking is known to be lower. The selection diagram is given in Fig. 3a and details on the SCMs used to generate data are given in Appendix B.\n\nProbability of cancer among smokers P FRpC “ 1 | S “ 1q . The C-factor decomposition and parameterization is given by the following derivations,\n\nP FRpc | sq “\n\nř\n\nP FRpc, sq c P FRpc, sq\n\nt,w P FRpc, s, t, wq c,t,w P FRpc, s, t, wq\n\nt,w QFRrs, csQFRrwsQFRrts c,t,w QFRrs, csQFRrwsQFRrts\n\n,\n\n(16)\n\nř\n\nř\n\n“\n\nwhere QFRrts “ QUKrts “ P UKpt | s, wq and QFRrws “ QUKrws “ P UKpwq, and,\n\nQFRrs, cs “\n\n1tξpw,t,uscq\n\nCUK\n\n“ cu1tξpw,usc,usq\n\nSFR\n\n“ suθUK usc\n\nθFR us\n\n.\n\nusc,us\n\nIn Fig. 4, we report estimated 100% credible intervals ˆl0 ă P FRpC “ 1 | S “ 1q ă ˆu0 as a function of the number of samples without prior information (purple) and with the prior information that P FRps | wq lies in an interval of width 0.1 around its true value (pink). The black and gray dotted lines are the actual values\n\nFigure 4: Bounding the probability of cancer.\n\n9\n\nř\n\nř\n\n“\n\nÿ\n\nUnder review as a conference paper at ICLR 2023\n\nP FRpC “ 1 | S “ 1q and P UKpC “ 1 | S “ 1q respectively. Notice that a relatively small number of samples is required to converge to stable bounds, and that the prior information narrows the credible interval reflecting this additional constraint. We also show for illustration that our Gibbs sampler recovers the true values P FRpC “ 1 | S “ 1q (pink) and P FRpC “ 1 | S “ 0q (purple) when trained on data from πFR, i.e. when probabilities are identified.\n\nPrediction performance across domains. Consider the task of designing cancer prediction rules for optimal performance in the french population πFR. We introduce an additional training domain to be able to define invariant predictors: data from the Swedish population πSW whose structural differences with πUK and with πFR are given in Fig. 3. Across πUK and πSW, the optimal invariant predictor (Def. 3) is given by EP UK rC | t, w, ss “ EP SWrC | t, w, ss which, however, is not equal to EP FR rC | t, w, ss as no set blocks the open path between the selection node SS and the cancer variable C in GFR,UK. We consider also the common strategy of using causal parents for prediction, i.e. using the prediction rule EP UKrC | t, ws (which, similarly, is not equal to EP FRrC | t, ws). For comparison, we consider the median value medpˆl0, ˆu0q for the optimal prediction rule EP FR rC | t, w, ss computed using data from πUK and πSW. We observe in Fig. 3d that indeed the prediction rule EP UKrC | t, w, ss underperforms in πFR: for reference EP FRrC | t, w, ss has a mean error of .1220, cautioning against naively transporting invariant prediction rules across domains. Similarly, using causal parents for prediction underperforms. In contrast, the median of the derived bound proves to be a slightly better predictor in this case and has a guarantee of optimal performance in the \"worst-case\" domain compatible with the selection diagrams (Prop. 4).\n\n5.2 PREDICTION OF NEURODEGENERATIVE DISEASES ACROSS HOSPITALS\n\nOur second experiment reconsiders the introductory example that described the design of prediction rules for the development of neurodegenerative diseases in a target hospital π ̊ in which no data has been recorded. Instead, we have access to data from two related studies conducted in hospitals πa and πb that, however, are known to differ with respect to the target domain notably in the distribution of blood pressure W , a known cause of neurodegenerative diseases. The causal protocol is given in Fig. 1 and is described in more depth in Sec. 2. Details on the SCMs used to generate data are given in Appendix B.\n\nGiven this information, we consider the task of designing a prediction rule for optimal mean squared error in the target hospital π ̊. Here, invariant predictors are well defined and given by the function f pw, x1, x2q “ EP a rY | w, x1, x2s “ EP b rY | w, x1, x2s although note that, in this example, this conditional expectation is not invariant in the target domain due to the difference in the causal mechanisms associated with blood pressure W , see Fig. 1d. Similarly, we can define causal predictors as EP arY | w, x2s and EP b rY | w, x2s which in this case are not equal across hospitals πa and πb due to the open path between SX1 and Y once we condition on W . The partial transportability task instead argues for approximating EP ̊rY | w, x1, x2s which, using the C-factor decomposition, is parameterized by P ̊py, w, x1, x2q “ Q ̊rX1, X2sQ ̊rW, Y s where Q ̊rX1, X2s “ P apx1, x2q by Lem. 1 and\n\nÿ\n\nQ ̊rW, Y s “\n\nuwy,uw\n\n1tξpw,x2,uwyq\n\nY a\n\n“ yu1tξpx1,uwy,uwq\n\nW ̊\n\n“ wuθa\n\nuwy\n\nθ ̊\n\nuw\n\n.\n\nThe median value of the resulting interval that encodes the uncertainty in the computation of EP ̊rY | w, x1, x2s as well as all baseline predictors are given in Fig. 5. We add the actual optimal (not computable) prediction rule EP ̊rY | w, x1, x2s for reference. Fig. 5 shows that the median outperforms and that baselines, although common strategies for prediction, can result in significantly worse out-of-distribution performance in examples where unobserved confounding as well as structural differences between domains play a role.\n\nEP a ry|w, x1, x2s EP a ry|w, x2s E\nP b ry|w, x2s medpˆl0, ˆu0q E\n\nP ̊ ry|w, x1, x2s\n\nπ ̊\n\n.3640 (.003)\n\n.4244 (.002)\n\n.4013 (.002)\n\n.2961 (.008)\n\n.2434 (.002)\n\nFigure 5: Performance comparisons.\n\n6 CONCLUSIONS\n\nThis paper investigated the problem of domain generalization from the perspective of transportability theory. We introduced the task of partial transportability that seeks to bound the value of an arbitrary\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nconditional expectation EP ̊rY | xs in an unseen domain π ̊ using selection diagrams and data from source domains. Using this formalism, we showed that invariant predictors and more general solutions to robust optimization problems derived in the literature are special cases of solutions to this task. Moreover, in systems of discrete observables, we showed that we can design provably consistent algorithms for inferring bounds that are sound and tight, and illustrated its performance on synthetic data.\n\nREFERENCES\n\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\n\narXiv preprint arXiv:1907.02893, 2019.\n\nAlexander Balke and Judea Pearl. Bounds on treatment effects from studies with imperfect compliance.\n\nJournal of the American Statistical Association, 92(439):1171–1176, 1997.\n\nElias Bareinboim and Judea Pearl. Transportability from multiple environments with limited experi-\n\nments: Completeness results. Advances in neural information processing systems, 27, 2014.\n\nElias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. Proceedings of the\n\nNational Academy of Sciences, 113(27):7345–7352, 2016.\n\nElias Bareinboim, Sanghack Lee, Vasant Honavar, and Judea Pearl. Transportability from multiple environments with limited experiments. Advances in Neural Information Processing Systems, 26, 2013.\n\nAlexis Bellot and Mihaela van der Schaar. Accounting for unobserved confounding in domain\n\ngeneralization. arXiv preprint arXiv:2007.10653, 2020.\n\nAlexis Bellot, Junzhe Zhang, and Elias Bareinboim. Scores for learning discrete causal graphs with unobserved confounders. Technical Report R-83, Causal AI Lab, Columbia University, 2022.\n\nAharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization, volume 28.\n\nPrinceton university press, 2009.\n\nConstantin Carathéodory. Über den variabilitätsbereich der fourier’schen konstanten von positiven harmonischen funktionen. Rendiconti Del Circolo Matematico di Palermo (1884-1940), 32(1): 193–217, 1911.\n\nDavid M Chickering and Judea Pearl. A clinician’s tool for analyzing non-compliance. In Proceedings\n\nof the National Conference on Artificial Intelligence, pp. 1269–1276, 1996.\n\nJuan Correa and Elias Bareinboim. General transportability of soft interventions: Completeness\n\nresults. Advances in Neural Information Processing Systems, 33:10902–10912, 2020.\n\nJuan D Correa and Elias Bareinboim. From statistical transportability to estimating the effect of\n\nstochastic interventions. In IJCAI, pp. 1661–1667, 2019.\n\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint\n\narXiv:2007.01434, 2020.\n\nSanghack Lee, Juan D Correa, and Elias Bareinboim. Generalized transportability: Synthesis of In Proceedings of the 34th AAAI Conference on\n\nexperiments from heterogeneous domains. Artificial Intelligence, 2020.\n\nYa Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain generalization via conditional invariant representations. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nChaochao Lu, Yuhuai Wu, Jo ́se Miguel Hernández-Lobato, and Bernhard Schölkopf. Nonlinear\n\ninvariant risk minimization: A causal approach. arXiv preprint arXiv:2102.12353, 2021.\n\nSara Magliacane, Thijs Van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions. Advances in neural information processing systems, 31, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLin Meng, Dongmei Chen, Yang Yang, Yang Zheng, and Rutai Hui. Depression increases the risk of hypertension incidence: a meta-analysis of prospective cohort studies. Journal of hypertension, 30 (5):842–851, 2012.\n\nKrikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10–18. PMLR, 2013.\n\nJ Pearl and A Paz. Graphoids: A graph-based logic for reasoning about relevance relations technical report 850038 (r-53-l). Cognitive Systems Laboratory, University of California, Los Angeles, 1985.\n\nJudea Pearl. Causality. Cambridge university press, 2009.\n\nJudea Pearl and Elias Bareinboim. Transportability of causal and statistical relations: A formal\n\napproach. In Twenty-fifth AAAI conference on artificial intelligence, 2011.\n\nJonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016.\n\nMateo Rojas-Carulla, Bernhard Schölkopf, Richard Turner, and Jonas Peters. Invariant models for causal transfer learning. The Journal of Machine Learning Research, 19(1):1309–1342, 2018.\n\nDenis Rosset, Nicolas Gisin, and Elie Wolfe. Universal bound on the cardinality of local hidden\n\nvariables in networks. arXiv preprint arXiv:1709.00707, 2017.\n\nDominik Rothenhäusler, Nicolai Meinshausen, Peter Bühlmann, and Jonas Peters. Anchor regression: Heterogeneous data meet causality. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 83(2):215–246, 2021.\n\nHerman Rubin and Oscar Wesler. A note on convexity in euclidean n-space. In Proc. Amer. Math.\n\nSoc, volume 9, pp. 522–523, 1958.\n\nIngmar Skoog and Deborah Gustafson. Update on hypertension and alzheimer’s disease. Neurological\n\nresearch, 28(6):605–611, 2006.\n\nAdarsh Subbaswamy and Suchi Saria. From development to deployment: dataset shift, causality, and\n\nshift-stable models in health ai. Biostatistics, 21(2):345–352, 2020.\n\nAdarsh Subbaswamy, Peter Schulam, and Suchi Saria. Preventing failures due to dataset shift: Learning predictive models that transport. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 3118–3127. PMLR, 2019.\n\nJin Tian and Judea Pearl. A general identification condition for causal effects. eScholarship, University\n\nof California, 2002.\n\nUS Department of Health and Human Services. The health consequences of smoking—50 years of\n\nprogress: a report of the surgeon general, 2014.\n\nJindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and Data Engineering, 2022.\n\nJunzhe Zhang, Jin Tian, and Elias Bareinboim. Partial counterfactual identification from observational\n\nand experimental data. arXiv preprint arXiv:2110.05690, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS\n\nProposition 6 (Prop. 1 restated). Given selection diagrams tGi,j : i, j “ 1, . . . , ku, the optimal invariant predictor is defined as the minimizer of EP irpY ́ f pZqq2s across all i “ 1, . . . , k, and belongs to the set of invariant predictors for which Z is a domain-independent Markov blanket for Y P V.\n\nProof. Assume not such that there exists an optimal invariant predictor EP irY | Zs, i “ 1, . . . , k, distinct from any invariant predictor defined conditional on a domain-independent Markov blanket, with Z not a domain-independent Markov blanket for Y P V. Then, by definition of a domainindependent Markov blanket, either pY ­KK S | ZqGi,j for i, j “ 1, . . . , k, in which case EP irY | Zs is not invariant across source domains, or there exists a W P Z such that pW KK Y | ZzW qGi,j for i, j “ 1, . . . , k, in which case EP irY | Zs “ EP irY | ZzW s. Now if ZzW is not a domainindependent Markov blanket we can continue removing independent variables from ZzW to reach a domain-independent Markov blanket concluding that EP irY | Zs is not distinct from an invariant predictor defined conditional on a domain-independent Markov blanket.\n\nŤ\n\nProposition 7 (Prop. 2 restated). Given a set of selection diagrams tGi,j : i, j “ 1, . . . , ku, let i,j ∆i,j be the set of variables in V whose causal mechanisms differ between any two source ∆ “ domains, and let S “ tSV : V P ∆u. The optimal invariant predictor solves the robust optimization problem in Eq. (2) if selection nodes in all selection diagrams tGi, ̊ : i “ 1, . . . , ku are given by S with edges tSV Ñ V uSV PS.\n\nProof. Given a set of selection diagrams tGi,j : i, j “ 1, . . . , ku, let ∆ “ i,j ∆i,j be the set of variables in V whose causal mechanisms differ between any two source domains, and let S “ tSV : V P ∆u.\n\nŤ\n\nAssume that selection nodes in all selection diagrams tGi, ̊ : i “ 1, . . . , ku are given by S (and with edges tSV Ñ V uSV PS). In that case, the optimal invariant predictor, written EP 1 rY | Zs “ EP M rY | Zs for any M P MpGq. Any additional variable W in the conditioning set is either irrelevant for prediction, i.e. EP M rY | Zs “ EP M rY | Z, W s, or breaks the independence between Y and selection nodes, which implies that EP M rY | Z, W s varies as a function of M . Since the the functional form of M (besides the arguments of functions) are not constrained by selection diagrams, for any fixed prediction function EP M rY | Z, W s, we can always find a domain M 1 P MpGq that makes the error E\nP M 1 rpY ́ EP M rY | Zsq2s which is fixed for any M 1.\n\nP M 1 rpY ́ EP M rY | Z, W sq2s arbitrarily large, and thus higher than E\n\nProposition 8 (Prop. 3 restated). For a given set of selection diagrams, let rlpxq, upxqs denote the solution of the partial transportability task for the query EP M rY | xs, M P MpG ̊q and EP 1 rY | zs, Z Ď X be the invariant predictor. Then, EP 1rY | zs P rlpxq, upxqs. Moreover, by definition EP M rY | xs P rlpxq, upxqs for a particular \"worst-case\" member M P MpG ̊q.\n\nProof. The set MpG ̊q represents all SCMs compatible with a target causal graph G ̊ that is only constrained by selection diagrams tGi, ̊ : i “ 1, . . . , ku. A selection node indicates a potential change between two domains and therefore, in principle all source domain SCMs tM i : i “ 1, . . . , ku are possible candidates for the target domain and thus M i P MpG ̊q i “ 1, . . . , k. Then, by the definition of the partial transportability task, EP 1rY | zs P rlpxq, upxqs where z is the value of Z in X.\n\nProposition 9 (Prop. 4 restated). For a given set of selection diagrams and data, let rlpxq, upxqs denote the solution of the partial transportability task for the query EP M rY | xs, M P MpG ̊q. Then,\n\nmax M PMpG ̊q\n\nEP M rpY ́ med\n\nEP M rY | Xsq2s\n\nM PMpG ̊q\n\nˆ\n\nď max\n\nM PMpG ̊q\n\nEP M rpY ́ EP M rY | Xsq2s `\n\nEP M rpupXq ́ lpXqq2s\n\n.\n\n ̇\n\n1 4\n\nUnder the condition that the irreducible error EP M rpY ́ EP M rY | Xsq2s is constant across EP M rY | Xs provably solves the robust optimization problem Eq. (2). M P MpG ̊q, medM PMpG ̊q\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nProof. For a given set of selection diagrams and data, let rlpxq, upxqs denote the solution of the partial transportability task for the query EP M rY | xs, M P MpG ̊q. Then,\n\nmax M PMpG ̊q\n\nEP M rpY ́ med\n\nEP M rY | Xsq2s\n\n ̃\n\nM PMpG ̊q\n\nˆ\n\n ̧\n\n ̇\n\n2\n\n“ max\n\nM PMpG ̊q\n\n“ max\n\nM PMpG ̊q\n\nď max\n\nM PMpG ̊q\n\nˆ\n\nˆ\n\nEP M r\n\nY ́ EP M rY | Xs ` EP M rY | Xs ́ med\n\nEP M rY | Xs\n\nM PMpG ̊q\n\ns\n\n ̇\n\nEP M rpY ́ EP M rY | Xsq2s ` EP M rpEP M rY | Xs ́ med\n\nEP M rY | Xsq2s\n\nM PMpG ̊q\n\n ̇\n\nEP M rpY ́ EP M rY | Xsq2s `\n\n1 4\n\nEP M rpupXq ́ lpXqq2s\n\n.\n\nThe second equality holds because the cross term in the expansion of the square equal 0 as EP M rpY ́ EP M rY | Xsqs “ 0. The inequality holds because the largest distance between EP M rY | Xs and the median of values EP M rY | Xs can reach as a function of M P MpG ̊q is half the distance between maximum and minimum values of EP M rY | Xs across M P MpG ̊q, that is pupXq ́ lpXqq{2. If EP M rpY ́ EP M rY | Xsq2s is equal to a constant value independent of M , it can be taken out of the maximization and we are left with the optimization problem,\n\narg min f\n\nmax M PMpG ̊q\n\nEP M rpEP M rY | Xs ́ f pXqq2s\n\n(17)\n\nFor any x and any f , we can always choose M such that |EP M rY | xs ́ f pxq| ě |EP M rY | For example, by choosing M such that EP M rY | xs “ xs ́ med EP M rY | xs depending on what EP M rY | xs minimizes the robust optimization\n\nEP M rY | xs|. EP M rY | xs or EP M rY | xs “ minM PMpG ̊q\n\nmaxM PMpG ̊q distance is larger. Therefore, f pxq :“ med\n\nM PMpG ̊q\n\nM PMpG ̊q\n\nproblem.\n\nTheorem 2 (Prop. 5 restated). Let M be an arbitrary SCM with graph G and let C be any ccomponent. Then, there exists a corresponding SCM N with finite exogeneous domain compatible with G such that QM rCs “ QN rCs, where for every exogenous variable U P UC, its cardinality dU “\n\nˇ ˇΩPapCq\n\nˇ ˇ.\n\nProof. The proof follows from Rosset et al. (2017) and Zhang et al. (2021). We include it below for completeness.\n\nWe first introduce some necessary notations and concepts. The probability distribution for every exogenous variables U Ă U is characterized with a probability space. It is frequently designated xΩU , FU , PU y where ΩU is a sample space containing all possible outcomes; FU is a σ-algebra containing subsets of ΩU ; PU is a probability measure on FU normalized such that PU pΩU q “ 1. Elements of FU are called events, which are closed under operations of set complement and unions of countably many sets. By means of PU a real number PU pAq P r0, 1s is assigned to every event A P FU ; it is called the probability of event A. For an arbitrary set of exogenous variables U, its realization U “ u is an element in the Cartesian product U PU ΩU . We may be interested in inferring whether a sequence of events A for every U P U occurs. Such an event is represented by a Â\nU PU ΩU which in turn generate a product of σ-algebras U PU FU . Define U PU AU Ď subset U PU PU to satisfy the following mutual independence condition given by the the product measure definition of the SCM,\n\nŚ Â\n\nŚ\n\nŚ\n\n ̃\n\ną\n\n ̧\n\nź\n\nP\n\nAU\n\n“\n\nPU pAU q.\n\nU PU\n\nU PU\n\nSuch P is a probability measure. Moreover,\n\nC\n\nG\n\ną\n\nâ\n\nâ\n\nΩU ,\n\nFU ,\n\nPU\n\n,\n\nU PUq\n\nU PU\n\nU PU\n\n14\n\n(18)\n\n(19)\n\nUnder review as a conference paper at ICLR 2023\n\ndefines a product of probability spaces xΩU , FU , PU y that describes measurable events over all exogeneous variables U partitioned into c-components. Let C be the collection of all c-components in G. c-components in C form a partition t V PC UV | C P Cu over exogenous variables U . Therefore, for every U P U, there must exist a unique ccomponent denoted by CU containing U . For any c-component C P C, let UC “ V PC UV the set of exogenous variables affecting (at least one of) endogenous variables in C. By the definition of c-components, the exogeneous variables do not overlap between c-components and it holds that,\n\nŤ\n\nŤ\n\n ̃\n\nč\n\n ̧\n\nź\n\n ̃\n\nč\n\n ̧\n\nP\n\nAU\n\n“\n\nPU\n\nAU\n\n.\n\nU PU\n\nCPCpGq\n\nU PC\n\n(20)\n\nFor any SCM M compatible with the causal graph G the joint distribution may be factorized into c-components,\n\nź\n\nP pvq “\n\nQrCspc, paCq.\n\nCPC\n\n(21)\n\nwhere QrCs is a C-factor and is a function of pc, paCq.\n\nTo parameterize this joint distribution it is thus sufficient to look at each C-factor separately. Let C be a generic c-component in G. Denote by m “ |UC| the number of exogeneous variables related to C. For convenience, we consistently write xΩi, Fi, Piy as the probability space of i-th exogeneous variable in C. The product of these probability spaces is thus written,\n\nC\n\nG\n\nmą\n\nmâ\n\nmâ\n\nΩi,\n\nFi,\n\nPi\n\n.\n\ni“1\n\ni“1\n\ni“1\n\n(22)\n\n(23)\n\nEach C-factor may thus be written,\n\nż\n\nQrCs “\n\nŚ\n\nź\n\nm\n\ni“1 Ωi\n\nV PC\n\n1tfV ppaV , uV q “ vud\n\nmâ\n\ni“1\n\nPi.\n\nOur goal is to show that all probabilities QrCs, induced by exogenous variables described by arbitrary probability spaces could be produced by a “simpler” generative process with discrete exogenous domains. QrCs defines a mapping between the space of possible realizations of the variables PapCq to the r0, 1s interval. Since PapCq are discrete variables with finite domains, the cardinality of the class of probability assignments that must be defined is also finite. It is given at most by the number of possible combinations of realizations of PapCq which is given by\n\nś\n\nV PPapCq |ΩV |.\n\nś\n\nLet ̄P be a vector representing probabilities QrCspc, paCq. Counting all possible combinations of outcomes for all possible conditioning sets, ̄P is therefore a vector of at most size V PPapCq |ΩV |. And since QrCspc, paCq is a probability mass function, it only takes a d “ vector with d ́ 1 dimensions to uniquely determine it. ̄P may thus be interpreted as a point in the pd ́ 1q-dimensional real space. Similarly, pP, 1q is vector in d-dimensional space where the d-th element is equal to 1.\n\nNow consider sampling a value U1 “ u1 from the underlying SCM and let Qu1 be the probability model with U1 “ u1.\n\nQu1 rCspc | paCq “\n\nŚ\n\n«ż\n\nź\n\nm\n\ni“2 Ωi\n\nV PC\n\n1tfV ppaV , uV q “ vud\n\nmâ\n\ni“2\n\nPi\n\n.\n\n(24)\n\nU1“u1\n\nff\n\nand ̄Pu1 is a pd ́ 1q-dimensional probability vector representing the probabilities of each one of the combinations PapCq given that U1 “ u1. We will show that P1 may equally well be represented by a discrete distribution. For this, let U “ t ̄Pu1 : u1 P Ω1u Ă Rd be the set of probability points that ş\n ̄Pu1dP1 is a convex mixture of points in U by can be constructed as u1 varies in Ω1. The average (Rubin & Wesler, 1958) that equals ̄Q since, ż\n\n«ż\n\nΩ1\n\nff\n\n1tfV ppaV , uV q “ vud\n\nPi\n\ndP1.\n\n(25)\n\nmâ\n\nź\n\n ̄P “\n\nΩ1\n\nŚ\n\nm\n\ni“2 Ωi\n\nV PC\n\ni“2\n\nU1“u1\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nBy construction, ̄P itself is a convex mixture of at most d ` 1 points in U. That is, by using Carathéodory’s theorem (Carathéodory, 1911),\n\n ̄P “\n\nd`1ÿ\n\nk“1\n\nwk ̄Pu1,k .\n\nReplacing the definition of ̄Pu1,k we obtain,\n\n«ż\n\nwk\n\nŚ\n\n ̄P “\n\nd`1ÿ\n\nk“1\n\nź\n\nm\n\ni“2 Ωi\n\nV PC\n\n1tfV ppaV , uV q “ vud\n\nmâ\n\ni“2\n\nPi\n\nff\n\nU1“u1,k\n\n(26)\n\n(27)\n\nThis means that we can replace the continuous measure P1 with a discrete probability set with outcomes tu1,1, . . . , u1,du and corresponding probabilities tw1, . . . , wdu with cardinality d and obtain a probability model that is equivalent to the original ̄P . This procedure can be repeated for all m exogeneous variables in the c-component C. We are thus left with a model,\n\nQrCspc, paCq “\n\nż\n\nŚ\n\nm\n\ni“1 Ωi\n\nź\n\nV PC\n\n1tfV ppaV , uV q “ vud\n\nmâ\n\ni“1\n\nPi,\n\n(28)\n\nequivalent to its discrete counterpart, ÿ\n\nÿ\n\nź\n\nQrCspc, paCq “\n\nuPUC\n\nu“1,...,d\n\nV PC\n\n1tfV ppaV , uV q “ vu\n\nź\n\nuPuc\n\nP puq,\n\n(29)\n\nwhere d “\n\nś\n\nV PPapCq |ΩV |.\n\nTheorem 3 (Thm. 1 restated). The solution rl, us to the partial transportability task defined over discrete SCMs is a tight bound over a target query EPπ ̊ ry | xs. The credible interval rˆl0, ˆu0s coincides with rl, us as ni Ñ 8 in all observable domains πi, i “ 1, . . . , k.\n\nProof. The proof strategy follows (Zhang et al., 2021) and shows convergence of the posterior by way of convergence of the likelihood of the data given one SCM M P MpGq. We look at ’convergence’ in a frequentist way, for increasing sample size the posterior will, with increasing probability, be low for any parameter configuration, i.e. for any SCM M R MpGq.\n\nBy the definition of the optimal bounds rl, us given by the solution to the partial transportability task,\n\nP p ̄v | l ă EPπ ̊ ry | xs ă uq Ñp 1.\n\n(30)\n\nTherefore if the prior on parameters pξ, θq defining SCMs is non-zero for any M P MpGq, also the posterior converges,\n\nP pl ă EPπ ̊ ry | xs ă u | ̄vq Ñp 1,\n\n(31)\n\nwhich is the definition credible intervals rl0, u0s as the 0th and 100th quantiles of the posterior distribution which coincide with rl, us asymptotically.\n\nB EXPERIMENTAL DETAILS\n\nAll experiments use 1000 burn-in MCMC samples that are discarded and 5000 MCMC samples considered as independent samples from the posterior distribution and used for the approximation of target queries.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nB.1 SCMS FOR THE SMOKING AND LUNG CANCER EXAMPLE\n\nUsing the functional dependencies specified by the selection diagram in Fig. 4, we define the SCMs for domains πUK, πFR, and πSW as follows.\n\nFor πUK we generate samples from P puwq, P pusq, P putq and P puscq given by independent Gaussian distributions with mean 0 and variance 1. Each generated puw, us, ut, uscq leads to a sample pw, s, t, cq as follows: w Ð 1tuw ą 0u, s Ð 1tw ` usc ` us ́ 2 ą 0u, t Ð 1ts ́ 0.5ut ́ 1 ą 0u, c Ð 1tt ́ 0.5w ` usc ́ 1 ą 0u. For πFR we generate samples from P puwq, P pusq, P putq and P puscq given by independent Gaussian distributions with mean 0 and variance 1. Each generated puw, us, ut, uscq leads to a sample pw, s, t, cq as follows: w Ð 1tuw ą 0u, s Ð 1tw ` usc ` 1.5us ́ 1 ą 0u, t Ð 1ts ́ 0.5ut ́ 1 ą 0u, c Ð 1tt ́ 0.5w ` usc ́ 1 ą 0u. Notice that the causal mechanism for S has changed while everything else is unchanged.\n\nFor πSW we generate samples from P puwq, P pusq, P putq and P puscq given by independent Gaussian distributions with mean 0 and variance 1. Each generated puw, us, ut, uscq leads to a sample pw, s, t, cq as follows: w Ð 1tuw ą 0.5u, s Ð 1tw ` usc ` us ́ 2 ą 0u, t Ð 1ts ́ 0.5ut ́ 1 ą 0u, c Ð 1tt ́ 0.5w ` usc ́ 1 ą 0u.\n\nB.2 SCMS FOR THE NEURODEGENERATIVE DISEASE PREDICTION EXAMPLE\n\nUsing the functional dependencies specified by the selection diagrams in Fig. 1, we define the SCMs for domains π ̊, πa, and πb as follows.\n\nFor the target domain π ̊ we generate samples from P puwyq, P pux2 , P puwq and P pux1,x2 q given by independent Gaussian distributions with mean 0 and variance 1. Each generated puwy, ux1,x2 , ux2, uwq leads to a sample px1, x2, w, yq as follows: x1 Ð 1tux1 ą 0u, x2 Ð 1tux1,x2 ` ux2 ą 0u, w Ð 1tx1 ` uwy ` 1.5uw ́ 1 ą 0u, y Ð 1tw ́ uwy ` 0.1x1 ́ 1 ą 0u. For source domain πa, the distribution of exogenous as well as structural assignment agree with π ̊ except in the assignment of W which is given by w Ð 1tx1 ` uwy ́ uw ` 1 ą 0u.\n\nFor source domain πb, the distribution of exogenous as well as structural assignment agree with π ̊ except in the assignment of W and X1. The selection diagram specifies that the assignment of W agrees with πa and is thus given by w Ð 1tx1 ` uwy ́ uw ` 1 ą 0u while the assignment of X1 changes and is given by x1 Ð 1tux1 ́ 0.5 ą 0u. All other components of the SCM are the same.\n\n17",
    "reference": "# Summary Of The Paper\n\nThe paper formulates partial transportability to bound the query in the target domain in the domain generalization problem. The authors show that invariance learning is a special case of partial transportability tasks and show that invariant predictors and more general solutions to robust optimization problems. They evaluate the algorithm on a synthetic dataset.\n\n# Strength And Weaknesses\n\nPros: \n\n1. It’s novel to introduce partial transportability to the DG problem.\n\n2. Instead of the point estimate, deriving a tight bound of the conditional expectation in the unseen domain is novel.\n\n3. Illustrating invariance learning as a special case of partial transportability task is novel.\n\nCons:\n\n1. A related work is necessary to demonstrate the background of the methods used in this paper, for example, transportability problem, etc.\n\n2. Lacking datasets in experiment evaluations. The authors only evaluate the algorithm on one dataset. \n\n3. The author should review [1] in detail and describe the differences because their proposed method has some overlaps with this paper. \n\n[1] Zhang, Junzhe, Jin Tian, and Elias Bareinboim. \"Partial counterfactual identification from observational and experimental data.\" International Conference on Machine Learning. PMLR, 2022.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well organized\n\n# Summary Of The Review\n\nThis paper tackles the domain generalization problem with an under-studied causal method and models each domain as a different causal graph (SCM). Considering this paper researching an under-studied method in the DG field, I vote for a weak accept.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCAUSAL IMITATION LEARNING VIA INVERSE REINFORCEMENT LEARNING\n\nKangrui Ruan∗, Junzhe Zhang∗, Xuan Di, and Elias Bareinboim Columbia University, New York, NY 10027, USA {kr2910,junzhez,sharon.di,eliasb}@columbia.edu\n\nABSTRACT\n\nOne of the most common ways children learn when unfamiliar with the environment is by mimicking adults. Imitation learning concerns an imitator learning to behave in an unknown environment from an expert’s demonstration; reward signals remain latent to the imitator. This paper studies imitation learning through causal lenses and extends the analysis and tools developed for behavior cloning (Zhang, Kumor, Bareinboim, 2020) to inverse reinforcement learning. First, we propose novel graphical conditions that allow the imitator to learn a policy performing as well as the expert’s behavior policy, even when the imitator and the expert’s state-action space disagree, and unobserved confounders (UCs) are present. When provided with parametric knowledge about the unknown reward function, such a policy may outperform the expert’s. Also, our method is easily extensible and allows one to leverage existing IRL algorithms even when UCs are present, including the multiplicative-weights algorithm (MWAL) (Syed & Schapire, 2008) and the generative adversarial imitation learning (GAIL) (Ho & Ermon, 2016). Finally, we validate our framework by simulations using real-world and synthetic data.\n\n1\n\nINTRODUCTION\n\nReinforcement Learning (RL) has been deployed and shown to perform extremely well in highly complex environments in the past decades (Sutton & Barto, 1998; Mnih et al., 2013; Silver et al., 2016; Berner et al., 2019). One of the critical assumptions behind many of the classical RL algorithms is that the reward signal is fully observed, and the reward function could be well-specified. In many real-world applications, however, it might be impractical to design a suitable reward function that evaluates each and every scenario (Randløv & Alstrøm, 1998; Ng et al., 1999). For example, in the context of human driving, it is challenging to design a precise reward function, and experimenting in the environment could be ill-advised; still, watching expert drivers operating is usually feasible.\n\nIn machine learning, the imitation learning paradigm investigates the problem of how an agent should behave and learn in an environment with an unknown reward function by observing demonstrations from a human expert (Argall et al., 2009; Billard et al., 2008; Hussein et al., 2017; Osa et al., 2018). There are two major learning modalities that implements IL – behavioral cloning (BC) (Widrow, 1964; Pomerleau, 1989; Muller et al., 2006; M ̈ulling et al., 2013; Mahler & Goldberg, 2017) and inverse reinforcement learning (IRL) Ng et al. (2000); Ziebart et al. (2008); Ho & Ermon (2016); Fu et al. (2017). BC methods directly mimic the expert’s behavior policy by learning a mapping from observed states to the expert’s action via supervised learning. Alternatively, IRL methods first learn a potential reward function under which the expert’s behavior policy is optimal. The imitator then obtains a policy by employing standard RL methods to maximize the learned reward function. Under some common assumptions, both BC and IRL are able to obtain policies that achieve the expert’s performance (Kumor et al., 2021; Swamy et al., 2021). Moreover, when additional parametric knowledge about the reward function is provided, IRL may produce a policy that outperforms the expert’s in the underlying environment (Syed & Schapire, 2008; Li et al., 2017; Yu et al., 2020).\n\nFor concreteness, consider a learning scenario depicted in Fig. 1a, describing trajectories of humandriven cars collected by drones flying over highways (Krajewski et al., 2018; Etesami & Geiger, 2020). Using such data, we want to learn a policy X ← π(Z) deciding on the acceleration (action) X ∈\n\n∗\n\nEqual contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nZ\n\nX\n\nY\n\nU (a)\n\nU2\n\nY\n\nU1\n\nX\n\nZ\n\nU (b)\n\nZ2\n\nX2\n\nZ1\n\nY\n\nZ1\n\nX1\n\nU (c)\n\nZ3\n\nY3\n\nX3\n\nY1\n\nX1\n\nY2\n\nX2\n\nU\n\nZ2\n\n(d)\n\nFigure 1: Causal diagrams where X represents an action (shaded red) and Y represents a latent reward (shaded blue). Input covariates of the policy scope S are shaded in light red.\n\n{0, 1} of the demonstrator car based on velocities and locations Z of surrounding cars. The driving performance is measured by a latent reward signal Y . Consider an instance where Y ← (1 − X)Z + X(1−Z) and values of Z are drawn uniformly over {0, 1}. A human expert generates demonstrations following a behavior policy such that P (X = 1 | Z = 0) = 0.6 and P (X = 0 | Z = 1) = 0.4. Evaluating the expert’s performance gives E[Y ] = P (X = 1, Z = 0) + P (X = 0, Z = 1) = 0.5. Now we apply standard IRL algorithms to learn a policy X ← π(Z) so that the imitator’s driving performance, denoted by E[Y | do(π)], is at least as good as the expert’s performance E[Y ]. Detailed derivations of IRL policy are shown in (Ruan et al., 2023, Appendix A). Note that E[Y |z, x] = x + z − 2xz belongs to a family of reward functions fY (x, z) = αx + βz − γxz, where 0 < α < γ. E [fY (X, Z)]−E [fY (X, Z) | do(π)]. A typical IRL imitator solves a minimax problem minπ maxfY The inner step “guesses” a reward function being optimized by the expert; while the outer step learns a policy maximizing the learned reward function. Applying these steps leads to a policy π∗ : X ← ¬Z with the expected reward E[Y | do(π∗)] = 1, which outperforms the sub-optimal expert.\n\nDespite the performance guarantees provided by existing imitation methods, both BC and IRL rely on the assumption that the expert’s input observations match those available to the imitator. More recently, there exists an emerging line of research under the rubric of causal imitation learning that augments the imitation paradigm to account for environments consisting of arbitrary causal mechanisms and the aforementioned mismatch between expert and imitator’s sensory capabilities (de Haan et al., 2019; Zhang et al., 2020; Etesami & Geiger, 2020; Kumor et al., 2021). Closest to our work, Zhang et al. (2020); Kumor et al. (2021) derived graphical criteria that completely characterize when and how BC could lead to successful imitation even when the agents perceive reality differently. Still, it is unclear how to perform IRL-type training if some expert’s observed states remain latent to the imitator, which leads to the presence of unobserved confounding (UCs) in expert’s demonstrations. Perhaps surprisingly, naively applying IRL methods when UCs are present does not necessarily lead to satisfactory performance, even when the expert itself behaves optimally.\n\nTo witness, we now modify the previous highway driving scenario to demonstrate the challenges of UCs. In reality, covariates Z (i.e., velocities and location) are also affected by the car horn U1 of surrounding vehicles and the wind condition U2. However, due to the different perspectives of drones (recording from the top), such critical information (i.e, U1, U2 ) is not recorded by the camera and thus remains unobserved. Fig. 1b graphically describes this modified learning setting. More specifically, consider an instance where Z ← U1 ⊕ U2, Y ← ¬X ⊕ Z ⊕ U2; ⊕ is the exclusive-or operator; and values of U1 and U2 are drawn uniformly over {0, 1}. An expert driver, being able to hear the car horn U1, follows a behavior policy X ← U1 and achieves the optimal performance E[Y ] = 1. Meanwhile, observe that E[Y |z, x] = 1 belongs to a family of reward functions fY (x, z) = α (where E [fY (X, Z)] − E [fY (X, Z) | do(π)] leads to an IRL policy π∗ with α > 0). Solving minπ maxfY expected reward E[Y |do(π∗)] = 0.5, which is far from the expert’s optimal performance E[Y ] = 1.\n\nAfter all, a question that naturally arises is, under what conditions an IRL imitator procedure can perform well when UCs are present, and there is a mismatch between the perception of the two agents? In this paper, we answer this question and, more broadly, investigate the challenge of performing IRL through causal lenses. In particular, our contributions are summarized as follows. (1) We provide a novel, causal formulation of the inverse reinforcement learning problem. This formulation allows one to formally study and understand the conditions under which an IRL policy is learnable, including in settings where UCs cannot be ruled out a priori. (2) We derive a new graphical condition for deciding whether an imitating policy can be computed from the available data and knowledge, which provides a robust generalization of current IRL algorithms to non-Markovian settings, including GAIL (Ho & Ermon, 2016) and MWAL (Syed & Schapire, 2008). (3) Finally, we move beyond this graphical condition and develop an effective IRL algorithm for structural causal models (Pearl, 2000) with\n\n2\n\nPublished as a conference paper at ICLR 2023\n\narbitrary causal relationships. Due to the space constraints, all proofs are provided in (Ruan et al., 2023, Appendix B). For a more detailed survey on imitation learning and causal inference, we refer readers to (Ruan et al., 2023, Appendix E).\n\n1.1 PRELIMINARIES\n\nWe use capital letters to denote random variables (X) and small letters for their values (x). DX represents the domain of X and PX the space of probability distributions over DX . For a set X, let |X| denote its dimension. The probability distribution over variables X is denoted by P (X). Similarly, P (Y | X) represents a set of conditional distributions P (Y | X = x) for all realizations x. We use abbreviations P (x) for probabilities P (X = x); so does P (Y = y | X = x) = P (y | x). Finally, indicator function 1{Z = z} returns 1 if Z = z holds true; otherwise 0.\n\nThe basic semantic framework of our analysis rests on structural causal models (SCMs) (Pearl, 2000, Ch. 7). An SCM M is a tuple ⟨U , V , F, P (U )⟩ with V the set of endogenous, and U exogenous variables. F is a set of structural functions s.t. for fV ∈ F, V ← fV (pa V , uV ), with PAV ⊆ V , UV ⊆ U . Values of U are drawn from an exogenous distribution P (U ), inducing distribution P (V ) over endogenous variables V . Since the learner can observe only a subset of endogenous variables, we split V into a partition O ∪ L where variable O ⊆ V are observed and L = V \\ O remain latent to the leaner. The marginal distribution P (O) is thus referred to as the observational distribution. An atomic intervention on a subset X ⊆ V , denoted by do(x), is an operation where values of X are set to constants x, replacing the functions fX = {fX : ∀X ∈ X} that would normally determine their values. For an SCM M , let Mx be a submodel of M induced by intervention do(x). For a set Y ⊆ V , the interventional distribution P (s|do(x)) induced by do(x) is defined as the distribution over Y in the submodel Mx, i.e., PM (Y |do(x)) ≜ PMx(Y ). We leave M implicit when it is obvious from the context.\n\nEach SCM M is associated with a causal diagram G which is a directed acyclic graph where (e.g., see Fig. 1) solid nodes represent observed variables O, dashed nodes represent latent variables L, and arrows represent the arguments PAV of each function fV ∈ F. Exogenous variables U are not explicitly shown; a bi-directed arrow between nodes Vi and Vj indicates the presence of an unobserved confounder (UC) affecting both Vi and Vj. We will use family abbreviations to represent graphical relationships such as parents, children, descendants, and ancestors. For example, the set of parent nodes of X in G is denoted by pa(X)G = ∪X∈X pa(X)G; ch, de and an are similarly defined. Capitalized versions Pa, Ch, De, An include the argument as well, e.g. Pa(X)G = pa(X)G ∪ X. For a subset X ⊆ V , the subgraph obtained from G with edges outgoing from X / incoming into X removed is written as GX /GX respectively. G[X] is a subgraph of G containing only nodes X and edges among them. A path from a node X to a node Y in G is a sequence of edges, which does not include a particular node more than once. Two sets of nodes X, Y are said to be d-separated by a third set Z in a DAG G, denoted by (X ⊥⊥ Y |Z)G, if every edge path from nodes in X to nodes in Y is “blocked” by nodes in Z. The criterion of blockage follows (Pearl, 2000, Def. 1.2.3). For a more detailed survey on SCMs, we refer readers to (Pearl, 2000; Bareinboim et al., 2022).\n\n2 CAUSAL INVERSE REINFORCEMENT LEARNING\n\nWe investigate the sequential decision-making setting concerning a set of actions X, a series of covariates Z, and a latent reward Y in an SCM M . An expert (e.g., a physician, driver), operating in SCM M , selects actions following a behavior policy, which is the collection of structural functions fX = {fX | X ∈ X}. The expert’s performance is evaluated as the expected reward E[Y ]. On the other hand, a learning agent (i.e., the imitator) intervenes on actions X following an ordering X1 ≺ · · · ≺ Xn; each action Xi is associated with a set of features PA∗ i ⊆ O \\ {Xi}. A policy π over actions X is a sequence of decision rules π = {π1, . . . , πn}. Each decision rule πi(Xi | Zi) is a probability distribution over an action Xi ∈ X, conditioning on values of a set of covariates Zi ⊆ PA∗ i . Such policies π are also referred to as dynamic treatment regimes (Murphy et al., 2001; Chakraborty & Murphy, 2014), which generalize personalized medicine to time-varying treatment settings in healthcare, in which treatment is repeatedly tailored to a patient’s dynamic state.\n\nA policy intervention on actions X following a policy π, denoted by do(π), entails a submodel Mπ from a SCM M where structural functions fX associated with X (i.e., the expert’s behavior policy) are replaced with decision rules Xi ∼ πi(Xi | Zi) for every Xi ∈ X. A critical assumption\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nthroughout this paper is that submodel Mπ does not contain any cycles. Similarly, the interventional distribution P (V | do(π)) induced by policy π is defined as the joint distribution over V in Mπ.\n\nThroughout this paper, detailed parametrizations of the underlying SCM M are assumed to be unknown to the agent. Instead, the agent has access to the input: (1) a causal diagram G associated with M , and (2) the expert’s demonstrations, summarized as the observational distribution P (O). The goal of the agent is to output an imitating policy π∗ that achieves the expert’s performance. Definition 1. For an SCM M = ⟨U , V , F, P (U )⟩, an imitating policy π∗ is a policy such that its expected reward is lower bounded by the expert’s reward, i.e., EM [Y | do(π∗)] ≥ EM [Y ].\n\nIn words, the right-hand side is the expert’s performance that the agent wants to achieve, while the left-hand side is the real reward experienced by the agent. The challenge in imitation learning arises from the fact that the reward Y is not specified and latent, i.e., Y ̸∈ O. This precludes approaches that identify E[Y |do(π)] directly from the demonstration data (e.g., through the do- or soft-do-calculus Pearl (2000); Correa & Bareinboim (2020)).\n\nThere exist methods in the literature for finding an imitating policy in Def. 1. Before describing their details, we first introduce some necessary concepts. For any policy π, we summarize its associated state-action domain using a sequence of pairs of variables called a policy scope S. Definition 2 (Lee & Bareinboim (2020)). For an SCM M , a policy scope S (for short, scope) over actions X is a sequence of tuples {⟨Xi, Zi⟩}n\n\ni=1 where Zi ⊆ PA∗\n\ni for every Xi ∈ X.\n\nWe will consistently use π ∼ S to denote a policy π associated with scope S. For example, consider a policy scope S = {⟨X1, {Z1}⟩, ⟨X2, {Z2}⟩} over actions X1, X2 in Fig. 1c. A policy π ∼ S is a sequence of distributions π = {π1(X1 | Z1), π2(X2 | Z2)}.\n\nZhang et al. (2020); Kumor et al. (2021) provide a graphical condition that is sufficient for learning an imitating policy via behavioral cloning (BC) provided with a causal diagram G. For a policy scope S = {⟨Xi, Zi⟩}n i=1, let G(i), i = 1, . . . , n, denote a manipulated graph obtained from G by the following steps: for all j = i + 1, . . . , n, (1) remove arrows coming into every action Xj; and (2) add direct arrows from nodes in Zj to Xj. Formally, the sequential π-backdoor criterion is defined as: Definition 3 (Kumor et al. (2021)). Given a causal diagram G, a policy scope S = {⟨Xi, Zi⟩}n i=1 is said to satisfy the sequential π-backdoor criterion in G (for short, π-backdoor admissible) if at each Xi ∈ X, one of the following conditions hold: (1) Xi is not an ancestor of Y in G(i), i.e., X ̸∈ An(Y )G(i); or (2) Zi blocks all backdoor path from Xi to Y in G(i), i.e., (Y ⊥⊥ Xi|Zi) in G(i) .\n\nXi\n\n(Kumor et al., 2021) showed that whenever a π-backdoor admissible scope S is available, one could learn an imitating policy π∗ ∼ S by setting π∗ i (xi | zi) = P (xi | zi) for every action Xi ∈ X. For instance, consider the causal diagram G in Fig. 1c. Scope S = {⟨X1, {Z1}⟩, ⟨X2, {Z2}⟩} is π-backdoor admissible since (X1 ⊥⊥ Y |Z1) and (X2 ⊥⊥ Y |Z2) hold in G, which is a super graph containing both manipulated G(1) and G(2). An imitating policy π∗ = {π∗ 2} is thus obtainable by setting π∗ 2(X2 | Z2) = P (X2 | Z2). While impressive, a caveat of their results is that the performance of the imitator is restricted by that of the expert, i.e., E[Y | do(π∗)] = E[Y ]. In other words, causal BC provides an efficient way to mimic the expert’s performance. If the expert’s behavior is far from optimal, the same will hold for the learning agent.\n\n1(X1 | Z1) = P (X1 | Z1) and π∗\n\n1, π∗\n\n2.1 MINIMAL SEQUENTIAL BACKDOOR CRITERION\n\nTo circumvent this issue, we take a somewhat different approach to causal imitation by incorporating the principle of inverse reinforcement learning (IRL) principle. Following the game-theoretic approach (Syed & Schapire, 2008), we formulate the problem as learning to play a two-player zero-sum game in which the agent chooses a policy, and the nature chooses an SCM instance. A key property of this algorithm is that it allows us to incorporate prior parametric knowledge about the latent reward signal. When such knowledge is informative, our algorithm is about to obtain a policy that could significantly outperform the expert with respect to the unknown causal environment, while at the same time are guaranteed to be no worse. Formally, let M = {∀M | GM = G, PM (O) = P (O)} denote the set of SCMs compatible with both the causal diagram G and the observational distribution P (O). Fix a policy scope S. Now consider the optimization problem defined as follows.\n\nν∗ = min\n\nπ∼S\n\nmax M ∈M\n\nEM [Y ] − EM [Y | do(π)].\n\n(1)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nThe inner maximization in the above equation can be viewed as an causal IRL step where we attempt to “guess” a worst-case SCM ˆM compatible with G and P (O) that prioritizes the expert’s policy. That is, the gap in the performance between the expert’s and the imitator’s policies is maximized. Meanwhile, since the expert’s reward EM [Y ] is not affected by the imitator’s policy π, the outer minimization is equivalent to a planning step that finds a policy π∗ optimizing the learned SCM ˆM . Obviously, the solution π∗ is an imitating policy if gap ν∗ = 0. In cases where the expert is sub-optimal, i.e., E ˆM [Y ] < E ˆM [Y | do(π)] for some policies π, we may have ν∗ < 0. That is, the policy π∗ will dominate the expert’s policy fX regardless of parametrizations of SCM M in the worst-case scenario. In other words, π∗ to some extent ignores the sub-optimal expert, and instead exploits prior knowledge about the underlying model.\n\nDespite the clear semantics in terms of causal models, the optimization problem in Eq. (1) requires the learner to search over all possible SCMs compatible with the causal diagram G and observational distribution P (O). In principle, it entails a quite challenging search since one does not have access to the parametric forms of the underlying structural functions F nor the exogenous distribution P (U ). It is not clear how the existing optimization procedures can be used.\n\nIn this paper, we will develop novel methods to circumvent this issue, thus leading to effective imitating policies. Our first algorithm relies on a refinement of the sequential π-backdoor, based on the concept of minimality. A subscope S ′ of a policy scope S = {⟨Xi, Zi⟩}n i=1, denoted by S ′ ⊆ S, is a sequence {⟨Xi, Z′ i ⊆ Zi for every Xi ∈ X. A proper subscope S ′ ⊂ S is a subscope in S other than S itself. The minimal π-backdoor admissible scope is defined as follows. Definition 4. Given a causal diagram G, a π-backdoor admissible scope S is said to be minimal if there exists no proper subscope S ′ ⊂ S satisfying the sequential π-backdoor in G.\n\ni=1 where Z′\n\ni⟩}n\n\nTheorem 1. Given a causal diagram G, if there exists a minimal π-backdoor admissible scope S = {⟨Xi, Zi⟩}n\n\ni=1 in G, consider the following conditions:\n\n1. Let effective actions X ∗ = X ∩ An(Y )GS and effective covariates Z∗ = (cid:83) 2. For i = 1, . . . , n + 1, let X ∗\n\n<i = {∀Xj ∈ X ∗ | j < i} and Z∗\n\n<i = (cid:83)\n\nXi∈X ∗ Zi; Zj.\n\nXj ∈X ∗\n\n<i\n\nThen, for any policy π ∼ S, the expected reward E[Y | do(π)] is computable from P (O, Y ) as:\n\nE[Y | do(π)] =\n\n(cid:88)\n\nx∗,z∗\n\nE[Y | x∗, z∗]ρπ(x∗, z∗)\n\n(2)\n\nwhere the occupancy measure ρπ(x∗, z∗) = (cid:81)\n\nXi∈X ∗ P (cid:0)zi | x∗\n\n<i, z∗\n\n<i\n\n(cid:1) πi(xi | zi).\n\nTo illustrate, consider again the causal diagram G in Fig. 1c; the manipulated diagram G(2) = G and G(1) is obtained from G by removing Z2 ↔ X2. While scope S1 = {⟨X1, {Z1}⟩, ⟨X2, {Z2}⟩} satisfies the sequential π-backdoor, it is not minimal since (X1 ⊥⊥ Y ) in G(1) . On the other hand, S2 = {⟨X1, ∅⟩, ⟨X2, {Z2}⟩} is minimal π-backdoor admissible since (X2 ⊥⊥ Y | Z2) holds true in G(2) ; and the covariate set {Z2} is minimal due to the presence of the backdoor path X2 ← Z2 → Y .\n\nX1\n\nX2\n\nLet us focus on the minimal π-backdoor admissible scope S2. Note that GS2 is a subgraph obtained from G by removing the bi-directed arrow Z2 ↔ X2. We must have effective actions X ∗ = {X1, X2} and effective covariates Z∗ = {Z2}. Therefore, Z∗ <3 = {Z2}. For any policy π ∼ S2, Thm. 1 implies E[Y | do(π)] = (cid:80) E[Y | x1, x2, z2]P (z2|x1)π2(x2|z2)π(x1). On the other hand, the same result in Thm. 1 does not necessarily hold for a non-minimal π-backdoor admissible scope. For instance, consider again the non-minimal scope S1 = {⟨X1, {Z1}⟩, ⟨X2, {Z2}⟩}. The expected reward E[Y | do(π)] of a policy π ∼ S2 is not computable from Eq. (2), and is ultimately not identifiable from distribution P (O, Y ) in G (Tian, 2008).\n\n<2 = ∅ and Z∗\n\n<1 = Z∗\n\nx1,x2,z2\n\n2.2\n\nIMITATION VIA INVERSE REINFORCEMENT LEARNING\n\nOnce a minimal π-backdoor admissible scope S is found, there exist effective procedures to solve for an imitating policy in Eq. (1). Let R be a hypothesis class containing all expected rewards EM [Y | x∗, z∗] compatible with candidate SCMs M ∈ M , i.e., R = {EM [Y | x∗, z∗] | ∀M ∈ M }. Applying the identification formula in Thm. 1 reduces the optimization problem in Eq. (1) as follows:\n\nν∗ = min\n\nπ∼S\n\nmax r∈R\n\n(cid:88)\n\nx∗,z∗\n\nr(x∗, z∗) (ρ(x∗, z∗) − ρπ(x∗, z∗))\n\n(3)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nwhere the expert’s occupancy measure ρ(x∗, z∗) = P (x∗, z∗) and the agent’s occupancy measure ρπ(x∗, z∗) is given by Eq. (2). The above minimax problem is solvable using standard IRL algorithms. The identification result in Thm. 1 ensures that the learned policy applies to any SCM compatible with the causal diagram and the observational data, thus robust to the unobserved confounding bias in the expert’s demonstrations. Henceforth, we will consistently refer to Eq. (3) as the canonical equation of causal IRL. In this paper, we solve for an imitating policy π∗ in Eq. (3) using state-of-the-art IRL algorithms, provided with common choices of parametric reward functions. These algorithms include the multiplicative-weights algorithm (MWAL) (Syed & Schapire, 2008) and the generative adversarial imitation learning (GAIL) (Ho & Ermon, 2016). We refer readers to Algs. 3 and 4 in (Ruan et al., 2023, Appendix C) for more discussions on the pseudo-code and implementation details.\n\nCausal MWAL (Abbeel & Ng, 2004; Syed & Schapire, 2008) study IRL in Markov decision processes where the reward function r(x∗, z∗) is a linear combination of k-length feature expectations vectors φ(x∗, z∗). Particularly, let r(x∗, z∗) = w · φ(x∗, z∗) for a coefficient vector w contained in a convex set Sk = (cid:8)w ∈ Rk | ∥w∥1 = 1 and w ⪰ 0(cid:9). Let φ(i) be the i-th component of feature vector φ and let deterministic policies with scope S be ordered by π(1), . . . , π(n). The canonical equation in Eq. (3) is reducible to a two-person zero-sum matrix game under linearity. Proposition 1. For a hypothesis class R = {r = w · φ | w ∈ Sk}, the solution ν∗ of the canonical equation in Eq. (3) is obtainable by solving the following minimax problem:\n\nν∗ = min\n\nπ∼S\n\nmax w∈Sk\n\nw⊤Gπ,\n\n(4)\n\nwhere G is a k × n matrix given by G(i, j) = (cid:80)\n\nx∗,z∗ φ(i)(x∗, z∗) (ρ(x∗, z∗) − ρπ(j)(x∗, z∗)).\n\nThere exist effective multiplicative weights algorithms for solving the matrix game in Eq. (4), including MW (Freund & Schapire, 1999) and MWAL (Syed & Schapire, 2008).\n\nCausal GAIL (Ho & Ermon, 2016) introduces the GAIL algorithm for learning an imitating policy in Markov decision processes with a general family of non-linear reward functions. In particular, r(x∗, z∗) takes values in the real space R, i.e., r ∈ RX ∗,Z∗ = {r : DX ∗ × DZ∗ (cid:55)→ R}. The complexity of reward function r is penalized by a convex regularization function ψ(r), i.e.,\n\nwhere RX ∗,Z∗\n\nν∗ = min\n\nπ∼S\n\nmax r∈RX×Z\n\n(cid:88)\n\nx∗,z∗\n\nr(x∗, z∗) (ρ(x∗, z∗) − ρπ(x∗, z∗)) − ψ(r)\n\n(5)\n\nHenceforth, we will consistently refer to Eq. (5) as the penalized canonical equation of causal IRL. It is often preferable to solve its conjugate form. Formally, Proposition 2. For a hypothesis class R = {r : DX ∗ × DZ∗ (cid:55)→ R} regularized by ψ, the solution ν∗ of the penalized canonical equation in Eq. (5) is obtainable by solving the following problem:\n\nν∗ = min\n\nπ∼S\n\nψ∗ (ρ − ρπ)\n\n(6)\n\nwhere ψ∗ be a conjugate function of ψ and is given by ψ∗ = maxr∈RX×Z a⊤r − ψ(r).\n\nEq. (6) seeks a policy π which minimizes the divergence of the occupancy measures between the imitator and the expert, as measured by the function ψ∗. The computational framework of generative adversarial networks (Goodfellow et al., 2014) provides an effective approach to solve such a matching problem, e.g., the GAIL algorithm (Ho & Ermon, 2016).\n\n3 CAUSAL IMITATION WITHOUT SEQUENTIAL BACKDOOR\n\nIn this section, we investigate causal IRL beyond the condition of minimal sequential π-backdoor. Observe that the key to the reduction of the canonical causal IRL equation in Eq. (3) lies in the identification of expected rewards E[Y | do(π)] had the latent reward Y been observed. Next we will study general conditions under which E[Y | do(π)] is uniquely discernible from distribution P (O, Y ) in the causal diagram G, called the identifiability of causal effects (Pearl, 2000, Def. 3.2.4).\n\nDefinition 5 (Identifiability). Given a causal diagram G and a policy π ∼ S, the expected reward E[Y | do(π)] is said to be identifiable from distribution P (O, Y ) in G if E[Y | do(π)] is uniquely computable from P (O, Y ) in any SCM M compatible with G.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nWe say a policy scope S is identifiable (from P (O, Y ) in G) if for all policies π ∼ S, the corresponding expected rewards E[Y | do(π)] are identifiable from P (O, Y ) in G. Our next result shows that whenever an identifiable policy scope S is found, one could always reduce the causal IRL problem to the canonical optimization equation in Eq. (3).\n\nTheorem 2. Given a causal diagram G, a policy scope S is identifiable from P (O, Y ) in G if and only if for any policy π ∼ S, the expected reward E[Y | do(π)] is computable from P (O, Y ) as\n\nE[Y | do(π)] =\n\n(cid:88)\n\nx∗,z∗\n\nE[Y | x∗, z∗]ρπ(x∗, z∗)\n\n(7)\n\nwhere subsets X ∗ ⊆ X, Z∗ ⊆ O \\ X; and the imitator’s occupancy measure ρπ(x∗, z∗) is a function of the observational distribution P (O) and policy π.\n\nThm. 2 suggests a general procedure to learn an imitating policy via causal IRL. Whenever an identifiable scope S is found, the identification formula in Eq. (7) permits one to reduce the optimization problem in Eq. (1) to the canonical equation in Eq. (3). One could thus obtain an imitating policy π ∼ S by solving Eq. (3) where the expert’s occupancy measure ρ(x∗, z∗) = P (x∗, z∗) and the imitator’s occupancy measure ρπ(x∗, z∗) is given by Eq. (7). As an example, consider the frontdoor diagram described in Fig. 2a and a policy scope Figure 2: Frontdoor S = {⟨X, ∅⟩}. The expected reward E[Y | do(π)] = (cid:80) x′ E[Y | do(x′)]π(x′) and E[Y | do(x′)] is identifiable from P (X, Y, Z) using the frontdoor adjustment formula (Pearl, 2000, Thm. 3.3.4). The expected reward E[Y | do(π)] of any policy π(X) could be written as:\n\nZ (b) GS\n\n(a) G\n\nY\n\nX\n\nX\n\nY\n\nZ\n\nE[Y | do(π)] =\n\n(cid:88)\n\nz,x\n\nE[Y | x, z]P (x)\n\nP (z|x′)π(x′).\n\n(cid:88)\n\nx′\n\n(8)\n\nLet occupancy measures ρ(x, z) = P (x, z) and ρπ(x, z) = P (x) (cid:80) learn an imitating policy in the frontdoor diagram by solving the canonical equation given by:\n\nx′ P (z|x′)π(x′). We could thus\n\nν∗ = min\n\nπ∼S\n\nmax r∈R\n\n(cid:88)\n\nx,z\n\nr(x, z) (ρ(x, z) − ρπ(x, z)) ,\n\n(9)\n\nwhere R is a hypothesis class of the reward function r(x, z) ≜ E[Y | x, z]. The solution π∗(X) is an imitating policy performing at least as well as the expert’s behavior policy if the gap ν∗ ≤ 0.\n\nNext, we will describe how to obtain the identification formula in Eq. (7) provided with an identifiable scope S. Without loss of generality, we will assume that the reward Y is the only endogenous variable that is latent in the causal diagram G, i.e., V = O ∪ {Y }. We will utilize a special type of clustering of nodes in the causal diagram G, called the confounded component (for short, c-component).\n\n∗\n\nDefinition 6 (C-component (Tian & Pearl, 2002)). For a causal diagram G, a subset C ⊆ V is a c-component if any pair Vi, Vj ∈ C is connected by a bi-directed path in G.\n\nFor instance, the frontdoor diagram in Fig. 2a contains two c-components C1 = {X, Y } and C2 = {Z}. We will utilize a sound and complete procedure IDENTIFY (Tian, 2002; 2008) for identifying causal effects E[Y | do(π)] of an arbitrary policy π ∼ S. Particularly, IDENTIFY takes as input the causal diagram G, a reward Y , and a policy scope S. It returns an identification formula for E[Y | do(π)] from P (O, Y ) if expected rewards of all policies π ∼ S are identifiable. Otherwise, IDENTIFY(G, Y, S) = “FAIL”. Details of IDENTIFY are shown in (Zhang et al., 2020, Appendix B). Recall that GS is the causal diagram of submodel Mπ induced by policy π ∼ S. Fig. 2b shows diagram GS obtained from the frontdoor graph G and scope S = {⟨X, ∅⟩} described in Fig. 2a. Let ZY = An(Y ) be ancestors of Y in GS . Our next result shows that IDENTIFY(G, Y, S) is ensured to find an identification formula of the form in Eq. (7) when it is identifiable.\n\nLemma 1. Given a causal diagram G, a policy scope S is identifiable from P (O, Y ) in G if and only if IDENTIFY(G, Y, S) ̸= “FAIL”. Moreover, IDENTIFY(G, Y, S) returns an identification formula of the form in Eq. (7) where X ∗ = Pa(CY ) ∩ X and Z∗ = Pa(CY ) \\ ({Y } ∪ X); and CY is a c-component containing reward Y in subgraph G[An(ZY )].\n\n∗\n\nOtherwise, one could always simplify the diagram G and project other latent variables L \\ {Y } using the projection algorithm (Tian, 2002, Sec. 4.5), without affecting the identifiability of target query E[Y | do(π)].\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFor example, for the frontdoor diagram G in Fig. 2a, the manipulated diagram GS with scope S = {⟨X, ∅⟩} is described in Fig. 2b. Since ZY = An(Y )GS = {X, Z, Y }, CY is thus given by {X, Y }. Lem. 1 implies that X ∗ = Pa({X, Y }) ∩ {X} = {X} and Z∗ = Pa({X, Y }) \\ {X, Y } = {Z}. Applying IDENTIFY(G, Y, {⟨X, ∅}) returns the frontdoor adjustment formula in Eq. (8).\n\n3.1 SEARCHING FOR IDENTIFIABLE POLICY SCOPES\n\nThe remainder of this section describes an effective algorithm to find identifiable policy scopes S had the latent reward signal Y been observed. Let S denote the collection of all identifiable policy scopes S from distribution P (O, Y ) in the causal diagram G. Our algorithm LISTIDSCOPE, described in Alg. 1, enumerates elements in S. It takes as input a causal diagram G, a reward signal Y , and subsets L = ∅ and R = (cid:83)n i . More specifically, LISTIDSCOPE maintains two scopes Sl ⊆ Sr (Step 2). It performs backtrack search to find identifiable scopes S in G such that Sl ⊆ S ⊆ Sr. It aborts branches that either (1) all subscopes in Sr are identifiable (Step 3); or (2) all subscopes containing Sl are non-identifiable (Step 6). The following proposition supports our aborting criterion. Lemma 2. Given a causal diagram G, for policy scopes S ′ ⊆ S, S ′ is identifiable from distribution P (O, Y ) in G if S is identifiable from P (O, Y ) in G.\n\ni=1 PA∗\n\nAt Step 7, LISTIDSCOPE picks an arbitrary variable V that is included in input covariates R but not in L. It then recursively returns all identifiable policy scopes S in G: the first recursive call returns scopes taking V as an input for some actions Xi ∈ X and the second call return all scopes that do not consider V when selecting values for all actions X. We say a policy π is associated with a collection of policy scopes S, denoted by π ∼ S, if there exists S ∈ S so that π ∼ S. It is possible to show that LISTIDSCOPE produces a collection of identifiable scopes that is sufficient for the imitation task.\n\nAlgorithm 1: LISTIDSCOPE\n\n1: Input: G, Y and subsets L ⊆ R 2: Output: a set of identifiable policy scopes S 3: Let scopes Sr = {⟨Xi, R ∩ PA∗\n\ni=1 and\n\ni ⟩}n\n\nSl = {⟨Xi, L ∩ PA∗\n\ni ⟩}n\n\ni=1.\n\n′′ then\n\nOutput Sr.\n\n4: if IDENTIFY(G, Y, Sr) ̸= “FAIL 5: 6: end if 7: if IDENTIFY(G, Y, Sl) ̸= “FAIL Pick an arbitrary V ∈ R \\ L. 8: LISTIDSCOPE(G, Y, L ∪ {V }, R). 9: LISTIDSCOPE(G, Y, L, R \\ {V }). 10: 11: end if\n\n′′ then\n\nTheorem 3. For a causal diagram G and a reward Y , LISTIDSCOPE(G, Y, ∅, (cid:83)n there is π∗ ∼ S∗ where E[Y | do(π)] = E[Y | do(π∗)].\n\ni=1 PA∗\n\ni ) enumerates a subset S∗ ⊆ S so that for any π ∼ S,\n\nMoreover, LISTIDSCOPE outputs identifiable policy scopes with a polynomial delay. This follows from the observation that LISTIDSCOPE searches over a tree of policy scopes with height at most |(cid:83)n\n\ni | and IDENTIFY(G, Y, S) terminates in polynomial steps w.r.t. the size of diagram G.\n\ni=1 PA∗\n\n4 EXPERIMENTS\n\nIn this section, we demonstrate our framework on various imitation learning tasks, ranging from synthetic causal models to real-world datasets, including highway driving (Krajewski et al., 2018) and images (LeCun, 1998). We find that our approach is able to incorporate parametric knowledge about the reward function and achieve effective imitating policies across different causal diagrams. For all experiments, we evaluate our proposed Causal-IRL based on the canonical equation formulation in Eq. (3). As a baseline, we also include: (1) standard BC mimicking the expert’s nominal behavior policy; (2) standard IRL utilizing all observed covariates preceding every Xi ∈ X while being blind to causal relationships in the underlying model; and (3) Causal-BC (Zhang et al., 2020; Kumor et al., 2021) that learn an imitating policy with the sequential π-backdoor criterion. We refer readers to (Ruan et al., 2023, Appendix D) for additional experiments and more discussions on the experimental setup.\n\nBackdoor Consider an SCM instance compatible with Fig. 1c including binary observed variables Z1, X1, Z2, X2, Y ∈ {0, 1}. Causal-BC utilizes a sequential π-backdoor admissible scope {⟨X1, {Z1}⟩, ⟨X2, {Z2}⟩}; while Causal-IRL utilizes the scope {⟨X1, ∅⟩, ⟨X2, {Z2}⟩} satisfying the minimal sequential π-backdoor. Simulation results, shown in Fig. 3a, reveal that Causal-IRL consistently outperforms the expert’s policy and other imitation strategies by exploiting additional parametric knowledge about the expected reward E[Y | X1, X2, Z2]; Causal-BC is able to achieve the expert’s performance. Unsurprisingly, neither BC nor IRL is able to obtain an imitating policy.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) Backdoor\n\n(b) Highway Driving\n\n(c) MNIST Digits\n\n(d) Infinite MDPUC\n\nFigure 3: Simulation results (a, b, c, d) for our experiments, where y-axis represents the expected reward of learned policies in the actual causal model; the grey dashed line denotes the expert’s reward.\n\nHighway Driving We consider a learning scenario where the agent learns a driving policy from the observed trajectories of a human expert. Causal diagram of this example is provided in (Ruan et al., 2023, Appendix D, Fig. 4) where X1 is the accelerations of the ego vehicle at the previous step; Z1 is both longitudinal and lateral historical accelerations of the ego vehicle two steps ago; X2 is the velocity of the ego vehicle; Z2 is the velocity of the preceding vehicle; W indicates the information from surrounding vehicles. Values of X1, X2, Z1, Z2 are drawn from a real-world driving dataset HighD Krajewski et al. (2018). The reward Y is decided by a non-linear function fY (X2, Z2, UY ). Both Causal-IRL and Causal-BC utilize the scope {⟨X1, ∅⟩, ⟨X2, {Z2}⟩}. Causal-IRL also exploits the additional knowledge that the expected reward E[Y | X1, X2, Z2] is a monotone function via reward augmentation (Li et al., 2017). Simulation results are shown in Fig. 3b. We found that Causal-IRL performs the best among all strategies. Causal-BC is able to achieve the expert’s performance. BC and IRL perform the worst among all and fail to obtain an imitating policy.\n\nMNIST Digits Consider again the frontdoor diagram in Fig. 2a. To evaluate the performance of our proposed approach in high-dimensional domains, we now replace variable Z with sampled images drawn from MNIST digits dataset (LeCun, 1998). The reward Y is decided by a linear function taking Z and an unobserved confounder UX,Y as input. The Causal-IRL formulates the imitation problem as a two-person zero-sum game through the frontdoor adjustment described in Eq. (9), which can be solved by the MW algorithm (Freund & Schapire, 1999; Syed & Schapire, 2008). As shown in Fig. 3c, simulation results reveal that Causal-IRL outperforms Causal-BC and BC; while IRL performs the worst among all the algorithms.\n\nInfinite MDPUC To demonstrate our proposed framework in the sequential decision-making setting with an infinite horizon, we consider a generalized Markov decision process incorporating unobserved confounders (Ruan & Di, 2022), called the MDPUC (Zhang & Bareinboim, 2022). This sequential model simulates real-world driving dynamics. By exploiting the Markov property over time steps, we are able to decompose the causal diagram over the infinite horizon into a collection of sub-graphs, one for each time step i = 1, 2, . . . . Fig. 1d shows the causal diagram spanning time steps i = 1, 2, 3. As a comparison, BC and IRL still utilize the stationary policy {⟨Xi, {Zi}⟩}. By applying Thm. 1 at each time step, we obtain a π-backdoor admissible policy scope {⟨Xi, {Zi, Xi−1, Zi−1}⟩} for Causal-IRL and Causal-BC. Simulation results are shown in Fig. 3d. One could see by inspection that Causal-IRL performs the best and achieves the expert’s performance.\n\n5 CONCLUSION\n\nThis paper investigates imitation learning via inverse reinforcement learning (IRL) in the semantical framework of structural causal models. The goal is to find an effective imitating policy that performs at least as well as the expert’s behavior policy from combinations of demonstration data, qualitative knowledge the data-generating mechanisms represented as a causal diagram, and quantitative knowledge about the reward function. We provide a graphical criterion (Thm. 1) based on the sequential backdoor, which allows one to obtain an imitating policy by solving a canonical optimization equation of causal IRL. Such a canonical formulation addresses the challenge of the presence of unobserved confounders (UCs), and is solvable by leveraging standard IRL algorithms (Props. 1 and 2). Finally, we move beyond the backdoor criterion and show that the canonical equation is achievable whenever expected rewards of policies are identifiable had the reward also been observed (Thms. 2 and 3).\n\n9\n\nBCIRLCausal BCCausal IRL0.00.20.40.60.81.0ExpertBCIRLCausal BCCausal IRL0.00.20.40.60.81.0ExpertBCIRLCausal BCCausal IRL0.00.20.40.60.81.0ExpertBCIRLCausal BCCausal IRL0.00.20.40.60.81.0ExpertPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nThis research was supported in part by the NSF, ONR, AFOSR, DoE, Amazon, JP Morgan, and The Alfred P. Sloan Foundation.\n\nETHICS STATEMENT\n\nThis paper investigates the theoretical framework of causal inverse RL from the natural trajectories of an expert demonstrator, even when the reward signal is unobserved. Input covariates used by the expert to determine the original values of the action are unknown, introducing unobserved confounding bias in demonstration data. Our framework may apply to various fields in reality, including autonomous vehicle development, industrial automation, and chronic disease management. A positive impact of this work is that we discuss the potential risk of training IRL policy from demonstrations with the presence of unobserved confounding (UC). Our formulation of causal IRL is inherently robust against confounding bias. For example, solving the causal IRL problem in Eq. (1) requires the imitator to learn an effective policy that maximizes the reward in a worst-case causal model where the performance gap between the expert and imitator is the largest possible. More broadly, automated decision systems using causal inference methods prioritize safety and robustness during their decision-making processes. Such requirements are increasingly essential since black-box AI systems are prevalent, and our understandings of their potential implications are still limited.\n\nREPRODUCIBILITY STATEMENT\n\nThe complete proof of all theoretical results presented in this paper, including Thms. 1 and 2, is provided in (Ruan et al., 2023, Appendix B). Details on the implementation of the proposed algorithms are included (Ruan et al., 2023, Appendix C). Finally, (Ruan et al., 2023, Appendix D) provides a detailed description of the experimental setup. Readers could find all appendices as part of the supplementary text after “References” section. We provided references to all existing datasets used in experiments, including HIGHD (Krajewski et al., 2018) and MNIST (LeCun, 1998). Other experiments are synthetic and do not introduce any new assets. Source codes for all experiments and simulations are released in the complete technical report (Ruan et al., 2023).\n\nREFERENCES\n\nPieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.\n\nBrenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning\n\nfrom demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.\n\nElias Bareinboim, Juan D. Correa, Duligur Ibeling, and Thomas Icard. On Pearl’s Hierarchy and the Foundations of Causal Inference, pp. 507–556. Association for Computing Machinery, New York, NY, USA, 1st edition, 2022.\n\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\nAude Billard, Sylvain Calinon, Ruediger Dillmann, and Stefan Schaal. Survey: Robot programming\n\nby demonstration. Handbook of robotics, 59, 2008.\n\nBibhas Chakraborty and Susan A Murphy. Dynamic treatment regimes. Annual review of statistics\n\nand its application, 1:447, 2014.\n\nJ. Correa and E. Bareinboim. A calculus for stochastic interventions: Causal effect identification and surrogate experiments. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, NY, 2020. AAAI Press.\n\nPim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In\n\nAdvances in Neural Information Processing Systems, pp. 11693–11704, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJalal Etesami and Philipp Geiger. Causal transfer for imitation learning and decision making under sensor-shift. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, NY, 2020. AAAI Press.\n\nYoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games\n\nand Economic Behavior, 29(1-2):79–103, 1999.\n\nJustin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-\n\nment learning. arXiv preprint arXiv:1710.11248, 2017.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672–2680, 2014.\n\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural\n\ninformation processing systems, 29, 2016.\n\nAhmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A\n\nsurvey of learning methods. ACM Computing Surveys (CSUR), 50(2):1–35, 2017.\n\nRobert Krajewski, Julian Bock, Laurent Kloeker, and Lutz Eckstein. The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pp. 2118–2125, 2018. doi: 10.1109/ITSC.2018.8569552.\n\nDaniel Kumor, Junzhe Zhang, and Elias Bareinboim. Sequential causal imitation learning with\n\nunobserved confounders. Advances in Neural Information Processing Systems, 2021.\n\nYann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\n\nSanghack Lee and Elias Bareinboim. Characterizing optimal mixed policies: Where to intervene and\n\nwhat to observe. Advances in Neural Information Processing Systems, 33, 2020.\n\nYunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual\n\ndemonstrations. Advances in Neural Information Processing Systems, 30, 2017.\n\nJeffrey Mahler and Ken Goldberg. Learning deep policies for robot bin picking by simulating robust\n\ngrasping sequences. In Conference on robot learning, pp. 515–524, 2017.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nUrs Muller, Jan Ben, Eric Cosatto, Beat Flepp, and Yann L Cun. Off-road obstacle avoidance through end-to-end learning. In Advances in neural information processing systems, pp. 739–746, 2006.\n\nKatharina M ̈ulling, Jens Kober, Oliver Kroemer, and Jan Peters. Learning to select and generalize striking movements in robot table tennis. The International Journal of Robotics Research, 32(3): 263–279, 2013.\n\nS A Murphy, M J van der Laan, J M Robins, and Conduct Problems Prevention Research Group. Marginal Mean Models for Dynamic Regimes. Journal of the American Statistical Association, 96 (456):1410–1423, December 2001.\n\nAndrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:\n\nTheory and application to reward shaping. In Icml, volume 99, pp. 278–287, 1999.\n\nAndrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,\n\nvolume 1, pp. 663–670, 2000.\n\nTakayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends in Robotics, 7(1-2): 1–179, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJudea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York,\n\n2000.\n\nDean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural\n\ninformation processing systems, pp. 305–313, 1989.\n\nJette Randløv and Preben Alstrøm. Learning to drive a bicycle using reinforcement learning and\n\nshaping. In ICML, volume 98, pp. 463–471. Citeseer, 1998.\n\nKangrui Ruan and Xuan Di. Learning human driving behaviors with sequential causal imitation learning. Proceedings of the AAAI Conference on Artificial Intelligence, 36(4):4583–4592, Jun. 2022. doi: 10.1609/aaai.v36i4.20382. URL https://ojs.aaai.org/index.php/AAAI/ article/view/20382.\n\nKangrui Ruan, Junzhe Zhang, Xuan Di, and Elias Bareinboim. Causal imitation learning via inverse reinforcement learning. Technical Report R-89, Causal AI Lab, Department of Computer Science, Columbia University, 2023.\n\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 1998.\n\nGokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, and Steven Wu. Of moments and matching: A game-theoretic framework for closing the imitation gap. In International Conference on Machine Learning, pp. 10022–10032. PMLR, 2021.\n\nUmar Syed and Robert E Schapire. A game-theoretic approach to apprenticeship learning.\n\nIn\n\nAdvances in neural information processing systems, pp. 1449–1456, 2008.\n\nJ. Tian. Studies in Causal Reasoning and Learning. PhD thesis, Computer Science Department,\n\nUniversity of California, Los Angeles, CA, November 2002.\n\nJin Tian. Identifying dynamic sequential plans. In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI’08, pp. 554–561, Arlington, Virginia, United States, 2008. AUAI Press. ISBN 0-9749039-4-9.\n\nJin Tian and Judea Pearl. A general identification condition for causal effects. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI 2002), pp. 567–573, Menlo Park, CA, 2002. AAAI Press/The MIT Press.\n\nB Widrow. Pattern-recognizing control systems. Compurter and Information Sciences, 1964.\n\nXingrui Yu, Yueming Lyu, and Ivor Tsang. Intrinsic reward driven imitation learning via generative\n\nmodel. In International Conference on Machine Learning, pp. 10925–10935. PMLR, 2020.\n\nJ. Zhang and E. Bareinboim. Can humans be out of the loop? Technical Report R-64, Causal Artificial Intelligence Lab, Columbia University, 2020. Also, to appear: Proc. of the 1st Conference on Causal Learning and Reasoning (CLeaR), 2022.\n\nJ. Zhang, D. Kumor, and E. Bareinboim. Causal imitation learning with unobserved confounders.\n\nAdvances in Neural Information Processing Systems, 33:12263–12274, 2020.\n\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse\n\nreinforcement learning. In Aaai, volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe authors considered the problem of imitation learning when the underlying causal structure of the environment is given. They provided a causal formulation of the problem of inverse RL (IRL) (equation (2)). They introduced the notion of minimal $\\pi$-backdoor admissible scope and showed that the effect of such policies on the average reward can be computed from the observational distribution over $(O,Y)$, i.e., the observable variables and the reward. Based on this, a canonical equation for IRL is given in (4) where for two settings of MWAL and GAIL, this formulation is simplified. Then, the authors used a method called IDENTIFY which is beyond the graphical condition in $\\pi$-backdoor admissible scope and it is sound and complete for checking whether a policy scope is identifiable from $P(O,Y)$. Finally, they proposed LISTIDSCOPE to enumerate all identifiable policy scopes.\n\n# Strength And Weaknesses\n\nStrength:\n\n- The authors provided a nice formulation of inverse RL in the case that we have access to the causal graph. Moreover, they also gave some examples of this formulation for MWAL and GAIL.\n- They presented a method that can enumerate all identifiable policy scopes from $P(O,Y)$.\n\nWeakness:\n\n- Regarding the gap $\\nu^*$, it is not clear why the authors only considered the cases that $\\nu^*\\leq 0$. This can only happen if the expert has suboptimal performance in the environment (or in causal language, in some SCM $M$) that is acting. \n\nIn the following, I give my detailed comments:\n- It would be great if the authors can give some real cases that the performance of an expert is suboptimal even in the environment that is acting in it. The example in the introduction is not convincing enough as it is not clear why the expert acts based on just $X$ which results in poor performance.\n- The authors considered a ``sequential decision-making\" setting however, it seems that $Y$ is not indexed with time steps. How can the results in the paper be extended to the more general setting where we have a sequence of $Y_i$'s? In the FAQ, it is mentioned that $Y$ can be a set of variables but it might be the case that the cumulative reward is identifiable while each $Y_i$ is not identifiable from $P(O,Y)$.\n- Under what conditions, is $\\nu^*$ positive?\n- What is the exact definition of effective actions and covariates in Theorem 1? What happens to other variables $\\mathbf{X}$ and $\\mathbf{Z}$ in Theorem 1?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is generally well-written but some more explanations can be given in the paper. For instance, it is good to discuss the projection algorithm mentioned in the footnote on page 7 and also mention why it is required to perform such projections. Regarding the novelty of the paper, I think the causal formulation of causal IRL and the algorithm LISTIDSCOPE are somewhat novel but it is not clear how causality can help to get better performance than the expert. The example in the introduction is not convincing. It seems that the results can be reproduced based on the explanation in the appendix.\n\n# Summary Of The Review\n\nThe submitted paper shows the advantage of considering the underlying causal diagram in the problem of inverse RL and provides a method to enumerate all identifiable policy scopes. Based on this, the canonical equation of causal IRL in (4) can be computed for such policies. The experiments showed that causal IRL can have better performance than the expert. The main ambiguity in the paper is for the case $\\nu^*>0$ and it is required to justify why the expert is not acting optimally even at least one SCM $M$.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nPROVABLY AUDITING ORDINARY LEAST SQUARES IN LOW DIMENSIONS\n\nAnkur Moitra & Dhruv Rohatgi Massachusetts Institute of Technology {moitra, drohatgi}@mit.edu\n\nABSTRACT\n\nAuditing the stability of a machine learning model to small changes in the training procedure is critical for engendering trust in practical applications. For example, a model should not be overly sensitive to removing a small fraction of its training data. However, algorithmically validating this property seems computationally challenging, even for the simplest of models: Ordinary Least Squares (OLS) linear regression. Concretely, recent work defines the stability of a regression as the minimum number of samples that need to be removed so that rerunning the analysis overturns the conclusion (Broderick et al., 2020), specifically meaning that the sign of a particular coefficient of the OLS regressor changes. But the only known approach for estimating this metric, besides the obvious exponentialtime algorithm, is a greedy heuristic that may produce severe overestimates and therefore cannot certify stability. We show that stability can be efficiently certified in the low-dimensional regime: when the number of covariates is a constant but the number of samples is large, there are polynomial-time algorithms for estimating (a fractional version of) stability, with provable approximation guarantees. Applying our algorithms to the Boston Housing dataset, we exhibit regression analyses where our estimator outperforms the greedy heuristic, and can successfully certify stability even in the regime where a constant fraction of the samples are dropped.\n\n1\n\nINTRODUCTION\n\nA key facet of interpretability of machine learning models is understanding how different subsets of the training data influence the learned model and its predictions. Computing the influences of individual training points has been shown to be a useful tool for enhancing trust in the model (Zhou et al., 2019), for tracing the origins of model bias (Brunet et al., 2019), and for identifying mislabelled training data and other model debugging (Koh & Liang, 2017). Modelling the influence of groups of training points has applications to measuring fairness (Chen et al., 2018), vulnerability to contamination of multi-source training data (Hayes & Ohrimenko, 2018), and (most relevant to this paper) identification of unstable predictions (Ilyas et al., 2022) and models (Broderick et al., 2020). In a high-stakes machine learning application, it would likely be alarming if some data points were so influential that the removal of, say, 1% of the training data dramatically changed the model. An ideal, trustworthy machine learning pipeline therefore should include validation that this does not happen.\n\nBut the obvious algorithm for checking if a model trained on n data points exhibits this instability would require computing the group influences of (cid:0) n (cid:1) different subsets of the data, which is computationally infeasible even for fairly small n. Instead, current methods for estimating the stability of a model simply use the first-order approximation of group influence: namely, the sum of individual influences of data points in the group. With this approximation, vulnerability of a model to dropping αn data points is heuristically estimated by dropping the αn most individually influential data points (Broderick et al., 2020; Ilyas et al., 2022).\n\nn/100\n\nThis heuristic can be thought of as using “local” stability as a proxy for “global” stability, and it has found substantial anecdotal success in diagnosing unstable models. Unfortunately, for correlated groups of data points, the first-order approximation of the group influence is often an underestimate (Koh et al., 2019), so large local stability does not actually certify that a model is provably stable to\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nremoving small subsets of data. In fact, stability certification is a challenging and open problem even in the simplest of models: linear regression via Ordinary Least Squares (OLS).\n\nConcretely, given a regression dataset, a natural metric for the stability of the OLS regressor is the minimum number of data points that need to be removed from the dataset to flip the sign of a particular coefficient of the regressor (e.g., in causal inference, the coefficient measuring the treatment effect). Recent work has used the local stability heuristic to diagnose unstable OLS regressions in several prominent economics studies (Broderick et al., 2020), identifying examples where even a statistically significant conclusion can be overturned by removing less than 1% of the data points. However, the converse question of validating stable conclusions remains unaddressed:\n\nGiven a regression dataset, can we efficiently certify non-trivial lower bounds on the stability of the OLS regressor?\n\nOur work takes steps towards addressing this question, via the following contributions:\n\n• We introduce a natural fractional relaxation of the above notion of OLS stability, where we allow removing fractions of data points, and seek to minimize the total removed weight. We call this finite-sample stability, and henceforth refer to the prior notion as “integral” stability.\n\n• We develop approximation algorithms for the finite-sample stability, with (a) provable guarantees under reasonable anti-concentration assumptions on the dataset, and (b) running time polynomial in the size of the dataset, so long as the dimension of the data is a constant (in contrast, the naive algorithm is exponential in the size of the dataset). Moreover, we prove that (at least for exact algorithms) exponential dependence of the running time on the dimension is unavoidable under standard complexity assumptions.\n\n• We use modifications of our algorithms to compute assumption-free upper and lower bounds on the finite-sample stability of several simple synthetic and real datasets, achieving tighter upper bounds than prior work and the first non-trivial lower bounds, i.e. certifications that the OLS regressor is stable.\n\nWhy define stability this way? The definition of integral stability was introduced in (Broderick et al., 2020), along with several variants (e.g. smallest perturbation which causes the first coordinate to lose significance). We choose the definition based on the sign of the first coordinate, because it has clear practical interpretation—does the first covariate positively or negatively affect the response?— which does not depend on choice of additional parameters such as significance level.\n\nWe study the fractional relaxation so that the stability is defined by a continuous optimization problem. Note that certifying a lower bound on fractional stability immediately certifies a lower bound on the integral stability; we will see later (Remark 3.1) that a near-converse holds in low dimensions.\n\nWhy is low-dimensional regression important? Given that much of machine learning happens in high-dimensional settings, where the number of covariates can even be larger than the number of datapoints, it is natural to wonder why low-dimensional settings are still important. First, in application areas such as econometrics, linear regressions with as few as two to four covariates are very common (Britto et al., 2022; Bianchi & Bigio, 2022; Hopenhayn et al., 2022), often serving as proofs-of-concept for more complex models. Second, even in settings where the number of covariates is larger, it is often the expectation that few covariates are relevant. In such applications, analysis often consists of a variable selection step followed by regression on a much-reduced set of covariates (Cai & Wang, 2011). In all these settings, understanding the stability of an estimator is important, and our work gives some of the first provable guarantees that avoid making strong distributional assumptions. Moreover our lower bounds show that certifying stability of truly high-dimensional models, even linear ones, is intractable.\n\n1.1 FORMAL PROBLEM STATEMENT\n\nWe are given a deterministic and arbitrary set of n samples (Xi, yi)n i=1, where each Xi is a vector of d real-valued covariates, and each yi is a real-valued response. We are interested in a single coefficient of the OLS regressor (without loss of generality, the first coordinate): in an application, the first covariate may be the treatment and the rest may be controls. The sign of this coefficient\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nis important because it estimates whether the treatment has a positive or negative effect. Thus, we want to determine if it can be changed by dropping a few samples from the regression. Formally, we consider the fractional relaxation, where we allow dropping fractions of samples: Definition 1.1. Fix (Xi, yi)n the weight-w OLS solution set of (Xi, yi)n\n\ni=1 with X1, . . . , Xn ∈ Rd and y1, . . . , yn ∈ R. For any w ∈ [0, 1]n,\n\ni=1 is\n\nOLS(X, y, w) := arg min\n\nβ∈Rd\n\n1 n\n\nn (cid:88)\n\ni=1\n\nwi(⟨Xi, β⟩ − yi)2.\n\nThe finite-sample stability of (Xi, yi)n\n\ni=1 is\n\nStability(X, y) :=\n\ninf w∈[0,1]n,β∈Rd\n\n{n − ∥w∥1 : β1 = 0 and β ∈ OLS(X, y, w)}.\n\nThis is the minimum number of samples (in a fractional sense) which need to be removed to zero out the first coordinate of the OLS regressor. If the OLS solution set contains multiple regressors, then it suffices if any regressor β in the solution set has β1 = 0. Our algorithmic goal is to compute Stability(X, y), or at least to approximate Stability(X, y) up to an additive εn error.\n\n1.2 RESULTS\n\nBy brute-force search, the (integral) stability can be computed in time 2n · poly(n). However, because the complexity is exponential in the number of samples, it is computationally infeasible even when the dimension d of the data is low, which is a common situation in many scientific applications. Similarly, the fractional stability (Definition 1.1) is the solution to a non-convex optimization problem in more than n variables, which seems no simpler. Can we still hope for a polynomial-time algorithm in constant dimensions? We show that the answer is yes. Theorem 1.2. There is an nO(d3)-time algorithm which, given n arbitrary samples (Xi, yi)n X1, . . . , Xn ∈ Rd and y1, . . . , yn ∈ R, and given k ≥ 0, decides whether Stability(X, y) ≤ k.\n\ni=1 with\n\nWe also show that the exponential dependence on dimension d is necessary under standard complexity assumptions: Theorem 1.3. Under the Exponential Time Hypothesis, there is no no(d)-time algorithm which, given (Xi, yi)n\n\ni=1 and k ≥ 0, decides whether Stability(X, y) ≤ k.\n\nThis theorem in particular rules out fixed-parameter tractability, i.e. algorithms with time complexity f (d) · poly(n). However, it only applies to exact algorithms. In practice, it is unlikely to matter whether Stability(X, y) = 0.01n or Stability(X, y) = 0.02n; in both cases, the conclusion is sensitive to dropping a very small fraction of the data. This motivates our next two algorithmic results on εn-additive approximation of the stability (where we think of ε > 0 as a constant). First, we make a mild anti-concentration assumption, under which the stability can εn-approximated in time roughly nd+O(1). While still not fixed-parameter tractable, this algorithm can now be run on moderate sized problems in low dimensions, unlike the algorithm in Theorem 1.2. Assumption A. Let ε, δ > 0. We say that samples (Xi, yi)n every β ∈ Rd, it holds that\n\ni=1 satisfy (ε, δ)-anti-concentration if for\n\n(cid:26)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ni ∈ [n] : |⟨Xi, β⟩ − yi| <\n\nδ √\nn\n\n(cid:13) (cid:13)Xβ(0) − y (cid:13)\n\n(cid:13) (cid:13) (cid:13)2\n\n(cid:27)(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ εn,\n\nwhere X : n × d is the matrix with rows X1, . . . , Xn, and β(0) ∈ OLS(X, y, 1) is any unweighted OLS regressor of y against X.\n\nSee Appendix F.1 for discussion of when Assumption A holds. Under this assumption, we present an O(εn)-approximation algorithm:\n\nTheorem 1.4. For any ε, δ, η > 0, there is an algorithm PARTITIONANDAPPROX with time complexity\n\n(cid:18)\n\nn +\n\nCd\n\nε2 log\n\n1 δ\n\nlog\n\n1 εη\n\n(cid:19)d+O(1)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nwhich, given ε, δ, η, and samples (Xi, yi)n ˆS such that with probability at least 1 − η,\n\ni=1 satisfying (ε, δ)-anti-concentration, returns an estimate\n\n| ˆS − Stability(X, y)| ≤ 12εn + 1.\n\nIn fact, PARTITIONANDAPPROX also can detect when Assumption A fails (see Theorem D.6 for a precise statement), so it can be used to compute unconditional lower bounds on stability with high probability (where the lower bound is provably tight if the data satisfies anti-concentration). Moreover, as discussed in Appendix F.1, the required anti-concentration is very mild. If ε, η > 0 are constants, the algorithm has time complexity nd+O(1), so long as the samples satisfy (ε, exp(−Ω(n)))-anticoncentration. This is true for arbitrary smoothed data. Finally, unlike the exact algorithm, PARTITIONANDAPPROX avoids heavy algorithmic machinery; it only requires solving linear programs.\n\nFixed-parameter tractability? Our final result is that εn-approximation of the stability is in fact fixed-parameter tractable, under a stronger anti-concentration assumption. Assumption B. Let ε, δ > 0. We say that samples (Xi, yi)n if for every β ∈ Rd+1, it holds that\n\ni=1 satisfy (ε, δ)-strong anti-concentration\n\n(cid:26)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ni ∈ [n] : |⟨X i, β⟩| <\n\nδ √\nn\n\n(cid:13)Xβ(cid:13) (cid:13) (cid:13)2\n\n(cid:27)(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ εn\n\nwhere X : n × (d + 1) is the matrix with columns (X T )1, . . . , (X T )d, y.\n\nThis assumption holds with constant ε, δ > 0 under certain distributional assumptions on (Xi, yi)n e.g. centered Gaussian mixtures with uniformly bounded condition number (Appendix F.2).\n\ni=1,\n\nd/(εδ2))d · poly(n)-time algorithm NETAPPROX which, Theorem 1.5. For any ε, δ > 0, there is a ( i=1 satisfying (ε, δ)-strong anti-concentration, returns an estimate ˆS given ε,δ, and samples (Xi, yi)n satisfying Stability(X, y) ≤ ˆS ≤ Stability(X, y) + 3εn + 1. Moreover, Stability(X, y) ≤ ˆS holds for arbitrary (Xi, yi)n\n\ni=1.\n\n√\n\nExtensions. Another model, frequently used in causal inference and econometrics, is instrumental variables (IV) linear regression. When the noise η in a hypothesized causal relationship y = ⟨X, β∗⟩ + η is believed to be endogenous (i.e. correlated with X), a common approach (Sargan, 1958; Angrist et al., 1996; Card, 2001) is to find a p-dimensional variable Z (the instrument) for which domain knowledge suggests that E[η|Z] = 0. Positing that β∗ is identified by the moment condition E[Z(y − ⟨X, β⟩)] = 0, the IV estimator set given samples (Xi, yi, Zi)n\n\ni=1 is then\n\nIV(X, y, Z) = {β ∈ Rd : Z T (w ⋆ (Xβ − y) = 0}\n\nwhere a ⋆ b denotes elementwise product, and Z : n × p and X : n × d are the matrices of instruments and covariates respectively. Stability can be defined as in Definition 1.1. Although for simplicity we state all of our results for OLS (i.e. the special case Z = X), it can be seen that Theorem 1.2 and Theorem 1.5 both extend directly to the IV regression setting. See Appendix G for further discussion.\n\nExperiments. We implement modifications of NETAPPROX and PARTITIONANDAPPROX which give unconditional, exact upper and lower bounds on stability, respectively. We use these algorithms to obtain tight data-dependent bounds on stability of isotropic Gaussian datasets for a broad range of signal-to-noise ratios, and we demonstrate heterogeneous synthetic datasets where our algorithms’ upper bounds are an order of magnitude better than upper bounds obtained by the prior heuristic. On the Boston Housing dataset (Harrison Jr & Rubinfeld, 1978), we regress house values against all pairs of features. For the majority of these regressions, we bound the stability within a factor of two. On the one hand, we detect many sensitive conclusions (including some which the greedy heuristic claims are stable); on the other hand, we certify that some conclusions are stable to dropping as much as half the dataset.\n\n1.3 ORGANIZATION\n\nIn Section 2 we review related work. In Section 3 we collect notation and formulas that will be useful later. In Section 4 we sketch the intuition behind our algorithmic results. Section 5 covers our experiments. In Appendices B, C, D, and E we prove Theorems 1.2, 1.3, 1.4, and 1.5 respectively.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nThere is a rich literature on topics related to finite-sample stability, including sensitivity analysis and robustness to distribution shift and contamination. Due to space constraints, here we only discuss the works most relevant to ours, and we postpone broader discussion to Appendix A.\n\nMost directly related is the prior work on heuristics for the (integral) stability (Broderick et al., 2020; Kuschnig et al., 2021). The heuristic given by Broderick et al. (2020) (to approximate the most-influential k samples) is simply the local approximation: compute the local influence of each sample at w = 1, sort the samples from largest to smallest influence, and output the top k samples. Subsequent work (Kuschnig et al., 2021) refines this heuristic by recomputing the influences after removing each sample, which alleviates issues such as masking (Chatterjee & Hadi, 1986). But this is still just a greedy heuristic, and it may fail when samples are jointly but not individually influential. Except under the strong assumption that the sample covariance remains nearly constant when we remove any εn samples (see Theorem 1 in Broderick et al. (2020), which relies on Condition 1 in Giordano et al. (2019)), the local influence approach can upper bound the finite-sample stability but cannot provably lower bound it. In fact, in Section 5 we provide examples where the greedy heuristic of Kuschnig et al. (2021) is very inaccurate due to instability in the sample covariance.\n\nClosely related to finite-sample stability, the s-value (Gupta & Rothenhäusler, 2021) is the minimum Kullback-Leibler divergence D(P ||P0) over all distributions P for which the conclusion is null, where P0 is the empirical distribution of the samples. Unfortunately, while the s-value is an interesting and well-motivated metric, computing the s-value for OLS estimation appears to be computationally intractable, and the algorithms given by Gupta & Rothenhäusler (2021) lack provable guarantees.\n\n3 PRELIMINARIES\n\nFor vectors u, v ∈ Rm, we let u ⋆ v denote the elementwise product (u ⋆ v)i = uivi. Throughout the paper, we will frequently use the closed-form expression for the (weighted) OLS solution set\n\nOLS(X, y, w) = {β ∈ Rd : X T (w ⋆ (Xβ − y)) = 0}\n\nwhere X : n × d is the matrix with rows X1, . . . , Xn. In particular, setting λ = β2:d, this means that the finite-sample stability can be rewritten as\n\nStability(X, y) =\n\ninf w∈[0,1]n,λ∈Rd−1\n\n{n − ∥w∥1 : X T (w ⋆ ( ̃Xλ − y)) = 0}\n\n(1)\n\nwhere (here and throughout the paper) ̃X : n×(d−1) is the matrix with columns (X T )2, . . . , (X T )d. Remark 3.1. As previously mentioned, the finite-sample stability always lower bounds the integral stability (the minimum number of samples that need to be removed to make the first coordinate of the regressor change sign), by continuity of the OLS solution set in w. Additionally, it can be seen from Equation 1 that a partial converse holds in low dimensions. For any feasible (w, λ), the set of w′ such that (a) (w′, λ) is feasible, and (b) ∥w∥1 = ∥w′∥1, has the form [0, 1]n ∩ V for some subspace V ⊆ Rn of codimension at most d + 1. Thus, there is some w′ ∈ [0, 1]n ∩ V with at most d + 1 non-integral weights. If Stability(X, y) = αn, then w′ witnesses that the first coordinate of the OLS regressor can be zeroed out by downweighting at most αn + d + 1 samples.\n\n4 OVERVIEW OF ALGORITHMS\n\nAn exact algorithm. Our main tool for Theorem 1.2 is the following special case of an important result due to Renegar (1992) on solving quantified polynomial systems of inequalities:\n\nTheorem 4.1 (Renegar (1992)). Given an expression\n\n∀x ∈ Rn1 : ∃y ∈ Rn2 : P (x, y),\n\nwhere P (x, y) is a system of m polynomial inequalities with maximum degree d, the truth value of the expression can be decided in time (md)O(n1n2).1\n\n1This is in the real number model; a similar statement can be made in the bit complexity model.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nRoughly, for a constant number of quantifier alternations, a quantified polynomial system can be decided in time exponential in the number of variables. Unfortunately, a naive formulation of the expression Stability(X, y) ≤ k, by direct evaluation of Equation 1, has n + d − 1 variables:\n\n∃λ ∈ Rd−1, w ∈ [0, 1]n :\n\nn (cid:88)\n\ni=1\n\nwi ≥ n − k and X T (w ⋆ ( ̃Xλ − y)) = 0.\n\nIntuitively, it may not be necessary to search over all w ∈ [0, 1]n; for fixed λ, the maximum-weight w is described by a simple linear program. Formally, the linear program can be rewritten (Lemma B.1) by the separating hyperplane theorem, so that the overall expression becomes:\n\n∃λ ∈ Rd−1 : ∀u ∈ Rd : ∃w ∈ [0, 1]n : ∥w∥1 ≥ n − k and\n\nn (cid:88)\n\ni=1\n\nwi(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ ≥ 0.\n\n(2)\n\nNow, for fixed λ and u, the maximum-weight w has very simple description: it only depends on the relative ordering of the n summands (⟨ ̃X, λ⟩ − yi)⟨Xi, u⟩. By classical results on connected components of varieties, since the summands have only 2d − 1 variables, the number of achievable orderings is only nΩ(d) rather than n!, and the orderings can be enumerated efficiently (Milnor, 1964; Renegar, 1992). This allows the quantifier over w ∈ [0, 1]n to be replaced by a quantifier over the nΩ(d) achievable orderings, after which Theorem 4.1 implies that the overall expression can be decided in time nΩ(d3). See Appendix B for details.\n\nApproximation via partitioning. Next, we show how to avoid the heavy algorithmic machinery used in the previous result. For Theorem 1.4, the strategy of PARTITIONANDAPPROX is to partition the OLS solution space Rd−1 into roughly nd regions, such that if we restrict λ to any one region, the bilinear program which defines the stability can be approximated by a linear program.\n\nConcretely, we start by writing the formulation (1) as\n\nn − Stability(x, y) =\n\nsup w∈[0,1]n,λ∈Rd−1\n\n \n\n(cid:88)\n\n\n\ni∈[n]\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nwi\n\nX T (w ⋆ ( ̃Xλ − y)) = 0\n\n \n\n\n\n.\n\n(3)\n\nThis has a nonlinear (and nonconvex) constraint due to the pointwise product between w and the residual vector ̃Xλ − y. Thus, we can introduce the change of variables gi = wi(⟨ ̃Xi, λ⟩ − yi) for i ∈ [n]. This causes two issues. First, the constraint 0 ≤ wi ≤ 1 becomes 0 ≤ gi/(⟨Xi, λ⟩ − yi) ≤ 1, which is no longer linear. To fix this, suppose that instead of maximizing over all λ ∈ Rd−1, we maximize over a region R ⊆ Rd−1 where each residual ⟨ ̃X, λ⟩ − yi has constant sign σi. The constraint 0 ≤ wi ≤ 1 then becomes one of two linear constraints, depending on σi. Let VR denote the value of Program 3 restricted to λ ∈ R. Then with the change of variables, we have that \n\n\n \n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nX T g = 0 ∀i ∈ [n] : σi = 1 0 ≤ gi ≤ ⟨Xi, λ⟩ − yi ⟨Xi, λ⟩ − yi ≤ gi ≤ 0 ∀i ∈ [n] : σi = −1\n\n,\n\n\n\nVR = sup\n\ng∈Rn,λ∈R\n\n(cid:88)\n\n\n\ni∈[n]\n\ngi ⟨Xi, λ⟩ − yi\n\nwith the convention that 0/0 = 1. Now the constraints are linear. Unfortunately, (and this is the second issue), the objective is no longer linear. The solution is to partition the region R further: if the region were small enough that every residual ⟨Xi, λ⟩ − yi had at most (1 ± ε)-multiplicative variation, then the objective could be approximated to within 1 ± ε by a linear objective. How many regions do we need? Let M = (cid:13) (cid:13)2 be the unweighted OLS error. If all the n and M in magnitude, for all λ ∈ Rd−1, then the regions residuals were bounded between δM/ could be demarcated by O(n log1+ε(n/δ)) hyperplanes, for a total of O(n log1+ε(n/δ))d regions. Of course, for some λ, some residuals may be very small or very large. But (ε, δ)-anti-concentration implies that for every λ, at most εn residuals are very small, and it can be shown that if λ is a weighted OLS solution, the total weight on samples with large residuals is low. Thus, for any region, we can exclude from the objective function the samples with residuals that are not well-approximated within the region, and this only affects the objective by O(εn).\n\n(cid:13)Xβ(0) − y(cid:13)\n\n√\n\nThis gives an algorithm with time complexity (nε−1 log(1/δ))d+O(1). To achieve the time complexity in Theorem 1.4, where the log(n/δ) is additive rather than multiplicative, we use subsampling. Every\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nresidual is still partitioned by sign, but we multiplicatively partition only a random ̃O(d/ε)-size subset of the residuals. Intuitively, most residuals will still be well-approximated in any given region. This can roughly be formalized via a VC dimension argument, albeit with some technical complications. See Section D for details and Appendix J for formal pseudocode of the algorithm.\n\nNet-based approximation. The algorithm NETAPPROX for Theorem 1.5 is intuitively the simplest. For any fixed λ ∈ Rd−1, Program 1 reduces to a linear program with value denoted S(λ). Thus, an obvious approach is to construct a net N ⊆ Rd−1 in some appropriate metric, and compute minλ∈N S(λ). This always upper bounds the stability, but to prove that it’s an approximate lower bound, we need S(λ) to be Lipschitz under the metric.\n\nThe right metric turns out to be\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n\n−\n\nd(λ, λ′) =\n\n ̃Xλ − y (cid:13) ̃Xλ − y (cid:13) (cid:13)2 Under this metric, Rd−1 essentially embeds into a d-dimensional subspace of the Euclidean sphere S n−1, and therefore has a γ-net of size O(1/γ)d. Why is S(λ) Lipschitz under d? First, if ̃Xλ − y equals ̃Xλ′ − y up to rescaling, then it can be seen from Program 3 that S(λ) = S(λ′). More generally, if the residuals are close up to rescaling, we apply the dual formulation of S(λ) from expression (2):\n\n ̃Xλ′ − y (cid:13) ̃Xλ′ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)\n\n.\n\nn − S(λ) = inf u∈Rd\n\nsup w∈[0,1]n\n\n∥w∥1 :\n\nn (cid:88)\n\ni=1\n\nwi(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ ≥ 0.\n\nFor any u, the optimal w for λ and u can be rounded to some feasible w′ for λ′ and u without decreasing the l1 norm too much, under strong anti-concentration. This shows that S(λ) and S(λ′) are close. See Appendix E for details and Appendix J for formal pseudocode of NETAPPROX.\n\n5 EXPERIMENTS\n\nIn this section, we apply (modifications of) NETAPPROX and PARTITIONANDAPPROX to several datasets in two and three dimensions. Due to space constraints, we defer detailed discussion of the algorithmic modifications to Appendix I.1; we simply note that the modifications are made to improve practical efficiency and usability. Most saliently, the modified algorithms do not rely on Assumptions A and B: the modified NETAPPROX provides an unconditional upper bound on stability (referred to henceforth as “net upper bound”), and the modified PARTITIONANDAPPROX provides an unconditional lower bound (“LP lower bound”). As a result, we are able to experimentally verify that our algorithms provide tight (and unconditional) bounds on stability for a variety of datasets.\n\nAs a baseline upper bound, we implement the greedy heuristic of Kuschnig et al. (2021) which refines Broderick et al. (2020). We are not aware of any prior work on lower bounding stability, so we implement a simplification of our full lower bound algorithm as a baseline. See Appendix I for implementation details, hyperparameter choices, and discussion of error bars.\n\n5.1 SYNTHETIC DATA\n\nHeterogeneous data. We start with a simple two-dimensional dataset with two disparate subpopulations, where the greedy baseline fails to estimate the stability but our algorithms give tight estimates. For parameters n, k, and σ, we generate k independent samples (Xi, yi), where Xi ∈ R2 has independent coordinates Xi1 ∼ N (−1, 0.01) and Xi2 ∼ N (0, 1), and yi = Xi1. Then, we generate n − k independent samples (Xi, yi) where Xi1 = 0 and Xi2 ∼ N (0, 1), and y ∼ N (0, 1). It always suffices to remove the first subpopulation, so the stability is at most k. However, the first subpopulation has small individual influences, because the OLS regressor on the whole dataset can nearly interpolate the first subpopulation. Thus, we expect that the greedy algorithm will fail to notice the first subpopulation, and therefore remove far more than k samples.\n\nIndeed, this is what happens. For n = 1000 and k varying from 10 to 500, we compare our net upper bound and LP lower bound with the baselines. As seen in Figure 1, our methods are always better\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) d = 2 and n = 1000; median of 10 trials for each noise level and algorithm\n\n(b) d = 3 and n = 500; one trial for each noise level and algorithm\n\nFigure 1: Heterogeneous data\n\nFigure 2: Isotropic Gaussian data\n\nthan the baselines, and certifiably approximate the stability within a small constant factor. In the regime where k is small, our upper bound outperforms the greedy upper bound by a factor of 30.\n\nIn the previous example, removing k samples caused a pathological change in Covariance shift. the sample covariance; it became singular. However, even modest, constant-factor instability in the sample covariance can cause the greedy algorithm to fail; see Appendix I.5 for details.\n\nIsotropic Gaussian data. Instability can arise even in homogeneous data, as a result of a low signalto-noise ratio (Broderick et al., 2020). But when the noise level is low, can we certify stability? For a broad range of noise levels, we experimentally show that this is the case. Specifically, for d ∈ {2, 3} and noise parameter σ ranging from 0.1 to 10, we generate n independent samples (Xi, yi)n i=1 where Xi ∼ N (0, Id) and yi = ⟨Xi, 1⟩ + N (0, σ2). For d = 2 and n = 1000 (Figure 2a), our LP lower bound is nearly tight with the upper bounds, particularly as the noise level increases (in comparison, the baseline lower bound quickly degenerates towards zero). For d = 3 and n = 500 (Figure 2b), the bounds are looser for small noise levels but still always within a small constant factor.\n\n5.2 BOSTON HOUSING DATASET\n\nThe Boston Housing dataset (Harrison Jr & Rubinfeld, 1978; Gilley et al., 1996) consists of data from 506 census tracts of Greater Boston in the 1970 Census. There are 14 real-valued features, one of which—the median house value in USD 1000s—we designate as the response. Unfortunately the entire set of features is too large for our algorithms, so for our experiments we pick various subsets of two or three features to use as covariates.\n\nA Tale of Two Datasets. We exemplify our results with two particular feature subsets. First, we investigate the effect of zn (percentage of residential land zoned for large lots) on house values, controlling for rm (average number of rooms per home) and rad (highway accessibility index) but no bias term. On the entire dataset, we find a modest positive effect: the estimated coefficient of zn is roughly 0.06. Both the greedy heuristic and our net algorithm find subsets of just 8% of the data (38-40 samples) which, if removed, would nullify the effect. But is this tight, or could there be a much smaller subset with the same effect? Our LP lower bound certifies that removing at least 22.4 samples is necessary.\n\nSecond, we investigate the effect of zn on house values, this time controlling only for crim (per capita crime rate). Our net algorithm finds a subset of just 27% of the data which was driving the effect, and the LP lower bound certifies that the stability is at least 8%. But this time, the greedy algorithm removes 90% of the samples, a clear failure. What happened? Plotting zn against crim reveals a striking heterogeneity in the data: 73% of the samples have zn = 0, and the remaining 27% of the samples (precisely those removed by the net algorithm) have crim < 0.83, i.e. very low crime rates. As in the synthetic example, this heterogeneity explains the greedy algorithm’s failure. But heterogeneity is very common in real data: in this case, it’s between the city proper and the suburbs, and in fact the OLS regressors of these two subpopulations on all 13 features are markedly different\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) Net upper bound (y) vs greedy upper bound (x)\n\n(b) LP lower bound (y) vs net upper bound (x)\n\n(c) crim (y) vs zn (x)\n\nFigure 3: Results from Boston Housing dataset. Figure (a) plots the net upper bounds on the y-axis against the greedy upper bounds on the x-axis; Figure (b) plots the LP lower bounds on the y-axis against the net upper bounds on the x-axis. In both (a) and (b), each mark corresponds to one of the 156 feature pairs. Figure (c) plots the feature zn against the feature crim (on log scale); each mark is one of the 506 datapoints.\n\n(Appendix I.6). Thus, it’s important to have algorithms with provable guarantees for detecting when heterogeneity causes (or doesn’t cause) unstable conclusions.\n\nAll-feature-pairs analysis. To be thorough, we also apply our algorithms to all 156 ordered pairs of features. For each pair, we regress the response (i.e. median house value) against the two features by Ordinary Least Squares, and we use our algorithms on this 2-dimensional dataset to estimate how many samples need to be removed to nullify the effect of the first feature on the response. We also compare to the greedy upper bound. See Figure 3 for a perspective on the results. In each figure, each point corresponds to the results of one dataset. The left figure plots the net upper bound against the greedy upper bound: we can see that our net algorithm substantially outperforms the greedy heuristic on some datasets (i.e. finds a much smaller upper bound) and never performs much worse. The right figure plots the LP lower bound against the net upper bound (along with the line y = x). For a majority of the datasets, the upper bound and lower bound are close. Concretely, for 116 of the 156 datasets, we certifiably estimate the stability up to a factor of two – some are sensitive to removing less than 10 samples, and some are stable to removing even a majority of the samples.\n\n6 CONCLUSIONS\n\nIn this work, we studied efficient estimation of the stability of OLS regressions to removing subsets of the training data. We showed that in low dimensions the problem is both theoretically and experimentally tractable, whereas in high dimensions exact computation of the stability likely requires exponential time. However, this is only the beginning of the story. Most immediately, since our lower bound algorithm takes time nΩ(d), our experiments were limited to no more than three dimensions. Certifying stability of OLS regressions from e.g. recent econometric studies may require additional heuristics or insights (e.g. developing a fixed-parameter tractable lower bound algorithm). Beyond that, identifying reasonable assumptions under which exponential dependence on dimension can be entirely circumvented is another valuable direction for future work.\n\nOf course, machine learning extends far beyond linear regression, and for more and more complex and opaque models, stability certification is all the more crucial as a tool for enhancing trustworthiness. Certainly, OLS is important in its own right, but inasmuch as it is a key building block in more complex machine learning systems (from regression trees (Loh, 2011) to generative adversarial networks (Mao et al., 2017) and policy iteration in linear MDPs (Lagoudakis & Parr, 2003)), our work on estimating stability of OLS is also a first step towards estimating stability for these systems.\n\nFinally, we remark that care must be taken when interpreting stability in practice. Large stability may increase trust in a model’s parameters or predictions, but it does not mean that conclusions drawn from the model are “correct.\" Conversely, even if the stability is small, the conclusions may still be useful, with the caveat that they may be driven by a small sub-population. Understanding whether this heterogeneity is problematic or not is context-dependent, and is a separate but important issue.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nJoshua D Angrist, Guido W Imbens, and Donald B Rubin. Identification of causal effects using instrumental variables. Journal of the American statistical Association, 91(434):444–455, 1996.\n\nAinesh Bakshi and Adarsh Prasad. Robust linear regression: Optimal rates in polynomial time. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pp. 102–115, 2021.\n\nDavid A Belsley, Edwin Kuh, and Roy E Welsch. Regression diagnostics: Identifying influential data\n\nand sources of collinearity. John Wiley & Sons, 1980.\n\nJavier Bianchi and Saki Bigio. Banks, liquidity management, and monetary policy. Econometrica,\n\n90(1):391–454, 2022.\n\nDiogo GC Britto, Paolo Pinotti, and Breno Sampaio. The effect of job loss and unemployment\n\ninsurance on crime in brazil. Econometrica, 90(4):1393–1423, 2022.\n\nTamara Broderick, Ryan Giordano, and Rachael Meager. An automatic finite-sample robustness metric: Can dropping a little data change conclusions. arXiv preprint arXiv:2011.14999, pp. 16, 2020.\n\nMarc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. Understanding the origins of bias in word embeddings. In International conference on machine learning, pp. 803–811. PMLR, 2019.\n\nT Tony Cai and Lie Wang. Orthogonal matching pursuit for sparse signal recovery with noise. IEEE\n\nTransactions on Information theory, 57(7):4680–4688, 2011.\n\nDavid Card. Estimating the return to schooling: Progress on some persistent econometric problems.\n\nEconometrica, 69(5):1127–1160, 2001.\n\nEnrique Castillo, Ali S Hadi, Antonio Conejo, and Alfonso Fernández-Canteli. A general method for local sensitivity analysis with application to regression models and other optimization problems. Technometrics, 46(4):430–444, 2004.\n\nMaxime Cauchois, Suyash Gupta, Alnur Ali, and John C Duchi. Robust validation: Confident\n\npredictions even when distributions shift. arXiv preprint arXiv:2008.04267, 2020.\n\nMichal ˇCern`y, Jaromír Antoch, and Milan Hladík. On the possibilistic approach to linear regression models involving uncertain, indeterminate or interval data. Information Sciences, 244:26–47, 2013.\n\nSamprit Chatterjee and Ali S Hadi. Influential observations, high leverage points, and outliers in\n\nlinear regression. Statistical science, pp. 379–393, 1986.\n\nIrene Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? Advances\n\nin neural information processing systems, 31, 2018.\n\nR Dennis Cook. Detection of influential observation in linear regression. Technometrics, 19(1):15–18,\n\n1977.\n\nIlias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. In International Conference on\n\nSever: A robust meta-algorithm for stochastic optimization. Machine Learning, pp. 1596–1606. PMLR, 2019.\n\nJohn Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally\n\nrobust optimization. arXiv preprint arXiv:1810.08750, 2018.\n\nPanos Giannopoulos, Christian Knauer, and Günter Rote. The parameterized complexity of some geometric problems in unbounded dimension. In International Workshop on Parameterized and Exact Computation, pp. 198–209. Springer, 2009.\n\nOtis W Gilley, R Kelley Pace, et al. On the harrison and rubinfeld data. Journal of Environmental\n\nEconomics and Management, 31(3):403–405, 1996.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nRyan Giordano, William Stephenson, Runjing Liu, Michael Jordan, and Tamara Broderick. A swiss army infinitesimal jackknife. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1139–1147. PMLR, 2019.\n\nSuyash Gupta and Dominik Rothenhäusler. The r-value: evaluating stability with respect to distribu-\n\ntional shifts. arXiv preprint arXiv:2105.03067, 2021.\n\nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL https://www.\n\ngurobi.com.\n\nAli S Hadi and Jeffrey S Simonoff. Procedures for the identification of multiple outliers in linear\n\nmodels. Journal of the American statistical association, 88(424):1264–1272, 1993.\n\nDavid Harrison Jr and Daniel L Rubinfeld. Hedonic housing prices and the demand for clean air.\n\nJournal of environmental economics and management, 5(1):81–102, 1978.\n\nJamie Hayes and Olga Ohrimenko. Contamination attacks and mitigation in multi-party machine\n\nlearning. Advances in neural information processing systems, 31, 2018.\n\nHugo Hopenhayn, Julian Neira, and Rish Singhania. From population growth to firm demographics: Implications for concentration, entrepreneurship and the labor share. Econometrica, 90(4):1879– 1914, 2022.\n\nPeter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004.\n\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Understanding predictions with data and data with predictions. In International Conference on Machine Learning, pp. 9525–9587. PMLR, 2022.\n\nSookyo Jeong and Hongseok Namkoong. Robust causal inference under covariate shift via worst-case subpopulation treatment effects. In Conference on Learning Theory, pp. 2079–2084. PMLR, 2020.\n\nMichael J Kearns and Umesh Vazirani. An introduction to computational learning theory. MIT press,\n\n1994.\n\nAdam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regression.\n\nIn Conference On Learning Theory, pp. 1420–1430. PMLR, 2018.\n\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In\n\nInternational conference on machine learning, pp. 1885–1894. PMLR, 2017.\n\nPang Wei W Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang. On the accuracy of influence functions for measuring group effects. Advances in neural information processing systems, 32, 2019.\n\nNikolas Kuschnig, Gregor Zens, and Jes Cuaresma. Hidden in plain sight: Influential sets in linear\n\nmodels. 2021.\n\nMichail G Lagoudakis and Ronald Parr. Least-squares policy iteration. The Journal of Machine\n\nLearning Research, 4:1107–1149, 2003.\n\nEdward E Leamer. Global sensitivity results for generalized least squares estimates. Journal of the\n\nAmerican Statistical Association, 79(388):867–870, 1984.\n\nWei-Yin Loh. Classification and regression trees. Wiley interdisciplinary reviews: data mining and\n\nknowledge discovery, 1(1):14–23, 2011.\n\nXudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2794–2802, 2017.\n\nJohn Milnor. On the betti numbers of real varieties. Proceedings of the American Mathematical\n\nSociety, 15(2):275–280, 1964.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nWolfgang Polasek. Regression diagnostics for general linear regression models. Journal of the\n\nAmerican Statistical Association, 79(386):336–340, 1984.\n\nJames Renegar. On the computational complexity and geometry of the first-order theory of the reals. part i: Introduction. preliminaries. the geometry of semi-algebraic sets. the decision problem for the existential theory of the reals. Journal of symbolic computation, 13(3):255–299, 1992.\n\nJohn D Sargan. The estimation of economic relationships using instrumental variables. Econometrica:\n\nJournal of the Econometric Society, pp. 393–415, 1958.\n\nAman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional\n\nrobustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.\n\nHideo Tanaka, Isao Hayashi, and Junzo Watada. Possibilistic linear regression analysis for fuzzy data.\n\nEuropean Journal of Operational Research, 40(3):389–396, 1989.\n\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science,\n\nvolume 47. Cambridge university press, 2018.\n\nJianlong Zhou, Zhidong Li, Huaiwen Hu, Kun Yu, Fang Chen, Zelin Li, and Yang Wang. Effects of influence on user trust in predictive decision making. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1–6, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA FURTHER RELATED WORK\n\nLocal and global sensitivity metrics. Post-hoc evaluation of the sensitivity of a statistical inference to various types of model misspecification has long been recognized as an important research direction. Within this area, there is a distinction between local sensitivity metrics, which measure the sensitivity of the inference to infinitesimal misspecifications of the assumed model M0 (e.g. Polasek (1984); Castillo et al. (2004); Belsley et al. (1980)), and global sensitivity metrics, which measure the set of possible inferences as the model ranges in some fixed set M around M0 (e.g. Leamer (1984); Tanaka et al. (1989); ˇCern`y et al. (2013)). For OLS in particular, there is a well-established literature on the influences of individual data points (Cook, 1977; Chatterjee & Hadi, 1986), which falls under local sensitivity analysis, since deleting a single data point is an infinitesimal perturbation to a dataset of size n as n → ∞. In contrast, identifying jointly influential subsets of the data (the “global” analogue) has been a long-standing challenge due to computational issues (see e.g. page 274 of Belsley et al. (1980)). Existing approaches typically focus on identifying outliers in a generic sense rather than with respect to a specific inference (Hadi & Simonoff, 1993), or study computationally tractable variations of deletion (e.g. constant-factor reweighting (Leamer, 1984)).\n\nRobustified estimators. Ever since the work of Tukey and Huber, one of the central areas of statistics has been robustifying statistical estimators to be resilient to outliers (see, e.g. Huber (2004)). While a valuable branch of research, we view robust statistics as incomparable if not orthogonal to post-hoc sensitivity evaluation, for three reasons. First, samples that drive the conclusion (in the sense that deleting them would nullify the conclusion) are not synonymous with outliers: removing an outlier that works against the conclusion only makes the conclusion stronger. Indeed, outlier-trimmed datasets are not necessarily finite-sample robust (Broderick et al., 2020). Rather, finite-sample stability (along with the s-value (Gupta & Rothenhäusler, 2021)), in the regime where a constant fraction of samples is removed, may be thought of as a measure of resilience to heterogeneity and distribution shift.\n\nSecond, it is unreasonable to argue that using robustified estimators obviates the need for sensitivity evaluation. Robust statistics has seen a recent algorithmic revival, with the development of computationally efficient estimators, for problems such as linear regression, that are robust in the strong contamination model (e.g. Klivans et al. (2018); Diakonikolas et al. (2019); Bakshi & Prasad (2021)). However, even positing that the strong contamination model is correct, estimation guarantees for these algorithms require strong, unverifiable (and unavoidable (Klivans et al., 2018)) assumptions about the uncorrupted data, such as hypercontractivity. Sensitivity analyses should support modeling assumptions, not depend upon them.\n\nThird and perhaps most salient, classical estimators such as OLS are ubiquitous in practice, despite the existence of robust estimators. This alone justifies sensitivity analysis of the resulting scientific conclusions.\n\nDistributionally robust optimization. A recent line of work in machine learning (Sinha et al., 2017; Duchi & Namkoong, 2018; Cauchois et al., 2020; Jeong & Namkoong, 2020) suggests that the lack of resilience of Empirical Risk Minimization to distribution shift can be mitigated by minimizing the supremum of risks with respect to distributions near the empirical training distribution (under e.g. Wasserstein distance or an f -divergence). Again, this approach of robustifying the estimator is valuable but incomparable to sensitivity analysis.\n\nB PROOF OF THEOREM 1.2\n\nIn this section, we show how to exactly compute the stability of a d-dimensional dataset in time nO(d3), proving Theorem 1.2. Our main tool is Theorem 4.1, a special case of an important result due to Renegar (1992) on solving quantified polynomial systems of inequalities.\n\nThe expression Stability(X, y) ≤ k can indeed be written as a polynomial system of (degree-2) equations, with only an ∃ quantifier. Unfortunately, the number of variables in this naive formulation is n + d − 1 (n for the weights and d − 1 for the regressor), which yields an algorithm exponential in n. Thus, to take advantage of the above theorem, we need to reformulate the expression with fewer\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nvariables. The following lemma rewrites the stability, via the separation theorem for convex sets, in a form where the variable reduction will become apparent. Lemma B.1. For any (Xi, yi)n\n\ni=1 and k ≥ 0, it holds that Stability(X, y) ≤ k if and only if\n\n∃λ ∈ Rd−1 : ∀u ∈ Rd : ∃w ∈ [0, 1]n : ∥w∥1 ≥ n − k ∧\n\nn (cid:88)\n\ni=1\n\nwi(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ ≥ 0,\n\n(4)\n\nwhere ̃X : n × (d − 1) is the matrix with columns (X T )2, . . . , (X T )d.\n\nProof. From formulation (1) of the stability, we know that Stability(X, y) ≤ k if and only if\n\n∃λ ∈ Rd−1 : ∃w ∈ [0, 1]n : ∥w∥1 ≥ n − k ∧ X T (w ⋆ ( ̃Xλ − y)) = 0.\n\nFix λ ∈ Rd−1. Define the set\n\nD(n − k) =\n\n(cid:110)\n\n(w ⋆ ( ̃Xλ − y)) : w ∈ [0, 1]n ∧ ∥w∥1 ≥ n − k\n\n(cid:111)\n\n.\n\nWe are interested in the predicate D(n−k)∩ker(X T ) ̸= ∅, or equivalently 0 ∈ D(n−k)+ker(X T ). Observe that D(n − k) is convex, since w ranges over a convex set. Thus, by the separation theorem for a point and a convex set, 0 ∈ D(n − k) + ker(X T ) if and only if for every v ∈ Rn, we have supx∈D(n−k)+ker(X T )⟨v, x⟩ ≥ 0. If v is not orthogonal to ker(X T ), then the inner product can be made arbitrarily large. Thus, it suffices to restrict to v ∈ span(X T ), in which case the supremum is simply over x ∈ D(n − k). That is, 0 ∈ D(n − k) ∩ ker(X T ) if and only if\n\n∀u ∈ Rd : ∃w ∈ [0, 1]n : ∥w∥1 ≥ n − k ∧\n\nQuantifying over λ, we get the claimed expression.\n\n(cid:68)\n\nXu, (w ⋆ ( ̃Xλ − y))\n\n≥ 0.\n\n(cid:69)\n\nThe expression in Lemma B.1 still has O(n) variables. However, we can now actually eliminate the variable w at the cost of increasing the number of equations. This is because the optimal w for fixed λ and u only depends on the relative order of the terms (⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩. We make the following definition: Definition B.2. For any λ ∈ Rd−1 and u ∈ Rd, let π(λ, u) be the unique permutation on [n] such that for all 1 ≤ i ≤ n − 1,\n\n(⟨ ̃Xπi, λ⟩ − yπi)⟨Xπi, u⟩ ≥ (⟨ ̃Xπi+1 , λ⟩ − yπi+1)⟨Xπi+1 , u⟩,\n\nand such that equality implies πi < πi+1. Let Π = {π(λ, u) : λ ∈ Rd−1, u ∈ Rd}.\n\nThen it can be seen that for fixed λ and u, the optimal choice of w has coefficients 1 on π(λ, u)1, . . . , π(λ, u)⌊n−k⌋, and coefficient n − k − ⌊n − k⌋ for π(λ, u)⌊n−k⌋+1: if there is any feasible w which makes the sum non-negative, then this choice of w makes the sum non-negative as well. Denoting this vector by w(π(λ, u)), we have that in Equation 4 it suffices to restrict to w ∈ {w(π) : π ∈ Π}.\n\nA priori, the number of achievable permutations could be n!, in which case we would not have gained anything. However, because π(λ, u) is defined by low-degree polynomials in only 2d − 1 variables, we can actually show that |Π| is at most exponential in d, using the following result: Theorem B.3 (Sign Partitions (Milnor, 1964; Renegar, 1992)). Let g1, . . . , gm : Rn → R be arbitrary polynomials each with total degree at most d. Let SG(g) be the set of vectors σ ∈ {−1, 0, 1}m such that σ is an achievable sign vector, i.e. there exists some x ∈ Rn with sign(gi) = σi for all i ∈ [m]. Then |SG(g)| ≤ (md)O(n). Moreover, SG(g) can be enumerated in time (md)O(n).\n\nPutting everything together, we have the following theorem, which proves Theorem 1.2. Theorem B.4. For any permutation π on [n], define w(π) ∈ [0, 1]n by\n\nw(π)πi =\n\n \n\n\n\n1 n − k − ⌊n − k⌋ 0\n\nif i ≤ n − k if i = n − k + 1 otherwise\n\n.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nThen for any k ∈ [0, n], it holds that Stability(X, y) > k if and only if\n\n∀λ ∈ Rd−1 : ∃u ∈ Rd : ∀π ∈ Π :\n\nn (cid:88)\n\ni=1\n\nw(π)i(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ < 0.\n\n(5)\n\nMoreover, Π can be enumerated in time nO(d). Thus, the expression Stability(X, y) > k can be decided in time nO(d3).\n\nProof. Fix λ ∈ Rd−1 and u ∈ Rd. If\n\n∃π ∈ Π :\n\nn (cid:88)\n\ni=1\n\nw(π)i(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ ≥ 0,\n\nthen because ∥w(π)∥1 ≥ n − k, we obviously get\n\n∃w ∈ [0, 1]n : ∥w∥1 ≥ n − k ∧\n\nn (cid:88)\n\ni=1\n\nwi(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ ≥ 0.\n\n(6)\n\n(7)\n\nif\n\nis\n\n(6)\n\nfalse,\n\nthen in particular w(π(λ, u)) produces a negative sum Conversely, (cid:80)n i=1 w(π(λ, u))i(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩. But by construction, w(π(λ, u)) maximizes this sum, over all w ∈ [0, 1]n with ∥w∥1 = n − k. Therefore no weight vector with l1 norm exactly n − k produces a nonnegative sum, and increasing the norm cannot help. Thus, (7) and (6) are equivalent. Quantifying over λ and u, we have Stability(X, y) ≤ k if and only if\n\n∃λ ∈ Rd−1 : ∀u ∈ Rd : ∃π ∈ Π :\n\nn (cid:88)\n\ni=1\n\nw(π)i(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ ≥ 0.\n\nTaking the negation yields expression (5). If we can compute Π, then this expression is a ∀∃-system of polynomial inequalities with 2d − 1 variables and |Π| degree-2 inequalities, so by Theorem 4.1 it can be decided in time |Π|O(d2). It remains to show that Π can be enumerated in time nO(d) (which bounds |Π|).\n\nFor any i, j ∈ [n] with i < j define the polynomial\n\nfi,j(λ, u) = (⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ − (⟨ ̃Xj, λ⟩ − yj)⟨Xj, u⟩.\n\nFor any λ and u, the permutation π(λ, u) is determined by the signs of the polynomials {fi,j}i<j at (λ, u). But by Theorem B.3, the set of sign vectors can be computed in time nO(d). So Π can be found in time nO(d) as well.\n\nC PROOF OF THEOREM 1.3\n\nIn this section, we prove Theorem 1.3. That is, we show that exactly computing the stability requires nΩ(d) time under the Exponential Time Hypothesis, by a simple reduction from the Maximum Feasible Subsystem problem. This latter problem is already known to take nΩ(d) time in d dimensions under the Exponential Time Hypothesis:\n\nTheorem C.1 (Theorem 13 in Giannopoulos et al. (2009)). Suppose that there is an no(d)-time algorithm for the following problem: given n vectors v1, . . . , vn ∈ Rd, real numbers c1, . . . , cn ∈ R, and an integer 0 ≤ k ≤ n, determine whether\n\nmax λ∈Rd\n\nn (cid:88)\n\ni=1\n\n1[⟨vi, λ⟩ = ci] ≥ k.\n\nThen the Exponential Time Hypothesis is false.\n\nFrom this, it can be easily seen that (exactly) computing stability also requires nΩ(d) time.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTheorem C.2. Suppose that there is an no(d)-time algorithm for the following problem: given X1, . . . , Xn ∈ Rd and y1, . . . , yn ∈ R, as well as an integer 0 ≤ k ≤ n, determine whether\n\nThen the Exponential Time Hypothesis is false.\n\nStability(X, y) ≤ n − k.\n\nProof. We reduce to Maximum Feasible Subsystem. Given v1, . . . , vn ∈ Rd and c1, . . . , cn ∈ R, define Xi = (ci, vi) ∈ Rd+1 and yi = ci. Then the regressor e1 = (1, 0, . . . , 0) ∈ Rd+1 perfectly fits the data set, i.e.\n\nn (cid:88)\n\n(⟨Xi, e1⟩ − yi)2 = 0.\n\nThus, for any w ∈ [0, 1]n,\n\ni=1\n\n(cid:40)\n\nOLS(X, y, w) =\n\nβ ∈ Rd+1 :\n\nwi(⟨Xi, β⟩ − yi)2 = 0\n\n.\n\n(cid:41)\n\nn (cid:88)\n\ni=1\n\nSuppose that Stability(X, y) ≤ n − k. Then there is some w ∈ [0, 1]n and β1 = 0 with ∥w∥1 ≥ k and (cid:80)n i=1 wi(⟨Xi, β⟩ − yi)2 = 0. Let S ⊆ [n] be the support of w; then |S| ≥ k. Moreover for every i ∈ S, it holds that ⟨Xi, β⟩ − yi = 0. Since β1 = 0, by definition of Xi and yi, this implies that ⟨vi, β2:d+1⟩ = ci. Thus,\n\nn (cid:88)\n\n1[⟨vi, β2:d+1⟩ = ci] ≥ k.\n\nConversely, suppose that there exists some λ ∈ Rd and set S ⊆ [n] of size k such that ⟨vi, λ⟩ = ci for all i ∈ S. Define w = 1S ∈ [0, 1]n, and define β = (0, λ). Then it is clear that\n\ni=1\n\nn (cid:88)\n\ni=1\n\nwi(⟨Xi, β⟩ − yi)2 =\n\n(⟨vi, λ⟩ − ci)2 = 0.\n\n(cid:88)\n\ni∈S\n\nThus, β ∈ OLS(X, y, w), so Stability(X, y) ≤ n − k. This completes the reduction.\n\nRemark C.3. Maximum Feasible Subsystem does have an εn-additive approximation algorithm in time ̃O((d/ε)d), by subsampling. Thus, proving nΩ(d)-hardness for εn-approximation of stability would require a different technique.\n\nD PROOF OF THEOREM 1.4\n\nIn this section, we prove Theorem 1.4. The main idea of PARTITIONANDAPPROX is that under Assumption A, we can approximate the stability by partitioning Rd−1 into roughly nd regions and solving a linear program on each region. See Appendix J for the complete algorithm.\n\nD.1 PARTITIONING SCHEME\n\n(cid:13)2, where β(0) ∈ OLS(X, y, 1). Let S ⊆ [n] be a Given samples (Xi, yi)n uniformly random size-m subset of [n]. Let R1, . . . , Rp be the closed connected subsets of Rd−1 cut out by the following set of equations E:\n\ni=1, let M = (cid:13)\n\n(cid:13)Xβ(0) − y(cid:13)\n\n\n\n \n\n⟨ ̃Xi, λ⟩ − yi = 0 ⟨ ̃Xi, λ⟩ − yi = σM ⟨ ̃Xi, λ⟩ − yi = σδM/ ⟨ ̃Xi, λ⟩ − yi = σ(1 + ε)kδM/\n\n√\n\nn\n\n√\n\n∀i ∈ [n]\n\n∀i ∈ [n], ∀σ ∈ {−1, 1}\n\n∀i ∈ [n], ∀σ ∈ {−1, 1}\n\n√\n\nn ∀i ∈ S, ∀σ ∈ {−1, 1}, ∀0 ≤ k ≤ ⌈log1+ε(\n\nn/δ)⌉\n\nFormally, we define a region for every feasible assignment of equations to {=, <, >}, and then replace each strict inequality by a non-strict inequality, so that the region is closed. First, we observe that the number of regions is not too large, and in fact we can enumerate the regions efficiently.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nLemma D.1. Each region Ri is the intersection of O(|E|) linear equalities or inequalities. Moreover, the regions R1, . . . , Rp can be enumerated in time O(|E|)d+O(1).\n\nProof. Order the set of equations E arbitrarily. We recursively construct the set of regions demarcated by the first t equations. For each such region, we solve a linear program to check if the t + 1th hyperplane intersects the interior of the region. If so, we split the region according to the sign of the t + 1th hyperplane. The overall time complexity of this procedure is O(p · poly(n, |E|)), where p is the final number of regions. But the number of regions which can be cut out by t hyperplanes in Rd is at most O(t)d by standard arguments.\n\nNext, we argue that even though we only multiplicatively partitioned a small subset of the residuals, with high probability most residuals are well-approximated. More precisely, we show that for the (random) region R containing a fixed point λ∗, with high probability, for every other λ in the region, most residuals at λ are multiplicatively close to the corresponding residuals at λ∗. This can be proven by the generalization bound for function classes with low VC dimension: Theorem D.2 (Theorem 3.4 in Kearns & Vazirani (1994)). Let X be a set. Let C ⊂ {0, 1}X be a binary concept class with VC dimension d. Let D be a distribution on X × {0, 1}. Let ε, δ > 0 and pick m ∈ N satisfying\n\nm ≥ C1\n\n(cid:18) d ε\n\nlog\n\n1 ε\n\n+\n\n1 ε\n\nlog\n\n1 δ\n\n(cid:19)\n\nfor a sufficiently large constant C1. Let (xi, yi)i∈[m] be m independent samples from D. Then with probability at least 1 − δ, the following holds. For all h ∈ C with h(xi) = yi for all i ∈ [m],\n\nPr (x,y)∼D\n\n[h(x) ̸= y] ≤ ε.\n\nSpecifically, for any λ ∈ Rd−1, we define a function fλ : [n] → {0, 1} as the indicator function of samples for which the residual at λ is close to the residual at λ∗. Then the region containing λ∗ is precisely the set of functions which perfectly fit S × {1}, and the generalization bound implies that all such functions fit most of [n] × 1, which is what we wanted to show. The following lemma formalizes this argument, with some additional steps to deal with very small and very large residuals. Lemma D.3. Let λ∗ ∈ Rd−1 and let η > 0. Let R be the region containing λ∗. Let BM ⊆ [n] be the set of i ∈ [n] such that |⟨ ̃Xi, λ∗⟩ − yi| > M , and let BδM ⊆ [n] be the set of i ∈ [n] such that |⟨ ̃Xi, λ∗⟩ − yi| ≤ δM/\n\nn. If\n\n√\n\n(cid:18)\n\nm ≥\n\nC ε\n\nd log\n\n1 ε\n\n+ log\n\n1 η\n\n(cid:19)\n\nfor an absolute constant C, then with probability at least 1 − η over the choice of S, the following holds. For every λ ∈ R, the number of i ∈ [n] \\ (BM ∪ BδM ) such that\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n|⟨ ̃Xi, λ⟩ − yi| |⟨ ̃Xi, λ⟩ − yi|\n\n(cid:12) (cid:12) (cid:12) − 1 (cid:12) (cid:12)\n\n> ε\n\nis at most εn.\n\nProof. Let σ ∈ {−1, 0, 1}n be defined by σi = sign(⟨ ̃Xi, λ∗⟩ − yi). Note that by construction of the regions, sign(⟨ ̃Xi, λ⟩ − yi) = σi for every λ ∈ R. Let S′ = S ∩ ([n] \\ (BM ∪ BδM )). If n′ := |[n] \\ (BM ∪ BδM )| ≤ εn, then the lemma statement is trivially true. Otherwise, by a Chernoff bound,\n\n(cid:20)\n\n|S′| ≥\n\nPr\n\n(cid:21)\n\nm\n\nn′ 2n\n\n≥ 1 − exp(−Ω(n′m/n)) ≥ 1 − η/3.\n\nCondition on the event |S′| ≥ n′m/(2n). Then S′ is uniform over size-m′ subsets of [n] \\ (BM ∪ BδM ). Define X = [n], and define a concept class C = {hλ : X → {0, 1}|λ ∈ Rd−1} as the set of binary functions\n\nhλ(i) = 1\n\n(cid:104)\n\nσi\n\n(cid:16)(cid:68) ̃Xi, λ − (1 + ε)λ∗(cid:69)\n\n(cid:17)\n\n(cid:105)\n\n+ εyi\n\n≤ 0\n\n.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n√\n\nLet D be the distribution on X × {0, 1} where (i, s) ∼ D has i ∼ Unif([n] \\ (BM ∪ BδM )) and s = 1. Then S′ × {1} consists of at least n′m/(2n) independent samples from D. For any λ ∈ R, we claim that the function hλ fits S′ × {1} perfectly. Indeed, for any i ∈ S′, we know that n ≤ σi(⟨ ̃Xi, λ∗⟩ − yi) ≤ M , since i ̸∈ BM ∪ BδM . So by construction of the regions, it δM/ holds that\n\nσi(⟨ ̃Xiλ⟩ − yi) σi(⟨ ̃Xiλ∗⟩ − yi) From the right-hand side of the equation, we precisely get that hλ(i) = 1, as claimed. By Lemma D.4, we have vc(C) ≤ d. Thus, since\n\n1 1 + ε\n\n≤ 1 + ε.\n\n≤\n\n|S′| ≥\n\nn′m 2n\n\n≥\n\nCd (2n/n′)ε\n\nlog\n\n1 ε\n\n+\n\nC (2n/n′)ε\n\nlog\n\n1 η\n\n,\n\nif C is a sufficiently large absolute constant, we can apply Theorem D.2 with failure parameter ε′ := nε/(2n′) to get that with probability at least 1 − η/3,\n\nsup λ∈R\n\nPr (i,s)∼D\n\n[hλ(i) ̸= s] ≤ ε′.\n\nEquivalently, for every λ ∈ R, the number of i ∈ [n] \\ (BM ∪ BδM ) such that σi(⟨ ̃Xi, λ⟩ − yi) > (1 + ε)σi(⟨ ̃Xi, λ⟩ − yi) is at most ε′n′ = εn/2. An identical argument with the binary functions\n\ngλ(i) = 1\n\n(cid:104)\n\nσi\n\n(cid:16)(cid:68) ̃Xi, λ − (1 − ε)λ∗(cid:69)\n\n(cid:17)\n\n(cid:105)\n\n≥ 0\n\n− εyi\n\nproves that with probability at least 1−η/3, for every λ ∈ R, the number of i ∈ [n]\\(BM ∪BδM ) such that σi(⟨ ̃Xi, λ⟩ − yi) < (1 − ε)σi(⟨ ̃Xi, λ⟩ − yi) is at most εn/2. Overall, by the union bound (taking into account the event that |S′| < n′m/(2n)), with probability at least 1 − η it holds that for every λ ∈ R, the number of i ∈ [n]\\(BM ∪BδM ) such that either σi(⟨ ̃Xi, λ⟩−yi) > (1+ε)σi(⟨ ̃Xi, λ⟩−yi) or σi(⟨ ̃Xi, λ⟩ − yi) < (1 − ε)σi(⟨ ̃Xi, λ⟩ − yi) is at most εn as claimed.\n\nIt remains to bound the VC dimension of the function class. Lemma D.4. For any n, d > 0 and any (Xi, yi)i∈[n] ⊂ Rd × R, the VC dimension of the concept class C = {hλ : [n] → {0, 1}|λ ∈ Rd}, where\n\nis at most d + 1.\n\nhλ(i) = 1[⟨Xi, λ⟩ + yi ≤ 0],\n\nProof. Note that extending the domain of the concepts cannot decrease the VC dimension. Thus, if we define C′ = {h′\n\nλ(X, y) = 1[⟨X, λ⟩ + y ≤ 0], then vc(C′) ≥ vc(C). But all of the concepts in C′ are affine halfspaces in d + 1 dimensions, so vc(C′) ≤ d + 1.\n\nλ : Rd × R → {0, 1} : λ ∈ Rd} by h′\n\nD.2 ALGORITHM\n\nFor every region R we can identify some arbitrary representative λ0(R) ∈ R and a sign pattern σ ∈ {−1, 1}n such that σi = 1 implies ⟨ ̃Xi, λ⟩ − yi ≥ 0 for all λ ∈ R, and σi = −1 implies ⟨ ̃Xi, λ⟩ − yi ≤ 0 for all λ ∈ R. Ideally, we want to compute V = maxR VR where\n\nVR := sup\n\ng∈Rn,λ∈R\n\n \n\n\n\n(cid:88)\n\ni∈[n]\n\ngi ⟨ ̃Xi, λ⟩ − yi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nX T g = 0 0 ≤ gi ≤ ⟨ ̃Xi, λ⟩ − yi ∀i ∈ [n] : σi = 1 ⟨ ̃Xi, λ⟩ − yi ≤ gi ≤ 0 ∀i ∈ [n] : σi = −1\n\n \n\n\n\n(8)\n\nHowever, this is not a linear program. Let BM = BM (R) ⊆ [n] be the set of i ∈ [n] such that |⟨ ̃Xi, λ0(R)⟩ − yi| > M , and let BδM = BδM (R) ⊆ [n] be the set of i ∈ [n] such that\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n|⟨ ̃Xi, λ0(R) − yi| < δM/\n\n√\n\nn. For each region R we compute\n\n(ˆg(R), ˆλ(R))\n\n:= arg sup g∈Rn,λ∈R\n\n\n\n \n\nand define\n\n(cid:88)\n\ni∈[n]\\(BM ∪BδM )\n\n(cid:18)\n\nmin\n\ngi ⟨ ̃Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\nˆV (R) =\n\n(cid:88)\n\ni∈[n]\n\nˆg(R)i ⟨ ̃Xi, ˆλ(R)⟩ − yi\n\n(9)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nX T g = 0 0 ≤ gi ≤ ⟨ ̃Xi, λ⟩ − yi ∀i ∈ [n] : σi = 1 ⟨ ̃Xi, λ⟩ − yi ≤ gi ≤ 0 ∀i ∈ [n] : σi = −1\n\n\n\n \n\n(10)\n\nwith the convention that 0/0 = 1. Note that although there is a min in the objective of Program 10, it is equivalent to a linear program by a standard transformation: we can introduce variables s1, . . . , sn with the constraints si ≤ 1 and si ≤ gi/(⟨Xi, λ0(R)⟩ − yi), and change the objective to maximize (cid:80)\n\ni∈[n] si. That is, the following program also computes (ˆg(R), ˆλ(R)):\n\n(ˆg(R), ˆλ(R))\n\n=\n\narg sup g∈Rn,λ∈R,s∈Rn\n\n\n\n \n\n(cid:88)\n\nsi\n\ni∈[n]\\(BM ∪BδM )\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(11)\n\nX T g = 0 ∀i ∈ [n] : σi = 1 0 ≤ gi ≤ ⟨Xi, λ⟩ − yi ∀i ∈ [n] : σi = −1 ⟨Xi, λ⟩ − yi ≤ gi ≤ 0 si ≤ 1 ∀i ∈ [n] \\ (BM ∪ BδM ) si ≤ gi/(⟨Xi, λ0(R)⟩ − yi) ∀i ∈ [n] \\ (BM ∪ BδM ) (12)\n\n\n\n \n\nLemma D.5. Let (w∗, λ∗) be an optimal solution to Program 1. Suppose that the event of Lemma D.3 holds (with respect to λ∗). Then either maxR BδM (R) > εn, or\n\nV − 12εn − 1 ≤ max\n\nR\n\nˆV (R) ≤ V.\n\nif we define w ∈ Rn by wi = Proof. For the RHS, observe that for any region R, ˆg(R)i/(⟨Xi, ˆλ(R)⟩ − yi), with the convention that 0/0 = 1, then the second and third constraints of Program 10 ensure that w ∈ [0, 1]n. Additionally, the first constraint ensures that\n\nX T (cid:88)\n\ni∈[n]\n\nwi(⟨ ̃Xi, ˆλ(R)⟩ − yi) = 0.\n\nThus, (w, ˆλ(R)) is feasible for the original problem. This means that ˆV (R) = ∥w∥1 ≤ V . To prove the lower bound, suppose that maxR BδM (R) ≤ εn. Consider the specific region R containing the optimal parameter vector λ∗ (if there are multiple choose any), and let BM = BM (R) and BδM = BδM (R). Define g∗ ∈ Rn by g∗ i (⟨Xi, λ∗⟩ − yi). Let Bapx ⊂ [n] \\ (BM ∪ BδM ) be the set of i ∈ [n] \\ (BM ∪ BδM ) such that\n\ni = w∗\n\n(cid:20) 1\n\n1 + ε\n\n(⟨ ̃Xi, λ0(R)⟩ − yi) > ⟨ ̃Xi, λ∗⟩ − yi\n\n(cid:21)\n\n(cid:104)\n\n(cid:105) ⟨ ̃Xi, λ∗⟩ − yi > (1 + ε)(⟨ ̃Xi, λ0(R)⟩ − yi)\n\n.\n\n∨\n\nBy Lemma D.3 (applied specifically to λ = λ0(R)), we know |Bapx| ≤ εn. By assumption, we know that |BδM | ≤ εn. And we know that if i ∈ BM then |⟨ ̃Xi, λ0(R)⟩ − yi| ≥ M , so the same holds for\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nall λ ∈ R and in particular for λ∗. Thus\n\n(cid:88)\n\ni∈BM\n\nw∗\n\ni ≤\n\n≤\n\n≤\n\n≤\n\n1 M 2\n\n1 M 2\n\n1 M 2\n\n1 M 2\n\n= 1\n\n(cid:88)\n\nw∗\n\ni (⟨ ̃Xi, λ∗⟩ − yi)2\n\ni∈BM (cid:88)\n\nw∗\n\ni (⟨ ̃Xi, λ∗⟩ − yi)2\n\ni∈[n]\n\n(cid:88)\n\nw∗\n\ni (⟨Xi, β(0)⟩ − yi)2\n\ni∈[n] (cid:13) (cid:13)Xβ(0) − y (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\nwhere the third inequality is because λ∗ ∈ OLS(X, y, w∗), and the equality is by definition of M . Finally, for any i ∈ [n] \\ (BM ∪ BδM ∪ Bapx), we have\n\n(cid:18)\n\ng∗ i\n⟨Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n≥ min\n\n(cid:18) 1\n\n1 + ε\n\ng∗ i\n⟨Xi, λ∗⟩ − yi\n\n(cid:19)\n\n, 1\n\n=\n\n1 1 + ε\n\ng∗ i\n⟨Xi, λ∗⟩ − yi\n\n.\n\nmin\n\nTherefore V =\n\n(cid:88)\n\nw∗\n\ni\n\ni∈[n]\n\n=\n\n(cid:88)\n\ni∈[n]\\(BM ∪BδM ∪Bapx)\n\n≤ (1 + ε)\n\n(cid:88)\n\ni∈[n]\\(BM ∪BδM ∪Bapx)\n\ng∗ i\n⟨Xi, λ∗⟩ − yi\n\n(cid:88)\n\n+\n\ni∈BM\n\ng∗ ⟨Xi, λ∗⟩ − yi\n\n+\n\n(cid:88)\n\nw∗\n\ni +\n\n(cid:88)\n\nw∗\n\ni\n\ni∈BδM\n\ni∈Bapx\n\n(cid:18)\n\nmin\n\ng∗ i\n⟨Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n+ 1 + 2εn\n\n≤ (1 + ε)\n\n(cid:88)\n\ni∈[n]\\(BM ∪BδM )\n\n(cid:18)\n\nmin\n\ng∗ i\n⟨Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n+ 1 + 2εn.\n\nThe sum in the last line above is precisely the objective of Program 10 at (g∗, λ∗). Moreover, (g∗, λ∗) is feasible for Program 10 because X T g∗ = X T (w∗ ⋆(⟨ ̃Xi, λ∗⟩−yi)) = 0 and g∗ i /(⟨ ̃Xi, λ∗⟩−yi) = wi ∈ [0, 1] for all i ∈ [n]. Thus, the optimal solution (ˆg(R), ˆλ(R)) satisfies the inequality\n\n(cid:88)\n\nmin\n\ni∈[n]\\(BM ∪BδM )\n\n(cid:18)\n\ng∗ i\n⟨ ̃Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n≤\n\n(cid:88)\n\nmin\n\ni∈[n]\\(BM ∪BδM )\n\n(cid:18)\n\nˆg(R) ⟨ ̃Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n.\n\n1\n\nFinally, let ˆBapx ⊆ [n] be the set of i ∈ [n] \\ (BM ∪ BδM ) such that (cid:20) (1 + ε)2 (⟨ ̃Xi, λ0(R)⟩ − yi) > ⟨ ̃Xi, ˆλ(R)⟩ − yi By Lemma D.3, the residuals at λ0(R) multiplicatively approximate the residuals at λ∗ except for εn samples, and the residuals at ˆλ(R) also multiplicatively approximate the residuals at λ∗ except for εn samples. Thus, | ˆBapx| ≤ 2εn, so we have\n\n(cid:105) (cid:104) ⟨ ̃Xi, ˆλ(R)⟩ − yi > (1 + ε)2(⟨Xi, λ0(R)⟩ − yi)\n\n∨\n\n(cid:21)\n\n.\n\n(cid:88)\n\nmin\n\ni∈[n]\\BM\n\n(cid:18)\n\nˆg(R) ⟨Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n=\n\n(cid:88)\n\nmin\n\ni∈[n]\\(BM ∪BδM ∪ ˆBapx)\n\n(cid:18)\n\nˆg(R) ⟨Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n(cid:18)\n\nmin\n\nˆg(R) ⟨Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n(cid:18)\n\nmin\n\nˆg(R) ⟨Xi, λ0(R)⟩ − yi\n\n(cid:19)\n\n, 1\n\n+ 2εn\n\nˆg(R) ⟨Xi, ˆλ(R)⟩ − yi\n\n+ 2εn\n\n(cid:88)\n\n+\n\ni∈BδM ∪ ˆBapx\n\n(cid:88)\n\n≤\n\ni∈[n]\\(BM ∪BδM ∪ ˆBapx)\n\n≤ (1 + ε)2\n\n(cid:88)\n\ni∈[n]\\(BM ∪BδM ∪ ˆBapx)\n\n≤ (1 + ε)2 ˆV (R) + 2εn.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nWe conclude that V ≤ (1 + ε)3 ˆV (R) + (1 + ε)2εn + 1 + 2εn. Since ˆV (R) ≤ n, simplifying gives V ≤ ˆV (R) + 12εn + 1 as claimed.\n\nUsing the above lemma in conjunction with Lemma D.3 immediately gives the desired theorem (from which Theorem 1.4 is a direct corollary).\n\nTheorem D.6. For any ε, δ, η > 0, there is an algorithm PARTITIONANDAPPROX with time complexity\n\n(cid:18)\n\nn +\n\nCd\n\nε2 log\n\nn δ\n\nlog\n\n1 εη\n\n(cid:19)d+O(1)\n\ni=1, either outputs ⊥ or an estimate ˆS. If the which, given ε, δ, η, and arbitrary samples (Xi, yi)n output is ⊥, then the samples do not satisfy (ε, δ)-anti-concentration (Assumption A). Moreover, the probability that the output is some ˆS such that\n\nis at most η.\n\n| ˆS − Stability(X, y)| > 12εn + 1\n\n(cid:13)Xβ(0) − y(cid:13)\n\nProof. The algorithm PARTITIONANDAPPROX does the following (see Appendix J for pseudocode). Let m = Cε−1(d log ε−1 + log η−1) where C is the constant specified in Lemma D.3, and let M = (cid:13) (cid:13)2 where β(0) ∈ OLS(X, y, 1). Let E be the set of equations described in Section D.1, with respect to a uniformly random subset S ⊆ [n] of size m. Let R1, . . . , Rp be the closed connected regions cut out by E. By Lemma D.1, we can enumerate R1, . . . , Rp in time O(|E|)d+O(1); each is described by at most |E| linear constraints. For each R, we can find a representative λ0(R) ∈ R by solving a feasibility LP on R, and by solving n LPs on R, we can find a sign pattern σ ∈ {−1, 1}n such that σi = 1 implies ⟨ ̃Xi, λ⟩ − yi ≥ 0 for all λ ∈ R, and σi = −1 implies ⟨ ̃Xi, λ⟩ − yi ≤ 0 for all λ ∈ R. We also compute BM = {i ∈ [n] : |⟨ ̃Xi, λ0(R)⟩ − yi| > M } √\nand BδM {i ∈ [n] : |⟨ ̃Xi, λ0(R)⟩ − yi| < δM/ n}. If |BδM | > εn, then return ⊥. Otherwise, compute (ˆg(R), ˆλ(R)), the solution to Program 12, and compute ˆV (R) = (cid:80)n .\n\ni=1\n\nˆg(R)i ⟨ ̃Xi,ˆλ(R)⟩−yi\n\nFinally, after iterating through all regions, output maxi∈[p]\n\nˆV (Ri).\n\nCorrectness follows from Lemmas D.3 and D.5. For each region, the time complexity is poly(|E|). As there are O(|E|)d regions, the overall time complexity is O(|E|)d+O(1). Observing that |E| = O(n + mε−1 log(n/δ)) completes the proof.\n\nE PROOF OF THEOREM 1.5\n\nIn the previous section, we approximated the nonlinear program (1) by partitioning Rd−1 into regions where the program could be approximated by a linear program. This approach had the disadvantage of requiring that in each region, the signs of the residuals ⟨ ̃Xi, λ⟩ − yi were constant (so that the program could be reparametrized to have linear constraints), which necessitates making Ω(nd) regions. In this section, we instead make use of the fact that for fixed λ, Program 1 is a linear program and therefore efficiently solvable. Our algorithm NETAPPROX is simply to (carefully) choose a finite subset N ⊂ Rd−1, solve the linear program for each λ ∈ N , and pick the best answer. The following lemma describes how to compute N , which will be an ε-net over Rd−1 in an appropriate metric. Lemma E.1. For any (Xi, yi)n d/γ)d such that for any λ ∈ Rd−1 with ̃Xλ ̸= y, there is some λ′ ∈ N with ̃Xλ′ ̸= y, and some σ ∈ {−1, 1}, such that\n\ni=1 and γ > 0, there is a set N ⊆ Rd−1 of size |N | ≤ (2\n\n√\n\nMoreover, N can be computed in time O(\n\nd/γ)d.\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n ̃Xλ − y (cid:13) ̃Xλ − y (cid:13) (cid:13)2 √\n\n(cid:13) (cid:13) (cid:13)\n\n−\n\n ̃Xλ′ − y (cid:13) ̃Xλ′ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n\n≤ γ.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nProof. Let A : n × d be the matrix with columns (X T )2, . . . , (X T )d, −y. Let d′ = rank(A) and let A = U DV T be the singular value decomposition of A, where D = diag(s1, . . . , sd′) is the diagonal matrix of nonzero singular values of A. Let M be a “marginally-random” γ-net over the unit sphere S d′−1 under the l2 metric, where by “marginally-random” we mean that M is chosen from some distribution, and every point of the net has marginal distribution uniform over S d′−1 (e.g. take any fixed γ-net and apply a uniformly random rotation). Also define B = V D−1. Suppose Bd is not identically zero. Then define\n\nN =\n\n(cid:26) (Bm)1:d−1 (Bm)d\n\n(cid:27)\n\n: m ∈ M\n\n.\n\nSince each m ∈ M is a generic unit vector (by the marginally-random property) and Bd is nonzero, we have that all (Bm)d are nonzero with probability 1, so the above set is well-defined. Moreover, for any m ∈ M,\n\n ̃X\n\n(Bm)1:d−1 (Bm)d\n\n− y = A\n\nBm (Bm)d\n\n=\n\nU m (Bm)d\n\n.\n\nNow let λ ∈ Rd−1 with ̃Xλ ̸= y, and define v = DV T [λ; 1]/\n\n(cid:13) (cid:13) (cid:13)\n\n ̃Xλ − y\n\n(cid:13) (cid:13) (cid:13)2\n\n∈ Rd′\n\n. Observe that\n\n∥v∥2 = ∥U v∥2 =\n\n∥A[λ; 1]∥2 (cid:13) (cid:13) ̃Xλ − y (cid:13) (cid:13) (cid:13)2 (cid:13)\n\n= 1.\n\nSo there is some m ∈ M with ∥v − m∥2 ≤ γ. Let λ′ = (Bm)1:d−1/(Bm)d ∈ N and let σ = sign((Bm)d). Then\n\n ̃Xλ − y (cid:13) ̃Xλ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n= U v\n\nand\n\nThus,\n\n ̃Xλ′ − y (cid:13) ̃Xλ′ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n=\n\nU m/(Bm)d ∥U m/(Bm)d∥2\n\n= sign((Bm)d) ·\n\nU m ∥U m∥2\n\n= σU m.\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n ̃Xλ − y (cid:13) ̃Xλ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n− σ\n\n ̃Xλ′ − y (cid:13) ̃Xλ′ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n\n= ∥U v − U m∥2 = ∥v − m∥2 ≤ γ\n\nas desired. On the other hand, if Bd is identically zero, then Vd = 0, so y = (AT )d = 0. This boundary case can be avoided by picking any nonzero covariate (X T )i among (X T )2, . . . , (X T )d and replacing y by y + c(X T )i for a generic c ∈ R; this does not change Stability(X, y).\n\nNote that a γ-net M of S d′−1 with |M| ≤ (2 by discretizing the cube [−1, 1]d′ √\n|N | ≤ |M| ≤ (2\n\nd/γ)d.\n\n√\n\nd/γ)d can be constructed in time O((\n\n√\n\n√\n\nd/γ)d) d/γ)d) as well, and\n\n. Thus, N can be constructed in time O((\n\nAll that is left is to show that under strong anti-concentration, the value of the linear program is Lipschitz in λ (under the metric described in the previous lemma). This proves Theorem 1.5.\n\nd/(εδ2))d · poly(n) which, given arbitrary samples (Xi, yi)n\n\n√\n\nTheorem E.2. Let ε, δ > 0. (2 ing Stability(X, y) ≤ ˆS. Moreover, if (Xi, yi)n fact\n\nThere is an algorithm NETAPPROX with time complexity i=1, produces an estimate ˆS satisfyi=1 satisfy (ε, δ)-strong anti-concentration, then in\n\nStability(X, y) ≤ ˆS ≤ Stability(X, y) + 3εn + 1.\n\nProof. For any λ ∈ Rd−1, define\n\nV (λ) = sup\n\nw∈[0,1]n\n\n(cid:110)\n\n∥w∥1\n\n(cid:12) (cid:12)\n\n(cid:12)X T (w ⋆ ( ̃Xλ − y)) = 0\n\n(cid:111)\n\n,\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nso that n−Stability(X, y) = supλ∈Rd−1 V (λ). For any fixed λ, we can compute V (λ) in polynomial time, since it is defined as an LP with n variables and d constraints. Fix γ = εδ2. The algorithm NETAPPROX does the following (see Appendix J for pseudocode): if X T ( ̃Xλ−y) = 0 has a solution, then output 0. Otherwise, let N be the net guaranteed by Lemma E.1. Then compute V (λ) for every λ ∈ N , and output the estimate\n\nˆS = n − max √\n\nλ∈N\n\nV (λ).\n\n√\n\nd/γ)d · Since the algorithm involves solving (2 poly(n) as claimed. Next, since the algorithm is maximizing V (λ) over a subset of Rd−1, it’s clear that ˆS ≥ Stability(X, y). It remains to prove the upper bound on ˆS. Recall from Lemma B.1 that for any λ ∈ Rd−1,\n\nd/γ)d linear programs, the time complexity is (2\n\n(cid:40)\n\nV (λ) = inf u∈Rd\n\nsup w∈[0,1]n\n\n∥w∥1\n\nwi(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ ≥ 0\n\n.\n\n(cid:41)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\ni=1\n\nLet λ∗ be a maximizer of V (λ) and for notational convenience let k = V (λ∗). If ̃Xλ∗ = y, then the algorithm correctly outputs 0. Otherwise, by the guarantee of Lemma E.1, there is some λ ∈ N with ̃Xλ ̸= y and σ ∈ {−1, 1} such that\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n ̃Xλ∗ − y (cid:13) ̃Xλ∗ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n− σ\n\n ̃Xλ − y (cid:13) ̃Xλ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n\n≤ γ.\n\nPick any u ∈ Rd. Since V (λ∗) = k, there is some w∗ = w∗(σu) ∈ [0, 1]n such that ∥w∗∥1 ≥ k and\n\nn (cid:88)\n\ni=1\n\nw∗\n\ni (⟨ ̃Xi, λ∗⟩ − yi)⟨Xi, σu⟩ ≥ 0.\n\nWithout loss of generality, there is at most one coordinate i ∈ [n] such that wi is strictly between 0 and 1. Also, the above inequality implies that\n\nn (cid:88)\n\ni=1\n\nw∗\n\ni\n\n⟨ ̃Xi, λ⟩ − yi (cid:13) (cid:13) ̃Xλ − y (cid:13) (cid:13) (cid:13)2 (cid:13)\n\n⟨Xi, u⟩ ≥\n\n\n\nw∗\n\ni\n\n\n\nn (cid:88)\n\ni=1\n\n⟨ ̃Xi, λ⟩ − yi (cid:13) (cid:13) ̃Xλ − y (cid:13) (cid:13) (cid:13)2 (cid:13)\n\n− σ\n\n⟨ ̃Xi, λ∗⟩ − yi (cid:13) (cid:13) ̃Xλ∗ − y (cid:13) (cid:13) (cid:13)2 (cid:13)\n\n\n\n ⟨Xi, u⟩\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n ̃Xλ − y (cid:13) ̃Xλ − y (cid:13) (cid:13)2\n\n≥ −\n\n(cid:13) (cid:13) (cid:13) ≥ −γ ∥Xu∥2\n\n− σ\n\n ̃Xλ∗ − y (cid:13) ̃Xλ∗ − y (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n\n∥Xu∥2\n\nwhere the last two inequalities are by Cauchy-Schwarz and the guarantee of Lemma E.1, respectively. Now define w ∈ [0, 1]n by the following procedure. Initially set w := w∗. Iterate through [n] in increasing order of (⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ and repeatedly set the current coordinate wi := 0, until\n\nn (cid:88)\n\ni=1\n\nwi(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ ≥ 0.\n\nObviously, this procedure will terminate with a feasible w. If it terminates after making t updates, then ∥w∥1 ≥ ∥w∗∥1 − t. Throughout the procedure, the sum (cid:80)n i=1 wi(⟨ ̃Xi, λ⟩ − yi)⟨Xi, u⟩ is nondecreasing. If |⟨ ̃Xi, λ⟩ − yi| ≥ (δ/ i = 1,\n\nand |⟨Xi, u⟩| ≥ (δ/\n\nn) ∥Xu∥2 and w∗\n\nn)\n\n√\n\n√\n\nthen the sum increases by at least (δ2/n)\n\n(cid:13) γn/δ2 ≤ εn. But the number of steps with |⟨ ̃Xi, λ⟩ − yi| < (δ/ (cid:13) (cid:13)2 (ε, δ)-strong anti-concentration. Similarly, the number of steps with |⟨Xi, u⟩| < (δ/ most εn. And the number of steps with 0 < w∗ We conclude that V (λ) ≥ V (λ∗) − 3εn − 1, so ˆS ≤ Stability(X, y) + 3εn + 1.\n\nn) ∥Xu∥2 is at i < 1 is at most 1. Thus, ∥w∥1 ≥ ∥w∗∥1 − 3εn − 1.\n\nis at most εn by √\n\n∥Xu∥2. So the number of such steps is at most ̃Xλ − y\n\n(cid:13) (cid:13) (cid:13)\n\nn)\n\n√\n\n(cid:13) (cid:13) (cid:13) (cid:13) ̃Xλ − y (cid:13) (cid:13)\n\n(cid:13) ̃Xλ − y (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)2\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nF MOTIVATION FOR ANTI-CONCENTRATION ASSUMPTIONS\n\nF.1 SMOOTHING IMPLIES ANTI-CONCENTRATION\n\nIn this section, we show that anti-concentration (Assumption A) holds under the mild assumption that the response variable is smoothed. In the following proposition, notice that we always have the crude bound (cid:13)\n\n(cid:13)Xβ(0) − r(cid:13)\n\n(cid:13)2 ≤ ∥r∥2.\n\ni=1 be arbitrary with X1, . . . , Xn ∈ Rd and r1, . . . , rn ∈ Proposition F.1. Let σ > 0 and let (Xi, ri)n R. Suppose that Z1, . . . , Zn ∼ N (0, σ2) are independent Gaussian random variables, and define yi = ri + Zi for i ∈ [n]. Then with probability at least 1 − n−d − 2e−n, the dataset (Xi, yi)n satisfies (2d/n, δ)-anti-concentration where\n\ni=1\n\nδ = min\n\n(cid:32)\n\n√\n\nn\n\nσ\n\nd (cid:13)\n\n(cid:13)Xβ(0) − r(cid:13) (cid:13)2\n\n(cid:33)\n\n,\n\n1 √\n\n3n3\n\n2d\n\nand β(0) ∈ OLS(X, r, 1).\n\nProof. Without loss of generality, β(0) = (X T X)†X T r. Define ˆβ = (X T X)†X T y; we want to . Let Q = X(X T X)†X T ; then y − X ˆβ ∼ N ((I − Q)r, σ2(I − Q)). upper bound\n\n(cid:13) (cid:13)\n\n(cid:13)X ˆβ − y\n\n(cid:13) (cid:13) (cid:13)2\n\n√\n\n(cid:13) (cid:13) (cid:13)2\n\n(cid:13)r − Xβ(0)(cid:13)\n\nBut (I − Q)r = r − Xβ(0), and N (0, σ2(I − Q)) is stochastically dominated by N (0, σ2I). So (cid:13) (cid:13)y − X ˆβ (cid:13) n with probability at least 1 − 2e−n, by the tail bound for χ2\n\n≤ (cid:13) random variables. Next, we claim that with high probability, for every set S ⊆ [n] of size |S| = 2d, there is no β ∈ Rd such that |Xiβ − yi| ≤ σ/(n3 2d) for all i ∈ [S]. Fix some S. We bound the probability that for all β ∈ Rd,\n\n(cid:13)2 + 2σ\n\n√\n\n(cid:88)\n\n(Xiβ − yi)2 ≤\n\ni∈S\n\nσ2 n6\n\n√\n\nbecause this event contains the event that all residuals in S are at most σ/(n to restrict to the OLS estimator ˆβOLS = (X T have\n\nS yS. Now defining P = XS(X T\n\nS XS)†X T\n\n2d). Now it suffices S XS)†X T S , we\n\nyS − XS\n\nˆβOLS = (rS + ZS) − P (rS + ZS) = (I − P )(rS + ZS) ∼ N ((I − P )rS, σ2(I − P )).\n\nNote that I − P is an orthogonal projection onto a space of dimension d′ := |S| − rank(XS) ≥ d, so there is a matrix M ∈ R2d×d′\n\nbe such that M M T = I − P and M T M = Id′. Then we can write\n\nyS − XS\n\nˆβOLS = σM A + (I − P )rS\n\n= M (σA + M T rS)\n\nfor a random vector A ∼ N (0, Id′). This means that 2. But for any μ ∈ R, note that |ξ + μ| stochastically dominates |ξ| where ξ ∼ N (0, 1), so (ξ + μ)2 stochastically dominates ξ2, and thus (cid:13) 2. Therefore\n\n(cid:13)σA + M T rS\n\n(cid:13)σA + M T rS\n\n(cid:13) (cid:13) (cid:13)yS − XS\n\nˆβOLS\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n= (cid:13)\n\n(cid:13) 2\n(cid:13)\n\n(cid:20)(cid:13) (cid:13) (cid:13)yS − XS\n\nPr\n\nˆβOLS\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n≤\n\n(cid:21)\n\nσ2 n6\n\n(cid:13) 2\n2 stochastically dominates ∥σA∥2 (cid:13) (cid:20)\n\n(cid:21)\n\n≤ Pr\n\nA2\n\n1 + · · · + A2\n\nd′ ≤\n\n1 n6\n\n.\n\nFor any 1 ≤ i ≤ d′, since the density of Ai is bounded above by 1/\n\n√\n\n2π, we have\n\nPr[A2\n\n1 + · · · + A2\n\nd ≤ 1/n6] ≤\n\nd′ (cid:89)\n\ni=1\n\nPr[|Ai| ≤ 1/n3] ≤\n\n24\n\n(cid:32) √ 2\n√\n\nn3\n\nπ\n\n(cid:33)d′\n\n≤ n−3d′\n\n.\n\nPublished as a conference paper at ICLR 2023\n\nRecalling that d′ ≥ d, this shows that for a fixed S ⊆ [n] of size 2d, with probability at least 1 − n−3d, there is no β ∈ Rd with |Xiβ − yi| ≤ σ/(n3 2d) for all i ∈ S. A union bound over sets S of size 2d proves the claim.\n\n√\n\nFinally, we use the above bounds to show that the smoothed data satisfies (2d/n, δ)-anti-concentration. We consider two cases. First, if (cid:13) n, then with probability 1 − 2e−n we have (cid:13)2 ≤ σ (cid:13) (cid:13)X ˆβ − y (cid:13)\n\nn, so for any β ∈ Rd, the number of i ∈ [n] satisfying\n\n(cid:13)Xβ(0) − r(cid:13)\n\n≤ 3σ\n\n√\n\n√\n\n(cid:13) (cid:13) (cid:13)2\n\n|⟨Xi, β⟩ − yi| ≤\n\nδ √\nn\n\n(cid:13) (cid:13)\n\n(cid:13)X ˆβ − y\n\n(cid:13) (cid:13) (cid:13)2\n\n≤\n\n1 √\n\n3n3\n\n2nd\n\n·\n\n(cid:13) (cid:13)\n\n(cid:13)X ˆβ − y\n\n(cid:13) (cid:13) (cid:13)2\n\n≤\n\nσ √\n\n2d\n\nn3\n\nis at most εn.\n\nF.2 DISTRIBUTIONAL ASSUMPTIONS FOR STRONG ANTI-CONCENTRATION\n\nIn this section, we show that under reasonable distributional assumptions on the samples (Xi, yi)n i=1, strong anti-concentration (Assumption B) holds with constant ε, δ > 0. First, it holds if the samples (Xi, yi) are i.i.d. and have an arbitrary multivariate Gaussian joint distribution.\n\nProposition F.2. Let n, d, ε > 0 and set δ = ε/4. Let Σ : d × d be symmetric and positive-definite. Let Z1, . . . , Zn ∼ N (0, Σ) be independent and identically distributed. If n ≥ Cd log(n)/ε2 for some absolute constant C, then with probability at least 1 − exp(−Ω(ε2n)), samples (Zi)n i=1 satisfy (ε, δ)-strong anti-concentration, i.e. for all β ∈ Rd, it holds that\n\n(cid:26)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ni ∈ [n] : |⟨Zi, β⟩| <\n\nδ √\nn\n\n∥Zβ∥2\n\n(cid:27)(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ εn\n\nwhere Z : n × d is the matrix with rows Z1, . . . , Zn.\n\n(cid:80)n\n\nProof. Let ˆΣ = 1 i . Then by concentration of Wishart matrices, it holds with probability at least 1 − 2 exp(−(n − d)/2) that ˆΣ ⪯ 2Σ (see e.g. Exercise 4.7.3 in Vershynin (2018)). In this event, ∥Zβ∥2\n\n2 /n ≤ 2E|⟨Z1, β⟩|2 for all β ∈ Rd.\n\ni=1 ZiZ T\n\nn\n\nLet F = {fβ : β ∈ Rd} be the class of binary functions fβ(x) = 1[|⟨x, β⟩|2 < (ε2/4)E|⟨Z1, β⟩|2]. Observe that every function in F is the intersection of parallel half-spaces, so F has VC dimension O(d). Moreover, for any β,\n\nEfβ(Z1) = Pr[|⟨Z1, β⟩|2 < (ε2/4)βT Σβ] ≤\n\nε 2\n\nsince ⟨Z1, β⟩ ∼ N (0, βT Σβ), and Gaussian random variables are anti-concentrated (Pr(|ξ| < c) ≤ c for any c > 0 and ξ ∼ N (0, 1)). Thus, by the Vapnik-Chervonenkis bound and assumption on n,\n\n(cid:32)\n\nPr\n\nsup f ∈F\n\n1 n\n\nn (cid:88)\n\ni=1\n\nfβ(Zi) > Efβ(Z1) + ε/2\n\n≤ exp(O(d log n) − Ω(ε2n)) ≤ exp(−Ω(ε2n)).\n\n(cid:33)\n\nSo with probability at least 1 − exp(−Ω(ε2n)) it holds that for all β ∈ Rd,\n\n(cid:12) (cid:12)\n\n(cid:8)i ∈ [n] : |⟨Zi, β⟩|2 < (ε2/4)βT Σβ(cid:9)(cid:12)\n\n(cid:12) ≤ εn.\n\nThis means that with probability at least 1 − exp(−Ω(ε2n)) − exp(−(n − d)/2), we have that for all β ∈ Rd,\n\n(cid:110)\n\n(cid:12) (cid:12) (cid:12)\n\ni ∈ [n] : |⟨Zi, β⟩|2 < (ε2/8) ∥Zβ∥2\n\n2 /n\n\n(cid:111)(cid:12) (cid:12) (cid:12) ≤ εn\n\nas desired.\n\nSecond, strong anti-concentration holds if the samples are drawn from a mixture of centered Gaussian distributions, with arbitrary weights, so long as each covariance matrix has bounded largest and smallest eigenvalues.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nProposition F.3. Let n, d, k, ε > 0 and let Σ1, . . . , Σk be symmetric and positive-definite matrices. Suppose that there are constants λ, Λ > 0 such that λI ⪯ Σ1, . . . , Σk ⪯ ΛI, and define κ = Λ/λ. For arbitrary weights w1, . . . , wk ≥ 0 with w1 + · · · + wk = 1, let Z1, . . . , Zn ∼ (cid:80)k i=1 wiN (0, Σi) be independent and identically distributed. If n ≥ Cd log(n)/ε2 for some absolute constant C, then with probability at least 1 − exp(−Ω(ε2n)), samples (Zi)n i=1 satisfy (ε, ε/(4κ))-strong anticoncentration.\n\nProof. The proof is similar to that of the previous proposition. First, note that each Zi can be coupled i ⪯ ξξT . So with probability at least 1 − 2 exp(−(n − d)/2) it holds with ξ ∼ N (0, ΛI) so that ZiZ T 2 for all β ∈ Rd. that 1\n\ni ⪯ 2λI and thus ∥Zβ∥2\n\n2 /n ≤ 2Λ ∥β∥2\n\ni=1 ZiZ T\n\n(cid:80)n\n\nn\n\nNext, let F = {fβ : β ∈ Rd} be the class of binary functions fβ(x) = 1[|⟨x, β⟩|2 < ε2λ ∥β∥2 Then F has VC dimension O(d), and for any β,\n\n2 /4].\n\nEfβ(Z1) = Pr[|⟨Z1, β⟩|2 < ε2λ ∥β∥2\n\n2 /4] ≤\n\nε 2\n\nsince ⟨Z1, β⟩ ∼ (cid:80)k Chervonenkis bound, with probability at least 1 − exp(−Ω(ε2n)), we get that for all β ∈ Rd,\n\ni=1 wiN (0, βT Σiβ) is stochastically dominated by N (0, λ ∥β∥2\n\n2). By the Vapnik-\n\n|{i ∈ [n] : |⟨Zi, β⟩|2 < (ε2/4)λ ∥β∥2\n\n2 | ≤ εn.\n\nCombining with the upper bound on ∥Zβ∥2 exp(−(n − d)/2) that for all β ∈ Rd,\n\n2, we have with probability at least 1 − exp(−Ω(ε2n)) −\n\n|{i ∈ [n] : |⟨Zi, β⟩|2 < (ε2/(8κ)) ∥Zβ∥2\n\n2 /n}| ≤ εn\n\nas claimed.\n\nG EXTENSION TO IV LINEAR REGRESSION\n\nWe extend the definition of stability to measure the stability of the sign of a coefficient of the IV linear regressor: Definition G.1. For samples (Xi, yi, Zi)n instruments Zi ∈ Rp, the ordinary IV estimator set with weight vector w ∈ [0, 1]n is\n\ni=1 with covariates Xi ∈ Rd, response yi ∈ Rd, and\n\nIV(X, y, Z, w) = {β ∈ Rd : Z T (w ⋆ (Xβ − y)) = 0}.\n\nThe finite-sample stability of (Xi, yi, Zi)n\n\ni=1 is then defined\n\nIV-Stability(X, y, Z) :=\n\ninf w∈[0,1]n,β∈Rd\n\n{n − ∥w∥1 : β1 = 0 ∧ β ∈ IV(X, y, Z, w)}.\n\nFor any k, the expression IV-Stability(X, y, Z) ≥ k is still defined as a bilinear system of equations in β and w. Our exact algorithm for Stability(X, y) never uses that the weighted residual (w ⋆ (Xβ − y)) is multiplied by X T in the OLS solution set, rather than some arbitrary matrix Z T ; all that matters is that this matrix has at most d rows. Thus, with a bound on the number of instruments, the algorithm generalizes to computing IV-Stability(X, y, Z): Theorem G.2. There is an nO(dp(d+p))-time algorithm which, given n arbitrary samples i=1 with X1, . . . , Xn ∈ Rd and Z1, . . . , Zn ∈ Rp and y1, . . . , yn ∈ R, and given (Xi, yi, Zi)n k ≥ 0, decides whether IV-Stability(X, y, Z) ≤ k.\n\nThe algorithm NETAPPROX also generalizes to IV regression. To state the guarantee, we define an anti-concentration assumption for IV regression data. Assumption C. Let ε, δ > 0. We say that samples (Xi, yi, Zi)n concentration if for every α ∈ Rp and β ∈ Rd it holds that\n\ni=1 satisfy (ε, δ)-strong anti-\n\n|{i ∈ [n] : |⟨Zi, α⟩| < (δ/\n\nn) ∥Zα∥2}| ≤ εn\n\n√\n\nand\n\n|{i ∈ [n] : |⟨ ̃Xi, β⟩ − yi⟩| < (δ/\n\n√\n\nn)\n\n(cid:13) (cid:13) (cid:13)\n\n ̃Xβ − y\n\n(cid:13) (cid:13) (cid:13)2\n\n}| ≤ εn.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nThen it is easy to see that the proof of Theorem 1.5 immediately extends to give the following result: d/(εδ2))d · poly(n)-time algorithm which, given i=1 satisfying (ε, δ)-strong anti-concentration, returns an estimate ˆS\n\nTheorem G.3. For any ε, δ > 0, there is a ( ε, δ, and samples (Xi, yi, Zi)n satisfying\n\n√\n\nIV-Stability(X, y, Z) ≤ ˆS ≤ IV-Stability(X, y, Z) + 3εn + 1.\n\nH HEURISTIC FOR LOWER BOUNDING STABILITY\n\nIn this section we explain the “LP lower bound” which we applied in Section 5 to provide exact lower bounds on stability of various datasets.\n\nGiven a list of thresholds T and a subset size m, we randomly pick a set S ⊆ [n] of size m, and enumerate the regions R1, . . . , Rp defined by the hyperplanes\n\n⟨ ̃Xi, λ⟩ − yi = t\n\n∀i ∈ S, ∀t ∈ T.\n\nFix one such region R. Similar to PARTITIONANDAPPROX, we use the change of variables gi = (1 − wi)(⟨ ̃Xi, λ⟩ − yi), and enforce the constraint X T g = X T ( ̃Xλ − y). For some of the i ∈ [n], the residual ⟨ ̃Xi, λ⟩ − yi will have constant sign on the entire region. For these samples, we enforce the constraint 0 ≤ gi ≤ ⟨ ̃Xi, λ⟩ − yi (if the residual is non-negative) or ⟨ ̃Xi, λ⟩ − yi ≤ gi ≤ 0 (if the residual is non-positive). However, because we didn’t include a hyperplane for every sample, it’s likely that for some i ∈ [n], the residual attains both signs within the region, in which case the constraint wi ∈ [0, 1] is not convex in g and λ. Thus, we relax the constraint to\n\ninf λ∈R\n\n⟨ ̃Xi, λ⟩ − yi ≤ gi ≤ sup\n\nλ∈R\n\n⟨ ̃Xi, λ⟩ − yi.\n\nNote that this is indeed a relaxation, because the interval contains 0. Finally, let K+ ⊆ [n] be the set of indices for which the residual is non-negative on R, and let K− ⊆ [n] be the set of indices for which the residual is non-positive. Then we minimize the objective gi inf λ∈R⟨ ̃Xi, λ⟩ − yi\n\ngi supλ∈R⟨ ̃Xi, λ⟩ − yi\n\n(cid:88)\n\n(cid:88)\n\n+\n\n.\n\ni∈K−\n\ni∈K+\n\nBecause this objective is less than or equal to (cid:80) i∈[n] gi/(⟨ ̃Xi, λ⟩ − yi), and because we only relaxed constraints, this program has value at most VR (the value of the exact non-linear program restricted to λ ∈ R).\n\nCompared to the provable approximation algorithm described previously, this algorithm is a heuristic because each region will typically have samples for which the residual changes sign within the region. As rough intuition, if the residual ⟨ ̃Xi, λ⟩ − yi remains fairly small throughout the region then the relaxation of the constraint on wi may not be problematic. However, if the residual can attain large magnitude in the region, then relaxing the constraint on wi may significantly change the value of the program. However, we may expect that if the partition is sufficiently fine, then a region where some residual is allowed to blow up may have many samples whose residuals are forced to be large. This motivates the use of an additional heuristic to refine the certification algorithm: if (w∗, β∗) are the optimal solution to the sensitivity problem, and β(0) = OLS(X, y, 1) is the original regressor, then it must hold that\n\nn (cid:88)\n\ni=1\n\nw∗\n\ni (⟨Xi, β∗⟩ − yi)2 ≤\n\nn (cid:88)\n\ni=1\n\nw∗\n\ni (⟨Xi, β(0)⟩ − yi)2 ≤\n\nn (cid:88)\n\n(⟨Xi, β(0)⟩ − yi)2.\n\ni=1\n\nHowever, if R is the region containing the optimal solution and if ∥w∗∥1 ≥ n − k, then\n\nn (cid:88)\n\ni=1\n\nw∗\n\ni (⟨Xi, β∗⟩−yi)2 ≥\n\ninf S⊆[n]:|S|=n−k\n\n(cid:88)\n\n(⟨Xi, β∗⟩−yi)2 ≥\n\ni∈S\n\ninf S⊆[n]:|S|=n−k\n\n(cid:88)\n\ni∈S\n\ninf λ∈R\n\n(⟨ ̃Xi, λ⟩−yi)2.\n\nThus, if we compute Qi(R) = inf λ∈R(⟨ ̃Xi, λ⟩ − yi)2 (by computing the interval of achievable residuals) for each i ∈ [n], then we can lower bound k (conditioned on R containing the optimal solution) by sorting Q1(R), . . . , Qn(R) and finding the smallest subset K ⊆ [n] such that (cid:80)\n\ni̸∈K Qi(R) ≤ (cid:13)\n\n(cid:13)Xβ(0) − y(cid:13) 2\n(cid:13)\n\n2.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nTo summarize the algorithm, for each region we compute the maximum of the LP lower bound and the residual lower bound, and the output of the algorithm is the minimum over all regions. See Algorithm 4 in Appendix J for detailed pseudocode.\n\nI FURTHER EXPERIMENTAL DETAILS\n\nI.1\n\nIMPLEMENTED ALGORITHMS\n\nNet upper bound. We implement NETAPPROX with one modification: instead of deterministically picking the net M by discretization (see Lemma E.1), we let M be a set of random unit vectors from S d′−1, and then compute N as in Lemma E.1. Instead of parametrizing the algorithm by the desired approximation error ε, we parametrize by |M|. Despite this change, the algorithm still provides a provable, exact upper bound on Stability(X, y).\n\nLP lower bound. The disadvantage of NETAPPROX is that it only lower bounds the stability under an assumption that seems hard to check. The PARTITIONANDAPPROX algorithm is better, because it unconditionally, with high probability, outputs either an accurate estimate or a failure symbol ⊥. However, the Ω(nd) time complexity (needed so that in each region, all n residuals have constant sign) may be prohibitively slow in practice. For this reason, we introduce a heuristic simplification of PARTITIONANDAPPROX which provably lower bounds the stability with no assumptions.\n\nAt a high level, we decrease the number of regions by ignoring the requirement that within each region all residuals have constant sign. The algorithm is parametrized by a list of thresholds L and a subset size m, and the regions are demarcated by the hyperplanes ⟨ ̃Xi, λ⟩ − yi = t for m random choices i ∈ [n] and all t ∈ L. Now, for samples which do not have constant residual in a particular region R, the constraints wi ∈ [0, 1] are nonconvex after the change of variables. We relax these constraints to linear constraints, and relax the objective function to skip the “bad” samples. Heuristically, this relaxation should not lose too much on samples where the residual remains small throughout R, but may be problematic if the residual blows up. This motivates the use of a complementary lower bound heuristic based on the minimum squared-loss achievable by any λ ∈ R. See Appendix H for details.\n\nBaseline greedy upper bound (Broderick et al., 2020; Kuschnig et al., 2021). We implement the greedy algorithm described by Kuschnig et al. (2021), which refines the algorithm of Broderick et al. (2020): iteratively remove the sample with the largest local influence until the sign of the first coefficient of the OLS is reversed. After each step, recompute the influences.\n\nBaseline lower bound. This algorithm simply computes the squared-loss-based lower bound (used in our full lower bound algorithm) for each region. See Appendix H for details.\n\nI.2 HYPERPARAMETER CHOICES\n\nThe net upper bound has only one hyperparameter (the number of trials), which should be chosen as large as possible subject to computational constraints. The LP lower bound has two hyperparameters (the size of the random subsample, and the set of thresholds). We choose these ad hoc subject to our computational constraints. Experiments to determine the optimal tradeoff between the subsample size and threshold set could be useful. However, we note that because the LP lower bound unconditionally lower bounds the stability, no matter what hyperparameters we choose, in practice it suffices to try several sets of hyperparameters and compute the maximum of the resulting lower bounds.\n\nHeterogeneous data experiment. For each dataset, we applied the net upper bound with 1000 trials, the LP lower bound with L = {−0.01, 0, 0.01} and m = 30, and the baseline lower bound with L = {−0.01, 0, 0.01} and m = 1000.\n\nIsotropic Gaussian data. For each dataset with noise level σ, we applied the net upper bound with 10d trials, the LP lower bound with L = {−σ, 0, σ} and m = 30, and the baseline lower bound with L = {−σ, 0, σ} and m = 1000.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nBoston Housing dataset. For the two-dimensional datasets, we applied the net upper bound with 100 trials and the LP lower bound with L = {0} and m = 100. For the three-dimensional dataset we applied the net upper bound with 1000 trials and the LP lower bound with L = {0} and m = 30.\n\nI.3\n\nINDEPENDENT TRIALS & ERROR BARS\n\nFor randomized algorithms with two-sided errors, standard practice is to run the algorithm multiple times on the same dataset and report e.g. the median and error bars. However, our algorithms provide unconditional upper/lower bounds on the stability, so this is not necessary. If we did run the LP lower bound multiple times, we could simply report the maximum outcome, and this would be a valid lower bound on the stability; similarly for the net upper bound we could report the minimum outcome.\n\nHowever, for our synthetic two-dimensional datasets (Figures 1 and 2a), because the data itself is random, we construct each dataset 10 independent times. For each algorithm and each dataset we compute the stability bound. Then, we report the median bound and error bars across the 10 independent-but-identically-constructed datasets. All error bars are 25th and 75th percentiles.\n\nThere are no error bars in our synthetic three-dimensional experiment (Figure 2b) because we only conducted one trial per noise level (rather than 10) due to computational constraints.\n\nI.4 COMPUTATIONAL DETAILS\n\nAll experiments were done in Python on a Microsoft Surface Laptop, using GUROBI (Gurobi Optimization, LLC, 2022) with an Academic License to solve the linear programs. Each plot took at most 30 hours to generate (specifically, the three-dimensional Gaussian isotropic data experiment took 3 hours for each of the 10 datasets, dominated by the time required for the LP lower bound algorithm; other plots were faster).\n\nI.5 OMITTED EXPERIMENT: COVARIANCE SHIFT\n\nConsider a dataset with n samples (Xi, yi) drawn from Xi ∼ N (0, Σ) and yi = −Xi1 + Xi2,\n\nwhere Σ =\n\n. Additionally, there are k + 1 outliers, in two types: the k type-I outliers\n\n(cid:21) (cid:20) 1 −1 2\n−1\n\n√\n\n(Xi, yi) have Xi = c(1, −3) and yi = −C large and negative; the one type-II outlier (Xi, yi) has n(1, 1) and yi chosen to exactly lie on the OLS best-fit hyperplane y = ⟨x, ˆβ⟩. Initially, Xi = ˆβ1 > 0, and clearly removing the last k + 1 samples suffices to flip the sign. However, the initial sample covariance is roughly ˆΣ = Σ + 11T =\n\n. The influence of a sample (Xi, yi) on\n\n(cid:21)\n\n(cid:20)2 0\n\n0 3\n\ncoordinate j of the OLS regressor ˆβ is ⟨( ˆΣ−1)j, Xi⟩ · (yi − ⟨Xi, ˆβ⟩). As a result, the type-I outliers have negative influence on ˆβ1, so the greedy algorithm initially does not remove them. The influence only becomes positive after removing the type-II outlier, because this shifts the sample covariance to Σ, and therefore flips the sign of ⟨( ˆΣ−1)1, Xi⟩.\n\nConstructing this example experimentally requires some care in the choices of k, c, and C: we need the total influence of type-A outliers (proportional to kcC) large enough that ˆβ1 > 0, and need kc2 small enough that the type-I outliers don’t affect the sample covariance much. Moreover, the number of trials needed by the net algorithm roughly scales with C. We take n = 1000, k = 30, c = 0.2, and C = 300. Applying the greedy baseline and the net upper bound (with 1000 trials), we find that the former removes 97 samples while the latter removes roughly 16.7 samples. In this example, the failure of the greedy baseline can be attributed to the constant-factor shift in the sample covariance achieved by removing the type-B outlier.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nI.6 OMITTED FIGURES\n\nFigure 4: Coefficients of OLS regression for the suburb/city split (Figure (a)) and a random split with the same ratio (Figure (b)). In both cases, we first rescale all covariates to mean 0 and variance 1, and add a constant variable (the last coefficient in both plots). In Figure (a), we then plot the OLS regressor on all samples (in blue), the OLS regressor on all 134 zn > 0 samples (in orange), and the OLS regressor on all 472 zn = 0 samples (in green). In Figure (b), we pick a subset S of samples of expected size 134. We plot the OLS regressor on all samples (in blue), the OLS regressor on S (in orange), and the OLS regressor on Sc (in green).\n\nJ FORMAL PSEUDOCODE FOR ALGORITHMS\n\nAlgorithm 1 NETAPPROX\n\n1: procedure NETAPPROX((Xi, yi)n ̃X ← [(X T )2; . . . , (X T )d] 2: if X T ( ̃Xλ − y) = 0 has a solution λ then\n\ni=1, ε, δ)\n\n3: 4: 5: 6: 7:\n\nreturn 0\n\nγ ← εδ2 A ← [(X T )2; . . . ; (X T )d; −y], d′ ← rank(A) U, D, V ← SVD(A) (so that A = U DV T and D is diagonal matrix of the d′ nonzero\n\nsingular values) B ← V D−1 Define γ-net for S d′−1 by\n\n8: 9:\n\nM ← {R(±m1, ±m2, . . . , ±md′) : m1, . . . , md′ ∈ {0, γ/\n\n√\n\n√\n\nd′, 2γ/\n\nd′, . . . , 1}}\n\nwhere R : d′ × d′ is uniformly random rotation matrix Define γ-net for “residual space” (Lemma E.1) by\n\n10:\n\nN ←\n\n(cid:26) (Bm)1:d−1 (Bm)d\n\n(cid:27)\n\n: m ∈ M\n\n.\n\n11: 12: 13:\n\n14:\n\n15:\n\nˆS ← n for λ ∈ N do\n\nSolve linear program\n\nV (λ) = sup\n\nw∈[0,1]n\n\n(cid:110)\n\n∥w∥1\n\nˆS ← min( ˆS, n − V (λ))\n\nreturn ˆS\n\n(cid:12) (cid:12)\n\n(cid:12)X T (w ⋆ ( ̃Xλ − y)) = 0\n\n(cid:111)\n\n.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2 GENERATEREGIONS\n\nR ← {Rd} for (vi, ti) do\n\n1: procedure GENERATEREGIONS((vi, ci)N 2: 3: 4: 5: 6:\n\nRnext ← R for R ∈ L do\n\nSolve linear programs\n\ni=1) ▷ List of regions, each described by linear constraints\n\nclow = inf x∈R\n\n⟨vi, x⟩\n\nand\n\nchigh = sup x∈R\n\n⟨vi, x⟩\n\nif clow < c < chigh then\n\nAdd regions R ∩ {⟨vi, x⟩ ≤ ci} and R ∩ {⟨vi, x⟩ ≥ ci} to Rnew\n\nelse\n\nAdd region R to Rnew\n\nR ← Rnew\n\nreturn R\n\n7: 8: 9: 10:\n\n11: 12:\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 3 PARTITIONANDAPPROX\n\ni=1, δ, ε, η)\n\n1: procedure PARTITIONANDAPPROX((Xi, yi)n 2:\n\nβ(0) ← arg minβ∈Rd ∥Xβ − y∥2 (cid:13)Xβ(0) − y(cid:13) M ← (cid:13) (cid:13)2 ̃X ← [(X T )2; . . . ; (X T )d] m ← Cε−1(d log ε−1 + log η−1) Sample i1, . . . , im ∈ [n] without replacement H ← {} for i ∈ [n] do\n\n2\n\n▷ X : n × d matrix with rows X1, . . . , Xn\n\n▷ C > 0 is a universal constant\n\nAdd ( ̃Xi, yi) to H Add ( ̃Xi, yi + M ), ( ̃Xi, yi − M ), ( ̃Xi, yi + δM/\n\nfor j ∈ [m] do\n\n▷ Describes hyperplane ⟨ ̃X, λ⟩ = yi √\nn) and ( ̃Xi, yi − δM/\n\nn) to H\n\n√\n\nfor 0 ≤ k ≤ ⌈log1+ε(1/δ) do\n\nAdd ( ̃Xij , yij + (1 + ε)kδM ) and ( ̃Xij , yij − (1 + ε)kδM ) to H\n\nR ← GENERATEREGIONS(H) for R ∈ R do\n\nFind feasible point λ0(R) ∈ R BM ← {i ∈ [n] : |⟨ ̃Xi, λ0(R)⟩ − yi| > M } BδM ← {i ∈ [n] : |⟨ ̃Xi, λ0(R)⟩ − yi| < δM/ if |BδM | > εn then\n\n√\n\nn}\n\nreturn ⊥ for i ∈ [n] do\n\nif inf λ∈R⟨ ̃Xi, λ⟩ − yi ≥ 0 then\n\nσi ← 1\n\nelse\n\nσi ← −1\n\nSolve linear program\n\n(ˆg(R), ˆλ(R))\n\n3:\n\n4: 5: 6: 7: 8:\n\n9:\n\n10:\n\n11: 12:\n\n13:\n\n14: 15: 16:\n\n17:\n\n18: 19: 20: 21:\n\n22: 23: 24: 25: 26:\n\n← arg sup\n\ng∈Rn,λ∈R,s∈Rn\n\n27:\n\nreturn\n\n\n\n \n\n(cid:88)\n\nsi\n\ni∈[n]\\(BM ∪BδM )\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nX T g = 0 0 ≤ gi ≤ ⟨ ̃Xi, λ⟩ − yi ⟨ ̃Xi, λ⟩ − yi ≤ gi ≤ 0 ∀i ∈ [n] : σi = −1 si ≤ 1 ∀i ∈ [n] \\ (BM ∪ BδM ) si ≤ gi/(⟨ ̃Xi, λ0(R)⟩ − yi) ∀i ∈ [n] \\ (BM ∪ BδM )\n\n∀i ∈ [n] : σi = 1\n\n\n\n \n\nmax R∈R\n\n(cid:88)\n\ni∈[n]\n\nˆg(R)i ⟨ ̃Xi, ˆλ(R)⟩ − yi\n\n.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 LP Lower Bound\n\ni=1, L, m)\n\n1: procedure LPLOWERBOUND((Xi, yi)n 2:\n\nβ(0) ← arg minβ∈Rd ∥Xβ − y∥2 ̃X ← [(X T )2; . . . ; (X T )d] Sample i1, . . . , im ∈ [n] without replacement H ← {} for j ∈ [m] do\n\n2\n\n3: 4: 5: 6: 7: 8:\n\n9: 10: 11:\n\n12: 13:\n\n14: 15:\n\nfor t ∈ L do\n\nAdd ( ̃Xij , yij + t) to H R ← GENERATEREGIONS(H) for R ∈ R do\n\nFor each i ∈ [n], compute\n\nli = inf λ∈R\n\n⟨ ̃Xi, λ⟩ − yi\n\nand\n\nri = sup λ∈R\n\n⟨ ̃Xi, λ⟩ − yi.\n\nSet K+ = {i ∈ [n] : li ≥ 0} and K− = {i ∈ [n] : ri ≤ 0} \\ K−. Solve linear program\n\nˆS(R) ← inf\n\ng∈Rn,λ∈R\n\n\n\n \n\n(cid:88)\n\ni∈K+\n\ngi ri\n\n(cid:88)\n\n+\n\ni∈K−\n\ngi li\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nX T g = X T ( ̃Xλ − y) 0 ≤ gi ≤ ⟨ ̃Xi, λ⟩ − yi ⟨ ̃Xi, λ⟩ − yi ≤ gi ≤ 0 li ≤ gi ≤ ri\n\n∀i ∈ K+\n\n∀i ∈ K− ∀i ∈ [n] \\ (K+ ∪ K−)\n\n\n\n \n\n.\n\nFor each i ∈ K+ ∪ K− let Qi = min(l2 Let Q(1) ≤ · · · ≤ Q(n) be the sorted numbers Q1, . . . , Qn and compute\n\ni ); for each i ∈ [n] \\ (K+ ∪ K−) let Qi = 0.\n\ni , r2\n\n(cid:40)\n\nˆN (R) ← sup\n\nk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn−k (cid:88)\n\ni=1\n\nQ(i) >\n\n(cid:13) (cid:13)Xβ(0) − y (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n(cid:41)\n\n.\n\n16:\n\nreturn\n\nmax( ˆS(R), ˆN (R)).\n\nmin R∈R\n\n33",
    "reference": "# Summary Of The Paper\n\nThe paper studies the stability of linear regression problem by defining it as the minimum samples (in $\\ell_1$-norm) to be removed to zero out the first coordinate. The paper provides a $O(n^{d^3})$ algorithm to exactly solve the problem, and also provide a lower bound stating that the computation complexity cannot be $O(n^{o(d)})$. Further, approximate algorithms are provided and experiments are carried out on real dataset to identify stability.\n\n# Strength And Weaknesses\n\nUnlike the previous results using influence functions to approximate the local stability, the paper study the exact problem--which means it is able to identify perturbation of the solution beyond local regime. The theory and experiments are sound. However, even with the relaxed approximate algorithm, the computation cost seems still very big.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written, and provide good insight into studying stability of optimization problem.\n\n# Summary Of The Review\n\nThe paper is well written and organized. The problem is clearly formulated, and the authors tackle the exact stability problem instead of using local approximations from influence functions, which is novel and interesting to me. By exactly auditing linear regression, the authors provide theoretical guarantees for such identification and the algorithm works well on real datasets. Although the complexity of the algorithm seems to suffer from curse of dimensionality, but it is a good starting point for study stability problem in its exact form. I think this is a good paper and would like to accept.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nE-FORCING: IMPROVING AUTOREGRESSIVE MODELS BY TREATING IT AS AN ENERGY-BASED ONE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAutoregressive generative models are commonly used to solve tasks involving sequential data. They have, however, been plagued by a slew of inherent flaws due to the intrinsic characteristics of chain-style conditional modeling (e.g., exposure bias or lack of long-range coherence), severely limiting their ability to model distributions properly. In this paper, we propose a unique method termed EForcing for training autoregressive generative models that takes advantage of a well-designed energy-based learning objective. By leveraging the extra degree of freedom of the softmax operation, we are allowed to make the autoregressive model itself an energy-based model for measuring the likelihood of input without introducing any extra parameters. Furthermore, we show that with the help of E-Forcing, we can alleviate the above flaws for autoregressive models. Extensive empirical results, covering numerous benchmarks demonstrate the effectiveness of the proposed approach.\n\n1\n\nINTRODUCTION\n\nBy factorizing the joint distribution into the product of a series of conditional distributions, autoregressive generative models (abbr. ARGMs) (Vaswani et al., 2017; Dai et al., 2019; van den Oord et al., 2016a;b; Salimans et al., 2017; Chen et al., 2018) simplify the difficult challenge of modeling high-dimensional joint distributions. They can be trained efficiently via maximum likelihood and generate samples of exceptional quality, making this technique popular for modeling distributions, especially for sequential data. Nonetheless, despite their potency and flexibility, and huge success, ARGMs still have inherent weaknesses due to the intrinsic characteristics of chain-style conditional modeling, especially when the training data is less diverse 1. For example, ARGMs usually suffer from a discrepancy in distributions of input contexts between the training and inference stages, which causes a consequent performance drop, i.e., Exposure Bias (Ranzato et al., 2016; Bengio et al., 2015). Besides, due to the nature of the greedy selection of beam search approximations, the decoded results from ARGMs may also lack long-range coherence (Deng et al., 2020).\n\nEarlier work, both heuristic and theoretical, has been proposed to address these concerns. For instance, the exposure bias problem of ARGMs can be alleviated to some extent with scheduled sampling (Bengio et al., 2015; Mihaylova & Martins, 2019), by mixing input contexts from both real data and autoregressive generation, during the training stage. However, this scheme introduces some new problems like the over-correcting (Zhang et al., 2019) issue. In addition, at the inference stage, sampling methods such as beam search is employed to generate diverse candidates with high likelihoods, improving the quality of generated sequences. Nevertheless, these approaches result in only marginal improvements in temporal coherence.\n\nIn this paper, we propose an elegant solution, i.e., E-Forcing, for the above problems of ARGMs by leveraging a deep connection between ARGMs and Energy-based models (EBMs). EBMs are a popular class of generative models that have demonstrated their effectiveness in modeling high-dimensional distributions in a variety of machine learning applications, without requiring the transformation of the target distribution into a product of conditional distributions (Zhao et al., 2017;\n\n1When trained on massive datasets under which the underlying distribution is diverse enough, such as in large language models, this problem can be relieved because the training data covers a lot of corner cases making the model much harder to go off distribution.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nArbel et al., 2021; Gao et al., 2021). As a result, several studies (Deng et al., 2020; Bakhtin et al., 2021; Durkan & Nash, 2019) have made their attempts to benefit ARGMs from the advantages of EBMs. However, though some positive results were obtained, the existing works preferred a two-stage optimization, which first obtained a well-trained ARGM and then trained an additional EBM based on it. Such an optimization strategy not only introduced a heavy training process for EBM but also did not enable ARGMs themselves to benefit from the properties of EBM in modeling the joint distribution in a temporally more coherent way, and required more training parameters to estimate energy scores, burdening the intricacy of the learning task.\n\nOur method of combing ARGMs and EBMs takes a different approach, which seamlessly integrates energy-based models into autoregressive models by utilizing the extra degree of freedom within the final softmax layer of the model. We show that in this way the ARGM can be trained using an energy-based learning objective, which allows the ARGM to avoid those intrinsic concerns, such as exposure bias, with the help of energy-based models as former work did (Deng et al., 2020; Bakhtin et al., 2021) whilst being free of increasing the learning model’s complexity. This property makes our E-Forcing rather easy to be applied in the training process of any ARGM for any specific task, as no structural changes are required.\n\nBesides, we follow the predominant approach for training explicit density generative models to minimize the KL divergence between the (empirical) data distribution and model distribution, which gives rise to the gradient-based contrastive divergence (CD) methods (Hinton, 2002; Kim & Bengio, 2016) for energy-based models. Typically, these methods require a Markov Chain Monte Carlo (MCMC) process to sample data from the EBM for the “negative phase” gradient estimation, which is extremely time-consuming and, meanwhile, inapplicable for discrete data, such as text. To solve this, we present a way to estimate those “negative phase” gradients through those samples generated with the network’s autoregressive view instead of the EBM view, making the training feasible. Since our method combines the EBM and ARGM seamlessly as a whole, i.e., the ARGM is also an EBM itself, the exposure bias problem can be mitigated due to the fact that autoregressively sampled data is involved in the “negative phase” of CD methods. On top of it, unlike ARGMs, which factor the joint distribution into a product of conditional distributions, EBMs are able to model the joint distribution directly and score each input at the sequence level instead of at the token level, which makes them capable of modeling long-range coherence.\n\nIn summary, the following contributions are made to this paper: i) We introduce a novel scheme by integrating the EBM view into autoregressive generative models seamlessly; ii) We proposed a novel method, named E-Forcing, for efficiently optimizing the energy-based autoregressive model via contrastive divergence based on importance sampling but not MCMC; iii) We successfully decrease the inherent flaws of autoregressive models — exposure bias and weak temporal coherence — by leveraging E-Forcing’s two-phase optimization, which makes use of both real and generated data; iv) We demonstrate clear improvements of the proposed methods on various tasks such as language modeling, machine translation, and image generation.\n\n2 BACKGROUND AND RELATED WORKS\n\n2.1 ENERGY-BASED MODELS\n\nLet pd denote the data distribution. Energy-based models (LeCun et al., 2006) are interested in learning an unnormalized energy function Eθ(x) that defines the density(mass) function πθ(x) as\n\nπθ(x) =\n\nexp(−Eθ(x)) Zθ\n\n,\n\n(1)\n\nwhere Eθ : X → R denotes an energy function which aims to map a data sample from data space X to an energy scalar, and Z(θ) = (cid:80) x exp(−Eθ(x)) denotes the normalizing constant, also known as the partition function, which can be barely estimated. Any function can be used as an energy function to represent an EBM as long as it can generate a single scalar given some input x and the normalizing constant is finite2. Contrastive divergence algorithms are commonly used to optimize EBMs via maximum log-likelihood (Hinton, 2002; Kim & Bengio, 2016; Grathwohl et al., 2020).\n\n2Without constraining the parametrization of Eθ, this can be achieved by bounding the region of space in\n\nwhich x takes its allowed values.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nCorrespondingly, the gradient of the log-likelihood, which needs to be maximized, with respect to θ can be expressed as\n\n(cid:104)\n\n∇θEpd(x)\n\n(cid:105)\n\nlog πθ(x)\n\n(cid:104)\n\n= Eπθ(x)\n\n(cid:105)\n\n∇θEθ(x)\n\n(cid:104)\n\n− Epd(x)\n\n(cid:105)\n\n∇θEθ(x)\n\n.\n\n(2)\n\nThe first term on the right-hand side of Eq.2 is usually called the “negative phase” term while the second term is called the “positive phase” term.\n\nIn general, due to the challenge of sampling from EBMs, training EBMs by contrastive divergence methods (Hinton, 2002; Kim & Bengio, 2016; Grathwohl et al., 2021) is difficult, especially on high-dimensional data. MCMC methods (Nijkamp et al., 2019; Du & Mordatch, 2019; Grathwohl et al., 2020) are usually adopted for data sampling. However, these methods require enormous extra computing overheads and are not applicable when the input is discrete such as text sequences (Deng et al., 2020). As a result, a variety of recent works attempt to explore the strategy of training an EBM without MCMC. In particular, Bakhtin et al. (2021); Xu et al. (2021); Gao et al. (2020) optimize the EBMs by using noise contrastive estimation (NCE) (Gutmann & Hyv ̈arinen, 2010; Ma & Collins, 2018). Durkan & Nash (2019) estimate the intractable normalization component by utilizing ARGMs and importance sampling. Bengio et al.; Che et al. (2020); Wang et al. (2021) skirt the challenge of collecting data in the high-dimensional data space by performing sampling using a carefully crafted latent space, which improves sampling efficiency.\n\n2.2 MODELING DISTRIBUTIONS AUTOREGRESSIVELY\n\nModeling high-dimensional data distributions directly is usually a rather challenging task due to “the curse of dimensionality” (Bellman, 1954). One alternative method is to sequential the random variables and then factorize the joint probability distribution into the product of conditionals based on the sequence structure, which is the core idea of autoregressive generative models (ARGMs). ARGMs have been very successful, in particular for sequential data. For example, ARGMs have been widely used in language modeling (Vaswani et al., 2017; Dai et al., 2019; Radford et al., 2019), audio synthesis (van den Oord et al., 2016a), and even image generation (van den Oord et al., 2016c;b; Salimans et al., 2017).\n\nHowever, the advantages of ARGMs are balanced to some extent by issues of (1) exposure bias (Ranzato et al., 2016; Bengio et al., 2015; Song et al., 2020), due to the discrepancy in input context distributions between the training and inference stages, and (2) weak long-range coherence (Deng et al., 2020), due to the inherent greedy selection of one token at a time without look-ahead.\n\n2.3\n\nTHE MIXTURE OF EBMS AND GENERATIVE MODELS\n\nThe seminal idea of combing a generative model and an energy-based model has been explored by a plethora of great works (Pang et al., 2020; Durkan & Nash, 2019; Xie et al., 2019; 2020; Xiao et al., 2021; Bakhtin et al., 2021; Che et al., 2020; Arbel et al., 2021; Deng et al., 2020; Bakhtin et al., 2021; Durkan & Nash, 2019). In particular, Pang et al. (2020) aimed to learn an energy-based model (EBM) in the latent space of a generator model, so that the EBM can act as a prior model on the generator model’s top-down network. VAEBM, a symbiotic composition of a variational auto-encoder and an EBM, was proposed by (Xiao et al., 2021). Arbel et al. (2021) proposed a novel training method for a GAN/EBM combined model by leveraging the Donsker-Varadham representation of KL-divergence.\n\nAmong these works, Residual EBM (Deng et al., 2020; Bakhtin et al., 2021; Durkan & Nash, 2019) and EBR (Naskar et al., 2020) may be the most related works to our paper. Authors of these works have made their attempt to benefit ARGMs from the advantages of EBMs. However, different from our work, these works utilize a two-stage optimization scheme, which first obtained a well-trained generative model and then trained an additional EBM on top of it. Such an optimization strategy does not enable ARGMs themselves to benefit from the properties of EBM in modeling the joint distribution. Besides, in order to benefit from the EBM, complicated re-sampling or re-ranking schemes are needed during inference time. It also increases parameters since it uses independent networks to represent the ARGM and the EBM, burdening the intricacy of the learning task. In contrast, we introduce the EBM inside the ARGM, treating the ARGM directly as an EBM itself.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 TREATING THE ARGM AS AN EBM\n\nIn this section, we present the overall framework of our E-Forcing method for training better autoregressive models. Let (x1, . . . , xK) be a random sequence of length K drawn from the real data distribution pd, xk denote the random variable at time step k, and x<k represent the random subsequence before time step k, i.e. x<k = (x1, x2, . . . , xk−1). The general spirit of our design is to model the joint distribution pd(xk, x<k) by integrating an EBM inside the autoregressive model qθ. Formally, given an autoregressive model qθ(x1, . . . , xK) = (cid:81)K k=1 qθ(xk|x<k) parameterized by θ, we introduce K independent energy-based models pθ(xk, x<k) for each time step k ≤ K, with the formulation following\n\npθ(xk, x<k) = qθ(x<k) ·\n\ne−φθ(xk,x<k) Zθ\n\n,\n\n(3)\n\nwhere Zθ is equal to Eqθ [(cid:80) e−φθ(xk,x<k)], indicating the normalization constant, φθ(·) represents the energy function. Essentially, pθ(xk, x<k) is a product EBM, defined as the product of qθ and another EBM φθ.\n\nxk\n\n3.1 DEFINITION OF THE ENERGY FUNCTION\n\nWe define the energy function φθ(xk, x<k) using xk’s corresponding component of network’s output logits given the input context x<k (e.g., given a sequence “This is Friday.” and assuming the corresponding index of the token “Friday” in the vocabulary is i, then the value of −φθ(“Friday”, “This is”) is the i-th component of the output logit, namely, the input tensor of the final softmax layer).\n\nThe rationale behind such a design of energy function is out of the extra degree of freedom concealed inside the softmax transformation S : RM → (0, 1)M , which can convert an unnormalized vector with size M into a probability distribution consisting of M probabilities\n\nS([z1, . . . , zM ]) = [\n\nez1 i=1 ezi\n\n(cid:80)M\n\n, . . . ,\n\nezM i=1 ezi\n\n(cid:80)M\n\n].\n\n(4)\n\nIt’s easy to observe that the softmax operation is unaffected by the input vector’s overall magnitude, that is, S([z1, . . . , zM ]) = S([z1, . . . , zM ] + C), ∀C ∈ R. Such a property allows us to model the energy function by using the ARGM itself instead of introducing a new network.\n\n3.2 ENERGY-BASED LEARNING OBJECTIVE\n\nOther than making the qθ to match pd, E-forcing has an additional training objective to make the K parametric distributions pθ(xk, x<k) to match the real data distribution pd(xk, x<k) at any time step k ≤ K. This can be achieved by minimizing the Kullback-Leibler (KL) divergence between the distributions for each time step of a sequence,\n\n(cid:16)\n\nDKL\n\npd(xk, x<k)||pθ(xk, x<k)\n\n(cid:17)\n\n, ∀k ∈ [1, K],\n\n(5)\n\nWe attempt to use contrastive divergence methods (Hinton et al., 1995; Kim & Bengio, 2016) to minimize the objective 5 by descending the gradient w.r.t. θ according to Eq. 2 for each time step. Specifically, given an arbitrary time step k, we have the corresponding gradient of objective 5 with respect to θ\n\n∇θLEBM −CD = Epd\n\n(cid:104)\n\n∇θEθ(xk, x<k)\n\n(cid:124)\n\n(cid:123)(cid:122) Positive Phase Gradient\n\n(cid:105)\n\n(cid:125)\n\nwhere Eθ(xk, x<k) = φθ(xk, x<k) − log qθ(x<k).\n\n− Epθ (cid:124)\n\n(cid:104)\n\n∇θEθ(xk, x<k)\n\n(cid:123)(cid:122) Negative Phase Gradient\n\n.\n\n(cid:105)\n\n(cid:125)\n\n(6)\n\nOptimization via Eq. 6 involves sampling data from the model distribution pθ and can thus lead to the discovery of non-data-like samples, whose likelihood is then explicitly reduced as the corresponding energy increases during the training. E-Forcing is therefore not plagued by the exposure bias problem naturally. Besides, because we model the joint distribution at each time step, E-Forcing can assess\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nthe sequence up to the current time step as a whole and generate more coherent data using energy sampling (Deng et al., 2020). However, the negative phase gradient is frustrating to compute, especially for discrete data (e.g. text) where common MCMC methods (Welling & Teh, 2011) can not even be applied. Therefore, we propose a novel variant of contrastive divergence methods for E-Forcing’s optimization in Section 4.\n\n4 OPTIMIZATION\n\nThe key obstacle of optimizing the objective 5 via contrastive divergence methods (Hinton, 2002)(i.e. descends the gradient of Eq. 6) is sampling data from the model distribution pθ for estimating the negative phase gradient. The common MCMC algorithms are not desirable for generating “negative” samples because they are rather time-consuming, and not applicable to discrete data. In order to make the optimization process both efficient and feasible, we modified the original CD methods by means of the importance sampling technique (Horvitz & Thompson, 1952), which holds two parts of gradient estimation.\n\n4.1 POSITIVE PHASE GRADIENTS\n\nSince the training set consists of i.i.d. samples sampled from the real distribution pd, the computing of positive phase gradients is not difficult. To be specific, by replacing Eθ(xk, x<k) with the form φθ(xk, x<k) − log qθ(x<k) in Eq.6, the positive phase gradient G(k) + (θ) with respect to parameter θ can be written into\n\nG(k)\n\n+ (θ) = Epd\n\n(cid:104)\n\n∇θφθ(xk, x<k) − ∇θ log qθ(x<k)\n\n(cid:105) .\n\n(7)\n\nSince carrying out sample estimation of the expectation over the data distribution pd is viable, and the score φθ(xk, x<k) can be acquired by simply accessing the output logit of ARGM (according to the definition of φθ in Sec. 3), the first term of the positive phase gradient G(k) + can likewise be readily computed. Besides, we can observe that the second term Epd [−∇θ log qθ(x<k)] of G(k) + (θ) is the negative gradient of likelihood qθ(x<k)’s logarithm, which is exactly the objective of maximizing the autoregressive generative model qθ’s log-likelihood.\n\n4.2 NEGATIVE PHASE GRADIENTS\n\nThe estimation of negative phase gradients G(k) − (θ) = Epθ [∇θφθ(xk, x<k)−∇θ log qθ(x<k)], on the other hand, is more involved. Sampling data from pθ is required for estimating the expectation Epθ , whereas pθ is the introduced energy-based autoregressive model, which is an explicit autoregressive generative model and we can only access its modeled density(mass) function pθ.\n\nInspired by the idea of importance sampling, we substitute the troublesome estimation of the expectation over distribution pθ with the expectation over distribution qθ, which is the underlying autoregressive model that can generate samples considerably easier. Accordingly, the negative phase gradient Exk,x<k∼pθ [∇θEθ(xk, x<k)] has the following form (See the detailed derivation in Appendix B),\n\nG(k)\n\n− (θ) = Eqθ\n\n(cid:104)\n\nw(x<k)(cid:2)∇θφθ(xk, x<k) − ∇θ log qθ(x<k)(cid:3)(cid:105)\n\n,\n\nwhere w(x<k) =\n\n(cid:80)\n\nxk\n\ne−φθ(xk,x<k)\n\nEqθ(x′\n\n<k)[(cid:80)\n\ne−φθ(xk,x′\n\n<k)]\n\nxk\n\n.\n\n(8)\n\n(9)\n\nAccording to Eq.8, all the estimated expectations only need sampling from the autoregressive model qθ rather than the distribution pθ, and the reweighing weight w in Eq. 9 does not involve expectation computation over distribution pθ either. Generally speaking, producing data from an autoregressive model is a simple ancestral sampling process and naturally suitable for discrete data, as compared with sampling straight from an explicit generative density estimator, which needs MCMC approaches (Durkan & Nash, 2019). Besides, the term Ex<k∼qθ(x<k)[w(x<k)∇θ log qθ(x<k)] in Eq. 8 can be regarded as a re-weighted gradient of qθ’s information entropy with respect to θ. This term can be optimized similarly to the teacher-forcing training of the autoregressive model with\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nthe “teacher” sequence generated autoregressively by the model itself. The scheduled sampling methods (Bengio et al., 2015; Ranzato et al., 2016; Mihaylova & Martins, 2019) are similar to this term but without the re-weighting factor.\n\nMoreover, the reweighing weight w of Eq. 9 can be further refined (see the derivation in Appendix B.3) and we can observe that w(x<k) = μ(x<k)/Ex′ μ(x<k), where μ(x<k) = pθ(x<k)/qθ(x<k), indicating the possibility of which distribution (pθ or qθ) the input context x<k is most likely to come from. Correspondingly, w(x<k) reflects the context x<k’s relative magnitude of μ(x<k) compared with the average among all potential contexts—the larger the value of w(x<k), the more likely the context x<k in the data space coming from pθ, which is modeled by the product of autoregressive models and EBMs. During training, those input sequences with contexts more likely from pθ than qθ will be assigned larger weights w while others will be assigned smaller weights w.\n\n<k\n\n4.3 FINAL OPTIMIZATION OF E-FORCING\n\nAlgorithm 1 Optimizing ARGMs with E-Forcing Given: a training dataset E ∼ pd , random-initialized autoregressive model qθ, K ∈ N is the\n\ngeneration length for iteration i = 1;i ≤max iterations;i + 1 do i=1 ∼ E\n\nSample minibatch B = {(ci, si)}n if i≤N then\n\n∇θLtotal ← (cid:80)K\n\nk=1 ∇θL(k)\n\nAR(B)\n\nelse\n\n▷ si is of length K, ci is the context of si ▷ After N iterations, we start applying E-Forcing\n\nAutoregressively generate |B| samples from qθ conditioned on ci, denoted as ̃B ∇θLtotal ← (cid:80)K\n\nAR(B) + λk∇θL(k)\n\nEBM −CD(B, ̃B)\n\nk=1 ∇θL(k)\n\nend if Update θ ← θ − η∇θLtotal\n\n▷ η denotes learning rate\n\nend for\n\nFinally, with the help of the above estimation of gradients regarding two phases of Eq. 6, we are able to optimize the product EBM pθ via descending the estimated gradient of contrastive divergence loss ∇θLEBM −CD for any time step k\n\n∇θL(k)\n\nEBM −CD(θ) = G(k)\n\n+ (θ) − G(k)\n\n− (θ).\n\n(10)\n\nEq. 10 can be easily estimated by using “positive” samples from the given training dataset and autoregressively generated “negative” samples from qθ.\n\nNevertheless, training the model from scratch with the energy-based learning objective alone can not work well in practice. The reason is simple: at the initial stage of the training process, what we have is just a randomly initialized network that can barely generate anything meaningful. This fact indicates disjoint supports between the real distribution pd and modeled distribution pθ. Importance sampling fails in this case. Hence, to make the optimization more feasible, we pre-train the entire model with an autoregressive loss LAR by teacher-forcing for a few epochs before introducing the energy-based learning objective. In sum, the final gradient concerning parameter θ at each update iteration is\n\n∇θLtotal(θ) =\n\nK (cid:88)\n\nk=1\n\n∇θL(k)\n\nAR(θ) + λk∇θL(k)\n\nEBM −CD(θ),\n\n(11)\n\nwhere λk adjusts the ratio between the two objectives for the time step k. The intact optimization procedure is shown in Algorithm 13. We found that an exponentially descending configuration of coefficients λk according to the order of time steps works well. One possible reason is that such a set of coefficients can remedy the imbalanced training signal by negative phase gradients in Eq. 8 among time steps.\n\n3We take K to be the length of a segment of the transformer.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n5 EXPERIMENTS\n\nTo empirically corroborate the effectiveness of E-Forcing and show its broad applicability, we have conducted extensive experiments on applications, such as language modeling and machine translation. In this section, we will first introduce these experimental setups and analyze the obtained results. Besides, we also carried out a series of experiments to further show our E-Forcing method’s ability to resolve ARGMs’ inherent flaws(i.e. exposure bias and incoherence generation). More experimental settings as well as analytical experiments are shown in Appendix A and C.\n\n5.1 APPLICATION TO LANGUAGE MODELING\n\nFor the language modeling task, three different datasets, WikiText-103 (Merity et al., 2017), Toronto Book Corpus (Zhu et al., 2015; Kiros et al., 2015), and CC-news (Mackenzie et al., 2020), are chosen as testbeds; two autoregressive network structures are used to evaluate the effectiveness: vanilla Transformer (Vaswani et al., 2017) (“Tr-Base” for short) and Transformer-XL (Dai et al., 2019) (“Tr-XL” for short). We regard the vanilla training with the teacher forcing technique as the baseline method. Besides, we also compared our E-Forcing with the residual EBM Deng et al. (2020) method, which is a typical method to improve the performance of language models by utilizing EBMs. In order to benefit from the EBM, the residual EBM method requires a new network to estimate the energy scores and imposes a Top-K energy resampling scheme during inference4.\n\nThe final results are reported in Table 1. We can see from the results that E-Forcing outperforms two pure autoregressive models with clear margins over all three benchmarks. Specifically, on the Wikitext-103 benchmark, our E-Forcing improves the performance of the Transformer-Base model and Transformer-XL model by 0.62 PPL points (from 30.56 to 29.94) and 0.30 PPL points (from 24.20 to 23.90) respectively; on CC-news and Toronto Book Corpus benchmarks, our method obtains 0.51 ppl and 0.47 ppl performance gain respectively and gets further improvement once energy resampling technique was applied. Besides, though residual EBM’s learning parameters are twice as ours and their method is unable to directly benefit autoregressive models without Top-K energy resampling, our E-Forcing achieves comparable results to them, even slightly better on Toronto Book Corpus and Wikitext-103 benchmarks.\n\nMethod\n\nCC-News\n\nToronto Book Corpus WikiText103\n\nPPL ↓\n\nBaseline (Tr-Base) Baseline (Tr-XL) Residual EBM (Tr-Base) Residual EBM (Tr-XL) E-Forcing (Tr-Base) E-Forcing (Tr-XL) E-Forcing + E.R. (Tr-Base) E-Forcing + E.R. (Tr-XL)\n\n18.29 -\n15.57-15.58 -\n15.78 -\n15.63-15.67 -\n\n17.57 -\n16.98-17.00 -\n17.10 -\n16.89-16.93 -\n\n30.56 24.20 29.88-29.93 23.85-23.87 29.94 23.90 29.81-29.84 23.79-23.82\n\nTable 1: Language modeling performance of different models on three benchmarks. Evaluation is conducted using perplexity (PPL). E.R. is the abbreviation of Energy Resampling technique (Bakhtin et al., 2021), which serves as a necessary module of Residual EBM.\n\n5.2 APPLICATION TO NEURAL MACHINE TRANSLATION\n\nWe further evaluate E-Forcing’s effectiveness on neural machine translation (NMT), which can be regarded as a conditional generation task. We mainly analyze E-Forcing on the IWSLT14 dataset, which includes six different language pairs ({German, Spanish, Italian} → English and English → {German, Spanish, Italian}) (Hereafter we abbreviate English, German, Spanish, Italian as “EN”,\n\n4It is worth noting that Top-K energy resampling can not get the PPL directly. Bakhtin et al. (2021) provides\n\na way to approximate PPL, which leads to an estimated interval of PPL.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nLabel Smoothing\n\nScheduled Sampling\n\nBeam Searching\n\nBase\n\nE-Forcing\n\n-\n\n✔\n\n✔\n\n-\n\n✔\n\n✔\n\n-\n\n-\n\n✔\n\n-\n\n-\n\n✔\n\n- 5 B -\n5 B -\n5 B -\n5 B -\n5 B -\n5 B\n\nBLEU ↑\n\nDE→ EN\n\nEN→ DE\n\nEN→ IT\n\nIT→ EN\n\nES→ EN\n\nEN→ ES\n\n32.44±0.06 33.62±0.07 33.68±0.03 34.61±0.08 34.23±0.06 35.10±0.04 32.99±0.10 34.06±0.06 33.97 ±0.08 34.93 ±0.05 34.58 ±0.09 35.36 ±0.05\n\n26.64±0.10 27.41±0.08 27.62±0.04 28.46±0.06 27.96±0.03 28.73±0.04 27.15±0.03 27.97±0.08 28.03 ±0.04 28.91 ±0.12 28.38 ±0.12 29.11 ±0.04\n\n27.92±0.03 28.72±0.04 28.81±0.07 29.72±0.10 29.26±0.11 29.97±0.07 28.33±0.12 29.26±0.09 29.13 ±0.02 30.04 ±0.11 29.56 ±0.10 30.25 ±0.09\n\n30.48±0.08 31.39±0.05 31.42±0.07 32.29±0.03 31.93±0.08 32.64±0.12 31.13±0.04 31.90 ±0.13 31.84 ±0.11 32.56 ±0.04 32.11 ±0.03 32.82 ±0.11\n\n38.61±0.11 39.55±0.12 39.85±0.13 40.64±0.07 40.16±0.03 40.91±0.06 39.56±0.01 40.30 ±0.03 40.32 ±0.03 41.01 ±0.06 40.93 ±0.03 41.58 ±0.07\n\n35.42±0.09 36.38±0.07 36.71±0.09 37.48±0.05 37.21±0.04 37.93±0.10 36.07±0.02 36.92 ±0.09 36.96 ±0.07 37.73 ±0.12 37.56 ±0.07 38.19 ±0.03\n\nAvg.\n\n31.92 32.85 33.02 33.87 33.46 34.21 32.54 33.40 33.38 34.20 33.85 34.55\n\nTable 2: Comparison of BLEU scores between our approach E-Forcing and the base ARGM trained just with cross-entropy loss on six translation pairs of IWSLT14 datasets. We use “-” to denote that the training trick is not used while “✔” indicates we use it. “5 B” represents we use beam searching with 5 beams.\n\n“DE”, “ES”, “IT”). In addition, we also reported the result of E-Forcing over the WMT16 (English → German) benchmark, which is a relatively larger dataset, in Table 3.\n\nResults concerning IWSLT14 are shown in Table 2, where we use a six-layer vanilla transformer as the base autoregressive model. We test not only the pure performance of E-Forcing but also the compatibility with other techniques. In detail, we can observe that (1) without any particular engineering, E-Forcing outperforms the vanilla training with cross-entropy singly(teacherforcing) by 0.62 (31.92 → 32.54) BLEU points on average, especially on three translation pairs— 38.61 → 39.56 on Spanish-to-English, 30.48 → 31.13 on Italian-to-English, 35.42 → 36.07 on English-to-Spanish. (2) E-Forcing is compatible with other techniques like scheduled sampling, which can help alleviate the exposure bias problem to some extent. They are not mutually exclusive and can work together to further improve the performance of the base ARGM. (3) However, since scheduled sampling can reduce exposure bias and beam search can somewhat alleviate the flaws caused by greedy selection at each time step, the performance gain of E-Forcing when all these tactics are combined is only 0.34 (34.21 → 34.55), which is lower than the 0.62 (31.92 → 32.54) obtained when the model is purely trained without these other techniques.\n\nAdditionally, Table 3 shows the performance of E-Forcing on the WMT16 English → German task. For two different model sizes, enabling label smoothing (L.S.) improves model performance by 0.52 and 0.35, respectively. The performance of the base transformer model further increases to 28.36 BLEU points when scheduled sampling (S.S.) is used, while the larger model improves to 29.23 points. E-Forcing paired with label smoothing and scheduled sampling yields the highest scores of 28.62 and 29.44, respectively. Overall, our training strategy outperforms ARGM’s vanilla teacher-forcing training and can have uniformly favorable impacts across different models and dataset sizes.\n\n5.3\n\nEFFECT ON THE EXPOSURE BIAS\n\nModel\n\nL.S.\n\nS.S. E-Forcing BLEU ↑\n\nTr-Base\n\nTr-Large\n\n- ✔\n✔ ✔\n\n- ✔\n✔ ✔\n\n- -\n✔ ✔\n\n- -\n✔ ✔\n\n- -\n- ✔\n\n- -\n- ✔\n\n27.56 28.04 28.36 28.62 28.70 29.05 29.23 29.44\n\nTable 3: Translation performance of proposed EForcing on WMT16 English→German, evaluated with BLEU. We uniformly use 5 beams when applying beam search. “L.S.” denotes Label Smoothing and “S.S.” denotes Scheduled Sampling.\n\nWe follow the analytic experiments in the work (Zhang et al., 2019) to show that our E-Forcing is capable of alleviating the exposure bias problem. To be concrete, we randomly select 1K pairs from the training data for each translation pair and use the trained autoregressive model which applied E-Forcing to decode the source sentences, and then count the ground truth words whose probabilities in the predicted distributions produced by our E-Forcing are greater than those produced by the baseline and denote the number as N . The ratio of N to the total number of words tested is calculated. The detailed results are shown in Table 4. We find that the results on all different tasks are greater\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nthan 50%, which demonstrates the ability of our E-Forcing in alleviating the exposure bias problem to some extent.\n\nTrans. Pairs DE→ EN EN→ DE EN→ IT IT→ EN ES→ EN EN→ ES\n\nN Total Ratio\n\n14203 22148 64.12%\n\n14554 23057 63.12%\n\n14976 23654 63.31%\n\n13952 23744 59.76%\n\n16021 23860 68.33%\n\n15359 22775 67.43%\n\nTable 4: The effect of E-Forcing on the exposure bias problem. Each test set of translation tasks contains 1K sentences selected randomly. N denotes the ground truth words whose probabilities in the predicted distributions produced by E-Forcing are greater than those produced by the baseline.\n\n5.4\n\nEFFECT ON THE INCOHERENCE OF GENERATION\n\nTranslation Task\n\nScheduled E-Forcing Training Sampling\n\nTarget Sentence Length [25, 49)\n\n[0, 25)\n\n[50, ∞)\n\nDe→ En\n\nIt→ En\n\nEs→ En\n\n- ✔\n✔\n\n- ✔\n✔\n\n- ✔\n✔\n\n- -\n✔\n\n- -\n✔\n\n- -\n✔\n\n37.72 ±0.04 38.20 ±0.07 38.37 ±0.06 35.20 ±0.03 35.52 ±0.09 35.56 ±0.10 43.37 ±0.05 43.61 ±0.09 43.84 ±0.10\n\n33.24 ±0.06 33.76 ±0.03 33.92 ±0.09 32.73 ±0.02 33.25 ±0.08 33.33 ±0.13 39.67 ±0.08 40.00 ±0.04 40.35 ±0.05\n\n30.86 ±0.07 31.08 ±0.06 31.43 ±0.04 26.86 ±0.05 26.95 ±0.14 27.21 ±0.07 37.14 ±0.06 37.38 ±0.06 38.07 ±0.04\n\nAll Test\n\n34.61 ±0.08 35.10 ±0.04 35.36 ±0.05 32.29 ±0.03 32.64 ±0.12 32.82 ±0.11 40.64 ±0.07 40.91 ±0.06 41.58 ±0.07\n\nTable 5: Performance comparison on the IWSLT14 test set for the different lengths of sentences on three translation tasks (German to English, Italian to English, and Spanish to English). Performance is evaluated by the BLEU score.\n\nWe also attempted to quantitatively validate that our E-Forcing can benefit ARGMs by improving the coherence of generation. Table 5 shows the BLEU scores of generated translations on the IWSLT14 test set with respect to different lengths of the source sentences. Intuitively, due to the cumulative effect of greedy selection at each time step, the collection of samples with longer sentences ought to be more plagued by the incoherence of generations problem. Our approach can outperform the vanilla training in all three length intervals, especially in the lengthy sentence interval [50, ∞], indicating that our E-Forcing can resolve the incoherence problem to a degree.\n\n6 CONCLUSIONS AND FUTURE WORK\n\nIn this paper, we propose a novel training method dubbed E-Forcing for ARGMs by treating them as EBMs. This is achieved by defining the energy function using the softmax operation’s extra degree of freedom within an autoregressive network. We further design a unique way to improve the training of E-Forcing using importance sampling. Experimental results demonstrate the effectiveness of E-Forcing to alleviate exposure bias and incoherence problems of ARGMs. In the future, we expect to extend E-Forcing on other sequential generation tasks (e.g. text summarization, audio generation) and incorporate the proposed methodology into other advanced autoregressive architectures.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMichael Arbel, Liang Zhou, and Arthur Gretton. Generalized energy based models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nShaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural\n\nInformation Processing Systems (NeurIPS), 2019a.\n\nShaojie Bai, J. Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019b.\n\nAnton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott, Marc’Aurelio Ranzato, and Arthur Szlam.\n\nResidual energy-based models for text. J. Mach. Learn. Res., 22:40:1–40:41, 2021.\n\nSatanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 65–72, Ann Arbor, Michigan, jun 2005.\n\nRichard Ernest Bellman. The Theory of Dynamic Programming. Santa Monica, CA, 1954.\n\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 1171–1179, 2015.\n\nYoshua Bengio, Gr ́egoire Mesnil, Yann N. Dauphin, and Salah Rifai. Better mixing via deep representations. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28, pp. 552–560.\n\nTong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator In Advances in Neural Information Processing Systems 33: Annual driven latent sampling. Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nXi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm ̈assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 863–871, 2018.\n\nJunyoung Chung, C ̧ aglar G ̈ulc ̧ehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\n\ngated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 2978–2988, 2019.\n\nYuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual energybased models for text generation. In 8th International Conference on Learning Representations, ICLR 2020, 2020.\n\nYilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 3603–3613, 2019.\n\nConor Durkan and Charlie Nash. Autoregressive energy machines. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, volume 97, pp. 1735–1744, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRuiqi Gao, Erik Nijkamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai, and Ying Nian Wu. Flow contrastive estimation of energy-based models. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 7515–7525, 2020.\n\nRuiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P. Kingma. Learning energybased models by diffusion recovery likelihood. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nWill Grathwohl, Kuan-Chieh Wang, J ̈orn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\n\nWill Sussman Grathwohl, Jacob Jin Kelly, Milad Hashemi, Mohammad Norouzi, Kevin Swersky, and David Duvenaud. No MCMC for me: Amortized sampling for fast and stable training of energy-based models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nMichael Gutmann and Aapo Hyv ̈arinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceedings, pp. 297–304, 2010.\n\nG. Hinton, P. Dayan, B. Frey, and R. Neal. The “wake-sleep” algorithm for unsupervised neural\n\nnetworks. Science, 268 5214:1158–61, 1995.\n\nGeoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural\n\nComput., 14(8):1771–1800, 2002.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nDaniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement from\n\na finite universe. Journal of the American statistical Association, 47(260):663–685, 1952.\n\nTaesup Kim and Yoshua Bengio. Deep directed generative models with energy-based probability\n\nestimation. CoRR, abs/1606.03439, 2016.\n\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 3294–3302, 2015.\n\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based\n\nlearning. Predicting structured data, 1(0), 2006.\n\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\n\nBranches Out, pp. 74–81, Barcelona, Spain, jul 2004.\n\nZhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 3698–3707, 2018.\n\nJoel M. Mackenzie, Rodger Benham, Matthias Petri, Johanne R. Trippas, J. Shane Culpepper, and Alistair Moffat. Cc-news-en: A large english news corpus. In CIKM ’20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, pp. 3077–3084, 2020.\n\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330, June 1993.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\n\nTsvetomila Mihaylova and Andr ́e F. T. Martins. Scheduled sampling for transformers. In Fernando Emilio Alva-Manchego, Eunsol Choi, and Daniel Khashabi (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28 - August 2, 2019, Volume 2, pp. 351–356, 2019.\n\nSubhajit Naskar, Amirmohammad Rooshenas, Simeng Sun, Mohit Iyyer, and Andrew McCallum. Energy-based reranking: Improving neural machine translation using energy-based models. CoRR, abs/2009.13267, 2020.\n\nErik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent nonpersistent short-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5233–5243, 2019.\n\nAaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and KoarXiv preprint\n\nray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv:1606.05328, 2016.\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.\n\nBo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space energy-based prior model. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nHieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4095–4104, 10–15 Jul 2018.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.\n\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\n\nKaitao Song, Xu Tan, and Jianfeng Lu. Neural machine translation with error correction.\n\nIn Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pp. 3891–3897, 2020.\n\nA ̈aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In The 9th ISCA Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15 September 2016, pp. 125, 2016a.\n\nA ̈aron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 4790–4798, 2016b.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA ̈aron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1747–1756, 2016c.\n\nAaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In\n\nInternational Conference on Machine Learning, pp. 1747–1756. PMLR, 2016.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017.\n\nYezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu, and Dongsheng Li. Energy-based\n\nopen-world uncertainty modeling for confidence calibration. CoRR, abs/2107.12628, 2021.\n\nMax Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 681–688, 2011.\n\nZhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. VAEBM: A symbiosis between variational autoencoders and energy-based models. In 9th International Conference on Learning Representations, ICLR 2021, 2021.\n\nJ. Xie, Z. Zheng, X. Fang, S. Zhu, and Y. Wu. Cooperative training of fast thinking initializer and slow thinking solver for conditional learning. IEEE Transactions on Pattern Analysis & Machine Intelligence, (01):1–1, mar 2019. ISSN 1939-3539.\n\nJianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, and Ying Nian Wu. Cooperative training of descriptor and generator networks. IEEE Trans. Pattern Anal. Mach. Intell., 42(1):27–45, 2020.\n\nMinkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative dynamics for molecular conformation generation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nWen Zhang, Yang Feng, Fandong Meng, Di You, and Qun Liu. Bridging the gap between training and inference for neural machine translation. In Anna Korhonen, David R. Traum, and Llu ́ıs M`arquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1, pp. 4334–4343, 2019.\n\nJunbo Jake Zhao, Micha ̈el Mathieu, and Yann LeCun. Energy-based generative adversarial networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\n\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 19–27, 2015.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA EXPERIMENTAL SETTINGS\n\nIn this section, we introduce detailed setups of different benchmarks as well as the information of corresponding datasets.\n\nA.1 DATASETS\n\nWe conducted our experiments based on 7 datasets over different learning tasks:\n\n1. WikiText-103 comprises 103 million training tokens from 28 thousand articles, with an\n\naverage length of 3.6 thousand tokens per article.\n\n2. Toronto Book Corpus consists of fiction books in 16 different genres, totaling about half a\n\nbillion words.\n\n3. CC-news is a de-duplicated subset of the English portion of the CommonCrawl news dataset,\n\nwhich totals around 16 Billion words.\n\n4. IWSLT14 contains about 170k training sentence pairs, 7k valid pairs, and 7k test pairs. It has six different domains of language, and each two of them can consist of a translation pair.\n\n5. WMT16 contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling.\n\n6. MNIST is a large collection of handwritten digits. It has a training set of 60,000 examples\n\nand a test set of 10,000 examples.\n\n7. CIFAR-10 is a subset of the Tiny Images dataset and consists of 60000 32x32 color images.\n\nThe images are labeled with one of 10 mutually exclusive classes.\n\nA.2\n\nIMPLEMENTING SETUPS\n\nHyper-Parameters\n\nNumber of Layers Hidden Embed Size FC-Layer Embed Size Attention Heads Dropout Learning Rate lr scheduler Warm up Updates Weigth Decay E-Forcing Start Epoch\n\nTranslation-IWSLT14 Tr-Base\n\nTranslation-WMT16 Tr-Base\n\nTr-Large\n\nLanguage Modeling Tr-XL\n\nTr-Base\n\n12 512 1024 4\n0.3 5e-4 inverse sqrt 4000 1e-4 15\n\n12 512 2048 8\n0.3 1e-3 inverse sqrt 4000 0.0 15\n\n12 1024 4096 16 0.3 1e-3 inverse sqrt 4000 0.0 10\n\n6 512 2048 8\n0.1 5e-4 inverse sqrt 4000 1e-2 15\n\n16 410 2100 10 0.1 2.5e-4 cosine 10000 0.0 10\n\nTable 6: Hyperparameters of different model structures and datasets. “Tr-Base”, “Tr-Large”, and “Tr-XL” indicate Transformer-Base, Transformer-Large, and Transformer-XL respectively\n\nWe uniformly use the Adam optimizer. The training will be stopped once the model has not obtained better performance for 20 epochs on the validation set. For translation tasks, the length of generated fake sentences, which is used for the computing of the negative phase in Eq. 10, is dependent on the source sequence whilst for language modeling tasks, we fix the length of generated fake sentences as 50 during training. The model structures for language modeling and machine translation tasks are shown in Table 6. As for the model structures of the image generation task, we use the official structure reported by PixelCNN (van den Oord et al., 2016c) and Gated PixelCNN (van den Oord et al., 2016b) without modification. The source code will be released once upon acceptance. We use the same batch of samples generated autoregressively to approximate both the expectations in Eq.10 and weight w (i.e., shared), which does not need to sample twice. The number of samples in a batch is dynamic while the maximum number of the total tokens in a batch is fixed (4096 in our experiments). If the length of sequences in a batch is 32, then it includes 4096 / 32 = 128 samples in total. It is a common strategy in language generation tasks and has been used in many frameworks(e.g.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFairseq (Ott et al., 2019)). We generate samples autoregressively as many as the number of sequences in the current batch at each update iteration.\n\nB DERIVATION OF THE NEGATIVE PHASE GRADIENT\n\nIn this section, we show the detailed derivation of Eq. 8. Formally, as shown in Sec. 3, given an autoregressive model qθ(x<k) = (cid:81)k−1 l=1 qθ(xl|x<l) (k denotes the time step) with parameters θ, we define a product of the autoregressive model and an EBM as follows\n\npθ(xk, x<k) = qθ(x<k) ·\n\ne−φθ(xk,x<k) Zθ\n\n,\n\n(12)\n\nwhere qθ(x<k) = (cid:81)k−1 Ex′ xk w.r.t θ of “negative phase” is\n\n<k∼qθ [(cid:80)\n\ne−φθ(xk,x′\n\nl=1 qθ(xl|x<l), Zθ\n\nto <k)]. The optimization of pθ(xk, x<k) includes two phases, and the gradient\n\nthe normalization term and equal\n\nis\n\n−Ex<k∼pθ [∇θ log qθ(x<k)] + Exk,x<k∼pθ [∇θφθ(xk, x<k)].\n\n(13)\n\nNext, we will show the specific derivation about how to transform Eq. 13 into Eq. 8.\n\nB.1 DERIVATION OF THE FIRST TERM\n\nThe first term Ex<k∼pθ [∇θ log qθ(x<k)] can be processed as follows\n\nEx<k∼pθ [∇θ log qθ(x<k)] =\n\n=\n\n(cid:88)\n\nx<k (cid:88)\n\npθ(x<k)∇θ log qθ(x<k)\n\n(cid:88)\n\npθ(xk, x<k)∇θ log qθ(x<k)\n\nx<k\n\nxk\n\n=\n\n(cid:88)\n\nx<k\n\nqθ(x<k)\n\n(cid:80)\n\nxk\n\ne−φθ(xk,x<k)\n\nZθ\n\n∇θ log qθ(x<k)\n\n=Ex<k∼qθ(x<k)[w(x<k)∇θ log qθ(x<k)],\n\n(14)\n\n(cid:80)\n\nxk\n\ne−φ(xk ,x<k )\n\ne−φθ (xk ,x′\n\n<k\n\n)]\n\nbecause\n\nwhere we have w(x<k) =\n\nw(x<k) =\n\nE\n\nx′\n\n<k\n\n∼qθ (x<k )[(cid:80)\n\nxk\n\n(cid:80)\n\nxk\n\ne−φ(xk,x<k)\n\n(cid:80)\n\nxk\n\ne−φ(xk,x<k)\n\n=\n\n(cid:80)\n\nx<k\n\n(cid:80)\n\nxk\n\nqθ(x<k)e−φθ(xk,x<k)\n\nZθ (cid:80)\n\ne−φ(xk,x<k)\n\n=\n\n(cid:80)\n\nx<k\n\nxk\n\nqθ(x<k) (cid:80)\n\n=\n\n(cid:80) Ex<k∼qθ(x<k)[(cid:80)\n\nxk\n\nxk\n\nxk e−φ(xk,x<k)\n\ne−φθ(xk,x<k)\n\ne−φθ(xk,x<k)]\n\n15\n\n(15)\n\n.\n\nUnder review as a conference paper at ICLR 2023\n\nB.2 DERIVATION OF THE SECOND TERM\n\nThen, we tackle the second term Exk,x<k∼pθ [∇θφθ(xk, x<k)] as follows\n\nEpθ\n\n(cid:2)∇θφθ(xk, x<k)(cid:3) =\n\n=\n\n=\n\n(cid:88)\n\nxk,x<k\n\n(cid:88)\n\nxk,x<k\n\n(cid:88)\n\nxk,x<k\n\npθ(xk, x<k)∇θφθ(xk, x<k)\n\npθ(xk, x<k)\n\nqθ(xk, x<k) qθ(xk, x<k)\n\n∇θφθ(xk, x<k)\n\nqθ(xk, x<k)\n\nqθ(x<k) · e−φθ(xk,x<k) Zθ · qθ(xk, x<k)\n\n∇θφθ(xk, x<k)\n\n= Exk,x<k∼qθ(xk,x<k)[\n\ne−φθ(xk,x<k) qθ(xk|x<k)\n\n·\n\n1 Zθ\n\n∇θφθ(xk, x<k)]\n\nqθ(x<k)\n\nqθ(x<k)\n\n(cid:88)\n\nxk\n\n(cid:88)\n\n=\n\n=\n\n(cid:88)\n\nx<k\n\n(cid:88)\n\nx<k\n\n= Eqθ(x<k)[\n\n(cid:88)\n\nxk\n\n= Eqθ(x<k)[\n\n(cid:88)\n\nxk (cid:88)\n\n= Eqθ(x<k)[\n\nxk\n\nqθ(xk|x<k)\n\ne−φθ(xk,x<k) qθ(xk|x<k)\n\n·\n\n1 Zθ\n\n∇θφθ(xk, x<k)\n\ne−φθ(xk,x<k) ·\n\n1 Zθ\n\n∇θφθ(xk, x<k)\n\nxk e−φθ(xk,x<k) Zθ\n\n∇θφθ(xk, x<k)]\n\ne−φθ(xk,x<k)\n\n(cid:80)\n\nxk\n\ne−φθ(xk,x<k)\n\n(cid:80)\n\nxk\n\n·\n\ne−φθ(xk,x<k)\n\nZθ\n\n∇θφθ(xk, x<k)]\n\nqθ(xk|x<k)w(x<k)∇θφθ(xk, x<k)]\n\n= Eqθ(x<k)[Ea∼qθ(xk|x<k)[w(x<k)∇θφθ(xk, x<k)]] = Exk,x<k∼qθ(xk,x<k)[w(x<k)∇θφθ(xk, x<k)]\n\n(16)\n\n(cid:80)\n\nxk\n\ne−φ(xk ,x<k )\n\nwhere w(x<k) is also equal to equivalent form of the gradient of the negative phase without any expectation over pθ as\n\n. Combining Eq. 14 and Eq. 16, we can obtain an\n\nZθ\n\n−Ex<k∼qθ(x<k)[w(x<k)∇θ log qθ(x<k)] + Exk,x<k∼qθ(xk,x<k)[w(x<k)∇θφθ(xk, x<k)],\n\n(17)\n\nwhere w(x<k) =\n\n(cid:80)\n\nxk\n\ne−φ(xk,x<k)\n\nEx′\n\n<k∼qθ(x<k)[(cid:80)\n\nxk\n\ne−φθ(xk,x′\n\n<k)]\n\n.\n\n(18)\n\nB.3 FURTHER REFINEMENT OF w\n\nThe reweighing weight w can be further deduced as\n\nw(x<k) =\n\n(cid:80)\n\nxk\n\ne−φ(xk,x<k)\n\nEx′\n\n<k∼qθ(x<k)[(cid:80)\n\nxk\n\ne−φθ(xk,x′\n\n<k)]\n\n=\n\n(cid:80)\n\npθ(xk,x<k) qθ(x<k)\n\nxk\n\nEx′\n\n<k∼qθ(x<k)[(cid:80)\n\nxk\n\npθ(xk,x<k) qθ(x<k)\n\n]\n\n(19)\n\npθ(x<k) qθ(x<k)\n\n=\n\n<k∼qθ(x<k)[ pθ(x<k) qθ(x<k) ] where μ(x<k) is defined as pθ(x<k) ̃qθ(x<k) .\n\nEx′\n\n=\n\nμ(x<k)\n\nEx′\n\n<k\n\nμ(x<k)\n\n,\n\nC MORE EXPERIMENTAL ANALYSIS\n\nC.1 ANALYSIS TO TOP-K ENERGY RE-SAMPLING\n\nTop-K energy re-sampling in the inference stage is introduced by Bakhtin et al. (2021), which collects many candidate sequences generated autoregressively in the inference stage and then re-samples\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTrans. Pairs DE→ EN EN→ DE EN→ IT IT→ EN ES→ EN EN→ ES\n\nk\n\n0 5\n10\n\n34.93 34.97 34.95\n\n28.91 28.92 28.95\n\n30.04 30.08 30.07\n\n32.56 32.60 32.59\n\n41.01 41.07 41.03\n\n37.73 37.71 37.75\n\nTable 7: The effect of Top-K correction in the inference stage. We tested BLEU scores of using different k on different translation pairs of IWSLT14 dataset.\n\nfrom them depending on their energy scores estimated by the network. To measure the effectiveness of the Top-K energy re-sampling towards our method, we conduct an ablation study on neural machine translation task by selecting different K = {0, 5, 10}. The results are shown in Table 7 and performances are evaluated by using the BLEU score. From Table 7, we observe that the benefits brought by Top-K sampling is minor (K={5, 10}), when compared with the model without Top-K sampling (K=0). This together with the results shown in Table 1 shows that our E-Forcing can considerably benefit the base autoregressive model even without the energy resampling technique.\n\nC.2 APPLICATION TO IMAGE GENERATION\n\nIn order to illustrate the effectiveness and generality of our method in processing different modality tasks, we further show the results of applying E-Forcing to image generation in this section. We apply E-Forcing to Pixel-CNN (Van Oord et al., 2016) and its variant Gated Pixel-CNN (Oord et al., 2016). Experiments are carried out on the MNIST and CIFAR-10 datasets.\n\nTable 8 summarizes the quantitative results measured by per-pixel negative log-likelihood (NLL). We can see that with the help of our E-Forcing, both the Pixel-CNN and the Gated Pixel-CNN can obtain improvements in all datasets (0.17 → 0.15 and 3.14 → 3.07 for Pixel-CNN on MNIST and CIFAR10 respectively and 0.14 → 0.12 and 3.03 → 2.97 for Gated Pixel-CNN on MNIST and CIFAR10 respectively). This is further evidence in favor of the energy-based learning objective for improving autoregressive models.\n\nModel\n\nPixel-CNN Pixel-CNN (w/E-Forcing) Gated Pixel-CNN Gated Pixel-CNN (w/E-Forcing)\n\nTest (Train) NLL ↓ MNIST\n\nCIFAR-10\n\n0.17 (0.13) 0.15 (0.12) 0.14 (0.11) 0.12 (0.10)\n\n3.14 (3.08) 3.07 (2.98) 3.03 (2.90) 2.97 (2.87)\n\nTable 8: Performance of E-Forcing with different base networks on MNIST and CIFAR-10 in bits/dim (lower is better), training performance in brackets.\n\nC.3 THE EFFECT OF DIFFERENT START EPOCHS OF E-FORCING\n\nModel Structure Tr-Base Tr-XL\n\nIn addition, we have studied the effect of different start epochs of E-Forcing on the performance of language modeling, which can be seen in Table 9. From this, we may deduce that starting E-Forcing training at the 15th and 10th epoch yields the best results for Transformer-Base and TransformerXL respectively, whereas starting earlier or later yields a performance decline. It is reasonable because, if EForcing was introduced too early, the autoregressive model may not have been optimized well at that moment. As a result, the quality of generation for the “negative phase” would be terrible, making energy-based training unstable. On the other hand, the underlying autoregressive model can be modified only marginally if E-Forcing was introduced when the ARGM training is virtually complete.\n\nTable 9: Exploring the effect of different start epochs of E-Forcing on Wikitext103 benchmark. Performances are evaluated by perplexity (PPL).\n\nStart Epoch of E-Forcing 15 29.94 23.96\n\n20 30.05 24.05\n\n25 30.29 24.16\n\n5 30.38 24.12\n\n10 30.12 23.90\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (a) Cross entropy loss curves on IWSLT14 Spanish to English translation task on training set. The blue and orange colors represent base model and E-Forcing respectively; (b) Cross entropy loss curves on IWSLT14 Spanish → English translation task on test set.\n\nC.4 ANALYSIS TO MODEL’S CONVERGENCE\n\nIn this section, we will investigate the convergence of our E-Forcing. To begin, we first train a base Transformer model (“Tr-Base” architecture shown in Table 6) on the IWSLT14 Spanish to English training set for baseline and E-Forcing method respectively, and then record the training loss and test loss (in cross-entropy) at the end of each epoch. The loss curves are plotted in Figure 1. From Figure 1, we can see that (1) at the start of the training, our E-Forcing converges slightly faster than the baseline. (2) As the training process progresses, the cross entropy of the baseline on the training set will gradually decrease, at a faster rate than E-Forcing. On the other hand, the test loss curve of the baseline will fall initially and then slowly rise after 50 epochs while E-Forcing always remains stable convergence. This phenomenon also shows that our E-Forcing method can effectively prevent over-fitting so that obtaining better generalization.\n\nC.5 ABLATION STUDY WITH DIFFERENT ARCHITECTURE CHOICES\n\nTraining Methods GRU LSTM ENAS DEQ Tr-XL TNet\n\nTeacher-Forcing E-Forcing\n\n92.48 90.12\n\n78.93 76.97\n\n58.60 56.89\n\n57.10 55.55\n\n54.55 53.49\n\n54.19 53.24\n\nTable 10: The ablation study of E-forcing over different choices of the architecture of AR models with the comparison of vanilla teacher-forcing training. We tested PPL scores using different AR models on the Penn Treebank dataset\n\nIn this section, we conducted an ablation study to investigate our E-Forcing model’s generalization ability over different sequential models. We tested over 6 different sequential models, which are GRU (Chung et al., 2014), LSTM (Hochreiter & Schmidhuber, 1997), ENAS (Pham et al., 2018), TrelisNet(TNET for short) (Bai et al., 2019b), DEQ (Bai et al., 2019a) and Transformer-XL (Dai et al., 2019) on Penn Treebank (Marcus et al., 1993) dataset, which is a relatively small dataset and widely used in machine learning for NLP (Natural Language Processing) research. In general, we can observe that our E-Forcing can achieve improvement over all base AR models applied, which indicates it is a universally applicable training method for AR models.\n\n18\n\n(a)(b)Under review as a conference paper at ICLR 2023\n\nC.6 CASES STUDIES\n\nTo better understand the advantages of our method in correcting error tokens, we also prepare some translation cases in IWSLT14 German → English, as shown in Table 11.\n\nSource Sentence(German)\n\nPredicted Target Sentence(English)\n\nwenn ich ihnen 600 zeitschriften zeige und sie in 10 kategorien aufteile oder ich ihnen 400 zeitschriften zeige, und diese in 20 kategorien aufteile, dann glauben sie, dass ich ihnen mehr auswahl und eine bessere auswahlerfahrung gegeben habe, als ich ihnen die 400 gegeben h ̈atte gegen ̈uber dem, wenn ich ihnen die 600 gegeben h ̈atte.\n\nund ich weiß definitiv, dass es f ̈ur mich – in meiner situation – sehr gef ̈ahrlich w ̈are, anzufangen, diesen dunklen pfad der vermutung sozusagen herunterzusickern – besonders in dem umstand, in dem ich mich in meiner karriere gerade befinde.\n\nwir haben das licht ausgeschaltet, legten es in ein vakuum und saugten die ganze luft aus und k ̈uhlten es bis fast zum jetzt, ganz alleine im aufzug, war das st ̈uck metall frei, sich zu verhalten wie immer es wollte.\n\nund im grunde k ̈onnen sie das betrachten, wissen sie, als eine tyrannei des erinnernden selbst, und sie k ̈onnen sich das erinnernde selbst denken als eins, das sozusagen das erlebende selbst schleppt durch erfahrungen, die das erlebende selbst nicht braucht.\n\nwir sind an der schwelle zu erstaunlichen, erstaunlichen ereignissen auf vielen gebieten. und doch denke ich wirklich, dass wir hunderte, 300 jahre vor die aufkl ̈arung zur ̈uck gehen m ̈ussten, um eine zeit zu finden, in der wir fortschritt bek ̈ampft haben, in der wir ̈uber diese dinge heftiger getritten haben, an mehr fronten als jetzt.\n\nGroundTruth: if i show you 600 magazines and i divide them up into 10 categories, versus i show you 400 magazines and divide them up into 20 categories, you believe that i have given you more choice and a better choosing experience if i gave you the 400 than if i gave you the 600.\n\nBaseline: if i show you 600 magazines and i split them in 10 categories, or i’m showing them 400 magazines, and i’m going to split them up into 20 categories, you think i’ve given them more choices and better choice than i would have given them the 400 over the time that i gave them the 600. Baseline + S.S.: if i show you 600 magazines and i give you 400 magazines in 10 categories, and i give you 400 magazines, and i can split them up in 20 categories, then you think i’ve given you more choice and a better selection than i would have given you the 400 of which if i gave you the 600. Ours: if i show you 600 magazines and i divide them into 10 categories, or i show you 400 magazines, and i divide them into 20 categories, you think i’ve given you more choices and better selection experience than i gave you the 400 of whom if i gave you the 600. GroundTruth: and i definitely know that, in my case – in my situation – it would be very dangerous for me to start sort of leaking down that dark path of assumption, particularly given the circumstance that i’m in right now in my career. Baseline: and i know definitely, for me, it would be very dangerous to begin to do this dark path of suspect – especially in the circumstance that i’m in my career right now. Baseline + S.S.: and i know definitely it would be – in my situation – very dangerous to start, to kind of settle down this dark path of presumption – especially in the circumstance in which i’m in my career right now. Ours: and i definitely know that it’s for me – in my situation – very dangerous to start to sickle down this dark path of suspection, in particular, in the circumstance of where i’m in my career right now. GroundTruth: we turned off the lights, and then we put it in a vacuum and sucked out all the air, and then we cooled it down now, all alone in the elevator, the little chunk of metal is free to act however it wanted. Baseline: we turned the light off, put it in a vacuum and sucked it out all the air and cooled it up until almost now, all the way alone, the piece of metal was open to behave as it was. Baseline + S.S.: we turned the lights off, we put it into a vacuum, and we sucked all the air, and we cooled it all the way up to now, all over the place, the piece of metal was free to behave whatever it wanted. Ours: we turned off the lights, we put it into a vacuum and we sucked all the air out, and we cooled it up until almost now, all alone in the elevator, the piece of metal was free to behave whatever it wanted. GroundTruth: and basically you can look at this, you know, as a tyranny of the remembering self, and you can think of the remembering self sort of dragging the experiencing self through experiences that the experiencing self doesn’t need. Baseline: and basically, you can think of this, you know, as a tyranny of self, and you can think of the memorable self as one that kind of weaves the living self through experiences that don’t need the life itself. Baseline + S.S.: and basically, you can look at this, you know, as a tyrannei of memorial self, and you can think of the memorial self as one that kind of sucks the living self through experiences that don’t need the living self. Ours: and basically, you can look at that, you know, as a tyranny of the remembering self, and you can think of the memory itself as one, which is sort of dragging the living self through experiences that the living self doesn’t need. GroundTruth: we’re on the verge of amazing, amazing events in many fields, and yet i actually think we’d have to go back hundreds, 300 years, before the enlightenment, to find a time when we battled progress, when we fought about these things more vigorously, on more fronts, than we do now.\n\nBaseline: we are at the threshold of amazing, amazing events in many areas, and yet i really think that we have to go back hundreds and 300 years before the enlightenment to find a time when we have fought progress in which we have driven more of these things than now. Baseline + S.S.: we’re at the threshold of amazing, amazing events in many areas. and yet, i really think that we have to go back hundreds and hundreds of years before the enlightenment to find a time when we have struggled with progress in which we have driven on these things more powerful, more fronts than now. Ours: we’re at the threshold to amazing, amazing events in many areas, and yet i really think that we have to go back hundreds and 300 years before the enlightenment to find a time when we fought progress, where we’ve been fighting about these things to more fronts than we have now.\n\nTable 11: Translation cases on IWSLT14 De→En test set, generated by the baseline method, baseline with scheduled sampling and our E-Forcing. The italic font means the mismatch translation\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nC.7 EVALUATION WITH OTHER METRICS\n\nTrans. Pairs\n\nDe → En\n\nIt → En\n\nEs → En\n\nScheduled E-Forcing\n\nMetrics\n\nSampling\n\nTraining\n\nROUGE-1 ↑ ROUGE-2↑ ROUGE-L↑ METEOR↑ BLEU↑\n\n- ✔\n\n✔\n\n- ✔\n\n✔\n\n- ✔\n\n✔\n\n-\n\n- ✔\n\n-\n\n- ✔\n\n-\n\n- ✔\n\n66.51\n\n66.83\n\n67.46\n\n64.50\n\n64.73\n\n65.27\n\n71.10\n\n71.36\n\n71.91\n\n43.69\n\n44.08\n\n44.77\n\n40.65\n\n40.97\n\n41.51\n\n49.47\n\n49.53\n\n50.17\n\n63.69\n\n64.02\n\n64.78\n\n61.69\n\n61.94\n\n62.49\n\n68.78\n\n68.96\n\n69.65\n\n64.35\n\n64.61\n\n65.13\n\n62.18\n\n62.51\n\n62.80\n\n68.94\n\n69.28\n\n69.63\n\n34.61\n\n35.10\n\n35.36\n\n32.29\n\n32.64\n\n32.82\n\n40.64\n\n40.91\n\n41.58\n\nTable 12: Comparison of ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and BLEU scores between our approach E-Forcing and the base ARGM trained just with cross-entropy loss on three translation pairs of IWSLT14 datasets. The value is expressed in percentage. We use “Tr-Base” as the network architecture.\n\nTo further evaluate the effectiveness of our proposed E-Forcing, we also evaluate our method by using other metrics, such as ROUGE Lin (2004) and METEOR Banerjee & Lavie (2005) for neural machine translation. The results are shown in Table 12. In Table 12, the improvements of E-Forcing in different metrics is consistent with the conclusion of Table 2, which further prove the effectiveness of our E-Forcing method.\n\n20",
    "reference": "# Summary Of The Paper\n\nAR models are very common in many domains, especially language models. Maximum likelihood is the widely used approach for training these models. However, ML training of AR models causes some other issues such as exposure bias. Techniques such as scheduled sampling have been introduced before to address these issues. \nThis paper introduces a new training algorithm for AR models by viewing it as an energy-based model.  \nHere the authors define the model probability as $p(x_1, \\cdots, x_k) = q(x_{<k}) \\frac{e^{-Q(x_k, x_{<k})}}{Z_\\theta}$. They defined $Q(x_k, x_{<k})$ using the logits before the softmax of the $k$th token, so $p(x_1, \\cdots, x_k) = q(x_i, \\cdots, x_k)$ (am I right?). \nHowever instead of ML training which normalize $e^{-Q(x_k, x<k)}$ with $Z_\\theta$, they are training $p(x_i, \\cdots, x_k)$ using contrastive divergence (CD). This formulation helps to better regulate the distribution on the $k$th softmax since CD training contrast $Q(x_k, x_{<k})$ with a sample from the model (using importance sampling given the introduced form for the energy) rather than the log partition function. \nThe final loss function is a combination of cross entropy and CD.\n\nThe authors study the proposed method for different tasks of language modeling, machine translation, as well as image generation.\n\n# Strength And Weaknesses\n\nStrength:\n-The paper is well-written and easy to follow.\n-The idea of viewing AR models as EBMs is not novel but the provided formulation is neat and interesting. \n-I like the extensive study of the proposed approach.\n\nWeakness:\nThe improvement especially in the translation tasks is not considerable. Other training algorithms on top of ML training of AR can achieve similar improvement, for example, combining it with RL training. \nBhattacharyya et al. (2021) reported a significant improvement over base AR-NMT by using an external EBM defined over the whole sentence (although some of the improvements come from using pre-trained language models).  \nAlso, their training algorithm can directly be applied to AR by viewing E as \\sum_i E_i. \n\nQuestions:\n1) The author claims this approach can help with long-range dependencies, but I am not seeing how defining the energy model using the last softmax can help with that. Is that the effect of weights on training log q(x<k)? Even so, it is not similar to defining an explicit energy model over the whole sentence! \n\nBhattacharyya S, Rooshenas A, Naskar S, Sun S, Iyyer M, McCallum A. Energy-based reranking: Improving neural machine translation using energy-based models. ACL 2021.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear and easy to follow. The proposed method is novel although some parts such as using importance sampling for estimating partition function have been introduced before for similar joint distribution form: p(x) = q(x) exp (-E(x)).\n\n# Summary Of The Review\n\nThe paper is interesting but the experimental results do not show considerable benefits for the approach. I don't think that the proposed training algorithm would be adopted by the community for training AR models.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nQ-LEARNING DECISION TRANSFORMER: LEVERAGING DYNAMIC PROGRAMMING FOR CONDITIONAL SEQUENCE MODELLING IN OFFLINE RL\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecent works have shown that tackling offline reinforcement learning (RL) with a conditional policy produces promising results. The Decision Transformer (DT) combines the conditional policy approach and a transformer architecture, showing competitive performance against several benchmarks. However, DT lacks stitching ability – one of the critical abilities for offline RL to learn the optimal policy from sub-optimal trajectories. This issue becomes particularly significant when the offline dataset only contains sub-optimal trajectories. On the other hand, the conventional RL approaches based on Dynamic Programming (such as Qlearning) do not have the same limitation; however, they suffer from unstable learning behaviours, especially when they rely on function approximation in an off-policy learning setting. In this paper, we propose the Q-learning Decision Transformer (QDT) to address the shortcomings of DT by leveraging the benefits of Dynamic Programming (Q-learning). It utilises the Dynamic Programming results to relabel the return-to-go in the training data to then train the DT with the relabelled data. Our approach efficiently exploits the benefits of these two approaches and compensates for each other’s shortcomings to achieve better performance. We empirically show these in both simple toy environments and the more complex D4RL benchmark, showing competitive performance gains.\n\n1\n\nINTRODUCTION\n\nThe transformer architecture employs a self-attention mechanism to extract relevant information from high-dimensional data. It achieves state-of-the-art performance in a variety of applications, including natural language processing (NLP) (Vaswani et al., 2017; Radford et al., 2018; Devlin et al., 2018) or computer vision (Ramesh et al., 2021). Its translation to the RL domain, the Decision transformer (DT) (Chen et al., 2021), successfully applies the transformer architecture to offline reinforcement learning tasks with good performance when shifting their focus on the sequential modelling. It employs a goal conditioned policy which converts offline RL into a supervised learning task, and it avoids the stability issues related to bootstrapping for the long term credit assignment (Srivastava et al., 2019; Kumar et al., 2019b; Ghosh et al., 2019). More specifically, DT considers a sum of the future rewards – return-to-go (RTG), as the goal and learns a policy conditioned on the RTG and the state. It is categorised as a reward conditioning approach.\n\nAlthough DT shows very competitive performance in the offline reinforcement learning (RL) tasks, it fails to achieve one of the desired properties of offline RL agents, stitching. This property is an ability to combine parts of sub-optimal trajectories and produce an optimal one (Fu et al., 2020). Below, we show a simple example of how DT (reward conditioning approaches) would fail to find the optimal path.\n\nTo demonstrate the limitation of the reward conditioning approaches (DT), consider a task to find the shortest path from the left-most state to the rightmost state without going down to the fail state in Fig. 1. We set the reward as −1 at every time step and −10 for the action going down to the fail state. The training data covers the optimal path, but none of the training data trajectories has the entire optimal path. The agent needs to combine these two trajectories and come up with the optimal path. The reward conditioning approach essentially finds a trajectory from the training data\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: A simple example demonstrates the decision transformer approach’s issue (lack of stitching ability) – fails to find the shortest path to the goal. In contrast, Q-learning finds the shortest path. The numbers on the arrows are rewards on the path and the numbers on the states are RTGs.\n\nFigure 2: Evaluation results for conservative Q-learning (CQL), Decision Transformer (DT) and Qlearning Decision Transformer (QDT). The left two plots (simple and maze2d environments) show that the DT does not perform as it fails to stitch trajectories, and the right plot shows that CQL fails to learn from a sparse reward scenario (delayed reward). In contrast, QDT achieves consistently good results across all the environments.\n\nthat gives the ideal reward and takes the same action as the trajectory. In this simple example, trajectory 2 has a meagre reward. Hence, it always follows the path of trajectory 1 despite trajectory 2 giving the optimal path for the first action.\n\nIn contrast to the reward conditioning approaches (DT), Q-learning1 does not suffer from the issue and finds the optimal path quickly in this simple example. Q-learning takes each time step separately and propagates the best future rewards backwards. Hence it can learn from the first optimal action from trajectory 2. However, Q-learning has some issues on a long time horizon and sparse reward scenarios. It attempts propagating the value function backwards to its initial state, often struggling to learn across long time horizons and sparse reward tasks. This is especially true when Q-learning uses function approximation in an off-policy setting as discussed in Section 11.3 in (Sutton & Barto, 1998).\n\nHere, we devise a method to address the issues above by leveraging Q-learning to improve DT. Our approach differs from other offline RL algorithms that often propose a new single architecture of the agent and achieves better performance. We propose a framework that improves the quality of the offline dataset and obtains better performance from the existing offline RL algorithms. Our approach exploits the Q-learning estimates to relabel the RTG in the training data for the DT agent. The motivation for this comes from the fact that Q-learning learns RTG value for the optimal policy. This suggests that relabelling the RTG in the training data with the learned RTG should resolve the DT stitching issue. However, Q-learning also struggles in situations where the states require a large time step backward propagation. In these cases, we argue that DT will help as it estimates the sequence of states and actions without backward propagation. Our proposal (QDT) exploits the strengths of each of the two different approaches to compensate for other’s weaknesses and achieve a more robust performance. Our main evaluation results are summarised in Fig. 2. The left two\n\n1In this paper, we will use the Q-learning and Dynamic Programming interchangeably to indicate any RL\n\nalgorithm relying on the Bellman-backup operation.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nplots (simple and maze2d environments) show that DT does not perform well as it fails to stitch trajectories, while the right plot illustrates that CQL (Q-learning algorithm for offline reinforcement learning) fails to learn in a sparse reward scenario (delayed reward). These results indicate that neither of these approaches works well for all environments, and we might have abysmal results by selecting the wrong type of algorithms. In contrast, QDT performs consistently well across all environments and shows robustness against different environments. Through our evaluations, we also find that some of the evaluation results in the prior works may not be directly comparable, and it causes some contradicting conclusions. We touch on the issue in Section 6.\n\n2 PRELIMINARIES\n\nOffline Reinforcement Learning. The goal of RL is to learn a policy that maximises the expected sum of rewards in a Markov decision process (MDP), which is a four-tuple (S, A, p, r) where S is a set of states, A is a set of actions, p is the state transition probabilities, and r is a reward function.\n\nIn the online or on-policy RL settings, an agent has access to the target environment and collects a new set of trajectories every time it updates its policy. The trajectory consists of {st, at, rt}T t=0 where st, at and rt are the state, action and reward at time t respectively, and T is the episode time horizon.\n\nIn off-policy RL case, the agent also has access to the environment to collect trajectories, but it can update its policy with the trajectories collected with other policies. Hence, it improves its sample efficiency as it can still make use of past trajectories.\n\nOffline RL goes one step further than off-policy RL. It learns its policy purely from a static dataset that is previously collected with an unknown behaviour policy (or policies). This paradigm can be precious in case of the interaction with the environment being expensive or high risk (e.g., safety critical applications).\n\nDecision Transformers. DT architecture (Chen et al., 2021) casts the RL problem as conditional sequence modelling. Unlike the majority of prior RL approaches that estimates value functions or compute policy gradients, DT outputs desired future actions from the target sum of future rewards RTGs, past states and actions.\n\nτ = (Rt−K+1, st−K+1, at−K+1, · · · , Rt−1, st−1, at−1, Rt, st) .\n\n(1)\n\nEquation 1 shows the input of a DT, where K is the context length, R is RTGs (Rt = (cid:80)T is states and a is actions. Then DT outputs the next action (at).\n\nt′=t rt′), s\n\nDT employs Transformer architecture (Vaswani et al., 2017), which consists of stacked self-attention layers with residual connections. It has been shown that the Transformer architecture successfully relates scattered information in long input sequences and produces accurate outputs (Vaswani et al., 2017; Radford et al., 2018; Devlin et al., 2018; Ramesh et al., 2021).\n\nConservative Q learning. In this work, we use the conservative Q learning (CQL) framework (Kumar et al., 2020) for the Q-learning algorithm. CQL is an offline RL framework that learns Qfunctions that are lower-bounds of the true values. It augments the standard Bellman error objective with a regulariser which reduces the value function for the out-of-distribution state-action pair while maintaining ones for state-action pairs in the distribution of the training dataset. In practice, it uses the following iterative update equation to learn the Q-function under a learning policy μ(a|s).\n\nˆQk+1 ← arg min\n\nQ\n\nα (cid:0)Es∼D,a∼μ(a|s)[Q(s, a)] − Es,a∼D[Q(s, a)](cid:1)\n\n+\n\n1 2\n\nE\n\ns, a, s′ ∼ D a′ ∼ μ(a′|s′)\n\n(cid:20)(cid:16)\n\nr(s, a) + γ ˆQk(s′, a′) − Q(s, a)\n\n(cid:17)2(cid:21)\n\n,\n\n(2)\n\nwhere D is the training dataset and γ is a discount factor. Kumar et al. (2020) showed that while the resulting Q-function, ˆQμ := limk→∞ ˆQk may not be a point-wise lower-bound, it is a lower bound of V (s), i.e. Eμ(a|s)[ ˆQμ(s, a)] ≤ V μ(s).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 METHOD\n\nWe propose a method that leverages Dynamic Programming approach (Q-learning) to compensate for the shortcomings of the reward conditioning approach (DT) and build a robust algorithm for the offline RL setting. Our proposal consists of three steps. First, the value function is learned with Q-learning. Second, the offline RL dataset is refined by relabelling the RTG values with the result of Q-learning. Finally, the DT is trained with the relabelled dataset. The first and third steps do not require any modifications of the existing algorithms.\n\nThe reward conditioning approach (DT) takes an entire trajectory sequence and conditions on it using the sum of the rewards for that given sequence. Such an approach struggles on tasks requiring stitching (Fu et al., 2020) – the ability to learn an optimal policy from sub-optimal trajectories by combining them. In contrast, the Q-learning alternative propagates the value function backwards for each time step separately with the Bellman backup, and pools the information for each state across trajectories. It therefore does not have the same issue. Our proposal tackles the stitching issue of the reward conditioning approach by relabelling the RTG values with the learned Q-functions. With the relabelled dataset, the reward conditioning approach (DT) can then utilize optimal sub-trajectories from their respective sub-optimal trajectories.\n\nWe now discuss how to relabel the RTGs values with the learned Q-functions. Replacing all of the RTGs values with Q-functions is not adequate because not all the learned Q-functions are accurate, especially in a long time horizon and sparse reward case. Ideally, we would like to replace the RTGs values where the learned Q-functions are accurate. In this work, we employ the CQL framework for the offline Q-learning algorithm, which learns the lower bound of the value function. We replace the RTGs values when the RTG in the trajectory is lower than the lower bound. With this approach, our method substitutes the RTGs values where the learned value function is indeed accurate (or closer to the true values). We also replace all RTG values prior to the replaced RTG along with the trajectory by using reward recursion (Rt−1 = rt−1 + Rt). This propagates the replaced RTG values to all the time steps prior to the replaced point. To apply this idea, we initialise the last state RTG to zero (RT = 0), and then we start the following process from the end of the trajectory to the initial state backwards in time. First, the state value is computed for the current state with the learned value function ˆV (st) = Ea∼π(a|st)[ ˆQ(st, a)], where the π is the learned policy. Next, the value function is compared ( ˆV (st)) against the RTG value for the current state (Rt). If the value function is greater than that of the RTG, the RTG for the previous time step is set from (Rt−1) to rt−1 + ˆV (st), otherwise it is set to rt−1 + Rt. We repeat this process until the initial state is reached. This process is summarised in Algorithm 1.\n\nThe above relabelling process might introduce inconsistencies between the reward and RTG within the DT input sequence (Eq. 1). The RTG value is sum of the future rewards, hence it must always be Rt = rt + Rt+1. However, the relabelling process might break this relationship. To maintain this consistency within the input sequence of DT, we regenerate the RTG for the input sequence ({ ˆRt−K+1, · · · , ˆRt−1, ˆRt}) by copying the last RTG ( ˆRt = Rt) and then repeatedly apply ˆRt′ = t + ˆRt′+1 backwards until t′ = t − K + 1. We repeat this for each the input sequences to maintain r′ the consistency of the rewards and RTGs. This process is summarised in Algorithm 2.\n\nAlgorithm 1 Relabelling return-to-go\n\nAlgorithm 2 Generating return-to-go for DT\n\nInput\n\nInput\n\nrewards learned value function time horizon (trajectory length)\n\nreturn to go for time t context length\n\nrelabelled return to go\n\nrelabelled return to go for DT\n\nr1:T ˆV (s) T\nOutput R1:T RT ← 0 τ ← T while τ > 0 do\n\nend while\n\nRτ −1 ← rτ −1 + max(Rτ , ˆV (sτ )) τ ← τ − 1\n\nrt−K+1:t rewards Rt K\nOutput ˆR1:T ˆRt ← Rt τ ← t − 1 while τ > t − K do ˆRτ ← rτ + ˆRτ +1 τ ← τ − 1\n\nend while\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTheoretical considerations of QDT. Q-learning Decision Transformer (QDT) relies on DT as the agent algorithm, which can be seen as a reward conditioning model. A reward conditioning model takes the states and RTG as inputs and outputs actions. If we assume the model is trained with the state st and the optimal state-action value function (Q∗(st, at)), then we can guarantee that the model will output the optimal action (arg maxa Q∗(st, a)) for as long as it is given st and maxa Q∗(st, a) as inputs (Srivastava et al., 2019). In practice, we do not know the optimal value function Q∗(s, a), hence DT (and similarly other reward conditioning approaches) uses RTG instead. RTG is collected through the behaviour policy (or policies) and often is not optimal – with the majority of values being much lower than the corresponding optimal value function (Q∗(s, a)). As QDT uses CQL to learn the optimal conservative value function, Th. 3.2 in Kumar et al. (2020) shows that the conservative value function is a lower bound of the true value function. Hence the QDT relabelling process moves the RTG in the training dataset closer to the optimal value function (see Appendix D).\n\n4 RELATED WORK\n\nOffline reinforcement learning. The offline RL learns its policy purely from a static dataset that was previously collected with an unknown behaviour policy (or policies). As the learned policy might differ from the behaviour policy, the offline algorithms must mitigate the effect of the distributional shift (Agarwal et al., 2020; Prudencio et al., 2022). One of the most straightforward approaches to address the issue is by constraining the learned policy to the behaviour policy (Fujimoto et al., 2019; Kumar et al., 2019a; Wu et al., 2019). Other methods constrain the learned policy by making conservative estimates of future rewards (Kumar et al., 2020; Yu et al., 2021). Some model-based methods estimate the model’s uncertainty and penalize the actions whose consequences are highly uncertain (Janner et al., 2019; Kidambi et al., 2020). Some approaches address the distributional shift without restricting the learned policy. One such approach group is weighted imitation learning (Wang et al., 2018; Peng et al., 2019; Wang et al., 2020; Nair et al., 2020; Chen et al., 2020; Siegel et al., 2020; Brandfonbrener et al., 2021), which carries out imitation learning by putting higher weights on the good state-action pairs. It usually uses an estimated advantage function as the weight. As this approach imitates the selected parts of the behaviour policy, and it naturally restricts the learned policy within the behaviour policy. The other group of the approaches without restricting the learning policy is conditional sequence modelling, which learns a policy conditioned with a particular metric for the future trajectories. Some examples of the metrics are sum of the future rewards (Srivastava et al., 2019; Chen et al., 2021), a certain state (sub goal) (Codevilla et al., 2018; Ghosh et al., 2019; Lynch et al., 2020) and even learned features from the future trajectory (Furuta et al., 2021).\n\nOur approach does not belong to any of these groups but is related to the approach of learning pessimistic value function, the conditional sequence modelling and weighted imitation learning approaches. Essentially, our method is a conditional sequence modelling approach as it learns the following action conditioned on the current state and the sum of the future rewards, but the training data is augmented by the result of the learned pessimistic value function. Also, the overall high-level structure is somewhat similar to the weighted imitation learning, which learns the value function and uses it to weight the training data in the following imitation learning stage. However, each component is very different from ours, and it uses the value function to weight the training data, whereas our approach relabels the RTG values by tracing back the trajectory with the learned value function as well as the trajectory itself where the learned value function is not reliable. Also, in our approach, the policy is learned with conditional sequence modelling, whereas they use non-conditional nonsequential models. We believe we can apply our relabelling approach to the weighted imitation learning algorithms, and it is an exciting avenue for future work.\n\nData centric approach. Andrew Ng recently spoke about the importance of the training data to achieve good performance from a machine learning model and suggests we should spend more of our effort on data than on the model (Data-centric Approach) (Press, 2021). He said, ”In the Datacentric Approach, the consistency of the data is paramount and using tools to improve the data quality that will allow multiple existing models to do well.” Our method can be seen as Data-centric Approach for offline RL, as we focus on improving the training data and using the existing models. Our method provides a tool to improve data quality.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n5 EVALUATION\n\nWe investigate the performance of QDT relative to the offline RL algorithm with the Dynamic Programming based approach as well as the reward conditioning approach. As QDT utilises the result of CQL and it is considered as the state-of-art offline RL method, we pick CQL as the benchmark for the Dynamic Programming based approach and DT for the reward conditioning approach for the same reason. From the evaluations in this section, we would like to demonstrate the benefits and weaknesses of the Dynamic Programming approach and the reward conditioning approach and how our proposed approach (QDT) helps their weaknesses.\n\nWe start our investigation with a simple environment with sub-optimal trajectories. As it is a simple environment, a Dynamic Programming approach (CQL) should work well, and as it uses sub-optimal trajectories, the reward conditioning approach (DT) will struggle. It is interesting to see how much QDT helps in the circumstance. We also evaluate them on Maze2D environments designed to test the stitching ability with different levels of complexity. We expect that DT struggle whereas CQL and QDT performs well on them. Then, we evaluate the algorithms on complex control tasks – Open AI Gym MuJoCo environments with delayed (sparse) reward as per Chen et al. (2021). They have zero rewards at all the non-terminal states and put the total reward at the terminal state. It should make the Dynamic Programming approach (CQL) learning harder as it requires propagating the reward from the terminal state all way to the initial state. Finally, we show the evaluation results for Open AI Gym MuJoCo environments with the original dense reward setting for the reference.\n\nSimple environment. To highlight the benefit of QDT, we evaluate our method in a simple environment, which has 6-by-6 discrete states and eight discrete actions. The goal of the task is to find the shortest path from the start to the goal state. We prepare an offline RL dataset with a hundred episodes from a uniformly random policy and then remove an episode that achieves close to the optimal total reward to make sure it only contains sub-optimal trajectories. Refer to Appendix B for further details of the environment and the dataset.\n\nTable 1 show the summary of the evaluation results. We also evaluate the performance of CQL, which is used for relabeling. It shows vanilla DT fails badly, which indicates DT struggles to learn from sub-optimal trajectories, whereas CQL performs well as it employs a Dynamic Programming approach, which can pool information across trajectories and successfully figure out the near-optimal policy. It shows QDT performs similar to CQL, which indicates that although QDT uses the conditional policy approach, it overcomes its limitation and learns the near-optimal policy from the sub-optimal data. Further details and results are available in Appendix B.\n\nTable 1: Simple Environment Evaluation Results. Average and standard deviation scores are reported over ten seeds.\n\nTotal Reward\n\nCQL 40.0 ± 0.0\n\nDT 15.9 ± 4.4\n\nQDT 42.2 ± 6.3\n\nMaze2D environments. Maze2D domain is a navigation task requiring an agent to reach a fixed goal location. The tasks are designed to provide tests of the ability of offline RL algorithms to be able to stitch together parts of different trajectories (Fu et al., 2020). It has four kinds of environments – open, umaze, medium and large, and they are getting more complex mazes in the order (Fig. 3) 2. Also, it has two kinds of reward functions – normal and dense. The normal gives a positive reward only when the agent reaches the goal, whereas the dense gives the rewards at every step exponentially proportional to the negative distance between the agent and the goal. For the model, we use the DT source code provided by the authors 3 and d3rlpy 4 (Imai & Seno, 2021) – offline RL library for CQL, then build QDT by adding small code (replacing the return-to-go) to the DT source code before its training.\n\nTable 2 shows the summary of the results. All of the numbers in the table are the normalised total reward (score) such that 100 represents an expert policy (Fu et al., 2020). CQL works well, espe-\n\n2https://github.com/rail-berkeley/d4rl/wiki/Tasks 3https://github.com/kzl/decision-transformer 4https://github.com/takuseno/d3rlpy\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFour Maze2D Figure 3: environment layouts (from left to right: open, umaze, medium and large).\n\nTable 2: Maze2D Evaluation Results. Average and standard deviation scores are reported over 5 seeds. The result for each seed is obtained by evaluating the last learned model on the target environment. The best average values are marked in bold.\n\nDataset\n\nd maze2d-open-v0\n\nr a\nw e\nR\n\nmaze2d-umaze-v1 maze2d-medium-v1 maze2d-large-v1\n\nd maze2d-open-dense-v0\n\nr a\nw e\nR\n\nmaze2d-umaze-dense-v1 maze2d-medium-dense-v1 maze2d-large-dense-v1\n\ne s\nr a\np S\n\ne s\nn e\nD\n\nCQL 216.7 ± 80.7 94.7 ± 23.1 41.8 ± 13.6 49.6 ± 8.4 307.6 ± 43.5 72.7 ± 10.1 70.9 ± 9.2 90.9 ± 19.4\n\nDT 196.4 ± 39.6 31.0 ± 21.3 8.2 ± 4.4 2.3 ± 0.9 346.2 ± 14.3 −6.8 ± 10.9 31.5 ± 3.7 45.3 ± 11.2\n\nQDT 190.1 ± 37.8 57.3 ± 8.2 13.3 ± 5.6 31.0 ± 19.8 325.7 ± 61.4 58.6 ± 3.3 42.3 ± 7.1 62.2 ± 9.9\n\ncially with the dense rewards. DT struggles in many cases due to the lack of stitching ability. (These environments are designed to test the stitching ability.) QDT clearly improves DT performance, especially where CQL performs well. It indicates that QDT brings the stitching capability to DT approach. We discuss the performance gap between CQL and QDT in Section 6.\n\nOpen AI Gym MuJoCo environments with delayed (sparse) reward. We also evaluate our approach (QDT) on complex control tasks – Open AI gym MuJoCo environments with the D4RL offline RL datasets (Fu et al., 2020). The Open AI gym MuJoCo environments consist of three tasks Hopper, HalfCheetah and Walker2d. We test on medium and medium-replay v2 datasets. To demonstrate the shortcoming of the Dynamic Programing approach (CQL), we follow Chen et al. (2021) and evaluate the algorithms with a delayed (sparse) reward scenario in which the agent does not receive any reward along the trajectory and receives the sum of the rewards at the final time step. Again we use the DT and CQL models from the existing source code for the MuJoCo Gym environments without any modifications and add extra code for the relabelling of the RTG values.\n\nTable 3 shows the simulation results (scores) for the delayed reward case. We also copy the simulation results from Chen et al. (2021) for DT and CQL for the reference. All of the numbers in the table are the normalised total reward (score) such that 100 represents an expert policy (Fu et al., 2020). As expected, CQL struggles to learn a good policy, whereas the DT shows good performance. Also, QDT performs similar to DT even though they are using the results of CQL that performs badly. It indicates that QDT successfully use the information from CQL where it is useful. One exception is the medium-replay-walker2d result. QDT performs worse than DT here. Through some investigations, we found that the CQL algorithm overestimates the value function in the majority of the states in the medium-replay-walker2d dataset. We touch the issue in the following discussion section.\n\nOpen AI Gym MuJoCo environments. We also evaluate our approach (QDT) on Open AI gym MuJoCo environments with the original dense reward for the reference. As they have dense rewards and contain reasonably good trajectories, both CQL and DT would work well.\n\nTable 4 shows the summary of our simulation results for CQL, DT and QDT. We also copy the simulation results from Chen et al. (2021) for DT and Emmons et al. (2021) for CQL for the reference. Firstly, we can see that our simulation results are aligned with the references except for the medium-replay-hopper result. Because it has a relatively high variance, it is probably due to the small number of samples (five random seeds). Secondly, CQL performs equal or better than DT and QDT in this evaluation. It is understandable as they have dense rewards (they do not require propagating value function in the trajectory). Finally, from the comparison between DT and QDT, QDT performs the same as DT.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Open AI Gym MuJoCo with Delayed Reward Evaluation Results. Average and standard deviation scores are reported with 5 seeds. Our simulation results are in the Results columns, best average boldfaced. Ref.∗2 are the results copied from Chen et al. (2021). We are not sure which version of dataset the authors used for Ref.∗2, and only Hopper results are available in the paper.\n\nDataset\n\nm Hopper-v2\n\nu\n\ni\n\nd e\n\nM\n\nm u\n\ni\n\nd e\n\nM\n\na l\n\np e\nR\n\nHalfCheetah-v2 Walker2d-v2\n\ny Hopper-v2\n\nHalfCheetah-v2 Walker2d-v2\n\nCQL\n\nResults 23.3 ± 1.0 1.0 ± 1.0 0.0 ± 0.4 7.7 ± 5.9 7.8 ± 6.9 3.2 ± 1.7\n\nRef.∗2 5.2 −\n− 2.0 −\n−\n\nDT\n\nResults 57.3 ± 2.4 42.2 ± 0.2 69.9 ± 2.0 50.8 ± 14.3 33.0 ± 4.8 51.6 ± 24.6\n\nRef.∗2 60.7 ± 4.5 −\n− 78.5 ± 3.7 −\n−\n\nQDT Results 50.7 ± 5.0 42.4 ± 0.5 63.7 ± 6.4 38.7 ± 26.7 32.8 ± 7.3 29.6 ± 15.5\n\nTable 4: Open AI Gym MuJoCo Evaluation Results. Average and standard deviation scores are reported over 5 seeds. Our simulation results are in Results columns. The best average values are marked in bold. Ref.∗1 is the results copied from Emmons et al. (2021). Ref.∗2 is the results copied from Chen et al. (2021).\n\nDataset\n\nm Hopper-v2\n\nu\n\ni\n\nd e\n\nM\n\nm u\n\ni\n\nd e\n\nM\n\na l\n\np e\nR\n\nHalfCheetah-v2 Walker2d-v2\n\ny Hopper-v2\n\nHalfCheetah-v2 Walker2d-v2\n\nCQL\n\nResults 69.4 ± 13.1 49.2 ± 0.5 83.0 ± 0.6 96.2 ± 7.9 49.8 ± 0.5 76.5 ± 21.1\n\nRef.∗1 64.6 49.1 82.9 97.8 47.3 86.1\n\nDT\n\nResults 60.3 ± 5.5 42.1 ± 0.5 73.3 ± 2.5 63.7 ± 12.2 34.1 ± 1.1 60.2 ± 13.9\n\nRef.∗2 67.6 ± 1.0 42.1 ± 0.1 74.0 ± 1.4 82.7 ± 7.0 36.6 ± 0.8 66.6 ± 3.0\n\nQDT Results 66.5 ± 6.3 42.3 ± 0.4 67.1 ± 3.2 52.1 ± 20.3 35.6 ± 0.5 58.2 ± 5.1\n\n6 DISCUSSION\n\nStitching ability. To demonstrate the stitching ability, we evaluate the performance of each algorithm with varying degrees of the sub-optimal dataset. We pick the medium-replay dataset for the MuJoCo Gym environment as it contains trajectories generated by various agent levels and removes the best X% of the trajectories. As X is increased, more good trajectories are removed from the dataset. Thereby moving further away from the optimal setup. Fig. 4 shows the CQL, DT and QDT results as well as the best trajectory return in the dataset. It shows that CQL offers better results than the best trajectory within the dataset except X = 0, where the trajectory contains the best score; hence it can not be better than that. In contrast, DT fails to exceed the best trajectory, which indicates DT fails to stitch the sub-optimal trajectories. QDT performs better than DT and becomes close to the CQL results at X = 40 and 50 (in the regime with 60 − 50% bottom trajectories).\n\nFigure 4: Evaluation results (scores) for CQL, DT and QDT with the hoppermedium-replay-v2 dataset removed top X% trajectories. The shaded area shows one standard deviation range of the results. It also has the maximum score in the dataset as a reference. CQL results are generally better than the maximum score, which indicates CQL successfully stitches sub-optimal trajectories, whereas DT fails to do so. QDT improves DT through relabelling, being better than the maximum score on the right-hand side of the plot.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nPerformance gap between QDT and CQL. Although QDT improves DT on the sub-optimal dataset scenario (Fig. 4), QDT does not perform as well as CQL for the range of small X. The results from Emmons et al. (2021) indicate that DT can perform as well as CQL when plenty of good trajectories are available (medium-expert dataset). It implies that there is still room for improvements for DT and QDT approaches with datasets that contain far from optimal trajectories. To address this, we are considering using a Q-learning algorithm specific to QDT approach, but this is left for the future work.\n\nConservative weight. CQL has a hyperparameter called conservative weight, denoted by α in Eq. 2. It weights the regulariser term, where the higher value, the more conservative are the value function estimations. Ideally, we would like to set it as small as possible so that the estimated value function becomes a tighter lower bound; however, too small conservative weight might break the lower bound guarantee, and the learned value function might give a higher value than the true value (Kumar et al., 2021). Empirically, we discovered that this is exactly what happens in our delayed reward experiment (Table 3) for the medium-replay-waker2d dataset example. The value function learned by CQL in the dataset has higher values than the corresponding true value in many states, and it causes the wrong relabelling of RTG and, subsequently, a worse QDT performance. We evaluated it with higher α values – increased from 5.0 to 100. Though this improves the QDT result from 30.3 ± 16.2 to 46.9 ± 13.8, it is still worse than DT. This is left for future work for further investigation. In this paper, we assume we have access to the environment in order to optimise the hyperparameters. However, this should be done purely offline for a proper offline RL setting. Although there are some proposals (Paine et al., 2020; Fu et al., 2021; Emmons et al., 2021), this is still an active research area.\n\nReproducing results for benchmarking. There have been many attempts to establish a benchmark for the offline RL approaches by building datasets (Fu et al., 2020; Agarwal et al., 2020), sharing their source code, as well as producing a library focusing on offline RL (Imai & Seno, 2021). However, we still found some conflicting results between papers. The leading cause of the issue is the requiring a vast amount of effort and computational power to reproduce the other researcher’s results. As a result, most authors have no choice but to re-use the original results from state-of-the-art papers in the literature to establish a comparison. However, this leads to conflicting results due to the difficulties of reproducing all the details involved in these very diverse experimental setups. For example, many offline RL papers use D4RL MuJoCo datasets to evaluate their algorithms and compare them against other approaches. In this case, the datasets have three versions – namely, v0, v1 and v2. While not always clearly stated, most papers use version v0. However, some use version v2, which causes some of the conflicting results. For example, Chen et al. (2021) appears to evaluate their model with the v2 dataset while referencing other papers’ results that use v0. A second issue with benchmarking the results in this manner is the usual insufficient number of simulations. As the simulations require large processing power, it is not feasible to run a large number of simulations. Most authors (including us) evaluate only three random seeds, which is often insufficient to compare the results. In this paper, we emphasise and analyse carefully the results from the simple environment, as they helps demonstrate the characteristics of the algorithm. The more complex and realistic environments are still helpful; however, the estimated variance suggest that several cases should be handled with care when extracting conclusions.\n\n7 CONCLUSIONS\n\nWe proposed Q-learning Decision Transformers, bringing the benefits of Dynamic Programming (Q-learning) approaches to reward conditioning sequence modelling methods to address some of their well-known weaknesses. Our approach provides a novel framework for improving offline reinforcement learning algorithms. In this paper, to illustrate the approach, we use existing state-ofthe-art algorithms for both Dynamic Programming (CQL) and reward conditioning modelling (DT). Our evaluation shows the benefits of our approach over existing offline algorithms in line with the expected behaviour. Although the results are encouraging, there is room for improvement. For example, the QDT results for Maze2D (Table 2) are better than DT but still not as good as CQL. On the other hand, the QDT results for Gym MuJoCo delayed reward (Table 3) are significantly better than CQL but not as good as DT in the walker2d environment. These need further investigation. We are also interested in trying different Dynamic Programming and reward conditioning algorithms in the proposed framework.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline\n\nreinforcement learning. pp. 104–114, 2020.\n\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304–29320, 2021.\n\nDavid Brandfonbrener, William F Whitney, Rajesh Ranganath, and Joan Bruna. Quantile filtered\n\nimitation learning. arXiv preprint arXiv:2112.00950, 2021.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.\n\nXinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Bestaction imitation learning for batch deep reinforcement learning. Advances in Neural Information Processing Systems, 33:18353–18363, 2020.\n\nFelipe Codevilla, Matthias M ̈uller, Antonio L ́opez, Vladlen Koltun, and Alexey Dosovitskiy. End-\n\nto-end driving via conditional imitation learning. pp. 4693–4700, 2018.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning, 2020.\n\nJustin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R Zhang, Yutian Chen, Aviral Kumar, et al. Benchmarks for deep offpolicy evaluation. arXiv preprint arXiv:2103.16596, 2021.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.\n\nHiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline\n\nhindsight information matching. arXiv preprint arXiv:2111.10364, 2021.\n\nDibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019.\n\nHado Hasselt. Double q-learning. Advances in neural information processing systems, 23, 2010.\n\nMichita Imai and Takuma Seno. d3rlpy: An offline deep reinforcement library. 12 2021.\n\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-\n\nbased policy optimization. Advances in Neural Information Processing Systems, 32, 2019.\n\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. Advances in neural information processing systems, 33: 21810–21823, 2020.\n\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy qlearning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019a.\n\nAviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint\n\narXiv:1912.13465, 2019b.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n\nAviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for\n\noffline model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021.\n\nAviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. When should we prefer offline rein-\n\nforcement learning over behavioral cloning? arXiv preprint arXiv:2204.05618, 2022.\n\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and\n\nPierre Sermanet. Learning latent plans from play. pp. 1113–1132, 2020.\n\nAshvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement\n\nlearning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.\n\nTom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement learning. arXiv preprint arXiv:2007.09055, 2020.\n\nXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n\nGil\n\nPress.\n\nAndrew\n\nng\n\nlaunches\n\na\n\ncampaign\n\nfor\n\ndata-centric\n\nai.\n\nhttps://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-datacentric-ai/?sh=82961be74f57, June 2021.\n\nRafael Figueiredo Prudencio, Marcos R O A Maximo, and Esther Luna Colombini. A survey on offline reinforcement learning: Taxonomy, review, and open problems. arXiv preprint arXiv:2203.01387, 2022.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\n\nstanding by generative pre-training. 2018.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.\n\nNoah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396, 2020.\n\nRupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja ́skowski, and J ̈urgen Schmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877, 2019.\n\nRichard S Sutton and Andrew G Barto. Reinforcement Learning. The MIT Press, 1998.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nQing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imitation learning for batched historical data. Advances in Neural Information Processing Systems, 31, 2018.\n\nZiyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. Advances in Neural Information Processing Systems, 33:7768–7778, 2020.\n\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\n\narXiv preprint arXiv:1911.11361, 2019.\n\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in Neural Information Processing Systems, 34, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAppendices\n\nA SIMPLE ENVIRONMENT EXAMPLE TRAJECTORY DATA AND ITS\n\nCOMPUTATION\n\nThis section describes the trajectory data and some computation details for the simple example shown in Fig. 1. We bring the figure here and added the state IDs in the circle (Fig. 5). The two\n\nFigure 5: A simple example demonstrates the decision transformer approach’s issue (lack of stitching ability) – fails to find the shortest path to the goal. In contrast, Q-learning finds the shortest path.\n\ntrajectories of training data are:\n\ntrajectory1 = [s0=S, a0=up, r0=−1,\n\ns1=1, a1=down, r1=−1, s2=2, a2=right, r2=−1, s3=G, a3=N/A, r3=0]\n\ntrajectory2 = [s0=S, a0=right, r0=−11, s1=2, a1=down, r1=−10, s2=F, a2=N/A, r2=0].\n\nWe compute the return-to-go (RTG) from the reward rt as Eq. 4.\n\nRt =\n\nT (cid:88)\n\nτ =0\n\nrτ ,\n\n(3)\n\n(4)\n\nwhere Rt is RTG at time step t and T is the episode length. The trajectories with the RTGs becomes as follows:\n\ntrajectory1 = [s0=S, a0=up, r0=−1, R0=−3,\n\ns1=1, a1=down, r1=−1, R1=−2, s2=2, a2=right, r2=−1, R2=−1, s3=G, a3=N/A, r3=0, R3=0]\n\ntrajectory2 = [s0=S, a0=right, r0=−1, R0=−11,\n\ns1=2, a1=down, r1=−10, R1=−10, s2=F, a2=N/A, r2=0, R2=0].\n\n(5)\n\nDT (the reward-conditioned approach) is trained to predict actions from the state and RTG, so it takes [st, Rt] as the input and outputs at. (Here, we assume the context length K = 1 for DT for simplicity.) For example, in the t = 0 case, the DT agent is trained to predict a = up from [s=S, R=−3] (trajectory 1) and a = right from [s=S, R=−11] (trajectory 2). For the evaluation, we set the RTG the best value (−2 in this case) at t = 0, and then the agent predicts the action from [s=S, R=−2]. Because the input [s=S, R=−2] is closer to [s=S, R=−3] (trajectory 1) than [s=S, R=−11] (trajectory 2), the agent predict a = up (trajectory 1) despite the optimal action is a = right (trajectory 2).\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nB SIMPLE ENVIRONMENT EVALUATION DETAILS\n\nB.1 ENVIRONMENT\n\nThe environment has 6-by-6 discrete states and eight discrete actions as shown in Fig. 6. The goal of the task is to find the shortest path from the start to the goal state. Each time step gives -10 reward and +100 reward at the goal. The optimal policy gives +50 total reward (= 100 − 10 ∗ 5). We also remap the action so that the same action index is not always optimal. The mapping differs for each state but is fixed across the episodes.\n\nFigure 6: A simple 2D maze environment, which has 6-by-6 grid world and eight actions for moving eight directions. -10 reward at each time step and +100 reward for the goal. The optimal trajectory keeps moving up-right to the goal, which has total reward +50 (= 100 − 10 ∗ 5). The action is remapped so that the same action index is not always the optimal action. The mapping differs for each state, but fixed across the episodes.\n\nB.2 DATASET\n\nWe prepare an offline RL dataset with a hundred episodes from a uniformly random policy and then remove an episode that achieves a positive total reward to make sure it only contains sub-optimal trajectories. As a result, the dataset used in this evaluation has one hundred episodes and 4,454 time steps. The maximum return of the hundred episodes is -10.0, the minimum return is -490 as we terminate the episode at 50 time step, and the average return is -415.5.\n\nB.3 CQL MODEL DETAILS\n\nWe build the CQL model for the simple environment based on Double Q-learning (Hasselt, 2010) and employ an embedding lookup table module to convert the discrete state to continuous high dimensional embedding space. The detailed model parameters are in Table 5.\n\nTable 5: Simple Enviornment CQL Model Parameters\n\nParameter\n\nValue\n\nState embedding dimension\n\n32\n\nDQN type\n\nfully connected\n\nDQN number of layers\n\nDQN number of units\n\n2\n\n32\n\nOptimizer\n\nOptimizer betas\n\nOptimizer learning rate\n\nTarget network update rate\n\nBatch size\n\nAdam\n\n0.9, 0.999\n\n5.0e-4\n\n1.0e-2\n\n128\n\nNumber of training steps Conservative weight (α)\n\n1000 updates\n\n0.5\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB.4 DT AND QDT MODEL DETAILS\n\nOur DT and QDT model for the simple environment is constructed based on minGPT open-source code5. The detailed model parameters are in Table 6.\n\nTable 6: Simple Environment DT/QDT Model Parameters\n\nParameter\n\nNumber of layers\n\nNumber of attention heads\n\nEmbedding dimension\n\nValue\n\n4\n\n4\n\n64\n\nNonlinearity function\n\nReLU\n\nBatch size Context length K\n\nreturn-to-go conditioning\n\nDropout\n\nLearning rate\n\n64\n\n2\n\n50\n\n0.1\n\n4.0e-4\n\nB.5 FURTHER EVALUATION RESULTS FOR SIMPLE ENVIRONMENT\n\nThe following tables have the simple environment results for all ten seeds. Table 7 shows the reward for the highest value during the training period. Table 8 shows the reward with the model at the end of training. DT and QDT have more significant differences between these two tables than the CQL results, which indicates that DT and QDT have overfitting issues and unstable learning behaviour.\n\nTable 7: Simple Environment Full Results (Best). The results from the best performing model during the training.\n\nCQL DT QDT\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n0.0\n\n18.2\n\n20.4\n\n11.2\n\n13.8\n\n12.6\n\n8.4\n\n19.6\n\n21.2\n\n14.4\n\n18.8\n\n15.9\n\n4.4\n\n43.6\n\n42.0\n\n49.2\n\n42.6\n\n39.2\n\n27.8\n\n47.2\n\n47.4\n\n37.4\n\n46.0\n\n42.2\n\n6.3\n\nn e\nt\n\nr o\nf\n\ns t\nl\n\nu s\ne r\n\ns d\ne e\ns\n\nm o\nd n\na r\n\nmean\n\nstd.\n\nC OPEN AI GYM MUJOCO AND MAZE2D EVALUATION DETAILS\n\nC.1 CQL MODEL DETAILS\n\nFor MuJoCo Gym CQL evaluation, we use d3rlpy library (Imai & Seno, 2021). It provides a script to run the evaluation (d3rlpy/reproduce/offline/cql.py), and it uses the same hyperparameters as\n\n5https://github.com/karpathy/minGPT\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Simple Environment Full Results (Last). The results from the model at the end of the training.\n\nCQL\n\nDT\n\nQDT\n\n40.0\n\n40.0\n\n40.0\n\n40.0\n\n30.0\n\n40.0\n\n40.0\n\n30.0\n\n40.0\n\n40.0\n\n38.0\n\n4.2\n\n-39.2\n\n8.6\n\n-25.4\n\n-20.8\n\n-50.2\n\n-26.0\n\n9.4\n\n-35.0\n\n-10.2\n\n7.8\n\n-18.1\n\n21.3\n\n13.8\n\n35.8\n\n46.6\n\n16.6\n\n29.2\n\n19.6\n\n44.0\n\n47.4\n\n23.2\n\n35.0\n\n31.1\n\n12.5\n\nn e\nt\n\nr o\nf\n\ns t\nl\n\nu s\ne r\n\ns d\ne e\ns\n\nm o\nd n\na r\n\nmean\n\nstd.\n\nKumar et al. (2020). For Mazed2d simulations, we re-use the same d3rlpy script with the same hyperparameter settings.\n\nC.2 DT AND QDT MODEL DETAILS\n\nFor DT simulations, we use the code provided by the original paper authros6 for both MuJoCo Gym and Maze2D environments. For QDT simulations, we added extra code to relabelling the return-togo to the DT script (decision-transformer/gym/experiment.py). The relabelling code is described in Algorithm 1 and 2.\n\nC.3 EVALUATION PROCESS\n\nCQL We train the CQL model with five random seeds for 500,000 updates with 256 batch size, then evaluate the model at the end of the training with 10 episode roll-outs. We inherit these CQL settings from d3rlpy offline RL library (Imai & Seno, 2021).\n\nDT We train the DT model with five random seeds for 100,000 updates with 64 batch size, then evaluate the model at the end of the training with 100 episode roll-outs. We inherit these DT settings from the source code provided by the DT paper authors7 (Chen et al., 2021).\n\nQDT We train the QDT model with five random seeds, each of them employing its own trained CQL model to relabel the dataset. QDT model is trained for 100,000 updates for MuJoCo Gym and 150,000 updates for maze2d with 64 batch size, then evaluate the model at the end of the training with 100 episode roll-outs – same as DT.\n\nC.4 HYPER PARAMETER SEARCH\n\nWe use the same hyper-parameter settings as the original papers (Kumar et al., 2020; Chen et al., 2021). However, we did some hyper-parameter searches for the conservative weight (α). It is because the optimal conservative weight value could be different for CQL and QDT.\n\nFor MuJoCo Gym environments, we start with α = 10.0 for medium dataset and α = 5.0 for medium-replay dataset. We take these values from the CQL paper. Then, reduce these values to see if the performance of CQL and QDT varies. Table 9 and Table 10 shows CQL and QDT results\n\n6https://github.com/kzl/decision-transformer 7https://github.com/kzl/decision-transformer\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nrespectively. These results show that α = 10.0 for medium dataset and α = 5.0 for medium-replay dataset perform well for QDT and do not degrade performance significantly for CQL. Also, they are the same values as the original paper, so we decide to keep them the same as the paper.\n\nTable 9: CQL results for Open AI Gym MuJoCo with conservative weight parameter (α) sweep. Average and standard deviation scores are reported over three seeds.\n\nDataset\n\nm Hopper-v2\n\nu\n\ni\n\nd e\n\nM\n\nm u\n\ni\n\nd e\n\nM\n\na l\n\np e\nR\n\nHalfCheetah-v2 Walker2d-v2\n\ny Hopper-v2\n\nHalfCheetah-v2 Walker2d-v2\n\nα = 10.0 68.7 ± 16.4 48.9 ± 2.4 83.3 ± 0.5\n\nCQL\n\nα = 5.0 72.5 ± 9.5 51.8 ± 2.4 86.2 ± 0.5 95.4 ± 11.6 49.9 ± 2.9 88.9 ± 3.7\n\nα = 2.5 83.6 ± 3.8 57.0 ± 1.1 43.5 ± 43.6 87.5 ± 24.7 51.8 ± 2.7 50.6 ± 36.3\n\nα = 1.25\n\n90.7 ± 14.5 54.3 ± 0.2 16.8 ± 14.2\n\nTable 10: QDT results for Open AI Gym MuJoCo with conservative weight parameter (α) sweep. Average and standard deviation scores are reported over three seeds.\n\nDataset\n\nm Hopper-v2\n\nu\n\ni\n\nd e\n\nM\n\nm u\n\ni\n\nd e\n\nM\n\na l\n\np e\nR\n\nHalfCheetah-v2 Walker2d-v2\n\ny Hopper-v2\n\nHalfCheetah-v2 Walker2d-v2\n\nα = 10.0 68.6 ± 7.5 42.2 ± 0.5 65.9 ± 3.6\n\nQDT\n\nα = 5.0 65.3 ± 1.3 42.2 ± 0.05 70.1 ± 2.4 55.3 ± 28.0 35.7 ± 0.6 59.1 ± 2.8\n\nα = 2.5 57.5 ± 6.6 42.1 ± 0.4 68.8 ± 6.9 40.2 ± 5.9 35.5 ± 0.4 64.3 ± 5.9\n\nα = 1.25\n\n64.0 ± 22.9 33.0 ± 0.5 45.2 ± 39.5\n\nFor maze2d environment, we start with α = 10.0 which is the value used in the CQL paper for MuJoCo Gym environments medium datasets. Then, reducing these values to see if the performance of CQL varies. Table 11 shows the simulation results. We pick α = 1.0 as it performs the best. It is possible that even lower values might perform better. We see QDT shows good improvement over DT with α = 1.0, so we use the value for this paper. We would like to try further optimisation in the future.\n\nTable 11: CQL results for Maze2D with conservative weight parameter (α) sweep. Average and standard deviation scores are reported over three seeds.\n\nDataset maze2d-umaze-v1 maze2d-medium-v1 maze2d-large-v1\n\nα = 10.0 27.3 ± 12.2 −3.5 ± 1.3 −2.5 ± 0.0\n\nCQL α = 2.0 66.1 ± 9.8 36.6 ± 3.7 40.8 ± 6.0\n\nα = 1.0 96.0 ± 32.2 35.9 ± 15.3 53.2 ± 7.0\n\nD JUSTIFICATION OF REPLACING RTG WITH THE LEARNED VALUE\n\nFUNCTION\n\nDefine the optimal state value function as V ∗(st), the learned lower bound of the value function as ˆV (st) and the corresponding return-to-go value as Rt. We show that when ˆV (st) > Rt, the error in ˆV (st) is smaller than the error in Rt. We start from the condition,\n\nˆV (st) > Rt V ∗(st) − ˆV (st) < V ∗(st) − Rt.\n\n16\n\n(6)\n\nUnder review as a conference paper at ICLR 2023\n\nAs ˆV (st) is the lower bound of V ∗(st), V ∗(st) ≥ ˆV (st). Hence both sides of the above equation are non-negative. We can take the absolute of both terms, and we get,\n\n|V ∗(st) − ˆV (st)| < |V ∗(st) − Rt|.\n\n(7)\n\nThis indicates that the error in ˆV (st) is smaller than the error in Rt.\n\nE FURTHER DISCUSSIONS\n\nE.1 WHY CQL OUTPERFORMS DT/QDT ON MAZE2D, BUT FAILS ON MUJOCO GYM\n\nDELAYED REWARD?\n\nIt is because maze2d are simpler environments and have shorter episodes than the MuJoCo control tasks. Table 12 shows that the action dimension, the state (observation) dimension and the episode length averaged over the top 5% returns in the dataset. It can be seen that MuJoCo tasks have higher action/state dimensions and longer episode lengths than Maze2d. Also, the evaluation results for the Sparse maze2d-medium and -large show some notable performance loss against the Dense counterparts, which is aligned with the fact that their episode lengths are longer than the maze2dopen and -umaze.\n\nTable 12: MuJoCo Gym and Maze2D environments comparison. The table shows that the action dimension, the state (observation) dimension and the episode length averaged over the top 5% returns in the dataset.\n\nEnvironment\n\nhopper halfcheetah walker2d maze2d-open maze2d-umanze maze2d-medium maze2d-large\n\nAction Dimension 3\n6 6\n2 2\n2 2\n\nState Dimension 11 17 17 4\n4 4\n4\n\nGood Episode Average Length 708.2 1000.0 996.7 49.8 128.6 224.1 314.6\n\nE.2 WHY QDT OUTPERFORMS DT ON MAZE2D WHEREAS IT DOES NOT ON GYM DESPITE\n\nBOTH HAVING DENSE REWARDS?\n\nIt is due to the difference in the training data. maze2d dataset is designed to test the stitching ability; hence it only has sub-optimal trajectories, whereas the MuJoCo Gym dataset has some optimal trajectories. If the dataset has some optimal trajectories, DT will perform well. On the other hand, if the dataset has only suboptimal trajectories, DT will struggle, and QDT improves such cases by utilising the information in CQL.\n\nAs maze2d only has suboptimal trajectories, DT struggles with them, and QDT can perform better than DT. For MuJoCo Gym cases, the dataset has some optimal trajectories; hence DT performs well, and so as QDT.\n\nStrictly speaking, there are some exceptions. MuJoCo halfcheetah-medium and halfcheetahmedium-replay dataset does not have an optimal trajectory, still QDT performs similarly to DT. It is because even CQL struggles to achieve good performance on these datasets. (CQL only performs similarly to DT even though CQL can stitch the suboptimal trajectories.) As CQL struggled, QDT could not get much help from CQL.\n\nThe other exception is maze2d-open and maze2d-open-dense. These datasets have good trajectories. It is actually aligned with our evaluation results. The results for maze2d-open and maze2d-opendense show good performance with DT.\n\nTable 13 shows the maximum, 95 percentile and 90 percentile values of the normalised returns (score) in the dataset. As we discussed above, Maze2d has suboptimal trajectories (except open\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nand open-dense), and MuJoCo Gym has (near) optimal trajectories – a score close to 100 (except halfcheetah).\n\nTable 13: Scores in MuJoCo Gym and Maze2D datasets. This table shows that maximum score, 95 percentile score and 90 percentile score values for each dataset.\n\nDataset maze2d-open-v0 maze2d-open-dense-v0 maze2d-umaze-v1 maze2d-umaze-dense-v1 maze2d-medium-v1 maze2d-medium-dense-v1 maze2d-large-v1 maze2d-large-dense-v1 hopper-medium-v2 hopper-medium-replay-v2 halfcheetah-medium-v2 halfcheetah-medium-replay-v2 walker2d-medium-v2 walker2d-medium-replay-v2\n\nmax. score 232.4 188.9 21.1 -1.4 12.8 8.9 16.9 14.6 99.5 98.6 45.0 42.4 92.0 89.9\n\n95 pct. score 130.7 128.4 13.2 -11.7 6.8 4.0 6.5 7.9 63.2 46.4 43.0 39.9 83.4 66.6\n\n90 pct. score 116.2 117.4 10.3 -18.3 4.9 0.3 -2.5 -2.4 57.0 31.5 42.5 39.2 82.4 42.5\n\nE.3 WHY QDT PERFORMS CLOSE TO DT, NOT CQL IN FIG. 4 (GYM HOPPER)?\n\nThe main reason is that QDT employs DT as its agent algorithm. The difference lays in its training data. If the environment/dataset has specific characteristics that work against DT approach, those also work against QDT. Some of these properties, such as dataset sub-optimality, are fixed/mitigated by QDT. However, there may be other elements that are against DT and QDT, e.g., the environment having a few critical states (Kumar et al., 2022). If this is also behind the gap between CQL and DT, then it is possible QDT performs close/the same as DT.\n\nKumar et al. (2022) studied the Dynamic Programming approach and the imitation learning approach and compared the upper bounds of their sub-optimality (the difference between the return from the optimal policy and the learned policy). They show that the Dynamic Programming approach is preferred over imitation learning when the environment has a few critical states – the return of the episode mostly depends upon the actions in these states. The results in Kumar et al. (2022) are based on theoretical analysis (sub-optimality upper bounds). Hence, it is possible that the imitation learning approach (DT and QDT) can perform as well as or better than Dynamic Programming approaches (such as CQL) in practice. Kumar et al. (2022) empirically shows that the goal-conditioned approach remains competitive by selecting the right level of model capacity and the goal. There are still many open and ongoing discussions regarding the comparison.\n\nE.4 EXTRA RESULTS FOR REMOVING TOP X%\n\nWe run the same experiment as the Stitching ability subsection in Section 6 on the other two MuJoCo Gym environments. The results (Fig. 7) do not show a clear benefit of QDT over DT. We think it is because the cause of the gap between CQL and DT is not just the sub-optimality in the dataset (still the sub-optimality can be the cause of the difference, but it is not the only cause in these cases.)\n\nE.5 CONSISTENCY RELABELLING ABLATION EXPERIMENT\n\nWe have tried the ablation experiment for the consistency relabelling (Algorithm 2) on a subset of environments. The results are summarised it in Table 14. We run ten random seeds for Simple Environment and three for others. The Simple Environment result shows some benefits of using Algorithm 2 in its average value, although it is not significant. For the other more complex environments, we do not see clear benefits of Algorithm2. We think this is because the changes applied by Algorithm2 are relatively minor compared to the original RTG variations. We think it is better to\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Evaluation results (scores) for CQL, DT and QDT with the halfcheetah-medium-replay-v2 and walker2d-medium-replay-v2 dataset removed top X% trajectories. The shaded area shows one standard deviation range of the results. It also has the maximum score in the dataset as a reference.\n\nkeep Algorithm 2, at least for now because the training data could have non-realistic (inconsistent) samples without the algorithm.\n\nTable 14: Scores in Simple Environment, MuJoCo Gym and Maze2D datasets. This table shows QDT results and QDT without the consistency relabelling (Algorithm 2).\n\nDataset\n\nSimple Environment hopper-medium-v2 halfcheetah-medium-v2 walker2d-medium-v2 maze2d-large-v1\n\nQDT\n\n42.2 ± 6.3 65.3 ± 2.0 42.2 ± 2.3 71.3 ± 2.4 35.0 ± 24.2\n\nQDT w/o Alg.2 29.7 ± 13.8 65.7 ± 3.9 42.4 ± 0.1 80.2 ± 10.8 23.0 ± 5.0\n\nE.6 AGGREGATED EVALUATION RESULTS\n\nWe compute the aggregated evaluation results for each group of environments (maze2d, MuJoCo Gym delayed reward and MuJoCo Gym) with three different metrics – median, Interquantile mean (IQM) and mean (Fig. 8). It uses 95% stratified bootstrapped confidence interval (Agarwal et al., 2021).\n\nThe results support our conclusions 1) DT struggles in maze2d, but QDT improves DT performance by getting help from CQL. 2) CQL fails in MuJoCo Gym delayed reward. 3) DT and QDT perform similarly in MuJoCo Gym. Note that QDT has never failed in any of these groups of environments.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Aggregated evaluation results (scores) for each group of environments (maze2d, MuJoCo Gym delayed reward and MuJoCo Gym) with three different metrics – median, Interquantile mean (IQM) and mean. The results support our conclusions 1) DT struggles in maze2d, but QDT improves DT by getting help from CQL. 2) CQL fails in MuJoCo Gym delayed reward. 3) DT and QDT perform similarly in MuJoCo Gym.\n\n20",
    "reference": "# Summary Of The Paper\n\nThis paper focuses on the problems with two popular offline RL algorithms: DT (which struggles with inferring optimal policies from partial solutions, referred to as the stitching problem) and CQL (which struggles to propagate rewards with a long time horizon). To address the shortcomings of both methods, the authors propose a method (QDT) that modifies returns in training trajectories with value functions (learned via CQL) when returns are lower than learned value functions, then uses this data to train DT. \n\nOn environments selected to test either stitching ability (Maze2D) or learning with sparse rewards (Mujoco with delayed reward), the authors demonstrate that, while QDT rarely outperforms both DT and CQL, it can learn when either of the two methods fails.\n\n# Strength And Weaknesses\n\nThe paper does a good job of defining and demonstrating the problems through both examples and the choice of environments. The experiments are fairly extensive and I appreciated the thorough discussion on the limitations of the method. The results are mostly consistent with the hypothesis that DT struggles with the stitching problem and CQL struggles with the delayed rewards.  \n\nGiven the problems with DT are most acute when the training dataset does not contain (near) optimal trajectories, I found the experiment with an increased percentage of removed top trajectories (Fig 4) particularly interesting, and would appreciate seeing the same experiment run on multiple environments. It should be noted QDT starts outperforming DT only in the regime with 60-50% bottom trajectories. Overall QDT seems much closer to DT than CQL in performance. Why is that the case?  \n\nSince the main contribution is the method by which the returns are relabelled, Section 3 should be rewritten to be clearer and move the theory and pseudo-algorithm from the Appendix to Section 3. The lack of strong theoretical justification for the given procedure is one of the main weaknesses of the paper.  \n\nCQL outperforms DT/QDT on Maze2D even with sparse reward, which seems inconsistent with the initial hypothesis. Why is this the case? Separating dense and sparse environments in Table 2 would highlight this better. \n\nI'd be also curious to see the results with more random seeds if the authors ran more experiments since the submission.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe specific approach proposed in this paper is novel to my knowledge, though I am not following offline RL literature very closely. \n\nThe writing is mostly clear, though it could benefit from improving the structure of the paper (see my comments on Section 3) and making the writing overall tighter.  \n\nThe paper mentions, but does not elaborate on the details of the hyper-parameter search. This should be added to the Appendix. \n\nOther suggestions to improve clarity:\n- add calculations for Figure 1 in the Appendix (while easy to check on their own, it would help some in following the paper) \n- I don't think p4 of Related work adds much to the paper, I would skip it and prioritize elaborating more on the methods section\n- bold the best methods in Table 2, 3 and 4; maybe highlight which are sparse vs dense environments in Table 2\n- clarify the meaning of the shaded area in Fig 4\n\n# Summary Of The Review\n\nTo improve the ability of DT to learn from partial solutions, the authors propose a method (QDT) that modifies returns in training trajectories with value functions (learned via CQL). On environments selected to test either stitching ability or learning with sparse rewards, the authors demonstrate that, while QDT rarely outperforms both DT and CQL, it can learn when either of the two methods fails.   \n\nThe paper would be stronger if the proposed method had: (a) stronger theoretical justification, (b) better empirical results (regularly matching the best method in either setting), (c) the authors did a more thorough investigation into the performance difference between CQL and QDT (partly mentioned as left for future work in Section 6: conservative weight).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLONG TERM FAIRNESS VIA PERFORMATIVE DISTRIBUTIONALLY ROBUST OPTIMIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFairness researchers in machine learning (ML) have coalesced around several fairness criteria which provide formal definitions of what it means for an ML model to be fair. However, these criteria have some serious limitations. We identify four key shortcomings of these formal fairness criteria and address them by extending performative prediction to include a distributionally robust objective. Performative prediction is a recent framework developed to understand the effects of when deploying a model influences the distribution on which it is making predictions. We prove a convergence result for our proposed repeated distributionally robust optimization (RDRO). We further verify our results empirically and develop experiments to demonstrate the impact of using RDRO on learning fair ML models.\n\n1\n\nINTRODUCTION\n\nIn the past two decades, machine learning (ML) has moved from the confines of research institutes and university laboratories to become a core element of the global economy. ML models are now deployed at enormous scales in complex environments, often making high stakes decision. Too often, however, this is done without adequate concern for the fairness and robustness of these ML models. Fairness in ML is a burgeoning research area, but much of the work in fairness, particularly in defining formal fairness criteria, has been limited to the static classification setting.\n\nEfforts to define fairness in ML have resulted in myriad criteria being proposed, many of which are equivalent to, or relaxations of, three core definitions of fairness: independence, separation, and sufficiency Barocas et al. (2019); Chouldechova (2017); Corbett-Davies et al. (2017); Dwork et al. (2012); Hardt et al. (2016b); Berk et al. (2021); Zafar et al. (2017); Kleinberg et al. (2017); Woodworth et al. (2017). These formal fairness criteria assume a sensitive characteristic or protected demographic group for whom we want to ensure our model is non-discriminatory. The fairness criteria are then properties of the joint distribution of this characteristic, the output of the classifier, and the true labels of the data.\n\nWhile these fairness definitions have been a useful starting point in the consideration of discrimination by ML models, they have several limitations. 1. They are not equivalent and, in most scenarios, they are incompatible. 2. They apply only to static supervised learning problems and ignore the dynamic environments characteristic of many real world scenarios with fairness concerns. 3. They rely on having access to demographic information. The definitions can only be used if one has access to the sensitive characteristic, which is often not the case. 4. They ignore intersectionality. The criteria do not take into account individuals who may sit at the intersection of several sensitive demographic groups. Fairness is fundamentally a philosophical and political question, and the notion of having a single, universal formal definition of fairness for ML is likely na ̈ıve. For this reason, this work does not attempt to formally define fairness and largely ignores the first problem noted above. We do, however, attempt to address issues 2, 3, and 4 by drawing upon two recent areas of research with implications for fairness in ML: performative prediction and distributionally robust optimization (DRO).\n\nDRO offers a compelling and flexible method for training non-discriminatory algorithms without needing access to demographic information. Performative prediction, on the other hand, attempts to outline a theoretical framework through which we can reason about ML models in dynamic environments, when the act of deploying a model influences the distribution on which it is making decisions.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nWe combine these two areas of research to extend the performative prediction framework developed in Perdomo et al. (2020). The work in performative prediction has thus far only concerned risk minimization and empirical risk minimization (ERM), so we extend definitions to include a distributionally robust objective and prove an analogous convergence result to that shown in Perdomo et al. (2020). Due to space constraints, we provide an extended discussion of related work in section A.1 of the appendix.\n\n2 BACKGROUND\n\n2.1 DISTRIBUTIONALLY ROBUST OPTIMIZATION\n\nThe de facto objective used in most supervised learning settings is ERM. In ERM we attempt to approximate minimization of the expected loss over the data generating distribution by minimizing the average loss over our data set:\n\nˆh = arg min\n\nh∈H\n\n1 n\n\nn (cid:88)\n\ni=1\n\nl(h(xi), yi),\n\nwhere h represents a hypothesis or model, H a hypothesis class, and l a non-negative loss function. ERM is intuitively appealing and has important theoretical guarantees associated with it Vapnik (1991). It can, however, be problematic when it comes to fairness concerns. Since we are averaging the loss over our data points, in general, ERM causes an algorithm to focus on majority cases while ignoring minority cases or rare events.\n\nDRO, on the other hand, considers the distributionally robust problem in which we construct an uncertainty set around the data generating distribution and attempt to minimize the expected loss on the worst-case distribution within this uncertainty set. Following Duchi & Namkoong (2021) we define our uncertainty set as\n\nwhere Df (Q||P ) is an f -divergence between probability distributions Q and P .\n\nUf (P ) = {Q : Df (Q||P ) ≤ ρ},\n\nFormally the distributionally robust problem is as follows:\n\n(cid:26)\n\nminimize θ∈Θ\n\nsup Q≪P0\n\n{EQ[l(θ; X)] : Q ∈ Uf (P0)}\n\n(cid:27)\n\n,\n\nwhere Θ ⊂ Rd is the parameter (model) space, P0 is the data generating distribution on the measure space (X , A), X is a random element of X and l : Θ × X → R is a loss function.\n\nIn this formulation of DRO, the uncertainty set is determined by an f -divergence between Q and P0 and {EQ[l(θ; X)] : Q ∈ Uf (P0)} is the set of all expected losses over the f -divergence ball of radius ρ, centred at P0. Alternative DRO formulations utilizing different measures of distance between probability distributions such as Wasserstein balls have also been explored Wald (1945); Wozabal (2012); Pflug & Wozabal (2007); Lee & Raginsky (2018).\n\nUnlike ERM, DRO does not equally weight each data point, but instead up-weights data points on which the model is achieving high loss. This means that the model should achieve somewhat uniform performance on individuals across demographic groups Duchi & Namkoong (2021); Duchi et al. (2020); Namkoong & Duchi (2016); Hashimoto et al. (2018). Note that the distributionally robust objective does not require any access to demographic information and, at least potentially, naturally accounts for intersectionality.\n\nOne can optimize the distributionally robust objective directly via the primal form, specified above, or alternatively through a dual formulation. For convex losses, the dual form is jointly convex in the parameters of the model and the dual variables Duchi & Namkoong (2021). A detailed discussion of the dual formulation can be found in A.2. The dual formulation also helps provide intuition for why DRO is more likely to result in fair ML models.\n\n2.2 PERFORMATIVE PREDICTION\n\nPerformative prediction attempts to formalize the notion of a model affecting the distribution on which it is making predictions in a type of feedback loop. There are many examples of scenarios with\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nfairness concerns in which this is likely to occur, such as predictive policing or college admission decisions.\n\nThis notion of performativity of an algorithm is captured by a distribution map, D(θ), which maps the data generating distribution to a new distribution as a function of the parameters of the model which has been deployed. In performative prediction, the problem of risk minimization becomes performative risk minimization and involves minimizing the expected loss on the induced distribution rather than the data generating distribution. The performative risk of a model is defined as\n\nP R(θ) = EZ∼D(θ)[l(Z; θ)].\n\nSolution concepts differ for performative prediction and traditional supervised learning, as the distribution shift complicates the learning problem. To capture this distinctive problem, Perdomo et al. (2020) define performative optimality and performative stability. Definition 2.1. (performative optimality) A model, fθP O , is performatively optimal if the following relationship holds:\n\nθP O = arg min\n\nθ∈Θ\n\nEZ∼D(θ)[l(Z; θ)].\n\nEquivalently, θP O = arg minθ∈Θ P R(θ) where P R(θ) is the performative risk as defined above.\n\nA performatively optimal point is a minimizer of the performative risk. An alternative solution concept is referred to as performative stability. Definition 2.2. (performative stability) A model, fθP S , is performatively stable if the following relationship holds:\n\nθP S = arg min\n\nθ∈Θ\n\nEZ∼D(θP S )[l(Z; θ)].\n\nDefine DP R(θ, θ′) := EZ∼D(θ)[l(Z; θ′)] as the decoupled performative risk; arg minθ∈Θ DP R(θP S, θ).\n\nthen θP S =\n\nA performatively stable model is not necessarily a minimizer of the performative risk, but it is optimal on the distribution it induces.\n\nPerformative prediction presents a special case of learning under distribution drift, where the distribution drift is a function of the model that was deployed. A common approach in supervised learning under distribution drift is to retrain a model on newly collected data. While this does not directly minimize performative risk, it is a realistic representation of the strategy taken by many machine learning practitioners and can be a reasonable solution in some scenarios.\n\nDefinition 2.3. (repeated risk minimization) Repeated risk minimization (RRM) refers to the procedure where, starting from an initial model fθ0 , we perform the following sequence of updates for every t ≥ 0:\n\nθt+1 = G(θt) := arg min\n\nθ∈Θ\n\nEZ∼D(θt)[l(Z; θ)].\n\nIn Perdomo et al. (2020) the authors show that under certain smoothness and convexity conditions on the loss function and distribution map, RRM is a contraction mapping that converges to a fixed point which is a performatively stable model. The smoothness condition on the distribution map is called ε-sensitivity and ensures that the distribution shift is relatively small for small changes in the model parameters. Section A.3 contains an extended discussion of performative prediction and formally defines all the relevant concepts.\n\n3 CONVERGENCE OF REPEATED DISTRIBUTIONALLY ROBUST\n\nOPTIMIZATION\n\nIn order to extend the performative prediction framework to encompass DRO, we need to redefine many of the concepts from Perdomo et al. (2020) to account for the distributionally robust objective. The supremum over the uncertainty set introduces additional complications necessitating novel definitions and an altered proof for convergence to a performatively stable model. We define robust performative risk, robust performative optimality, robust performative stability, and repeated distributionally robust optimization (RDRO).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nWe begin by redefining performative risk and performatively optimal and stable models in terms of a robust objective. In the following D(θ) is a distribution map and Df (Q||D(θ)) is an f -divergence ball of radius ρ.\n\nDefinition 3.1. (robust performative risk) The robust performative risk of a model, θ, is\n\nRPR(θ) = sup\n\n{EZ∼Q[l(Z; θ)] : Df (Q||D(θ)) ≤ ρ}.\n\nQ≪D(θ)\n\nRobust performative risk differs from performative risk in that the induced distribution, D(θ), is now the centre of an f -divergence ball over which a supremum of the expected loss is taken.\n\nDefinition 3.2. (robust performative optimality) A model fθP O is robustly performatively optimal if the following relationship holds:\n\nθP O = arg min\n\nθ∈Θ\n\nsup Q≪D(θ)\n\n{EZ∼Q[l(Z; θ)] : Df (Q||D(θ)) ≤ ρ}.\n\nEquivalently, θP O = arg minθ∈Θ RPR(θ) where RPR(θ) is the robust performative risk as defined above.\n\nDefinition 3.3. (robust performative stability) A model fθP S is robustly performatively stable if the following relationship holds:\n\nθP S = arg min\n\nθ∈Θ\n\nsup Q≪D(θP S )\n\n{EZ∼Q[l(Z; θ)] : Df (Q||D(θP S)) ≤ ρ}.\n\nDefine RDPR(θ, θ′) := supQ≪D(θ){EZ∼Q[l(Z; θ′)] : Df (Q||D(θ)) ≤ ρ} as the robust decoupled performative risk; then θP S = arg minθ∈Θ RDPR(θP S, θ).\n\nWe discussed earlier that previous work has shown that repeated risk minimization converges to a performatively stable model under certain assumptions on the loss and distribution map Perdomo et al. (2020). We can analogously define repeated distributionally robust optimization as follows.\n\nDefinition 3.4. (repeated distributionally robust optimization) Repeated distributionally robust optimization (RDRO) refers to the procedure where, starting from an initial model fθ0 , we perform the following sequence of updates for every t ≥ 0:\n\nθt+1 = G(θt) := arg min\n\nθ∈Θ\n\nsup Q≪D(θt)\n\n{EZ∼Q[l(Z; θ)] : Df (Q||D(θt)) ≤ ρ}.\n\nAs with RRM, this is an iterative procedure where we optimize the distributionally robust objective at each time step t. While it is only a small departure from RRM conceptually, the DRO objective has fundamentally different mathematical properties than risk minimization making it unclear whether RDRO and RRM should exhibit similar behaviour.\n\nThe assumptions on the loss function and distribution map required for convergence of RRM do not suffice for RDRO, as the supremum over the uncertainty set alters the mathematical properties of the robust objective. We adapt these conditions to extend them to the distributionally robust objective. The original conditions from Perdomo et al. (2020) can be found in A.3.\n\nDefinition 3.5. (robust β-joint smoothness) We say the distributionally robust objective is robustly β-jointly smooth if for all θ, θ′ ∈ Θ and z, z′ ∈ Z the gradient,\n\n∇θ\n\nsup Q≪D(θ)\n\n{EZ∼Q[l(Z; θ)] : Q ∈ Uf (D(θ))},\n\nexists and is β-Lipschitz in z and θ.\n\nDefinition 3.6. (robust γ-strong convexity) We say the distributionally robust objective is robustly γ-strongly convex if for all θ, θ′ ∈ Θ and z ∈ Z\n\nsup Q≪D(θ)\n\n{EZ∼Q[l(Z; θ)] : Q ∈ Uf (D(θ))}\n\nis γ-strongly convex.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition 3.7. (robust ε-sensitivity) EZ∼Q[l(Z; θ)]. Assume that a distribution map, D(·), is εLet D∗(θ) = arg maxQ:Q∈Uf (D(θ)) sensitive1. We say that this distribution map is robustly ε-sensitive if there exists ω > 0 such that for any θ, θ′ ∈ Θ\n\nW1 (D∗(θ), D∗(θ′)) ≤ ωW1 (D(θ), D(θ′)) ≤ ωε||θ − θ′||,\n\nwhere W1 denotes the Wasserstein-1 distance.\n\nWe now state our convergence theorem for DRO in the performative prediction setting. This theorem says that under the conditions outlined above, repeatedly optimizing a distributionally robust objective will converge to a performatively stable model at a linear rate. The proof can be found in A.4.\n\nTheorem 3.8. Suppose that the distributionally robust objective satisfies definitions 3.5 and 3.6, and that the distribution map satisfies definition 3.7. Then the following statements are true:\n\n1. ||G(θ) − G(θ′)||2 ≤ ωε β\n\nγ ||θ − θ′||2, for all θ, θ′ ∈ Θ\n\n2. If ωε < γ\n\nβ , the iterates θt of RDRO converge to a unique robustly performatively stable\n\npoint θP S at a linear rate: ||θt − θP S||2 ≤ δ for t ≥\n\n(cid:16)\n\n1 − ωε β\n\nγ\n\n(cid:17)−1\n\nlog\n\n(cid:16) ||θ0−θP S ||2\n\n(cid:17)\n\n.\n\nδ\n\nNote that G(θ) := arg minθ∈Θ supQ≪D(θ){EZ∼Q[l(Z; θ)] : Q ∈ Uf (D(θ))}.\n\nThe proof of this theorem shows that RDRO is a contraction mapping and is essentially an application of the Banach fixed point theorem. Definitions 3.6, 3.5, and 3.7 provide smoothness conditions for the mapping sufficient to ensure contraction.\n\nThe importance of this theorem is that it suggests that RDRO should exhibit similar convergence behaviour to RRM. As discussed earlier, however, training models with ERM is liable to result in models which are discriminatory towards minority subgroups, while models trained with DRO should exhibit uniform performance across different subgroups. What this implies is that RRM may converge to unfair fixed points, while RDRO should converge to fair fixed points.\n\n4 EXPERIMENTS\n\nWe run experiments to empirically verify the convergence of RDRO as compared to RRM, as well as to demonstrate the utility of DRO in learning fair models in the presence of data characterized by majority and minority subgroups. Implementation details can be found in A.5.\n\n4.1 RRM AND RDRO ON CREDIT EXTENSION DECISIONS\n\nFor our first experiment we reproduce an experiment from Perdomo et al. (2020) using a credit extension dataset from the Kaggle competition Give Me Some Credit. The dataset contains information about potential borrowers and the task is to predict whether or not an individual will default on their debt. We construct a distribution map by strategically manipulating individuals features in response to the deployed classifier Hardt et al. (2016a).\n\nIn order to add a performative element to the prediction problem, we identify 3 of the 10 features as strategic features which will be altered as a function of the parameters of our model. Strategic classification is a particular instance of performative prediction in which individuals adversarially adjust their features to maximize the likelihood of classification to the positive class. Strategic classification has been explored in relation to fairness concerns in previous work Hardt et al. (2016a); Bruckner et al. (2012); Miller et al. (2020); Milli et al. (2019); D’Amour et al. (2020); Hu et al. (2019).\n\nAs described in Perdomo et al. (2020) and Hardt et al. (2016a), we assume individuals have linear utilities u(θ, x) = −⟨θ, x⟩ and quadratic costs c(x′, x) = 1 2. The constant ε controls the cost individuals incur by altering their features. Individuals thus pay a cost to manipulate their\n\n2ε ||x′ − x||2\n\n1The formal definition of ε-sensitivity is given in A.3.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Plot of the normalized distance between successive values of θ for ERM.\n\nFigure 2: Plot of the normalized distance between successive values of θ for DRO.\n\nfeatures in order to minimize the likelihood of the model predicting that they will default on their loan, but are unable to change the true outcome, y ∈ {0, 1}, of whether or not they default. Given linear utilities and quadratic costs as described here, the individuals’ best response is to manipulate their features as\n\nx′\n\nS = xS − εθS,\n\nwhere xS, x′ tion map is ε-sensitive can be found in Perdomo et al. (2020).\n\nR|S| and |S| is the number of strategic features. The explanation of why this distribu-\n\nS\n\nThe procedure for updating the data according to the distribution map for strategic classification is explained in the box below.\n\nInput: Base distribution P , a classifier fθ, a cost function c, a utility function u. Sampling procedure for D(θ):\n\n1. Sample (x, y) ∼ P 2. Compute best response xBR ← arg maxx′ u(x′, θ) − c(x′, x) 3. Output sample (xBR, y)\n\nWe train two logistic regression models using RRM and RDRO. The magnitude of the distribution shift is controlled by a parameter, ε. The larger the value of ε, the larger the distribution shift. The ε-sensitivity condition of the distribution map is thus only satisfied for small values of ε. Further details are given in A.6.\n\nFigures 1 and 2 contain the distance between the learned parameters of the model on successive iterates. A distance of zero means that the model has converged to a performatively stable point. These figures show us RRM and RDRO exhibit similar convergence behaviour. Both RRM and RDRO converge for ε = 0.01, and RDRO also converges for ε = 1.0. Both RRM and RDRO converge to small neighbourhoods for ε = 10, while neither converge for ε = 100. This result empirically supports our theoretical work, suggesting that RRM and RDRO exhibit similar convergence behaviour.\n\n4.2 STATIC CLASSIFICATION WITH ERM AND DRO\n\nThe credit dataset does not contain any demographic information, so to investigate the fairness properties of the fixed points resulting from RRM and RDRO we generate synthetic data, sampled from two multivariate normal distributions with different means. We refer to the majority sub-probability distribution as group A and the minority distribution as group B. Data from each individual distribution is linearly separable, but the combined distribution is not, meaning that a linear classifier must trade-off performance between the two subgroups.\n\nTo build intuition for the differing behaviour of ERM and DRO on a classification task involving subgroups and fairness concerns, we first investigate a static classification scenario, before exploring long-term fairness via performative prediction.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nOur data is generated from bivariate Gaussian distributions and the label, y, of a given data point is 1 if the sum of its features are greater than the sum of the means of the two Gaussian distributions from which we draw samples and 0 otherwise. We have two subgroups within our data, A and B. We vary the proportion of samples from each subgroup in the experiments. The exact data generating process is given in A.6\n\nAlthough this dataset is extremely simple, it is characterized by a feature that represents a central concern for fairness in machine learning, namely that the conditional probability distributions, P (y|x), are significantly different for distinct subsets of the data. This example is intended to represent a simplified abstract instance of a population with majority and minority subgroups in order to see how the behaviour of ERM and DRO differ in this circumstance.\n\nWe generate three distinct datasets on which to train our algorithms, each made up of differing proportions of the two subgroups A and B. Each dataset contains a sample of 10,000 data points, with samples distributed according the values of cA and cB, parameters which control the proportion of the data coming from each subgroup. The accuracy of the models on the three datasets is summarized in Tables 5 and 6 below. For both ERM and DRO we use L2-regularized logistic regression trained with stochastic gradient descent. The step-size for all algorithms is fixed at 0.05 and we train for 15,000 epochs.\n\nModels trained with the distributionally robust objective have the additional complication that we must specify a value for the radius of the χ2-divergence ball, i.e. ρ. The larger the value of ρ, the more we can expect a DRO model to differ from an ERM model because as the χ2-divergence ball grows, the worst case distribution can be further and further from the data generating distribution. Conversely, in the limit as ρ → 0, we recover ERM as the χ2-divergence ball shrinks to contain only the data generating distribution.\n\nChoosing the value of ρ is a challenging decision, as the performance of a model varies significantly as ρ changes. If one has access to demographic information, it is possible to conduct a grid search over possible ρ values in order to find a value that results in a model with the desirable fairness properties. Doing this, however, largely defeats the purpose of DRO. As explained earlier, a central advantage to using DRO, rather than some fairness constrained optimization technique, is that DRO does not require access to demographic information.\n\nIn this experiment we work directly with the dual formulation of DRO and set η = 0.56. The dual formulation of DRO is explained in A.2 This value was chosen empirically to achieve relatively uniform accuracy across group A and group B for an 80/20 split between the two subgroups. As the values of cA and cB change, we can see that the performance of DRO changes for a given value of η and hence ρ, as η∗ depends on ρ.\n\nObjective ERM\n\nDRO\n\nGroup A\nB All Data A\nB All Data\n\n[cA = 0.6, cB = 0.4] 0.797 0.701 0.759 0.665 0.869 0.747\n\n[cA = 0.8, cB = 0.2] 0.907 0.652 0.856 0.751 0.744 0.750\n\n[cA = 0.95, cB = 0.05] 0.966 0.592 0.948 0.780 0.766 0.780\n\nTable 1: Accuracy by Group for ERM and DRO.\n\nWe first examine the performance of ERM (Table 1). As the data is not linearly separable, ERM must learn a decision boundary that trades off performance between the two subgroups. Because the ERM objective treats the loss on each data point equally, the model learns a decision boundary that is more accurate for the majority group than for the minority group. This discrepancy in accuracy of predictions worsens the smaller the majority group is. For instance, when 95% of the data comes from group A, the logistic regression model trained with an ERM objective achieves 96% accuracy on group A members, but only 59.2% accuracy on group B members.\n\nModels trained with a DRO objective behave much differently (Table 1). For the 80/20 split and 95/5 split, the DRO models learn relatively fair decision boundaries, effectively balancing perfor-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Group A, DRO.\n\nFigure 4: Group B, DRO.\n\nFigure 5: Group A, ERM.\n\nFigure 6: Group B, ERM.\n\nFigure 7: Decision boundaries for ERM and DRO classifiers with samples from groups A and B with cA = 0.95 and cB = 0.05. Shading indicates predicted label and data point colour indicates true label.\n\nmance on both subgroups A and B. For the 60/40 split, however, DRO actually learns a model that performs significantly better on the minority group than the majority group. This model is in a sense discriminatory against the majority group. This is the result of the radius of the χ2-divergence ball being too large and the model thus overly focusing on a worst case distribution.\n\nIn Figures 3, 4, 5, and 6 we plot data from groups A and B for data generated with cA = 0.95, cB = 0.05 with the learned decision boundary from a DRO and ERM model, respectively, overlaid. The background shading indicates the predicted label with blue representing ˆy = 0 and red representing ˆy = 1, while the colour of the data points indicate their true label.\n\n4.3 RRM AND RDRO WITH SUBGROUPS\n\nWe have seen empirical confirmation of convergence of RDRO and RRM in our first experiment, and the static classification with subgroups has demonstrated the potential for DRO as a tool to achieve fairness in ML models, but neither of these address the question of long term fairness under dynamically changing distributions. To investigate this question, we generate data in a similar fashion to the static classification experiment (details in A.6), and add a performative element to the model.\n\nThe data in this experiment is composed of a mixture of multivariate normal distributions with different means. 80% of the data is drawn from the majority subgroup, while the remaining 20% is drawn from the minority subgroup. We again implement a strategic manipulation distribution map and train models using RRM and RDRO for 30 iterations. As we saw with the Give Me Some Credit dataset, for small values of ε we observe convergence, whereas for larger values of ε we do not observe convergence. Plots illustrating this behaviour can be found in A.6.\n\nThe accuracy of the models for the values of ε for which we observe convergence are given in Tables 4.3 and 4.3. We can see that RRM converges to a performatively stable model that achieves high\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nERM Performative Accuracy\n\nGroup A\nB All Data\n\nε = 0.01 0.893 0.540 0.834\n\nε = 0.25 0.896 0.540 0.837\n\nε = 0.5 0.898 0.540 0.838\n\nDRO Performative Accuracy ε = 0.25 0.710 0.660 0.701\n\nε = 0.01 0.687 0.670 0.684\n\nε = 0.5 0.738 0.660 0.725\n\nTable 2: Accuracy by Group for ERM and DRO after 30 iterations.\n\naccuracy on the majority group, but performs poorly on the minority group. RDRO on the other hand, results in models that perform similarly across both subgroups in the data.\n\nThis result, while perhaps not that surprising in light of the two previous experiments, is important, as it demonstrates that not only does DRO exhibit similar convergence behaviour to ERM, but DRO converges to fair fixed points, whereas ERM converges to discriminatory fixed points in the presence of heterogeneous data composed of minority and majority subgroups. Recall also that DRO is not given access to group information, but still learns to achieve more uniform performance across subgroups, as it is attempting to minimize the worst case loss across all probability distributions within the χ2-divergence ball surrounding the data generating distribution. Further details of this experiment, as well as additonal experiments are provided in A.6.\n\n5 CONCLUSION AND FUTURE WORK\n\nPerformative prediction offers a rigorous framework in which to reason about decision making in dynamic environments. Existing theory for performative prediction had only been developed for models trained with ERM objectives, however, which is liable to result in models that discriminate against minority groups.\n\nWe extend the performative prediction framework to DRO and prove a convergence result for RDRO. We also investigate RRM and RDRO empirically and find that while they exhibit similar convergence behaviour, RRM converges to fixed points that perform well on a majority subgroup, but poorly on a minority subgroup, whereas RDRO converges to models which successfully balance performance across majority and minority subgroups.\n\nWe hope that this work helps to further our still very limited understanding of fairness properties of ML algorithms in changing and dynamic environments, which charactierize many real-world settings with fairness concerns. By combining performative prediction with a distributionally robust objective, we have tried to overcome some of the shortcomings of the dominant fairness criteria and to offer a flexible and general tool to ML practitioners and researchers concerned with ensuring that algorithms do not discriminate against vulnerable minority groups.\n\nThis work represents a preliminary investigation into the combination of DRO and performative prediction as a method to understand and learn fair algorithms, and, as such, leaves many important avenues for future work open. Further theoretical investigation of the convergence properties of RDRO would yield better understanding of the distributionally robust objective, as this work eschewed answering important questions of stability of worst case distributions as well as questions of smoothness and strong convexity of distributionally robust objectives.\n\nIt would also be very interesting to investigate both the convergence properties and practical efficacy of a parametric DRO objective, rather than an f -divergence based robust objective. Recent work suggests that parametric DRO can outperform f -divergence based DRO and offers practitioners and theoreticians greater flexibility than with analytic DRO Michel et al. (2022; 2021).\n\nAdditionally, performative prediction and distributionally robust optimization have connections and similarities to many other well established areas of research in machine learning, and it is likely that existing results in areas such as bandits, reinforcement learning, online learning, stochastic optimization, and out of distribution generalization could greatly contribute to our understanding of DRO and performative prediction.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAJ Abdallat. A push for digital transformation: The global ai investment boom. 2021.\n\nTara Balakrishnan, Michael Chui, Bryce Hall, and Nicolaus Henke. The state of ai in 2020. Mckinsey\n\nAnalytics, 2020.\n\nSolon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning.\n\nfairml-\n\nbook.org, 2019. http://www.fairmlbook.org.\n\nA. Ben-Tal, L. El Ghaoui, and A.S. Nemirovski. Robust Optimization. Princeton Series in Applied\n\nMathematics. Princeton University Press, 2009.\n\nAharon Ben-Tal, Dick den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341–357, 2013. ISSN 0025-1909.\n\nRichard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, 50(1):3–44, 2021.\n\nStephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 3\n\n2004. ISBN 0521833787.\n\nGavin Brown, Shlomi Hod, and Iden Kalemaj. Performative prediction in a stateful world. 2020.\n\nMichael Bruckner, Christian Kanzow, and Tobias Scheffer. Static prediction games for adversarial\n\nlearning problems. Journal of Machine Learning Research, 13(85):2617–2654, 2012.\n\nAlexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism\n\nprediction instruments. Big data, 5 2:153–163, 2017.\n\nSam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. KDD ’17, pp. 797–806. Association for Computing Machinery, 2017. ISBN 9781450348874.\n\nI. Csiszar. Information-type measures of difference of probability distributions and indirect obser-\n\nvation. Studia Scientifica Mathematica, 2:299–318, 1967.\n\nI. Csisz ́ar and P.C. Shields. Information theory and statistics: A tutorial. Foundations and Trends in\n\nCommunications and Information Theory, 1(4):417–528, 2004. ISSN 1567-2190.\n\nAlexander D’Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D. Sculley, and Yoni Halpern. Fairness is not static: Deeper understanding of long term fairness via simulation studies. FAT* ’20, pp. 525–534, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450369367.\n\nRoy Dong and Lillian J. Ratliff. Approximate regions of attraction in learning with decision-\n\ndependent distributions, 2021.\n\nJohn C. Duchi and Hongseok Namkoong. Learning models with uniform performance via distribu-\n\ntionally robust optimization. The Annals of Statistics, 49(3):1378 – 1406, 2021.\n\nJohn C. Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses for\n\nlatent covariate mixtures. CoRR, abs/2007.13982, 2020.\n\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness In Proceedings of the 3rd Innovations in Theoretical Computer Science ISBN\n\nthrough awareness. Conference, ITCS ’12, pp. 214–226. Association for Computing Machinery, 2012. 9781450311151.\n\nMoritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classiIn Proceedings of the 2016 ACM Conference on Innovations in Theoretical ComISBN\n\nfication. puter Science, ITCS ’16, pp. 111–122. Association for Computing Machinery, 2016a. 9781450340571.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMoritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In\n\nNIPS, 2016b.\n\nTatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness with-\n\nout demographics in repeated loss minimization. In ICML, 2018.\n\nLily Hu, Nicole Immorlica, and Jennifer Wortman Vaughan. The disparate effects of strategic manipulation. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, pp. 259–268, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255.\n\nSookyo Jeong and Hongseok Namkoong. Robust causal inference under covariate shift via worstcase subpopulation treatment effects. In Jacob Abernethy and Shivani Agarwal (eds.), Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp. 2079–2084. PMLR, 2020.\n\nJon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In Christos H. Papadimitriou (ed.), 8th Innovations in Theoretical Computer Science Conference (ITCS 2017), volume 67 of Leibniz International Proceedings in Informatics (LIPIcs), pp. 43:1–43:23. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2017. ISBN 978-3-95977-029-3.\n\nChristian Kroer. Lecture Notes for Economics, AI, and Optimization. 2022.\n\nJaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nCelestine Mendler-D ̈unner, Juan Perdomo, Tijana Zrnic, and Moritz Hardt. Stochastic optimization for performative prediction. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 4929–4939. Curran Associates, Inc., 2020.\n\nPaul Michel, Tatsunori Hashimoto, and Graham Neubig. Modeling the second player in distributionally robust optimization. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n\nPaul Michel, Tatsunori Hashimoto, and Graham Neubig. Distributionally robust models with paraIn The Tenth International Conference on Learning Representations,\n\nmetric likelihood ratios. ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\n\nJohn Miller, Smitha Milli, and Moritz Hardt. Strategic classification is causal modeling in disguise. In Hal Daum ́e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 6917–6926. PMLR, 2020.\n\nJohn Miller, Juan Carlos Perdomo, and Tijana Zrnic. Outside the echo chamber: Optimizing the\n\nperformative risk. International Conference on Machine Learning (ICML) 2021, 2021.\n\nSmitha Milli, John Miller, Anca D. Dragan, and Moritz Hardt. The social cost of strategic classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, pp. 230–239. Association for Computing Machinery, 2019. ISBN 9781450361255.\n\nHongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.\n\nHongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJuan Perdomo, Tijana Zrnic, Celestine Mendler-D ̈unner, and Moritz Hardt. Performative prediction. In Hal Daum ́e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7599–7609. PMLR, 2020.\n\nGeorg Pflug and David Wozabal. Ambiguity in portfolio selection. Quantitative Finance, 7(4):\n\n435–442, 2007.\n\nAlfred Renyi. On measures of entropy and information. In The 4th Berkeley Symposium on Mathe-\n\nmatics, Statistics and Probability, pp. 547–561. University of California Press, 1961.\n\nAlexander Shapiro. Distributionally robust stochastic programming. SIAM Journal on Optimization,\n\n27(4):2258–2275, 2017.\n\nV. Vapnik. Principles of risk minimization for learning theory.\n\nIn Proceedings of the 4th International Conference on Neural Information Processing Systems, NIPS’91, pp. 831–838. Morgan Kaufmann Publishers Inc., 1991. ISBN 1558602224.\n\nAbraham Wald. Statistical decision functions which minimize the maximum risk. Annals of Math-\n\nematics, 46:265–280, 1945.\n\nBlake Woodworth, Suriya Gunasekar, Mesrob I. Ohannessian, and Nathan Srebro. Learning nonIn Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 discriminatory predictors. Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pp. 1920–1953. PMLR, 2017.\n\nDavid Wozabal. A framework for optimization under ambiguity. Annals of Operations Research,\n\n193(1):21–47, 2012.\n\nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment and disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, WWW ’17, pp. 1171–1180. International World Wide Web Conferences Steering Committee, 2017. ISBN 9781450349130.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 RELATED WORK\n\nThe two most influential areas of research for this work are performative prediction and distributionally robust optimization, and the papers most important for this work are Perdomo et al. (2020) and Duchi & Namkoong (2021). Perdomo et al. develop the performative prediction framework, while Duchi and Namkoong give a thorough summary of distributionally robust optimization and explain its potential application to fairness concerns in machine learning.\n\nFollowing the publication of Performative Prediction Perdomo et al. (2020), there have been a number of papers which have further explored performative prediction and extended some of the results from the original paper. For instance, Mendler-D ̈unner et al. (2020) prove results for stochastic optimization in performative prediction, Miller et al. (2021) prove new results relating performatively stable points to performatively optimal points, Brown et al. (2020) attempt to move toward sequential games for performative prediction by adding a notion of state to the performative prediction problem, and Dong & Ratliff (2021) approach performative prediction from a dynamical systems perspective allowing a move away from strict contraction mappings when examining convergence to performatively stable models.\n\nDistributionally robust optimization is an older area of research as compared to performative prediction, but it has only recently received more attention within the machine learning community, largely due to work by Hongseook Namkoong and John Duchi Namkoong & Duchi (2017); Duchi et al. (2020); Jeong & Namkoong (2020). Areas such as finance, where robust optimization is important as rare events have the potential to be catastrophic for a portfolio, have seen research on robust optimization for decades. See Ben-Tal et al. (2009) for a survey. Ben-Tal et al. (2013) Shapiro (2017) provide rigorous mathematical treatments of DRO with f -divergence balls.\n\nA number of recent papers have also attempted to better understand fairness in machine learning over time, albeit not within the performative prediction framework. Closely related to performative prediction, strategic classification Hardt et al. (2016a) models the prediction problem as an adversarial game where individuals manipulate their features in response to deployed models. Recent work Milli et al. (2019); Hu et al. (2019) has examined how strategic classification interacts with fairness concerns. Strategic classification is an interesting problem setting, but is not as general as performative prediction and only captures scenarios that can rightly be modelled as adversarial games. D’Amour et al. (2020) on the other hand, approach long-term fairness questions from a purely empirical perspective and use simulation studies to understand the long-term dynamics of algorithmic choices on populations in a variety of scenarios designed to reflect real-world applications. This approach is interesting, but the lack of theoretical framework limits their ability to generalize beyond specific simulations.\n\nFinally, the work most closely related to this paper, Hashimoto et al. (2018), examines the impact of repeatedly minimizing loss and deploying a model using empirical risk minimization and distributionally robust optimization on a population that changes as a function of the loss incurred. This work, however, does not fit into the performative prediction framework and applies only to a specific scenario in which individuals arrive according to a Poisson process and depart as a function of the loss. This research aims to extend this work by combining DRO and performative prediction in order to leverage the known results in both areas to provide a more general understanding of how DRO can impact long-term fairness.\n\nA.2 DISTRIBUTIONALLY ROBUST OPTIMIZATION\n\n2. An f -divergence, The notation Q ≪ P0 means that Q is absolutely continuous with respect to P0 Df (Q||P ), is a function that measures the difference between two probability distributions Q and P , although it is not a metric. There are a variety of different f -divergences, but their general definition is as follows Renyi (1961); Csisz ́ar & Shields (2004); Csiszar (1967).\n\n2If two measures, μ and ν, are on the same measure space (X , A), μ is said to be absolutely continuous\n\nwith respect to ν if μ(A) = 0 for every set A for which ν(A) = 0.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nDf (Q||P ) := (cid:82) f ( dQ If Q and P are absolutely continuous with respect to a reference distribution μ, then their probability densities q and p satisfy dP = p and dQ = q and the f -divergence can be written as\n\ndP )dP where f is a convex function such that f (1) = 0.\n\nDf (Q||P ) =\n\n(cid:90)\n\nf (\n\nq(x) p(x)\n\n)dμ(x).\n\nSome examples of f -divergences are KL-divergence (f (t) = t log t), Pearson χ2-divergence (f (t) = (t − 1)2, t2 − 1, t2 − t), and total variation distance (f (t) = 1 2 |t − 1|). An f -divergence ball centred at P of radius ρ is a set that contains all probability distributions whose f -divergence is within ρ of P . For example, {Q : Df (Q||P0) ≤ ρ} is an f -divergence ball that contains all probability distributions Q that are within ρ f -divergence distance from P0. This is analogous to the notion of an ε-ball surrounding a vector in a vector space.\n\nThe distributionally robust problem therefore, is to find a set of parameters, θ ∈ Θ, that minimize the worst case expected loss of all probability distributions Q that are within the f -divergence ball of radius ρ of our data generating distribution. This differs from ERM in that we do not seek to minimize the expected loss over our data generating distribution, but instead seek to minimize the worst case expected loss over a set of probability distributions nearby our data generating distribution.\n\nAs the name suggests, learning parameters that minimize the distributionally robust risk gives us a model that is robust to changes in the data generating distribution. Traditional risk minimization will return a model that minimizes risk for the data generating distribution, but offers no performance guarantees when that distribution changes, and can result in models that are brittle and do not perform well on out of distribution (OOD) examples. DRO, on the other hand, is a more conservative procedure that minimizes worst-case risk and should thus be robust to changes to the probability generating distribution that lie within the f -divergence ball of radius ρ.\n\nAs with traditional risk minimization, we do not have access to the theoretical data generating distribution. Instead, we solve the distributionally robust problem via the plug-in estimator\n\n(cid:40)\n\n(cid:41)\n\nˆθn ∈ arg min\n\nθ∈Θ\n\nRf (θ, ˆPn) := sup Q≪ ˆPn\n\n{EQ[l(θ; X)] : Df (Q|| ˆPn) ≤ ρ}\n\nwhere ˆPn is the empirical measure on Xi ∼iid P0. Duchi and Namkoong prove convergence guarantees and rates for the plug-in estimator along with some asymptotic results such as showing consistency of the estimator Duchi & Namkoong (2021).\n\nIt is not necessarily clear from the discussion so far what DRO has to do with fairness concerns in machine learning, but DRO in fact has characteristics that are very desirable for addressing issues of fairness and discrimination in learning algorithms. We will explain this shortly, but first we introduce dual reformulations of the distributionally robust problem as it provides some intuition as to the connection with fairness concerns.\n\nThe following theorem comes from Shapiro (2017). Let f ∗(s) := supt{st − f (t)} be the Fenchel conjugate.\n\nTheorem A.1. ([Shapiro (2017), Section 3.2]). Let P be a probability measure on (X , A) and ρ > 0. Then\n\nRf (θ; P ) = inf\n\nλ≥0,η∈R\n\n(cid:26)\n\n(cid:20)\n\nEP\n\nλf ∗\n\n(cid:18) l(θ; X) − η λ\n\n(cid:19)(cid:21)\n\n(cid:27)\n\n+ λρ + η\n\nfor all θ. Moreover, if the supremum on the left hand side is finite, there are finite λ(θ) ≥ 0 and η(θ) ∈ R attaining the infimum on the right hand side.\n\nAdditionally, Duchi & Namkoong (2021) provide a simplified version of this dual formulation for the Cressie-Read family of f -divergences, obtained by minimizing out λ > 0 from Theorem A.1.\n\nTheorem A.2. ([Duchi & Namkoong (2021), Section 2]). For any probability P on (X , A), k ∈ (1, ∞), k∗ = k/(k − 1), any ρ > 0, and ck(ρ) = (1 + k(k − 1)ρ)1/k, we have for all θ ∈ Θ\n\nRf (θ; P ) = inf η∈R\n\n(cid:26)\n\nck(ρ)EP\n\n(cid:104) (l(θ; X) − η)k∗\n\n+\n\n(cid:105)1/k∗\n\n+ η\n\n(cid:27)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nThe above formulations are jointly convex in (θ, λ, η) and (θ, η), respectively, for convex losses, l(θ; X), making them amenable to techniques from convex optimization such as interior point methods Boyd & Vandenberghe (2004).\n\nThe Cressie-Read family of f -divergences are parameterized by k ∈ (−∞, ∞) \\ {0, 1}, k∗ = k with\n\nk−1 ,\n\nfk(t) :=\n\ntk − kt + k − 1 k(k − 1)\n\nand\n\nf ∗\n\nk (s) :=\n\n(cid:104) ((k − 1)s + 1)k∗\n\n(cid:105) + − 1\n\n.\n\n1 k\n\nIssues of fairness arise in machine learning when an algorithm treats different demographic groups in a disparate fashion. As outlined above, this often means that algorithmic performance varies across different demographic groups. If this occurs, it is almost certainly the case that there exist distinct probability distributions over the demographic groups because if all demographic groups were characterized the the same probability distribution, classification algorithms should exhibit uniform performance across demographic groups.\n\nThis realization makes it easy to see why ERM is likely to discriminate, particularly against minority groups. ERM treats the loss on each data point equally, thus, if there is a majority and minority probability distribution and a learning algorithm must balance performance on these groups, an ERM objective is likely to result in a model that performs better on the majority group and worse on the minority group. It is exactly this type of problem that DRO is designed to resolve.\n\nUnlike ERM, DRO does not equally weight each data point, but instead up-weights data points on which the model is achieving high loss. This means that the model should achieve somewhat uniform performance on individuals across demographic groups. This can be seen by looking at the dual formulation in Theorem A.2. The DRO objective only considers losses above the optimal dual variable η∗(θ) and these losses are up-weighted by the Lk∗ (P )-norm. Losses that are less than the optimal dual variable are set to zero in the objective. Another way of saying this is that the DRO objective is equivalent to optimizing the tail-performance of a model Duchi & Namkoong (2021).\n\nIf DRO performs poorly on a subset of the data that is correlated with a distinct demographic group, these losses will be up-weighted by the dual formulation of the distributionally robust objective, pushing the model to improve performance on this subset. DRO does not offer any guarantees of uniform performance across demographic groups, but as we increase the value of ρ we will increase the loss incurred on “hard” regions of the data where the model performs poorly.\n\nA.3 PERFORMATIVE PREDICTION\n\nIn supervised learning we assume that our data is sampled i.i.d. from an unknown data generating distribution and that our model is then deployed to make predictions on data that follows this same distribution. In many scenarios, however, the very act of making predictions influences the data on which we wish to make these predictions. That is to say, our models are performative and instead of passively describing the world and making predictions about it, they actually induce change in the world.\n\nA simple example of this is predictive policing. In predictive policing we train a model to predict where crimes are likely to occur based on historical data and then deploy more resources to areas where the model predicts crimes are more likely to occur. The increased police patrols and surveillance results in more crimes being detected which might further increase the perceived crime rate in those areas. If this data is then used for future predictions it will result in a shifted distribution of the data as a result of the predictions of the previous model. Another example of performativity is an algorithm that weights different elements of a student’s CV such as SAT scores and GPA in order to make college admissions decisions. If the algorithm heavily weights SAT scores, over time it will become apparent to applicants that SAT scores are very important and they will dedicate more resources to improving SAT scores, thus changing the distribution of the data on which the algorithm is making predictions as a result of those very predictions.\n\nPerformative prediction is closely related to many other fields in machine learning including bandits, reinforcement learning, strategic classification, causal inference, convex optimization, and game theory, but the precise notion and formalism for performative prediction was only developed very\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nrecently in Perdomo et al. (2020). We formally specify the performative prediction problem now and contrast it with the supervised learning problem.\n\nAssume we have a measure space (Z, A) with Z a random element of Z and D the data generating distribution on this space. Let Θ ⊂ Rd be the parameter (model) space and l : Θ × Z → R be a loss function. The supervised learning problem is to minimize the the objective over this distribution. In the case of risk minimization the objective is the expected loss, l(Z; θ) with respect to D.\n\nR(θ) = EZ∼D[l(Z; θ)]. In contrast to this, performative prediction involves making predictions on a distribution that has been shifted as a result of deploying the model, D(θ). We refer to D(θ) as the distribution map. The concept that captures this notion of risk is known as performative risk and is formalized as follows: P R(θ) = EZ∼D(θ)[l(Z; θ)]. The difference between this and the supervised learning problem is that expected loss is now taken with respect to the induced distribution rather than the data generating distribution.\n\nThe notion of what constitutes a good model is different in supervised learning and performative prediction. In supervised learning the task is simpler - minimize the risk on the data generating distribution. In performative prediction however, we now need to consider how to minimize risk on a distribution that is different from that which generated our training data, and is in fact a function of whatever model we deploy. To capture these notions, Perdomo et al. (2020) define performative optimality and performative stability. Definition A.3. (performative optimality) A model fθP O is performatively optimal if the following relationship holds:\n\nθP O = arg min\n\nθ∈Θ\n\nEZ∼D(θ)[l(Z; θ)].\n\nEquivalently, θP O = arg minθ∈Θ P R(θ) where P R(θ) is the performative risk as defined above.\n\nA performatively optimal point is a minimizer of the performative risk. An alternative solution concept is referred to as performative stability. Definition A.4. (performative stability) A model fθP S is performatively stable if the following relationship holds:\n\nθP S = arg min\n\nθ∈Θ\n\nEZ∼D(θP S )[l(Z; θ)].\n\nDefine DP R(θ, θ′) := EZ∼D(θ)[l(Z; θ′)] as the decoupled performative risk; arg minθ∈Θ DP R(θP S, θ).\n\nthen θP S =\n\nA performatively stable model is not necessarily a minimizer of the performative risk, but it is optimal on the distribution it induces. Hence, if you have performative stability there is no need to retrain a model to cope with the induced distribution drift. Performative optimality and performative stability are distinct concepts and a performatively optimal point is not necessarily performatively stable, and vice versa. As explained in Perdomo et al. (2020), performatively stable models are fixed points of risk minimization. In game theoretic terms, we can consider performative prediction as a game in which one player deploys a model, θ, and the environment responds with some distribution map, D(θ). If D(θ) is a best response, then a performatively optimal point corresponds to a Stackelberg equilibrium, whereas a performatively stable point corresponds to a Nash equilibrium. From game theory we know that except in special cases (e.g. finite zero-sum games), Nash equilibria and Stackelberg equilibria do not necessarily coincide Kroer (2022).\n\nPerformative prediction presents a special case of learning under distribution drift, where the distribution drift is a function of the model deployed. A common approach in supervised learning under distribution drift is to retrain a model on newly collected data. While this does not directly minimize performative risk, it is a potentially reasonable solution in a variety of scenarios. Perdomo et al. (2020) prove theorems under which repeated risk minimization and repeated gradient descent converge to performatively stable models. We present these findings in detail here as they are relevant for our work. Definition A.5. (ε-sensitivity) We say that a distribution map D(·) is ε-sensitive if for all θ, θ′ ∈ Θ: W1 (D(θ), D(θ′)) ≤ ε||θ − θ′||2,\n\nwhere W1 denotes the Wasserstein-1 distance.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nWe also make assumptions on the loss function l(z; θ). Let Z := ∪θ∈Θsupp(D(θ)). Definition A.6. (joint smoothness) We say that a loss function l(z; θ) is β-jointly smooth if the gradient ∇θ is β-Lipschitz in θ and z, that is\n\n||∇θl(z; θ) − ∇θl(z; θ′)||2 ≤ β||θ − θ′||2,\n\n||∇θl(z; θ) − ∇θl(z′; θ)||2 ≤ β||z − z′||2,\n\nfor all θ, θ′ ∈ Θ and z, z′ ∈ Z\n\nDefinition A.7. (strong convexity) We say that a loss function l(z; θ) is γ-strongly convex if\n\nl(z; θ) ≥ l(z; θ′) + ∇θl(z; θ′)T (θ − θ′) +\n\nγ 2\n\n||θ − θ′||2 2,\n\nfor all θ, θ′ ∈ Θ and z ∈ Z. If γ = 0 this condition is equivalent to convexity.\n\nDefinition A.8. (repeated risk minimization) Repeated risk minimization (RRM) refers to the procedure where, starting from an initial model fθ0 , we perform the following sequence of updates for every t ≥ 0:\n\nθt+1 = G(θt) := arg min\n\nθ∈Θ\n\nEZ∼D(θt)[l(Z; θ)].\n\nDefinitions A.5, A.6, and A.7 are assumptions on the loss and distribution map that are required for Theorem 3.5 in Perdomo et al. (2020). We state that theorem now.\n\nTheorem A.9. ([Perdomo et al. (2020), Theorem 3.5]) Suppose that the loss l(z; θ) is β-jointly smooth and γ-strongly convex. If the distribution map D(·) is ε-sensitive, then the following statements are true:\n\n1. ||G(θ) − G(θ′)||2 ≤ ε β\n\nγ ||θ − θ′||2, for all θ, θ′ ∈ Θ\n\n2. If ε < γ\n\nβ , the iterates θt of RRM converge to a uniquely performatively stable point θP S at\n\na linear rate: ||θt − θP S||2 ≤ δ for t ≥\n\n(cid:16)\n\n1 − ε β\n\nγ\n\n(cid:17)−1\n\nlog\n\n(cid:16) ||θ0−θP S ||2\n\n(cid:17)\n\nδ\n\nThis theorem says that with the appropriate smoothness and convexity conditions satisfied on the loss and distribution map, repeatedly retraining a model by minimizing risk will converge to a unique performatively stable model at a linear rate. The proof proceeds by demonstrating that under the assumptions stated in Theorem A.9, the RRM operator is a contraction mapping. Perdomo et al. (2020) also show that without any of ε-sensitivty, β-joint smoothness, or γ-strong convexity one can produce examples that do not converge to a fixed point. Hence these assumptions are necessary conditions to guarantee convergence of RRM to a performatively stable model in the general case.\n\nA.4 DEFINITIONS AND PROOF FOR DISTRIBUTIONALLY ROBUST PERFORMATIVE\n\nPREDICTION\n\nWe begin by redefining performative risk and performatively optimal and stable models in terms of a robust objective. In the following D(θ) is a distribution map and Df (Q||D(θ)) is an f -divergence ball of radius ρ.\n\nDefinition A.10. (robust performative risk) The robust performative risk of a model, θ, is\n\nRP R(θ) = sup\n\n{EZ∼Q[l(Z; θ)] : Df (Q||D(θ)) ≤ ρ}.\n\nQ≪D(θ)\n\nRobust performative risk differs from performative risk in that the induced distribution, D(θ), is now the centre of an f -divergence ball over which a supremum of the expected loss is taken.\n\nDefinition A.11. (robust performative optimality) A model fθP O is robustly performatively optimal if the following relationship holds:\n\nθP O = arg min\n\nθ∈Θ\n\nsup Q≪D(θ)\n\n{EZ∼Q[l(Z; θ)] : Df (Q||D(θ)) ≤ ρ}.\n\nEquivalently, θP O = arg minθ∈Θ RP R(θ) where RP R(θ) is the robust performative risk as defined above.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition A.12. (robust performative stability) A model fθP S is robustly performatively stable if the following relationship holds:\n\nθP S = arg min\n\nθ∈Θ\n\nsup Q≪D(θP S )\n\n{EZ∼Q[l(Z; θ)] : Df (Q||D(θP S)) ≤ ρ}.\n\nDefine RDP R(θ, θ′) := supQ≪D(θ){EZ∼Q[l(Z; θ′)] : Df (Q||D(θ)) ≤ ρ} as the robust decoupled performative risk; then θP S = arg minθ∈Θ RDP R(θP S, θ).\n\nWe discussed earlier that previous work has shown that repeated risk minimization converges to a performatively stable model under certain assumptions on the loss and distribution map Perdomo et al. (2020). We can analogously define repeated distributionally robust optimization as follows.\n\nDefinition A.13. (repeated distributionally robust optimization) Repeated distributionally robust optimization (RDRO) refers to the procedure where, starting from an initial model fθ0, we perform the following sequence of updates for every t ≥ 0:\n\nθt+1 = G(θt) := arg min\n\nθ∈Θ\n\nsup Q≪D(θt)\n\n{EZ∼Q[l(Z; θ)] : Df (Q||D(θt)) ≤ ρ}.\n\nAs with RRM, this is an iterative procedure where we optimize the distributionally robust objective at each time step t. While it is only a small departure from RRM conceptually, the DRO objective has fundamentally different mathematical properties than risk minimization making it unclear whether RDRO and RRM should exhibit similar behaviour.\n\nWe now re-state and prove our convergence theorem for RDRO. The proof of this theorem is a straightforward adaptation of the proof in Perdomo et al. (2020). We first introduce two lemmas used in Perdomo et al. (2020) which we will make use of in our proof.\n\nLemma A.14. (First-order optimality condition) Let f be convex and let Ω be a closed convex set on which f is differentiable, then\n\nx∗ ∈ arg min\n\nx∈Ω\n\nf (x)\n\nif and only if\n\n∇f (x∗)T (y − x∗) ≥ 0, ∀y ∈ Ω. Lemma A.15. (Kantorovich-Rubinstein) A distribution map D(·) is robustly ε-sensitive if and only if for all θ, θ′ ∈ Θ:\n\nsup (cid:8)(cid:12)\n\n(cid:12)EZ∼D∗(θ)[g(Z)] − EZ∼D∗(θ′)[g(Z)](cid:12) EZ∼Q[l(Z; θ)].\n\narg max Q:Df (Q||D(θ))≤ρ\n\nwhere D∗(θ) =\n\n(cid:12) : g : RP → R, g 1-Lipschitz(cid:9) ≤ ωε||θ − θ′||2\n\nWe now state our theorem.\n\nTheorem A.16. Suppose that the distributionally robust objective satisfies definitions 3.5 and 3.6, and that the distribution map satisfies definition 3.7. Then the following statements are true:\n\n1. ||G(θ) − G(θ′)||2 ≤ ωε β\n\nγ ||θ − θ′||2, for all θ, θ′ ∈ Θ\n\n2. If ωε < γ\n\nβ , the iterates θt of RDRO converge to a unique robustly performatively stable\n\npoint θP S at a linear rate: ||θt − θP S||2 ≤ δ for t ≥\n\n(cid:16)\n\n1 − ωε β\n\nγ\n\n(cid:17)−1\n\nlog\n\n(cid:16) ||θ0−θP S ||2\n\n(cid:17)\n\n.\n\nδ\n\nNote that G(θ) := arg minθ∈Θ supQ≪D(θ){EZ∼Q[l(Z; θ)] : Df (Q||D(θ)) ≤ ρ}.\n\nProof. Fix θ, θ′ ∈ Θ. Let\n\nD∗(θ) =\n\narg max Q:Df (Q||D(θ))≤ρ\n\nEZ∼Q[l(Z; θ)].\n\nThat is, D∗(θ) is the distribution within the f -divergence ball centred at D(θ) with radius ρ that maximizes the expected loss. Further, let\n\nf (ξ) = EZ∼D∗(θ)[l(Z; ξ)]\n\nand\n\nf ′(ξ) = EZ∼D∗(θ′)[l(Z; ξ)].\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nThat is, f (ξ) and f ′(ξ) are the worst case losses for f -divergence balls centred at θ and θ′ respectively with radius ρ.\n\nWe now use Definition 3.6 and Definition 3.5 to ensure that f is γ-strongly convex and that the gradient of f exists and is β-jointly smooth. With our assumption of γ-strong convexity of f , G(θ) is the unique minimizer of f (x) and we have the following two inequalities\n\nf (G(θ)) − f (G(θ′)) ≥ (G(θ) − G(θ′))T ∇f (G(θ′)) +\n\nf (G(θ′)) − f (G(θ)) ≥\n\nγ 2\n\n||G(θ) − G(θ′)||2\n\n2\n\nγ 2\n\n||G(θ) − G(θ′)||2\n\n2\n\n(1)\n\n(2)\n\nInequality (1) comes from the definition of γ-strong convexity and inequality (2) comes from γstrong convexity and the first-order optimality condition since Lemma A.14 tells us (G(θ′) − G(θ))T ∇f (G(θ)) ≥ 0 because G(θ) is the minimizer of f (x).\n\nUsing these two inequalities we can derive the following\n\n−γ||G(θ) − G(θ′)||2\n\n2 ≥ f (G(θ)) − f (G(θ′)) −\n\nγ 2\n\n||G(θ) − G(θ′)||2\n\n2\n\n≥ (G(θ) − G(θ′))T ∇f (G(θ′))\n\nwhere we get the first inequality from (2) and the second from (1).\n\nNow, using β-joint smoothness in z and Cauchy-Schwarz we get\n\n||(G(θ) − G(θ′))T ∇θl(z; G(θ′)) − (G(θ) − G(θ′))T ∇θl(z′; G(θ′))||2\n\n≤ ||G(θ) − G(θ′)||2β||z − z′||2\n\nThat is, (G(θ) − G(θ′))T ∇θl(z; G(θ′) is ||G(θ) − G(θ′)||2β-Lipschitz in z. We will now use this and Kantorovich-Rubinstein (Lemma A.15). Let\n\ng(z) =\n\n(G(θ) − G(θ′))T ∇θl(z; G(θ′)) ||G(θ) − G(θ′)||2β\n\nThe function g(z) is 1-Lipschitz in z because we have just divided (G(θ) − G(θ′))T ∇θl(z; G(θ′)) by its Lipschitz constant. From Lemma A.15 and Definition 3.7 we have the following, with g(Z) as defined above\n\nEZ∼D∗(θ)[g(Z)] − EZ∼D∗(θ′)[g(Z)] ≤ ωε||θ − θ′||2\n\nUsing linearity of expectation and multiplying by ||G(θ) − G(θ′)||2β we get the following\n\n(G(θ) − G(θ′))T ∇f (G(θ′)) − (G(θ) − G(θ′))T ∇f ′(G(θ′))\n\n≥ −ωεβ||G(θ) − G(θ′)||2||θ − θ′||2\n\nNow again using Lemma A.14, we have (G(θ) − G(θ′))T ∇f ′(G(θ′)) ≥ 0, hence (G(θ) − G(θ′))T ∇f (G(θ′)) ≥ ωεβ||G(θ) − G(θ′)||2||θ − θ′||2. From our work above we showed that −γ||G(θ) − G(θ′)||2\n\n2 ≥ (G(θ) − G(θ′))T ∇f (G(θ′)). Putting this all together we get\n\n−γ||G(θ) − G(θ′)||2\n\n2 ≥ −ωεβ||G(θ) − G(θ′)||2||θ − θ′||2\n\nWe rearrange the above to get\n\n||G(θ) − G(θ′)||2 ≤ ωε\n\nβ γ\n\n||θ − θ′||2\n\nwhich proves claim (1) of the theorem.\n\nClaim (2) follows easily. We note that θt = G(θt−1) from the definition of RDRO and G(θP S) = θP S by the definition of robust performative stability. Using the result of claim (1) we get\n\n||θt − θP S||2 ≤ ωε\n\n||θt−1 − θP S||2 ≤\n\nωε\n\n(cid:18)\n\n(cid:19)t\n\nβ γ\n\nβ γ\n\n||θ0 − θP S||2\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nNow we set\n\nand solve for t.\n\n(cid:19)t\n\n(cid:18)\n\nωε\n\nβ γ\n\n||θ0 − θP S||2 ≤ δ\n\n(cid:19)t\n\n(cid:18)\n\nωε\n\nβ γ\n\n||θ0 − θP S||2 ≤ δ\n\n(cid:18)\n\nt log\n\nωε\n\n(cid:19)\n\nβ γ\n\n+ log(||θ0 − θP S||2) ≤ log(δ)\n\n(cid:18)\n\nt log\n\nωε\n\n≤ log(δ) − log(||θ0 − θP S||2)\n\nβ γ\n\n(cid:19)\n\n(cid:19)\n\n(cid:18)\n\nt log\n\nωε\n\nβ γ\n\n(cid:19)\n\n(cid:18)\n\n≤ t\n\nωε\n\nβ γ\n\n− 1\n\n≤ log(δ) − log(||θ0 − θP S||2)\n\n(cid:18)\n\nt ≥ (log(δ) − log(||θ0 − θP S||2))\n\nωε\n\n(cid:18)\n\nt ≥\n\n1 − ωε\n\n(cid:19)−1\n\nlog\n\n(cid:18) ||θ0 − θP S||2 δ\n\nβ γ\n\n(cid:19)−1\n\n− 1\n\nβ γ\n(cid:19)\n\nNote that this theorem is essentially just an application of the Banach fixed point theorem as ωε < γ\nβ =⇒ ωε β\n\nγ < 1.\n\nWhile this proof provides us with conditions under which RDRO converges, it is somewhat unsatisfying as it leaves as an open question what is required for Definitions 3.5, 3.6, and 3.7 to be true. Unlike for performative prediction with risk minimization, making assumptions on our loss function does not guarantee that the distributionally robust objective will share those properties. In order to have smoothness of our objective and also robust ε-sensitivity of the distribution map we require some sort of stability or regularity of the worst case distributions as the centre of the f -divergence ball moves as a function of θ.\n\nThe robust ε-sensitivity condition could possibly be equivalently expressed as a smoothness condition for the f -divergence ball surrounding the data generating distribution. Further exploration of these questions would likely yield further understanding of the distributionally objective, even outside of the context of performative prediction.\n\nIt could also be possible that robust β-joint smoothness is not a necessary condition for convergence of RDRO and that a convergence proof may be possible working with subgradients or directional derivatives, but if this is the case it would necessitate a different, and likely more involved, proof technique than the one used here. An alternative approach would also be to work with the dual reformulation of the DRO problem given in Theorem A.2 or to use an alternative probability distance measure rather than f -divergence balls that may be more amenable to this type of analysis.\n\nThese approaches come with their own complications, however, and likely require answering similar questions. We believe these are interesting questions worthy of further research, but their investigation is beyond the scope of this work. Instead, we present empirical work which demonstrates the convergence of RDRO in a variety of scenarios.\n\nA.5\n\nIMPLEMENTATION\n\nWe provide a brief explanation of the implementation details for ERM and DRO. Both risk minimization and distributionally robust optimization require access to the data generating distribution, i.e. infinite data, which is obviously impossible, so we perform empirical risk minimization and utilize the plug-in estimator for distributionally robust optimization instead. Further, for DRO we make use of the dual formulation (Theorem A.2) and perform optimization on this objective rather than working with the primal form. Following Duchi & Namkoong (2021) and Hashimoto et al. (2018), we use χ2-divergence balls in our implementation.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nIn all of our experiments we use linear models trained with gradient descent. We use a fixed stepsize and a fixed number of epochs for training. The logistic regression utilizes an L2-regularized cross-entropy loss function.\n\nThe dual formulation of the distributionally robust objective allows for a simple training procedure where we treat the dual variable η as a hyperparameter. Recall that for convex losses, l(θ; Z), the dual formulation is jointly convex in (θ, η). The training procedure is thus as follows, for a given η, compute the approximate minimizer (cid:98)θη\n\nminimize θ∈Θ\n\nE(l(θ; Z) − η)2\n\n+.\n\nBecause the dual is convex in η we can use binary search to find the optimal dual variable η∗. The loss (l(θ; Z) − η)2 + is merely the ReLu function applied to the usual loss with η subtracted and then squared which allows us to train models using gradient descent methods. Hence, using binary search we train models with different values of η until we find the optimal η∗. The optimal value, η∗, depends on the data and also the radius of our f -divergence ball, ρ. The value of ρ is a hyperparameter that we must specify before training our model.\n\nA.6 EXPERIMENTS\n\nA.6.1 RRM AND RDRO FOR CREDIT DATASET\n\nOur first experiment reproduces the experimental work from Perdomo et al. (2020). We use a dataset from a Kaggle competition titled Give Me Some Credit. The dataset contains relevant information for predicting credit scores. The target variable is a binary variable indicating whether or not an individual has experienced financial distress in the past two years. The original dataset contains historical information for 250,000 borrowers.\n\nFollowing Perdomo et al. (2020) we balance the dataset to contain an equal number of positive and negative cases for the target variable and normalize predictor variables to have a mean of zero and variance of one. The reduced dataset contains 18,358 entries.\n\nWe train L2-regularized logistic regression models on the data for both ERM and DRO. Both models are trained with stochastic gradient descent with a fixed step-size of α = 0.03 for 5000 epochs. The step-size and number of epochs were chosen empirically to approximately replicate performance from Perdomo et al. (2020) on the base distribution. Additionally, a fixed value of ρ was chosen as a radius of the χ2-divergence ball for DRO. The value of ρ was chosen so that the accuracy of the DRO model on the full dataset was significantly, but not drastically, different than that of the ERM model.\n\nIn order to add a performative element to the prediction problem, we identify 3 of the 10 features as strategic features which will be altered as a function of the parameters of our model. Strategic classification is a particular instance of performative prediction in which individuals adversarially adjust their features to maximize the likelihood of classification to the positive class. Strategic classification has been explored in relation to fairness concerns in previous work Hardt et al. (2016a); Bruckner et al. (2012); Miller et al. (2020); Milli et al. (2019); D’Amour et al. (2020); Hu et al. (2019).\n\nAs described in Perdomo et al. (2020) and Hardt et al. (2016a), we assume individuals have linear utilities u(θ, x) = −⟨θ, x⟩ and quadratic costs c(x′, x) = 1 2. The constant ε controls the cost individuals incur by altering their features. Individuals thus pay a cost to manipulate their features in order to minimize the likelihood of the model predicting that they will default on their loan, but are unable to change the true outcome, y ∈ {0, 1}, of whether or not they default. Given linear utilities and quadratic costs as described here, the individuals’ best response is to manipulate their features as\n\n2ε ||x′ − x||2\n\nx′\n\nS = xS − εθS,\n\nwhere xS, x′ tion map is ε-sensitive can be found in Perdomo et al. (2020).\n\nR|S| and |S| is the number of strategic features. The explanation of why this distribu-\n\nS\n\nThe procedure for updating the data according to the distribution map for strategic classification is explained in the box below.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nInput: Base distribution P , a classifier fθ, a cost function c, a utility function u. Sampling procedure for D(θ):\n\n1. Sample (x, y) ∼ P 2. Compute best response xBR ← arg maxx′ u(x′, θ) − c(x′, x) 3. Output sample (xBR, y)\n\nUsing a logistic regression classifier and the strategic classification distribution map sampling procedure outlined above, we run ERM and DRO for 30 iterations on our dataset with values of ε ∈ {0.1, 1, 10, 100}. We observe similar convergence behaviour for both ERM and DRO. As the value of ε grows, the inequality ε < γ β no longer holds, meaning that the conditions of Theorems A.9 and 3.8 are not satisfied and we do not necessarily have a contraction mapping. We plot the normalized distance between values of θ for successive iterations of ERM and DRO in Figures 1 and 2. The distance between iterates is calculated as\n\n1 ||θS||2\n\n· ||θt+1 − θt||2,\n\nwhere θS is the value of θ0 on the strategic features.\n\nA.6.2 BUILDING INTUITION THROUGH SIMPLE EXAMPLES\n\nWe now investigate the convergence of ERM and DRO on simple synthetic datasets to build intuition for the behaviour of the two algorithms. We move away from the assumptions required for our general theoretical guarantees for convergence to a fixed point and experiment with distribution maps that are not necessarily ε-sensitive. We begin with a regression problem and then investigate a classification problem.\n\nRegression\n\nWe start with a simple mean prediction task with data sampled from a mixture of two univariate Gaussians, XA ∼ N (μA, σ2) and XB ∼ N (μB, σ2). 80% of the data is sampled from XA and 20% from XB, i.e. X = 0.2XA + 0.8XB. We train a linear regression model using gradient descent with backtracking line search to predict the mean of the data. We initialize our data with values μA = 4, μB = 4, and σ2 = 0.01. We choose a small value of σ2 so that the variance and finite samples have only a small impact on the convergence behaviour of ERM and DRO. We select an f -divergence ball radius of ρ = 4 for DRO.\n\nWe use θ to denote the learned parameter of the model, μ to denote the true mean of the data generating distribution, and μA and μB to denote the true means of the XA and XB. In other words, θ = ˆμ is the estimated mean from the model. Where necessary, we indicate with subscripts ERM and DRO to indicate which model we are referring to.\n\nWe investigate three different distribution maps which we call D0, D1, and D2. For each map the means of the normal distributions from which we sample are adjusted as a function of θ. Hence, the induced distribution is\n\nDi(θ) = 0.2N (DA\n\ni (θ), σ2) + 0.8N (DB\n\ni (θ), σ2).\n\nThese distribution maps were chosen because they are simple to understand, but reveal important differences in the way ERM and DRO behave when their predictions alter the distribution on which they are learning. The distribution maps are specified in Table 3.\n\nD0(·)\n\nD1(·)\n\nDA DB\n\n0 (θ) = N (θ, σ2) DA 0 (θ) = N ( θ 2 , σ2) DB\n\n1 (θ) = N (μA, σ2) DA 1 (θ) = N ( θ DB\n\n2 , σ2)\n\nD2(·) 2 (θ) = N (2θ, σ2) 2 (θ) = N ( θ 2 , σ2)\n\nTable 3: Distribution maps for mean-prediction experiment.\n\nThe distribution map determines the evolution of the distribution over the data at each iteration of deploying and learning our model, which in turn changes the learned θt = ˆμt. Despite starting from\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Learned values of θ for ERM.\n\nFigure 9: Learned values of θ for DRO.\n\nthe same initial distribution in each case, the induced distributions vary widely. This demonstrates the importance of performativity and illustrates why a static supervised learning framework fails to adequately capture the complex dynamics of prediction problems which involve performativity.\n\nWe run the experiment for 50 iterations, where one iteration involves training a model on data sampled from the current distribution and then updating the distribution via the distribution map. We summarize the evolution of the learned parameter θt over time for both ERM and DRO in Figures 8 and 9. Given the simplicity of the learning problem, the learned values of θ closely approximate true mean of the distribution, μ, over time. The approximate values to which ERM and DRO converge are given in Table 4.\n\nIt is interesting to note that even with this simple mean prediction task we observe significant differences in the behaviour of ERM and DRO over time. For all three distribution maps we observe ERM’s tendency to focus on the majority group over the minority group. Recall that 20% of the data is sampled from XA and 80% from XB so we refer to group A as the minority group and group B as the majority group. For D0, ERM quickly converges toward zero as group B’s mean evolves as μB = θ 2 . Interestingly, however, it does not converge to zero, but instead remains fixed at approximately θ = 0.004 even though θ = 0 is the performatively optimal value.\n\nDRO displays similar behaviour for this distribution map, but does not converge as close to zero. As noted, performative stability and performative optimality are distinct solution concepts, and performatively optimal points only coincide with performatively stable points in specific settings.\n\nMethod\n\nD0(·)\n\nERM θERM = 0.004 θDRO = 0.128 DRO\n\nD1(·) θERM = 1.33 θDRO = 2.66\n\nD2(·) θERM = 0.008 θDRO = ∞\n\nTable 4: Values to which θ converges for ERM and DRO.\n\nThe second distribution map, D1, reveals differences in ERM and DRO that are relevant for fairness. For D1, the mean of the majority group again evolves as μB = θ 2 , but this time the minority group’s mean remains unchanged with μA = 4. As ERM averages the loss over all data points, it converges to a predicted mean much closer to μB than to μA. DRO, on the other hand, converges to a value that balances performance on prediction of the means of both minority and majority groups. In fact the distance from the true means of group A and group B is almost identical.\n\n|θDRO − μA| = |2.66 − 4| = 1.34 |θDRO − μB| = |2.66 − 1.33| = 1.33\n\nERM, however, is a much better predictor of the global mean of the data. This simple example illustrates a trade-off we can expect between ERM and DRO in terms of fairness versus global\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Learned values of θ for ERM with D2.\n\nFigure 11: Learned values of θ for DRO with D2.\n\nperformance.\n\nμERM = 0.2μA + 0.8μB = (0.2)(4) + (0.8)(0.665) = 1.332 μDRO = 0.2μA + 0.8μB = (0.2)(4) + (0.8)(0.1.33) = 1.864\n\nThe final distribution map, D2, demonstrates that ERM and DRO can behave entirely differently under some circumstances. For this distribution map ERM converges to a similar value to that under D0, that is θERM = 0.008. This makes some sense intuitively as μA = 2θ whereas for D0 we had μA = θ. DRO, on the other hand, diverges with θDRO going to infinity. The reason for this is that for the first iteration DRO learns a value of θDRO that is larger than 4. Similarly, for each successive iteration the learned value of θDRO gets larger which in turn causes the true means of the data to grow as a function of the learned θDRO. Interestingly, if you swap the majority and minority groups the behaviour of DRO remains nearly identical, while ERM diverges at a faster rate than DRO. This again demonstrates how DRO treats minority and majority groups similarly, while ERM learns a function that prioritizes performance on majority groups. We illustrate this in Figures 10 and 11\n\nClassification\n\nWe now move to a classification task and analyze the behaviour of logistic regression classifiers trained using ERM and DRO. The classification task is more complex than the simple mean prediction task, so for this experiment we analyze only the static supervised learning setting in order to reduce complexity and elucidate the relevant differences between ERM and DRO.\n\nOur data is generated from bivariate Gaussian distributions and the label, y, of a given data point is 1 if the sum of its features are greater than the sum of the means of the two Gaussian distributions from which we draw samples and 0 otherwise. As with the regression experiments, we have two subgroups within our data, A and B. We vary the proportion of samples from each subgroup in the experiments. Precisely, the data generating process is as follows:\n\nXA ∼ N (μμμA, ΣΣΣA) XB ∼ N (μμμB, ΣΣΣB)\n\nX = cAXA + cBXB cA, cB ∈ (0, 1), and cA + cB = 1\n\nwhere\n\nμμμA =\n\n(cid:21)\n\n(cid:20)μ1 A\nμ2 A\n\n,\n\nμμμB =\n\n(cid:21)\n\n,\n\n(cid:20)μ1 B\nμ2 B\n\nΣΣΣA =\n\n(cid:20)σ1 A\n0\n\n(cid:21)\n\n,\n\n0 σ2 A\n\nΣΣΣB =\n\n(cid:20)σ1 B\n0\n\n(cid:21)\n\n.\n\n0 σ2 B\n\nAnd for a data point, x = [x1\n\ni , x2\n\ni ]T , with i ∈ {A, B},\n\ny =\n\n(cid:26)0 1\n\nif x1 if x1\n\ni + x2 i + x2\n\ni ≤ μ1 i > μ1\n\ni + μ2 i + μ2\n\ni\n\ni\n\n.\n\nHence, if μμμA ̸= μμμB, the data is not linearly separable and the logistic regression model must trade off performance across the two subgroups. We provide scatter plots of samples from the data generating\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Sample of 360 data points from the data generating distribution with cA = 0.8 and cB = 0.2.\n\nFigure 13: Sample of 600 data points from the data generating distribution with cA = 0.8 and cB = 0.2.\n\nA = 1 and μi\n\ndistribution below with μi B = 0.1 for i ∈ {0, 1}, and cA = 0.8, cB = 0.2. Figure 12 contains a sample of 360 data points coloured by the value of their target variable, with crosses representing data points belonging to group B and circles representing data points belonging to group A. Figure 13 contains a sample of 500 and 100 data points respectively from group A and group B coloured by the value of their target variable.\n\nB = 0.7, σi\n\nA = σi\n\nIn Figure 12 we can see that the data is not linearly separable, as some members of group B who belong to the positive class have features that place them lower than the threshold for positive classification for group A. Although this dataset is extremely simple, it is characterized by a feature that represents a central concern for fairness in machine learning, namely that the conditional probability distributions, P (y|x), are significantly different for distinct subsets of the data. This example\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nis intended to represent a simplified abstract instance of a population with majority and minority subgroups in order to see how the behaviour of ERM and DRO differ in this circumstance.\n\nWe generate three distinct datasets on which to train our algorithms, each made up of differing proportions of the two subgroups A and B. Each dataset contains a sample of 10,000 data points, with samples distributed according the values of cA and cB. The accuracy of the models on the three datasets is summarized in Tables 5 and 6 below. For both ERM and DRO we use L2-regularized logistic regression trained with stochastic gradient descent. The step-size for all algorithms is fixed at 0.05 and we train for 15,000 epochs.\n\nModels trained with the distributionally robust objective have the additional complication that we must specify a value for the radius of the χ2-divergence ball, i.e. ρ. The larger the value of ρ, the more we can expect a DRO model to differ from an ERM model because as the χ2-divergence ball grows, the worst case distribution can be further and further from the data generating distribution. Conversely, in the limit as ρ → 0, we recover ERM as the χ2-divergence ball shrinks to contain only the data generating distribution.\n\nChoosing the value of ρ is a challenging decision, as the performance of a model varies significantly as ρ changes. If one has access to demographic information, it is possible to conduct a grid search over possible ρ values in order to find a value that results in a model with the desirable fairness properties. Doing this, however, largely defeats the purpose of DRO. As explained earlier, a central advantage to using DRO rather than some fairness constrained optimization technique is that DRO does not require access to demographic information. In this experiment we work directly with the dual formulation of DRO and set η = 0.56. This value was chosen empirically to achieve relatively uniform accuracy across group A and group B for an 80/20 split between the two subgroups. As the values of cA and cB change, we can see that the performance of DRO changes for a given value of η and hence ρ, as η∗ depends on ρ.\n\nGroup A\nB All Data\n\n[cA = 0.6, cB = 0.4] 0.797 0.701 0.759\n\n[cA = 0.8, cB = 0.2] 0.907 0.652 0.856\n\n[cA = 0.95, cB = 0.05] 0.966 0.592 0.948\n\nTable 5: Accuracy by Group for ERM.\n\nGroup A\nB All Data\n\n[cA = 0.6, cB = 0.4] 0.665 0.869 0.747\n\n[cA = 0.8, cB = 0.2] 0.751 0.744 0.750\n\n[cA = 0.95, cB = 0.05] 0.780 0.766 0.780\n\nTable 6: Accuracy by Group for DRO.\n\nWe first examine the performance of ERM (Table 5). As the data is not linearly separable, ERM must learn a decision boundary that trades off performance between the two subgroups. Because the ERM objective treats the loss on each data point equally, the model learns a decision boundary that is more accurate for the majority group than for the minority group. This discrepancy in accuracy of predictions worsens the smaller the majority group is. For instance, when 95% of the data comes from group A, the logistic regression model trained with an ERM objective achieves 96% accuracy on group A members, but only 59.2% accuracy on group B members.\n\nModels trained with a DRO objective behave much differently. For the 80/20 split and 95/5 split, the DRO models learn relatively fair decision boundaries, effectively balancing performance on both subgroups A and B. For the 60/40 split, however, DRO actually learns a model that performs significantly better on the minority group than the majority group. This model is in a sense discriminatory against the majority group. This is the result of the radius of the χ2-divergence ball being too large and the model thus overly focusing on a worst case distribution.\n\nAs mentioned above, the correct choice of ρ is not necessarily obvious, but it greatly impacts the performance of the model. Duchi et al. provide some recommendations and heuristics for choosing\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Group A, DRO.\n\nFigure 15: Group B, DRO.\n\nFigure 16: Group A, ERM.\n\nFigure 17: Group B, ERM.\n\nFigure 18: Decision boundaries for ERM and DRO classifiers with samples from groups A and B with cA = 0.95 and cB = 0.05. Shading indicates predicted label and data point colour indicates true label.\n\nvalues of ρ in Duchi & Namkoong (2021). Along with altering the value of ρ, one can change the f -divergence ball by varying the the value of k for f -divergences in the Cressie-Read family. This has a similar effect to changing the value of ρ.\n\nIn Figures 14, 15, 16, and 17 we plot data from groups A and B for data generated with cA = 0.95, cB = 0.05 with the learned decision boundary from a DRO and ERM model, respectively, overlaid. The background shading indicates the predicted label with blue representing ˆy = 0 and red representing ˆy = 1, while the colour of the data points indicate their true label.\n\nNeither the regression nor the classification experiments in this section are designed to be realistic representations of real-world applications, but rather are intended to provide a simple setting in which to investigate important differences in the behaviour of models trained with ERM versus DRO objectives. In the next section we investigate a slightly more complex classification task in the performative prediction setting and examine how DRO and ERM may impact fairness considerations when deploying a model influences the distribution on which it is making predictions.\n\nA.6.3 FAIRNESS AND RDRO\n\nThe data generating process is as follows:\n\nXA ∼ N (μμμA, ΣΣΣA) XB ∼ N (μμμB, ΣΣΣB)\n\nX = cAXA + cBXB cA, cB ∈ (0, 1), and cA + cB = 1\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\nμμμA =\n\n\n\n \n\nμ1 A\n... μ10 A\n\n\n\n  ,\n\nμμμB =\n\n\n\n  , ΣΣΣA =\n\n\n\n \n\nμ1 B\n... μ10 B\n\n\n\n \n\nσ1 A · · · ... . . . · · ·\n\n0\n\n\n\n  , ΣΣΣB =\n\n0\n\n... σ10 A\n\n\n\n \n\nσ1 B · · · ... . . . · · ·\n\n0\n\n\n\n  .\n\n0\n\n... σ10 B\n\nAnd for a data point, x = [x1\n\ni , . . . , x10\n\ni ]T , with i ∈ {A, B},\n\ny =\n\n(cid:26)0 1\n\nif x1 if x1\n\ni + · · · + x10 i + · · · + x10\n\ni ≤ μ1 i > μ1\n\ni + · · · + μ10 i + · · · + μ10\n\ni\n\ni\n\n.\n\nFor our experiment we set the parameters of the data generating process as\n\nμi μi\n\nA = 1 B = 0.8,\n\nσi σi\n\nA = 0.1 B = 0.1,\n\ncA = 0.8\n\ncB = 0.2.\n\nThese parameters were selected as an attempt to capture the notion that an underprivileged minority group may have features that make them appear to be less qualified, despite having the same proportion of qualified individuals as a dominant majority group. The data generating process obviously represents this at a high level of abstraction and is much less complex than most real-world applications. With that said, we believe the data generating process effectively encapsulates this abstracted characteristic that is central to concerns for learning fair models. Examples of the type of situation that this experiment is intended to represent are college admissions or hiring, where an underprivileged minority group may not, on average, have CVs that appear as impressive as their peers from the majority group due to a lack of opportunity, but are nevertheless equally qualified for the school or job.\n\nWe once again examine a strategic classification scenario, as in experiment 1, but our data now contains subgroups, allowing us to analyze the impact of ERM and DRO on fairness and model performance in the performative setting. We examine four different distribution maps by varying the parameter ε ∈ {0.01, 0.25, 0.5, 10} and set 5 of the 10 features to be strategic (i.e. manipulable).\n\nFor both ERM and DRO we train L2-regularized logistic regression models with λ = 0.0001. We use stochastic gradient descent with a fixed step-size of α = 0.2 and train for 8000 epochs on samples of 1,200 data points. We use a fixed radius ρ of the χ2-divergence ball for DRO. All parameters were chosen empirically to give good performance on the base distribution.\n\nFirst, we examine the convergence behaviour of both algorithms by plotting the normalized distance between successive iterates of the learned parameters, θt, in Figures 19 and 20. We observe that ERM does not converge for any value of ε, while DRO converges for only ε = 0.01. It is unclear why the algorithms do not converge for other values of ε. The failure to converge could be related to using a fixed, rather than decaying step-size, or because the conditions for the contraction mapping are not met (recall that the conditions are sufficient, but not necessary for convergence of a particular instance). For the three smaller values of ε, both ERM and DRO converge to a small neighbourhood, whereas for ε = 10, neither ERM nor DRO exhibit any convergence.\n\nTo demonstrate the effect of the convergence, or lack thereof, of θ on the model’s performance, we plot the average supervised and performative L2-regularized binary cross-entropy loss for ERM and DRO for ε = 0.5 and ε = 10 in Figure 21. The blue lines indicate the optimization phase and the green lines indicate the effect of the distribution shift after the classifier deployment. That is, the dots at the end of a green dotted line represent performative loss, while the dots at the end of a blue line represent supervised loss. In the plots we can see that even though ERM and DRO do not converge for ε = 0.5, the models quickly achieve relatively stable loss on the classification task. Note that the average L2-regularized binary cross-entropy loss is the objective for which ERM is optimizing, but not the objective for which DRO is optimizing.\n\nGiven that this experiment is a balanced binary classification task, the metric in which we are principally interested is accuracy. Furthermore, as we are interested in comparing the fairness properties of DRO and ERM, it is important to analyze the performance of the models on the subgroups within the population, as well as the population as a whole. In Figures 22 and 23 we plot the the accuracy of the two models for ε = 0.5. We see that the performative accuracy of ERM initially degrades significantly, before quickly converging to approximately 84% on the full population.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 19: Plot of the normalized distance between successive values of θ for ERM.\n\nFigure 20: Plot of the normalized distance between successive values of θ for DRO.\n\nConversely, the performative accuracy for DRO is actually initially higher than the accuracy on the distribution on which the model was trained, but it also converges relatively quickly to an accuracy of approximately 72.5% on the full population. This is once again due to DRO having a different optimization objective than ERM. The average cross-entropy loss acts as a surrogate loss for the 0-1 loss and hence aims to maximize accuracy on the full dataset. The objective used for DRO focuses on the tails of the distribution and therefore does not maximize accuracy across the full dataset. The improvement of the DRO model’s accuracy after the distribution shift is not an inherent property of DRO, but rather is specific to this particular dataset and distribution map. We see in Figure 22 at iteration t = 2 that ERM also occasionally achieves better performance after distribution shift. In general, the improvement or degradation of accuracy depends on the learned parameters of the model, the distribution map, the data, and the loss function.\n\nIn Figure 25 we plot the accuracy of ERM and DRO for the subgroups A and B for ε = 0.5. We can see from these plots that the fairness properties of ERM and DRO are preserved in the performative prediction setting.\n\nERM converges to an accuracy of approximately 90% on group A and only approximately 54% on group B. DRO, on the other hand, converges to much more equal accuracy across the two subgroups, achieving an accuracy of approximately 74% for group A and approximately 66% for group B. We summarize the accuracies to which ERM and DRO converge in Tables 7 and 8. We do not include\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 21: Plots of the average L2-regularized binary cross-entropy supervised and performative loss.\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 22: ERM\n\nFigure 24: ERM and DRO accuracy on the full population across successive iterations.\n\nFigure 23: DRO\n\nthe accuracy for ε = 10 because neither ERM nor DRO converged to a small enough neighbourhood, and therefore did not converge in accuracy. We see that the algorithms converge to similar accuracy values for all the values of ε.\n\nThis result, while perhaps not that surprising, is important, as it demonstrates that not only does DRO exhibit similar convergence behaviour to ERM, but DRO converges to fair fixed points, whereas ERM converges to discriminatory fixed points in the presence of heterogeneous data composed of minority and majority subgroups. Recall also that DRO is not given access to group information, but still learns to achieve more uniform performance across subgroups, as it is attempting to minimize the worst case loss across all probability distributions within the χ2-divergence ball surrounding the data generating distribution.\n\nERM Performative Accuracy\n\nGroup A\nB All Data\n\nε = 0.01 0.893 0.540 0.834\n\nε = 0.25 0.896 0.540 0.837\n\nε = 0.5 0.898 0.540 0.838\n\nTable 7: Accuracy by Group for ERM after 30 iterations.\n\nA.7 FINAL THOUGHTS\n\nWe began this work with a discussion of why research into fairness in machine learning is such an urgent issue and we would like to reemphasize this point. Machine learning is no longer a niche research area. It has become a fundamental driving force of the global economy and now attracts billions of dollars in investment around the world Abdallat (2021); Balakrishnan et al. (2020). This\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 25: ERM and DRO accuracy on subgroups A and B across successive iterations.\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nDRO Performative Accuracy\n\nGroup A\nB All Data\n\nε = 0.01 0.687 0.670 0.684\n\nε = 0.25 0.710 0.660 0.701\n\nε = 0.5 0.738 0.660 0.725\n\nTable 8: Accuracy by Group for DRO after 30 iterations.\n\nchange has resulted in enormous opportunity for individuals with expertise in machine learning and it appears that this trend will only increase in the coming years.\n\nThe reality, however, is that this emergence of AI as an economic powerhouse has not occurred in a particularly responsible manner. Machine learning researchers and engineers have benefited greatly from the investment in AI and the community has a responsibility to ensure that the future development of the field aligns with and supports universal rights, freedoms, and values. Chief among these responsibilities is ensuring that the development and deployment of machine learning models do not harm the most vulnerable among us. We hope that this work contributes to this endeavor.\n\n33",
    "reference": "# Summary Of The Paper\n\nThis paper studies distributional robustness in the setting of performative prediction. Specifically, it proposes to use DRO with performative prediction as a way to ensure long-term fairness where the demographic information is not available beforehand. The paper further explicates the definitions, assumptions, and constructs require to perform repeated RM in this setting. It theoretically shows that the repeated DRO in this setting should converge under given conditions. It also experimentally shows the benefits of using this approach.\n\n# Strength And Weaknesses\n\nStrength:\n1. The problem of ensuring long-term fairness where the demographic information is not available, is an interesting and relevant problem.\n2. The idea of doing DRO in performative prediction is an intuitive and interesting problem to study.\n3. The paper presents the required definitions, assumptions, and background materials clearly and rigorously.\n4. The experiments show interesting benefits of performing RDRO.\n\nWeakness:\n1. The motivation and the proposition seem disconnected to me. Though the paper begins with the problem of ensuring long-term fairness without demographic info, I do not see explicitly any proof or discussion to connect them. For example, it would be interesting to know how does it relate or contradict existing fairness metrics. Or even if we learn a \\theta^t with RDRO, how much fairness violation it can induce in terms of any group fairness metric? How far or close the classifier learned with DRDO is with the fair classifiers learned using demographic information? As none of these pieces are available, it reads more like a DRO paper for performative predictions with some side-effects on fairness than a paper pointed to fairness. Please clarify if I miss any detail.\n2. The definitions and assumptions are mathematically clear. But some intuition or an example to comprehend them would be very helpful.\n3. The cost of RDRO wrt the performative prediction is not discussed. It would be nice to understand how hard/easy it is do RDRO than PP and what is the cost in terms of sample complexity.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper has a clear and smooth language. The definitions, assumptions, and backgrounds are very clear. The idea of RDRO in performative prediction is novel and interesting. What it lacks is an intuition about where the assumed setting holds and fails, which can be explained with example. Also, a clean theoretical discussion connecting why RDRO is good for the long-term fairness problem under concern is not logically evident.\n\n# Summary Of The Review\n\nI think the paper studies an interesting question and an interesting problem setup. It is cleanly written. But much remains to be discussed and shown than the present draft under considerations. It will be imperative to answer the questions in weakness to judge the value of the propositions.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCODEGEN: AN OPEN LARGE LANGUAGE MODEL FOR CODE WITH MULTI-TURN PROGRAM SYNTHESIS\n\nErik Nijkamp∗, Bo Pang∗, Hiroaki Hayashi∗,\n\nLifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong\n\nSalesforce Research\n\nABSTRACT\n\nProgram synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multiturn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.\n\n1\n\nINTRODUCTION\n\nCreating a program has typically involved a human entering code by hand. The goal of program synthesis is to automate the coding process, and generate a computer program that satisfies the user’s specified intent. Some have called it the holy grail of computer science (Manna & Waldinger, 1971; Gulwani et al., 2017). Successful program synthesis would not only improve the productivity of experienced programmers but also make programming accessible to a wider audience.\n\nTwo key challenges arise when striving to achieve program synthesis: (1) the intractability of the search space, and (2) the difficulty of properly specifying user intent. To maintain an expressive search space, one needs a large search space, which poses challenges in efficient search. Previous work (Joshi et al., 2002; Panchekha et al., 2015; Cheung et al., 2013) leverages domain-specific language to restrict the search space; however, this limits the applicability of synthesized programs. On the contrary, while being widely applicable, general-purpose programming languages (e.g., C, Python) introduce an even larger search space for possible programs. To navigate through the enormous program space, we formulate the task as language modeling, learning a conditional distribution of the next token given preceding tokens and leverage transformers (Vaswani et al., 2017) and large-scale self-supervised pre-training. This approach has seen success across modalities (Devlin et al., 2019; Lewis et al., 2020; Dosovitskiy et al., 2021). Likewise, prior works have developed pre-trained language models for programming language understanding (Kanade et al., 2020; Feng et al., 2020).\n\nTo realize program synthesis successfully, users must employ some means to communicate their intent to the models such as a logical expression (which specifies a logical relation between inputs\n\n∗ Equal contribution.\n\nCorrespondence to: Erik Nijkamp (erik.nijkamp@salesforce.com), Bo Pang (b.pang@salesforce.com), Hiroaki Hayashi (hiroakihayashi@salesforce.com), Yingbo Zhou (yingbo.zhou@salesforce.com), Caiming Xiong (cxiong@salesforce.com).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nand outputs of a program), pseudo-code, input-output examples, or a verbalized specifications in natural language. On the one hand, a complete formal specification enjoys the exact specifications of user intent but may require domain expertise and effort from users to translate the intent to such a form. On the other hand, specification merely based on input-output examples is less costly but may under-specify the intent, leading to inaccurate solutions. Previous work has benefited from various methods and their combinations as the input to program synthesis models, including pseudocode (Kulal et al., 2019), a part of a program and its documentation (Chen et al., 2021), or natural language paragraph with input-output examples (Hendrycks et al., 2021). However, we argue that a truly user-friendly form of intent is natural language text.\n\nTo overcome these challenges, we propose a multi-turn program synthesis approach, where a user communicates with the synthesis system by progressively providing specifications in natural language while receiving responses from the system in the form of synthesized subprograms, such that the user together with the system complete the program in multiple steps. The following two considerations motivate this approach.\n\nFirst, we speculate that factorizing a potentially long and complicated specification into multiple steps would ease the understanding by a model and hence enhance program synthesis. In the multi-turn approach, a model can focus on the specification associated with one subprogram and avoid arduously tracking the complicated dependency among subprograms. This effectively reduces the search space besides the convenience of specifying user intent. Indeed, our speculations are confirmed in our experiments with higher quality synthesized programs through the multi-turn approach.\n\nSecond, code exhibits a weak pattern of interleaved natural and programming language, which may be exploitable. Such a pattern is formed by programmers who explain the functionality of a program with comments. With the language modeling objective, we hypothesize that the interleaving pattern provides a supervision signal for the model to generate programs given natural language descriptions over multiple turns. The signal is highly noisy or weak, because only a subset of data would exhibit such a pattern, comments may be inaccurate or uninformative, and some of them may even be placed at an irrelevant position. However, up-scaling the model and data size might overcome such weak supervision, allowing the model to develop multi-turn program synthesis capacity. This enables user intent to be expressed in multiple turns, that is, the intent can be decomposed and fulfilled part by part while each turn can easily be expressed in natural language.\n\nIn this work, we develop a multi-turn programming benchmark to measure the models’ capacity for multi-turn program synthesis. To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps with a user who specifies the intent in each turn in natural language. Please refer to Figure 1 for an example where the model synthesizes a program to extract the user name of an email address. Performance on the benchmark is measured by pass rate on expert-written test cases. To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis. With the emergence of multi-turn program synthesis capacity in large language models that benefits problem-solving, we believe this benchmark will foster future research in program synthesis.\n\nOur Contributions Our work shares the basic idea of adopting language models for program synthesis with the recent and concurrent efforts (Chen et al., 2021; Austin et al., 2021; Li et al., 2022) with a single-turn user intent specification. In addition, we contribute with respect to four aspects:\n\n• We study multi-turn program synthesis emerging in autoregressive models under scaling laws. • We leverage this capacity to introduce a multi-turn program synthesis paradigm. • We investigate its properties quantitatively with a novel multi-turn programming benchmark.1 • We will open source model checkpoints2 and the custom training library: JAXFORMER.3\n\nFor program synthesis, no large-scale models competitive with Codex are available as open-source. This hinders progress, given that the expensive compute resources required to train these models are only accessible to a limited number of institutions. Our open source contribution allows a wide range of researchers to study and advance these models, which may greatly facilitate research progress.\n\n1Benchmark: https://github.com/salesforce/CodeGen/tree/main/benchmark 2Checkpoints: https://github.com/salesforce/CodeGen 3Training: https://github.com/salesforce/jaxformer\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 MODEL TRAINING\n\nTo evaluate the emergence of multi-turn programming capabilities under scaling laws, we adopt standard transformer-based autoregressive language models, varying (1) the number of model parameters (350M, 2.7B, 6.1B, 16.1B) and (2) the number of tokens of programming languages in the training corpora. For scaling the training, a custom library JAXFORMER for TPU-v4 hardware was developed and will be released as open-source, including the trained model weights.\n\n2.1 DATASETS\n\nThe family of CODEGEN models is trained sequentially on three datasets: THEPILE, BIGQUERY, and BIGPYTHON.\n\nThe natural language dataset THEPILE is an 825.18 GiB English text corpus collected by Gao et al. (2020) for language modeling (MIT license). The dataset is constructed from 22 diverse high-quality subsets, one of which is programming language data collected from GitHub repositories with >100 stars that constitute 7.6% of the dataset. Since the majority of THEPILE is English text, the resulting models are called as natural language CODEGEN models (CODEGEN-NL).\n\nThe multi-lingual dataset BIGQUERY is a subset of Google’s publicly available BigQuery dataset, which consists of code (under open-source license) in multiple programming languages. For the multilingual training, the following 6 programming languages are chosen: C, C++, Go, Java, JavaScript, and Python. Thus, we refer to models trained on the BIGQUERY as multi-lingual CODEGEN models (CODEGEN-MULTI).\n\nThe mono-lingual dataset BIGPYTHON contains a large amount of data in the programming language, Python. We have compiled public, non-personal information from GitHub consisting of permissively licensed Python code in October 2021. Consequently, we refer to models trained on BIGPYTHON as mono-lingual CODEGEN models (CODEGEN-MONO).\n\nThe pre-processing follows: (1) filtering, (2) deduplication, (3) tokenization, (4) shuffling, and (5) concatenation. For details on THEPILE, we refer to Gao et al. (2020). For BIGQUERY and BIGPYTHON, we refer to Appendix A. Table 5 summarizes the statistics of the training corpora.\n\n2.2 MODELS\n\nThe CODEGEN models are in the form of autoregressive transformers with next-token prediction language modeling as the learning objective trained on a natural language corpus and programming language data curated from GitHub. The models are trained in various sizes with 350M, 2.7B, 6.1B, and 16.1B parameters. The first three configurations allow for direct comparison with open-sourced large language models trained on text corpus, GPT-NEO (350M, 2.7B) (Black et al., 2021) and GPT-J (6B) (Wang & Komatsuzaki, 2021). See Table 6 in Appendix A for model specifications.\n\nThe CODEGEN models are trained in a sequential nature over datasets. CODEGEN-NL is first trained on THEPILE. CODEGEN-MULTI is initialized from CODEGEN-NL and trained on BIGQUERY. Finally CODEGEN-MONO is initialized from CODEGEN-MULTI and trained on BIGPYTHON.\n\nThe emergence of program synthesis conditional on descriptions in natural language may stem from the size of the models and data, training objective, and nature of the training data itself. This is called emergence since we do not explicitly train the model on comment-code pairs. Similar phenomena are observed in a wide range of natural language tasks where a large-scale unsupervised language model can solve unseen tasks in a zero-shot fashion (Brown et al., 2020). The emergence phenomena or surprising zero-shot generalization is often attributed to the large scale of the model and the data.\n\nWhile our focus is not to reveal the underlying mechanism on why program synthesis capacity emerges from simple language modeling, we make an attempt to provide an explanation given the nature of our modeling approach and the training data. The data consists of regular code from GitHub (without manual selection), for which some data exhibits a pattern of interleaved natural and programming language, which we believe provides a noisy supervision signal for the program synthesis capacity due to the next-token prediction training objective. However, we emphasize that such a data pattern is highly noisy and weak, because only a subset of data exhibits such a pattern, e.g., comments may be inaccurate or uninformative, and some of them may even be placed at an irrelevant\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nModel\n\nGPT-NEO 350M GPT-NEO 2.7B GPT-J 6B\n\nCODEX 300M CODEX 2.5B CODEX 12B code-cushman-001∗ code-davinci-001∗ code-davinci-002∗\n\nCODEGEN-NL 350M CODEGEN-NL 2.7B CODEGEN-NL 6.1B CODEGEN-NL 16.1B\n\nCODEGEN-MULTI 350M CODEGEN-MULTI 2.7B CODEGEN-MULTI 6.1B CODEGEN-MULTI 16.1B\n\nCODEGEN-MONO 350M CODEGEN-MONO 2.7B CODEGEN-MONO 6.1B CODEGEN-MONO 16.1B\n\npass@k [%]\n\nk = 1\n\nk = 10\n\nk = 100\n\n0.85 6.41 11.62\n\n13.17 21.36 28.81 33.5 39.0 47.0\n\n2.12 6.70 10.43 14.24\n\n6.67 14.51 18.16 18.32\n\n12.76 23.70 26.13 29.28\n\n2.55 11.27 15.74\n\n20.37 35.42 46.81 54.3 60.6 74.9\n\n4.10 14.15 18.36 23.46\n\n10.61 24.67 28.71 32.07\n\n23.11 36.64 42.29 49.86\n\n5.95 21.37 27.74\n\n36.27 59.50 72.31 77.4 84.1 92.1\n\n7.38 22.84 29.85 38.33\n\n16.84 38.56 44.85 50.80\n\n35.19 57.01 65.82 75.00\n\nTable 1: Evaluation results on the HumanEval benchmark. Each pass@k (where k ∈ {1, 10, 100}) for each model is computed with three sampling temperatures (t ∈ {0.2, 0.6, 0.8}) and the highest one among the three are displayed, which follows the evaluation procedure in Chen et al. (2021). Results for the model marked with ∗ are from Chen et al. (2022).\n\nposition. Therefore, we believe two main factors contribute to the program synthesis capacity: 1) large scale of model size and data size and 2) noisy signal in training data.\n\nThe scaling of such LLMs requires data and model parallelism. To address these requirements, a training library JAXFORMER (https://github.com/salesforce/jaxformer) was developed for efficient training on Google’s TPU-v4 hardware. We refer to Appendix A for further details on the technical implementation and sharding schemes. Table 6 summarizes the hyper-parameters.\n\n3 SINGLE-TURN EVALUATION\n\nWe first evaluate our CODEGEN using an existing program synthesis benchmark: HumanEval (MIT license) (Chen et al., 2021). HumanEval contains 164 hand-written Python programming problems. Each problem provides a prompt with descriptions of the function to be generated, function signature, and example test cases in the form of assertions. The model needs to complete a function given the prompt such that it can pass all provided test cases, thus measuring the performance by functional correctness. Since a user intent is specified in a single prompt and provided to the model once, we regard the evaluation on HumanEval as a single-turn evaluation, to distinguish it from the multi-turn evaluation which we introduce in the next section. Following Chen et al. (2021), we recruit nucleus sampling (Holtzman et al., 2020) with top-p where p = 0.95.\n\n3.1 HUMANEVAL PERFORMANCE SCALES AS A FUNCTION OF MODEL SIZE AND DATA SIZE\n\nWe compare our models to the Codex models (Chen et al., 2021), which demonstrate the state-ofthe-art performance on HumanEval. Moreover, our models are compared to open-sourced large language models, GPT-NEO (Black et al., 2021) and GPT-J (Wang & Komatsuzaki, 2021). These are trained on THEPILE (Gao et al., 2020), and thus similar to our CODEGEN-NL models, in terms of training data and model size. All models are evaluated with temperature t ∈ {0.2, 0.6, 0.8}, and we compute pass@k where k ∈ {1, 10, 100} for each model. For direct comparison to the results by Chen et al. (2021), we choose the temperature that yields the best-performing pass@k for each\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nCODEGEN-MONO\n\n350M\n\n2.7B\n\n6.1B\n\n16.1B\n\nPass Non-Pass\n\n3.78 ± 0.23 5.18 ± 0.19\n\n3.66 ± 0.14 4.37 ± 0.18\n\n3.35 ± 0.13 3.88 ± 0.13\n\n3.12 ± 0.11 3.40 ± 0.11\n\nTable 2: Average prompt perplexity↓ (± standard error) of CODEGEN-MONO models on pass and non-pass problems.\n\nk. The results of our models and baselines are summarized in Table 1. Our CODEGEN-NL models (350M, 2.7B, 6.1B) outperform or perform on par with the respective GPT-NEO and GPT-J models. Further training CODEGEN-NL on multilingual programming language data (BIGQUERY) leads to CODEGEN-MULTI. The multilingual CODEGEN models outperform the models trained on THEPILE (GPT-NEO, GPT-J, CODEGEN-NL) by a large margin. We then finetune CODEGEN-MULTI on a Python-only dataset (BIGPYTHON), resulting in CODEGEN-MONO. The program synthesis capacity is improved substantially. Therefore, the Python program synthesis capacity enhances as the amount of Python training data increases. For almost all models, as expected, increasing the size of the model improves overall performance.\n\nOur Python-monolingual CODEGEN models have competitive or improved performance, compared to the current state-of-the-art models, Codex. CODEGEN-MONO 2.7B underperforms CODEX 2.5B when k = 100 but outperforms it when k ∈ {1, 10}. While it is only half the size, our CODEGENMONO 6.1B demonstrates pass@k scores approaching those of the best-performing Codex, CODEX 12B. Our largest model CODEGEN-MONO 16.1B is competitive or outperforms it depending on k.\n\n3.2 BETTER USER INTENT UNDERSTANDING YIELDS BETTER SYNTHESIZED PROGRAMS\n\nThe success of a program synthesis system highly depends on how well it understands user intent. When the system is based on a language model, the perplexity of problem prompts provides a proxy for the system’s understanding of user intent specifications. A low perplexity of an intent specification under a model indicates that this intent specification is compatible with the knowledge learned by the model from the training data. We investigate whether better prompt understanding, with lower prompt perplexity as a proxy, leads to more functionally accurate programs.\n\nWe partition all problems into pass versus non-pass ones. A pass problem is one that at least one sample from 200 samples passes all test cases, while for a non-pass problem none of the 200 samples pass all test cases. We compute the average perplexity of the problem prompts of the pass problems and that of the non-pass ones, based on samples from CODEGEN-MONO models. The results are displayed in Table 2 (see Appendix F for the results on CODEGEN-NL and CODEGEN-MULTI). The prompts of the pass problems have lower perplexity than those of the non-pass ones. This finding implies that program synthesis is more likely to be successful when the user intent specification is understood better by the model. Indeed, some training data contains interleaved sequences of natural language comments and programs, where the comments describe the functionality of the following program. We thus speculate that user intent specifications similar to such a pattern would be better understood by the model, and hence lead to better program synthesis. Inspired by this pattern, we propose to specify user intent in multiple turns such that the model focus on a partial problem at a time, which would make user intent understanding by the model easier.\n\n4 MULTI-TURN EVALUATION\n\nIn this section, we propose and study a multi-step program synthesis paradigm where program synthesis is decomposed into multiple steps and the system synthesizes a subprogram in each step. To examine such a paradigm, we first develop a Multi-Turn Programming Benchmark (MTPB). MTPB consists of 115 problems written by experts, each of which includes a multi-step descriptions in natural language (prompt). To solve a problem, a model needs to synthesize functionally correct subprograms (1) following the description at the current step and (2) considering descriptions and synthesized subprograms at previous steps (e.g., correct backreference of functions and/or variables defined in the previous steps). An illustrative example is shown in Figure 1.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: An illustrative example for the Multi-Turn Programming Benchmark, performing the task of extracting the user name of an email address. 1 Each problem consists of prompts pi and unit tests, where some prompts include templates (i.e. {input}) that are filled with test case inputs before it is fed to the model. In the displayed example, the input is a string containing abc.xyz@example.com, which replaces {input} in p2, and the expected output is abc xyz. 2 Our model conditions on the concatenation of interleaved past prompts and generated responses. 3 Generated responses from each turn are concatenated and executed, where the output is compared to the answer.\n\n4.1 BENCHMARK CONSTRUCTION\n\nWe (4 authors) start by defining4 a set of 115 problems requiring a diverse range of programming knowledge, including math, array operations, string manipulations, algorithms, data science, and problems that require other knowledge, such that the number of problems in each category is roughly balanced.5 For each problem, we construct a triplet consisting of multi-turn prompts P , test case inputs I, and test case outputs O. Multi-turn prompts P are designed following the two constraints: (1) the problem is decomposed into 3 or more turns, (2) a single turn cannot be attributed to solving the problem. For example, implementing a linear regression model could be phrased as “Perform linear regression on x and y”. Since the main task is fully expressed in this prompt, understanding this prompt is sufficient to perform the task. We avoid such cases via manual inspection and distribute problem-solving over turns. Together with the prompts, we task the problem author to prepare 5 sets of test case inputs I and outputs O to evaluate model outputs with functional correctness. To reduce wrongly rewarding false positive solutions that give meaningless programs but pass the tests, we examine and revise such cases to ensure the test quality.\n\nUnlike HumanEval for which models are expected to complete a partially defined function, MTPB problems only provide the prompts, thereby models have to generate the solution from scratch.6 While the free-form generation may allow for more potential solutions, the lack of an entry point to provide test case inputs makes it challenging to test the generated code on diverse test cases. To overcome this challenge, we instead embed test case inputs within prompts. Specifically, prompts are written with Python’s formatted string7 where input values are substituted for the variable name when a specific test case is applied to the problem. For example, a prompt, “Define a string named ‘s’\n\n4Problem writing was performed in a closed book format, i.e. we are not allowed to consult with online\n\nresources while writing the problems.\n\n5See Appendix D for a complete listing. 6To guide sampling in Python, we prefix the prompt with: # Import libraries.\\n import numpy as np. 7https://docs.python.org/3/reference/lexical_analysis.html#f-strings\n\n6\n\nSampleConcatenateTurn 1Turn 2Turn 3Turn 4Turn 5“abc xyz”Execute# Import re and define a regular expression that matches an ...import reemail_regex = re.compile( \"([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})\")# Search for an email address in “... abc.xyz@example.com ...” and ...address = email_regex.search(“... abc.xyz@example.com ...”)# Remove the substring starting from the @ symbol from \"address\".address = address.group(0)address = address[:address.find(\"@\")]# Replace non-alphabetical symbols with a whitespace in \"address\".address = re.sub(\"[^a-zA-Z]+\", \" \", address)# Print out \"address\".print(address)GenerationActual OutputEvaluation23“abc xyz”DiscourseExpected OutputInputHumanImport re and define a regular expression that matches an email address.import reemail_regex = re.compile(\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")ModelModeladdress = email_regex.search(“... abc.xyz@example.com ...”)Search for an email address in “{input}” and store the first match to a variable \"address\".HumanRemove the substring starting from the @ symbol from \"address\".HumanReplace non-alphabetical symbols with a whitespace in \"address\".Humanaddress = address.group(0)address = address[:address.find(\"@\")]ModelModeladdress = re.sub(\"[^a-zA-Z]+\", \" \", address)Print out \"address\".HumanModelprint(address)1“... abc.xyz@example.com ...”✓Published as a conference paper at ICLR 2023\n\nData\n\nModel\n\nPass Rate↑ [%]\n\n350M 2.7B\n\n6.1B\n\n16.1B\n\n-\n\nTHEPILE THEPILE BIGQUERY BIGPYTHON -\n- -\n\nGPT-NEO & GPT-J CODEGEN-NL CODEGEN-MULTI CODEGEN-MONO code-cushman-001 code-davinci-001 code-davinci-002\n\n0.79 0.23 4.09 16.98 -\n- -\n\n8.17 15.31 20.82 38.72 -\n- -\n\n18.86 19.37 25.51 43.52 -\n- -\n\n- 30.33 26.27 47.34 -\n- -\n\n- -\n- -\n56.77 55.28 59.86\n\nTable 3: Evaluation results on the Multi-Turn Programming Benchmark. The multi-turn program synthesis performance varies as a function of model size (columns) and code data size (rows).\n\nPrompt\n\nPPL↓\n\nPass Rate↑ [%]\n\n350M\n\n2.7B\n\n6.1B\n\n16.1B\n\n350M 2.7B\n\n6.1B\n\n16.1B\n\nSingle-Turn Multi-Turn\n\n13.92 ± 1.89 10.09 ± 0.62\n\n11.67 ± 1.46 8.90 ± 0.52\n\n10.58 ± 1.20 8.18 ± 0.43\n\n10.25 ± 0.99 8.05 ± 0.43\n\n5.75 16.98\n\n25.43 38.72\n\n28.48 43.52\n\n38.74 47.34\n\nTable 4: Comparison between multi- and concatenated single-turn specifications on perplexity (PPL) and program synthesis performance (as measured by pass rate) under CODEGEN-MONO models.\n\nwith the value {var}.”, together with a test case input var = ‘Hello’ will be formatted into “Define a string named ‘s’ with the value ‘Hello’.” Also see 1 in Figure 1 for an example.\n\n4.2 EXECUTION ENVIRONMENT AND SOLUTION EVALUATION\n\nFor execution, the history of pairs of prompts and generated completions is concatenated into a self-contained program (see 3 in Figure 1 for an example). The program is then executed in an isolated Python environment following the single-turn HumanEval benchmark (Chen et al., 2021). However, the problems in HumanEval are constructed in such a way that a known function signature is completed, thus invocation of the generated code under a set of functional unit tests is trivial. In our multi-turn case, no such entry point (or return value) is guaranteed to be generated. To circumvent the issue of a missing return signature (or value), the last prompt of the multi-turn problems in MTPB is always specified to print out the resulting state to the terminal. Then, the benchmark execution environment overloads the Python print(args) function and stores args on a stack. If the sampled code for the last prompt of a problem does not include the print() statement, which is a valid convention to print on the terminal in Python or specifically Jupyter notebooks, then the AST of the generated code will be mutated to inject an invocation of print(). Finally, a type-relaxed equivalence check (e.g., an implicit conversion between lists and tuples) of args against the predefined gold output of the problem is performed to determine test failure or success.\n\n4.3 MULTI-STEP PROGRAMMING CAPACITY SCALES WITH MODEL SIZE AND DATA SIZE\n\nIn this analysis, we investigate how the model size and data size affect the program synthesis capacity in a multi-turn paradigm. In the MTPB, each problem has 5 test cases and we sample 40 samples for each test case with each model, based on which the pass rate is computed for each problem. The MTPB evaluation results (average pass rate) for our CODEGEN models, baselines, and OpenAI Codex models8 are shown in Table 3. Clearly, the performance on the MTPB improves as a function of the model size and data size. This suggests that the capacity of multi-step program synthesis scales as a function of the model size and data size. The models are simply trained with an autoregressive language modeling objective. While the model and the data scale up, multi-turn program synthesis capacity emerges, that is, the capacity to synthesize programs in a multi-turn fashion.\n\n8Accessed on November 10th, 2022.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\ns e\nt a\nR\n\ns s\na P\n\nn\n\ni\n\ne c\nn e\nr e\nf f\ni\n\nD\n\n25\n\n20\n\n15\n\n10\n\n5\n\n0\n\n22.06\n\n22.53\n\n19.67\n\n14.19\n\n14.63\n\n8.5\n\n11.51\n\n2.99\n\nEasy Medium Hard\n\n9.06\n\n9.35\n\n0.19\n\n−0.25\n\n350M\n\n2.7B\n\n6.1B\n\n16.1B\n\nNumber of Model Parameters\n\nFigure 2: Difference in average pass-rate of problems in single-turn and multi-turn formulation over levels of problem difficulty. The improvement is sizable for most model sizes and difficulty levels, except for easy problems with larger models.\n\n4.4 BETTER USER SPECIFICATION UNDERSTANDING WITH MULTI-TURN FACTORIZATION\n\nWe hypothesize that multi-turn factorization enhances the model’s understanding of user intent specifications, which in turn lead to higher program synthesis capacity. To test this hypothesis, we form a single-turn counterpart of multi-turn specifications by concatenating each specification into a single turn. As discussed in Section 3.2, we adopt the prompt perplexity as a proxy for user intent understanding. Thus, we compare the perplexity of the multi-turn prompts and that of the concatenated single-turn prompts under the four CODEGEN-MONO models.\n\nThe average perplexity (see Appendix E for the calculation details) over all the problems in the MTPB is displayed in the left panel of Table 4. For all models, the single-turn specification has a higher average perplexity than the multi-turn specification. It implies that the multi-turn user specifications can be better understood by the models. We notice that the average perplexity for both multi-turn and single-turn intent specifications under larger models is slightly lower than that under smaller models, indicating that the larger ones understand the user intent better than the smaller ones.\n\nWe compare the program synthesis pass rate with the multi-turn prompts to that with the concatenated single-turn prompts. The results are shown in the right panel of Table 4. Multi-turn specifications lead to close to or more than 10 percentage points over single-turn specifications for all model sizes. Together with the perplexity analysis above, it appears that factorizing a user specification into multiple steps and leveraging the emerged capacity of large language models allow them to digest the specification more easily and synthesize programs more successfully.\n\nFurthermore, we categorize the problems by difficulty level based on their average pass rates (“hard” with less than 30%, “easy” with larger than 70%), and examine the interaction effect between difficulty level and model size on the improvement by multi-turn factorization. See the results in Figure 2. Across almost all model sizes and difficulty levels, multi-turn prompts lead to significant improvement over single-turn prompts and most improvements are nearly or higher than 10 percentage points. Interestingly, the larger models (6.1B and 16.1B) are invariant to multi-turn factorization for easy problems (see the two short bars, 0.19% and −0.25%, in Figure 2). This implies that when the problems can be easily understood by the model (due to the combined effect of easiness of the problems and the high capacity of larger models), it is not necessary or beneficial to factorize the specifications. This is in fact consistent with our motivating assumption that factorizing complicated specifications would ease problem understanding and improve program synthesis.\n\n4.5 QUALITATIVE EXAMPLES\n\nTo further understand the differences in model behavior over model sizes, we examine cases where large models have contrasting performances to smaller models. We specifically select problems for which CODEGEN-MONO 16.1B and CODEGEN-MONO 2.7B show a significant discrepancy in performance. On problems where CODEGEN-MONO 16.1B performed significantly worse compared to CODEGEN-MONO 2.7B, we observe that the larger model becomes inflexible due to taking the prompt literally. For example, initializing a number always results in an integer, despite the prompt asking to cast into a string (Figure 3), or the “return” keyword in a prompt triggers a function definition while the intent is to directly generate an executable program (Figure 4). However in general, larger-scale models overcome mistakes due to prompt misinterpretation by smaller models, including assigning multiple variables at the same time (Figure 5) or understanding the concept of any comparison (Figure 6).\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n5 RELATED WORK\n\nProgram Synthesis While program synthesis has a long history, two inherent challenges remain unsolved: (1) intractability of the program space and (2) difficulty in accurately expressing user intent (Manna & Waldinger, 1971; Gulwani et al., 2017). A large body of prior research attempted to address (1) by exploring methods like stochastic search techniques (Parisotto et al., 2017; Schkufza et al., 2013) and deductive top-down search (Gulwani, 2011; Polozov & Gulwani, 2015). However, the scalability of these approaches is still limited. User intent can be expressed with various methods: formal logical specifications, input-output examples, and natural language descriptions. Complete and formal specifications require too much effort, while informal ones like input-output examples often under-specify problems (Gulwani, 2011). Well-learned conditional distribution and language understanding capacity owing to the large-scale model and data allows for efficient solutions for these two challenges. Several works investigate converting conversational intents into programmable representations, such as SQL (Yu et al., 2019a;b) or dataflow graph (Andreas et al., 2020). Our proposed benchmark requires the generation of Python, which is more general and complex.\n\nLarge Language Models Transformers capture dependency among sequence elements through attention mechanism (Bahdanau et al., 2014) and are highly scalable. It has been successfully applied to natural language processing (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), computer vision (Dosovitskiy et al., 2021), and many other areas (Oord et al., 2018; Jumper et al., 2021). Prior works, such as CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et al., 2020), and CodeT5 (Wang et al., 2021), have applied transformers towards code understanding but these mostly focus on code retrieval, classification, and program repair. Several recent and concurrent efforts explore using large language models for program synthesis (Chen et al., 2021; Austin et al., 2021; Li et al., 2022; Fried et al., 2022) and its effectiveness (Vaithilingam et al., 2022). While they focus on generating code in a single turn, we propose to factorize the specifications into multiple turns and demonstrate that it is highly effective to improve synthesis quality. It is worth pointing out that Austin et al. (2021) explored refining the code in multiple iterations, but it is essentially a single-turn approach since a complete program is produced in every single turn. Prompting pre-trained language models with intermediate information to improve task performance has attracted interest (Nye et al., 2021; Wei et al., 2022). Our proposed MTPB also allows the model to leverage past turns as context.\n\nBenchmarks for Program Synthesis To quantitatively evaluate program synthesis models, several benchmarks have been proposed with different input forms. A popular input forms include preceding code in the same line (Raychev et al., 2016), pseudo-code (Kulal et al., 2019), a docstring and function signature (Chen et al., 2021), or problem description (Hendrycks et al., 2021). In most of those cases, only directly relevant input information is given to the model. In contrast, a few previous works instantiate benchmarks that measure the ability to generate programs given surrounding program context beyond the target program, such as variables and other methods (Iyer et al., 2018) or alternating “cells” of preceding code and text blocks (Agashe et al., 2019), while the primary focus is to generate the target program itself. We propose a new benchmark that requires a progressive generation of subprograms through multi-turn prompts.\n\n6 CONCLUSION\n\nWe study program synthesis with large causal language models trained on large corpora of code data. The capacity to understand long context and generate coherent responses emerges from the simple language modeling as the model size and data size scale up. Leveraging this capacity and observing that better user intent understanding leads to better program synthesis, we propose a multi-step program synthesis approach in which program synthesis is achieved through a multi-turn specification and code generation. Moreover, we develop the Multi-Turn Programming Benchmark (MTPB) to investigate our models’ capacity on synthesizing programs in such a multi-step paradigm. Our experiments show that the multi-step program synthesis capacity scales as a function of the model size and data size. The intent specifications, which are specified in multiple steps, are digested more easily by the models and lead to more accurate program synthesis. We open-source the training code and the model checkpoints to facilitate future research and practical applications in this area.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nBROADER IMPACT AND ETHICAL CONSIDERATIONS\n\nAll variants of CODEGEN are firstly pre-trained on the Pile, which includes a small portion of profane language. Focusing on the GitHub data that best aligns our expected use case of program synthesis, Gao et al. (2020) report that 0.1% of the data contained profane language, and has sentiment biases against gender and certain religious groups. Thus, while we did not observe in our samples, CODEGEN may generate such content as well. In addition to risks on natural language outputs (e.g., docstrings), generated programs may include vulnerabilities and safety concerns, which are not remedied in this work. Models should not be used in applications until being treated for these risks.\n\nREFERENCES\n\nRajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. Juice: A large scale distantly supervised dataset for open domain context-based code generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5436–5446, 2019.\n\nJacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, et al. Task-oriented dialogue as dataflow synthesis. Transactions of the Association for Computational Linguistics, 8:556–571, 2020.\n\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/ 10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen.\n\nCodet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n\nAlvin Cheung, Armando Solar-Lezama, and Samuel Madden. Optimizing database-backed applica-\n\ntions with query synthesis. ACM SIGPLAN Notices, 48(6):3–14, 2013.\n\nColin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. Pymt5: multi-mode translation of natural language and python code with transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9052–9065, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\n\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1536–1547, 2020.\n\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nSumit Gulwani. Automating string processing in spreadsheets using input-output examples. ACM\n\nSigplan Notices, 46(1):317–330, 2011.\n\nSumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and\n\nTrends® in Programming Languages, 4(1-2):1–119, 2017.\n\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id= sD93GOzH3i5.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\n\ndegeneration. In ICLR, 2020. URL https://openreview.net/forum?id=rygGQyrFvH.\n\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1643–1652, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1192. URL https://aclanthology.org/ D18-1192.\n\nRajeev Joshi, Greg Nelson, and Keith Randall. Denali: A goal-directed superoptimizer. ACM\n\nSIGPLAN Notices, 37(5):304–314, 2002.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual embedding of source code. In International Conference on Machine Learning, pp. 5110–5121. PMLR, 2020.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),\n\n2015. URL http://arxiv.org/abs/1412.6980.\n\nSumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing Systems, 32, 2019.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode, Feb 2022.\n\nZohar Manna and Richard J Waldinger. Toward automatic program synthesis. Communications of\n\nthe ACM, 14(3):151–165, 1971.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n\nPavel Panchekha, Alex Sanchez-Stern, James R Wilcox, and Zachary Tatlock. Automatically improving accuracy for floating point expressions. ACM SIGPLAN Notices, 50(6):1–11, 2015.\n\nEmilio Parisotto, Abdel rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. In ICLR (Poster), 2017. URL https://openreview. net/forum?id=rJ0JwFcex.\n\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural\n\nnetworks. In International conference on machine learning, pp. 1310–1318. PMLR, 2013.\n\nOleksandr Polozov and Sumit Gulwani. Flashmeta: A framework for inductive program synthesis. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications, pp. 107–126, 2015.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020.\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–16. IEEE, 2020.\n\nVeselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision trees.\n\nACM SIGPLAN Notices, 51(10):731–747, 2016.\n\nEric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. ACM SIGARCH\n\nComputer Architecture News, 41(1):305–316, 2013.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with\n\nrotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n\nPriyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pp. 1–7, 2022.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz In Advances in neural information\n\nKaiser, and Illia Polosukhin. Attention is all you need. processing systems, pp. 5998–6008, 2017.\n\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\n\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. Codet5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, 2021.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. CoSQL: A conversational text-to-SQL challenge In Proceedings of the 2019 towards cross-domain natural language interfaces to databases. Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1962–1979, Hong Kong, China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1204. URL https://aclanthology.org/D19-1204.\n\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir Radev. SParC: Cross-domain semantic parsing in context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4511–4523, Florence, Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1443. URL https://aclanthology.org/P19-1443.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA MODEL TRAINING\n\nTo evaluate the emergence of multi-turn program synthesis capabilities under scaling laws, we adopt standard transformer-based autoregressive language models, varying (1) the number of model parameters (350M, 2.7B, 6.1B, 16.1B) and (2) the number of tokens of programming languages in the training corpora. For scaling the models, a custom library JAXFORMER for training large language models on TPU-v4 hardware was developed and will be released as open source, including the trained model weights.\n\nA.1 DATASETS\n\nDataset\n\nLanguage\n\nRaw Size\n\nFinal Size\n\nFinal Tokens\n\nTHEPILE\n\nNatural Language Code\n\n825.18 GiB 1159.04 GiB 95.16 GiB\n\n95.16 GiB\n\nBIGQUERY\n\nC C++ Go Java JavaScript Python\n\n1772.1 GiB 205.5 GiB 256.4 GiB 335.1 GiB 1282.3 GiB 196.8 GiB\n\n48.9 GiB 69.9 GiB 21.4 GiB 120.3 GiB 24.7 GiB 55.9 GiB\n\nBIGPYTHON\n\nPython\n\n5558.1 GiB\n\n217.3 GiB\n\n354.7B 31.6B\n\n19.7B 25.5B 9.6B 35.4B 9.7B 19.3B\n\n71.7B\n\nTable 5: Approximate statistics for training corpora along the pre-processing steps.\n\nFor each dataset, the pre-processing shares the following steps: (1) filtering, (2) deduplication, (3) tokenization, (4) shuffling, and (5) concatenation. For details on THEPILE, we refer to Gao et al. (2020). For BIGQUERY and BIGPYTHON, in (1) files are filtered by file extension, and files with average lines length of <100 characters, a maximum line length of 1, 000, and >90% of the characters being decimal or hexadecimal digits are removed. For (2), exact duplicates based on their SHA-256 hash are removed, which amounts to a substantial portion of the raw data due to forks and copies of repositories. For (3), the BPE vocabulary of GPT-2 is extended by special tokens representing repeating tokens of tabs and white spaces. In the multi-lingual setting of BIGQUERY, a prefix is prepended to indicate the name of the programming language. For (4), each year of data is randomly shuffled. For (5), sequences are concatenated to fill the context length of 2, 048 tokens with a special token as a separator. Table 5 summarizes the statistics of the training corpora.\n\nCODEGEN-NL models are randomly initialized and trained on THEPILE. CODEGEN-MULTI models are initialized from CODEGEN-NL and then trained on the BIGQUERY. CODEGEN-MONO models are initialized from CODEGEN-MULTI and then trained on BIGPYTHON.\n\nA.2 MODELS\n\nOur models are autoregressive transformers with the regular next-token prediction language modeling as the learning objective. The family of CODEGEN models is trained in various sizes with 350M, 2.7B, 6.1B, and 16.1B parameters. The first three configurations allow for direct comparison with opensourced large language models trained on text corpus, GPT-NEO (350M, 2.7B) (Black et al., 2021) and GPT-J (6B) (Wang & Komatsuzaki, 2021). See Table 6 in Appendix A for model specifications.\n\nThe architecture follows a standard transformer decoder with left-to-right causal masking. For the positional encoding, we adopt rotary position embedding (Su et al., 2021). For the forward pass, we execute the self-attention and feed-forward circuits in parallel for improved communication overhead following Wang & Komatsuzaki (2021), that is, xt+1 = xt + mlp(ln(xt + attn(ln(xt)))) is altered to xt+1 = xt + attn(ln(xt)) + mlp(ln(xt)) for which the computation of self-attention, attn(), and feed-forward, mlp(), with layer-norm, ln(), is simultaneous. The architecture and hyper-parameter choices were optimized specifically for the hardware layout of TPU-v4.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nModel\n\nDataset\n\nHyper-parameter\n\nCODEGEN\n\nCODEGEN-NL\n\nTHEPILE\n\nCODEGEN-MULTI\n\nBIGQUERY\n\nCODEGEN-MONO\n\nBIGPYTHON\n\nNumber of layers Number of heads Dimensions per head Context length Batch size Weight decay\n\nLearning rate Warm-up steps Warm-up / Total steps\n\nLearning rate Warm-up steps Total steps\n\nLearning rate Warm-up steps Total steps\n\n350M\n\n20 16 64 2,048 500k 0.1\n\n3.0e−4 3k 350k\n\n1.8e−4 3k 150k\n\n1.8e−4 3k 150k\n\n2.7B\n\n32 32 80 2,048 1M 0.1\n\n1.6e−4 3k 350k\n\n0.8e−4 3k 150k\n\n0.8e−4 3k 150k\n\n6.1B\n\n33 16 256 2,048 2M 0.1\n\n1.2e−4 3k 350k\n\n0.4e−4 3k 150k\n\n0.4e−4 3k 150k\n\n16.1B\n\n34 24 256 2,048 2M 0.1\n\n0.9e−4 3k 350k\n\n0.5e−4 3k 150k\n\n0.5e−4 3k 150k\n\nTable 6: Hyper-parameters for model specification and optimization for the family of CODEGEN models.\n\nA.3 TRAINING\n\nThe scaling of large language models requires data and model parallelism. Google’s TPU-v4 hardware with a high-speed toroidal mesh interconnect naturally allows for efficient parallelism. To efficiently utilize the hardware, the training of the models is implemented in JAX (Bradbury et al., 2018). For parallel evaluation in JAX the pjit()9 operator is adopted. The operator enables a paradigm named single-program, multiple-data (SPMD) code, which refers to a parallelism technique where the same computation is run on different input data in parallel on different devices.10 Specifically, pjit() is the API exposed for the XLA SPMD partitioner in JAX, which allows a given function to be evaluated in parallel with equivalent semantics over a logical mesh of compute.\n\nOur library JAXFORMER recruits a designated coordinator node to orchestrate the cluster of TPUVMs11 with a custom TCP/IP protocol. For data parallelism, the coordinator partitions a batch and distributes the partitions to the individual TPU-VMs. For model parallelism, two schemes for the sharding of model parameters are supported: (1) Intra-TPU-VM, where parameters are sharded across MXU cores12 inside a physical TPU-v4 board and replicated across boards following Shoeybi et al. (2019); Wang & Komatsuzaki (2021); (2) Inter-TPU-VM, where parameters are sharded across TPU-v4 boards and activations are replicated following Rajbhandari et al. (2020).\n\nBoth intra-TPU-VM and inter-TPU-VM sharding schemes are implemented based on our specific pjit() a logical mesh specification (r, p, c) with r replicas of the parameters, p partitions of the parameters, and c logical cores per board over nb TPU boards with each nc logical cores such that d × p = nb and r × p × c = nb × nc.\n\nThe intra-TPU-VM scheme is adopted for models of size of less or equal to 6B parameters, the total amount of model and optimizer parameters fit into the combined HBM memory of a single TPU-v4 board. For instance, a TPU-v4-512 slice with nb = 64 and nc = 4 would be configured as (r, p, c) = (64, 1, 4). That is, the parameters are being replicated across r = 64 boards with p = 1 total inter-board partitions and intra-board parallelism across c = 4 logical chips. In this configuration, the mean gradient is accumulated across boards via with_sharding_constraint(), effectively emulating the behavior of the xmap()13 operator.\n\n9https://jax.readthedocs.io/en/latest/_modules/jax/experimental/pjit.html 10https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html 11https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms 12Specifically, 4 TPU-v4 chips (i.e., 8 physical which amount 4 logical or virtual MXU cores). 13https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.maps.xmap.html\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nThe inter-TPU-VM scheme is adopted for models exceeding the size of 6B parameters for which the model and optimizer parameters have to be sharded across TPU-v4 boards. For instance, a TPU-v4-512 slice with nb = 64 and nc = 4 would be configured as (r, p, c) = (1, 64, 4). For larger slices such as TPU-v4-1024 with nb = 128, one may introduce redundancy in the parameter sharding, e.g., (r, p, c) = (2, 64, 4). In this configuration, the activations are replicated across boards via with_sharding_constraint(). Moreover, (r, p, c) allows for backwards compatibility for the logical hardware layout transition from TPU-v3 with c = 8 to TPU-v4 with c = 4 by adjusting p without the need for re-sharding.\n\nFor the optimization, Table 6 summarizes the hyper-parameters. We adopt the Adam (Kingma & Ba, 2015) optimizer with (β1, β2, ε) = (0.9, 0.999, 1e−08) and global gradient norm clipping (Pascanu et al., 2013) of 1.0. The learning rate function over time follows GPT-3 (Brown et al., 2020) with warm-up steps and cosine annealing. In summary, we mainly adopted the GPT-3 reference configurations with minor variations accounting for TPU optimizations. We did not have the compute capacity to optimize these hyper-parameters further.\n\nB PASS@k ESTIMATOR\n\nWe use the unbiased estimator proposed in Chen et al. (2021) to compute pass@k. For each task, n ≥ k samples are sampled. In particular, we use n = 200 and k ≤ 100. Suppose c is the number of correct samples, among the n samples, which pass all the unit tests. Then the unbiased estimator is defined as follows:\n\npass@k = EProblems\n\n1 −\n\n(cid:34)\n\n(cid:35)\n\n(cid:1)\n\n(cid:0)n−c k\n(cid:0)n k\n\n(cid:1)\n\n(1)\n\nDirectly computing this estimator is numerically unstable. We use the numerically stable numpy implementation introduced by Chen et al. (2021).\n\nC TYPE-RELAXED EQUIVALENCE CHECK FOR MTPB EVALUATION\n\nWe perform the following type-relaxation before assessing the equivalence between model outputs and the expected outputs.\n\n• Convert numpy arrays into correspondingly typed lists of standard types (e.g. np.int32 will\n\nbe cast to int).\n\n• pandas series are converted and compared in numpy array format.\n\n• For the rest, model outputs are cast into the type of gold standard outputs. • Floating numbers are compared with ε = 1e−6 as the tolerance threshold.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nD LIST OF MTPB PROBLEMS\n\nProblem Name\n\nProblem Description\n\nCategory\n\nSandwich string Normalize integer list Convert time Squared Fibonacci Compare counts Pandas mean Fizz buzz Bi-grams Top note Hex to binary Invert dict Class definition Longest number Linear regression Encrypt and decrypt Dedup custom objects Drunken python Morse code Two-sum k-means Even odd sum Shift zeros Bootstrap 95% CI Sum even digits Min-max diff Distinct chars Longer string Sum float digits Count vowels Factorial Max edge triangle Factorial & remainder Sum polygon angles Sum string numbers Min-max sum Vowel overlap Sum negative Load dataset Char length list Hex to RGB Majority vote Week later Sorted word weights Create Palindrome Simulate Backspace Data manipulation Sum non-overlap Detect digits Cascading functions Pluralize duplicates Highest altitude Truncate words Single element Remove elements Check array sum\n\nstring Append a string in the middle of another string. Normalize a list of positive integers and print formatted percentages. math math Convert units of time. math Print the squared Fibonacci numbers. array Compare the count of positive and negative numbers in a given list. D.S. Construct and compute the mean of a pandas DataFrame. Algo. Solve the fizz buzz problem. string Print the bi-grams of a sentence. dict Print the name with top note out of a dictionary. math Convert hex to binary and reverse. dict Detect an inversion of a given dictionary. class Create a POJO class. math Print the longest number. D.S. Fit linear regression model with specified function and sk-learn. Algo. Rotate alphabet for encryption, then reverse the operation. class Implement a class with __hash__ and obtain a count unique objects. string Convert between integer and string without using built-in functions. Algo. Encode a string into morse code given its conversion rule. Algo. Implement the two-sum problem on a given input pair. D.S. Implement and run k-means on sampled points. math Print the sum of even and odd numbers in a list. array Move all the zeros in a list to the right. D.S. Calculate the bootstrap 95% confidence interval of an array. math Sum the even digits between two numbers. array Compute the difference between max and min numbers in a list. string Print the sorted, case-insensitive unique characters of a string. string Compare and print the longer string given two strings. math Sum numbers before and after the decimal point of a float. string Count the number of vowels in a string. math Compute the factorial of n. math Finds the maximum range of a triangle’s third edge. math Compute the factorial and its remainder when divided. math Sum the angles in a polygon. string Add together two numbers represented in string. array Sum the range from the minimum to the maximum of a list. string Find the number of overlapped vowels of two words. math Calculate the sum of negative numbers in a list. D.S. Load from a file and print statistics. string Return a list of non-punctuation character lengths from words. math Convert a six hexadecimal digit string into list of RGB values. array Check if a certain element is the majority of a given list. string Print the formatted date of a week later given a date. math Check if the list of word weights (sum of ASCII values) are sorted. string Sum pairs of adjacent digits until the number is palindrome. string Apply the backspace characters in a string and print the modified. D.S. Manipulate a pandas DataFrame and split into train and test set. array Sum the integers in a (min, max) range that don’t appear in a list. array Find if a string contains digits. math Sequentially invoke function objects in a given list. dict Pluralize duplicated words in a list. array Given relative altitudes , find the highest altitude array Truncate a sentence so that it contains k words array Find the elements that appear one time in an array array Remove all the occurrences of an element in an array array Check whether the sum of an array is equal to a given value\n\nTable 7: Problems in MTPB, showing the problem 1 to 55. D.S. and Algo. refers to data science and algorithm.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nProblem Name\n\nProblem Description\n\nCategory\n\nMerge two sorted lists into one Find the max contiguous subarray and return the sum Find the largest integer but smaller than the square root Find the longest word in a word list Sum all the unique numbers in a list Compute the diagonal sum of a matrix Check condition number of a matrix is less than a threshold\n\nMerge sorted lists Maximum subarray Max square root integer Longest word Sum unique elements Diagonal sum Matrix condition number Matrix multiplication sum Compute matrix multiplication sum of two matrices Matrix determinant Log-sum-exp K nearest points Longest common prefix Duplicate elements First unique character Uncommon words Average words length Compare char freq Reverse string Square Sum diff Cosine sim Vector distance Smallest standard dev. Smallest means Coefficient of variation L1 norm Z-statistic Move negatives Remove alphabets Largest norm F1 score Add Space Remove outlier Convert to categorical Group by key Max stock profit Sum positions Find missing num Common num in matrix Sum Collatz Cup swap Reverse digits Calculate arrows Check interval num Length encoding Convert email Second largest Largest prefix sum Closest element to zero Consecutive unique char Highest frequency char Longest palindrome Count primes Rotate array Partition equal sets Square root integer Plus 1 Check square sum Compare standard dev. Matrix size Diff mean and median\n\nCompare two matrix determinants Compute the log of sum exponential input Find the k nearest points to the origin Find the longest common prefix of two strings Find duplicates in a list Find the first non-repeating character in a string Find uncommon words in two sentences Compute the average word length of a sentence Compare the character frequencies in two strings Reverse a string Difference between the square of sum and the sum of squares Compute the cosine similarity between two vectors Compare vector distances to the origin Find the smaller standard deviation given two lists Find the smaller mean given two lists Compute coefficient of variation given a list Compute the L1 norm given a list Compute z-statistic given a list Move all negative elements in a list to the end Remove alphabetical characters in a string Find the largest norm among n-dimensional points Given two arrays (pred, gold), calculate the F1 score Add spaces before capital letters Remove data points in the tail (2sigma) of normal distribution Convert values into categorical variables Group items in an array using a provided function Given an array of \"prices\", find the max profit Sum of all position indices where a value appear Find a missing number given a list and a max number Common numbers among rows in a matrix Obtain the sum of Collatz sequence starting from given number Name the location of a \"ball\" after cup swapping Reverse digits in a number with a stack Calculate arrowheads left and right Check if the interval (max-min) is included in a list Encode a string by converting repeated chars with counts Use regex to match email addresses and remove special chars Print out the second largest element in an array Return the largest prefix sum in an array Find the element which is the closest to 0 and print the distance Find the max length contiguous subarray with unique characters Obtain the frequency of the most frequent character Find the length of longest palindrome substring Calculate prime numbers in a range Rotate an array to the right k steps Check if an array can be split into two sets with equal sums Compute the integer part of square root Return the digits after an integer is added by 1 Check whether one integer is a sum of two square numbers Determine whether standard deviation is less than 1 Calculate the sum of row and column numbers Calculate the difference between mean and median for an array\n\nAlgo. Algo. Algo. Algo. Algo. D.S. D.S. D.S. D.S. D.S. array Algo. array Algo. Algo. Algo. string string math math math D.S. D.S. D.S. D.S. D.S. array string D.S. D.S. string D.S. D.S. array array array array array Algo. Algo. Algo. Algo. Algo. string string array array array string string string Algo. Algo. Algo. math math math D.S. D.S. D.S.\n\nTable 8: Problems in MTPB, showing the problem 56 to 115. D.S. and Algo. refers to data science and algorithm.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nE PERPLEXITY COMPUTATION FOR SINGLE- AND MULTI-TURN PROMPTS\n\ni=1 is the set of prompts for a given problem, and {si}n\n\nSuppose {pi}n i=1 are the n sub-programs synthesized by a model Pθ. Suppose ci−1 = [p1; s1; ...; pi−1; si−1] where [· ; ·] indicates concatenation, the conditional probability of pi is Probi = Pθ(pi|ci−1), and then the perplexity for the multi-turn prompts is computed as\n\nPPLMulti−turn = exp\n\n−\n\n(cid:32)\n\n(cid:33)\n\nlog Probi\n\n,\n\n1 m\n\nn (cid:88)\n\ni=1\n\n(2)\n\nwhere m is the total number of tokens of all prompts {pi}n its probability is Prob = Pθ(c), and the the perplexity for the single-turn prompts is computed as\n\ni=1. Suppose c = [p1; s1; ..., pn, sn], then\n\nPPLSingle−turn = exp\n\n−\n\n(cid:18)\n\n(cid:19)\n\nlog Prob\n\n.\n\n1 m\n\n(3)\n\nF PERPLEXITY COMPARISON FOR CODEGEN-NL AND CODEGEN-MULTI\n\nCODEGEN-NL\n\n350M 2.7B 6.1B\n\nPass Non-Pass\n\n4.53 4.96\n\n3.25 3.87\n\n2.78 3.65\n\nTable 9: Average prompt perplexity↓ of CODEGEN-NL models on pass and non-pass problems.\n\nCODEGEN-MULTI\n\n350M 2.7B\n\n6.1B\n\nPass Non-Pass\n\n4.78 5.64\n\n3.82 4.85\n\n3.82 4.80\n\nTable 10: Average prompt perplexity↓ of CODEGEN-MULTI models on pass and non-pass problems.\n\nG ADDITIONAL BENCHMARK RESULTS\n\nModel\n\npass@1\n\npass@10\n\npass@100\n\nCODEGEN-NL 350M CODEGEN-NL 2.7B CODEGEN-NL 6.1B CODEGEN-NL 16.1B CODEGEN-MULTI 350M CODEGEN-MULTI 2.7B CODEGEN-MULTI 6.1B CODEGEN-MULTI 16.1B CODEGEN-MONO 350M CODEGEN-MONO 2.7B CODEGEN-MONO 6.1B CODEGEN-MONO 16.1B INCODER 6B code-cushman-001 code-davinci-001 code-davinci-002\n\n0.96 5.34 8.15 10.92 7.46 18.06 18.35 20.94 14.59 27.31 32.48 35.28 21.30 45.90 51.80 58.10\n\n6.37 24.63 31.21 38.43 24.18 45.80 47.27 51.61 41.49 59.19 64.20 67.32 46.50 66.90 72.80 76.70\n\n19.91 48.95 55.27 62.76 46.37 65.34 67.92 70.02 63.00 74.24 76.81 80.09 66.20 79.90 84.10 84.50\n\nTable 11: Pass rates on Mostly Basic Python Problems (MBPP).\n\nWe also evaluated our models on Mostly Basic Python Problems (MBPP) (Austin et al., 2021). The results are displayed in Table 11. Following Chen et al. (2022), we sampled programs from the\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nsanitized MBPP for all of our models, with n = 100 and temperature= 0.8. The last four rows are from the aforementioned paper. In general we observe the consistent trend of improving the performance over different versions (NL, Multi, Mono), with our largest CODEGEN-MONO 16.1B approaching the results from code-cushman-001. While we do not know whether any of OpenAI models is the “Codex 12B” reported in Chen et al. (2021), we believe our model achieves reasonable results on MBPP as well. We also note that our CODEGEN-MONO 6.1B significantly outperformed INCODER 6B.\n\nH GENERATED SAMPLES\n\nH.1 CASES WHERE CODEGEN-MONO 16.1B UNDER-PERFORMS\n\n# Import libraries.\n\n# Import libraries.\n\nimport numpy as np\n\nimport numpy as np\n\n# Initialize a variable \"stack\" with an empty list, and \"num\" with 100 as a string.\n\n# Initialize a variable \"stack\" with an empty list, and \"num\" with 100 as a string.\n\nstack = [] num = \"100\"\n\nstack = [] num = 100\n\n# For each chracter in \"num\", append the\n\n# For each chracter in \"num\", append the\n\ncharacter to \"stack\".\n\ncharacter to \"stack\".\n\nfor i in range(len(num)):\n\nstack.append(num[i])\n\nfor i in num:\n\nstack.append(i)\n\n# Assign an empty string to a variable \"\n\n# Assign an empty string to a variable \"\n\nresult\", and concatenate characters popped from the last element of \"stack\" to \"result\" until \"stack\" is empty.\n\nresult\", and concatenate characters popped from the last element of \"stack\" to \"result\" until \"stack\" is empty.\n\nresult = \"\" while len(stack)!= 0:\n\nresult += stack.pop()\n\nresult = \"\" while len(stack) > 0:\n\nresult += stack.pop()\n\n# Cast \"result\" as integer and print it out.\n\n# Cast \"result\" as integer and print it out.\n\nprint(int(result))\n\nprint(int(result))\n\nCODEGEN-MONO 2.7B\n\nCODEGEN-MONO 16.1B\n\nFigure 3: Generated samples for “Reverse digits” problem. While being simplistic, we often observe that CODEGEN-MONO 16.1B assumes the type when assigning a number to a variable. Here on the right, despite being explicitly specified to initialize a number as a string, the larger model fails to do so. This causes an error in the following turn when the code attempts to iterate over num.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n# Import libraries.\n\n# Import libraries.\n\nimport numpy as np\n\nimport numpy as np\n\n# Assign the matrix \"[[3, 2], [2, 3]]\" to a\n\n# Assign the matrix \"[[3, 2], [2, 3]]\" to a\n\nvariable named \"my_matrix\".\n\nvariable named \"my_matrix\".\n\nmy_matrix = [[3, 2], [2, 3]]\n\nmy_matrix = [[3, 2], [2, 3]]\n\n# Assign the number \"1\" to a variable named\n\n# Assign the number \"1\" to a variable named\n\n\"t\".\n\nt = 1\n\n\"t\".\n\nt = 1\n\n# Compute the condition number of my_matrix\n\n# Compute the condition number of my_matrix\n\nand store as result.\n\nand store as result.\n\nresult = np.linalg.cond(my_matrix)\n\ncondition_number = np.linalg.cond(my_matrix)\n\n# Check whether the result is smaller than t\n\n# Check whether the result is smaller than t\n\n. If yes, return \"True\", otherwise return \"False\".\n\n. If yes, return \"True\", otherwise return \"False\".\n\nif result < t:\n\nprint(\"True\")\n\nelse:\n\nprint(\"False\")\n\ndef check_condition_number(condition_number,\n\nt): if condition_number < t:\n\nreturn True\n\nelse:\n\nreturn False\n\nCODEGEN-MONO 2.7B\n\nCODEGEN-MONO 16.1B\n\nFigure 4: Generated samples for “Matrix condition number” problem. Both models generate programs in the same scope until the last turn, where CODEGEN-MONO 16.1B generates in the form of function definition. Noticeably, the larger model generates return keyword that appears in the prompt, but it is only valid within a function.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nH.2 CASES WHERE CODEGEN-MONO 16.1B OUTPERFORMS\n\n# Import libraries.\n\n# Import libraries.\n\nimport numpy as np\n\nimport numpy as np\n\n# Define a variable \"pos\" with \"B\", \"swap\"\n\n# Define a variable \"pos\" with \"B\", \"swap\"\n\nwith [’AC’, ’CA’].\n\nwith [’AC’, ’CA’].\n\npos = [’B’,’swap’]\n\npos = \"B\" swap = [’AC’, ’CA’]\n\n# Write a function \"move\" that takes two strings x and y as input, and replace any appearance of x in y with an empty string, then return y.\n\n# Write a function \"move\" that takes two strings x and y as input, and replace any appearance of x in y with an empty string, then return y.\n\ndef move(x,y):\n\nreturn y.replace(x,’’)\n\ndef move(x, y):\n\n# For each element in \"swap\", if it contains \"pos\", call \"move\" on \"pos\" and the\n\ncurrent element and store the result to \"pos\".\n\nreturn y.replace(x, \"\")\n\n# For each element in \"swap\", if it contains \"pos\", call \"move\" on \"pos\" and the\n\ncurrent element and store the result to \"pos\".\n\ndef swap(swap):\n\nfor i in range(len(swap)):\n\nif swap[i] == ’pos’:\n\nfor i in swap:\n\nif pos in i:\n\nswap[i] = move(pos[0],pos[1])\n\npos = move(pos, i)\n\nreturn swap\n\n# Print out \"pos\".\n\nprint(pos)\n\n# Print out \"pos\".\n\nprint(pos)\n\nCODEGEN-MONO 2.7B\n\nCODEGEN-MONO 16.1B\n\nFigure 5: Generated samples for “Cup swap” problem. Here, CODEGEN-MONO 2.7B mistakenly treats “swap” as a part of a value t o be assigned, while CODEGEN-MONO 16.1B parses the intent correctly and assigns two variables in parallel.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n# Import libraries.\n\n# Import libraries.\n\nimport numpy as np\n\nimport numpy as np\n\n# Initialize the variable named lst1 with a\n\n# Initialize the variable named lst1 with a\n\nlist [’abc’, ’abc10’].\n\nlist [’abc’, ’abc10’].\n\nlst1 = [’abc’, ’abc10’]\n\nlst1 = [’abc’, ’abc10’]\n\n# Create a function called num_in_str() to\n\n# Create a function called num_in_str() to\n\ncheck whether a string contains a number .\n\ncheck whether a string contains a number .\n\ndef num_in_str(s):\n\nreturn any(i.isdigit() for i in s)\n\n# Call the function num_in_str() to find strings in lst1 that have numbers and assign them to a list named lst2\n\nlst2 = [i for i in lst1 if num_in_str(i)]\n\n# Print out lst2\n\nprint(lst2)\n\ndef num_in_str(string): for i in string:\n\nif i.isnumeric():\n\nreturn True\n\nelse:\n\nreturn False\n\n# Call the function num_in_str() to find strings in lst1 that have numbers and assign them to a list named lst2\n\nlst2 = []\n\nfor i in lst1:\n\nif num_in_str(i):\n\nlst2.append(i)\n\n# Print out lst2\n\nprint(lst2)\n\nCODEGEN-MONO 2.7B\n\nCODEGEN-MONO 16.1B\n\nFigure 6: Generated samples for “Detect digits” problem. CODEGEN-MONO 2.7B fails to implement the num_in_str, resulting in checking only the first character. In contrast, CODEGEN-MONO 16.1B successfully utilizes any function to scan all the characters in the given string.\n\nI ADDITIONAL ANALYSES ON MTPB\n\nWe conducted additional analyses to illustrate the relationship generated program length and pass rate and showed the results in Figure 7, Figure 8, and Figure 9. The relationship between generated program length and prompt length is shown in Figure 10.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Maximum Length of Completion versus Pass Rate.\n\nFigure 8: Maximum Length of Completion versus Pass Rate.\n\nFigure 9: Maximum Length of Completion versus Pass Rate.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Prompt Length versus Generated Program Length.\n\n25\n\n150200250Token count relationship between prompts and generated programs from CodeGen-16B-Monoy=xTurn020406080100Number of tokens in prompt020406080100120140Number of tokens in generated program",
    "reference": "# Summary Of The Paper\n\nThe paper presents several large autoregressive language models trained using large datasets scraped from the web, and in particular datasets of code from GitHub. The authors evaluate the models on tasks of generating code from natural language. First, the authors show state-of-the-art results on the HumanEval dataset and report that larger models also demonstrate lower perplexity on the prompt (the natural language program specification). The authors then describe the Multi-Turn Programming Benchmark (MTPB), in which the natural language intents are interleaved with code and each intent describes what the immediately subsequent code should do. Compared to a baseline where all the natural language is provided to the model concatenated together, the multi-turn version shows improvement.\n\n# Strength And Weaknesses\n\n# Strengths\n- The paper reports state-of-the-art results for the HumanEval benchmark for code generation from natural language specifications.\n- The authors provide downloadable checkpoints for the trained models, which can be a useful resource for the community.\n- The paper introduces a multi-turn natural language to code dataset where the natural language instructions are interleaved with the intended generated code. \n\n# Weaknesses\n- The model is evaluated only on one existing dataset, HumanEval, and not other similar datasets like APPS or MBPP.\n- While the work represents a significant expenditure of resources, the amount of technical novelty is relatively low, and some of the new aspects in the construction of the model itself are not studied or described as much as they could be:\n  - The \\textsc{BigPython} dataset seems responsible for much of the gains in the work, but the description of it in the paper is limited to \"We have compiled public, non-personal information from GitHub consisting of permissively licensed Python code in October 2021\".\n  - There is significant space dedicated to describing the TPU-based training setup with JAX, but not much about the reasoning behind it.\n- The paper doesn't have much experiments about hyperparameters and training data selection (e.g. the order in which the datasets were used for training), or further information/justification for the values used. \n- There is no release of training datasets.\n- The deduplication methodology used during dataset preprocessing may be too weak. Only removing exact duplicate files may still leave many files which are very similar (for example, varying only in some comments at the top of the file). For more discussion on another code dataset, see https://twitter.com/miltos1/status/1497126435261083649\n- Considering that copies of the HumanEval dataset may have been posted to GitHub, the authors should investigate the possibility of training dataset contamination.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n# Clarity\nI did not realize until quite late that the $\\textsc{CodeGen-Multi}$ and $\\textsc{CodeGen-Mono}$ models are trained on, respectively, The Pile plus the BigQuery dataset, and The Pile plus BigQuery plus BigPython. That fact is only mentioned in Section 4.3 and in the appendix, and not where the models are initially described (Section 2.1 and 2.2). This can be clearer in Section 2.\n\n# Originality\nThe multi-turn dataset introduced in the paper seems to have quite a bit of similarity to SPoC (from Kulal et al., 2019), although SPoC is generally at a lower level of granularity. A more detailed discussion about it would have been helpful.\n\n# Quality\n- The full model performs well on the HumanEval dataset, and performance scales with increasing data and model size, showing that the model training worked successfully. \n- It is unclear why Table 2 only has information about $\\textsc{CodeGen-Mono}$ and not the other models. More data here would be more convincing.\n- For the multi-turn dialogue data, the instructions for subsequent turns only make sense if the previous turns generated correct code (or at least code in line with the authors' intent). As such, it would be useful to see how well the model does when all the output for all previous turns are correct.\n\n# Summary Of The Review\n\nI think the model snapshots from the paper are a highly useful contribution to the community and I based my recommendation largely on that point.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nFor other models trained on code from GitHub and similar sources, like OpenAI Codex, there have been some concerns raised about whether they may violate the copyright of the original authors."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLOCALIZED GRAPH CONTRASTIVE LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nContrastive learning methods based on InfoNCE loss are popular in node representation learning tasks on graph-structured data. However, its reliance on data augmentation and its quadratic computational complexity might lead to inconsistency and inefficiency problems. To mitigate these limitations, in this paper, we introduce a simple yet effective contrastive model named Localized Graph Contrastive Learning (LOCAL-GCL in short). LOCAL-GCL consists of two key designs: 1) We fabricate the positive examples for each node directly using its firstorder neighbors, which frees our method from the reliance on carefully-designed graph augmentations; 2) To improve the efficiency of contrastive learning on graphs, we devise a kernelized contrastive loss, which could be approximately computed in linear time and space complexity with respect to the graph size. We provide theoretical analysis to justify the effectiveness and rationality of the proposed methods. Experiments on various datasets with different scales and properties demonstrate that in spite of its simplicity, LOCAL-GCL achieves quite competitive performance in self-supervised node representation learning tasks on graphs with various scales and properties.\n\n1\n\nINTRODUCTION\n\nSelf-supervised learning has achieved remarkable success in learning informative representations without using costly handcrafted labels (van den Oord et al., 2018; Devlin et al., 2019; Banville et al., 2021; He et al., 2020; Chen et al., 2020; Grill et al., 2020; Zhang et al., 2021; Gao et al., 2021). Among current self-supervised learning paradigms, InfoNCE loss (van den Oord et al., 2018) based multi-view contrastive methods (He et al., 2020; Chen et al., 2020; Gao et al., 2021) are recognized as the most widely adopted ones, due to their solid theoretical foundations and strong empirical results. Generally, contrastive learning aims at maximizing the agreement between the latent representations of two views (e.g. through data augmentation) from the same input, which essentially maximizes the mutual information between the two representations (Poole et al., 2019). Inheriting the spirits of contrastive learning on vision tasks, similar methods have been developed to deal with graphs and bring up promising results on common node-level classification benchmarks (Velickovic et al., 2019; Hassani & Ahmadi, 2020; Zhu et al., 2020b; 2021).\n\nThe challenge, however, is that prevailing contrastive learning methods rely on predefined augmentation techniques for generating positive pairs as informative training supervision. Unlike grid-structured data (e.g., images or sequences), it is non-trivial to define well-posed augmentation approaches for graph-structured data Zhu et al. (2021); Zhang et al. (2021). The common practice adopted by current methods resorts to random perturbation on input node features and graph structures (You et al., 2020), which might unexpectedly violate the underlying data generation and change the semantic information (Lee et al., 2021). Such an issue plays as a bottleneck limiting the practical efficacy of contrastive methods on graphs. Apart from this, the InfoNCE loss function computes all-pair distance for in-batch nodes as negative pairs for contrasting signals (Zhu et al., 2020b; 2021), which induces quadratic memory and time complexity with respect to the batch size. Given that the model is preferred to be trained in a full-graph manner (i.e., batch size = graph size) since the graph structure information might be partially lost through mini-batch partition, such a nature heavily constrains contrastive methods for scaling to large graphs.\n\nSome recent works seek negative-sample-free methods to resolve the scalability issue by harnessing asymmetric structures (Thakoor et al., 2021) or feature-level decorrelation objectives (Zhang et al., 2021). However, these methods either lack enough theoretical justification (Thakoor et al., 2021) or\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nnecessitate strong assumptions on the data distributions (Zhang et al., 2021). Moreover, they still require data augmentations to generate two views of the input graph, albeit non-contrastive and free from negative sampling. Some other works construct positive examples using the target’s k-nearestneighbors (kNN) in the latent space (Dwibedi et al., 2021; Koohpayegani et al., 2021; Lee et al., 2021). Nonetheless, the computation of nearest neighbors could be cumbersome, time-consuming, and, therefore hard to scale.\n\nPresented Work. To cope with the dilemmas above, in this paper we introduce Localized Graph Contrastive Learning (LOCAL-GCL in abbreviation), a light and augmentation-free contrastive model for self-supervised node-level representation learning on graphs. LOCAL-GCL benefits from two key designs. First, it does not rely on data augmentation to construct positive pairs. Instead, inspired by the graph homophiliy theory (McPherson et al., 2001), it directly treats the first-order neighboring nodes as the positive examples of the target node. This not only increases the number of positive examples for each node but also helps our model get rid of complicated data augmentations. Besides, the computation of positive pairs can be performed in linear time and space complexity w.r.t the graph size, bringing no additional cost for the model. Second, to deal with the quadratic complexity curse of contrastive loss (i.e., InfoNCE loss (van den Oord et al., 2018)), we propose a surrogate loss function in place of the negative loss term in the vanilla InfoNCE loss, which could be efficiently and accurately approximated within linear time and space complexity (Rahimi et al., 2007; Liu et al., 2020; Yu et al., 2016). Such a design greatly improves the efficiency of our model.\n\nWe evaluate the proposed methods on seven public node classification benchmarks with various scales. The empirical results demonstrate that though not using any graph augmentations, our method achieves state-of-the-art performance on six of seven datasets. On the challenging Ogbn-Arxiv dataset, our method can also give a competitive performance with a much training speed compared with other scalable models. Experiments on three heterophily graphs demonstrate that besides homophily graphs, LOCAL-GCL can also perform well on graphs with low homophily ratios.\n\nWe summarize the highlights of this paper as follows:\n\n1) We introduce LOCAL-GCL, a simple model for contrastive learning on graphs, where the positive example is fabricated using the first-order neighborhood of each node. This successfully frees node-level contrastive learning methods from unjustified graph augmentations.\n\n2) To overcome the quadratic complexity curse of contrastive learning, we propose a kernelized contrastive loss computation that can precisely approximate the original loss function within linear complexity w.r.t. graph size. This significantly reduces the training time and memory cost of contrastive learning on large graphs.\n\n3) Experimental results show that without data augmentation and other cumbersome designs LOCALGCL achieves quite competitive results on a variety of graphs of different scales and properties. Furthermore, LOCAL-GCL demonstrates a better balance of model performance and efficiency than other self-supervised methods.\n\n2 BACKGROUND AND RELATED WORKS\n\n2.1 CONTRASTIVE REPRESENTATION LEARNING\n\nInspired by the great success of contrastive methods in learning image representations (van den Oord et al., 2018; Hjelm et al., 2019; Tian et al., 2020; He et al., 2020; Chen et al., 2020), recent endeavors develop similar strategies for node-level tasks in graph domain (Velickovic et al., 2019; Hassani & Ahmadi, 2020; Zhu et al., 2020b; 2021). Among graph contrastive learning methods, the most popular methods should be those based on the InfoNCE loss (van den Oord et al., 2018) due to their simple concepts and better empirical performance. InfoNCE-based graph contrastive learning methods, including GRACE (Zhu et al., 2020b) and GCA (Zhu et al., 2021) aim to maximize the similarity of positive node-node (or graph-graph) pairs (e.g., two views generated via data augmentation) and minimize the similarity of negative ones (e.g., other nodes/graphs within the current batch). However, they require well-designated data augmentations that could positively inform downstream tasks (Tsai et al., 2021). The quadratic complexity also limits their applications to larger batch sizes/datasets.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2.2 AUGMENTATION-FREE SELF-SUPERVISED LEARNING ON GRAPHS\n\nBesides graph contrastive learning (Zhu et al., 2020b; 2021) and the similar two-branched models (Thakoor et al., 2021; Zhang et al., 2021) that use graph augmentations to create positive pairs, there is another line of work exploring the rich structure information in the graph to create selfsupervised signals. Graph AutoEncoders (Kipf & Welling, 2016), for example, learn node embeddings unsupervisedly through reconstructing the adjacency matrix, while GMI (Peng et al., 2020) maximizes the mutual information of both features and edges between inputs and outputs. Inspired by the success of contrastive learning, some recent work explores constructing positive pairs using reliable graph information instead of data augmentations. For example, SUGRL (Mo et al., 2022) employs two encoder models: one is a GCN, and the another is an MLP, to generate two sets of node embedding from different sources. Then for each node, the positive pair can be constructed using its GCN output and MLP output. AFGRL (Lee et al., 2021) and AF-GCL (Wang et al., 2022) treat nodes in the target node’s multi-hop neighborhood as candidate positive examples and use well-designed similarity measures to select the most similar nodes as positive examples. LOCAL-GCL, by contrast, treats all neighbors equally without discrimination. This not only simplifies the model design but also offers a theoretical guarantee for the effectiveness of the proposed model. Empirical results also demonstrate that our method can achieve better performance than them.\n\n2.3 RANDOM FOURIER FEATURES\n\nThough InfoNCE-loss-based based methods are successful in self-supervised node representation learning, their quadratic time and memory complexity with respect to the graph size prevent them from being applied to graphs with tens of thousands of nodes. This paper seeks to address this issue by optimizing a surrogate loss function with Random Fourier Features (RFF or Random Features in short) (Rahimi et al., 2007; Liu et al., 2020). RFF is an effective technique for enhancing the scalability of kernel methods such as SVM, ridge regression (Avron et al., 2017) and independence test (Zhang et al., 2018; Li et al., 2021). Also, recently, RFF has been adopted to develop linear Transformers by approximating the softmax attention (Choromanski et al., 2020; Peng et al., 2021). Given ddimensional vectors x and y and a shift-invariant kernel κ(·), RFF constructs an explicit mapping ψ: Rd → RD, such that κ(x, y) ≈ ψ(x)⊤ψ(y), which reduces the quadratic computation cost of the kernel matrix to a linear one w.r.t data size. Generally, given a positive definite shift-invariant kernel (cid:82) e−jω′∆k(∆)d∆, where κ(x, y) = f (x − y), the Fourier transform p of kernel κ is p(ω) = 1 ∆ = x − y. Then we could draw D i.i.d. samples ω1, · · · , ωD ∈ Rd from p, and ψ(x) is given by:\n\n2π\n\nψ(x) =\n\n(cid:2)cos(ω⊤\n\n1 x), · · · , cos(ω⊤\n\n1 x), · · · , sin(ω⊤\n\nDx)(cid:3)⊤\n\nDx), sin(ω⊤ D\n\n√\n\n.\n\n(1)\n\nLet W = [ω1, · · · , ωD] ∈ Rd×D be a linear transformation matrix, one may realize that the computation of Eq. 1 entails computing W ⊤x. Specifically, when κ(·) is a standard Gaussian kernel (a.k.a., RBF kernel), each entry of W can be directly sampled from a standard Gaussian distribution. The improved variants of RFF mainly concentrate on different ways to build the transformation matrix W , so as to further reduce the computational cost (Le et al., 2013) or lower the approximation variance (Yu et al., 2016).\n\n3 CONTRASTIVE LEARNING ON GRAPHS WITH INFONCE LOSS\n\nThe InfoNCE (van den Oord et al., 2018) loss has been employed in various areas and has shown great power in learning informative representations in a self-supervised manner. Given the embedding of a target data point zi, together with one positive embedding z+ i and a set of negative embeddings {z− i ) from the negative pairs {(zi, z−\n\nj=1 , the InfoNCE loss aims at discriminating the positive pair (zi, z+ j )}M −1\n\nj }M −1\n\nj=1 via the following contrastive objective function: exp(f (zi, z+\n\ni )/τ )\n\nLInfoNCE(i) = − log\n\n,\n\n(2)\n\nexp(f (zi, z+\n\ni )/τ ) +\n\nM −1 (cid:80)\n\nj=1\n\nexp(f (z+\n\ni , z−\n\nj )/τ )\n\nwhere f (·, ·) is a similarity measure, usually implemented as the simple dot product f (x, y) = x⊤y (He et al., 2020; Chen et al., 2020), and τ is the temperature hyperparameter. Note that zi, z+\n\ni\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ni are all l2 normalized to have a unit norm, i.e., ∥zi∥2\n\nand z− 2 = 1. In the context of node-level contrastive learning, each zi denotes the embedding of node i, which is the output of an encoder model which takes the node feature xi and the graph structure as input. As demonstrated in Eq. 2, the InfoNCE loss is composed of two terms: 1) the numerator term on the positive pair that maximizes the similarity between positive pairs (positive term); 2) the denominator term on M − 1 negative pairs and one positive pair (for simplicity, we call all the M terms in the denominator \"negative terms\") that encourages the embeddings of negative pairs to be distinguished (negative term). Like the study of contrastive learning on vision and language, recent node-level contrastive learning research mainly focuses on how to construct/select more informative positive and negative examples.\n\nThe most popular method for constructing positive examples on graphs is through data augmentation. For example, MVGRL (Hassani & Ahmadi, 2020) uses graph diffusion to generate a fixed, augmented view of the studied graph. GCC (Qiu et al., 2020) resorts to random walk to sample subgraphs of the anchored node as positive examples. GRACE (Zhu et al., 2020b) and GCA (Zhu et al., 2021) employ random graph augmentation–feature masking and edge removing to generate two views of the input graph before every training step. However, recent studies have shown that existing graph augmentations might unexpectedly change or lose the semantic information (Lee et al., 2021; Wang et al., 2022) of the target node, hurting the learned representation. This motivates us to investigate the possibility of constructing positive examples without using random, ambiguous graph augmentations.\n\nCompared with the construction of positive examples, negative examples are much easier to select. A common adopted and effective practice is to use all other nodes in the graph as negative examples (Zhu et al., 2020b; 2021; Zhang et al., 2021). However, the computation of the negative term of InfoNCE has O(|V|2d) time complexity and O(|V|2) memory complexity, where |V| is the number of nodes and d is the embedding dimension. Note that the real-world graphs are usually very large, Eq. 2 can hardly be used in a full-graph training manner. One plausible remedy is to use down-sampling to zoom in on a fraction of negative samples for once feedforward computation. However, theoretical analysis demonstrates InfoNCE loss benefits from a large number of negative examples (Poole et al., 2019), and empirical observations show that reducing the number of negative examples leads to worse performance on downstream tasks (Thakoor et al., 2021) (also see our experiments in Sec. 5.4). As a result, it would be promising to devise an efficient and scalable computation method for the contrastive loss above, without reducing the number of negative examples.\n\nIn the next section, we propose Localized Graph Contrastive Learning with Kernel Approximation (LOCAL-GCL in short) to mitigate the above issues. LOCAL-GCL distinguishes itself from previous contrastive methods in the following two aspects: 1) instead of constructing positive examples for target nodes using data augmentations or other techniques, we directly treat all the first-order neighbors of each target node as its positive examples; 2) we introduce a Gaussian kernelized surrogate loss function, which could be accurately and efficiently approximated with linear time and memory complexity in place of the vanilla InfoNCE loss.\n\n4 METHODOLOGY\n\n4.1 ANCHORING NODE CONTRASTIVE LEARNING ON TOPOLOGY\n\nContrastive learning is firstly investigated in unsupervised visual representation learning where the data points are i.i.d. sampled from the data distribution, and the ground-truth labels are unknown during training. As a result, we have to employ well-defined augmentations to create two views of the same image that are likely to share the same ground-truth label. Different from images or texts where each input data is often assumed i.i.d. generated from a certain distribution and there is no explicit prior knowledge about the relationship among different instances, in the graph domain, nodes within the same graph are highly inter-dependent given the input graph structure. That is, the graph structure could provide additional information that reflects the affinity of node pairs. Furthermore, in network (graph) science, it is widely acknowledged that many real-world graphs (such as social networks (Wu et al., 2019; McPherson et al., 2001) and citation networks (Ciotti et al., 2016)) conform with the homophily phenomenon that similar nodes may be more likely to attach than dissimilar ones (Zhu et al., 2003; McPherson et al., 2001). Such a phenomenon also inspires the core principle followed by the designs of modern graph neural networks, i.e., connected nodes should have similar embeddings in the latent space (Zhu et al., 2020a). This motivates us to directly treats all the first-order neighboring\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nnodes of the target node as positive examples. Formally, the loss for the positive pairs of node i can be written as\n\nLpos(i) = − log\n\n(cid:88)\n\nexp(z⊤\n\ni zj/τ )/|N (i)|.\n\n(3)\n\nFor the negative term, we can directly use all nodes in the graph as negative examples. Then the loss for the negative pairs of node i can be formulated as:\n\nj∈N (i)\n\nLneg(i) = log\n\nexp(z⊤\n\ni zk/τ ).\n\n(cid:88)\n\nk∈V\n\nThe overall loss function is simply the sum of Lpos and Lneg, then averaged over all nodes:\n\nLLocal−GCL = −\n\n1 |V|\n\nN (cid:88)\n\ni=1\n\nlog\n\n(cid:80)\n\nj∈N (i)\n\nexp(z⊤\n\ni zj/τ )/|N (i)|\n\n(cid:80)\n\nk∈V\n\nexp(z⊤\n\ni zk/τ )\n\n.\n\n(4)\n\n(5)\n\nThen we’d like to provide a theoretical analysis of the effectiveness of constructing positive examples directly with the first-order neighbors. First, let’s introduce some notations that will be used in the following analysis. We denote a graph by G = (V, E), where V is the node st and E is the edge set. The adjacency matrix and degree matrix are denoted by A and D, respectively. Let ̃A = D−1/2AD−1/2 be the symmetric normalized adjacency matrix, and L = I − ̃A be the symmetric normalized graph Laplacian matrix. Denote the eigenvalues of L in Zn ascending order by {λi}|V| i=1 ∈ R|V|×d and the one-hot label matrix by Y = {yi}|V|\n\ni=1. Finally we denote the node embedding matrix Z = {zi}|V|\n\ni=1 ∈ R|V|×c. Without loss of generality we assume d ≤ |V|.\n\nWe then give a formal definition of graph homophily ratio φ:\n\nDefinition 1. (Graph Homophily Ratio) For a graph G = (V, E) with adjacency matrix A, its homophily ratio φ is defined as the probability that two connected nodes share the same label:\n\nφ =\n\n(cid:80)\n\ni,j∈V Aij · 1[yi = yj] i,j∈V Aij\n\n(cid:80)\n\n(cid:80)\n\ni,j∈V Aij · 1[yi = yj] |E|\n\n=\n\n(6)\n\nWith the conclusions in Balestriero & LeCun (2022) and HaoChen et al. (2021), which build connections between contrastive loss and spectral method, the following theorem guarantees the linear classification error of the embeddings learned from LOCAL-GCL: Theorem 1. Let Z∗ be the global minimizer of Eq. 5, then for any labeling function ˆy : V → Rc with graph homophily φ, there exists a linear classifier B∗ ∈ Rd×c with norm ∥B∗∥F ≤ 1/(1 − λd) such that\n\nEi∈V\n\n(cid:104) ∥ˆy(i) − B∗⊤z∗\n\ni ∥2\n\n2\n\n(cid:105)\n\n≤\n\n1 − φ λd+1\n\n(7)\n\nSee proof in Appendix A.1. Theorem 1 demonstrates that the linear classification accuracy of the learned embeddings through LOCAL-GCL is bounded by the homophily ratio φ and the d+1 smallest eigenvalue λd+1. Specifically, the larger the homophily ratio, the smaller the prediction error. Besides, Eq. 7 indicates that a larger embedding dimension can lead to better classification accuracy, which is also validated empirically in Sec. 5.\n\n4.2 FAST AND EFFICIENT APPROXIMATION FOR THE NEGATIVE LOSS\n\nWe next probe into how to reduce the computation complexity of the negative loss term Lneg = (cid:80) Lneg(i), for which the |V|2 pair-wise similarities can be efficiently and precisely approximated\n\ni∈V with efficient computation methods for kernel functions (Rahimi et al., 2007; Liu et al., 2020).\n\nWe notice that the pair-wise similarity (in dot-product format) in Eq. 4 is essentially a Gaussian kernel function, i.e.,\n\nexp(z⊤\n\ni zj/τ ) = exp(\n\n2 − ∥zi − zj∥2 2τ\n\n2\n\n) = e ∗ exp(\n\n−∥zi − zj∥ 2τ\n\n) = e · κG(zi, zk; (cid:112)τ ),\n\n(8)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nwhere κG(zi, zk; (cid:112)τ ) is the Gaussian kernel function with bandwidth for a projection function ψ(x)(Rd → R2D) such that κG(x, y; able to use a surrogate negative loss function as a remedy of the previous one:\n\n√\n\nτ . This motivates us to seek τ ) ≈ ψ(x)⊤ψ(y). Then we are\n\n√\n\nLneg ≜ 1\n\n|V|\n\n(cid:88)\n\ni∈V\n\n\n\nlog\n\n(cid:88)\n\nj∈V\n\n\n\nψ(hi)⊤ψ(hj)\n\n =\n\n1 |V|\n\n\n\n\n\n(cid:88)\n\nlog\n\nψ(hi)⊤ (cid:88)\n\ni∈V\n\nj∈V\n\n\n\n\n\nψ(hj)\n\n\n\n .\n\n(9)\n\nOnce we obtain all the projected vectors {ψ(hi)}|V| j∈V ψ(hj) in Eq. 9 could be calculated within O(|V|D) in advance (where D is the dimension of projected vectors). In addition to the O(|V|D) cost for computing the loss, the overall computational cost will be O(|V|D + |V|D) = O(V D), which is linear to the number of nodes. Then we discuss how to formulate the projection function ψ(·) below.\n\ni=1, the summation term (cid:80)\n\nLinear-Order Projection The theory of Random Fourier Features (Rahimi et al., 2007) introduced in Sec. 2.3 demonstrates that a Gaussian kernel function κG(x, y; τ ) could be unbiasedly estimated with ψ(x)⊤ψ(y), and the projection function ψ(x) is defined as follows:\n\n√\n\nψ(x) =\n\n[cos(W ⊤x), sin(W ⊤x)] √\n\n=\n\nD\n\n(cid:2)cos(ω⊤\n\n1 x), · · · , cos(ω⊤\n\nDx), sin(ω⊤ D\n\n√\n\n1 x), · · · , sin(ω⊤\n\nDx)(cid:3)⊤\n\n.\n\n1 , · · · , ω⊤\n\n(10) where W = [ω⊤ D], and each ωi is sampled from the Gaussian distribution p(ω) = N (0, τ I). D is the number of total samples. Usually, the larger the sampling number D is, the more accurate the approximation will be: Theorem 2. Let {ωi}D i=1 be i.i.d samples from Gaussian distribution N (0, τ I), and ψ(x) is given by Eq. 10, then with probability at least 1−ε, the approximation error ∆ = |ψ(hi)⊤ψ(hj)−κG(hi, hj)| will be bounded by O\n\n(cid:16) 1−exp(−4/τ ) √\n\n(cid:17)\n\n.\n\n2Dε\n\nTheorem 2 suggests that the Gaussian kernel function could be accurately approximated with Random Fourier Features as long as we sample enough number of linear transformation vectors ω (i.e., D should be large enough). Note that the computation of linear projection W hi (i = 1 ∈ V) requires additional O(|V|dD) time. A large number of the projection dimension D makes the computation of Eq. 9 still expensive for high-dimensional data.\n\nd\n\n√\n\n1 , · · · , ω⊤\n\nLog-Order Projection To handle the above issues, we resort to Structured Orthogonal Random Features (SORF) (Yu et al., 2016), another Random Feature technique that imposes structural and orthogonality on the linear transformation matrix W . Different from the vanilla RFF which directly samples linear transformation vectors ω form normal distribution to construct the transformation D] ∈ Rd×D, SORF assumes D = d and constructs a structured ormatrix Wrff = [ω⊤ thogonal transformation matrix through the continued product of a series of structured matrixes σ HD1HD2HD3, where Di ∈ Rd×d, i = 1, 2, 3, are diagonal “sign-flipping” matriWsorf = ces, with each diagonal entry sampled from the Rademacher distribution, and H ∈ Rd×d is the normalized Walsh-Hadamard matrix. By this definition, the projected dimension is restricted to d, but could be extended to any dimension by concatenating multiple independently generated features or simply using a proportion of them. The stand-out merit of SORF is that it can be computed in O(|V|D log d) time using fast Hadamard transformation (Fino & Algazi, 1976), and hardly requires extra memory cost using in-place operations (Yu et al., 2016). This further reduces its complexity and endows our method with desirable scalability to not only larger dataset sizes but also larger embedding dimensions. If not specified, in the following section, the term LOCAL-GCL denotes our method equipped with SORF to approximate the negative loss.\n\n5 EXPERIMENTS\n\nWe conduct experiments to evaluate the proposed method by answering the following questions:\n\n• RQ1: How does LOCAL-GCL perform compared with other self-supervised learning\n\nmethods on graphs with different properties?\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n• RQ2: How do the specific designs of LOCAL-GCL, such as the embedding dimension and\n\nprojection dimension, affect its performance?\n\n• RQ3: What’s the empirical memory and time consumption of LOCAL-GCL compared with prior works? Is LOCAL-GCL able to scale to real-world large-scale graphs with satisfying performance?\n\n5.1 EXPERIMENTAL SETUPS\n\nDatasets. We evaluate LOCAL-GCL on various datasets with different scales and properties. Following prior works (Zhu et al., 2020b; Zhang et al., 2021) on self-supervised node representation learning, we adopt the 7 common small-scale benchmarking graphs: Cora, Citeseer, Pubmed, Amazon-Computer, Amazon-Photo, Coauthor-CS, and Coauthor-Physics. To evaluate the performance and scalability of LOCAL-GCL on larger graphs, we also adopt Ogbn-Arxiv with about 170k nodes, on which a lot of methods fail to scale due to memory issues. Furthermore, we adopt three widely used heterophily graphs, Chameleon, Squirrel, and Actor to evaluate the generalization ability of LOCAL-GCL on graphs where the graph homophily assumption does not hold. The detailed introduction and statistics of these datasets are presented in Appendix C.1\n\nBaselines. We consider representative prior self-supervised models for comparison. We classify previous methods into two types: 1) Augmentation-based, which uses data augmentations to generate positive or negative pairs. 2) Augmentation-free, which uses other information rather than any form of data augmentation to create self-supervised signals. For augmentation-based baselines, we consider DGI (Velickovic et al., 2019), MVGRL (Hassani & Ahmadi, 2020), GRACE (Zhu et al., 2020b), GCA Zhu et al. (2021), BGRL (Thakoor et al., 2021) and CCA-SSG (Zhang et al., 2021). For augmentation-free baselines, we consider GMI (Peng et al., 2020), SUGRL (Mo et al., 2022), AFGRL (Lee et al., 2021), AF-GCL (Wang et al., 2022).\n\nEvaluation Protocols. We follow the linear evaluation scheme in previous works (Velickovic et al., 2019; Hassani & Ahmadi, 2020; Zhu et al., 2021; Zhang et al., 2021): For each dataset, i) we first train the model on all the nodes in a graph without supervision by optimizing the objective in Eq. 5; ii) after the training ends, we freeze the parameters of the encoder and obtain all the nodes’ embeddings, which are subsequently fed into a linear classifier (i.e., a logistic regression model) to generate a predicted label for each node. In the second stage, only nodes in the training set are used for training the classifier, and we report the classification accuracy on testing nodes.\n\nImplementation Details. The model is implemented with PyTorch and DGL (Wang et al., 2019). All experiments are conducted on an NVIDIA V100 GPU with 16 GB memory unless specified. We adopt structure-net1 to do fast Walsh-Hadamard transformation, which enables much faster forward and backward computation with CUDA accelerations. We use the Adam optimizer (Kingma & Ba, 2015) for both self-supervised pretraining training and linear evaluation using logistic regression. Following previous works (Zhu et al., 2021; Zhang et al., 2021; Thakoor et al., 2021; Wang et al., 2022), we the random 1:1:8 split for Amazon-Computer, Amazon-Photo, Coauthor-CS and Amazon-Computer, and use the public recommended split for the remaining datasets. For each experiment, we report the average test accuracy with the standard deviation over 20 random initialization. If not specified, we use a two-layer GCN model as the encoder to generate node embeddings. More detailed hyperparameter settings for each dataset can be found in Appendix C.2\n\n5.2 NUMERICAL RESULTS\n\nResults on common graphs. We first report the results of node classification tasks on small-scale citation networks and social networks in Table 1. We see that although not relying on data augmentations or other complicated operations to create self-supervised signal, LOCAL-GCL performs competitively with our self-supervised baselines, achieving state-of-the-art performance in 6 out of 7 datasets. It is worth noting that the competitive InfoNCE-loss based contrastive methods GRACE and GCA suffer from OOM on Coauthor-Physics datasets due to the O(|V|2d) space complexity, while LOCAL-GCL can avoid such an issue thanks to the linear approximation of the negative loss.\n\n1https://github.com/HazyResearch/structured-nets\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison of self-supervised methods on benchmarking graphs. We group each method according to whether it relies on graph augmentation.\n\nMethods\n\nDGI MVGRL GRACE GCA BGRL CCA-SSG\n\nGMI SUGRL AFGRL AF-GCL LOCAL-GCL\n\nd e\ns a\nb -\ng u\nA\n\ne e\nr f\n- g\nu A\n\nCora\n\nCiteseer\n\nPubmed\n\nComputer\n\nPhoto\n\nCS\n\nPhysics\n\n82.3±0.6 83.5±0.4 81.9±0.4 82.3±0.4 82.7±0.6 84.2±0.4\n\n82.4±0.6 83.4±0.5 81.3±0.2 83.2±0.2 84.5±0.4\n\n71.8±0.7 73.3±0.5 71.2±0.5 72.1±0.4 71.1±0.8 73.1±0.3\n\n71.7±0.2 73.0±0.4 68.7±0.3 72.0±0.4 73.6±0.4\n\n76.8±0.6 80.1±0.7 80.6±0.4 80.7±0.5 79.6±0.5 81.6±0.4\n\n79.3±1.0 81.9±0.3 80.6±0.4 79.1±0.8 82.1±0.5\n\n83.95±0.47 87.52±0.11 86.25±0.25 87.85±0.31 89.69±0.37 88.74±0.28\n\n84.22±0.52 88.93±0.21 89.88±0.33 89.68±0.19 88.81±0.37\n\n91.61±0.22 91.74±0.07 92.15±0.24 92.49±0.09 93.07±0.28 93.14±0.14\n\n90.73±0.24 93.07±0.15 93.22±0.28 92.49±0.31 93.25±0.40\n\n92.15±0.63 92.11±0.12 92.93±0.01 93.10±0.01 92.59±0.17 93.31±0.22\n\nOOM 92.83±0.23 93.27±0.17 91.92±0.10 94.90±0.19\n\n94.51±0.52 95.33±0.03 OOM∗ OOM 95.48±0.08 95.38±0.06\n\nOOM 95.38±0.11 OOM 95.12±0.15 96.33±0.13\n\n∗ OOM indicates out-of-memory on an NVIDIA-V100 GPU of 16G memory.\n\nTable 2: Performance on Ogbn-Arxiv dataset. As recommended, we report both the validation accuracy and test accuracy.\n\nMethods\n\nDGI GMI MVGRL GRACE GCA BGRL CCA-SSG AFGRL\n\nValidation\n\nTest\n\n71.21±0.23 OOM OOM OOM OOM 72.71±0.22 72.31±0.18 OOM\n\n70.32±0.25 OOM OOM OOM OOM 71.54±0.17 71.21±0.20 OOM\n\nLOCAL-GCL\n\n72.29±0.25\n\n71.34±0.25\n\nTable 3: Performance on Heterophily graphs. Results of baseline methods are taken from Wang et al. (2022)\n\nMethods\n\nDGI GMI MVGRL GRACE GCA BGRL AF-GCL\n\nChameleon\n\nSquirrel\n\nActor\n\n60.27±0.70 52.81±0.63 53.81±1.09 61.24±0.53 60.94±0.81 64.86±0.63 65.28±0.53\n\n42.22±0.63 35.25±1.21 38.75±1.32 41.09±0.85 41.53±1.09 46.24±0.70 52.10±0.67\n\n28.30±0.76 27.28±0.87 32.09±1.07 28.27±0.43 28.89±0.50 28.80±0.54 28.94±0.69\n\nLOCAL-GCL\n\n68.74±0.49\n\n52.94±0.88\n\n33.91±0.57\n\nResults on Ogbn-Arxiv. Then, we evaluate the effectiveness and scalability of LOCAL-GCL on large-scale graphs taking Ogbn-Arxiv as an example. Following the practice in Thakoor et al. (2021), we expand the encoder to a 3-layer GCN model. We report the validation and test accuracy of baseline models and ours in Table 2. As demonstrated, many baseline methods cannot run in a full graph manner (on a GPU with 16GB memory). Compared with other scalable methods, LOCAL-GCL can give a competitive performance on Ogbn-Arxiv.\n\nResults on Heterophily graphs. Finally, we investigate the performance on non-homophily graphs, a much more challenging task as directly using first-order neighbors as positive examples without discriminating might be harmful to non-homophily graphs. The results on the three heterophily graphs are presented in Table 3. Counterintuitively, LOCAL-GCL achieves amazing performance, outperforming all the baseline methods. This is probably due to the following reasons: 1) Even though connected nodes may not share the same label, as long as the neighborhood distributions for different classes are different, LOCAL-GCL is able to recognize the neighborhood patterns of different classes and make the node embeddings for different classes distinguishable. This is also justified in one recent work showing that a GCN model can still perform well on heterophily graphs (Ma et al., 2022). 2) Data augmentation-based methods tend to keep the low-frequency information while discarding the high-frequency one (Wang et al., 2022; Liu et al., 2022), while high-frequency information is much more important for classification on heterophilic graphs (Bo et al., 2021). By contrast, edge-wise positive pairs enable LOCAL-GCL to learn the differences between connected nodes better, thus benefiting heterophilic graphs. We provide more explanations for this point in Appendix C.4.\n\n5.3 ABLATIONS\n\nAs demonstrated in Theorem 1 and Sec. 4.2, the embedding dimension d and projection dimension D should be two critical hyperparameters affecting the model’s performance. In this section, we test\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Sensitivity analysis of embedding dimension d and projection dimension D.\n\nFigure 2: Test accuracy and training time comparison of LOCAL-GCL, BGRL, CCA-SSG and GRACE with subsampling.\n\nLOCAL-GCL’ sensitivities with respect to them in Fig. 1. The observations are two-fold: 1) Similar to other self-supervised learning models (Hassani & Ahmadi, 2020; Zhu et al., 2021; Zhang et al., 2021; Lee et al., 2021), LOCAL-GCL that benefit from a large embedding dimension (e.g., 512), LOCAL-GCL’s performance also grows as we increase the embedding dimension d. However, the optimal embedding dimension for LOCAL-GCL is even larger, e.g., 2048 on Cora and Citeseer (note that other methods do not benefit from such a large embedding dimension). This could be justified by our analysis in Theorem 1, which demonstrates that with a larger embedding dimension, the bound of the prediction error will become lower (because the target eigenvalue λd+1 becomes larger). 2) Increasing the projection dimension can lead to better but minimal improvement. This indicates that in practice, LOCAL-GCL can perform quite well without using a huge projection D.\n\n5.4 SCALABILITY COMPARISON\n\nTo evaluate the scalability of LOCAL-GCL on real-world datasets, we compare the total training time and accuracy with state-of-the-art scalable methods BGRL (Thakoor et al., 2021) and CCASSG (Zhang et al., 2021) on Arxiv dataset. To further justify the necessity of adopting a large number of negative examples, we additionally adopt GRACE (Zhu et al., 2020b), a powerful non-scalable contrastive model. We use sub-sampling strategy to randomly sample a fixed number of negative examples every epoch so that the GRACE model could be fit into a GPU with 16G memory. In Fig. 2 we plot the model’s performance and the corresponding training time of different methods. We can see that compared with scalable methods CCA-SSG and BGRL, LOCAL-GCL achieves comparable performance but with the least training time. We can also observe that although reducing the number of negative examples (k in Fig. 2) can enable the model to be trained much faster, the performance drop is significant, which cannot make up for the efficiency benefit. Furthermore, if we continue to use an even larger number of negative examples, GRACE will soon run out of memory. This result demonstrates that LOCAL-GCL can better balance the training efficiency and model performance.\n\n6 CONCLUSIONS\n\nIn this paper, we have presented LOCAL-GCL, a simple, light-weighted yet effective contrastive model for self-supervised node representation learning. LOCAL-GCL incorporates two orthogonal techniques to address two key issues in contrastive learning on graph data: reliance on data augmentation and scalability, respectively. The former involves a new definition of positive examples such that the model is free from data augmentation whose design could be more difficult than its counterparts in vision and text, and calls for careful customization for different graph data and tasks. The latter devises an approximated contrastive loss, which reduces the quadratic complexity of traditional contrastive learning to linearity in terms of the graph size. Extensive experimental results on seven public graph benchmarks show its efficiency and efficacy.\n\n9\n\n1282565121024204867.570.072.575.077.580.082.585.0Accuracy(a) Effect of embedding dimensionCoraCiteSeerPubmed20484096819216384747678808284Accuracy(b) Effect of projection dimensionCoraCiteSeerPubmed20406080100Training time (min)60626466687072Test AccuracyLocal-GCL(ours)BGRLCCA-SSGGRACE(k=2)GRACE(k=4)GRACE(k=16)GRACE(k=2048)GRACE(k=8192)Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nHaim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and statistical guarantees. In ICML, pp. 253–262, 2017.\n\nRandall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods. arXiv preprint arXiv:2205.11508, 2022.\n\nHubert Banville, Omar Chehab, Aapo Hyvärinen, Denis-Alexander Engemann, and Alexandre Gramfort. Uncovering the structure of clinical eeg signals with self-supervised learning. Journal of Neural Engineering, 18(4):046020, 2021.\n\nDeyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph convolutional networks. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 3950–3957, 2021.\n\nTing Chen, Simon Kornblith ands Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\n\nfor contrastive learning of visual representations. In ICML, pp. 1597–1607, 2020.\n\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In ICLR, 2020.\n\nValerio Ciotti, Moreno Bonaventura, Vincenzo Nicosia, Pietro Panzarasa, and Vito Latora. Homophily\n\nand missing links in citation networks. EPJ Data Sci., 5(1):7, 2016.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\n\nbidirectional transformers for language understanding. In NAACL, pp. 4171–4186, 2019.\n\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. arXiv preprint arXiv:2104.14548, 2021.\n\nBernard J. Fino and V. Ralph Algazi. Unified matrix treatment of the fast walsh-hadamard transform.\n\nIEEE Transactions on Computers, 25(11):1142–1146, 1976.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\n\nembeddings. arXiv preprint arXiv:2104.08821, 2021.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-supervised learning. In NeurIPS, 2020.\n\nJeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised\n\ndeep learning with spectral contrastive loss. NeurIPS, 34:5000–5011, 2021.\n\nKaveh Hassani and Amir Hosein Khas Ahmadi. Contrastive multi-view representation learning on\n\ngraphs. In ICML, 2020.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for\n\nunsupervised visual representation learning. In CVPR, pp. 9726–9735, 2020.\n\nR. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In ICLR, 2019.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In NeurIPS, 2020.\n\nYannis Kalantidis, Mert Bülent Sariyildiz, Noé Pion, Philippe Weinzaepfel, and Diane Larlus. Hard\n\nnegative mixing for contrastive learning. In NeurIPS, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nThomas N. Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308,\n\n2016.\n\nSoroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Mean shift for self-\n\nsupervised learning. In ICCV, pp. 10326–10335, 2021.\n\nQuoc Le, Tamás Sarlós, Alex Smola, et al. Fastfood-approximating kernel expansions in loglinear\n\ntime. In ICML, pp. 244–252, 2013.\n\nNamkyeong Lee, Junseok Lee, and Chanyoung Park. Augmentation-free self-supervised learning on\n\ngraphs. arXiv preprint arXiv:2112.02472, 2021.\n\nYazhe Li, Roman Pogodin, Danica J. Sutherland, and Arthur Gretton. Self-supervised learning with\n\nkernel dependence maximization. arXiv preprint arXiv:2106.08320, 2021.\n\nFanghui Liu, Xiaolin Huang, Yudong Chen, and Johan AK Suykens. Random features for kernel approximation: A survey on algorithms, theory, and beyond. arXiv preprint arXiv:2004.11154, 2020.\n\nNian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian Pei. Revisiting graph contrastive learning from\n\nthe perspective of graph spectrum. arXiv preprint arXiv:2210.02330, 2022.\n\nYao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang.\n\nIs homophily a necessity for graph neural\n\nnetworks? In ICLR, 2022.\n\nJulian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. Image-based\n\nrecommendations on styles and substitutes. In SIGIR, pp. 43–52, 2015.\n\nMiller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social\n\nnetworks. Annual review of sociology, 27(1):415–444, 2001.\n\nYujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. Simple unsupervised graph\n\nrepresentation learning. In AAAI, pp. 7797–7805, 2022.\n\nGalileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. Query-driven active surveying for collective classification. In 10th International Workshop on Mining and Learning with Graphs, volume 8, 2012.\n\nHongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric\n\ngraph convolutional networks. In ICLR, 2020.\n\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.\n\nRandom feature attention. In ICLR, 2021.\n\nZhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. Graph representation learning via graphical mutual information maximization. In WWW, pp. 259–270, 2020.\n\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational\n\nbounds of mutual information. In ICML, pp. 5171–5180, 2019.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. GCC: graph contrastive coding for graph neural network pre-training. In KDD, pp. 1150–1160, 2020.\n\nAli Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS, pp.\n\n1177–1884, 2007.\n\nJoshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning\n\nwith hard negative samples. In ICLR, 2021.\n\nBenedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-Scale Attributed Node Embedding.\n\nJournal of Complex Networks, 9(2), 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\n\nCollective classification in network data. AI magazine, 29(3):93–93, 2008.\n\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Paul Hsu, and Kuansan Wang. An overview of microsoft academic service (MAS) and applications. In WWW, pp. 243–246, 2015.\n\nShantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Rémi Munos, Petar Velickovic, and Michal Valko. Bootstrapped representation learning on graphs. arXiv preprint arXiv:2102.06514, 2021.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, pp.\n\n776–794, 2020.\n\nYao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-supervised\n\nlearning from a multi-view perspective. In ICLR, 2021.\n\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n\nPetar Velickovic, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, and R. Devon\n\nHjelm. Deep graph infomax. In ICLR, 2019.\n\nHaonan Wang, Jieyu Zhang, Qi Zhu, and Wei Huang. Augmentation-free graph contrastive learning\n\nwith performance guarantee. arXiv Preprint arXiv:2204.04874, abs/2204.04874, 2022.\n\nMinjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J. Smola, and Zheng Zhang. Deep graph library: Towards efficient and scalable deep learning on graphs. arXiv, 1909.01315, 2019.\n\nQitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao, and Guihai Chen. Dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems. In WWW, pp. 2091–2102. ACM, 2019.\n\nJun Xia, Lirong Wu, Ge Wang, Jintao Chen, and Stan Z. Li. Progcl: Rethinking hard negative mining in graph contrastive learning. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 24332–24346. PMLR, 2022.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n\ncontrastive learning with augmentations. In NeurIPS, 2020.\n\nFelix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice,\n\nand Sanjiv Kumar. Orthogonal random features. In NIPS, pp. 1975–1983, 2016.\n\nHengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation\n\nanalysis to self-supervised graph neural networks. In NeurIPS, 2021.\n\nQinyi Zhang, Sarah Filippi, Arthur Gretton, and Dino Sejdinovic. Large-scale kernel methods for\n\nindependence testing. Statistics and Computing, 28(1):113–130, 2018.\n\nShaofeng Zhang, Meng Liu, Junchi Yan, Hengrui Zhang, Lingxiao Huang, Xiaokang Yang, and Pinyan Lu. M-mix: Generating hard negatives via multi-sample mixing for contrastive learning. In KDD, pp. 2461–2470. ACM, 2022.\n\nJiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. In NeurIPS, 2020a.\n\nXiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian\n\nfields and harmonic functions. In ICML, pp. 912–919, 2003.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020b.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning\n\nwith adaptive augmentation. In WWW, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS\n\nA.1 PROOF FOR THEOREM 1\n\nProof. To prove Theorem 1, we first introduce a lemma that shows the optimal representations learned from InfoNCE-like loss function (e.g., Eq. 5) derived from Balestriero & LeCun (2022):\n\nLemma 1. Denote the eigendecomposition of the normalized graph adjacency matrix by ̃A = U ΛU ⊤, where U and Λ are the eigenvectors and eigenvalues of ̃ respectively. Assume node embeddings are free vectors, then a global minimizer of the loss function in Eq. 5 is given by:\n\nZ∗ = (U Λ1/2):,1:d, (11) up to permutations of the eigenvector associated with the same eigenvalue. 1 : d in Eq. 11 indicates the largest d eigenvalues and the associated eigenvectors.\n\nBesides, HaoChen et al. (2021) has presented the following theoretical guarantee for the model learned with the matrix factorization loss:\n\nLemma 2. For a graph G with symmetric normalized graph adjacency matrix and Laplacian matrix ̃A and ̃L, let f ∗ mf ∈ arg minfmf :V→Rd be a minimizer of the matrix factorization loss: Lmf (F ) = ∥(I − ̃L − F F ⊤)∥2 F , where F is the embedding matrix. Then, for any labeling function ˆy : V → [r], there exists a linear classifier B∗ ∈ Rd×c with norm ∥B∗∥F ≤ 1/(1 − λd+1) such that:\n\nEv∈V\n\n(cid:104)\n\n∥\n\n→\n\ny (v) − B∗f ∗(v)∥2\n\n(cid:105)\n\n≤\n\n1 − φ λd+1\n\n,\n\n(12)\n\n2\n\nwhere φ is the graph homophily ratio defined in Eq. 6.\n\nThen we just need to connect Z∗ in Eq. 11 with the optimal solution of the rthe matrix factorization F = ∥ ̃A − F F ⊤∥. According to Eckart–Young–Mirsky theorem, loss Lmf (F ) = ∥I − ̃L − F F ⊤∥2 the optimal F F ⊤ is F F ⊤ = U:,1:dΛ:,1:dU ⊤ :,1:d As a result, F ∗ = Z∗ = (U Λ1/2):,1:d is exactly the global minimizer of Lmf (F ). Then the proof is complete.\n\nA.2 PROOF FOR THEOREM 2\n\nProof. To prove Theorem 2, we first introduce the following lemma given by Lemma 1 in (Yu et al., 2016).\n\nLemma 3. The Random Fourier Estimation KRF F (x, y) is an unbiased estimator of the Gaussian kernel, i.e., Eω(ψ(x)⊤ψ(y)) = e−∥x−y∥2 τ , then the variance of KRF F (x, y) is Vω(ψ(x)⊤ψ(y)) = (1 − e−z2\n\n2/2τ . Let z = ∥x − y∥2/ )2/2D.\n\n√\n\nThe lemma shows that the Random Fourier Features can achieve an unbiased approximation for the Gaussian kernel with a quantified variance.\n\nBack to our main theorem, we can derive the following probability using the Chebyshev’s inequality:\n\nP(∆ <\n\n1 − exp(−4/τ )\n\n√\n\n2Dε\n\n) ≥ 1 −\n\nVω(ψ(hi)⊤ψ(hj)) ∗ 2Dε (1 − exp(−4/τ ))2\n\n(13)\n\nwhere ∆ = |ψ(hi)⊤ψ(hj) − κG(hi, hj)| denotes the deviation of the kernel approximation. Using the result in Lemma 3, we can further obtain that the RHS of Eq. 13 is\n\n1 −\n\n(1 − e−z2\n\n)2 ∗ ε\n\n(1 − exp(−4/τ ))2\n\n(14)\n\nAs both hi and hj are l2-normalized, we have z2 = ∥hi − hj∥2 stated result:\n\n2/τ ≤ 4/τ , so we can conclude the\n\n(1 − e−z2 )2 ∗ ε (1 − exp(−4/τ ))2 ≥ 1 − ε\n\n(15)\n\nP(∆ <\n\n1 − exp(−4/τ )\n\n√\n\n2Dε\n\n) ≥ 1 −\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB ADVANTAGES OF LOCAL-GCL OVER PRIOR METHODS\n\nWe provide a systematic comparison for the proposed LOCAL-GCL with previous typical methods self-supervised node representation learning, including DGI (Velickovic et al., 2019), MVGRL (Hassani & Ahmadi, 2020), GRACE (Zhu et al., 2020b), GCA (Zhu et al., 2021), BGRL (Thakoor et al., 2021), CCA-SSG (Zhang et al., 2021) and AFGRL (Lee et al., 2021).\n\nB.1 TOTALLY AUGMENTATION-FREE WITH SINGLE BRANCH\n\nMost of previous self-supervised learning models (Velickovic et al., 2019; Hassani & Ahmadi, 2020; Zhu et al., 2020b; 2021; Thakoor et al., 2021; Zhang et al., 2021) require carefully-defined graph augmentations to obtain positive pairs. However, as noted above, it is hard and costly to design task and dataset-specific graph augmentations. Our method LOCAL-GCL naturally avoids data augmentation by using the first-order neighborhood information of each node. Moreover, different from all previous methods that have two branches of models (double amounts of inputs, hidden variables, outputs, etc.), our approach only requires a single branch, which makes our method light-weighted.\n\nB.2 PROJECTOR/PREDICTOR/DISCRIMINATOR-FREE\n\nMost of the previous methods require additional components besides the basic encoder for decent empirical results (Zhu et al., 2020b; 2021), to break symmetries to avoid trivial solutions (Thakoor et al., 2021; Lee et al., 2021) or to estimate some score functions in their final objectives (Velickovic et al., 2019; Hassani & Ahmadi, 2020). Compared with them, LOCAL-GCL is much simpler, in both conceptual and practical senses, without using any parameterized model but the basic encoder.\n\nB.3 THEORETICALLY GROUNDED WITH LINEAR COMPLEXITY\n\nEmpirically, InfoNCE-based contrastive methods (Zhu et al., 2020b; 2021) often show better performance than DIM-based methods (Velickovic et al., 2019; Hassani & Ahmadi, 2020) yet suffers from the scalability issue. Recent non-contrastive methods (Thakoor et al., 2021; Lee et al., 2021) could maintain decent performance with linear model complexity through asymmetric structures, while the rationale behind their success still remains unclear (Zhang et al., 2021). As a contrastive model, LOCAL-GCL is theoretically grounded (through maximizing the mutual information between target node’s embedding and its neighbor’s embeddings), and its kernelized approximation of negative loss enables it to scale to large graphs with linear time and memory complexity with respect to the graph size.\n\nC EXPERIMENT DETAILS AND ADDITIONAL EMPIRICAL RESULTS\n\nC.1 DATASETS\n\nThe statistics of the used datasets are presented in Table 4, and brief introduction and settings are as follows:\n\nCora, Citeseer, Pubmed are three widely used node classification benchmarks (Sen et al., 2008; Namata et al., 2012). Each dataset consists one citation network, where nodes represent papers and edges represent a citation relationship from one node to another. We use the public split, where each class has fixed 20 nodes for training, another fixed 500 nodes and 1000 nodes for validation/test, respectively for evaluation.\n\nCoauthor-CS, Coauthor-Physics are co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge (Sinha et al., 2015). Nodes are authors that are connected by an edge if they co-authored a paper; node features represent paper keywords for each author’s papers, and class labels indicate the most active fields of study for each author. As there is no public split for these datasets, we randomly split the nodes into train/validation/test (10%/10%/80%) sets.\n\nAmazon-Computer, Amazon-Photo are segments of the Amazon co-purchase graph (McAuley et al., 2015), where nodes represent goods, edges indicate that two goods are frequently bought\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Statistics of benchmark datasets.\n\nDataset\n\n#Nodes\n\n#Edges\n\n#Classes\n\n#Features\n\nφ\n\nCora Citeseer Pubmed Coauthor-CS Coauthor-Physics Amazon-Computer Amazon-Photo Ogbn-Arxiv Chameleon Squirrel Actor\n\n2,708 3,327 19,717 18,333 34,493 13,752 7,650 169,343 2,277 5,201 7,600\n\n10,556 9,228 88,651 327,576 991,848 574,418 287,326 2,332,386 36,101 217,073 33,544\n\n7 6\n3 15 5\n10 8\n40 5\n5 5\n\n1,433 3,703 500 6,805 8,451 767 745 128 2,325 2,089 931\n\n0.810 0.736 0.802 0.808 0.931 0.777 0.827 0.655 0.235 0.224 0.219\n\nTable 5: Details of hyper-parameters of the experimental results in Table 1\n\nDataset\n\nCora Citeseer Pubmed Amazon-Computer Amazon-Photo Coauthor-CS Coauthor-Physics\n\nEncoder GCN GCN GCN GCN GCN MLP MLP\n\n# Steps 50 20 50 50 50 50 50\n\nLocal-GCL\n\n# layers 2\n1 2\n2 2\n2 2\n\nlr 5e-4 1e-3 5e-4 5e-4 5e-4 5e-4 5e-4\n\nwd 1e-6 1e-4 0\n0 1e-6 1e-4 0\n\nd 2048 2048 1024 1024 2048 1024 1024\n\nD 8192 4096 8192 8192 8192 4096 4096\n\nτ 0.5 0.5 0.5 0.8 0.5 0.5 0.7\n\ntogether; node features are bag-of-words encoded product reviews, and class labels are given by the product category. We also use a 10%/10%/80% split for these two datasets.\n\nFor simplicity, we use the processed version of these datasets provided by Deep Graph Library (Wang et al., 2019)2. One can easily acquire these datasets using the api provided by DGL.\n\nOgbn-Arxiv is a directed graph, representing the citation network between all Computer Science (CS) arXiv papers (Hu et al., 2020). Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The embeddings of individual words are computed by running the skip-gram over the MAG corpus. All papers are also associated with the year that the corresponding paper was published. We use the official split in our experiments.\n\nChameleon and Squirrel (Rozemberczki et al., 2021) are Wikipedia networks introduced. Nodes represent web pages, and edges represent hyperlinks between them. Node features represent several informative nouns in the Wikipedia pages. The task is to predict the average daily traffic of the web page.\n\nActor is the actor-only induced subgraph of the film-director-actor-writer network used in Pei et al. (2020). Each node corresponds to an actor, and the edge between two nodes denotes co-occurrence on the same Wikipedia page. Node features correspond to some keywords in the Wikipedia pages. The task is to classify the nodes into five categories in term of words of actor’s Wikipedia.\n\nFor Chameleon, Squirrel and Actor, we use the raw data provided by the Geom-GCN (Pei et al., 2020) paper3, and we use the 10-fold split provided.\n\nC.2 HYPER-PARAMETERS\n\nWe provide detailed hyper-parameters on the seven benchmarks in Table 5. All hyper-parameters are selected through a small grid search, and the search space is provided as follows:\n\n2https://docs.dgl.ai/en/0.6.x/api/python/dgl.data.html 3https://github.com/graphdml-uiuc-jlu/geom-gcn\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n• Training steps: {20, 50}\n\n• Number of layers: {1, 2}\n\n• Embedding dimension d: {512, 1024, 2048}\n\n• Projection dimension D: {2048, 4096, 8192}\n\n• Temperature τ : {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}\n\n• Learning rate: {5e-4, 1e-3, 5e-3}\n\n• Weight decay: {0, 1e-6, 1e-4}\n\nC.3 EXTENSIVE ABLATION STUDIES\n\nWe further extend LOCAL-GCL with other designs of positive example construction, negative example selection, and self-supervised objective functions to evaluate the effectiveness of each single component of the proposed method.\n\nC.3.1 DIFFERENT STRATEGIES FOR CONSTRUCTING POSITIVE EXAMPLES USING\n\nNEIGHBORHOODS\n\nIn addition to treats each neighboring node equally as positive examples, we further explore two other positive sampling strategies: 1) computing the similarities between neighboring nodes, and use the most similar one as the positive example (we term it LOCAL-GCL-max); 2) reweighting the importance of different neighboring nodes according to their similarities, where more similar neighboring node should be assigned a larger weight (we term it LOCAL-GCL-weight).\n\nFor LOCAL-GCL-max, we first its the nearest neighbor:\n\nsi = arg min zj ,j∈N (i)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nzj ∥zj∥2\n\n2\n\n−\n\nzi ∥zi∥2\n\n2\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n,\n\nand the objective function is\n\nLLocalGCL−max = −\n\n1 |V|\n\n|V| (cid:88)\n\ni=1\n\nlog\n\nexp(z⊤\n\ni si/τ )\n\nexp(z⊤\n\ni zk/τ )\n\n(cid:80)\n\nk∈V\n\n.\n\nFor LocalGCL-weight, we first compute the pair-wise similarities of neighboring nodes:\n\nsimij = Ai,j ·\n\nz⊤ ∥zi∥2\n\ni zj 2∥zj∥2\n\n2\n\n.\n\nThen, for a target node i, it softmax the scores of its neighboring nodes as the weights:\n\nwi(j) = softmax[simij]j∈N (i).\n\nThe objective function of LocalGCL-weight is consequently defined as\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\nLLocalGCL−weight = −\n\n(cid:80)\n\nj∈N (i)\n\nlog\n\n1 |V|\n\n|V| (cid:88)\n\ni=1\n\nwi(j) exp(z⊤\n\ni zj/τ )/|N (i)|\n\n(cid:80)\n\nk∈V\n\nexp(z⊤\n\ni zk/τ )\n\n.\n\n(20)\n\nThe results of LOCAL-GCL-max and LOCAL-GCL-weight, together with the original LOCAL-GCL is presented in Table 6: From this table, we can observe that using the nearest neighboring node as the only one positive example is likely harmful (except on Citeseer) to the performance, this should be because this operation can discard a lot of useful information in the local neighborhood. Besides, LOCAL-GCL-weight improves the performance on Cora significantly but makes little difference on Citeseer and Pubmed. This might be because, without supervision, a higher similarity in the latent space does not necessarily indicate a higher probability of sharing the same label. Considering that LOCAL-GCL-weight requires additional computational cost for computing the weights, and the performance of the original LOCAL-GCL is already quite good, we just treat every neighboring node equally in this work.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Performance variation when using different strategies of constructing positive examples from neighboring nodes.\n\nLOCAL-GCL (origin) LOCAL-GCL-max LOCAL-GCL-weight\n\nCora 84.5 ± 0.4 83.6 ± 0.5 84.9 ± 0.4\n\nCiteseer 73.6 ± 0.4 73.7 ± 0.5 73.7 ± 0.4\n\nPubmed 82.1 ± 0.5 81.7 ± 0.5 82.2 ± 0.5\n\nTable 7: Test accuracy, training time cost and memory cost when using the positive pair constructing method in this paper compared with using data augmentation for BGRL and CCA-SSG.\n\nBGRL BGRL-Local CCA-SSG CCA-SSG-Local\n\nCoauthor-CS\n\nCoauthor-Physics\n\nAcc 92.59 ± 0.17 92.98 ± 0.23 93.31 ± 0.22 94.08 ± 0.19\n\nTime 30.4 min 18.2 min 1.1 min 0.7 min\n\nMemory 2.9G 1.8G 3.5G 2.2G\n\nAcc 94.48 ± 0.08 95.82 ± 0.11 95.38 ± 0.06 95.69 ± 0.09\n\nTime 57.7 min 32.3 min 5.9 min 4.3 min\n\nMemory 5.6G 3.7G 7.1G 4.4G\n\nC.3.2 COMBINING WITH NEGATIVE-SAMPLE-FREE SELF-SUPERVISED LEARNING METHODS\n\nusing the target node’s first-order neighbors as positive examples) could also benefit non-contrastive methods, like BGRL and CCA-SSG. Note that both BGRL and CCA-SSG use the exact same data augmentations methods proposed in GRACE (a contrastive method). As a result, we can simply replace the positive examples in BGRL, and on-diagonal terms in CCA-SSG with the ones used in this paper, thus making them free from data augmentations. In such as a case, the models do not require two-viewed data as inputs, so the size of inputs, intermediate variables, and outputs can be reduced by half. To better demonstrate this point, we further extend our method with BGRL and CCA-SSG.\n\nFor BGRL, denote the target embedding of node i by hi and the corresponding prediction by zi, our BGRL-Local optimizes the following loss function:\n\nLBGRL−Local = −\n\n2 N\n\n(cid:88)\n\n(cid:88)\n\nzj · hi\n\ni∈V\n\nj∈N (i)\n\n∥zj∥2\n\n2∥hi∥2\n\n2\n\n(21)\n\nFor CCA-SSG, denote the embedding of node i by hi, we first compute the local summary of node i by zi = 1\n\nhj. Then we compute two standardized embedding matrix by:\n\n(cid:80)\n\n|N (i)|\n\nj∈N (i)\n\n ̃H =\n\nH − μ(H) σ(H) ∗ (cid:112)|V|\n\n, ̃Z =\n\nZ − μ(Z) σ(Z) ∗ (cid:112)|V|\n\n.\n\nFinally, CCA-SSG-Local optimizes the following loss function:\n\nLCCA−SSG−Local =\n\n(cid:13) (cid:13) (cid:13)\n\n ̃H − ̃Z\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nF\n\n+ λ\n\n(cid:18)(cid:13) (cid:13) (cid:13)\n\n ̃H ⊤ ̃H − I\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nF\n\n+\n\n(cid:13) (cid:13) (cid:13)\n\n ̃Z⊤ ̃Z − I\n\n(cid:19)\n\n.\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nF\n\nWe report the performance of BGRL-Local and CCA-SSG-Local compared with the original BGRL and CCA-SSG on CS and Physics, together with their training time/memory cost in Table 7 (We didn’t choose the three citation networks as BGRL performs really bad on them).\n\n(BGRL takes much longer time for training, because it does require a lot of training epochs to converge.) As demonstrated in the table. The performance of BGRL and CCA-SSG both get improved after adopting neighboring nodes as positive examples, which highlights the value of graph structure information as self-supervised learning signals. Besides, it also reduces the training time and memory cost because it gets rid of the two-branch architecture caused by data augmentations.\n\nC.3.3 COMBINING WITH HARD NEGATIVE SAMPLING METHODS\n\nFurthermore, one important advantage of contrastive learning is that it can be incorporated with hard negative sampling techniques (Robinson et al., 2021; Kalantidis et al., 2020; Zhang et al.,\n\n17\n\n(22)\n\n(23)\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Effect of combining LOCAL-GCL with hard-negative sampling methods.\n\nLOCAL-GCL (origin) LOCAL-GCL +ProGCL-mix\n\nCora 84.5 ± 0.4 84.9 ± 0.5\n\nCiteseer 73.6 ± 0.4 74.2 ± 0.4\n\nPubmed 82.1 ± 0.5 82.3 ± 0.5\n\nTable 9: Bootstrapped confidence interval (with 95% confidence).\n\nCCA-SSG LOCAL-GCL\n\nCora [84.06, 84.30] [84.43, 84.75]\n\nCiteseer [73.05, 73.21] [73.45, 73.83]\n\nPubmed [81.51, 81.72] [81.82, 82.29]\n\nComputer [88.57, 88.86] [88.66, 89.06]\n\nPhoto [93.10, 93.22] [93.20, 93.48]\n\nCS [93.25, 93.41] [94.89, 94.98]\n\nPhysics [95.37, 95.41] [96.26, 96.35]\n\n2022; Xia et al., 2022). These works either assign weights to different negative samples so that true negative examples are more important than false negative examples, or use mix-up methods to generate even harder negative examples. With these techniques, the performance of contrastive learning can be greatly boosted. To demonstrate this point, we further combine our method with the hard negative mining strategy ProGCL-mix proposed in Xia et al. (2022), and we report the performance comparison on Cora, Citeseer and Pubmed in Table 8: The improvement is prominent (especially on Citeseer) thanks to the hard-negative sampling strategy(however, ProGCL-mix is not our contribution so we cannot include it in our method in this paper). Besides, we notice that the improvement on Pubmed is smaller than that on the other two datasets, we guess this is because there are only 3 classes of nodes on Pubmed, so it is less effective to mine hard negatives than 7 classes on Cora and 6 on Citeseer. By contrast, negative-sample-free method like BGRL and CCA-SSG cannot get benefits from it.\n\nC.4 ANALYSIS OF THE RESULTS ON HETEROPHILIC GRAPHS\n\nAlthough a little bit counter-intuitive, the better performance of our method on heterophilic graphs does exist, and can be justified through the differences between the two distinguished methods of generating positive pairs: 1) data augmentations on graph data; 2) two connected nodes being a positive pair. Two recent studies Wang et al. (2022) and Liu et al. (2022) both demonstrate that existing data augmentations on graphs, such as graph diffusion used in MVGRL (Hassani & Ahmadi, 2020), edge dropping and feature masking used in GRACE (Zhu et al., 2020b), GCA (Zhu et al., 2021), BGRL (Thakoor et al., 2021) and CCA-SSG (Zhang et al., 2021) have larger impacts on the high-frequency information of the graph than the low-frequency information. As a result, the differences between two graph views are about low-frequency components. Considering that contrastive learning aims at maximizing the mutual information shared between the two views, only the invariant information (low-frequency) is encouraged to be learned by the embeddings, while the middle and high-frequency information is discarded.\n\nOn the other hand, studies on GNNs for heterophilic graphs (Bo et al., 2021) demonstrate that for heterophilic graphs, the high-frequency information in the graph is more effective for downstream classification performance. Considering that data-augmentation-based methods focus on low-frequency information while neglecting high-frequency one, it is not strange that they cannot give satisfactory performance. By contrast, as high-frequency knowledge in the graph represents the differences between the node feature with its neighborhood features, the edge-wise contrastive loss used in our work can better capture the differences in the information between two connected nodes, thus performing better on heterophilic graphs.\n\nC.5 STATISTICAL SIGNIFICANCE TEST\n\nWe further conduct statistical significance tests to show that LOCAL-GCL does perform better than the previous SOTA method CCA-SSG (Zhang et al., 2021).\n\nIn Table 9 we present the bootstrapped confidence interval (with 95% confidence). We also conduct Wilcoxon signed-rank test on the proposed LOCAL-GCL and the second best model CCA-SSG, using the averaged test accuracy on the 7 datasets in Table 1. The p-value of CCA-SSG and LOCAL-GCL’s performance on the 7 datasets is 0.0325 < 0.05. As a result, we can conclude that there is a genuine performance difference between the performance of LOCAL-GCL and that of CCA-SSG.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nD REPRODUCIBILITY\n\nWe provide detailed codes and running instructions in the supplementary material.\n\n19",
    "reference": "# Summary Of The Paper\n\nThe paper introduces Local-GCL, a graph contrastive learning approach that tackles two of the most common issues of GCL models: the need for augmentation to generate positive pairs (which is non-trivial in the graph-learning setting) and the need for a high amount of negative pairs comparisons (whose quadratic complexity makes the problem intractable for large graphs). It does so by, respectively, creating positive pairs from first-order neighbors, and introducing an approximated contrastive loss computation that approximates the original loss with a much lower complexity (linear instead of quadratic). The authors provide theoretical justifications for their model and show empirically that it performs competitively to the state of the art while having a much smaller computational footprint.\n\n# Strength And Weaknesses\n\nStrengths:\n- the method proposes a method to significantly reduce the computational complexity of the calculation of negative loss in GCL models\n- the experiments show results which are promising, especially when compared to other models' running times\n\nWeaknesses:\n- the paper is not always clear due to mistakes or ambiguous definitions\n- I have some doubts about the contribution and the soundness of the theoretical part of the paper (see below)\n- the experimental section still leaves some open questions (related to the contribution brought by the two different components and the performances on heterophilic graphs)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI think the paper brings some interesting thoughts and should be quite reproducible, considering the code+instructions that will be provided in supplementary material.\n\nIn terms of clarity, there are a few issues due to mistakes in equations / definitions which I believe it would be important to address (in addition to typos, which only marginally reduce the understanding of the document):\n\n- Lemma 1 calls $\\Sigma$ the eigenvalues matrix $\\Lambda$.\n- Lemma 2 refers to $\\hat{y} = X \\rightarrow [r]$ while it should be $\\hat{y} = V \\rightarrow [r]$\n- Theorem 1: the dimensionality of $B^*$ is $k \\times c$, while it should be $d \\times c$\n- the RFF map is defined as being $\\phi : \\R^d \\rightarrow \\R^D$ but, while the $\\omega_i$ samples are just $D$, the dimensionality of $\\phi$ in Eq 1 seems $2D$ instead.\n- finally, I think using $\\phi$ to refer both to homophily ratio and to the RFF map might lead to some confusion\n\nSome additional issues relate to the soundness of the theoretical presentation. In my opinion, a few concepts require a better explaination that also takes into consideration assumptions and boundary conditions. For instance:\n\n- The simplification of Eq 9 that allows us to precalculate all the $\\phi(h_j)$ is possible because $\\phi(h_i)$ is fixed and can be taken out of the sum. This, I guess, is the reason why the negative loss is calculated on all nodes and not just the ones which do not belong to a given node's neighborhood. While the impact of this choice does not seem to be too important, I think it should be made explicit and thoroughly discussed (i.e. why it makes sense, when it might not work, how the error changes wrt the average degree vs graph size).\n\n- The proof of Theorem 1 relies on a set of assumptions which to me are not completely clear:\n\t- Lemma 1 builds above Theorem 5 from Balestriero and LeCun (2022) which specifically refers to the global minimiser of the SimCLR loss. How is Local-GCL's loss, which includes a negative loss component which is approximated, related to it? Is there a proof of its compatibility with the one of SimCLR?\n\n\t- Lemma 2 builds above Theorem B.3 from HaoChen et al. (2021) which explicitly introduces eigenvalues $\\lambda_1, \\dots, \\lambda_{k+1}$ as the smallest ones, while in the main body of the paper (just before Definition 1) they are defined as being in a descending order so the bigger $i$ is the smaller the eigenvalue (i.e. the first ones are the larger ones)\n\n\t- Correlation between dimensionality and performances is introduces without saying anything about boundary conditions. Nothing is said about what the maximum dimensionality is, and how it relates not just to the size of the graph but also its properties. For instance, if we have $k$ connected components, we cannot have $d > |V| - k$ because the smallest k eigenvalues would all be zero\n\nIn terms of novelty (and in the light of the previous issues) my impression is that, while the paper brings an interesting solution derived from previous works, in absence of a sound theory behind it the theoretical contribution would just look incremental.\n\nOn the experimental side:\n\n- it is not clear which of the two (orthogonal) model improvements contributes the most to  performances: intuitively, looking at GRACE's performances, I'd say it is the possibility of using all negative pairs in a more scalable way; getting positive pairs from the 1-hop neighbors, on the other hand, mainly allows us to get rid of augmentations. To verify this, I think it would be useful to have an ablation experiment showing how a model such as GRACE with approximate neg loss would perform. It looks like the main difference in GRACE is the presence of the intra-view negative pairs term, which I think could be computed as efficiently as the inter-view one. \n\n- In Table 1, the difference of the top two means in the Photo dataset is not really significant given the values of means and stds and the amount of experiments. While this does not make a difference in practice (I think the claim \"our approach is competitive with the SOA while being more scalable\" is correct), it would be fair to make both of the values bold as it cannot be statistically determined whether one is actually better than the other.\n\n- after the discussion on how positive pairs are generated, results on heterophilic graphs might come unexpected but, as the authors correctly say citing Ma et Al's work, this can happen e.g. due to the model learning to recognize different neigbor distributions. This claim, however, is unsupported by experiments which would allow us both to verify the reason of this behavior and learn (e.g. from counterexamples) when this model would work and when it would not.\n\n# Summary Of The Review\n\nThe paper aims at improving CGL by providing a model which is, at the same time, independent of graph augmentations and more scalable. Motivations are clear and relevant and the presentation of the related work is in my opinion sufficient to place this work in the proper context.\n\nWhile empirical results are in my opinion quite positive, the paper still leaves some open questions about the contributions brought individually by the two components implemented in the model. I also think some claims should be better supported, so that the theoretical contribution becomes more rigorous and less incremental.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nGRAPH CONTRASTIVE LEARNING WITH MODEL PERTURBATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGraph contrastive learning (GCL) has achieved great success in pre-training graph neural networks (GNN) without ground-truth labels. The performance of GCL mainly rely on designing high quality contrastive views via data augmentation. However, finding desirable augmentations is difficult and requires cumbersome efforts due to the diverse modalities in graph data. In this work, we study model perturbation to perform efficient contrastive learning on graphs without using data augmentation. Instead of searching for the optimal combination among perturbing nodes, edges or attributes, we propose to conduct perturbation on the model architectures (i.e., GNNs). However, it is non-trivial to achieve effective perturbations on GNN models without performance dropping compared with its data augmentation counterparts. This is because data augmentation 1) makes complex perturbation in the graph space, so it is hard to mimic its effect in the model parameter space with a fixed noise distribution, and 2) has different disturbances even on the same nodes between two views owning to the randomness. Motivated by this, we propose a novel model perturbation framework – PERTURBGCL to pre-train GNN encoders. We focus on perturbing two key operations in a GNN, including message propagation and transformation. Specifically, we propose weightPrune to create a dynamic perturbed model to contrast with the target one by pruning its transformation weights according to their magnitudes. Contrasting the two models will lead to adaptive mining of the perturbation distribution from the data. Furthermore, we present randMP to disturb the steps of message propagation in two contrastive models. By randomly choosing the propagation steps during training, it helps to increase local variances of nodes between the contrastive views. Despite the simplicity, coupling the two strategies together enable us to perform effective contrastive learning on graphs with model perturbation. We conduct extensive experiments on 15 benchmarks. The results demonstrate the superiority of PERTURBGCL: it can achieve competitive results against strong baselines across both node-level and graphlevel tasks, while requiring shorter computation time. The code is available at https://anonymous.4open.science/r/PerturbGCL-F17D.\n\n1\n\nINTRODUCTION\n\nGraph neural networks (GNN) (Kipf & Welling, 2016a; Hamilton et al., 2017; Gilmer et al., 2017) have become the de facto standard to model graph-structured data, such as social networks (Li & Goldwasser, 2019), molecules (Duvenaud et al., 2015), and knowledge graphs (Arora, 2020). Nevertheless, GNNs require task-specific labels to supervise the training, which is impractical in many scenarios where annotating graphs is challenging and expensive (Sun et al., 2019). Therefore, increasing efforts (Hou et al., 2022; Veliˇckovi ́c et al., 2018; Hassani & Khasahmadi, 2020; Thakoor et al., 2022) have been made to train GNNs in an unsupervised fashion, so that the pre-trained model or learned representations can be directly applied to different downstream tasks.\n\nRecently, graph contrastive learning (GCL) becomes the state-of-the-art approach for both graphlevel (You et al., 2020; 2021; Suresh et al., 2021; Xu et al., 2021) and node-level (Qiu et al., 2020; Zhu et al., 2021b; Bielak et al., 2021; Thakoor et al., 2022) tasks. The general idea of GCL is to create two views of the original input using data augmentation (Jin et al., 2020), and then encode them with two GNN branches that share the same architectures and weights (You et al., 2020). Then,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe model is optimized to maximize the mutual information between the two encoded representations according to contrastive objectives, such as InfoNCE (Oord et al., 2018) or Barlow Twins (Zbontar et al., 2021). As such, the performance of GCL mainly relies on designing high quality contrastive views (Zhang et al., 2021). Recently, intensive studies (You et al., 2020; Jin et al., 2020; Han et al., 2022) has been devoted to exploring effective augmentation strategies for graph data.\n\nDespite their success, finding desirable augmentations requires cumbersome efforts, since the optimal augmentations are domain-specific and vary from graph to graph (You et al., 2020; Yin et al., 2022). To tackle this problem, SimGRACE (Xia et al., 2022) introduced the idea of model perturbation. Instead of searching for the optimal combination among perturbing nodes, edges or attributes in the graph space, SimGRACE conducts perturbation in a unified parameter space by adding Gaussian noise to model weights. However, we observe that SimGRACE may lead to sub-optimal representations compared with its data augmentation counterparts because of two reasons. Firstly, the data augmentation in the graph space is rather complicated and beyond Gaussian distribution. As a result, the weight perturbation based on Gaussian noises cannot achieve similar effects as data perturbation on representation learning (as illustrated in Section 2.2) . Secondly, the weight perturbation does not consider local variances among different nodes in a graph, since the perturbation is data-agnostic. Therefore, it still remains an important yet unsolved challenge to develop effective model perturbation framework for GCL, so that it can produce effective representations on both node and graph learning tasks in a more efficient manner.\n\nTo tackle these challenges, in this work, we propose a novel framework – PerturbGCL to train GNN encoders via model perturbation. Different from SimGRACE (Xia et al., 2022) that only focuses on weight perturbation, we make one step further to disturb the message passing (MP) of GNNs, since it allows to provide local disturbances between contrastive views. Specifically, we present weightPrune to construct a perturbed model by pruning the transformation weights of the target one. Unlike the Gaussian noise in SimGRACE (Xia et al., 2022), the pruned model will co-evolve with the target GNNs, leading to an adaptive mining of the noise perturbation from the data, i.e., datadriven. Furthermore, we propose randMP to offer local disturbances on nodes among contrastive views. It works by conducting k times of message propagation steps in each contrastive model, where k is randomly sampled on-the-fly. Informally, performing MP k times can be thought of as conducting convolution on the anchor node’s k-hops of neighbors (Gao et al., 2018). On this basis, we can learn diverged but correlated representations from the two contrastive models with different k values due to the homophily theory (Altenburger & Ugander, 2018). Coupling the two strategies together yields a principled model perturbation solution tailored for GCL, whose effectiveness and efficiency have been empirically verified through our extensive experiments. We summarize our main contributions as follows:\n\n• We introduce Perturbed Graph Contrastive Learning (PerturbGCL), a principled contrastive learning method on graphs that works by perturbing GNN architectures. PerturbGCL is flexible and easy to implement. To the best of our knowledge, PerturbGCL is the first model perturbation work that can achieve promising results on both node and graph learning tasks.\n\n• PerturbGCL innovates to perturb GNN architectures from both the message passing and model weight perspectives via two effective perturbation strategies: randMP and weightPrune. By applying the two strategies jointly in contrastive models, PerturbGCL can be learned to mimic the effect of data augmentation from the model perturbation aspect.\n\n• Extensive experiments across 15 benchmark datasets demonstrate the superiority of our proposal. Specifically, PerturbGCL can outperform state-of-the-art baselines without using data augmentation across two evaluation scenarios. Moreover, PerturbGCL is easy to optimize and runs generally faster than the strong GCL baselines.\n\n2 METHODOLOGY\n\n2.1 NOTATIONS AND PRELIMINARIES\n\nNotations. Let G = (V, E, X) be an undirected graph, where V is the set of nodes and E is the set of edges. X ∈ R|V|×F is the node feature matrix where the i-th row of X denote the F -dimensional feature vector of the i-th node in V. We use fw denote the mapping function that encodes each node v ∈ G into a D-dimensional representation hv ∈ RD.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nGraph Neural Networks. To learn representations on graph data, we use graph neural networks (GNN) (Kipf & Welling, 2016a; Hamilton et al., 2017; Gilmer et al., 2017) as the encoder fw. Without loss of generality, we present GNN as a message passing network:\n\nh(l)\n\nv = σ(a(l−1)\n\nv Wl), a(l−1)\n\nv\n\n= g(l−1)(h(l−1)\n\nv\n\n, {h(l−1)\n\nu\n\n: u ∈ Nv}),\n\n(1)\n\nwhere h(l) v ∈ RD is the intermediate representation of node v at the l-the layer, Nv denotes the direct neighbors of node v. We use g to denote the message propagation (MP) function, which updates node representations by integrating its neighbors with a transformation function (Wu et al., 2019). Wl ∈ RD×D is the transformation weight matrix and σ is the activation function, such as ReLU.\n\nWe often use the final layer’s output as node-level representations, i.e., hv = hL v where L is the number of layers in a GNN. To get the graph-level representation hG ∈ RD for graph G, we further aggregate all node-level representations in a graph via a readout function:\n\nhG = READOUT({hL\n\nv : v ∈ V}), zG = h(hG) = MLP(hG).\n\n(2)\n\nHere READOUT(·) can be the simple average pooling function or more sophisticated ones (Ying et al., 2018; Gao & Ji, 2019). h(·) is the projection head, and zG ∈ RD denotes the embedding In the development of our method, we follow the existing graph contowards loss estimation. trastive learning practices and consider three state-of-the-art GNNs: GCN (Kipf & Welling, 2016a), GIN (Xu et al., 2018), and ResGCN (Chen et al., 2019).\n\nGraph Contrastive Learning. Graph contrastive learning (GCL) (You et al., 2020) has become an state-of-the-art approach for pre-training GNNs without ground-truth labels (Liu et al., 2021b;a). Unlike reconstruction-based methods (Perozzi et al., 2014; Kipf & Welling, 2016b), GCL is built upon a contrastive objective between the so-called positive pairs and negative pairs generated from the original data. Formally, given an anchor node v, let (zv, z+ v ) denote the representations of positive pairs and (zv, z−\n\nv ) be the negative pairs. The contrastive loss could be defined as:\n\nLCL =\n\n1 |V |\n\n(cid:88)\n\nv∈V\n\n− log\n\nexp(sim(zv, z+\n\nv )/τ )\n\nexp(sim(zv, z+\n\nv )/τ ) + (cid:80)\n\nu∈V,u̸=v exp(sim(zv, z−\n\nu )/τ )\n\n,\n\n(3)\n\nwhere τ is the temperature parameter, sim(·, ·) denotes the similarity function. By minimizing Eq. 3, the GNN encoder will be trained to enforce the similarity of the positive pairs while enlarging the distance of negative pairs in the hidden space. It is also worth noting that some GCL variants (Thakoor et al., 2022; Bielak et al., 2022) do not rely on negative samples. The key question in contrastive learning is how to generate effective positive (or negative) pairs. To this end, graph augmentation has been adopted as the golden rule in GCL (You et al., 2020; Jin et al., 2020). Typical graph augmentation techniques include edge perturbation, node masking and attribute masking.\n\n2.2 THE PROPOSED PERTURBGCL FRAMEWORK\n\nMotivation: How to perform model perturbation on GCL? Given a graph G = (A, X) where A is the adjacency matrix. For illustration purposes, we consider the mapping function (cid:3) fw(A, X; W) with one simple GCN (Wu et al., 2019) layer without activation function. Then, the hidden representation is computed by fw(A, X; W) = g(A, X)W, where W is the weight matrix and g(·, ·) is the message propagation operation defined in Eq. 1. g(A, X) = (cid:101)D− 1 2 Xv, where (cid:101)Dii = (cid:80) j(A+IG)ij is the degree matrix and IG is the identity matrix. An intuitive solution is adding Gaussian noise to the model weight and derive two contrastive views f ′ w(A, X; W) and f ′′\n\nw(A, X; W), as done in SimGRACE (Xia et al., 2022):\n\n2 (A+IG) (cid:101)D− 1\n\nf ′\n\nw(A, X; W) = g(A, X)W, w(A, X; p(W)) = g(A, X)p(W),\n\nf ′′\n\n(4)\n\nwhere p(W) = W + η∆W is the perturbation function on model weight, and ∆W ∼ N (0, δ2) represents the noise term sampled from Gaussian distribution with zero mean and variance δ2, and η is a hyperparameter to scale the magnitude of the perturbation. Since the learning task is to minimize the distance between the two views, SimGRACE trains the model so that it is robust to the Gaussian noise in the weight.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The alignment and uniformity performance (↓) of SimGRACE and augmentation-based GCL method (Thakoor et al., 2022) on the same perturbed graphs. Black circles ( ) indicate the results of baselines. Orange circles ( ) represent the results of SimGRACE. SimGRACE performs worse than standard GCL methods in modeling common perturbations achieved by data augmentation. Detailed experimental setups are listed in Appendix B. However, we found that the contrastive views created by SimGRACE could be sub-optimal compared with its data-augmentation counterparts (see Figure 1). To measure the quality of representations learned by GCL models, we consider two popular metrics: alignment and uniformity (Wang & Isola, 2020) expressed as:\n\n(cid:32)\n\n(cid:32)\n\nLalign(fw; α) ≜ E(x,y)∼Ppos[||hx − hy||α\n\n2 ], α > 0\n\nLuniform(fw; t) ≜ log E\n\n(x,y)i.i.d∼ Pdata\n\n[e−t||hx−hy||2\n\n2 ]. t > 0\n\n(5)\n\nPpos is the distribution of positive pairs, i.e., augmentations of the same sample, Pdata is the data distribution. Lalign is used to measure if positive samples stay close in the hidden space. Luniform is used to measure if random samples are scattered on the hypersphere of hidden space. In our experiments, we set α = t = 2 following (Xia et al., 2022).\n\nFigure 1 reports the test results of SimGRACE and augmentation-based GCL methods (Zhang et al., 2021; Thakoor et al., 2022; Zhu et al., 2020) on the same perturbed graphs created by random edge and attribute masking. Black circles ( ) represent the performance of baselines and SimGRACE, respectively. As can be observed, SimGRACE performs worse than three baselines across different datasets with a great margin. These results shed light on the bottleneck of SimGRACE in capturing common perturbations achieved by data augmentation. This is because SimGRACE is limited by the Gaussian noise and cannot handle perturbations in graph space. We thus ask: Can we design more advanced model perturbation strategies so that they can achieve similar functions as data augmentation done in GCL?\n\n) and rrange circles (\n\n(cid:32)\n\n(cid:32)\n\nWhat is behind data augmentation in GCL? To answer this question, we start by analyzing the working mechanism behind the standard GCL framework. Let T (·) and q(·) denote augmentation functions on topology structure and node attributes, respectively. The two contrastive representations w(A, X; W) and f ′′ f ′\n\nw(A, X; W) of standard GCL (You et al., 2020) are defined as:\n\nw(A, X; W) = g(T ′(A), q′(X))W, f ′ w(A, X; W) = g(T ′′(A), q′′(X))W. f ′′\n\n(6)\n\nThat is, standard GCL framework learns to be robust to small disturbances (created by (T ′(·), q′(·)) and ((T ′′(·), q′′(·))) on the graph. We can easily observe two major properties of data augmentation as follows. ❶ It can disturb different nodes in a graph (different graphs in a graph set) differently. This is because T (·) and q(·) are random functions, such as edge masking, so they can have distinct effects even for the same input. ❷ The perturbation distribution incurred by data augmentation (e.g., (T ′(·), q′(·))) is complicated and often beyond Gaussian noise. As analyzed in Figure 1, although SimGRACE is trained to be robust to Gaussian noise to some extent, it cannot handle the perturbation in the graph space well. These properties motivate us to design tailored model perturbation strategies from the two aspects.\n\nOur PerturbGCL proposal. To birdge the gap, we propose a novel model perturbation framework called PerturbGCL. Figure 2 provide the overview our framework. In this work, we build (cid:3) PerturbGCL upon the GraphCL pipeline (You et al., 2020), and follows its major components, such as two GNN branches and a non-linear projection head. The main difference is that GraphCL augments the input graph to get two views and then process them with two branches that share the same GNN architecture and weights, while PerturbGCL processes the original input graph with two nonsymmetric GNN branches. One branch uses the original GNN model fw, while the other branch\n\n4\n\n2.01.81.61.4uniform0.200.220.240.260.280.30align (%)PubMedCCA-SSGGRACEBGRLSimGRACE2.502.252.001.751.501.25uniform0.380.400.420.44align (%)Amazon-PhotoCCA-SSGGRACEBGRLSimGRACE2.01.81.61.41.2uniform0.220.240.260.280.300.32align (%)Coauthor-CSCCA-SSGGRACEBGRLSimGRACEUnder review as a conference paper at ICLR 2023\n\nFigure 2: The overview of the proposed PerturbGCL framework. The original graph is fed into two asymmetric GNN branches: one is the target encoder fw to be trained, and the other is the perturbed version f ′ w that is pruned from the former online. The two branches share weights for their nonpruned parameters. Either branch has independent message propagation (MP) operations perturbed by a random number, i.e., k, to disturb nodes locally. Since the pruned branch is always obtained and updated from the latest target model, the two branches will co-evolve during training.\n\ndisturbs the message propagation process and model weights of fw. We conduct perturbation on two major GNN operations, including message passing (MP) and transformation. On this basis, we introduce the following simple yet effective perturbation strategies: randMP and weightPrune, towards effective model augmentation for GNN architectures.\n\nStrategy #1: weightPrune. Recently, model pruning has attracted increasing attention for model compression thanks to the popularity of the lottery ticket hypothesis (Frankle & Carbin, 2018). Work in (Chen et al., 2021) found that GNN can be pruned to a sparse sub-network without significant performance drop using rewinding techniques. These observations indicate the latent of pruning as a practical perturbation approach to GNN’s weights. Inspired by this, we propose weightPrune, which creates the perturbed branch by pruning the model parameters of the target encoder. Specifically, assuming W denote the weights of the target branch, mw be the mask of the pruned branch, which has the same size as W. At each iteration, we prune the target branch according to a pre-defined prune ratio s according to the magnitude of weight values, i.e., masking weights out if their magnitudes are ranked below s. By changing s, we can control the distortion degree of the target branch to a certain extent. After that, the target and perturbed branches will use W and W ⊙ mw respectively as model weights to generate representations, which are then fed into the contrastive loss. Since the mask indicator mw is continuously updated from the latest target model, the two branches will co-evolve during training.\n\nStrategy #2: randMP. Message passing is another critical component of the GNN architecture, since it offers the flexibility to aggregate features from multi-hop neighbors. Performing MP for k times over the graph G is equivalent to updating node v’s representation based on its k-hop local subgraph. In light of this, k could be naturally regarded as a perturbation factor, where different k values generate diverse but semantically correlated representations for the same node. To implement randMP, we will randomly sample two k values at each iteration: one is for the target branch (i.e., k′) and the other for the perturbed branch (i.e., k′′). Formally, if we assume g(A, X) = (cid:101)D− 1 2 )kXv. In experiments, we consider sampling k during the training because it may enforce the GNN encoder to learn generable representations invariant to different combinations of local enclosing graphs.\n\n2 Xv, performing MP k times gives g(A, X)k = ( (cid:101)D− 1\n\n2 (A + IG) (cid:101)D− 1\n\n2 (A + IG) (cid:101)D− 1\n\nTo sum up, different from existing methods (Xia et al., 2022; Thakoor et al., 2022) that disturb model weights with Gaussian noise, we suggest a principled approach to effectively perturb GNN architectures from their message propagation and feature transformation perspectives, via two simple perturbation techniques: randMP and weightPrune. randMP aims to map the same input graph into two semantically similar representations by conducting a random number of message passing steps. Meanwhile, weightPrune targets to increase the diversity of two representations via model pruning. Combining the two strategies enables us to spot the sweet point between the two view\n\n5\n\n!\"!\"\"Projection headProjection headMaximize agreementInput graph: #(%,')))⊙+,-.GNN encoder EmbeddingMessage propagationWeight prune!\"~0(1,2)!\"\"~0(1,2)33.-h(⋅)6,(%,';!\",))6,\"(%,';!\"\",)⊙+,)!\"times!\"\"timesUnder review as a conference paper at ICLR 2023\n\nrepresentations (Tian et al., 2020), i.e., correlated but diverged enough. The complete optimization procedure of our model is outlined in Algorithm 1 and 2 in Appendix.\n\n2.3 MORE DISCUSSIONS ON PERTURBGCL\n\nPerturbGCL is complementary to other GCL efforts. We focus on training GCL by only using model perturbation. It can be easily combined with existing contrastive learning advances, such as mature graph augmentation techniques (You et al., 2020) and the negative-sample free contrastive loss objectives (Bielak et al., 2022; Thakoor et al., 2022), as we will show in Section 3.4.\n\nComputational complexity analysis. In addition to saving a lot of time in searching for the optimal data augmentation strategies, we analyze the complexity of PerturbGCL. Given a graph G = (V, E) and the GNN encoder fw. The time complexity for most popular GNN architectures (Kipf & Welling, 2016a; Veliˇckovi ́c et al., 2017; Gilmer et al., 2017) is O(|E| + |V|), where O(|E|) and O(|V|) are mainly caused by the message propagation and feature transformation operations, respectively. PerturbGCL performs two encoder computations per update step (one for each GNN branch) plus a node-level projection head. Assuming that the backward pass to be approximately as costly as a forward pass and ignoring the cost for weight pruning as it is small and negligible. Thus the total time complexity per update step for PerturbGCL is 4Cencoder(K|E| + |V|s) + 2Chead(|V|) + Closs, where C. are constants depending on architecture of the different components, K is the maximum number of MP operations considered (e.g., K = 3), and s is the pruning ratio. It is worth noting that although our model at most takes K times MP operations in forward pass, due to weight pruning (e.g., s = 70%), the computation costs for feature transformation and backpropagation are significantly lower than standard GCL methods. Therefore, the total running cost of PerturbGCL can be further accelerated in practice. We empirically analyze the efficiency of our model in Section 3.5.\n\n3 EXPERIMENTS\n\nIn this section, we evaluate the performance of PerturbGCL. Specifically, we first visualize the weight distribution and the alignment between positive pairs in Section 3.1 to investigate what the proposed two strategies actually do. Then we evaluate the effectiveness of PerturbGCL in node classification with several benchmarks and SOTA baselines in Section 3.2. Next, we test the performance of PerturbGCL in graph classification under both unsupervised and semi-supervised settings in Section 3.3. After that, we evaluate the contributions of different components in Section 3.4, as well as how it improves the efficiency of GCL against standard methods in Section 3.5. The experiment setting and more experiments are summarized in Appendices A and E. Through the experiments, the main observations are highlighted.\n\nFigure 3: Visualization of weight distribution (from left to right: initial weights, PerturbGCL w/o. weightPrune, and PerturbGCL) on Coauthor-Phy. The x-axis indicates weight values and y-axis is the count. Obviously, the number of activated neurons after using weightPrune is significantly smaller than others. It shows that weightPrune can regularize the model.\n\n3.1 WHAT ARE weightPrune AND randMP DOING? A CASE STUDY\n\nWe visualize the weight distribution of PeruturbGCL during training in Figure 3 and Figure 8 (in Appendix). It shows that 1 by continuously pruning the target model along the training, weightPrune can regularize the target model progressively. From the weight histograms in Figure 3, we can see that the bars around zero become higher and higher, which indicates more neurons are inactivated in the end. This observation shows the regularization effect of weightPrune. Since effective\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The visualization of PerturbGCL w.r.t. different k values on the original graphs and the perturbed graphs generated by data augmentation. The x-axis indicates propagation steps and yaxis is the Lalign ↓. The gap between the blue and orange lines indicate the generalization ability. Apparently, performing more MP steps will increase the diversity of two positive views since Lalign increases. Sweet spots (i.e., minimum performance gap) exist across three scenarios. Table 1: Test accuracy on benchmark datasets in terms of node classification. We report both mean accuracy and standard deviation. A.R. denotes the averaged rank.\n\nMethod\n\nCora\n\nPubMed\n\nCompute\n\nPhoto\n\nCS\n\nPhy\n\nA.R. ↓\n\nSupervised\n\nGCN GAT\n\n81.5 83.0 ± 0.7\n\n79.0 79.0 ± 0.3\n\n86.51 ± 0.54 92.42 ± 0.22 93.03 ± 0.31 95.65 ± 0.16 86.93 ± 0.29 92.56 ± 0.35 92.31 ± 0.24 95.47 ± 0.15\n\n8.33 8.00\n\nRaw Features 47.9 ± 0.4 70.7 ± 0.6 DeepWalk 71.5 ± 0.4 GAE 82.3 ± 0.6 DGI 83.5 ± 0.4 MVGRL 81.9 ± 0.4 GRACE 83.4 ± 0.3 GCA 81.8 ± 0.3 BGRL −\nGBT 83.5 ± 0.3 InfoGCL CCA-SSG 84.2 ± 0.4 82.3 ± 0.4 SimGRACE 78.5 ± 0.3\n\nAFGRL\n\n69.1 ± 0.3 74.3 ± 0.9 72.1 ± 0.5 76.8 ± 0.6 80.1 ± 0.7 80.6 ± 0.4 80.3 ± 0.4 80.4 ± 0.5 −\n79.1 ± 0.2 81.6 ± 0.4 79.7 ± 0.2 79.3 ± 0.5\n\nUnsupervised\n\n73.81 ± 0.00 78.53 ± 0.00 90.37 ± 0.00 93.58 ± 0.00 14.33 85.68 ± 0.06 89.44 ± 0.11 84.61 ± 0.22 91.77 ± 0.15 13.67 85.27 ± 0.19 91.62 ± 0.13 90.01 ± 0.71 94.92 ± 0.07 12.50 83.95 ± 0.47 91.61 ± 0.22 92.15 ± 0.63 94.51 ± 0.52 11.00 87.52 ± 0.11 91.74 ± 0.07 92.11 ± 0.12 95.33 ± 0.03 7.50 89.53 ± 0.35 92.78 ± 0.45 91.12 ± 0.20 6.40 87.85 ± 0.31 92.49 ± 0.09 93.10 ± 0.01 95.68 ± 0.05 5.33 89.68 ± 0.31 92.87 ± 0.27 93.21 ± 0.18 95.56 ± 0.12 4.83 88.14 ± 0.33 92.63 ± 0.44 92.95 ± 0.17 95.07 ± 0.17 7.00 6.00 88.74 ± 0.28 93.14 ± 0.14 93.31 ± 0.22 95.38 ± 0.06 3.17 89.88 ± 0.33 93.22 ± 0.28 93.27 ± 0.17 95.69 ± 0.10 3.83 86.42 ± 0.35 91.55 ± 0.22 92.37 ± 0.33 94.37 ± 0.15 10.67\n\n−\n\n−\n\n−\n\n−\n\n−\n\nPerturbGCL 83.3 ± 0.5 82.10 ± 0.37 88.45 ± 0.77 93.62 ± 0.40 94.18 ± 0.09 95.85 ± 0.08 2.33\n\nregularization can improve the generalization ability of neural networks (Scholkopf & Smola, 2018), we believe that why the proposed weightPrune can improve the performance.\n\nTo investigate the effect of randMP, we report the impacts of different k values on PerturbGCL in terms of the alignment between positive views. From Figure 4, we observe that 2 randMP can improve the diversity of contrastive views when k increases, and sweet points widely exist In Figure 4, with the increase of k, the generalization gap tends to first across three datasets. It validates the effect of randMP in decrease to the sweet points and then increase a little bit. generating correlated but diverged views. On the other hand, it indicates the potential of randMP to improve the generalization ability, i.e., these sweet points.\n\n3.2 CAN PERTURBGCL PERFORM WELL ON NODE CLASSSIFICATION TASK?\n\nWe first examine the effectiveness of PerturbGCL in node classification. Results of 15 baseline methods across 6 benchmark datasets are collected in Table 1. We make the following observations: 3 PerturbGCL can achieve better node classification results than SOTA GCL methods without using data augmentation. From Table 1, PerturbGCL gains 4 best performances among 6 evaluation scenarios. On average, it ranks 2.33 among 13 augmentation-based baselines including strong methods, such as BGRL and CCA-SSG, which indicates the power of model perturbation based contrastive learning. Meanwhile, 4 PerturbGCL outperforms the model perturbation baseline – SimGRACE with great margins. Among 6 datasets, SimGRACE loses to PerturbGCL in all cases. Specifically, PerturbGCL improves SimGRACE 6.11%, 3.53%, 2.34%, 2.26%, 1.95%, and 1.56% on Cora, PubMed, Computer, Photo, CS, and Phy, respectively. This result is in line with our analysis in Section 2.2.\n\n3.3 CAN PERTURBGCL GENERALIZE WELL TO GRAPH CLASSIFICATION TASK?\n\nTo validate the effectiveness of PerturbGCL on graph classification, we compare it with state-ofthe-art graph-level GCL methods on different datasets. Table 2 and Table 9 (in Appendix) report\n\n7\n\n2468k0.8750.9000.9250.9500.9751.000PubMedPerturbed graphOriginal graph2468k0.080.100.120.140.160.18Amazon-PhotoPerturbed graphOriginal graph2468k0.10500.10750.11000.11250.11500.1175Coauthor-CSPerturbed graphOriginal graphUnder review as a conference paper at ICLR 2023\n\nTable 2: Test accuracy on benchmark datasets in TUdatasets in terms of the unsupervised setting for graph classification. − means that results are not available in published papers.\n\nMUTAG\n\nCOLLAB\n\nRDT-B\n\nRDT-M5K\n\nIMDB-B\n\nA.R. ↓\n\nMethods\n\nNCI1\n\nPROTEINS\n\nGL WL DGK\n\nnode2vec sub2vec graph2vec MVGRL InfoGraph GraphCL JOAO JOAOv2 SimGRACE\n\n− 80.01 ± 0.50 80.31 ± 0.46\n\n54.89 ± 1.61 52.84 ± 1.47 73.22 ± 1.81 −\n76.20 ± 1.06 77.87 ± 0.41 78.07 ± 0.47 78.36 ± 0.53 79.12 ± 0.44\n\n− 72.92 ± 0.56 73.30 ± 0.82\n\n57.49 ± 3.57 53.03 ± 5.55 73.30 ± 2.05 −\n74.44 ± 0.31 74.39 ± 0.45 74.55 ± 0.41 74.07 ± 1.10 75.35 ± 0.09\n\nDD\n\n− −\n−\n\n− −\n− −\n72.85 ± 1.78 78.62 ± 0.40 77.32 ± 0.54 77.40 ± 1.15 77.44 ± 1.11\n\n81.66 ± 2.11 80.72 ± 3.00 87.44 ± 2.72\n\n72.63 ± 10.20 61.05 ± 15.80 83.15 ± 9.25 75.40 ± 7.80 89.01 ± 1.13 86.80 ± 1.34 87.35 ± 1.02 87.67 ± 0.79 89.01 ± 1.31\n\n− −\n−\n\n− −\n− −\n70.65 ± 1.13 71.36 ± 1.15 69.50 ± 0.36 69.33 ± 0.34 71.72 ± 0.82\n\n77.34 ± 0.18 68.82 ± 0.41 78.04 ± 0.39\n\n− 71.48 ± 0.41 75.78 ± 1.03 82.00 ± 1.10 89.53 ± 0.84 89.53 ± 0.84 85.29 ± 1.35 86.42 ± 1.45 89.51 ± 0.89\n\n41.01 ± 0.17 46.06 ± 0.21 41.27 ± 0.18\n\n− 36.68 ± 0.42 47.86 ± 0.26 −\n55.99 ± 0.28 55.99 ± 0.28 55.74 ± 0.63 56.03 ± 0.27 55.91 ± 0.34\n\n65.87 ± 0.98 72.30 ± 3.44 66.96 ± 0.56\n\n− 55.26 ± 1.54 71.10 ± 0.54 63.60 ± 4.20 73.03 ± 0.87 71.14 ± 0.44 70.21 ± 3.08 70.83 ± 0.25 71.30 ± 0.77\n\nPerturbGCL 80.24 ± 0.45\n\n76.08 ± 0.30\n\n78.33 ± 0.37\n\n89.97 ± 0.50\n\n75.06 ± 0.87\n\n88.98 ± 0.67\n\n55.78 ± 0.72\n\n74.14 ± 0.50\n\n9.50 7.50 6.50\n\n10.67 11.50 8.00 9.67 3.63 4.13 5.63 4.75 3.13\n\n2.13\n\nFigure 5: Left: Ablation study of PerturbGCL. Middle: The impact of different contrastive objectives. Right: Empirical training curves of PerturbGCL with different s values.\n\nthe results on unsueprvised and semi-supervised settings, respectively. We observed that 5 PerturbGCL generally performs better than other baselines across two graph learning tasks. From the unsupervised setting (See Table 2), PerturbGCL achieves the best (or comparable best) results on 6 of 8 datasets, and obtain substantial improvements on COLLAB and IMDB-B datasets. In the semi-supervised setting (See Table 9 in Appendix), PerturbGCL generally performs better than other baselines across 7 comparisons and always ranks top three on all the datasets. These results demonstrate the effectiveness of PerturbGCL on graph learning task.\n\n3.4 ABLATION STUDY\n\nWe investigate the contributions of different components in PerturbGCL. Figure 5 and Figure 9 in Appendix report the results on graph and node datasets, respectively. We observe that 6 PerturbGCL benefits from the combination of randMP with weightPrune. From the figures, PerturbGCL consistently outperforms two variants (i.e., w/o MP and w/o WP) in all cases, which indicates the reciprocal effects of using randMP and weightPrune together. Moreover, 7 replacing weightPrune with Gaussian noise, PerturbGCL drops significantly. In both node and graph scenarios, PerturbGCL outperforms the ”noise” variant with a great margin. It verifies the effectiveness of the proposed weightPrune strategy.\n\nWe also test the results of PerturbGCL under different contrastive objectives, such as Barlow Twins (Bielak et al., 2021), Bootstrap (Thakoor et al., 2022), and InfoNCE (You et al., 2020) (Please refer to Appendix C for details). From Figure 5 (middle), we observe that 8 PerturbGCL performs generally better on InfoNCE and Barlow Twins objectives. Given that InfoNCE is standard contrastive loss and Barlow Twins is negative-sample free, PerturbGCL is ready to be applied on scenarios with informative negative sample or without negative samples by using different losses.\n\n3.5 CAN PERTURBGCL IMPROVE THE TRAINING EFFICIENCY OF GCL?\n\nWe compare the proposed PerturbGCL with strong GCL baselines in terms of the training costs in Table 3 and report the optimization curves in Figure 5 (right panel). Experimental configurations are listed in Appendix D. We observed that 9 using pure model perturbation, PerturbGCL is training efficient. From Table 3, we can see that PerturbGCL runs significantly faster per epoch than strong baselines in general, and the performance gap is particularly evident in graph datasets. Besides, PerturbGCL can converge within one hundred epochs in practice as shown in Figure 5 (Right). Thus, the total training time of PerturbGCL could be further reduced.\n\n8\n\nPhotoCSPhy5060708090100Accuracy (%)Noisew/o MPw/o WPPerturbGCLPubMedPhotoCS5060708090100Accuracy (%)Barlow TwinsInfoNCEBootstrap20406080100Epochs7.758.008.258.508.759.009.259.50Trining LossCoauthor-CSs=0.5s=0.7s=0.9Under review as a conference paper at ICLR 2023\n\nTable 3: Running time per epoch (in seconds). Baseline indicates BGRL and GraphCL for node and graph classification, respectively. All the methods are evaluated on GeForce RTX 2080 Ti GPUs.\n\nNode Benchmark\n\nGraph Benchmark\n\nPubMed Computer Photo CS NCI1 COLLAB RDT-B RDT-M2K\n\nBaseline PerturbGCL Speedup (vs Baseline)\n\n0.14 0.10 1.40x\n\n0.16 0.09 1.77x\n\n0.08 0.21 4.02 0.05 0.17 1.42 1.60x 1.23x 2.83x\n\n10.84 3.01 3.60x\n\n38.35 6.21 6.17x\n\n80.79 12.80 6.31x\n\n3.6 FURTHER ANALYSIS\n\nWe finally investigate the sensitivity of PerturbGCL w.r.t. the propagation degree K and prune ratio s in Figure 10 (Left) of Appendix, the impact of graph augmentation on PerturbGCL in Figure 10 (Middle), and the learning capacity of PerturbGCL in Figure 10 (Right). We can observe that 10 PerturbGCL performs stably when K ∈ [1, 2, 3, 4, 5] and s ∈ [0.7, 0.9]. In Figure 10 left, the performance of PerturbGCL when K = 0.9 (or 0.7) is consistently better than others. PerturbGCL is complementary with advanced graph augmentation. From Figure 10 (Middle), by feeding the augmented graphs as input, PerturbGCL can be further improved. However, the trade-off is that the improvement is not huge but the time to search optimal augmentation strategies is costing. Although PerturbGCL is trained based on original graphs, it can generalize to perturbed graphs well. As shown in Figure 10 (Right), PerturbGCL has lower Lalign and Luniform values than SimGRACE and strong GCL baseline, which indicates the effectiveness of the proposed model.\n\n4 RELATED WORK\n\nWe briefly introduce some related graph contrastive learning methods (Xie et al., 2022) and refer readers to (Zhou et al., 2020) for a comprehensive review of graph neural networks.\n\nGraph contrastive learning with data augmentation. Similar to contrastive learning on images (Chen et al., 2020), data augmentation is crucial to the success of contrastive learning on graphs (GCL). Recently there has been steady progress (You et al., 2020; 2021; Qiu et al., 2020; Lee et al., 2022; Luo et al., 2022) in designing or identifying informative augmentation strategies to boost the performance of GCL. GraphCL (You et al., 2020) introduces four augmentation prototypes for graphs, including node dropping, edge perturbation, attribute masking, and subgraph sampling. MoCL (Sun et al., 2021) and G-Mixup (Han et al., 2022) propose to utilize domain knowledge such as bioisosteres and graphon to aid augmentation. AutoGCL (Yin et al., 2022), JOAO (You et al., 2021), and GPA (Zhang et al., 2022) suggest to leverage extra AutoML techniques (Waring et al., 2020) to free human labor on augmentation choices. Unlike this line of research, we focus on training GCL methods without explicitly graph augmentation.\n\nGraph contrastive learning without data augmentation. To eliminate the influence of graph augmentation on GCL, AFGRL (Lee et al., 2022) suggests sampling nodes that share similar semantic information in the hidden space as positive samples. However, it requires non-negligible clustering efforts to spot positive pairs in the learning process. SimGRACE (Xia et al., 2022) proposes to train GCL by disturbing the model weight using Gaussian noise. However, SimGRACE could limit applications beyond Gaussian distribution, as illustrated in Figure 1. In this work, we introduce a tailored model perturbation framework for GNN encoders without constraining the noise distribution.\n\n5 CONCLUSION\n\nIn this work, we explore how to perform contrastive learning on graphs without using data augmentation, and propose a principled framework – PerturbGCL, which is built upon pure model perturbations. Specifically, motivated by the fact that GNN can be divided into message propagation and feature transformation operations, we develop two tailored perturbation strategies: randMP and weightPrune, to effectively disturb GNN’s two crucial operations accordingly. We build connections between our model perturbation strategies and well-established graph augmentation techniques to understand the working mechanism of PerturbGCL. Through extensive experiments across multiple datasets and different graph learning tasks, we show that PerturbGCL can achieve competitive results against strong baselines, while requiring substantially shorter computation time for training.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nBijaya Adhikari, Yao Zhang, Naren Ramakrishnan, and B Aditya Prakash. Sub2vec: Feature learning for subgraphs. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 170–182. Springer, 2018.\n\nKristen M Altenburger and Johan Ugander. Monophily in social networks introduces similarity\n\namong friends-of-friends. Nature human behaviour, 2(4):284–290, 2018.\n\nSiddhant Arora. A survey on graph neural networks for knowledge graph completion. arXiv preprint\n\narXiv:2007.12374, 2020.\n\nPiotr Bielak, Tomasz Kajdanowicz, and Nitesh V Chawla. Graph barlow twins: A self-supervised\n\nrepresentation learning framework for graphs. arXiv preprint arXiv:2106.02466, 2021.\n\nPiotr Bielak, Tomasz Kajdanowicz, and Nitesh V Chawla. Graph barlow twins: A self-supervised representation learning framework for graphs. Knowledge-Based Systems, pp. 109631, 2022.\n\nTianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A unified lottery\n\nticket hypothesis for graph neural networks. In ICML, pp. 1695–1706. PMLR, 2021.\n\nTing Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on\n\ngraph classification. arXiv preprint arXiv:1905.04579, 2019.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, pp. 1597–1607. PMLR, 2020.\n\nDavid K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al ́an Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. Advances in neural information processing systems, 28, 2015.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n\nnetworks. arXiv preprint arXiv:1803.03635, 2018.\n\nHongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learning,\n\npp. 2083–2092. PMLR, 2019.\n\nHongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional\n\nnetworks. In KDD, pp. 1416–1424, 2018.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 855–864, 2016.\n\nWilliam L Hamilton, Rex Ying, and Jure Leskovec.\n\nInductive representation learning on large\n\ngraphs. In NeurIPS, pp. 1025–1035, 2017.\n\nXiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for\n\ngraph classification. arXiv preprint arXiv:2202.07179, 2022.\n\nKaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on\n\ngraphs. In International Conference on Machine Learning, pp. 4116–4126. PMLR, 2020.\n\nZhenyu Hou, Xiao Liu, Yuxiao Dong, Chunjie Wang, Jie Tang, et al. Graphmae: Self-supervised\n\nmasked graph autoencoders. arXiv preprint arXiv:2205.10803, 2022.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang Tang. arXiv preprint\n\nSelf-supervised learning on graphs: Deep insights and new direction. arXiv:2006.10141, 2020.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. arXiv preprint arXiv:1609.02907, 2016a.\n\nThomas N Kipf and Max Welling.\n\nVariational graph auto-encoders.\n\narXiv preprint\n\narXiv:1611.07308, 2016b.\n\nNamkyeong Lee, Junseok Lee, and Chanyoung Park. Augmentation-free self-supervised learning\n\non graphs. arXiv preprint arXiv:2112.02472, 2021.\n\nNamkyeong Lee, Junseok Lee, and Chanyoung Park. Augmentation-free self-supervised learning In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.\n\non graphs. 7372–7380, 2022.\n\nChang Li and Dan Goldwasser. Encoding social information with graph convolutional networks forpolitical perspective detection in news media. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2594–2604, 2019.\n\nXiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-\n\nsupervised learning: Generative or contrastive. TKDE, 2021a.\n\nYixin Liu, Shirui Pan, Ming Jin, Chuan Zhou, Feng Xia, and Philip S Yu. Graph self-supervised\n\nlearning: A survey. arXiv preprint arXiv:2103.00111, 2021b.\n\nYouzhi Luo, Michael McThrow, Wing Yee Au, Tao Komikado, Kanji Uchino, Koji Maruhash, arXiv preprint\n\nand Shuiwang Ji. Automated data augmentations for graph classification. arXiv:2202.13248, 2022.\n\nJulian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pp. 43–52, 2015.\n\nChristopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663, 2020.\n\nAnnamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint arXiv:1707.05005, 2017.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repreIn Proceedings of the 20th ACM SIGKDD international conference on Knowledge\n\nsentations. discovery and data mining, pp. 701–710, 2014.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In KDD, pp. 1150–1160, 2020.\n\nBernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,\n\nregularization, optimization, and beyond. MIT press, 2018.\n\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\n\nCollective classification in network data. AI magazine, 29(3):93–93, 2008.\n\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G ̈unnemann. Pitfalls\n\nof graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nNino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Efficient graphlet kernels for large graph comparison. In Artificial intelligence and statistics, pp. 488–495. PMLR, 2009.\n\nNino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.\n\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web, pp. 243–246, 2015.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang.\n\nInfograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019.\n\nMengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: Contrastive learning on molecular graphs with multi-level domain knowledge. arXiv preprint arXiv:2106.04509, 2021.\n\nSusheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve\n\ngraph contrastive learning. NeurIPS, 34:15920–15933, 2021.\n\nShantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, R ́emi Munos, Petar Veliˇckovi ́c, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. In ICLR, 2022.\n\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? Advances in Neural Information Processing Systems, 33:6827–6839, 2020.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\nPetar Veliˇckovi ́c, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp. 9929–9939. PMLR, 2020.\n\nJonathan Waring, Charlotta Lindvall, and Renato Umeton. Automated machine learning: Review of the state-of-the-art and opportunities for healthcare. Artificial intelligence in medicine, 104: 101822, 2020.\n\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simpli-\n\nfying graph convolutional networks. In ICML, 2019.\n\nJun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. Simgrace: A simple framework for graph contrastive learning without data augmentation. In Proceedings of the ACM Web Conference 2022, pp. 1070–1079, 2022.\n\nYaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: A unified review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nDongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Informationaware graph contrastive learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? arXiv preprint arXiv:1810.00826, 2018.\n\nPinar Yanardag and SVN Vishwanathan. Deep graph kernels.\n\nIn Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1365–1374, 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. Autogcl: Automated graph contrastive learning via learnable view generators. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8892–8900, 2022.\n\nZhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. Advances in neural information processing systems, 31, 2018.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n\ncontrastive learning with augmentations. NeurIPS, 33:5812–5823, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-\n\nmated. arXiv preprint arXiv:2106.07594, 2021.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St ́ephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pp. 12310– 12320. PMLR, 2021.\n\nHengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation analysis to self-supervised graph neural networks. Advances in Neural Information Processing Systems, 34:76–89, 2021.\n\nXin Zhang, Qiaoyu Tan, Xiao Huang, and Bo Li. Graph contrastive learning with personalized\n\naugmentation. arXiv preprint arXiv:2209.06560, 2022.\n\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57–81, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020.\n\nYanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning.\n\n2021a.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning\n\nwith adaptive augmentation. In WWW, pp. 2069–2080, 2021b.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA EXPERIMENT SETUP\n\nIn this section, we introduce the datasets used in our experiments. Specifically, we adopt the following 6 popular node-level datasets and summarize their statistics in Table 4.\n\n• Cora, and PubMed: They are two widely used citation network datasets (Sen et al., 2008). Nodes represent documents and edges denote citation links. Each node has a sparse bagof-the-words feature vectors. Labels are defined as the academic topics.\n\n• Amazon-Computers and Amazon-Photo: They are two networks of co-purchase relationships constructed from Amazon (McAuley et al., 2015). Nodes indicate goods and edges represent the co-purchase relationships of two products. Each node has a sparse bag-ofwords feature encoding products reviews and is labeled with its category. They are widely used for node classification task. Nodes represent authors and edges indicate co-authorship relationships. Each node has a sparse bag-of-words feature based on paper keywords of the author. The task is to predict the most active research field of authors.\n\n• Coauthor-CS and Coauthor-Physics: They are two academic networks, which represent co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge (Sinha et al., 2015).\n\nMoreover, we also consider 9 graph-level benchmark datasets to verity the effectiveness of PerturbGCL on graph-learning task. Specifically, we use 5 social networks (COLLAB, REDDIT-BINARY, REDDIT-MULTI-5K, IMDB-BINARY, and GITHUB), and 2 molecules networks (NCI1 and MUTAG), and 2 bioinformatics networks (PROTEINS and DD) from the benchmark TUDdataset (Morris et al., 2020). Table 5 lists their statistics.\n\nTable 4: Dataset statistics of node-level benchmarks.\n\nData Cora PubMed\n\n# Nodes # Edges # Features Split ratio # Classes 5, 429 2, 708 19, 717 44, 338 Amazon-Computers 13, 752 245, 861 7, 650 119, 081 18, 333 81, 894 34, 493 247, 962\n\n85/5/15 85/5/15 −\n− −\n−\n\nAmazon-Photo Coauthor-CS Coauthor-Physics\n\n1, 433 500 767 745 6, 805 8, 415\n\n7 3\n10 8\n15 5\n\nA.1 SETUP FOR NODE CLASSIFICATION\n\nDataset. We use 2 Planetoid graphs (Cora and PubMed), and 4 widely used datasets (Shchur et al., 2018) (Amazon-Computers, Amazon-Photo, Coauthor-CS, and Coauthor-Physics) for experiments. For Cora and PubMed, we follow the common semi-supervised practice (Kipf & Welling, 2016a) to generate train/val/test data splits without any modifications. For Amazon-Computers, AmazonPhoto, Coauthor-CS and Coauthor-Physics, since there are no data splits available, so similar to BGRL (Thakoor et al., 2022) and GBT (Bielak et al., 2022), we generate 20 random train/val/test splits (10%/10%/80%).\n\nCompetitors. To have a rigorous and comprehensive comparison with state-of-the-art methods, we compare PerturbGCL with 2 classical unsupervised models: DeepWalk (Perozzi et al., 2014) and GAE (Kipf & Welling, 2016b), 9 standard self-supervised models, DGI (Veliˇckovi ́c et al., 2018), GRACE (Zhu et al., 2020), MVGRL (Hassani & Khasahmadi, 2020), GCA (Zhu et al., 2021b), BGRL (Thakoor et al., 2022), GBT (Bielak et al., 2022), InforGCL (Xu et al., 2021), CCA-SSG (Zhang et al., 2021), AFGRL (Lee et al., 2021), and one model perturbation method – SimGRACE (Xia et al., 2022). We also compare with supervised learning models including GCN (Kipf & Welling, 2016a) and GAT (Veliˇckovi ́c et al., 2017). The results of baselines are quoted from (Zhang et al., 2021; Xu et al., 2021; Xia et al., 2022) if not specified.\n\nEvaluation protocol. We follow the popular linear evaluation scheme to evaluate the performance of unsupervised models. Specifically, we first pre-train the model on the given graph without using ground-truth labels. Then, we freeze the parameters of the encoder and use it to generate node representations. After that, the generated node representations would be fed into a linear classification,\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Dataset statistics of graph-level benchmarks.\n\nNCI1 PROTEINS DD MUTAG COLLAB IMDB-BINARY\n\n| G | Avg.Nodes Avg.Edges #Label 4, 110 1, 113 1, 178 188 5, 000 1, 000 REDDIT-BINARY 2, 000 REDDIT-MULTI-5K 4, 999 12, 725\n\n32.30 72.82 715.66 19.79 2, 457.78 96.53 497.75 594.87 234.64\n\n29.87 39.06 284.32 17.93 74.49 19.77 429.63 508.52 113.79\n\n2 2\n2 2\n3 2\n2 5\n2\n\nGITHUB\n\nTable 6: Hyperparameters of PerturbGCL on node classification. We use GCN (Kipf & Welling, 2016a) as the backbone encoder.\n\nCora\n\nPubMed Computer\n\nPhoto\n\nCS\n\nPhy\n\n# layers hidden dim learning rate propagation step pruning ratio\n\n2 512 0.005 3\n0.5\n\n2 512 0.0005 3\n0.7\n\n2 512 0.001 3\n0.7\n\n2 512 0.00001 2\n0.9\n\n2 512 0.00001 3\n0.9\n\n2 512 0.00001 3\n0.9\n\ni.e., a simple logistic regression model, to make the prediction for each node. It is worth noting that only nodes in the training set are used as supervision when training the classifier, and we report the accuracy results on testing nodes.\n\nWe implement PerturbGCL with PyTorch and use Adam optimizer to train the model. The graph encoder fw is specified as a standard two-layer GCN model for all the datasets. We have two hyperparameters (pruning ratio s and random propagation step K) to tune. For each dataset, we search K ∈ [1, 2, 3] and s ∈ [0.5, 0.7, 0.9]. To avoid randomness, we report the mean accuracy with a standard deviation through 10 random initialization. The detailed hyperparameter settings are summarized in Table 6.\n\nA.2 SETUP FOR UNSUPERVISED GRAPH CLASSIFICATION\n\nDataset. For the unsupervised graph classification task, we adopt 8 benchmark datasets (NCI1, PROTEINS, DD, MUTAG, COLLAB, RDT-B, RDT-MSK, and IMDB-B) for experiments, following (You et al., 2020). There are 20 epochs pre-training under the naive-strategy. After the\n\nCompetitors. We compare with the kernel-based methods like graphlet kernel (GL) (Shervashidze et al., 2009), Weisfeiler-Lehman sub-tree kernel (WL) (Shervashidze et al., 2011), and deep graph kernel (DGK) (Yanardag & Vishwanathan, 2015), and other unsupervised graph representation meodels like node2vec (Grover & Leskovec, 2016), sub2vec (Adhikari et al., 2018), graph2vec (Narayanan et al., 2017), as well as the state-of-the-art GCL methods like MVGRL (Hassani & Khasahmadi, 2020), InforGraph (Sun et al., 2019), GraphCL (You et al., 2020), JOAO (You et al., 2021), and SimGRACE (Xia et al., 2022).\n\nEvaluation protocol. Following GraphCL (You et al., 2020), contrastively train the representation model using unlabeled graph data, and then fix the representation model and train a downstream classifier using labeled data. Specifically, we adopt SVM as the classifier and perform 10-fold cross validation. For each fold, we employ 90% of the data as the labeled data for training and the remaining 10% as the labeled testing data. To avoid randomness, we repeatedly run experiments for 5 times and report the averaged results.\n\nFollowing GraphCL (You et al., 2020), we use GIN (Xu et al., 2018) as the GNN backbone, and also search the best K and s from {1, 2, 3} and {0.5, 0.7, 0.9}, respectively. Table 7 reports the parameter configurations for all datasets.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Hyperparameters of PerturbGCL on unsupervised graph classification. We use GIN (Xu et al., 2018) as the backbone encoder.\n\nNCI1\n\nPROTEINS\n\nDD\n\nMUTAG COLLAB RDT-B RDT-M5K IMDB-B\n\n# layers hidden dim learning rate propagation step pruning ratio\n\n3 32 0.0001 2\n0.9\n\n3 32 0.0001 2\n0.7\n\n3 32 0.0001 3\n0.9\n\n3 32 0.0001 2\n0.7\n\n3 32 0.0001 3\n0.9\n\n3 32 0.0001 2\n0.9\n\n3 32 0.0001 2\n0.5\n\n3 32 0.0001 2\n0.5\n\nTable 8: Hyperparameters of PerturbGCL on semi-supervised graph classification. We use ResGCN (Chen et al., 2019) as the backbone encoder.\n\nNCI1\n\nPROTEINS\n\nDD\n\nMUTAG COLLAB RDT-B RDT-M5K GITHUB\n\n# layers hidden dim learning rate propagation step pruning ratio\n\n5 128 0.0001 2\n0.5\n\n5 128 0.0001 2\n0.7\n\n5 128 0.0001 3\n0.9\n\n3 128 0.0001 2\n0.7\n\n5 128 0.0001 3\n0.9\n\n5 128 0.0001 2\n0.7\n\n3 128 0.0001 2\n0.7\n\n3 128 0.0001 2\n0.7\n\nA.3 SETUP FOR SEMI-SUPERVISED GRAPH CLASSIFICATION\n\nDataset. We perform semi-supervised graph classification task 7 popular benchmark datasets (NCI1, PROTEINS, DD, COLLAB, RDT-B, RDT-M5K, and GITHUB) from TUDataset (Morris et al., 2020). There are 100 epochs pre-training under the default setting.\n\nCompetitors. We compare with unsupervised graph representation meodels: GAE (Kipf & Welling, 2016b), Infomax (DGI) (Veliˇckovi ́c et al., 2018), and ContextPred (Hu et al., 2019), and other stateof-the-art GCL methods like InforGraph (Sun et al., 2019), GraphCL (You et al., 2020), JOAO (You et al., 2021), and SimGRACE (Xia et al., 2022).\n\nEvaluation protocol. We employ a 10-fold cross validation on each dataset. For each fold, we use 80% of the data as the unlabeled data, 10% as labeled training data, and 10% as labeled testing data. For the augmentation only (Augmentations) experiments, we only perform 30 epochs of supervised training with augmentations using labeled data.\n\nFollowing GraphCL (You et al., 2020), we use ResGCN (Chen et al., 2019) as the GNN backbone, and also search the best K and s from {1, 2, 3} and {0.5, 0.7, 0.9}, respectively. Table 8 reports the parameter configurations for all datasets.\n\nAlgorithm 1 PerturbGCL on node level task\n\n1: Input: Original graph G = (V, E, X), GNN encoder fw(·) with weight W , projection head\n\nh(·), the maximum propagation step K, and pruning ratio s 2: Initialize the encoder fw(·) and set mask indicator to ones. 3: for iterate 1, 2, ... times until convergence do 4: 5: 6: 7: 8:\n\nSample the random propagation steps k′, k′′ from the uniform distribution U (1, K) Conduct weight pruning to update the mask indicator mw Compute the target representation hv according to 1 by performing k′ times of g() Get zv = h(hv) according to 2 Compute the perturbed representation h+ using masked weight W ⊙ mw Get z+ v = h(hv) define LCL = 1\n\nexp(sim(zv,z+\n\nv )/τ )\n\n(cid:80)\n\nv∈V − log\n\nexp(sim(zv,z+\n\nv )/τ )+(cid:80)\n\nu∈V,u̸=v exp(sim(zv,z−\n\nu )/τ )\n\n|V |\n\n9:\n\n10:\n\nv according to 1 by performing k′′ times of g() and\n\naccording to 3\n\nOptimize fw(·), h(·) to minimize LCL\n\n11: 12: end for 13: return the pre-trained GNN encoder fw(·)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 PerturbGCL on graph level task\n\n1: Input: Original graph G = (V, E, X), GNN encoder fw(·) with weight W , projection head\n\nfor sampling batches Gn from G, where n = 1, 2, ..., N do\n\nh(·), the maximum propagation step K, and pruning ratio s 2: Initialize the encoder fw(·) and set mask indicator to ones. 3: for iterate 1, 2, ... times until convergence do 4: 5: 6: 7: 8: 9:\n\nSample the random propagation steps k′, k′′ from the uniform distribution U (1, K) Conduct weight pruning to update the mask indicator mw Compute the target representation hv according to 1 by performing k′ times of g() Get zG = READOUT(h(hv)v∈Gn ) according to 2 Compute the perturbed representation h+ and using masked weight W ⊙ mw Get z+ Gn\n\n= READOUT(h(h+\n\nv )v∈Gn )\n\nv according to 1 by performing k′′ times of g()\n\ndefine LCL = 1\n\n|G|\n\n(cid:80)\n\nGn∈G − log\n\nexp(sim(zGn ,z+\n\nGn\n\nexp(sim(zGn ,z+ )/τ )+(cid:80)\n\nGn\n\n)/τ )\n\nu∈G,u̸=v exp(sim(zGn ,z−\n\nu )/τ )\n\naccord-\n\n10:\n\n11:\n\ning to 3 Optimize fw(·), p(·) to minimize LCL\n\n12: 13: 14: end for 15: return The pre-trained GNN encoder f (·)\n\nend for\n\nB DETAILS FOR TOY EXAMPLE\n\nTo verify the limitation of SimGRACE (Xia et al., 2022) on handling perturbation created by data augmentation. We select three popular data augmentation based baselines: GRACE (Zhu et al., 2020), BGRL (Thakoor et al., 2022), and CCA-SSG (Zhang et al., 2021). To measure the qualify of the representation models on learning representations for input data, we adopt the widely used alignment and uniformity metrics (Wang & Isola, 2020) for quantitative analysis. According to (Wang & Isola, 2020), both metrics are the smaller the better.\n\nEvaluation setting. For all methods, we first pre-train them according to their own configurations on PubMed, Amazon-Photo, and Coauthor-CS datasets. Then, we use data augmentation strategies to construct two perturbed views. Specifically, we following this detailed empirical study (Zhu et al., 2021a) and adopt edge perturbation and attribute masking as the default perturbation function on the input graph. To have a fair comparison, we fix the random seed and generated two shared perturbed graphs, and then feed the two views into BGRL, GRACE, CCA-SSG, and SimGRACE to obtain node representations for all nodes in the graph. After that, we use the obtained node representations of two views to compute the alignment and uniformity according to Eq. 5. WE repeat the process for 10 times and report the averaged results in Figure 1.\n\nTable 9: Test accuracy on benchmark datasets in TUdatasets in terms of semi-supervised graph classification.\n\nDataset\n\nNCI1\n\nPROTEINS\n\nDD\n\nCOLLAB\n\nRDT-B\n\nRDT-M5K\n\nGITHUB\n\nA.R. ↓\n\nNo pre-train Augmentations\n\nGAE InfoGraph ContextPred Infomax GraphCL JOAO JOAOv2 SimGRACE\n\nPerturbGCL\n\n73.72 ± 0.24 73.59 ± 0.32\n\n74.36 ± 0.24 74.86 ± 0.26 73.00 ± 0.30 74.86 ± 0.26 74.63 ± 0.25 74.48 ± 0.25 74.86 ± 0.39 74.60 ± 0.41\n\n70.40 ± 1.54 70.29 ± 0.64\n\n73.56 ± 0.41 74.30 ± 0.81\n\n73.71 ± 0.27 74.19 ± 0.13\n\n86.63 ± 0.27 87.74 ± 0.39\n\n70.51 ± 0.17 72.27 ± 0.40 70.23 ± 0.63 72.27 ± 0.40 74.17 ± 0.34 72.13 ± 0.92 73.31 ± 0.48 74.03 ± 0.51\n\n74.54 ± 0.68 75.78 ± 0.34 74.66 ± 0.51 75.78 ± 0.34 76.17 ± 1.37 75.69 ± 0.67 75.81 ± 0.73 76.48 ± 0.52\n\n75.09 ± 0.19 73.76 ± 0.29 73.60 ± 0.37 73.76 ± 0.29 74.23 ± 0.21 75.30 ± 0.32 75.53 ± 0.18 74.74 ± 0.28\n\n87.69 ± 0.40 88.66 ± 0.95 84.76 ± 0.52 88.66 ± 0.95 89.11 ± 0.19 88.14 ± 0.25 88.79 ± 0.65 88.86 ± 0.62\n\n51.33 ± 0.44 52.01 ± 0.20\n\n53.58 ± 0.13 53.61 ± 0.31 51.23 ± 0.84 53.61 ± 0.31 52.55 ± 0.45 52.83 ± 0.54 52.71 ± 0.28 53.97 ± 0.64\n\n60.87 ± 0.17 60.91 ± 0.32\n\n63.89 ± 0.52 65.21 ± 0.88 −\n65.21 ± 0.88 65.81 ± 0.79 65.00 ± 0.30 66.60 ± 0.60 65.33 ± 0.35\n\n75.23 ± 0.52\n\n74.11 ± 0.42\n\n76.65 ± 0.57\n\n74.50 ± 0.41\n\n88.69 ± 0.63\n\n55.39 ± 0.12\n\n68.40 ± 0.10\n\n9.86 9.00\n\n7.14 4.71 10.50 5.71 3.86 6.14 3.57 3.29\n\n2.14\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nC MORE CONTRASTIVE LOSS FUNCTIONS\n\nAlthough InfoNCE (You et al., 2020) (illustrated in Eq. 3) is the widely used contrastive objective in learning GCL models, some other training objectives have been proposed recently, such as Barlow Twins (Bielak et al., 2021), Bootstrap (Thakoor et al., 2022). Different from InfoNCE, the other two contrastive objectives are negative-sample free, so they can avoid the efforts to identify informative negative samples during the training. Specifically, the core idea of Bootstrap function is to maximize the difference between the positive pairs, defined as:\n\nL = −\n\n2 N\n\nN −1 (cid:88)\n\nv∈V\n\nzvh+\n\nv\n\n∥zv∥ (cid:13)\n\n(cid:13)h+\n\nv\n\n.\n\n(cid:13) (cid:13)\n\n(7)\n\nv is the hidden representation encoded by GNN encoder, and zv = h(h+\n\nHere h+ v ), where h(·) is the prediction head. In our cases, since we take the original graph as input, so we do not have symmetric define of the loss function as done in (Thakoor et al., 2022). This might be the reason why our model PerturbGCL performs not good using this objective.\n\nBarlow Twins (Bielak et al., 2021) is a recent endeavor to reduce the usage of negative samples. This objective is originally proposed in image domain by (Zbontar et al., 2021). The general idea of Barlow Twins is to minimize the redundancy in the hidden dimension. Specifically, given the hidden representation of two views (Z and Z+), it first compute the empirical cross-correlation matrix C ∈ RD×D as below:\n\nCi,j =\n\n(cid:80)\n\nn Zn,iZ+ n (Zn,i)2(cid:113)(cid:80)\n\nn\n\nn,j\n\n(cid:113)(cid:80)\n\n,\n\n(cid:0)Z+\n\nn,j\n\n(cid:1)2\n\n(8)\n\nwhere n is the the batch indexes and i, j are the indexes of embeddings. The cross-correlation matrix C is optimized to be equal to the identity matrix. To be specific, it is composed of two parts: 1) the invariance term and 2) the redundancy reduction term. The first one forces the on diagonal elements Ci,i to be equal to one, hence making the embeddings invariant to the applied augmentations. The second term optimizes the off-diagonal elements Ci,j to be equal to zero – this results in decorrelated components of the embedding vectors. Formally, the loss function LBT is computed by:\n\nLBT =\n\n(cid:88)\n\ni\n\n(1 − Ci,i)2 + λ\n\n(cid:88)\n\n(cid:88)\n\nC2\n\ni,j.\n\ni\n\nj̸=i\n\n(9)\n\nThe λ > 0 parameter defines the trade-off between the invariance and redundancy reduction terms. In our experiments, we set λ = 1 D following (Bielak et al., 2021).\n\nFigure 6: The alignment and uniformity plot for BGRL (Thakoor et al., 2022), SimGRACE (Xia et al., 2022), GRACE (Zhu et al., 2020), CCA-SSG (Zhang et al., 2021), and our PerturbGCL on the same perturbed graphs generated by data augmentation. Black circles ( ) indicate the baselines. ) represent the performance of SimGRACE. Red starts (⋆) are the results of Orange circles ( PerturGCL.\n\n(cid:32)\n\n(cid:32)\n\nD EFFICIENCY ANALYSIS\n\nTo evaluate the efficiency of the proposed PerturbGCL, we compare it with two strong GCL baselines: BGRL (Thakoor et al., 2022) on node classification data and GraphCL (You et al., 2020) on graph classification data. It is worth to note that since BGRL and GraphCL require to search for the\n\n18\n\n2.42.22.01.81.61.4uniform0.180.200.220.240.260.280.30align (%)PubMedCCA-SSGGRACEBGRLSimGRACEPerturbGCL2.52.01.5uniform0.340.360.380.400.420.44align (%)Amazon-PhotoCCA-SSGGRACEBGRLSimGRACEPerturbGCL2.01.81.61.41.2uniform0.220.240.260.280.300.32align (%)Coauthor-CSCCA-SSGGRACEBGRLSimGRACEPerturbGCLUnder review as a conference paper at ICLR 2023\n\noptimal augmentation strategies via trial-and-error, which is super expensive in practice. To simplify the comparison, we fix their optimal augmentations and only record the running time on the optimal augmentations. For graph-level datastes, GraphCL and PerturbGCL use 3 GIN layers with hidden dimension 32 as the backbone encoder. The propagation step and pruning ratio of PerturbGCL are set as K = 2 and s = 0.7. For node-level datasets, BGRL and PerturbGCL use 2 GCN layers with hidden dimension 512 as the backbone encoder. The propagation step and pruning ratio of PerturbGCL are set as K = 3 and s = 0.9. We conduct experiments on a server with AMD EPYC 7282 16-Core processors, 252 GB memory, and one GeForce RTX 2080 Ti GPUs (24GB). To avoid randomness, the reported results in Table 3 are the averaged performance over 100 training epochs.\n\nExcept the improvement in Table 3, another thing we want to mention is that the practical speedup of PerturbGCL should be significantly higher than the results shown in Table 3. This is mainly because the data augmentation based GCL methods require a lot of efforts to search for the best augmentation strategies, such as the best augmentation types and their corresponding perturbation ratios.\n\nE MORE RESULTS\n\nFigure 7: Empirical training curves of PerturbGCL on graph benchmarks with different pruning ratios s.\n\nFigure 8: Visualization of second layer weight distribution during training process(from left to right: initial weights, PerturbGCL w/o. weightPrune, and PerturbGCL) on Coauthor-CS. The x-axis indicates weight values and y-axis is the corresponding count.\n\nFigure 9: Ablation study of PerturbGCL on node benchmarks.\n\n19\n\n020406080100Epochs3.003.253.503.754.004.254.504.75Trining LossNCI1s=0.5s=0.7s=0.920406080100Epochs3.03.54.04.55.0Trining LossRDT-M5Ks=0.5s=0.7s=0.9020406080100Epochs3.003.253.503.754.004.254.504.75Trining LossGITHUBs=0.5s=0.7s=0.9PROTEINSCOLLABIMDB-B50556065707580Accuracy (%)Noisew/o MPw/o WPPerturbGCLUnder review as a conference paper at ICLR 2023\n\nFigure 10: Left: Hyperparameter Analysis on Coauthor-CS. Middle: PerturbGCL with data augmentation. Right: The alignment and uniformity results of PerturbGCL.\n\n20\n\ns0.10.30.50.70.9K123450.900.910.920.930.940.95PubMedCSPhyCOLLABPROTEINSIMDB-BDataset020406080100Accuracy (%)PerturbGCLPerturbGCL+aug2.01.81.61.41.2uniform0.220.240.260.280.300.32align (%)Coauthor-CSCCA-SSGGRACEBGRLSimGRACEPerturbGCL",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors propose a principled framework, namely PerturbGCL, for augmentation-free graph contrastive learning. Considering that GNN can be divided into the two parts of message propagation and feature transformation operations, the authors develop two tailored perturbation strategies, namely randMP and weightPrune, to effectively disturb GNN. Through extensive experiments on the node classification task and graph classification task, the authors show that PerturbGCL can achieve competitive results against the SOTA graph contrastive learning methods.\n\n# Strength And Weaknesses\n\nStrength\n\nS1: The proposed method is simple and effective.\n\nS2: The authors verify PerturbGCL on two downstream tasks.\n\nS3: The authors provide the code of PerturbGCL.\n\nWeakness\n\nW1: The paper is not well written. There are lots of syntax errors and unclear representation in this paper.\n1. In Section 2.1 (Page 2), where the i-th row of X denote -> where the i-th row of X denotes; in Page 4, Figure 2 provide the overview our framework.-> Figure 2 provides the overview of our framework; in the title of Figure 4, The gap between the blue and orange lines indicate -> The gap between the blue and orange lines indicates; the vertical spacing between Figure 4 and Table 1 is too small; in Table 1, compute -> computers; In Table 5, |G| -> |D| (D denotes the graph set)\n2. In Table 1, the exact digits of numbers are not the same, e.g., on Cora, the result of PerturbGCL is 83.3 ± 0.5; while on PubMed,  the result of PerturbGCL is 82.10 ± 0.37.\n\nW2: The novelty is limited.\nThe model perturbation or augmentation-free method has been explored in graph contrastive learning by prior work. RandMP and weightPrune are more like two tricks rather than a novel framework.\n\nW3: Lack of comparison with important baselines.\n1. For the node classification task, considering that the design of PerturbGCL is based on the framework of GraphCL, why do not compare PerturbGCL with GraphCL?\n2. Some other graph contrastive learning methods without using data augmentation, such as AFGRL [1], are not compared with PerturbGCL.\n\nOthers.\n1. The abstract is too long. I think that reducing the content of the abstract appropriately is helpful to highlight the core of this work.\n2. In Table 3, the authors only compare PerturbGCL with BGRL or GraphCL on time costs. Why not compare with other graph contrastive learning methods, such as SimGRACE? Since SimGRACE does not use data augmentation, I guess that SimGRACE may be more efficient than BGRL and GraphCL.\n3. In Table 4, the split ratios of Amazon-Computers, Amazon-Photo, Coauthor-CS, and Coauthor-Physics are not given. Also, please check whether the split ratio of Cora and PubMed is right (note that 85+5+15=105, not 100).\n4. In Algorithm 1, the pruning rato s, as one of the inputs, is not used in this algorithm; g() is not given a specific definition or statement.\n5. In Algorithm 2, the pruning rato s, as one of the inputs, is not used in this algorithm; the input data should be a graph set D, not a graph G; in Line 12, p(\\cdot)->h(\\cdot).\n6. The results in Figure 6 are important and I suggest moving Figure 6 into the main body of the paper.\n\n[1] Lee N, Lee J, Park C. Augmentation-free self-supervised learning on graphs[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(7): 7372-7380.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI think the representation quality and originality of this paper are not good. Also, the novelty of this work is limited.\n\n# Summary Of The Review\n\nPlease see my comment in 'Strength And Weaknesses'.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSYNERGISTIC NEUROMORPHIC FEDERATED LEARNING WITH ANN-SNN CONVERSION FOR PRIVACY PROTECTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFederated Learning (FL) has been widely researched for the growing public data privacy issues, where only model parameters, instead of private data, are communicated. However, recent studies debunk the privacy protection of FL, showing that private data can be leaked from the communicated gradients or parameter updates. In this paper, we propose a framework called Synergistic Neuromorphic Federated Learning (SNFL) that enhances privacy during FL. Before uploading the updates of the client model, SNFL first converts clients’ Artificial Neural Networks (ANNs) to Spiking Neural Networks (SNNs) via calibration algorithms. In a way that not only loses almost no accuracy but also encrypts the client model’s parameters, SNFL manages to obtain a more performant model with high privacy. After the aggregation of various SNNs parameters, the server distributes the parameters back to the clients. This design offers a smooth convergence to continue the model training under the ANN architecture. The proposed framework is demonstrated to be private, introducing a lightweight overhead as well as yielding prominent performance boosts. Extensive experiments with different kinds of datasets have demonstrated the efficacy and the practicability of our method. In most of our experimental IID and not extreme Non-IID scenarios, the SNFL technique has significantly enhanced the model performance. For instance, SNFL improves the accuracy of FedAvg on Tiny-ImageNet by 13.79%. Besides, the original image cannot be reconstructed after 280 iterations of attacks with the SNFL method, whereas it can be reconstructed after just 70 iterations with FedAvg.\n\n1\n\nINTRODUCTION\n\nRecent advancements in machine learning, particularly deep learning, rely heavily on large data sets to obtain decent inference performance. Due to the growing demand for data, it is now necessary to feed models with information from multiple entities. However, this transfer, exchange, and trade of data among entities may violate the General Data Protection Regulation (GDPR) and get punished by the Act (Wachter, 2018), posing an unprecedented challenge to the field of machine learning. Federated learning (McMahan et al., 2017) then emerges and flourishes as a privacy-preserving approach by training a shared model collaboratively while keeping data locally. Despite that the data are stored locally, clients that join the federated learning need to transmit their local gradients to the server to update the shared model. Recent studies Zhu & Han (2020); Zhao et al. (2020); Huang et al. (2021) have revealed that sensitive local data could be leaked from these transmitted local gradients via model inversion attack Zhu & Han (2020). To defend against such kind of attack and prevent privacy leakage, defense strategies including differential privacy (Geyer et al., 2017), secure multi-party computation (Byrd & Polychroniadou, 2020), and MixUp (Zhang et al., 2017) have been developed. In exchange for privacy, the cost is then either severe computational overheads (Hardy et al., 2017) or unavoidable accuracy losses (Kim et al., 2021).\n\nWhat’s the intrinsic source of privacy in these defense strategies? If we consider this question from an information theory perspective, it is indeed the asymmetry of entropy in the encryption and decryption steps for clients and servers when partial encryption information is kept locally only. From this standpoint, as long as an encryption method is capable of inevitability between clients and servers while still allowing for effective aggregation, it would be feasible to improve the privacy\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Right: Workflow of Synergistic Neuromorphic Federated Learning with ANN-SNN Conversion (SNFL). Left: Each client communicates parameters ∇WA generated by the model trained on private local data. The attacker updates randomized dummy input and label to minimize the gradient distance ||∇WA −∇W ′||. When the optimization is complete, the attacker can obtain the training set from the client. However, in SNFL, the client’s model has been converted and calibrated to SNN before communication.\n\nfor federated learning. Recent progression in neuromorphic computing, especially the conversion from traditional artificial neural networks (ANNs) to spiking neural networks (SNNs) (Deng & Gu, 2020), provides a pair of source ANN and target SNN that both achieve high accuracy, with the source ANN not recoverable from the resulted SNN (Li et al., 2021b). This property fits naturally with the demand for privacy protection in federated learning. Indeed, if we train ANNs on clients and only send the converted SNNs with partial parameters to the server for aggregation, we can then expect to obtain a feasible privacy-protected FL algorithm with an effective parameter-sharing paradigm. In addition, such an ANN-SNN conversion is lightweight and performance-preserving (or even performance-improving) by careful design. Fig. 1 illustrates the pipeline of our proposed method.\n\nBesides the natural feasibility of SNNs (Esser et al., 2016; Kim et al., 2019), this synergistic framework also brings two additional benefits that are special for federated learning. First, in contrast to existing noise injection methods (e.g., differential privacy Geyer et al. (2017)), our ANN-SNN conversion process is optimized to improve performance by fine-tuning SNN’s weights rather than trading off performance drop versus noise level. As a result, our method is able to achieve even better performance against standard federated learning. Second, the SNN emits discrete spikes and is not differentiable, thus the induced synergistic FL could be more robust to small perturbations and adversarial attacks like white-box attacks (Liang et al., 2021). Our contributions are summarized as follows:\n\n• Innovation/Privacy: We design a federated learning framework where the server and clients run two different models in a privacy-preserving manner as a new solution. To the best of our knowledge, our work is one of the first to train different types of neural network models on server and clients.\n\n• Accuracy: Compared to the conventional approach, extensive experiments validate SNFL can\n\ndeliver similar or superior accuracy relative to other common methods.\n\n• Effectiveness: Based on the SNFL framework, we analyze the backdoor attack and develop a\n\nmethod to simply detect it through abnormal SNN thresholds.\n\n2 RELATED WORK\n\n2.1 FEDERATED LEARNING (FL)\n\nIn federated learning, each client computes a model update, i.e. gradient, on its local data. While sharing gradients was assumed to leak little information about the client’s private data, recent papers (Zhu & Han, 2020; Zhao et al., 2020; Huang et al., 2021) devised \"gradient inversion attack\" in which an attacker listening to one client’s communications with the server can begin to reconstruct the client’s private data. To defend against this, methods such as gradient clipping (Sun et al., 2019),\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nperturbing gradients (Zhu & Han, 2020), and robust aggregation (Blanchard et al., 2017; Goryczka & Xiong, 2015; Yin et al., 2018) are commonly used.\n\n2.2 SPIKING NEURAL NETWORK (SNN)\n\nConventionally, there are two distinct routes to obtain a deep SNN (Deng et al., 2020):(1) direct training SNN from scratch, and (2) converting a pretrained ANN to SNN. In this work, we mainly focus on the conversion-based method. ANN-SNN conversion directly reuses the features learned in ANN to obtain a performant SNN. However, it requires a trade-off between inference latency and task performance. Data-based normalization (Diehl et al., 2016) and threshold balancing (Sengupta et al., 2018) are the basic methods of ANN-to-SNN conversion. Then Rueckauer et al. (2016); Han et al. (2020) propose the soft mechanism to reduce information loss by membrane potential reset. Recently, Deng & Gu (2020) analyze the conversion error and propose a shift method to reduce it by half. Li et al. (2020) propose a light pipeline and an advanced pipeline, which apply layer-wise calibration algorithms to modify the network parameters to diminish the conversion error, significantly reducing the required simulation length. We adopt the layer-wise calibration algorithm for high-performance and low-latency SNN in the SNFL framework.\n\n3 PRELIMINARIES\n\nIn this section, we briefly introduce the concept and the baseline method for Federated Learning (FL). We also point out the privacy issue in FL, which can lead to the leakage of user data.\n\nFederated Learning (FL). FL enables mobile devices to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud. Formally, assuming we have K clients, the optimization objective in FL is to solve the following empirical risk minimization problem:\n\nmin θ\n\nL(θ) =\n\n1 K\n\nK (cid:88)\n\ni=1\n\nli(θ), where li(θ) = l(xi,yi)∼Di(xi, yi; θ).\n\n(1)\n\nHere, θ is the model parameters vector and li(θ) denotes the loss function of θ evaluated on the i-th client’s dataset Di. (xi, yi) is the input-label pair on Di. The goal of this objective is to achieve minimum average loss on each client. We assume {Di}K i=1 are randomly sampled from Dtrain, where both IID sampling and Non-IID sampling are considered in this work.\n\nTo ensure the minimization of average loss on all clients while not sharing the local input data, the clients will upload their model parameters to the server periodically. A communication round is used for client upload, server aggregation, and server distribution. Here we introduce the FedAvg (McMahan et al., 2017) communication for client-server update.\n\n• In the r-th communication round, each client i uploads it’s local model parameters change ∆θ(r) the server. The ervser aggregates the local model updates from all participating clients, given by\n\ni\n\nto\n\nθ(r) = θ(r−1) +\n\n1 K\n\nK (cid:88)\n\ni=1\n\n∆θ(r)\n\ni\n\n,\n\n(2)\n\nafter which the server distributes the aggregated parameters (θ(r)) to clients.\n\n• Upon receiving the aggregated parameters from server, the clients start their own local learning i=1, which creates new model parameters update for the next\n\nusing the private datasets, i.e., {Di}K round ∆θ(r+1) , given by\n\ni\n\n∆θ(r+1)\n\ni\n\n= ηi∇θ(r) l(xi,yi)∼Di(xi, yi, θ(r)),\n\n(3)\n\nwhere ηi is the learning rate in local learning. The clients usually update multiple iterations with gradient descent (we only show one update in above equation).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nGradient-Inversion Attack. Proposed in Zhu & Han (2020), the Deep Leakage from Gradients (DLG) attack can utilize the gradient information to reverse the private local data and the label information, that is, given the model parameters update ∆θi one can obtain the similar (xi, yi) pairs. This is done by mimicking the real gradient (also the parameters update, see Eq. (3)). Formally, DLG first randomly initialize a dummy input and a dummy label (x′ i), and optimize them by minimizing the discrepancy between real gradients and current gradients, given by\n\ni, y′\n\n||∇θl(x′\n\ni, y′\n\ni) − ∇θl(xi, yi)||2\n\nF + αR(x′),\n\nmin i,y′ x′\n\ni\n\n(4)\n\nwhere ∇θl(xi, yi) is the real gradient uploaded to the server. R(x′) is an image prior loss function with α as the coefficient. The current gradient ∇θl(x′ i, therefore, can be made as close as possible to the real gradient and generate corresponding input data and label.\n\nSo far, some defensive methods have been proposed. For example, Gradient Pruning (Zhu & Han, 2020), Gradient Noise (Zhu & Han, 2020), and Mixup (Zhang et al., 2017) aim to provide less or distributed information in gradients. However, these methods sacrifice task performance for better privacy. In this paper, we seek a method for preserving privacy during federated learning while not jeopardizing its accuracy.\n\n4 METHODOLOGY\n\nIn this section, we introduce our method—combining both Artificial Neural Networks (ANNs) and Spiking Neural Networks (SNNs) for privacy-preserving federated learning.\n\n4.1 SPIKING NEURAL NETWORKS\n\nCompared to artificial neurons i.e., ReLU: max(0, x), spiking neurons are biologically-inspired, where each neuron maintains a variable dubbed membrane potential v. Here, we describe the dynamics using the iterative expression of the Integrate-and-Fire (IF) neuron model (Liu & Wang, 2001), which is favorable for ANN-SNN conversion regime (Rueckauer et al., 2016; Han et al., 2020). Formally, at time step t, the IF neuron receives the pre-synaptic input, and then charges the membrane potential, given by\n\nv(t) = v(t) + I(t), s(t) =\n\n(cid:26)Vth 0\n\nif v(t + 1) ≥ Vth otherwise\n\n, v(t + 1) = v(t) − s(t),\n\n(5)\n\nwhere I(t) is the pre-synaptic input calculated by the weights W and the spike s from last layer. As long as the membrane potential exceeds the firing threshold Vth, the neuron will elicit a spike s, otherwise, it will stay silent. For fired neurons, the membrane potential will be reset by subtraction, i.e., the third term in Eq. (5). Noting that the threshold V (l) th differs between layers, the server can further apply weight normalization (Diehl et al., 2016) to convert the output {0, V (l) th } to a binary spike {0, 1}.\n\n4.2 SYNERGISTIC NEUROMORPHIC FEDERATED LEARNING\n\nIn the conversion from a source ANN to a target SNN, the discrepancy of output activation between the two networks will accumulate layer-by-layer, resulting in significantly different output at the final output layer. To address this issue, a layer-wise parameter calibration technique (Li et al., 2021b) is proposed to adjust the SNN parameters so that its activation frequency gets close to the activation in the source ANN. Mathematically, denote the average spike rate over time in SNN is ̄s, we can write the conversion error as e = a − ̄s, where a is the activation in source ANN. For each channel c, the average activation is then calculated as μc(a) = 1 j=1 xc,i,j, where w, h are the width and height of the feature. The bias calibration (BC) algorithm computes the spatial mean of the error term, given by\n\n(cid:80)w\n\n(cid:80)h\n\ni=1\n\nwh\n\nμc(e) = μc(a) − μc( ̄s). (6) Afterwards, μc(e) can be added to the c-th channel of bias term b in SNNs. When calculating μc(e), we need to estimate μc(a) and μc( ̄s) based on a small calibration dataset (e.g. 128 images), which is not accessible on the server set. Given that the small calibration dataset is a subset of the client’s\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nprivate dataset and differs between clients, the attacker will be unable to recover every client’s μc(e). Our framework can take advantage of this algorithm to encrypt the gradient information, therefore improving privacy in FL. The overall pipeline (Algo. 1) in our SNFL is described as follows:\n\nClient Encryption. Before uploading the parameter updates to the server, the clients convert the ANN models into SNN models. The conversion first replaces all the ReLU neurons into IF neurons. Due to there is no corresponding module in SNN for BN layer, Rueckauer et al. (2017) propose γ\nto absorb the BN parameters to the weight and bias, which can be represented as WS ← WA σ , bS ← β + (bA − μ) γ σ , where WA, bA are the weight and bias of ANN model, WS, bS are the weight and bias of SNN model, μ, σ are the running mean and standard deviation and γ, β are the transformation parameters of the Batch Normalization (BN) layer. Then, all clients run bias calibration algorithm to update the bias parameters for SNN. Note that each local BC process will infer its own local model using its private dataset to record some activation, i.e., ai and ̄si for the i-th client’s ANN and SNN. The bias parameters can then be calibrated as bi S + μ(ai) − μ( ̄si). S\nBecause the curious server couldn’t recover μ(ai) − μ( ̄si), and that couldn’t recover bS from b′ S, it couldn’t possibly recover bA. The BC algorithm has two advantages: (1) the uploaded SNN has higher accuracy since its parameters are calibrated, and (2) the uploaded parameters are not the same as the original ANNs, which prevents leakage from gradients. In the sharing step, clients send parameters to the server, which include the weight (W i ′), threshold (V i) of SNN models as well as BN layer (γi, σi, βi, μi) of ANN models.\n\nS), bias (bi\n\n′ ← bi\n\nS\n\nS , bS\n\nServer Aggregation. In the global communication round, the server will receive the encrypted parameters from clients. Then, on the server side, we apply FedAvg (McMahan et al., 2017) (cf. Eq. (2)) to aggregate the massive clients updates. The server averages parameters uploaded by all clients to obtain the averaged SNN model (W S S, V S) and BN layer parameters (γS, σS, βS, μS), where the subscript S/A indicates that the parameters are from the SNN or ANN model and the superscript S indicates that the model is owned by the server. Thus, the aggregated model is also SNN. Since SNN has discrete spikes, it might be less sensitive to a small amount of random noise, which leads to the robustness of SNN (Venkatesha et al., 2021). In practice, we find that the aggregation of SNNs loses less task performance than that of ANNs in most cases. Note that clients do not require the aggregated SNN’s parameters. So the server has to take an additional step while processing the σi WA from each client to obtain the parameters that clients require. The server gets W i γi from the i-th client and then applies (cf. Eq. (2)) to aggregate all WA from clients to obtain W S A , A , γS, σS, βS, μS which is the parameter required by clients. In the sharing step, the server sends W S to each client.\n\nA ← W i\n\nS\n\nServer Distribution. Following the FedAvg setting, clients use these parameters updated by the server to recreate the ANN model and continue to train the ANN using the local dataset. Note that clients will not receive the updates of bias parameters from the server; rather, they continue to use their original ANN bias (before calibration) for the next round of training. On the client-side, they lose less information compared to FedAvg since the bias parameters are kept intact, which helps them learn a better local model. On the server-side, it always uses SNN model for evaluation.\n\n5 EXPERIMENTS\n\nIn FL, it is frequently vital to consider the presence of semi-honest (honest-but-curious) adversaries for the sake of privacy protection. The adversary is honest in the sense that he/she faithfully follows the collaborative learning protocol, but he/she may be curious about the training data of other participants. On the one hand, given the presence of semi-honest partners, private data must be kept as secure as possible, while a certain amount of information must be transferred across parties for the sake of learning utility. In this section, we conduct experiments to demonstrate the benefits of SNFL in protecting privacy and, in most cases, improving accuracy.\n\n5.1 PRIVACY\n\nThe parameters that the server can get are ∆W i W i\n\nA(r) = W i A(r − 1), ∆bi(r)′ = bi(r)′ − bi(r − 1)′, where r is the global round and i is the i-th client.\n\nS(r − 1), ∆W i\n\nS(r) = W i\n\nS(r) − W i\n\nA(r) −\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Synergistic Neuromorphic Federated Learning (SNFL) Input:Set of K clients with local datasets; B is the local minibatch size, EL is the number of local epochs, EG is the number of Global communication round, and η is the learning rate. Parameter: fSG is the global SNN model of server, fSL is the local SNN model of client, fAL is the local ANN model of client Output: Well-trained model fSG Algorithm: MAIN\n\nTrain fAL with local private dataset\n\nBroadcast the weight parameters of current global ANN model w′ ∈ fSG to all clients for Client c ← 0 to K do\n\nTransplants w′ to local ANN model fAL for epoch n ← 0 to EL do\n\n1: Initialize global SNN model fSG and local ANN model fAL with random weights 2: for round m ← 0 to EG do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for Procedure:ANN-TO-SNN ()\n\nend for Randomly select P participating clients Aggregate the parameters of fSL using FedAvg and obtain fSG Send fSG parameters to the clients\n\nend for Perform ANN-TO-SNN() and obtain fSL\n\n3:\n\n1: for all i = 1, 2, . . . , p-th layers in the ANN do 2:\n\nCollect input data x(i),output data x(i+1) in one batch Get MMSE threshold V (i) Get SNN output ̄s(i+1) Compute Error term e(i+1) = x(i+1) − ̄s(i+1) Calibrate bias term b(i) ← b(i) + μ(ei+1)\n\nth using grid search\n\n4: 5: 6: 7: end for 8: output Converted SNN model\n\nAttack on SNN model: We employ gradient inversion attack (Zhu & Han, 2020) (cf.) Eq. (4) on LeNet (LeCun et al., 2015) with batch size 1 and optimize for 280 iterations. We use CIFAR10 (Krizhevsky et al., 2010) to evaluate the attack and defense performance. The leaking process is visualized in Fig. 2 (a). This attack can recover every image from ANN model gradients. However, when the model is converted into SNN, the attack is rendered ineffective. This is because, as mentioned in 4.2, SNN replaces the differentiable ReLU neurons into non-differentiable IF neurons. For SNN, there is no gradient.\n\nAttack on ANN model converted from SNN model: We also consider a case where an attacker ignores the bias calibration and forcibly converts the SNN model to an ANN model. We employ a more sophisticated gradient inversion attack (Huang et al., 2021) on ResNet20 (Sengupta et al., 2018) with a more realistic setting in which the attacker is unaware of the exact batch size. AN N 1 → SN N → AN N 2. The difference between AN N 2 and AN N 1 is the value of bias. As shown in Fig. 2 (b), the images reconstructed by ANN2 are more blurry than the images reconstructed by ANN1.\n\n5.2 ACCURACY\n\nImplementation Details: We perform experiments on widely adopted benchmarks CIFAR10 (Krizhevsky et al., 2010), CIFAR100 (Krizhevsky et al., 2010), and Tiny-ImageNet constructed from ImageNet (Russakovsky et al., 2015). To simulate federated learning scenario, we randomly split the training set of each dataset into N parties, and assign one training party to each client. Namely, each client owns its local training set. We are interested in different partitions: IID and Non-IID, where the overall label distribution across clients is the same in the IID setting, whereas class proportions and the number of data points of each client are unbalanced in the Non-IID setting. Especially, for the Non-IID setting, we impose data shift as follows (Li et al., 2022): The size of the local dataset |Di|\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Results of gradient inversion attack. The left half shows the attack on ANN model and SNN model, respectively. The right half shows the attack on ANN model and ANN model reconverted from SNN model.\n\nTable 1: Accuracy(%) on CIFAR10, CIFAR100, and TinyImageNet test dataset with IID. \"P\" means the number of participating clients. \"Client Accuracy\" shows the mean and variance of all clients’ ANN models. \"Server Accuracy\" is the accuracy of the server model.\"S-\" means applying SNFL.\n\nDataset\n\nCIFAR10\n\nCIFAR100\n\nTiny-ImageNet\n\nCIFAR10\n\nCIFAR100\n\nTiny-ImageNe\n\nP\n\n5 10 15 5\n10 15 5\n10 15\n\n5 10 15 5\n10 15 5\n10 15\n\nFedAvg\n\nS-FedAvg\n\nFedProx\n\nS-Fedprox MOON\n\nS-MOON\n\nSOLO\n\nClient Accuracy\n\n94.4±0.06 92.0±0.11 88.6±0.18 70.1±0.08 67.0±0.13 60.2±0.26 39.0±0.02 31.0±0.01 22.1±0.02\n\n94.7±0.06 93.1±0.07 91.1±0.08 71.4±0.22 68.9±0.17 66.8±0.19 43.7±0.01 41.3±0.03 35.9±0.04\n\n94.6±0.06 91.4±0.08 84.8±0.10 70.3±0.12 67.5±0.19 59.7±0.11 38.2±0.01 29.9±0.00 26.5±0.02\n\n95.4±0.05 95.1±0.09 95.0±0.12 72.8±0.39 72.8±0.18 71.7±0.22 41.7±0.00 39.2±0.02 37.7±0.31\n\n94.8±0.08 92.9±0.07 89.5±0.09 71.5±0.24 69.2±0.10 67.2±0.21 25.2±0.12 21.5±0.38 18.2±0.24\n\n95.3±0.04 94.7±0.09 94.4±0.08 72.2±0.56 72.1±0.45 71.7±0.30 34.6±3.08 34.9±1.22 32.8±0.84\n\n89.6±0.18 84.2±0.89 79.7±1.07 52.4±0.88 40.2±0.46 31.4±2.50 19.5±1.45 9.38±0.66 5.31±0.38\n\nServer Accuracy\n\n94.5 92.2 88.8 70.2 62.1 60.9 38.9 31.1 22.9\n\n95.2 95.3 94.8 73.1 72.3 72.4 43.4 41.3 35.9\n\n94.6 91.6 85.0 70.4 67.8 60.1 38.2 30.0 26.6\n\n95.2 95.1 95.0 73.5 73.5 72.0 41.5 38.7 35.9\n\n94.9 92.8 89.5 72.2 69.6 68.0 30.5 25.9 20.9\n\n94.4 94.6 94.3 73.6 73.0 72.2 35.2 35.5 33.4\n\n/ /\n/ /\n/ /\n/ /\n/\n\nvaries across clients. We sample q ∼ DirN (β) and allocate a qi proportion of the total data samples to client i. We should note that if β is set to a smaller value, then the partition is more unbalanced. we use q ∼ DirN (β) to denote such a partitioning strategy.\n\nFor all the experiments, we use ResNet20 (Sengupta et al., 2018) architecture and SGD optimizer with a weight decay of 1e-5 and momentum of 0.9. The adopted learning is 0.1, which is multiplied by 0.1 at communication rounds 61 and 96. We set the total global communication rounds Eg at 100 and train each client for El = 5 epochs in every global communication round. The simulation length T of SNN model is 256.\n\nBaselines: (1) FedAvg (McMahan et al., 2017): it involves multiple local random gradient updates on the client nodes, followed by server model averaging updates. (2) FedProx(Li et al., 2020): it improves the local objective based on FedAvg and introduces an additional L2 regularization term in the local objective function to limit the distance between the local model and the global model. A hyper-parameter μ is introduced to control the weight of the L2 regularization. (3) MOON (Li et al., 2021a): it corrects the local updates by maximizing the agreement of representation learned by the current local model and the representation learned by the global model. (4) SOLO: each client uses local private data to train its own model without communicating with others.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Accuracy(%) on CIFAR10 and CIFAR100 with different degrees of Non-IID. There are 10 participating clients.\n\nDataset\n\nCIFAR10\n\nCIFAR100\n\nPartition q ∼ DirN (2.0) q ∼ DirN (1.0) q ∼ DirN (0.5) q ∼ DirN (2.0) q ∼ DirN (1.0) q ∼ DirN (0.5)\n\nFedAvg 92.15 90.83 88.67 68.81 66.3 68.17\n\nS-FedAvg FedProx\n\n94.74 94.6 86.88 72.54 72.47 71.47\n\n92.54 91.8 89.97 68.52 67.58 67.81\n\nS-FedProx MOON S-MOON 92.59 91.83 90.95 69.07 70.27 70.23\n\n94.87 94.45 93.88 72.4 71.78 71.63\n\n94.74 94.6 93.38 73.95 73.02 72.29\n\nTable 3: The impact of bias. \"✓/✗\" denotes whether to do bias calibration (BC), or whether the server shares its bias with clients (Bias Back). \"T=\" means the simulation length T of SNN model. The dataset is CIFAR10.\n\nBC Bias back ✗\n✗ ✓\n✓\n\n✗ ✓\n✗ ✓\n\nT=64 94.46 94.4 94.75 94.51\n\nT=128 T=256 95.14 95.08 95.00 94.82 95.14 95.05 95.08 94.91\n\nValidation in the IID Case: Since a real-world federated system involves many devices, a federated learning model must be scalable with the number of devices. In this experiment, we verify the cases of 5, 10, and 15 clients, respectively. As shown in Table. 5.1, we present the test accuracy on all datasets before and after applying SNFL. From a horizontal perspective, it can be observed that applying SNN-conversion increases accuracy for all baseline methods, even with an accuracy gain of up to 13.79%. This is particularly inspiring because SNPL requires no modification to the original federated training process. One can easily get considerable accuracy profits by simply post-processing the trained global model. Comparing the accuracy gains of different methods after applying SNFL and whole data calibration, we find that FedProx and MOON have the greatest improvement. From the vertical, the accuracy of baseline methods using SNFL drops more slowly as the number of clients increases. For example, on CIFAR100, when 5 clients participate, the accuracy of S-MOON is 0.71% higher than MOON, while the difference between S-MOON and MOON increases to 4.46% when 15 clients participate. This reflects the SNFL benefits that are better suited to the federated learning situation with a large number of clients.\n\nValidation in the Non-IID Case: A key challenge in FL is the Non-IID data among the parties. As shown in Table. 5.2, although accuracy decreases to varying degrees as β decreases, SNFL can still increase the performance of model, with MOON and Fed seeing the most benefit. For instance, on CIFAR10, when β = 0.5, S-FedProx is 3.91% higher than FedProx and S-MOON is 2.43% higher than MOON.\n\n5.3 BIAS ANALYSIS\n\nCompared to FedAvg, SNFL has two different operations on bias. (1) In the conversion between ANN and SNN, one client uses its local private data to calibrate the SNN model’s bias. (2) When sending back server’s model to clients, standard FL returns all parameters of server’s model including bias, but the server of SNFL keeps bias as a private key and does not share it with clients. Since both clients and server use bias as their private key, SNFL secures the privacy of all parties to some extent. To investigate the effect of the above two operations on bias, we design four ablation studies (Table. 5.2). It can be seen that the extra BC operations have no significant effect on the final server accuracy when clients use their own saved bias instead of the server’s bias. There is a slight increase in accuracy if the server does not share its bias with clients. In other words, we are protecting privacy while maintaining the performance of the model, rather than trading precision for privacy as in previous studies.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Bias and threshold analysis. (A) The L2 distance between the true bias and the inferred bias. (B) Scatter image for the thresholds of the last two layers. 2/10 clients are attack clients. (C) The thresholds comparison between the normal clients and the poisoned clients. The dataset is CIFAR10.\n\nThen, we simulate an honest but curious server, which attempts to infer one client ANN’s full parameters. In this experiment, we used 10 clients with IID and Non-IID (q ∼ DirN (0.5)). Since the malicious server cannot directly access the bias of ANN in SNFL, it can only calculate a new bias using the bias of SNN and ANN’s BN parameters. Fig. 3 (A) shows the L2 distance between the true bias and the inferred bias. In the IID case, the disparity between them is much greater than in Non-IID case, showing BC needs more huge bias shift (more huge difference between ANN and SNN) when layer-by-layer calibration in the IID case.\n\n5.4 BACKDOOR ATTACK AND DETECTION\n\nIn federated learning, backdoor attack (Bagdasaryan et al., 2020) attempts to cause the model to make wrong judgments about data with a certain characteristic (trigger), but the model does not affect on the main task. In other words, the attacked model still exhibits high accuracy on the test dataset, but its output will be different from the output of the clean model when input activates the backdoor trigger. Since the server cannot access the client’s training data in federated learning, it is difficult to determine whether the global model has been poisoned through data detection.\n\nIn recent paper (Bhagoji et al., 2019), the server identifies the abnormal client based on clustering or mean detection on the model weight and bias. However, these detection methods are cumbersome since the number of parameters in ANN is enormous, e.g., the number of parameters in ResNet20 has reached over 11 million. For SNFL, the server adopts the SNN model, which has a special type of parameter called \"threshold\". As shown in Fig. 3 B and C, we find that the thresholds (only 20 thresholds in ResNet20, including the converted pooling layer) of poisoned clients and normal clients are significantly different. The distance between the normal clients’ threshold set and the cluster center is less than 17.62, whereas the distance between the poisoned client and the cluster center is more than 103.53, making the distinction very clear.\n\n6 CONCLUSION\n\nIn this paper, we propose that SNFL, a simple FL framework, protects privacy while improving model accuracy in most cases. To the best of our knowledge, this is the first paper that allows clients and server to use different types of neural network models. SNFL can be thought of as a lightweight encryption component add-on for any global federated objective. We investigate the root cause of its privacy from both theoretical and experimental perspectives and conduct experiments to demonstrate that the framework does not jeopardize the model’s performance. Empirical results demonstrate that SNFL can result in both more privacy and more accurate models compared with the strong baseline. Our work suggests several interesting directions for future studies, such as exploring the applicability of SNFL to other attacks and its ability to migrate to other algorithms.\n\n9\n\nABCUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nEugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pp. 2938–2948. PMLR, 2020.\n\nArjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated In International Conference on Machine Learning, pp.\n\nlearning through an adversarial lens. 634–643. PMLR, 2019.\n\nPeva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 118–128, 2017.\n\nDavid Byrd and Antigoni Polychroniadou. Differentially private secure multi-party computation for federated learning in financial applications. In Proceedings of the First ACM International Conference on AI in Finance, pp. 1–9, 2020.\n\nLei Deng, Yujie Wu, Xing Hu, Ling Liang, Yufei Ding, Guoqi Li, Guangshe Zhao, Peng Li, and Yuan Xie. Rethinking the performance comparison between snns and anns. Neural Networks, 121:294 – 307, 2020.\n\nShikuang Deng and Shi Gu. Optimal conversion of conventional artificial neural networks to spiking\n\nneural networks. In International Conference on Learning Representations, 2020.\n\nPeter U Diehl, Guido Zarrella, Andrew Cassidy, Bruno U Pedroni, and Emre Neftci. Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware. In 2016 IEEE International Conference on Rebooting Computing (ICRC), pp. 1–8. IEEE, 2016.\n\nSteven K Esser, Paul A Merolla, John V Arthur, Andrew S Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J Berg, Jeffrey L McKinstry, Timothy Melano, Davis R Barch, et al. Convolutional networks for fast, energy-efficient neuromorphic computing. Proceedings of the national academy of sciences, 113(41):11441–11446, 2016.\n\nRobin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client\n\nlevel perspective. arXiv preprint arXiv:1712.07557, 2017.\n\nSlawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions with differential privacy. IEEE transactions on dependable and secure computing, 14(5):463–477, 2015.\n\nBing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13558–13567, 2020.\n\nStephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption. arXiv preprint arXiv:1711.10677, 2017.\n\nYangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and Sanjeev Arora. Evaluating gradient inversion attacks and defenses in federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nM. Kim, O Günlü, and R. F. Schaefer. Federated learning with local differential privacy: Trade-offs\n\nbetween privacy, utility, and communication. 2021.\n\nSeijoon Kim, Seongsik Park, Byunggook Na, and Sungroh Yoon. Spiking-yolo: Spiking neural\n\nnetwork for energy-efficient object detection. arXiv preprint arXiv:1903.06530, 2019.\n\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced\n\nresearch). URL http://www. cs. toronto. edu/kriz/cifar. html, 5(4):1, 2010.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYann LeCun et al. Lenet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet,\n\n20(5):14, 2015.\n\nQinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10713–10722, 2021a.\n\nQinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pp. 965–978. IEEE, 2022.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020.\n\nYuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, and Shi Gu. A free lunch from ann: Towards efficient, accurate spiking neural networks calibration. arXiv preprint arXiv:2106.06984, 2021b.\n\nLing Liang, Xing Hu, Lei Deng, Yujie Wu, Guoqi Li, Yufei Ding, Peng Li, and Yuan Xie. Exploring adversarial attack in spiking neural networks with spike-compatible gradient. IEEE transactions on neural networks and learning systems, 2021.\n\nYing-Hui Liu and Xiao-Jing Wang. Spike-frequency adaptation of a generalized leaky integrate-and-\n\nfire model neuron. Journal of computational neuroscience, 10(1):25–45, 2001.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\n\nBodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, and Michael Pfeiffer. Theory and tools for the conversion of analog to spiking convolutional neural networks. arXiv: Statistics/Machine Learning, (1612.04052):0–0, 2016.\n\nBodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conversion of continuous-valued deep networks to efficient event-driven networks for image classification. Frontiers in neuroscience, 11:682, 2017.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n\nAbhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. Going deeper in spiking\n\nneural networks: Vgg and residual architectures. Frontiers in Neuroence, 13, 2018.\n\nZiteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really\n\nbackdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.\n\nYeshwanth Venkatesha, Youngeun Kim, Leandros Tassiulas, and Priyadarshini Panda. Federated learning with spiking neural networks. IEEE Transactions on Signal Processing, 69:6183–6194, 2021.\n\nSandra Wachter. Normative challenges of identification in the internet of things: Privacy, profiling,\n\ndiscrimination, and the gdpr. Computer law & security review, 34(3):436–449, 2018.\n\nDong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In International Conference on Machine Learning, pp. 5650–5659. PMLR, 2018.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nBo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients.\n\narXiv preprint arXiv:2001.02610, 2020.\n\nLigeng Zhu and Song Han. Deep leakage from gradients. In Federated learning, pp. 17–31. Springer,\n\n2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1\n\nIMPACT OF NUMBER OF CLIENTS\n\nThe total number of clients is one aspect of scalability that influences the performance of a federated learning system. To study the impact of client count, we use the case with CIFAR10 in IID setting. We observe, in Table. A.1, that there is a sharp drop as the number of clients increases in standard FL. However, in SNFL, the server model accuracy lowers just little as show in Fig. A.1.\n\nTable 1: Impact of Number of Clients-CIFAR10\n\nDataset Party\n\nClients\n\nServer\n\n5 10 15 60 5\n10 15 60\n\nFedAvg 94.38±0.06 92.02±0.11 88.62±0.18 57.01±0.18 94.45 92.19 88.80 57.62\n\nS-FedAvg 94.71±0.06 93.09±0.07 91.10±0.08 90.14±0.02 95.22 95.25 94.76 90.31\n\nFedProx 94.59±0.06 91.39±0.08 84.76±0.10 73.46±0.05 94.64 91.6 84.97 73.94\n\nS-Fedprox 95.40±0.05 95.11±0.09 95.01±0.12 90.88±0.02 95.18 95.07 94.98 91.00\n\nMOON 94.80±0.08 92.90±0.07 89.50±0.10 59.68±0.22 94.85 92.81 89.5 60.48\n\nS-MOON 95.26±0.04 94.73±0.09 94.36±0.08 90.57±0.038 94.44 94.62 94.27 90.66\n\nFigure 1: Impact of Number of Clients-CIFAR10\n\nA.2 SENSITIVITY TO STRAGGLERS\n\nBecause real-world networks are inherently unstable, assuming updates communication would be effective from all selected devices is impracticable. Hence, the model needs to be robust enough to handle devices that fail to communicate updates. These devices are referred to as stragglers. In this section, we analyze the impact of stragglers on the performance of final SNN model. We use a case with a total of N = 60 clients. In each round, we randomly select P N clients upload parameters to server. We consider different levels of probabilities and summarize in Table. A.2. We found that when the number of stragglers decreased, the accuracy of FedAvg increased first and subsequently declined to 57.62%, whereas the accuracy of S-FedAvg increased amazingly consistently to 90.31%.\n\nTable 2: The accuracy with various client drop probabilities.\n\nProbability FedAvg S-FedAvg\n\n0.1 65.07 73.29\n\n0.2 66.05 83.53\n\n0.5 68.16 89.87\n\n0.7 65.76 90.13\n\n1 57.62 90.31\n\nA.3 SENSITIVITY TO THE LENGTH OF SIMULATION T\n\nThe forwarding pass in SNN is repeated for T steps to get the final result, where the final result is the expectation of the ultimate layer’s output across T steps. This allows the flexibility of adjusting T to\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nbalance between the latency and accuracy of SNNs for different application scenarios. We conduct experiments on CIFAR10 with different simulation length T , as shown in Table. A.3. We discovered that increasing T improved SNN accuracy to a certain extent. However, as T increases to a certain point, its influence on accuracy decreases.\n\nTable 3: The accuracy with different SNN simulation steps.\n\nSimulation steps S-FedAvg S-FedProx S-MOON\n\n64 89.26 90.52 89.95\n\n128 90 90.96 90.51\n\n192 90.23 90.94 90.66\n\n256 90.31 91 90.66\n\n320 90.26 90.99 90.74\n\n13",
    "reference": "# Summary Of The Paper\n\nFederated learning based on client-side uploading of spiking neural networks (SNNs) is considered. Each client's SNN model parameters are obtained by converting locally updated regular NNs (called ANNs here). The server obtains and downloads the aggregated global ANN parameters from the SNN parameters uploaded from the clients. Experiments show performance improvements and enhanced robustness against attacks, compared to baselines.\n\n# Strength And Weaknesses\n\nStrengths: timely topic (enhancing robustness of FL under privacy attacks) and interesting solution.\n\nWeaknesses: I am confused with the very basic premise here. Each client uploads the updated SNN weights and the BN parameters. The server then converts them to the corresponding local ANN weights before aggregation. So if the eavesdropper can access to the uploaded parameters of a client, then she can reproduce the ANN model weights of that client (as the serve does it easily). So, where's the protection against privacy threats that the ANN-SNN conversion process is supposed to bring to the table?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI am stuck with the very basic premise of the paper as given above, but other than that the paper is clearly presented. The method is properly motivated, and the work is original given that my question can be resolved.\n\n# Summary Of The Review\n\nThe paper deals with a timely topic (enhancing robustness of FL under privacy attacks) and appears to provides an interesting solution. However, I am confused with the very basic premise here. Each client uploads the updated SNN weights (converted from the updated ANN) along with the BN parameters. The server then converts them to the corresponding local ANN weights before aggregation and downloading. So if the eavesdropper can access the uploaded parameters of a client, then she can reproduce the ANN model weights of that client in a given iteration (as the serve does it easily). So, where's the protection against privacy threats that the ANN-SNN conversion process is supposed to bring to the table? My evaluation score would be a tentative one and I will reevaluate after/once my confusion gets cleared.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nGLOBAL EXPLAINABILITY OF GNNS VIA LOGIC COMBINATION OF LEARNED CONCEPTS\n\nSteve Azzolin1, Antonio Longa1,3, Pietro Barbiero2, Pietro Li`o2 & Andrea Passerini1 1University of Trento, 2University of Cambridge, 3Fondazione Bruno Kessler {steve.azzolin}@studenti.unitn.it, {pb737, pl219}@cam.ak.uk {antonio.longa, andrea.passerini}@unitn.it\n\nABSTRACT\n\nWhile instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.\n\n1\n\nINTRODUCTION & RELATED WORK\n\nGraph Neural Networks (GNNs) have become increasingly popular for predictive tasks on graph structured data. However, as many other deep learning models, their inner working remains a black box. The ability to understand the reason for a certain prediction represents a critical requirement for any decision-critical application, thus representing a big issue for the transition of such algorithms from benchmarks to real-world critical applications.\n\nOver the last years, many works proposed Local Explainers (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope et al., 2019; Magister et al., 2021) to explain the decision process of a GNN in terms of factual explanations, often represented as subgraphs for each sample in the dataset. We leave to Yuan et al. (2022) a detailed overview about Local Explainers, who recently proposed a taxonomy to categorize the heterogeneity of those. Overall, Local Explainers shed light over why the network predicted a certain value for a specific input sample. However, they still lack a global understanding of the model. Global Explainers, on the other hand, are aimed at capturing the behaviour of the model as a whole, abstracting individual noisy local explanations in favor of a single robust overview of the model. Nonetheless, despite this potential in interpretability and debugging, little has been done in this direction. GLocalX (Setzu et al., 2021) is a general solution to produce global explanations of black-box models by hierarchically aggregating local explanations into global rules. This solution is however not readily applicable to GNNs as it requires local explanations to be expressed as logical rules. Yuan et al. Yuan et al. (2020) proposed XGNN, which frames the Global Explanation problem for GNNs as a form of input optimization (Wu et al., 2020), using policy gradient to generate synthetic prototypical graphs for each class. The approach requires prior domain knowledge, which is not always available, to drive the generation of valid prototypes. Additionally, it cannot identify any compositionality in the returned explanation, and has no principled way to generate alternative explanations for a given class. Indeed, our experi-\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nmental evaluation shows that XGNN fails to generate meaningful global explanations in all the tasks we investigated.\n\nConcept-based Explainability (Kim et al., 2018; Ghorbani et al., 2019; Yeh et al., 2020) is a parallel line of research where explanations are constructed using “concepts” i.e., intermediate, high-level and semantically meaningful units of information commonly used by humans to explain their decisions. Concept Bottleneck Models (Koh et al., 2020) and Prototypical Part Networks (Chen et al., 2019a) are two popular architectures that leverage concept learning to learn explainable-by-design neural networks. In addition, similarly to Concept Bottleneck Models, Logic Explained Networks (LEN) (Ciravegna et al., 2021a) generate logic-based explanations for each class expressed in terms of a set of input concepts. Such concept-based classifiers improve human understanding as their input and output spaces consist of interpretable symbols (Wu et al., 2018; Ghorbani et al., 2019; Koh et al., 2020). Those approaches have been recently adapted to GNNs (Zhang et al., 2022; Georgiev et al., 2022; Magister et al., 2022). However, these solutions are not conceived for explaining already learned GNNs.\n\nOur contribution consists in the first Global Explainer for GNNs which i) provides a Global Explanation in terms of logic formulas, extracted by combining in a fully differentiable manner graphical concepts derived from local explanations; ii) is faithful to the data domain, i.e., the logic formulas, being derived from local explanations, are intrinsically part of the input domain without requiring any prior knowledge. We validated our approach on both synthetic and real-world datasets, showing that our method is able to accurately summarize the behaviour of the model to explain in terms of concise logic formulas.\n\n2 BACKGROUND\n\n2.1 GRAPH NEURAL NETWORKS\n\nGiven a graph G = (V, E) with adjacency matrix A where Aij = 1 if there exists an edge between nodes i and j, and a node feature matrix X ∈ R|V|×r where Xi is the r-dimensional feature vector of node i, a GNN layer aggregates the node’s neighborhood information into a d-dimensional refined representation H ∈ R|V|×d. The most common form of aggregation corresponds to the GCN (Kipf & Welling, 2016) architecture, defined by the following propagation rule:\n\nH k+1 = σ( ̃D− 1\n\n2 ̃A ̃D− 1\n\n(1) where ̃A = A + I, ̃D is the degree matrix relative to ̃A, σ an activation function, and W ∈ RF ×F is a layer-wise learned linear transformation. However, the form of Eq 1 is heavily dependent on the architecture and several variants have been proposed (Kipf & Welling, 2016; Veliˇckovi ́c et al., 2017; Gilmer et al., 2017)\n\n2 H kW k)\n\n2.2 LOCAL EXPLAINABILITY FOR GNNS\n\nMany works recently proposed Local Explainers to explain the behaviour of a GNN (Yuan et al., 2022). In this work, we will broadly refer to all of those whose output can be mapped to a subgraph of the input graph (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope et al., 2019). For the sake of generality, let LEXP(f, G) = ˆG be the weighted graph obtained by applying the local explainer LEXP to generate a local explanation for the prediction of the GNN f over the input graph G, where each ˆAij relative to ˆG is the likelihood of the edge (i, j) being an important edge. By binarizing the output of the local explainer ˆG with threshold θ ∈ R we achieve a set of connected components ̄Gi such that (cid:83) ̄Gi ⊆ ˆG. For convenience, we will henceforth refer to each of these ̄Gi as local explanation.\n\ni\n\n3 PROPOSED METHOD\n\nThe key contribution of this paper is a novel Global Explainer for GNNs which allows to describe the behaviour of a trained GNN f by providing logic formulas described in terms of humanunderstandable concepts (see Fig. 1). In the process, we use one of the available Local Explainers (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustration of the proposed method.\n\net al., 2019) to obtain a local explanation for each sample in the dataset. We then map those local explanations to some learned prototypes which will represent the final high-level concepts (e.g. motifs in a graph). Finally, for the formulas extraction, we input the vector of concept activations to an Entropy-based LEN (E-LEN) (Barbiero et al., 2021; Ciravegna et al., 2021b) which is trained to match the predictions of f . In the following, we will describe every step in more detail.\n\nLocal Explanations Extraction: The first step of our pipeline consists in extracting local explanations. In principle, every Local Explainer whose output can be mapped to a subgraph of the input sample is compatible with our pipeline (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope et al., 2019). Nonetheless, in this work, we relied on PGExplainer (Luo et al., 2020) since it allows the extraction of arbitrary disconnected motifs as explanations and it gave excellent results in our experiments. The result of this preprocessing step consists in a list D of local explanations, which are provided as input to the GLGExplainer architecture. More details about the binarization are available in Section 4.2.\n\nEmbedding Local Explanations: The following step consists in learning an embedding for each local explanation that allows clustering together functionally similar local explanations. This is achieved with a standard GNN h which maps any graph ̄G into a fixed-sized embedding h( ̄G) ∈ Rd. Since each local explanation ̄G is a subgraph of an input graph G, in our experiments we used the original node features of the dataset. Note, however, that those features can be arbitrarily augmented to make the aggregation easier. The outcome of this aggregation consists in a set E = {h( ̄G), ∀ ̄G ∈ D} of graph embeddings.\n\nConcept Projection: Inspired by previous works on prototype learning (Li et al., 2017; Chen et al., 2019b), we project each graph embedding e ∈ E into a set P of m prototypes {pi ∈ Rd|i = 1, . . . , m} via the following distance function:\n\nd(pi, e) = sof tmax\n\nlog(\n\n(cid:18)\n\n∥e − p1∥2 + 1 ∥e − p1∥2 + ε\n\n), . . . , log(\n\n∥e − pm∥2 + 1 ∥e − pm∥2 + ε\n\n(cid:19) )\n\ni\n\n(2)\n\nPrototypes are initialized randomly from a uniform distribution and are learned along with the other parameters of the architecture. As training progresses, the prototypes will align as prototypical representations of every cluster of local explanations, which will represent the final groups of graphical concepts. The output of this projection is thus a set V = {ve, ∀e ∈ E} where ve = [d(p1, e), .., d(pm, e)] is a vector containing a probabilistic assignment of graph embedding e (thus the local explanation that maps to it) to the m concepts, and will be henceforth referred to as concept vector.\n\nFormulas Learning: The final step consists of an E-LEN, i.e., a Logic Explainable Network (Ciravegna et al., 2021a) implemented with an Entropy Layer as first layer (Barbiero et al.,\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n2021). An E-LEN learns to map a concept activation vector to a class while encouraging a sparse use of concepts that allows to reliably extract Boolean formulas emulating the network behaviour. We train an E-LEN to emulate the behaviour of the GNN f feeding it with the graphical concepts extracted from the local explanations. Given a set of local explanations ̄G1 . . . ̄Gni for an input graph Gi and the corresponding set of the concept vectors v1 . . . vni, we aggregate the concept vectors via a pooling operator and feed the resulting aggregated concept vector to the E-LEN, providing f (Gi) as supervision. In our experiments we used a max-pooling operator. Thus, the Entropy Layer learns a mapping from the pooled concept vector to (i) the embeddings z (as any linear layer) which will be used by the successive MLP for matching the predictions of f . (ii) a truth table T explaining how the network leveraged concepts to make predictions for the target class. Since the input pooled concept vector will constitute the premise in the truth table T , a desirable property to improve human readability is discreteness, which we achieve using the Straight-Through (ST) trick used for discrete Gumbel-Softmax Estimator (Jang et al., 2016). In practice, we compute the forward pass discretizing each vi via argmax, then, in the backward pass to favor the flow of informative gradient we use its continuous version.\n\nSupervision Loss: GLGExplainer is trained end-to-end with the following loss:\n\nL = Lsurr + λ1LR1 + λ2LR2\n\n(3)\n\nwhere Lsurr corresponds to a Focal BCELoss (Lin et al., 2017) between the prediction of our E-LEN and the predictions to explain, while LR1 and LR2 are respectively aimed to push every prototype pj to be close to at least one local explanation and to push each local explanation to be close to at least one prototype (Li et al., 2017). The losses are defined as follows:\n\nLsurr = −y(1 − p)γ log p − (1 − y)pγ log(1 − p)\n\nLR1 =\n\n1 m\n\nLR2 =\n\n1 |D|\n\nm (cid:88)\n\nj=1\n\n(cid:88)\n\n ̄G∈D\n\n∥pj − h( ̄G)∥2\n\nmin ̄G∈D\n\nmin j∈[1,m]\n\n∥pj − h( ̄G)∥2\n\n(4)\n\n(5)\n\n(6)\n\nwhere p and γ represent respectively the probability for positive class prediction and the focusing parameter which controls how much to penalize hard examples.\n\n4 EXPERIMENTS\n\nWe conducted an experimental evaluation on synthetic and real-world datasets aimed at answering the following research questions:\n\n• Q1: Can GLGExplainer extract meaningful Global Explanations?\n\n• Q2: Can GLGExplainer extract faithful Global Explanations?\n\nThe source code of GLGExplainer, including the extraction of local explanations, as well as the datasets and all the code for reproducing the results is made freely available online1.\n\n4.1 DATASETS\n\nWe tested our proposed approach on three datasets, namely:\n\nBAMultiShapes: BAMultiShapes is a newly introduced extension of some popular synthetic benchmarks (Ying et al., 2019) aimed to assess the ability of a Global Explainer to deal with logical combinations of concepts. In particular, we created a dataset composed of 1,000 Barab ́asi-Albert (BA) graphs with attached in random positions the following network motifs: house, grid, wheel. Class 0 contains plain BA graphs and BA graphs enriched with a house, a grid, a wheel, or the three motifs together. Class 1 contains BA graphs enriched with a house and a grid, a house and a wheel, or a wheel and a grid.\n\n1https://github.com/steveazzolin/gnn_logic_global_expl\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nMutagenicity: The Mutagenicity dataset is a collection of 4,337 molecule graphs where each graph is labelled as either having a mutagenic effect or not. Based on Debnath et al. (1991), the mutagenicity of a molecule is correlated with the presence of electron-attracting elements conjugated with nitro groups (e.g. NO2). Moreover, compounds with three or more fused rings tend to be more mutagenic than those with one or two. Most previous works on this dataset have focused on finding the functional group NO2, since most of them struggle in finding compounds with more than two carbon rings.\n\nHospital Interaction Network (HIN): HIN is a new real-world benchmark proposed in this work. It represents the third-order ego graphs for doctors and nurses in a face-to-face interaction network collected in an hospital (Vanhems et al., 2013). There are four types of individuals in the network: doctors (D), nurses (N), patients (P), and administrators (A). Such typologies constitute the feature vector for each node, represented as one-hot encoding. Each ego network is an instance, and the task is to classify whether the ego in the ego network is a doctor or a nurse (without knowing its node features, which are masked). More details about the dataset construction are available in the Appendix.\n\nFor Mutagenicity we replicated the setting in the PGExplainer paper (Luo et al., 2020), while for BAMultiShapes and HIN we trained until convergence a 3-layer GCN. Details about the training of the networks and their accuracies are in the Appendix.\n\n4.2\n\nIMPLEMENTATION DETAILS\n\nLocal Explanations Extraction: As discussed in Section 3, we used PGExplainer (Luo et al., 2020) as the Local Explainer. However, we modified the procedure for discretizing weighted graphs into a set of disconnected motifs. Indeed, the authors in Luo et al. (2020) limited their analysis to graphs that contained the ground truth motifs and proposed to keep the top-k edges as a rule-of-thumb for visualization purposes. For Mutagenicity, over which PGExplainer was originally evaluated, we simply selected the threshold θ that maximises the F1 score of the local explainer over all graphs, including those that do not contain the ground-truth motif. For the novel datasets BAMultiShapes and HIN, we adopted a dynamic algorithm to select θ that does not require any prior knowledge about the ground truth motifs. This algorithm resembles the elbow-method, i.e., for each local explanation sort weights in decreasing order and chooses as threshold the first weight that differs by at least 40% from the previous one. We believe that threshold selection for Local Explainers is a fundamental problem to make local explainers actionable, but it is often left behind in favor of top-k selections where k is chosen based on the ground-truth motif. In the Appendix we show some examples for each dataset along with their extracted explanations.\n\nGLGExplainer: We implemented the Local Explanation Embedder h as a 2-layers GIN (Xu et al., 2018) network for each dataset except HIN, for which we found a GATV2 (Brody et al., 2021) to provide better performance as the attention mechanism allows the network to account for the relative importance of the type of neighboring individuals. All layers consist of 20 hidden units followed by a non-linear combination of max, mean, and sum graph pooling. We set the number of prototypes m to 6, 2, and 4 for BAMultiShapes, Mutagenicity, and HIN respectively (see Section 4.4 for an analysis showing how these numbers were inferred), keeping the dimensionality d to 10. We trained using ADAM optimizer with early stopping and with a learning rate of 1e−3 for the embedding and prototype learning components and a learning rate of 5e−4 for the E-LEN. The batch size was set to 128, the focusing parameter γ to 2, while the auxiliary loss coefficients λ1 and λ2 were set respectively to 0.09 and 0.00099. The E-LEN consists of an input Entropy Layer (Rm → R10), a hidden layer (R10 → R5), and an output layer with LeakyReLU activation function. We turned off the attention mechanism encouraging a sparse use of concepts from the E-LEN, as the end-toend architecture of GLGExplainer already promotes the emergence of predictive concepts, and the discretization step preceding the E-LEN encourages each concept activation vector to collapse on a single concept. All these hyper-parameters were identified via cross-validation over the training set.\n\n4.3 EVALUATION METRICS\n\nIn order to show the robustness of our proposed methodology, we have evaluated GLGExplainer on the following three metrics, namely: i) FIDELITY, which represents the accuracy of the E-LEN in matching the predictions of the model f to explain; ii) ACCURACY, which represents the accuracy\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Raw formulas as extracted by the Entropy Layer along with their test Accuracy. Each formula was rewritten to just keep positive literals.\n\nDataset\n\nBAMultiShapes\n\nRaw Formulas\n\nAccuracy\n\nClass0 ⇐⇒ P0 ∨ P3 ∨ P1 ∨ P4 ∨ P5\n\nClass1 ⇐⇒\n\n(P4 ∧ P3) ∨ (P5 ∧ P4) ∨ (P3 ∧ P1) ∨ (P5 ∧ P1) ∨ (P4 ∧ P1) ∨ (P4 ∧ P2) ∨ (P1 ∧ P2) ∨ (P3 ∧ P2) ∨ P2\n\nMutagenicity\n\nHIN\n\nClass0 ⇐⇒ P1 ∨ (P0 ∧ P1) Class1 ⇐⇒ P0\n\nClass0 ⇐⇒ P0 ∨ P1 ∨ P3 Class1 ⇐⇒ P2\n\n0.98\n\n0.83\n\n0.84\n\nof the formulas in matching the ground-truth labels of the graphs; iii) CONCEPT PURITY, which is computed for every cluster independently and measures how good the embedding is at clustering the local explanations. A more detailed description of those metrics is available in the Appendix. For BAMultiShapes and Mutagenicity, every local explanation was annotated with its corresponding ground-truth motif, or Others in case it did not match any ground-truth motif. For HIN, since no ground truth explanation is available, we labeled each local explanation with a string summarizing the users involved in the interaction (e.g., in a local explanation representing the interaction between a nurse and a patient, the label corresponds to NP). Since labelling each possible combination of interactions between 4 types of users would make the interpretation of the embedding difficult, we annotated the most frequent seven (D, N, NA, P, NP, MN, MP), assigning the rest to Others. Note that such unsupervised annotation may negatively impact the resulting Concept Purity since it ignores the correlation between similar but not identical local explanations.\n\n4.4 EXPERIMENTAL RESULTS\n\nIn this section we will go through the experimental results with the aim of answering the research questions defined above. Table 1 presents the raw formulas extracted by the Entropy Layer where, to improve readability, we have dropped negated literals from clauses. This means that for each clause in a formula, missing concepts are implicitly negated. Those formulas can be further rewritten in a more human-understandable format after inspecting the representative elements of each cluster as shown in Figure 2, where for each prototype pj the local explanation ̄G such that ̄G = argmax ̄G′∈Dd(pj, h( ̄G′)) is reported. The resulting Global Explanations are reported in Figure 3, where we included a qualitative comparison with the global explanations generated by XGNN (Yuan et al., 2020), the only available competitor for global explanations of GNNs. Finally, Table 2 shows the results over the three metrics as described in Section 4.3. Note that XGNN is not shown in the table as it cannot be evaluated according to these metrics.\n\nFigure 2: A representative element for each concept. For completeness, in the Appendix we report five random instances for each concept.\n\nQ1: Can GLGExplainer extract meaningful Global Explanations? The building blocks of the Global Explanations extracted by GLGExplainer are the graphical concepts that are learned in the concept projection layer. Figure 2 clearly shows that each concept rep-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Global explanations of GLGExplainer (ours) and XGNN. For Class 0 of BAMultiShapes, XGNN was not able to generate a graph with confidence ≥ 0.5. Note that for each clause, missing concepts are implicitly negated.\n\nTable 2: Mean and standard deviation for Fidelity, Accuracy, and Concept Purity computed over 5 runs with different random seeds. Since the Concept Purity is computed for every cluster independently, here we report mean and standard deviation across clusters over the best run according to the validation set.\n\nDataset\n\nFidelity\n\nAccuracy\n\nTrain\n\nTest\n\nTrain\n\nTest\n\nTest Concept Purity\n\nBAMultiShapes Mutagenicity HIN\n\n0.96 ± 0.03 0.82 ± 0.00 0.89 ± 0.00\n\n0.96 ± 0.03 0.81 ± 0.01 0.85 ± 0.02\n\n0.92 ± 0.03 0.78 ± 0.00 0.86 ± 0.01\n\n0.96 ± 0.03 0.79 ± 0.01 0.85 ± 0.02\n\n0.87 ± 0.24 1.00 ± 0.00 0.78 ± 0.18\n\nresents local explanations with specific characteristics, thus achieving the desired goal of creating interpretable concepts. Note that clusters corresponding to concepts are on average quite homogeneous (see Concept Purity in Table 2), and the concept representatives in the figure are faithful representations of the instances in their corresponding cluster. See the Appendix for further details, where we report five random instances for each concept. It is worth noting that this clustering emerges solely based on the supervision defined by Eq 3, while no specific supervision was added to cluster local explanations based on their similarity. This is the reason behind the emergence of the Mix cluster around p2 in the upper part of Figure 2, which represents all local explanations with at least two motifs that are present solely in Class 1 of BAMultiShapes.\n\nAdditionally, as shown in Figure 3, GLGExplainer manages to combine these building blocks into highly interpretable explanations. The explanation for BAMultiShapes almost perfectly matches the ground-truth formula, where the only difference is the conjunction of all motifs being assigned to Class 1 rather than Class 0. This however is due to a discrepancy between the ground-truth formula and what the GNN learned, as will be discussed in the answer to Q2. Note that the Mix cluster has been rewritten as the conjunction of the shapes it contains when extracting the human-interpretable\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nformulas. For Mutagenesis, the GLGExplainer manages to recover the well-known NO2 motif as an indicator of mutagenicity (Class 0). It is worth reminding that in all formulas in Figure 3, negative literals have been dropped from clauses for improved readability. Having formulas for Mutagenesis only two concepts, this implies that the formula for Class 0 actually represents NO2 itself ((NO2 ∧ OTHERS) ∨ (NO2 ∧ ¬ OTHERS) ⇐⇒ NO2). For HIN, the global explanations match the common belief that nurses tend to interact more with other nurses or with patients, while doctors tend to interact more frequently with other doctors (or patients, but less frequently than nurses).\n\nConversely to our results, XGNN (Yuan et al., 2020) fails to generate explanations matching either the ground truth or common belief about the dataset in most cases, while failing to generate any graph in others.\n\nQ2: Can GLGExplainer extract faithful Global Explanations? The high Accuracy reported in Table 2 shows that the extracted formulas are correctly matching the behaviour of the model in most samples, while being defined over fairly pure concepts as shown by the Concept Purity in the same Table. It is worth highlighting that GLGExplainer is not simply generating an explanation for the ground-truth labeling of the dataset, bypassing the GNN it is supposed to explain, but it is indeed capturing its underlying predictive behaviour. This can be seen by observing that over the training set, Fidelity is higher than Accuracy for all datasets. Note that on BAMultiShapes, training accuracy is lower than test accuracy. The reason for this train-test discrepancy can be found in the fact that the GNN fails to identify the logical composition of all three motifs (which are rare and never occur in the test set) as an indicator of Class 0. This can be seen by decomposing the GNN accuracy with respect to the motifs that occur in the data (Table 3), and observing that the accuracy for the group with all three motifs (All) is exactly zero. Quite remarkably, GLGExplainer manages to capture this anomaly in the GNN, as the clause involving all three motifs is learned as part of the formula for Class 1 instead of Class 0, as shown in Figure 3. The ability of GLGExplainer to explain this anomalous behaviour is a promising indication of its potential as a diagnostic tool for learned GNNs.\n\nTable 3: Accuracy of the model to explain on the train set of BAMultiShapes with respect to every combination of motifs to be added to the BA base graph. H, G, W stand respectively for House, Grid, and Wheel.\n\nClass 0\n\nClass 1\n\nMotifs Accuracy (%)\n\n∅ 1.0\n\nH 1.0\n\nG W All H + G H + W G + W 1.0\n\n0.98\n\n1.0\n\n0.0\n\n1.0\n\n0.85\n\nIn the rest of this section we show how the number of prototypes, that critically affects the interpretability of the explanation, can be easily inferred by trading-off Fidelity, Concept Purity and sparsity, and we provide an ablation study to demonstrate the importance of the Discretization trick.\n\nChoice of the number of prototypes: The number of prototypes in the experiments of this section was determined by selecting the smallest m which achieves satisfactory results in terms of Fidelity and Concept Purity, as measured on a validation set. Specifically, we aim for a parsimonious value of m to comply to the human cognitive bias of favoring simpler explanations to more complex ones (Miller, 1956). Table 4 reports how different values of m impact Fidelity and Concept Purity. The number of prototypes achieving the best trade-off between the different objectives was identified as 6, 2 and 4 for BAMultiShapes, Mutagenicity, and HIN respectively.\n\nRole of the Discretization trick: The Discretization trick was introduced in Section 3 to enforce a discrete prototype assignment, something essential for an unambiguous definition of the concepts on which the formulas are based. We ran an ablation study to evaluate its contribution to the overall performance of GLGExplainer. Figure 4 (left) shows the reduction in concept vector entropy achieved by GLGExplainer with the discretization trick (red curve, zero entropy by construction) as compared to GLGExplainer with the trick disabled (orange curve). Figure 4 (middle) reports the Fidelity over the training epochs for the two variants. The figure shows the effectiveness of the discretization trick in boosting Fidelity of the extracted formulas, which is more than double the one achieved disabling it. We conjecture that the reason for this behaviour is the fact that the discretization trick forces the hidden layers of the E-LEN to focus on the information relative to the closest\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Fidelity and Concept Purity as functions of the number m of prototypes in use. Results are referred to the validation set.\n\nMetric\n\nDataset\n\nm = 2 m = 4 m = 6 m = 8\n\nFidelity\n\nConcept Purity\n\nBAMultiShapes Mutagenicity HIN\n\nBAMultiShapes Mutagenicity HIN\n\n0.93 0.83 0.88\n\n0.42 0.97 0.45\n\n0.93 0.83 0.89\n\n0.73 0.99 0.80\n\n0.95 0.84 0.89\n\n0.84 0.96 0.77\n\n0.95 0.79 0.88\n\n0.91 0.99 0.70\n\nprototype, ignoring other positional information of local explanations. Thus, the E-LEN predictions are much better aligned with the discrete formulas being extracted, and indeed the Accuracy of the formulas matches the Fidelity of the E-LEN, which is shown in the right plot. On the other hand, GLGExplainer without discretization has a high Fidelity but fails to extract highly faithful formulas. Note that simply adding an entropy loss over the concept vector to the overall loss (Eq. 3) fails to achieve the same performance obtained with the discretization trick.\n\nFigure 4: Impact of the Discretization Trick. Each run is halted with early stopping on the validation set.\n\n5 CONCLUSION\n\nWe introduced GLGExplainer, the first Global Explainer for GNNs capable of generating explanations as logic formulas, represented in terms of learned human-interpretable graphical concepts. The approach is inherently faithful to the data domain, since it processes local explanations as extracted by an off-the-shelf Local Explainer. Our experiments showed that GLGExplainer, contrary to existing solutions, can faithfully describe the predictive behaviour of the model, being able to aggregate local explanations into meaningful high-level concepts and combine them into formulas achieving high Fidelity. GLGExplainer even managed to provide insights into some occasionally incorrect rules learned by the model. We believe that this approach can constitute the basis for investigating how GNNs build their predictions and debug them, which could substantially increase human trust in this technology.\n\nThe proposed GLGExplainer is inherently faithful to the data domain since it processes local explanations provided by a Local Explainer. However, the quality of those local explanations, in terms of representativeness and discriminability with respect to the task-specific class, has a direct effect on the Fidelity. If the generated concept vector does not exhibit any class-specific pattern, then the E-LEN will not be able to emulate the predictions of the model to explain. Despite being a potential limitation of GLGExplainer, this can actually open to the possibility of using the Fidelity as a proxy of local explanations quality, which is notoriously difficult to assess. We leave this investigation to future work. Despite tailoring our discussion on graph classification, our approach can be readily extended to any kind of classification task on graphs, provided that a suitable Local Explainer is available.\n\n9\n\n01002003004005000.000.250.500.751.001.251.501.75entropyConcept Vector Entropy0100200300400500epochs0.00.20.40.60.81.0%Accuracy01002003004005000.00.20.40.60.81.0%FidelityW DiscretizationW/o DiscretizationPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis research was partially supported by TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215.\n\nREFERENCES\n\nChirag Agarwal, Owen Queen, Himabindu Lakkaraju, and Marinka Zitnik. Evaluating explainability\n\nfor graph neural networks. In arXiv:2208.09339, 2022.\n\nPietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Pietro Li ́o, Marco Gori, and Stefano Melacci. Entropy-based logic explanations of neural networks, 2021. URL https://arxiv. org/abs/2106.06804.\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks?\n\narXiv\n\npreprint arXiv:2105.14491, 2021.\n\nChaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like that: Deep learning for interpretable image recognition. Advances in Neural Information Processing Systems, 32:8930–8941, 2019a.\n\nThis looks like that: Deep learning for\n\nChaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019b. URL https://proceedings.neurips.cc/paper/2019/file/ adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf.\n\ninterpretable image recognition.\n\nGabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Marco Gori, Pietro Li ́o, Marco Maggini,\n\nand Stefano Melacci. Logic explained networks. arXiv preprint arXiv:2108.05149, 2021a.\n\nGabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Marco Gori, Pietro Li ́o, Marco Maggini, and Stefano Melacci. Logic explained networks, 2021b. URL https://arxiv.org/abs/ 2108.05149.\n\nAsim Kumar Debnath, Rosa L. Lopez de Compadre, Gargi Debnath, Alan J. Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of Medicinal Chemistry, 34(2):786–797, 1991. doi: 10.1021/jm00106a046. URL https://doi.org/10. 1021/jm00106a046.\n\nDobrik Georgiev, Pietro Barbiero, Dmitry Kazhdan, Petar Veliˇckovi ́c, and Pietro Li`o. Algorithmic concept-based explainable reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 6685–6693, 2022.\n\nAmirata Ghorbani, James Wexler, James Y. Zou, and Been Kim. Towards automatic concept-based\n\nexplanations. In Neural Information Processing Systems (NeurIPS), pp. 9273–9282, 2019.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax, 2016.\n\nURL https://arxiv.org/abs/1611.01144.\n\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai, James Wexler, Fernanda B. Vi ́egas, and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In International Conference on Machine Learning (ICML), volume 80 of Proceedings of Machine Learning Research, pp. 2673–2682. PMLR, 2018.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. arXiv preprint arXiv:1609.02907, 2016.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pp. 5338–5348. PMLR, 2020.\n\nOscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions, 2017. URL https: //arxiv.org/abs/1710.04806.\n\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ́ar. Focal loss for dense object\n\ndetection, 2017. URL https://arxiv.org/abs/1708.02002.\n\nDongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network, 2020. URL https://arxiv. org/abs/2011.04573.\n\nLucie Charlotte Magister, Dmitry Kazhdan, Vikash Singh, and Pietro Li`o. Gcexplainer: Human-inthe-loop concept-based explanations for graph neural networks. arXiv preprint arXiv:2107.11889, 2021.\n\nLucie Charlotte Magister, Pietro Barbiero, Dmitry Kazhdan, Federico Siciliano, Gabriele Ciravegna, Fabrizio Silvestri, Mateja Jamnik, and Pietro Lio. Encoding concepts in graph neural networks. arXiv preprint arXiv:2207.13586, 2022.\n\nGeorge A Miller. The magical number seven, plus or minus two: Some limits on our capacity for\n\nprocessing information. Psychological review, 63(2):81, 1956.\n\nPhillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10772–10781, 2019.\n\nMattia Setzu, Riccardo Guidotti, Anna Monreale, Franco Turini, Dino Pedreschi, and Fosca Giannotti. GLocalX - from local to global explanations of black box AI models. Artificial Intelligence, 294:103457, may 2021. doi: 10.1016/j.artint.2021.103457. URL https://doi.org/10. 1016%2Fj.artint.2021.103457.\n\nCaihua Shan, Yifei Shen, Yao Zhang, Xiang Li, and Dongsheng Li.\n\nReinforcement learning enhanced explainer for graph neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, pp. 22523–22533. Curran Associates, URL https://proceedings.neurips.cc/paper/2021/file/ Inc., be26abe76fb5c8a4921cf9d3e865b454-Paper.pdf.\n\nvolume 34,\n\n2021.\n\nPhilippe Vanhems, Alain Barrat, Ciro Cattuto, Jean-Franc ̧ois Pinton, Nagham Khanafer, Corinne R ́egis, Byeul-a Kim, Brigitte Comte, and Nicolas Voirin. Estimating potential infection transmission routes in hospital wards using wearable proximity sensors. PLoS One, 8(9):e73970, 2013.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\nMinh N. Vu and My T. Thai. Pgm-explainer: Probabilistic graphical model explanations for graph\n\nneural networks, 2020. URL https://arxiv.org/abs/2010.05788.\n\nMike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale DoshiVelez. Beyond sparsity: Tree regularization of deep models for interpretability. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), AAAI Conference on Artificial Intelligence, pp. 1670– 1678. AAAI Press, 2018.\n\nWeibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael R. Lyu, and Yu-Wing Tai. Towards global explanations of convolutional neural networks with concept attribution. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8649–8658, 2020. doi: 10.1109/CVPR42600.2020.00868.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? arXiv preprint arXiv:1810.00826, 2018.\n\nChih-Kuan Yeh, Been Kim, Sercan ̈Omer Arik, Chun-Liang Li, Tomas Pfister, and Pradeep RavikuIn Neural\n\nmar. On completeness-aware concept-based explanations in deep neural networks. Information Processing Systems (NeurIPS), 2020.\n\nRex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks, 2019. URL https://arxiv.org/abs/ 1903.03894.\n\nHao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. XGNN: Towards model-level explanations of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, aug 2020. doi: 10.1145/3394486.3403085. URL https://doi.org/10.1145%2F3394486.3403085.\n\nHao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations, 2021. URL https://arxiv.org/abs/2102.05152.\n\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A\n\ntaxonomic survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nZaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. Protgnn: Towards selfexplaining graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 9127–9135, 2022.\n\nA APPENDIX\n\nA.1 DATASETS DETAILS\n\nWhile BAMultiShapes and Mutagenicity were already described in detail in Section 4.1, here we report more details about the newly introduced dataset for graph Explainability. Figure 7-9 presents some random examples for each dataset, with their extracted explanation in bold.\n\nHIN: Hospital Interaction Networks is a new real benchmark that we propose in this work. The dataset was collected using wearable sensors, equipped with radio-frequency identification devices (RFIDs) capturing face-to-face interactions. The devices record an interaction if and only if there is at least one exchanged signal within 20 seconds. The dataset was collected by Sociopatterns collaboration2 in the geriatric ward of a university hospital (Vanhems et al., 2013) in Lyon, France, over four days in December 2010. The individuals belong to four categories: medical doctors (M), nurses (N), administrative staff (A), and patients (P).\n\nSince the interaction network GT evolves over time, we convert the temporal network into a sequence of graph snapshots, aggregating interactions every five minutes (GT = [G1, G2, . . . , Gn]). For each static graph Gi, we extract the ego graph with radius 3 centered in each doctor and each nurse, obtaining a set of ego graphs. A GNN is trained in classifying between ego networks of doctor and nurse, where the feature of each ego node is masked.An illustrative example of this procedure is shown in Figure 5. In particular, the top of the figure shows a static snapshot (Gi) of the temporal graph. In the middle, we show three ego graphs (with a radius equal to 2 for convenience) centered on two doctors (D1 and D2) and a nurse (N1) respectively. Finally, the ego node features are masked, as depicted at the bottom of Figure 5. At the end of this extraction procedure, we obtain 880 and 2009 ego graphs for doctors and nurses respectively. To avoid class imbalance, we sub-sample the nurse class, obtaining a balanced dataset of 1760 graphs.\n\nA.2\n\nIMPLEMENTATION DETAILS\n\nA.2.1 TRAINING THE GNN f\n\nIn this section we will provide more details about the training of the GNN to explain f . While for Mutagenicity we limited to reproduce the results presented in (Luo et al., 2020) both in terms of\n\n2http://www.sociopatterns.org/\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Extraction of ego graphs in Hospital Interaction Network (HIN).\n\nmodel to explain (a 3 layers GCN Kipf & Welling (2016)) and local explanations, for BAMultiShapes and HIN we trained our own networks. For BAMultiShapes we adopted a 3-layers GCN (20-20-20 hidden units) with mean graph pooling for the final prediction, whilst for HIN we employed a 3-layers GCN (20-20-20 hidden units) with non-linear combination of sum, mean, and max graph pooling. The final model performances are reported in Table 5. In both cases we used ADAM optimizer, training until convergence and using the validation set to select the best epoch.\n\nA.2.2 EXPLAINERS\n\nIn this work we relied mainly on two off-the-shelf explainers, namely, PGExplainer (Luo et al., 2020) and XGNN (Yuan et al., 2020). Here we report some details about their usage.\n\nPGExplainer: For Mutagenicity and BAMultiShapes, we used the original implementation as provided by Luo et al. (2020). For BAMultiShapes we changed to original hyper-parameters to {epochs:5, args.elr=0.007, args.coff t0=1.0, args.coff size=0.0005 args.coff ent=0.000}. For HIN we instead used the PyTorch implementation provided by Agarwal et al. (2022), using as custom hyper-parameters {t0=1, t1=1, max epochs=30}. Finally, for BAMultiShapes and HIN, for which we extracted our own local explanations, we trained PGExplainer on the train split of the original dataset. It is worth mentioning that for HIN, after the local explanations extraction, we removed every local explanation not containing the ego node.\n\nXGNN: The official code provided here3 is specifically tailored for generating explanations for the Mutagenicity dataset. For the other two, despite the algorithm accepting an heavy optimization for the task at hand (like defining custom rewards functions for each task), we made minimal changes to the architecture and to the original hyper-parameters in order not to input any a-priori knowledge. For HIN, specifically, we adapted the node type generation to match the node types of the dataset, and the custom check validity function to define whether the generated graph is valid, i.e., it must contain a single ego node. For what concern the evaluation metrics presented in Section\n\n3https://github.com/divelab/DIG/tree/main/dig/xgraph/XGNN\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: Accuracies of the different models to explain.\n\nSplit BAMultiShapes Mutagenicity HIN\n\nTrain Val Test\n\n0.94 0.94 0.99\n\n0.87 0.86 0.86\n\n0.92 0.87 0.86\n\n4.3, since XGNN and GLGExplainer return explanations in two substantially different formats, we could not compare quantitatively the explanations provided by XGNN with ours. Thus, a direct comparison of XGNN under our metric is not possible.\n\nA.2.3 CHOOSING λ1 AND λ2\n\nIn Eq 3 we introduced the two parameters regulating the importance of the two auxiliary losses described in Section 3. Those parameters were kept fixed to, respectively, 0.09 and 0.00099 for all the experiments and were chosen via cross validation. In Figure 6 we show how the Fidelity over the validation set changes with different combinations of such hyper-parameters.\n\nFigure 6: How does the selection of λ1 and λ2 impact Fidelity?\n\nA.3 LOCAL EXPLANATIONS EMBEDDING\n\nFor layout reasons, we did not report in the main paper the 2D embedding plot for each dataset. However, we believe it is of great interest since it gives a visual sense of how similar local explanations get clustered in the same concept. Thus, we report in Figure 10 the 2D plot of local explanations embedding for BAMultiShapes, Mutagenicity, and HIN.\n\nIn addition to this, we report a graphical materialization of five random samples for each concept, in Figures 11-13.\n\n14\n\n20.000990.00090.090.9210.00090.090.92Fidelity0.500.620.750.881.00BAMultiShapes20.000990.00090.090.9210.00090.090.92Fidelity0.500.620.750.881.00MutagenicityPublished as a conference paper at ICLR 2023\n\nFigure 7: Random examples of input graphs along with their explanations in bold as extracted by PGExplainer, for BAMultiShapes.\n\nFigure 8: Random examples of input graphs along with their explanations in bold as extracted by PGExplainer, for Mutagenicity\n\n15\n\nClass 0Class 1CCCCCCOOOHHHHHHNNCCCCCCCCCCOHHHHHHHHHHHHHHHNCCCCCCCCCCCCCCCCCOOOOHHHHHHHHHHHHHHCCCCCCCCOOOOHHHHHHHHNNClass 0CCCCCCCCCCOOHHHHHHHHHNSCCCCCCCCCCCCCCOOOOOOHHHHHHHNNNCCCCCCCCCCOOOOHHHHHHNNCCCCCCCCCCHHHHHHHHHHHHHHCCCCCCCCHHHHHHHHBrBrCCCCCCHHHHHHHHHHHHHHHHHHHHHHHHNNNNNNNNNPPPCCCCCCCCHHHHHHHHHHClass 1CCCCCCCCOOClClClHHHHHHHCCCCCCCCCCHHHHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCOOOOOOHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHNNNNSPublished as a conference paper at ICLR 2023\n\nFigure 9: Random examples of input graphs along with their explanations in bold as extracted by PGExplainer, for HIN\n\nFigure 10: 2D PCA-reduced embedding for, respectively, BAMultiShapes and Mutagenicity and HIN\n\n16\n\nANNNEgoANNNAEgoPNNNPPNPNNAEgoMMAPMNNEgoANANNNClass 0NNNANAEgoNNNNPNPEgoNPPPPNNNNNAAANNEgoNAEgoNNNPNPNPMPPNNMMNPEgoNNANNMMPNANMEgoAMNMMEgoAMMMNClass 1NEgoNPNMNNNNNMEgoMPMPNMMMEgoMAN1.00.50.00.51.00.40.20.00.20.40.6p0p1p2p3p4p5local explanations embeddingshousegridwheelbahouse+gridhouse+wheelwheel+gridall1.00.50.00.51.00.40.20.00.20.40.6prototype assignmentsp0p1p2p3p4p5principal comp. 11.00.50.00.51.01.50.40.20.00.20.40.60.8p0p1local explanations embeddingsNO2Others1.00.50.00.51.01.50.40.20.00.20.40.60.8prototype assignmentsp0p1principal comp. 10.40.20.00.20.40.60.81.01.20.40.20.00.20.40.60.8p0p1p2p3local explanations embeddingsMNNAPATNPMNMPOTHERS0.40.20.00.20.40.60.81.01.20.40.20.00.20.40.60.8prototype assignmentsp0p1p2p3principal comp. 1principal comp. 2Published as a conference paper at ICLR 2023\n\nFigure 11: Five random local explanations for each concept in BAMultiShapes.\n\nFigure 12: Five random local explanations for each concept in Mutagenicity.\n\nFigure 13: Five random local explanations for each concept in HIN.\n\n17\n\nP0 BAP1 WheelP2 MixP3 GridP4 HouseP5 GridP0 OthersCCCOOOSCOOOSCCOOOOSP1 NO2OONOONOONCOONOONP0 Nurses(+ Adm.)NEgoEgoNNNEgoNEgoNEgoNAP1 PatientsPEgoPEgoPEgoPPPEgoEgoPP2 Doctors(+ Nur./Pat.)DEgoEgoDDDEgoDDEgoEgoDDP3 Nurses+PatientsPNEgoNPEgoEgoNPNEgoPPNPEgoPublished as a conference paper at ICLR 2023\n\nA.4 EVALUATION METRICS\n\nHere we will describe in more detail the metrics briefly introduced in Section 4.3:\n\n• Fidelity measures the accuracy between the prediction of the E-LEN and the one of the GNN to explain. It is computed as the accuracy between the class predictions of the ELEN and the GNN f .\n\n• Accuracy Barbiero et al. (2021) represents how well the learned formulas can correctly predict the class labels. To compute this metric, we treat the final formulas as a classifier that given an input concept vector predicts the class corresponding to the clause evaluated to true. In the cases in which either no clause or more clauses of different classes are evaluated to be true, the sample is always considered as wrongly predicted.\n\n• Concept Purity is computed for every cluster independently and measures how good the embedding is at clustering the local explanations. It was first proposed in Magister et al. (2021) for evaluating concept representations by means of graph edit distance. However, since the computation of the graph edit distance is expensive, in our work we adapted such metric to exploit the annotation of local explanations as described in Section 4.3. Specifically, in our cases such annotation corresponds to the typology of the motif represented by the local explanation. The computation of the metric can be summarized by:\n\nConceptP urity(Ci) =\n\ncount most f requent label(Ci) |Ci|\n\n(7)\n\nwhere Ci corresponds to the cluster having pi as prototype (i.e., the cluster containing every local explanation associated to prototype pi by the distance function d(., .) described in Section 3). count most f requent label(Ci) instead returns the number of local explanations annotated with the most present label in cluster Ci. The Concept Purity results reported in Table 2 are computed by taking the mean and the standard deviation across the m clusters.\n\nA.5 ENTROPY-BASED LOGIC EXPLAINED NETWORKS\n\nConcept-based classifiers (Koh et al., 2020) are a family of machine learning models predicting class memberships from the activation scores of k human-understandable categories, i.e., q : C (cid:55)→ Y, where C ⊆ [0, 1]k. Concept-based classifiers improve human understanding as their input and output spaces consist of interpretable symbols (Wu et al., 2018; Ghorbani et al., 2019; Koh et al., 2020). Logic Explained Networks (LENs (Ciravegna et al., 2021a)) are concept-based neural models providing for each class i simple logic explanations φi : ̄C (cid:55)→ {0, 1} for their predictions ̄qi( ̄c) ∈ {0, 1}. In particular LENs provide concept-based First-Order Logic (FOL) explanations for each classification task:\n\n∀ ̄c ∈ ̄C ⊆ ̄C : φi( ̄c) ↔ qi( ̄c). where φi is a concept-based formula in disjunctive normal form. The E-LEN employed in our work corresponds to a LEN with an Entropy Layer Barbiero et al. (2021) as first layer, which is the responsible for the extraction of the logic formulas. As mentioned in Section 4.2, given that our architecture already promotes by construction a singleton activation of concepts, the entropybased regularization described in Barbiero et al. (2021) promoting a parsimonious activation of the concepts, allowing the E-LEN to predict class memberships using few relevant concepts only, is removed.\n\n(8)\n\nA.5.1 EXTRACTION OF LOGIC FORMULAS\n\nConsidering the removal of the entropy-based regularization mentioned above, the process of formula extraction can be summarized as follows: given a classification task with r classes, and given a truth table T i for each of the r classes, the l-th row of the resulting table T i is obtained by concatenating together the l-th input concept activation vector ̄cl with the respective prediction qi( ̄cl):\n\nT i\n\nl = ( ̄cl∥qi( ̄cl))\n\n18\n\n(9)\n\nPublished as a conference paper at ICLR 2023\n\nThen, for every row l, where qi( ̄cl) = 1, concepts in ̄cl are connected with the AND operator resulting in a logic clause where concepts that appear as false in ̄cl are negated. Finally, the final formulas in disjunctive normal form for table T i are obtained by connecting every clause with the OR operator. Further details are available in Barbiero et al. (2021).\n\nA.5.2 ENHANCEMENTS OF GLGEXPLAINER TO THE E-LEN FRAMEWORK\n\nTo make clear the enhancements of our proposed GLGExplainer to the E-LEN framework, note that the original formulation of the E-LEN requires C to be known. Indeed, the experiments carried out in Barbiero et al. (2021) were all assuming a known mapping from the input space to C. In this case, however, since such mapping is not available, we aim at learning a set of human-understandable concepts. The choice of learning such concepts from local explanations, rather than generating them via any graph-generation process, allowed our contribution to be faithful to the data domain contrary to previous works on Global Explainability for GNNs Yuan et al. (2020). Since the E-LEN framework does not provide a principled way for dealing with the peculiarities of the graph domain, it cannot be directly applied on top of local explanations. We thus devised the method proposed in Section 3 in order to aggregate local explanations into clusters of similar subgraphs, with no further supervision than the one provided by Eq 3. The experimental evaluation in Section 4.4 shows that the learned clusters have indeed a human-understandable meaning, and the final formulas correctly allow to assess the performances of the model to explain. On the same vein, the framework proposed in Barbiero et al. (2021) is aimed at creating an interpretable classifier which can generate logic explanations of its predictions, resulting in an interpretable-by-design architecture. This is radically different from our scenario, where we provide explanations of an already trained model. Finally, given the fact that we removed the entropy-based regularization from the original implementation of the Entropy Layer4, we are basically restricting the usage of it to just the principled process of extracting logic formulas from the truth table T , introduced in Barbiero et al. (2021).\n\nA.6 DISTANCE FUNCTION d(., .)\n\nIn Section 3 we presented Eq. 2 as the distance function d(pi, e) to compute a relative assignment value for the graph embedding e to prototype pi, expressed as a probability value thanks to the softmax. For convenience, we report again the mathematical definition below:\n\nd(pi, e) = sof tmax\n\nlog(\n\n(cid:18)\n\n∥e − p1∥2 + 1 ∥e − p1∥2 + ε\n\n), . . . , log(\n\n∥e − pm∥2 + 1 ∥e − pm∥2 + ε\n\n(cid:19) )\n\ni\n\n(10)\n\nNote that when d(pi, e) has a high activation value for a prototype pi, then the graph embedding e is very close to prototype pi in the embedding space, thus meaning that the input graph leading to e exhibit a similar high-level concept to what pi represents.\n\nOverall, the output of the projection step described in Section 3 is a set V = {ve, ∀e ∈ E} where ve = [d(p1, e), .., d(pm, e)] is a vector containing a probabilistic assignment of graph embedding e to the m prototypes.\n\n4https://pypi.org/project/torch-explain/\n\n19",
    "reference": "# Summary Of The Paper\n\nThis paper proposes an approach to applying the Entropy-based Logic Explained Network (E-LEN) to generate global explanations for a GNN from computed local explanations of the GNN, where both local and global explanations are expressed by sub-graphs. Experimental results on three datasets demonstrate good performance in terms of fidelity, accuracy and concept purity.\n\n# Strength And Weaknesses\n\nStrengths:\n\n(1) The work may have a significant impact since generating global explanations for GNNs is seldom studied by now. There seems only one notable approach to generating global explanations for GNNs, namely XGNN (Yuan et al., 2020), but this approach is based on reinforcement learning and thus is unstable and time consuming in course of training.\n\n(2) Out of three experimental datasets, two are proposed by the paper for the first time. The authors promise to make them publicly available upon acceptance.\n\nWeaknesses:\n\n(1) The novelty is limited since the proposed approach seems to be only an application of E-LEN. I suggest the authors to strengthen the novelty for clarifying which enhancements are made beyond the framework of E-LEN.\n\n(2) There is no comparison with XGNN in terms of the main metrics namely fidelity, accuracy and concept purity. In particular, the proposed approach seems inferior to XGNN on Mutagenicity in terms accuracy according to the accuracy reported in (Yuan et al., 2020).\n\n(3) The presentation is generally good but can be improved by clarifying the following two questions.\n\nQ1: How to convert a prototype vector (obtained in the concept projection step) to a subgraph (learnt concept) as shown in Figure 3?\n\nQ2: Why not to use the same set of metrics provided in (Yuan et al., 2020) but introduce a new metric concept purity? Besides, concept purity has not been clearly defined in the paper; it should be defined in a more formal way, e.g., by using formulas.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe quality of the paper is fine except a lack of comparison with XGNN in terms of main metrics and some presentation problems. The clarity is fair considering that two important questions mentioned above have not been answered in the paper. The originality is marginal since the main idea is merely an application of the E-LEN framework to generate global explanations for GNNs. There is no code or data provided in the supplemental material.\n\n# Summary Of The Review\n\nThe paper addresses an important yet insufficiently-studied problem by applying the E-LEN framework. New datasets are proposed to accelerate the on-going studies. However, the proposed approach lacks comparison with the state-of-the-arts in main metrics. Besides, the paper is unclear in some important aspects and need to be improved by clarifying them.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nN/A"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nBRIDGING THE GAP TO REAL-WORLD OBJECTCENTRIC LEARNING\n\nMaximilian Seitzer1,† Max Horn2 Tianjun Xiao2 Bernhard Sch ̈olkopf2 1Max-Planck Institute for Intelligent Systems, T ̈ubingen, Germany 2Amazon Web Services 3Department of Computer Science, ETH Z ̈urich\n\nCarl-Johann Simon-Gabriel2 Thomas Brox2\n\nAndrii Zadaianchuk1,3,† Tong He2\n\nDominik Zietlow2 Zheng Zhang2 Francesco Locatello2\n\nABSTRACT\n\nHumans naturally decompose their environment into entities at the appropriate level of abstraction to act in the world. Allowing machine learning algorithms to derive this decomposition in an unsupervised way has become an important line of research. However, current methods are restricted to simulated data or require additional information in the form of motion or depth in order to successfully discover objects. In this work, we overcome this limitation by showing that reconstructing features from models trained in a self-supervised manner is a sufficient training signal for object-centric representations to arise in a fully unsupervised way. Our approach, DINOSAUR, significantly out-performs existing image-based object-centric learning models on simulated data and is the first unsupervised object-centric model that scales to real-world datasets such as COCO and PASCAL VOC. DINOSAUR is conceptually simple and shows competitive performance compared to more involved pipelines from the computer vision literature.\n\n1\n\nINTRODUCTION\n\nObject-centric representation learning has the potential to greatly improve generalization of computer vision models, as it aligns with causal mechanisms that govern our physical world (Sch ̈olkopf et al., 2021; Dittadi et al., 2022). Due to the compositional nature of scenes (Greff et al., 2020), object-centric representations can be more robust towards out-of-distribution data (Dittadi et al., 2022) and support more complex tasks like reasoning (Assouel et al., 2022; Yang et al., 2020) and control (Zadaianchuk et al., 2020; Mambelli et al., 2022; Biza et al., 2022). They are in line with studies on the characterization of human perception and reasoning (Kahneman et al., 1992; Spelke & Kinzler, 2007). Inspired by the seemingly unlimited availability of unlabeled image data, this work focuses on unsupervised object-centric representation learning.\n\nMost unsupervised object-centric learning approaches rely on a reconstruction objective, which struggles with the variation in real-world data. Existing approaches typically implement “slot”-structured bottlenecks which transform the input into a set of object representations and a corresponding decoding scheme which reconstructs the input data. The emergence of object representations is primed by the set bottleneck of models like Slot Attention (Locatello et al., 2020) that groups together independently repeating visual patterns across a fixed data set. While this approach was successful on simple synthetic datasets, where low-level features like color help to indicate the assignment of pixels to objects, those methods have failed to scale to complex synthetic or real-world data (Eslami et al., 2016; Greff et al., 2019; Burgess et al., 2019; Locatello et al., 2020; Engelcke et al., 2021).\n\nTo overcome these limitations, previous work has used additional information sources, e.g. motion or depth (Kipf et al., 2022; Elsayed et al., 2022). Like color, motion and depth act as grouping signals when objects move or stand-out in 3D-space. Unfortunately, this precludes training on most real-world\n\n†: Work done during an internship at Amazon Web Services. Correspondence to: hornmax@amazon.de, maximilian.seitzer@tuebingen.mpg.de\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nimage datasets, which do not include depth annotations or motion cues. Following deep learning’s mantra of scale, another appealing approach could be to increase the capacity of the Slot Attention architecture. However, our experiments (Sec. 4.3) suggest that scale alone is not sufficient to close the gap between synthetic and real-world datasets. We thus conjecture that the image reconstruction objective on its own does not provide sufficient inductive bias to give rise to object groupings when objects have complex appearance. But instead of relying on auxiliary external signals, we introduce an additional inductive bias by reconstructing features that have a high level of homogeneity within objects. Such features can easily be obtained via recent self-supervised learning techniques like DINO (Caron et al., 2021). We show that combining such a feature reconstruction loss with existing grouping modules such as Slot Attention leads to models that significantly out-perform other image-based object-centric methods and bridge the gap to real-world object-centric representation learning. The proposed architecture DINOSAUR (DINO and Slot Attention Using Real-world data) is conceptually simple and highly competitive with existing unsupervised segmentation and object discovery methods in computer vision.\n\n2 RELATED WORK\n\nOur research follows a body of work studying the emergence of object-centric representations in neural networks trained end-to-end with certain architectural biases (Eslami et al., 2016; Burgess et al., 2019; Greff et al., 2019; Lin et al., 2020; Engelcke et al., 2020; Locatello et al., 2020; Singh et al., 2022a). These approaches implicitly define objects as repeating patterns across a closed-world dataset that can be discovered e.g. via semantic discrete- or set-valued bottlenecks. As the grouping of low-level features into object entities is often somewhat arbitrary (it depends for example on the scale and level of detail considered), recent work has explored additional information sources such as video (Kosiorek et al., 2018; Jiang et al., 2020; Weis et al., 2021; Singh et al., 2022b; Traub et al., 2023), optical flow (Kipf et al., 2022; Elsayed et al., 2022; Bao et al., 2022), text descriptions of the scene (Xu et al., 2022) or some form of object-location information (e.g. with bounding boxes) (Kipf et al., 2022). In contrast, we completely avoid additional supervision by leveraging the implicit inductive bias contained in the self-supervised features we reconstruct, which present a high level of homogeneity within objects (Caron et al., 2021). This circumvents the scalability challenges of previous works that rely on pixel similarity as opposed to perceptual similarity (Dosovitskiy & Brox, 2016) and enables object discovery on real-world data without changing the existing grouping modules. Our approach can be considered similar to SLATE (Singh et al., 2022a), but with the crucial difference of reconstructing global features from a Vision Transformer (Dosovitskiy et al., 2021) instead of local features from a VQ-VAE (van den Oord et al., 2017).\n\nChallenging object-centric methods by scaling dataset complexity has been of recent interest: Karazija et al. (2021) propose ClevrTex, a textured variant of the popular CLEVR dataset, and show that previous object-centric models perform mostly poorly on it. Greff et al. (2022) introduce the MOVi datasets with rendered videos of highly realistic objects with complex shape and appearance. Arguably the most advanced synthetic datasets to date, we find that current state-of-the-art models struggle with them in the unsupervised setting. Finally, Yang & Yang (2022) show that existing image-based object-centric methods catastrophically fail on real-world datasets such as COCO, likely because they can not cope with the diversity of shapes and appearances presented by natural data. In contrast, we demonstrate that our approach works well on both complex synthetic and real-world datasets.\n\nIn the computer vision literature, structuring natural scenes without any human annotations has also enjoyed popularity, with tasks such as unsupervised semantic segmentation and object localization. Those tasks are interesting for us because they constitute established real-world benchmarks related to unsupervised object discovery, and we show that our method is also competitive on them. We refer to App. A for a detailed discussion of prior research in these areas.\n\n3 METHOD\n\nOur approach essentially follows the usual autoencoder-like design of object-centric models and is summarized in Figure 1: a first module extracts features from the input data (the encoder), a second module groups them into a set of latent vectors called slots, and a final one (the decoder) tries to reconstruct some target signal from the latents. However, our method crucially differs from other\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nResnet Encoder\n\nResNet\n\nViT Encoder\n\nDINO ViT\n\n(cid:181)\n\nImage\n\nDINO ViT\n\n(cid:181)\n\nRec. Loss\n\nPatch Features h N × Dfeat\n\nEncoder\n\nSlot Attention\n\nDecoder\n\nMLP MLP MLP\n\nMLP Decoder\n\nweighted sum\n\nTransformer Decoder\n\nSlots z K × Dslots\n\nReconstructed Patch Features y N × Dfeat\n\nTransformer\n\nauto regression\n\nFigure 1: Overview of the proposed architecture DINOSAUR. The image is processed into a set of patch features h by a frozen DINO ViT model (pre-trained using the self-supervised DINO method) and encoded via either a ResNet or the DINO ViT. Slot attention groups the encoded features into a set of slots. The model is trained by reconstructing the DINO features from the slots, either independently per-slot (MLP decoder) or jointly via auto regression (Transformer decoder).\n\napproaches in that instead of reconstructing the original inputs, the decoder is tasked to reconstruct features from self-supervised pre-training. We start with the discussion of this training signal in Sec. 3.1 and describe further architectural choices in Sec. 3.2.\n\n3.1 FEATURE RECONSTRUCTION AS A TRAINING SIGNAL\n\nWhy are models based on image reconstruction like Slot Attention not successful beyond simpler synthetic datasets? We hypothesize that reconstruction on the pixel level produces too weak of a signal for object-centricness to emerge; the task focuses (at least initially) strongly on low-level image features such as color statistics. This quickly decreases the reconstruction error, but the resulting model does not discover objects beyond datasets where objects are mostly determined by distinct object colors. Instead, if we had an (unsupervised) signal that required higher-level semantic information to reconstruct, there would be pressure on the slots to efficiently encode this information as well. Luckily, such signals can nowadays be easily obtained with self-supervised learning algorithms, which have been successful in learning powerful representations for vision tasks such as classification and object detection purely from images (Chen et al., 2020b; Grill et al., 2020; He et al., 2022). Thus, given K slots z ∈ RK×Dslots , the model is trained to reconstruct self-supervised features h ∈ RN ×Dfeat, by minimizing the following loss:\n\nLrec = ∥y − h∥2,\n\ny = Decoder(z).\n\n(1)\n\nThis loss can be viewed as a form of student-teacher knowledge distillation (Hinton et al., 2015), where the student has a particular form of bottleneck that condenses the high-dimensional, unstructured information contained in the teacher features into a lower-dimensional, structured form. We can also draw parallels between this loss and perceptual similarity losses for image generation (Dosovitskiy & Brox, 2016), that is, the optimization takes place in a space more semantic than pixel space.\n\nFor pre-training, we utilize the ImageNet dataset (Deng et al., 2009). From the student-teacher perspective, this means that the teacher additionally transports knowledge gained from a larger image collection to the (smaller) datasets at hand. It is well-known that using large datasets for pre-training can significantly improve performance, but to our knowledge, we are the first to exploit such transfer learning for object-centric learning. In general, studying the role additional data can play for object-centric learning is an interesting topic, but we leave that for future investigations.\n\nWhich self-supervised algorithm should we use? In our analysis (Sec. 4.3), we investigate several recent ones (DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021), MSN (Assran et al., 2022), MAE (He et al., 2022)). Interestingly, we find that they all work reasonably well for the emergence of real-world object grouping. In the following, we mainly apply the DINO method (Caron et al., 2021), because of its good performance and accessibility in open source libraries (Wightman, 2019). We experiment with features from ResNets (He et al., 2015) and Vision Transformers (ViTs) (Dosovitskiy et al., 2021), and find that the latter yield significantly better results.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3.2 AN ARCHITECTURE FOR REAL-WORLD OBJECT-CENTRIC LEARNING\n\nEncoder Previous work has shown that powerful feature extractors help in scaling object-centric methods to more complex data (Kipf et al., 2022). To this end, we experiment with two choices: a ResNet-34 encoder with increased spatial resolution used by Kipf et al. (2022), and Vision Transformers. Unfortunately, we were not able to optimize randomly initialized ViTs with our model, as training collapsed. Instead, we found it sufficient to initialize the ViT using weights from self-supervised pre-training, and keeping them fixed throughout training1. In terms of results, we find that the ResNet and the pre-trained ViT encoder perform similarly. However, the model converges faster with the pre-trained ViT, and it is also computationally more efficient: we can directly use the ViT outputs as the target features h. Consequently, we mainly use the ViT encoder in the following.\n\nSlot Attention Grouping The grouping stage of our model uses Slot Attention (Locatello et al., 2020) to turn the set of encoder features into a set of K slot vectors z ∈ RK×Dslots . This follows an iterative process where slots compete for input features using an attention mechanism, starting from randomly sampled initial slots. We largely use the original Slot Attention formulation (including GRU (Cho et al., 2014) and residual MLP modules), with one difference when using ViT features: we do not add positional encodings on the ViT features before Slot Attention, as we found the ViT’s initial position encodings to be sufficient to support spatial grouping of the features. Additionally, we add a small one-hidden-layer MLP that transforms each encoder feature before Slot Attention.\n\nFeature Decoding As we apply feature instead of image reconstruction as the training objective, we need a decoder architecture suitable for this purpose. To this end, we consider two different designs: a MLP decoder that is applied independently to each slot, and a Transformer decoder (Vaswani et al., 2017) that autoregressively reconstructs the set of features. We describe both options in turn.\n\nThe MLP decoder follows a similar design as the commonly used spatial broadcast decoder (Watters et al., 2019). Each slot is first broadcasted to the number of patches, resulting in a set of N tokens for each slot. To make the spatial positions of the tokens identifiable, a learned positional encoding is added to each token. The tokens for each slot are then processed token-wise by the same MLP, producing the reconstruction ˆyk for slot k, plus an alpha map αk that signifies where the slot is active. The final reconstruction y ∈ RN ×Dfeat is formed by taking a weighted sum across the slots:\n\ny =\n\nK (cid:88)\n\nk=1\n\nˆyk ⊙ mk,\n\nmk = softmax\n\nk\n\nαk\n\n(2)\n\nThe advantage of this simple design is its computational efficiency: as the MLP is shared across slots and positions, decoding is heavily parallelizable.\n\nThe Transformer decoder (Vaswani et al., 2017) reconstructs features y jointly for all slots in an autoregressive manner. In particular, the feature at position n is generated while conditioning on the set of previously generated features y<n and the set of slots z: yn = Decoder(y<n; z). This decoder design is more powerful than the MLP decoder as it can maintain global consistency across the reconstruction, which might be needed on more complex data. However, we found several drawbacks of the Transformer decoder: it does not work with training ResNet encoders from scratch, higher resolution target features (see App. D.5), and requires more effort to tune (see App. D.4). Thus, we recommend using the MLP decoder as the first choice when applying DINOSAUR to a new dataset. We note that Transformer decoders have also been previously explored by SLATE (Singh et al., 2022a) and STEVE (Singh et al., 2022b), but to reconstruct the discrete token map of a VQ-VAE (van den Oord et al., 2017).\n\nEvaluation Object-centric methods are commonly evaluated by inspecting masks associated with each slots. Previous approaches reconstructing to image-level typically use the decoder’s alpha mask for this purpose; for the MLP decoder, we also make use of this option. The Transformer decoder does not produce an alpha mask. Instead, we have two options: the attention masks of Slot Attention (used by SLATE), or the decoder’s attention mask over the slots. We found that the latter performed better (see Sec. D.6), and we use it throughout. As the masks from feature reconstruction are of low resolution, we bilinearly resize them to image resolution before comparing them to ground truth masks.\n\n1Another option would be further finetuning the pre-trained ViT, but we found that this leads to slots that do\n\nnot focus on objects. Combining ViT training with Slot Attention might require very careful training recipes.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nBroadly, we pursue two goals with our experiments: 1) demonstrating that our approach significantly extends the capabilities of object-centric models towards real-world applicability (Sec. 4.1), and 2) showing that our approach is competitive with more complex methods from the computer vision literature (Sec. 4.2). Additionally, we ablate key model components to find what is driving the success of our method (Sec. 4.3). The main task we consider in this work is object discovery, that is, finding pixel masks for all object instances in an image.\n\nDatasets We consider two synthetic and two real-world image datasets. As synthetic datasets, we use the MOVi datasets (Greff et al., 2022), recently introduced as challenging testbeds for object-centric methods. In particular, we use the variants MOVi-C and MOVi-E, which contain around 1 000 realistic 3D-scanned objects on HD backgrounds. For our purposes, the main difference is that MOVi-C contains 3–10, and MOVi-E 11–23 objects per scene. Note that we treat the video-based MOVi datasets as image datasets by randomly sampling frames. As real-world datasets, we use PASCAL VOC 2012 (Everingham et al., 2012) and MS COCO 2017 (Lin et al., 2014), commonly used for object detection and segmentation. Whereas PASCAL VOC contains many images with only a single large object, COCO consists of images with at least two and often dozens of objects. Both datasets represent a significant step-up in complexity to what object-centric models have been tested on so far. In App. B.2, we also report preliminary results on the KITTI driving dataset.\n\nTraining Details We train DINOSAUR using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 4·10−4, linear learning rate warm-up of 10 000 optimization steps and an exponentially decaying learning rate schedule. Further, we clip the gradient norm at 1 in order to stabilize training and train for 500k steps for the MOVI and COCO datasets and 250k steps for PASCAL VOC. The models were trained on 8 NVIDIA V100 GPUs with a local batch size of 8, with 16-bit mixed precision. For the experiments on synthetic data, we use a ViT with patch size 8 and the MLP decoder. For the experiments on real-world data, we use a ViT with patch size 16 and the Transformer decoder. We analyze the impact of different decoders in Sec. 4.3. The main results are averaged over 5 random seeds; other experiments use 3 seeds. Further implementation details can be found in App. E.1.\n\n4.1 COMPARISON TO OBJECT-CENTRIC LEARNING METHODS\n\nOur goal in this section is two-fold: 1) demonstrating that previous object-centric methods fail to produce meaningful results on real-world datasets and struggle even on synthetic datasets, and 2) showcase how our approach of incorporating strong pre-trained models results in a large step forward for object-centric models on both kinds of datasets.\n\nTasks We evaluate on the task object-centric models are most frequently tested on: object discovery (Burgess et al., 2019), that is, producing a set of masks that cover the independent objects appearing on an image. We also present preliminary results testing the quality of the learned representations on the COCO dataset in App. B.3, though this is not the main focus of our work.\n\nMetrics As common in the object-centric literature, we evaluate this task using foreground adjusted rand index (FG-ARI), a metric measuring cluster similarity. Additionally, we compute a metric based on intersection-over-union (IoU), the mean best overlap (mBO) (Pont-Tuset et al., 2017). mBO is computed by assigning each ground truth mask the predicted mask with the largest overlap, and then averaging the IoUs of the assigned mask pairs. In contrast to ARI, mBO takes background pixels into account, thus also measuring how close masks fit to objects. On datasets where objects have a semantic label attached (e.g. on COCO), we can evaluate this metric with instance-level (i.e. object) masks, and semantic-level (i.e. class) masks. This allows us to find model preferences towards instance- or semantic-level groupings.\n\nBaselines We compare our approach to a more powerful version of Slot Attention (Locatello et al., 2020) based on a ResNet encoder that has been shown to scale to more complex data (Elsayed et al., 2022). Further, we compare with SLATE (Singh et al., 2022a), a recent object-centric model that trains a discrete VQ-VAE (van den Oord et al., 2017) as the feature extractor and a Transformer as the decoder. We refer to App. E.2 for details about baseline configurations.\n\nAs it can be hard to gauge how well object-centric methods perform on new datasets solely from metrics, we add one trivial baseline: dividing the image into a set of regular blocks. These block masks\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nMOVi-C\n\nMOVi-E\n\ne g\na\n\nm\n\nI\n\nR U\nA S\nO N\nI D\n\nE T\nA L\nS\n\nn o\n\ni t\n\nn e\nt t\n\nA\n\nt\n\no\n\nl\n\nS\n\nFigure 2: Example results on the synthetic MOVi-C and MOVi-E datasets (Greff et al., 2022).\n\nFigure 3: Object Discovery on synthetic datasets (mean ± standard dev., 5 seeds) with 11 (MOVi-C) and 24 slots (MOVi-E). We report foreground adjusted rand index (FG-ARI) and mean best overlap (mBO). DINOSAUR uses a ViT-B/8 encoder with the MLP decoder.\n\n(see Fig. 18) thus show the performance of a method that only follows a geometric strategy to group the data, completely ignoring the semantic aspects of the image. Familiar to practitioners, this is a common failure mode of object-centric methods, particularly of Slot Attention. Last, we apply the KMeans algorithm on the DINO features and use the resulting clustering to generate spatial masks. This baseline shows to which extent objects are already trivially extractable from self-supervised features.\n\nResults on Synthetic Datasets (Fig. 2 and Fig. 3) Both Slot Attention and SLATE struggle on the challenging MOVi datasets, performing similar to the naive block masks and worse than the K-Means baselines. Our model achieves good performance on both MOVI-C and MOVi-E. In App. B.1, we also find that our method compares favorably to video methods that can use temporal information and/or weak supervision (Elsayed et al., 2022; Singh et al., 2022b)\n\nResults on Real-World Datasets (Fig. 4 and Fig. 5) As expected, Slot Attention can not handle the increased complexity of real-world data and degrades to non-semantic grouping patterns. For SLATE, semantic grouping begins to emerge (e.g. of backgrounds), but not consistently; it still performs worse than the K-Means baseline. Note that it was necessary to early-stop SLATE’s training as performance would degrade to Slot Attention’s level with more training. In contrast, DINOSAUR captures a variety of objects of different size, appearance and shape. To the best of our knowledge, we are the first to show a successful version of an object-centric model on unconstrained real-world images in the fully unsupervised setting. Our result represents a significant step-up in complexity of what object-centric\n\n6\n\nFG-ARImBO0204060MOVi-CFG-ARImBO0204060MOVi-EBlock MasksDINO K-MeansSlot AttentionSLATEDINOSAURPublished as a conference paper at ICLR 2023\n\nh\n\nt\n\nu r\n\nT\n\nd n\nu o\nr\n\nG\n\nR U\nA S\nO N\nI D\n\nE T\nA L\nS\n\nn o\n\ni t\n\nn e\nt t\n\nA\n\nt\n\no\n\nl\n\nS\n\nFigure 4: Example reults on COCO 2017, using 7 slots. Additional examples are provided in App. G.\n\nFigure 5: Object Discovery on real-world datasets (mean ± standard dev., 5 seeds) with 6 (PASCAL) and 7 slots (COCO). We report foreground adjusted rand index (FG-ARI) and instance/class mean best overlap (mBOi/mBOc). DINOSAUR uses a ViT-B/16 encoder with the Transformer decoder.\n\nmethods can handle. Note that the examples in Fig. 4 show mostly semantic rather than instance grouping emerging: this is a by-product of using the Transformer decoder. In contrast, the MLP decoder is biased towards instance grouping, an effect which we analyze in Sec. 4.3.\n\n4.2 COMPARISON TO COMPUTER VISION METHODS\n\nIn this section, our goal is to show that our method fares well on two tasks closely related to object discovery from the computer vision literature: unsupervised object localization and segmentation. Being competitive on these benchmarks is difficult, as there has been a stream of methods with quickly improving results recently (Wang et al., 2022; Hamilton et al., 2022; Zadaianchuk et al., 2023). Due to space issues, we defer most of the discussion to App. C.\n\nTasks, Metrics and Baselines We briefly introduce the two tasks: in object localization2, the goal is to find object location and size by predicting bounding boxes, and in unsupervised semantic segmentation the goal is to separate the image into semantically consistent labeled regions. For the latter, we consider two variations: object segmentation, where only foreground objects should get segmented and labeled, and scene decomposition, where each pixel of the image has to be labeled with a semantic class. We evaluate object localization in terms the fraction of images on which at least one object was correctly localized (CorLoc) (Vo et al., 2020), and semantic segmentation in terms of mean intersection-over-union over classes (mIoU). For semantic segmentation, we obtain class\n\n2This task is often called “object discovery” in the literature as well, but we term it “object localization” in\n\nthis work in order to avoid confusion with the task evaluated in the object-centric literature.\n\n7\n\nFG-ARImBOimBOc1020304050PASCAL VOC 2012FG-ARImBOimBOc010203040COCOBlock MasksDINO K-MeansSlot AttentionSLATEDINOSAURPublished as a conference paper at ICLR 2023\n\nTable 1: Representative comparisons on three tasks from the computer vision literature. We refer to App. C for a detailed discussion including more datasets, baselines, and metrics. Here, we compare with (a) DeepSpectral (Melas-Kyriazi et al., 2022) and TokenCut (Wang et al., 2022), (b) MaskContrast (Van Gansbeke et al., 2021) and COMUS (Zadaianchuk et al., 2023), and (c) SlotCon (Wen et al., 2022) and STEGO (Hamilton et al., 2022). DINOSAUR uses a ViT-B/16 encoder with the Transformer decoder (mean ± standard dev., 5 seeds).\n\n(a) Unsup. Object Localization.\n\n(b) Unsup. Object Segmentation.\n\n(c) Unsup. Scene Decomposition.\n\nCOCO-20k (CorLoc)\n\nPASCAL VOC 2012 (mIoU)\n\nCOCO-Stuff 27 (mIoU)\n\nDeepSpectral TokenCut\n\n52.2 58.8\n\nMaskContrast COMUS\n\n35.0 50.0\n\nSlotCon STEGO\n\n18.3 26.8\n\nDINOSAUR\n\n67.2 ±1.5\n\nDINOSAUR\n\n37.2 ±1.8\n\nDINOSAUR\n\n24.0 ±0.9\n\nlabels by running K-Means clustering on features associated with each slot after training the model, then assigning clusters to ground truth classes by maximizing IoU using Hungarian matching, similar to Van Gansbeke et al. (2021) (see App. C for details). On each task, we compare with the current state-of-the-art (Wang et al., 2022; Hamilton et al., 2022; Zadaianchuk et al., 2023), and a recent, but competitive method (Van Gansbeke et al., 2021; Melas-Kyriazi et al., 2022; Wen et al., 2022).\n\nResults (Table 1) For object localization, our method reaches comparable results to what has been previously reported. For object segmentation, our method falls behind the state-of-the-art, though it is still competitive with other recent work. Note that the best methods on this task employ additional steps of training segmentation networks which improves results and allows them to run at the original image resolution. In contrast, the masks we evaluate are only of size 14 × 14; we leave it to future work to improve the resolution of the produced masks. For the task of scene decomposition, DINOSAUR comes close to the current state-of-the-art. All in all, our method is competitive with often more involved methods on these benchmarks, demonstrating a further step towards real-world usefulness of object-centric methods.\n\n4.3 ANALYSIS\n\nIn this section, we analyze different aspects of our approach: the importance of feature reconstruction, the impact of the method for self-supervised pre-training, and the role of the decoder. Additional experiments are included in App. D.\n\nInsufficiency of Image Reconstruction We first test the hypothesis if a scaled-up Slot Attention model trained with image reconstruction could lead to real-world object grouping. Our experiments from Sec. 4.1 already show that a ResNet encoder is not sufficient. We additionally test a ViT-B/16 encoder under different training modes: training from scratch, frozen, or finetuning DINO pre-trained weights. We find that training from scratch results in divergence of the training process, and that both the frozen and finetuning setting fail to yield meaningful objects, resulting in striped mask patterns (see Fig. 12). Thus, even when starting from features that are highly semantic, image reconstruction does not give enough signal towards semantic grouping.\n\nResNet and Pre-Trained ViT Encoders Perform Similar Second, we analyze whether pre-training the encoder plays a crucial role for our method. To do so, we compare ResNet34 encoders trained from scratch with pre-trained ViT encoders, and find that the performance is overall similar (see Table 12). This also suggests that the feature reconstruction signal is the key component in our approach that allows object-centricness to emerge on real-world data. We expand in App. D.2.\n\nChoice of Self-Supervised Targets (Table 2) We now analyze the role of the self-supervised pre-training algorithm. To this end, we train DINOSAUR with a ResNet34 encoder (from scratch) on COCO, but reconstruct targets obtained from ViTs pre-trained with different methods: DINO, MoCo-v3, MSN, and MAE. Remarkably, all self-supervised schemes perform well for the task of object discovery (examples in Fig. 24). This demonstrates that self-supervised pre-training on ImageNet translates into a useful, general bias for discovering objects.\n\nChoice of Decoder (Table 3) We compare the choice of MLP vs. Transformer decoder for object discovery. Both options use a ViT-B/16 encoder. Generally, we find that the MLP decoder is better on ARI whereas the Transformer decoder is better on mBO. For MOVi-C, visual inspection (see Fig. 19)\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Comparing self-supervised reconstruction targets produced by a ViT-B/16 on COCO object discovery, with a ResNet34 encoder and the MLP decoder.\n\nAlgorithm\n\nFG-ARI\n\nmBOi\n\nmBOc\n\nDINO MoCo-v3 MSN MAE\n\n40.9 ±0.2 40.4 ±0.6 40.7 ±0.3 37.7 ±0.1\n\n27.9 ±0.0 28.1 ±0.2 27.6 ±0.1 28.1 ±0.1\n\n31.1 ±0.1 31.1 ±0.1 30.7 ±0.1 31.7 ±0.0\n\nTable 3: Comparing different decoders on object discovery, with a ViT-B/16 encoder. We also list mean squared reconstruction error (MSE).\n\nDataset\n\nDecoder\n\nARI mBO(i,c) MSE\n\nMOVi-C\n\nPASCAL\n\nCOCO\n\nMLP Transformer\n\nMLP Transformer\n\nMLP Transformer\n\n66.0 55.7\n\n24.6 24.8\n\n40.5 34.1\n\n35.0 42.4\n\n39.5 40.9 44.0 51.2\n\n27.7 30.9 31.6 39.7\n\n0.24 0.14\n\n0.33 0.17\n\n0.31 0.16\n\nP L\nM\n\nr e\nd o\nc e\nD\n\nr e\n\nr e\nd o\nc e\nD\n\nm\n\nr o\nf s\nn a\nr\n\nT\n\nFigure 6: Sensitivity to number of slots on COCO object discovery (see also App. D.3).\n\nFigure 7: MLP and Transformer decoder have different biases in how they group objects.\n\nshows that the Transformer tends to produce tighter masks and a cleaner background separation, but uses excess slots to split objects. For PASCAL and COCO, what’s striking is the Transformer decoders’ improvement of 9–10 class mBO. This reveals that the Transformer decoder is biased towards grouping semantically related instances into the same slot, which we suggest stems from its global view on the image, but also from the generally increased expressiveness of the architecture (cf. the lower reconstruction loss). In contrast, the MLP decoder is able to separate instances better (see Fig. 7 and also Fig. 23), which is reflected in the higher ARI scores. Researching how different decoder designs affect semantic vs. instance-level grouping is an interesting avenue for future work. In App. D.4 and App. D.5, we further study different decoder properties.\n\n5 CONCLUSION\n\nWe presented the first image-based fully unsupervised approach for object-centric learning that scales to real-world data. Our experiments demonstrate significant improvements on both simulated and real-world data compared to previously suggested approaches and even achieve competitive performance with more involved pipeline methods from the computer vision literature.\n\nThis work only takes a first step towards the goal of representing the world in terms of objects. As such, some problems remain open. One issue concerns semantic vs. instance-level grouping. As evident from the presented examples, our approach covers a mix of both, with semantically related objects sometimes being grouped into a single slot. While we found the type of decoder to influence this behavior, more fine-grained control is needed. A related issue is the detail of the decomposition, e.g. whether objects are split into parts or stay whole. We found this to be dependent on the number of slots, with a fixed number often being inappropriate (see Fig. 6 and App. D.3). How models can dynamically choose a suitable level of detail while staying unsupervised but controllable will be an important challenge to fully master the ambiguities the real world inherently presents.\n\nIn this work, we mainly focused on object discovery. Future work could further examine the properties of the learned slot representations, for instance robustness to distribution shifts, generalization and usefulness for downstream tasks. Another interesting direction is how our approach can be combined with image generation to build flexible and compositional generative models of natural data.\n\n9\n\n5101520Slots10203040FG-ARIBlock MasksDINO K-MeansSlot AttentionSLATEDINOSAURPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nAppendix E.1 contains detailed information about the DINOSAUR architecture and all hyperparamers used for all experiments. Appendix E.2 contains details about how baselines were trained. Appendix F contains information about task evaluation and datasets. All datasets used in this work (MOVi, PASCAL VOC 2012, COCO, KITTI) are public and can be obtained on their respective web pages. Source code will be made available under https://github.com/amazon-science/ object-centric-learning-framework.\n\nREFERENCES\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a Visual Language Model for Few-Shot Learning. In NeurIPS, 2022. URL https://openreview. net/forum?id=EbMuimAbPbs.\n\nHassan Alhaija, Siva Mustikovela, Lars Mescheder, Andreas Geiger, and Carsten Rother. Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes. International Journal of Computer Vision, 2018. URL https://arxiv.org/abs/1708.01566.\n\nRim Assouel, Pau Rodriguez, Perouz Taslakian, David Vazquez, and Yoshua Bengio. Object-centric Compositional Imagination for Visual Abstract Reasoning. In ICLR Workshop on the Elements of Reasoning: Objects, Structure and Causality, 2022. URL https://openreview.net/ forum?id=rCzfIruU5x5.\n\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael G. Rabbat, and Nicolas Ballas. Masked Siamese Networks for LabelEfficient Learning. In ECCV, 2022. URL https://arxiv.org/abs/2204.07141.\n\nZhipeng Bao, Pavel Tokmakov, Allan Jabri, Yu-Xiong Wang, Adrien Gaidon, and Martial Hebert. Discovering Objects that Can Move. CVPR, 2022. URL https://arxiv.org/abs/2203. 10159.\n\nOndrej Biza, Robert Platt, Jan-Willem van de Meent, Lawson LS Wong, and Thomas Kipf. BindIn ICLR Workshop on the Elements of Reasoning: ing Actions to Objects in World Models. Objects, Structure and Causality, 2022. URL https://openreview.net/forum?id= HImz8BuUclc.\n\nChristopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. MONet: Unsupervised Scene Decomposition and Representation. arXiv:1901.11390, 2019. URL https://arxiv.org/abs/1901.11390.\n\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and Stuff Classes in Context.\n\nIn CVPR, 2018. URL https://arxiv.org/abs/1612.03716.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging Properties in Self-Supervised Vision Transformers. ICCV, 2021. URL https://arxiv.org/abs/2104.14294.\n\nMark Chen, Alec Radford, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative Pretraining from Pixels. In ICML, 2020a. URL https://proceedings.mlr. press/v119/chen20s.html.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple FrameIn ICML, 2020b. URL https:\n\nwork for Contrastive Learning of Visual Representations. //proceedings.mlr.press/v119/chen20j.html.\n\nXinlei Chen, Saining Xie, and Kaiming He. An Empirical Study of Training Self-Supervised Vision\n\nTransformers. ICCV, 2021. URL https://arxiv.org/abs/2104.02057.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. PiCIE: Unsupervised Semantic Segmentation Using Invariance and Equivariance in Clustering. In CVPR, 2021. URL https: //arxiv.org/abs/2103.17070.\n\nKyunghyun Cho, Bart van Merrienboer, C ̧ aglar G ̈ulc ̧ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In EMNLP, 2014. URL https://arxiv.org/abs/1406. 1078.\n\nMinsu Cho, Suha Kwak, Cordelia Schmid, and Jean Ponce. Unsupervised Object Discovery and Localization in the Wild: Part-based Matching with Bottom-up Region Proposals. In CVPR, 2015. URL https://arxiv.org/abs/1501.06170.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\n\nImageNet: A Largescale Hierarchical Image Database. In CVPR, 2009. doi: 10.1109/CVPR.2009.5206848. URL https://ieeexplore.ieee.org/document/5206848.\n\nAndrea Dittadi, Samuele Papa, Michele De Vita, Bernhard Sch ̈olkopf, Ole Winther, and Francesco Locatello. Generalization and Robustness Implications in Object-Centric Learning. In ICML, 2022. URL https://proceedings.mlr.press/v162/dittadi22a.html.\n\nAlexey Dosovitskiy and Thomas Brox. Generating Images with Perceptual Similarity Metrics based on Deep Networks. In NeurIPS, 2016. URL https://proceedings.neurips.cc/ paper/2016/hash/371bce7dc83817b7893bcdeed13799b5-Abstract.html.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\n\nGamaleldin Fathy Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael Curtis Mozer, and Thomas Kipf. SAVi++: Towards End-to-End Object-Centric Learning from Real-World Videos. In NeurIPS, 2022. URL https://openreview.net/forum?id=fT9W53lLxNS.\n\nMartin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations. In ICLR, 2020. URL https://openreview.net/forum?id=BkxfaTVFwH.\n\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement. In NeurIPS, 2021. URL https://openreview. net/forum?id=nRBZWEUhIhW.\n\nS. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, and Geoffrey E. Hinton. Attend, Infer, Repeat: Fast Scene Understanding with Generative Models. In NeurIPS, 2016. URL https://proceedings.neurips.cc/paper/ 2016/hash/52947e0ade57a09e4a1386d08f17b656-Abstract.html.\n\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012), 2012. URL http://www.pascal-network. org/challenges/VOC/voc2012/workshop/index.html.\n\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets Robotics: The International Journal of Robotics Research, 2013. URL https://www.\n\nKITTI Dataset. cvlibs.net/publications/Geiger2013IJRR.pdf.\n\nAnirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Sch ̈olkopf. Recurrent Independent Mechanisms. In ICLR, 2021. URL https: //openreview.net/forum?id=mLcmdlEUxy-.\n\nKlaus Greff, Rapha ̈el Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-Object Representation Learning with Iterative Variational Inference. In ICML, 2019. URL https://arxiv.org/ abs/1903.00450.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nKlaus Greff, Sjoerd Van Steenkiste, and J ̈urgen Schmidhuber. On the Binding Problem in Artificial Neural Networks. arXiv:2012.05208, 2020. URL https://arxiv.org/abs/2012. 05208.\n\nKlaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: A Scalable Dataset Generator. In CVPR, 2022. URL https://arxiv.org/abs/2203.03570.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap Your Own LaIn NeurIPS, 2020. URL https: tent - A New Approach to Self-Supervised Learning. //arxiv.org/abs/2006.07733.\n\nMark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised Semantic Segmentation by Distilling Feature Correspondences. In ICLR, 2022. URL https://openreview.net/forum?id=SaKO6z6Hl0c.\n\nBharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic Contours from Inverse Detectors. In ICCV, 2011. URL https://ieeexplore.ieee.org/ document/6126343.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\n\nRecognition. In CVPR, 2015. URL https://arxiv.org/abs/1512.03385.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked Autoencoders are Scalable Vision Learners. In CVPR, 2022. URL https://arxiv.org/ abs/2111.06377.\n\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the Knowledge in a Neural Network. In NeurIPS 2014 Deep Learning Workshop, 2015. URL https://arxiv.org/abs/1503. 02531.\n\nHaiyang Huang, Zhi Chen, and Cynthia Rudin. SegDiscover: Visual Concept Discovery via Unsupervised Semantic Segmentation. arXiv:2204.10926, 2022. URL https://arxiv.org/abs/ 2204.10926.\n\nOlivier J. H ́enaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisserman, Jo ̃ao Carreira, and Relja Arandjelovi’c. Object Discovery and Representation Networks. In ECCV, 2022. URL https://arxiv.org/abs/2203.08777.\n\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: A General Architecture for Structured Inputs & Outputs. In ICLR, 2022. URL https://openreview.net/ forum?id=fILj7WpI-g.\n\nXu Ji, Jo ̃ao F. Henriques, and Andrea Vedaldi. Invariant Information Clustering for Unsupervised Image Classification and Segmentation. In ICCV, 2019. URL https://arxiv.org/abs/ 1807.06653.\n\nJindong Jiang, Sepehr Janghorbani, Gerard de Melo, and Sungjin Ahn. SCALOR: Generative World Models with Scalable Object Representations. In ICLR, 2020. URL https://openreview. net/pdf?id=SJxrKgStDH.\n\nDaniel Kahneman, Anne Treisman, and Brian J. Gibbs. The Reviewing of Object Files: Objectspecific Integration of Information. Cognitive psychology, 1992. URL https://www. sciencedirect.com/science/article/abs/pii/001002859290007O.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nLaurynas Karazija, Iro Laina, and Christian Rupprecht. ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-Object Segmentation. In NeurIPS Track on Datasets and Benchmarks, 2021. URL https://arxiv.org/abs/2111.10265.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015.\n\nURL https://arxiv.org/abs/1412.6980.\n\nThomas Kipf, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional Object-centric In ICLR, 2022. URL https://openreview.net/forum?id= Learning from Video. aD7uesX1GF_.\n\nAdam Kosiorek, Hyunjik Kim, Yee Whye Teh,\n\nAttend, 2018. 7417744a2bac776fabe5a09b21c707a2-Abstract.html.\n\nSequential In NeurIPS, https://proceedings.neurips.cc/paper/2018/hash/\n\nGenerative Modelling of Moving Objects.\n\nand Ingmar Posner.\n\nInfer, Repeat:\n\nURL\n\nHarold W. Kuhn. The Hungarian Method for the Assignment Problem. Naval Research Logistics Quarterly, 1955. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/ nav.3800020109.\n\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In ECCV, 2014. URL https://arxiv.org/abs/1405.0312.\n\nZhixuan Lin, Yi-Wu Fu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition. In ICLR, 2020. URL https://openreview.net/forum? id=rkl03ySYDH.\n\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-Centric Learning with Slot Attention. In NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/ file/8511df98c02ab60aea1b2356c013bc0f-Paper.pdf.\n\nDavide Mambelli, Frederik Tr ̈auble, Stefan Bauer, Bernhard Sch ̈olkopf, and Francesco Locatello. Compositional Multi-object Reinforcement Learning with Linear Relation Networks. In ICLR Workshop on the Elements of Reasoning: Objects, Structure and Causality, 2022. URL https: //openreview.net/forum?id=HFUxPr_I5ec.\n\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization. In CVPR, 2022. URL https://arxiv.org/abs/2205.07839.\n\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The Role of Context for Object Detection and Semantic Segmentation in the Wild. In CVPR, 2014. URL https://ieeexplore.ieee.org/document/ 6909514.\n\nYassine Ouali, C ́eline Hudelot, and Myriam Tami. Autoregressive Unsupervised Image Segmentation.\n\nIn ECCV, 2020. URL https://arxiv.org/abs/2007.08247.\n\nJordi Pont-Tuset, Pablo Arbel ́aez, Jonathan T. Barron, Ferran Marques, and Jitendra Malik. Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(1), 2017. doi: 10.1109/TPAMI. 2016.2537320. URL https://ieeexplore.ieee.org/document/7423791.\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio G ́omez Colmenarejo, Alexander Novikov, Barth maron Gabriel, Mai Gim ́enez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A Generalist Agent. TMLR, 2022. URL https://openreview.net/forum?id=1ikK0kHjvj.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nBernhard Sch ̈olkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Towards Causal Representation Learning. IEEE - Advances in Machine Learning and Deep Neural Networks, 2021. URL https://arxiv.org/abs/ 2102.11107.\n\nOriane Simeoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick P ́erez, Renaud Marlet, and Jean Ponce. Localizing Objects with Self-Supervised Transformers and no Labels. In BMVC, 2021. URL https://arxiv.org/abs/2109.14279.\n\nGautam Singh, Fei Deng, and Sungjin Ahn. Illiterate DALL-E Learns to Compose. In ICLR, 2022a.\n\nURL https://openreview.net/forum?id=h0OYV0We3oh.\n\nGautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos. In NeurIPS, 2022b. URL https://openreview.net/ forum?id=eYfIM88MTUE.\n\nElizabeth S. Spelke and Katherine D. Kinzler.\n\nDevelopmental Science, 2007. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j. 1467-7687.2007.00569.x.\n\nCore Knowledge.\n\nManuel Traub, Sebastian Otte, Tobias Menge, Matthias Karlbauer, Jannik Thuemmel, and Martin V. Butz. Learning What and Where: Disentangling Location and Identity Tracking Without Supervision. In ICLR, 2023. URL https://openreview.net/forum?id=NeDc-Ak-H_.\n\nA ̈aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning. In NeurIPS, 2017. URL https://proceedings.neurips.cc/paper/2017/ hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html.\n\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool. Unsupervised ICCV, 2021. URL https:\n\nSemantic Segmentation by Contrasting Object Mask Proposals. //arxiv.org/abs/2102.06191.\n\nWouter Van Gansbeke, Simon Vandenhende, and Luc Van Gool. Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation. arXiv:2206.06363, 2022. URL https: //arxiv.org/abs/2206.06363.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. In https://papers.nips.cc/paper/2017/hash/\n\nGomez, Lukasz Kaiser, and Illia Polosukhin. NeurIPS, 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nAttention is All you Need.\n\n2017.\n\nURL\n\nHuy V. Vo, Patrick P ́erez, and Jean Ponce. Toward Unsupervised, Multi-object Discovery in Largescale Image Collections. In ECCV, 2020. URL https://arxiv.org/abs/2007.02662.\n\nHuy V. Vo, Elena Sizikova, Cordelia Schmid, Patrick P ́erez, and Jean Ponce. Large-scale Unsupervised Object Discovery. In NeurIPS, 2021. URL https://arxiv.org/abs/2106.06650.\n\nYangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L. Crowley, and Dominique Vaufreydaz. Self-Supervised Transformers for Unsupervised Object Discovery Using Normalized Cut. In CVPR, 2022. URL https://arxiv.org/abs/2202.11539.\n\nNick Watters, Loic Matthey, Chris P. Burgess, and Alexander Lerchner. Spatial Broadcast Decoder: A Simple Architecture for Disentangled Representations in VAEs. In ICLR Learning from Limited Labeled Data Workshop, 2019. URL https://openreview.net/forum? id=S1x7WjnzdV.\n\nMarissa A Weis, Kashyap Chitta, Yash Sharma, Wieland Brendel, Matthias Bethge, Andreas Geiger, and Alexander S. Ecker. Benchmarking Unsupervised Object Representations for Video Sequences. JMLR, 2021. URL https://jmlr.org/papers/v22/21-0199.html.\n\nXin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, and Xiaojuan Qi. Self-Supervised In NeurIPS, 2022. URL https:\n\nVisual Representation Learning with Semantic Grouping. //openreview.net/forum?id=H3JObxjd8S.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nRoss Wightman. PyTorch Image Models, 2019. URL https://github.com/rwightman/\n\npytorch-image-models.\n\nJiahao Xie, Xiaohang Zhan, Ziwei Liu, Yew-Soon Ong, and Chen Loy Change. Unsupervised Object-Level Representation Learning from Scene Images. In NeurIPS, 2021. URL https: //openreview.net/forum?id=X2K8KVEaAXG.\n\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On Layer Normalization in the Transformer Architecture. In ICML, 2020. URL https://proceedings.mlr.press/v119/xiong20b. html.\n\nJiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT: Semantic Segmentation Emerges from Text Supervision. In CVPR, 2022. URL https://arxiv.org/abs/2202.11094.\n\nJianwei Yang, Jiayuan Mao, Jiajun Wu, Devi Parikh, David D Cox, Joshua B. Tenenbaum, and Chuang Gan. Object-centric Diagnosis of Visual Reasoning. arXiv:2012.11587, 2020. URL https://arxiv.org/abs/2012.11587.\n\nYafei Yang and Bo Yang. Promising or Elusive? Unsupervised Object Segmentation from Realworld Single Images. In NeurIPS, 2022. URL https://openreview.net/forum?id= DzPWTwfby5d.\n\nAndrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Self-supervised Visual Reinforcement Learning with Object-centric Representations. In ICLR, 2020. URL https://openreview. net/forum?id=xppLmXCbOw1.\n\nAndrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu, Francesco Locatello, and Thomas Brox. Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations. In ICLR, 2023. URL https://openreview.net/forum?id=1_jFneF07YC.\n\n15\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n\n17\n\n18\n\n18 18 19\n\n20\n\n20 21 22\n\n24\n\n24 25 25 28 28 29 30\n\n30\n\n30 33\n\n34\n\n36\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nTable of Contents\n\nA Extended Related Work\n\nB Expanded Comparison with Object-Centric Methods\n\nB.1 Comparison to Video Methods .\nB.2 Results on the KITTI Driving Dataset . B.3 Object Property Prediction on COCO .\n\n.\n\n.\n\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\nC Expanded Comparison with Computer Vision Methods\n\n. C.1 Unsupervised Object Localization . .\nC.2 Unsupervised Semantic Segmentation . .\nC.3 Unsupervised Semantic Segmentation Baseline (STEGO) on MOVi\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n.\n\nD Additional Analysis\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n. .\n. .\n.\n\n. .\n\n. .\n. .\n. .\n.\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. D.1 Choice of Pre-Training Method . .\nD.2 Comparison of ResNet and Pre-Trained ViT Encoders D.3 Number of Slots Analysis . .\nD.4 How Does Decoder Scale Interact With Object Discovery? . D.5 The Transformer Decoder Performs Poor with Larger ViTs . .\n. D.6 Mask Type Analysis . D.7 Does DINOSAUR Perform Semantic Grouping on MOVi? .\n\n. .\n.\n\n. .\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nE Implementation Details\n\nE.1 Architecture and Hyperparameters .\n. E.2 Baselines .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nF Dataset and Evaluation Settings\n\nG Additional Examples\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nA EXTENDED RELATED WORK\n\nUnsupervised Semantic Segmentation is the challenging task of structuring natural scene into semantically coherent regions without any human annotations. Methods for this task (Ji et al., 2019; Ouali et al., 2020; Cho et al., 2021; Van Gansbeke et al., 2021; 2022; Hamilton et al., 2022; Zadaianchuk et al., 2023) are typically separated into two independent steps: first, representations for each pixel by a pre-trained self-supervised model are refined; second, these representations are clustered on the whole dataset to assign a class category to each pixel. DINOSAUR combines those steps into a simple method that groups semantically meaningful regions jointly with learning region representations.\n\nApproaches that try to learn and cluster dense representations without additional assumptions (Ji et al., 2019; Ouali et al., 2020) were shown to be effective only on small-scale datasets with narrow visual domains. More advanced methods incorporate geometric consistency (Cho et al., 2021) or unsupervised saliency detection (Van Gansbeke et al., 2022; Zadaianchuk et al., 2023) to simplify dense representation learning and the grouping task. Additionally, using self-supervised DINO representations (Caron et al., 2021) by contrasting them (Hamilton et al., 2022) or to self-train a semantic segmentation model (Van Gansbeke et al., 2022; Melas-Kyriazi et al., 2022; Zadaianchuk et al., 2023) allows obtaining a more accurate image decomposition to semantic categories.\n\nObject Localization is another research direction that aims at structuring the scene from images, but by predicting object-level features such as bounding boxes or category. Recent works (Vo et al., 2020; 2021; Simeoni et al., 2021; Melas-Kyriazi et al., 2022; Wang et al., 2022) often group pre-trained, self-supervised features of individual images into initial object proposals, and then aggregate and cluster these proposals across images to assign them labels. Self-training can then be used on top to achieve multi-object detection on natural scenes.\n\nAt first, rOSD (Vo et al., 2020; 2021) used features obtained from a supervised classifier to localize salient objects in the image. Next, LOST (Simeoni et al., 2021) showed that dense, self-supervised DINO features can also be used for localization of the most prominent object in the image. Finally, DeepSpectral (Melas-Kyriazi et al., 2022) and TokenCut (Wang et al., 2022) showed that additionally transforming DINO features to a spectral embedding can significantly improve the quality of the object proposals. While those methods can be combined with self-training of object detectors like R-CNN, they are limited by the quality of the set of salient objects used as pseudo-labels. In contrast, DINOSAUR splits each scene to a set of object proposals enabling multi-object scene decomposition without an additional recognition network.\n\nArchitectures making use of slot-like components have also enjoyed popularity aside from the research explicitly focused on object-centric learning. For instance, the Perceiver model learns to distill a large set of input tokens from different modalities into a reduced set of embeddings, which can be seen as slots (Jaegle et al., 2022). Prominent recent successes of large-scale multi-modal modeling such as Flamingo (Alayrac et al., 2022) or Gato (Reed et al., 2022) are based on the Perceiver architecture. In dynamics modeling, recurrent independent mechanisms (Goyal et al., 2021) use a competition between independent modules to read inputs into a slot-structured latent state.\n\nRecent work on contrastive learning such as SlotCon (Wen et al., 2022) or Odin (H ́enaff et al., 2022) also can be seen as learning slot representations, although with a focus on semantic (class-based) slots rather than instances. In the case of SlotCon, this is due to the reliance on prototypes which naturally lead to semantically focused representations whereas in the case of Odin, the contrastive objective does not encourage differentiation of one or multiple instances. Further, Odin is positioned as a strategy to pre-train a backbone which can then be finetuned on downstream tasks and is not intended to be an unsupervised method that directly leads to instance-level grouping. Note that even though Odin also evaluates object discovery on COCO using the mBO metric, the obtained numbers are not comparable to ours as Odin generates 255 proposal masks by running K-Means 128 times with different numbers of clusters. Finally, ORL (Xie et al., 2021) addresses the reliance of many contrastive methods on ImageNet-specific biases, which leads to intra-image contrasting of the same object under different augmentations. This is done by identifying correspondences of objects in different images using models trained via intra-image contrasting and subsequently training the model using inter-image contrasting by selecting objects of the same type from different images.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Object discovery on synthetic datasets (mean ± standard dev., 5 seeds), corresponding to results from Fig. 3. We additionally add results (over 3 seeds) for the video methods STEVE (Singh et al., 2022b) and SAVi++ (Elsayed et al., 2022). Note that this version of SAVi++ is unconditional, i.e. it does not have access to the objects of the first frame. We train SAVi++ with (unsupervised) image reconstruction, and (supervised) optical flow reconstruction. To ensure a valid comparison, STEVE and SAVi++ are trained on video, but evaluated by processing each video frame independently.\n\nMOVi-C\n\nMOVi-E\n\nFG-ARI\n\nmBO\n\nFG-ARI\n\nmBO\n\nBlock Masks DINO K-Means Slot Attention SLATE DINOSAUR (ViT-S/8) DINOSAUR (ViT-B/8)\n\nSTEVE SAVi++ (Image Recon.) SAVi++ (Flow Recon.)\n\n43.8 48.9 43.8 ±0.3 43.6 ±1.3 67.2 ±0.3 68.6 ±0.4\n\n44.3 ±0.8 10.4 ±0.1 30.0 ±1.5\n\n27.5 38.5 26.2 ±1.0 26.5 ±1.1 38.6 ±0.1 39.1 ±0.2\n\n24.3 23.6 24.0 ±1.2 23.6 ±0.5 34.1 ±0.1 35.5 ±0.2\n\n45.4 50.2 45.0 ±1.7 44.4 ±0.9 64.7 ±0.7 65.1 ±1.2\n\n44.5 ±2.8 15.9 ±1.1 19.8 ±2.0\n\nB EXPANDED COMPARISON WITH OBJECT-CENTRIC METHODS\n\nIn this section, we present additional results on object discovery. In particular, we compare to two stateof-the-art video-based object-centric methods: STEVE (Singh et al., 2022b) and SAVi++ (Elsayed et al., 2022) (Sec. B.1). We also present preliminary results on the KITTI driving dataset (Alhaija et al., 2018) (Sec. B.2), and a study of the learned slot representations on COCO (Sec. B.3).\n\nB.1 COMPARISON TO VIDEO METHODS\n\nHaving access to motion information is often thought to be advantageous for discovering objects (Greff et al., 2020; Kipf et al., 2022), as object identity and motions are stable through time. It is thus interesting how our method compares against models that have been trained on videos. To this end, we test two recent video-based object-centric: STEVE (Singh et al., 2022b) and SAVi++ (Elsayed et al., 2022). STEVE is an extension of SLATE (Singh et al., 2022a) to videos, whereas SAVi++ is a scaled-up version of SAVi (Kipf et al., 2022), which in turn is the video extension of Slot Attention (Locatello et al., 2020). We train both of them on the synthetic MOVi-C (with 11 slots) and MOVi-E (with 24 slots), using the original hyperparameters proposed for those datasets. However, we train SAVi++ unconditionally, that is, using random learnt slot initializations instead of providing first-frame supervision of the object positions. We also had to reduce the batch size of SAVi++ from 64 to 32 due to computational reasons. STEVE is trained for 200k steps, and SAVi++ for 300k steps. To ensure a valid comparison to the image-based methods, we evaluate FG-ARI on the individual frames of the MOVi datasets, reinitializing slots for each frame.\n\nResults (Table 4) Somewhat surprisingly, the video-based methods do not perform better than the image-based methods. For both STEVE and SAVi++, there is a large gap to our method. STEVE performs similar to its image-based counterpart SLATE. Compared to the results reported by Elsayed et al. (2022), SAVi++ performs considerably worse, falling behind its image counterpart Slot Attention. Even when having access to optical flow supervision, SAVi++ does not produce meaningful object groupings. We hypothesize this is because SAVi++ was not designed for the unconditional setting, and model and/or hyperparameter changes are needed to alleviate this.\n\nB.2 RESULTS ON THE KITTI DRIVING DATASET\n\nWe present prelimary results on KITTI (Alhaija et al., 2018), consisting of real-world videos of city-street driving. In particular, we evaluate on the instance segmentation annotations, containing 200 independently annotated frames. For training, we use all 147 videos by randomly sampling frames from them, resulting in 95 869 training images. We compare with “Discovering Objects that Can Move” (Bao et al., 2022), a recent video-based object-centric method that uses motion masks to\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nsupervise the attention masks. To be consistent, we evaluate at the same mask resolution as them, namely at 1242 × 375 pixels.\n\nBecause of the skewed aspect ratio and high resolution, and because our model is trained on square images, we evaluate in a sliding window fashion. In particular, we partition the image into 4 nonoverlapping windows of size 375 × 375 (adding padding of 129 black pixels to both sides), process each image individually into K masks of size 375 × 375 using our model, then join the masks spatially and removing padding. See Fig. 8 for an example window. We use the resulting 4 · K masks of size 1242 × 375 for evaluation. This kind of sliding window approach has the obvious flaw that objects can not span more than window. We leave it to future work to improve working with high-resolution images.\n\nTuning DINOSAUR to work on KITTI was more difficult than for COCO or PASCAL VOC. For instance, using the Transformer decoder did not result in object groupings, and we had to resort to the MLP decoder with a small slot size of 32. We also had difficulties training a ResNet encoder on KITTI. We hypothesize this is because the KITTI dataset does not have a lot of diversity, and so high-capacity architectures can easily overfit to the expected scene layout instead of learning to discover objects.\n\nTable 5: Object discovery on KITTI (mean ± standard dev., 5 seeds).\n\nMethod\n\nFG-ARI mBO\n\nBao et al. (2022) DINOSAUR\n\n47.1\n\n70.3 ±0.8 19.4 ±0.3\n\nFigure 8: Example masks on KITTI.\n\nResults (Table 5) We train the model with K = 9 slots, yielding 36 slots for evaluation. Our model clearly outperforms Bao et al. (2022)’s method in terms of FG-ARI, even though it is trained with motion supervision. However, there is ample room for improvement: visual inspection shows that while our method manages to capture larger objects in the foreground (mainly cars), the many small objects in the background of the scene are grouped together with non-objects, e.g. houses. This explains why FG-ARI is relatively high (as large objects influence this metric more), while mBO is relatively low (as the metric is averaged over all objects). This hints at another difficulty of real-world data which current object-centric methods are not well equipped to deal with: diversity of object scales.\n\nB.3 OBJECT PROPERTY PREDICTION ON COCO\n\nWe study the usefulness of the learned representations for downstream applications by predicting object properties from the slots on COCO. To this end, we supervise shallow predictors using the slot representations from unsupervised object-centric models. As properties, we use the class, and the xand y-coordinates of each object (center-of-mass).\n\nWe closely follow the experimental design of Dittadi et al. (2022), and did not attempt to tune their settings to optimize for performance. As downstream models, we test a linear predictor, and a one-hidden layer MLP. Each model is applied in parallel to all slots for an image, i.e. it receives a slot as input, and outputs a vector of property predictions for this slot. We use the cross-entropy loss for the categorical class prediction, and the MSE loss for the continuous x- and y-coordinates, where the overall loss is just the sum of the individual losses. We match the predicted set of vectors to the target set of vectors using the Hungarian method (Kuhn, 1955), minimizing the total loss of the assignment. As metrics, we report Top-1 and Top-5 average-class accuracy for the 80-way class prediction and R2 (coefficient of determination) for the x- and y-coordinates.\n\nWe use 10 000 random images from the COCO dataset to train, and another 1 000 random images as a validation set. We report metrics on the 5 000 validation images of COCO. Images that contain no objects due to center cropping are filtered out. As COCO images can contain many objects, but our method typically only uses a low number of slots (e.g. 7), we keep only the 6 largest objects in the image. This way, all ground truth objects can in principle be matched by a slot. We train for 15 000 steps, and use the model with the lowest validation loss for testing.\n\nThe results are listed in Fig. 9. We compare DINOSAUR using a DINO-pretrained ViT-B/16 encoder with a Slot Attention model, all using 7 slots. On class prediction, our method has 25% higher accuracy than Slot Attention. On coordinate prediction, DINOSAUR reaches similar performance to Slot Attention on the x-coordinate, but exceeds it on the y-coordinate, where Slot Attention drops-off.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: Object property prediction on the validation set of COCO, showing mean and standard deviation over 3 seeds. We train a linear predictor (left) and a one-hidden layer MLP (right) to predict object properties from the slot representations. For class prediction, we show Top-1 and Top-5 accuracy. For coordinate prediction, we show R2 (coefficient of determination, higher is better). We compare our method using MLP and Transformer decoders with Slot Attention.\n\nWe also find that on coordinate prediction, the representations from the Transformer decoder are a bit worse than from the MLP decoder. This may be because the Transformer decoder often captures several disconnected objects in one slot, making it more difficult to predict the center-of-mass. Using a non-linear instead of a linear predictor slightly improves accuracy on all properties; using more hidden layers than one did not improve the performance further in our experiments. Overall, we conclude that the representation DINOSAUR learns are more useful than Slot Attention’s for downstream tasks on real-world data. However, our method also still leaves room for improvement. Finally, we note that this experiment only constitutes a first step in evaluating the learned representations, and that further investigations could be conducted, e.g. taking into account robustness to distribution shifts.\n\nC EXPANDED COMPARISON WITH COMPUTER VISION METHODS\n\nIn this section, we extend the comparison to computer vision methods with a more detailed discussion. In addition to the results from Sec. 4.2, we present results on more datasets (PASCAL VOC 2012 for object localization, COCO for object segmentation, COCO-Stuff 171 for scene decomposition), additional metrics (detection rate for object localization, per-pixel accuracy for scene decomposition), and list further baseline methods. Finally, we also run the unsupervised semantic segmentation method STEGO on the MOVi datasets (Sec. C.3).\n\nC.1 UNSUPERVISED OBJECT LOCALIZATION\n\nDatasets and Metrics In the computer vision literature, the task of object localization refers to finding the location of one or more objects that repeatedly appear in a collection of images (Cho et al., 2015). In particular, objects have to be discovered by proposing bounding boxes that “correctly localize” them. An object counts as correctly localized if its bounding box has an IoU-overlap of at least 50% with a proposed bounding box. Like prior work (Vo et al., 2020; Melas-Kyriazi et al., 2022), we report the fraction of images on which at least one object was correctly localized (CorLoc), and the average fraction of objects correctly localized per image (detection rate). As our method produces masks, we use the tightest bounding box around each mask as the proposal. We evaluate on the trainval split of PASCAL VOC 2012 and COCO-20k, as is standard in the literature (Vo et al., 2020; Simeoni et al., 2021). COCO-20k is a subset of COCO 2014 filtered to contain no instances annotated as crowd.\n\nResults (Table 6) The results show that our method manages to focus on the salient parts of the data. On the majority of images, at least one object is captured by a slot. Also, our method reaches results comparable or better to what has been reported in the literature on this task. Interestingly, zero-shot transferring a model trained on COCO to PASCAL yields better results than a model trained on PASCAL itself; we explain this by the size of the training data, which is around 10× larger for COCO.\n\n20\n\nClass(Top-1)Class(Top-5)XY0.00.20.40.60.81.0Accuracy/R2LinearPredictorClass(Top-1)Class(Top-5)XY0.00.20.40.60.81.0MLPPredictorDINOSAUR,MLPDINOSAUR,Transf.SlotAttentionPublished as a conference paper at ICLR 2023\n\nTable 6: Object Localization (mean ± standard dev., 5 seeds). We report CorLoc and detection rate and compare with rOSD (Vo et al., 2020), DINO-Seg (Simeoni et al., 2021), LOST (Simeoni et al., 2021), DeepSpectral (Melas-Kyriazi et al., 2022) and TokenCut (Wang et al., 2022)\n\nPASCAL VOC 2012\n\nCOCO-20k\n\nCorLoc\n\nDetRate\n\nCorLoc DetRate\n\n51.9 46.2 64.0 66.4 72.1\n\n41.2 –\n– –\n–\n\n48.5 42.1 50.7 52.2 58.8\n\n12.0 –\n– –\n–\n\nrOSD DINO-Seg LOST DeepSpectral TokenCut\n\n69.8 ±4.9 52.1 ±3.9 Ours Ours (COCO Transfer) 77.9 ±1.0 58.4 ±0.9\n\n67.2 ±1.5 31.0 ±0.9\n\nGround Truth\n\nDINOSAUR\n\nFigure 10: Object Localization on PASCAL VOC 2012.\n\nTable 7: Unsupervised Semantic Segmentation (mean ± standard dev., 5 seeds). We report mean intersection over union (mIoU). For scene decomposition, we additionally report per-pixel accuracy (pAcc). STEGO results are presented without CRF post-processing for fair comparison with other methods. Results marked † were produced ourselves using the official implementation.\n\n(a) Object Segmentation. We compare with MaskContrast (Van Gansbeke et al., 2021), DeepSpectral (Melas-Kyriazi et al., 2022), MaskDistill (Van Gansbeke et al., 2022), and COMUS (Zadaianchuk et al., 2023).\n\nPASCAL VOC 2012 COCO\n\nMaskContrast DeepSpectral MaskDistill COMUS\n\n35.0 37.2 45.8 50.0\n\n– –\n– 19.6\n\nOurs\n\n37.2 ±1.8\n\n13.9 ±0.4\n\n(b) Scene Decomposition. We compare with IIC (Ji et al., 2019), SegDiscover (Huang et al., 2022), PiCIE (Cho et al., 2021), SlotCon (Wen et al., 2022), and STEGO (Hamilton et al., 2022).\n\nCOCO-Stuff 27\n\nCOCO-Stuff 171\n\nmIoU\n\npAcc\n\nmIoU\n\nIIC SegDiscover PiCIE + H. SlotCon STEGO\n\n6.7 14.3 14.4 18.3 26.8\n\n21.8 56.5 50.0 42.4 54.8\n\n– –\n– –\n10.0†\n\npAcc\n\n– –\n– –\n32.5†\n\nOurs\n\n24.0 ±0.9 44.9 ±1.2 13.1 ±0.3 27.0 ±0.3\n\nC.2 UNSUPERVISED SEMANTIC SEGMENTATION\n\nIn this task, each pixel of the image has to be assigned a semantic class label. As in the unsupervised setting, no direct correspondence to ground truth classes are given, a successful model needs to output segments that consistently cover the same semantic content on different images.\n\nTasks, Metrics and Baselines We report results on two different versions of this task: in object segmentation, only foreground classes have to segmented; in scene decomposition, the background has to be split into semantic categories as well. For object segmentation, we evaluate on PASCAL VOC 2012 and COCO 2017, with instance masks merged to semantic class masks. For scene decomposition, we evaluate on the COCO-Stuff dataset (Caesar et al., 2018), in two different variants: COCO-27 uses a reduced set of 27 coarse classes from the COCO class hierarchy, whereas COCO-171 uses the full set of 80 foreground and 91 background classes available in the dataset. To the best of our knowledge, we are the first to report results on this challenging setting. We evaluate with mean intersection-over-union over classes (mIoU) and per-pixel accuracy (pAcc).\n\nDetails on Obtaining a Semantic Segmentation with DINOSAUR For unsupervised semantic segmentation, we need to assign a discrete label to each discovered mask. As our method does not directly output labels required for semantic segmentation, we obtain them by running K-Means clustering on features associated with each slot after training the model, then assigning clusters to ground truth classes by maximizing IoU using Hungarian matching. Similar ways of evaluating are commonly used in the literature (Van Gansbeke et al., 2021; Melas-Kyriazi et al., 2022). In particular, for each slot generated by the model, we compute a feature vector by taking a weighted average between the slot’s mask and ViT features (see below). Each vector is then L2-normalized, and the set\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nof vectors from all images is clustered by running k-means clustering, and each slot is associated with a cluster. Finally, the clusters are assigned to ground truth classes by maximizing the total IoU between clusters and classes using the Hungarian method (Kuhn, 1955). For K-Means clustering, to avoid sensitivity to initialization, we run 20 repetitions and use the run with the lowest sum of squared distances to the cluster centers. As we noticed there is still some variance between different evaluation runs, we run the inference and clustering procedure three times and use the average as the final value for the training run.\n\nFeatures for clustering (Table 8). Which features should we use for clustering the slots? We initially experimented with the slots themselves, but found that they do not cluster well. Instead, we use a weighted average of the slots masks and pre-trained ViT features to produce a feature to cluster for each slot. We compare features from ViT block 9 and ViT block 12 and find that the best block depends on the training method. In the following, we use block 9 for supervised training, DINO and MSN, and block 12 for MoCo-v3 and MAE.\n\nTable 8: Comparing ViT-B/16 blocks and slot features for clustering on COCO-27 (mIoU).\n\nMethod\n\nSlots Block 9 Block 12\n\nSupervised 8.0 13.4 DINO 13.5 MoCo-v3 11.9 MSN 6.7 MAE\n\n18.0 24.0 20.5 21.8 10.1\n\n13.3 21.1 22.4 16.8 11.2\n\nNumber of clusters. Results for unsupervised semantic segmentation are dependent on the number of clusters used. Prior methods (Hamilton et al., 2022; Van Gansbeke et al., 2022) typically use the number of classes labeled in the datasets as the number of clusters. For unsupervised object segmentation, this presents an issue for our method, as it captures entities in both foreground and background, and thus clustering using the number of (foreground) object classes leads to over-merging of classes. To ensure a fairer comparison, we instead chose the number of clusters by estimating the how many foreground and background classes can appear on the different datasets. In particular, for PASCAL VOC 2012, we use 105 clusters (the number of classes occurring on at least 0.5% of images, using the PASCAL-Context (Mottaghi et al., 2014) labeling and class statistics), and for COCO, we use 172 clusters (the number of classes labeled on the COCO-Stuff dataset (Caesar et al., 2018)). As this results in more clusters than ground truth classes, Hungarian matching leaves some clusters unassigned; we treat all unassigned clusters as being assigned to the “background” class. For scene decomposition, we simply use 27 clusters for COCO-Stuff 27 and 172 clusters for COCO-Stuff 172.\n\nResults on Object Segmentation (Table 7a) This task is not naturally suited for our method, which decomposes the scene into semantically meaningful parts without distinguishing fore- and background. We still obtain competitive results with recently published work (Van Gansbeke et al., 2021; Melas-Kyriazi et al., 2022). Moreover, DeepSpectral, MaskDistill and COMUS employ additional steps of training segmentation networks which improves results and allows them to run at the original image resolution. In contrast, the masks we evaluate are only of size 14 × 14; we leave it to future work to improve the resolution of the produced masks.\n\nResults on Scene Decomposition (Table 7b) We are competitive with the state-of-the-art method STEGO on the setting with 27 coarse classes. On the novel setting with all 171 classes, our method improves upon STEGO by 3.2 IoU points. This demonstrates that DINOSAUR is able to handle the full complexity of real-world scenes somewhat better.\n\nC.3 UNSUPERVISED SEMANTIC SEGMENTATION BASELINE (STEGO) ON MOVI\n\nAn interesting question is to what degree unsupervised semantic segmentation methods (which we compare to on real-word data in Sec. C.2) can already solve the object discovery task on synthetic datasets designed to benchmark object-centric learning methods. If this were the case, it would showcase that those datasets are flawed in the sense that instance-level grouping (one goal of object-centric learning) is not needed to tackle them. To this end, we test the state-of-the-art scene segmentation method STEGO (Hamilton et al., 2022) on the MOVi datasets.\n\nTable 9: Results for STEGO on the MOVi datasets.\n\nDataset Clusters ARI FG-ARI mBO\n\nMOVi-C\n\nMOVi-E\n\n22 27\n\n22 27\n\n40.0 39.5\n\n41.1 41.6\n\n20.1 19.8\n\n15.6 15.8\n\n24.1 23.8\n\n11.5 12.0\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nMOVi-C\n\nMOVi-E\n\ne g\na\n\nm\n\nI\n\nO G\nE T\nS\n\nFigure 11: Running the unsupervised semantic segmentation method STEGO (Hamilton et al., 2022) on the MOVi datasets, using 22 clusters for MOVi-C and 27 clusters for MOVi-E. Color codes correspond to the different clusters. Results are presented without CRF post-processing.\n\nSTEGO assigns each pixel to one of several clusters/classes, with the clustering performed over the full dataset. We run STEGO with 22 and 27 clusters, corresponding to the 17 semantic categories of the MOVi dataset (see Sec. F), plus 5 or 10 clusters which can be used for the backgrounds. We use a DINO ViT-B/8 backbone for feature extraction, other hyperparameters are listed in Sec. E.2. To evaluate on the object discovery task, we use the segmentation masks produced by STEGO as if they were object masks. That is, each cluster/class mask is used as a separate object mask.\n\nWe list quantitative results in Table 9 and show examples in Fig. 11. In addition to foreground ARI (FG-ARI) and mean best overlap (mBO), we list ARI, which takes the background pixels into account as well. STEGO is able to segment objects and background to some degree. However, objects are not properly split, with a single cluster often covering several objects from different semantic categories. Compared to our method, STEGO’s masks are also significantly more cluttered and less geometrically consistent. Quantitatively, STEGO performs significantly worse than all object-centric methods. There is a large difference between ARI and FG-ARI, indicating that STEGO is often able to separate out the background, but struggles to separate the objects. For mBO on MOVi-C, STEGO is only 2 points worse than our Slot Attention and STEVE baselines, showing that on scenes with a moderate amount of objects, the masks can capture the objects to some degree (but do not succeed in separating them, see Fig. 11). Overall, this result suggests that a current state-of-the-art unsupervised semantic segmentation method can not satisfactorily solve the object discovery task, justifying the existence of datasets like MOVi to study object-centric learning methods.\n\nInputs\n\nReconstructions\n\nMasks\n\nFigure 12: Finetuning a DINO pre-trained ViT encoder using image reconstruction as a training signal fails to yield meaningful results and results in striped mask patterns (see discussion in Sec. 4.3).\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nD ADDITIONAL ANALYSIS\n\nIn this section, we conduct several experiments that did not fit into the main part. A brief overview:\n\nTable 10: Results on object localization and semantic segmentation for decoder analysis from Sec. 4.3.\n\nSec. D.1: Comparing different pre-training schemes for feature extraction and reconstruction.\n\nSec. D.2: Comparing training a ResNet encoder from scratch vs. using a pre-trained ViT encoder.\n\nSec. D.3: How does the number of slots affect the quantitative and qualitative results of our method,\n\nboth at training and test time?\n\nSec. D.4: How does decoder scale affect the quality of object discovery?\n\nSec. D.5: Analyzing the poor performance of the Transformer decoder with larger ViTs.\n\nSec. D.6: Comparing the quality of masks from slot attention and the Transformer decoder. Sec. D.7: Does DINOSAUR perform semantic segmentation on the MOVi datasets?\n\nTable 10: Additional results for decoder analysis in Fig. 3 on Object Localization and Unsupervised Semantic Segmentation (mean ± standard dev., 3 seeds), using a ViT-B/16 encoder and 6 slots (PASCAL VOC 2012) or 7 slots (COCO). For localization, results are given on PASCAL VOC 2012 trainval (training on PASCAL VOC 2012) and COCO-20k (training on COCO). For segmentation, results are given for object segmentation (PASCAL VOC 2012, COCO) and scene decomposition (Stuff-27, Stuff-171); when the training dataset is different from the evaluation dataset, this tests zero-shot transfer (i.e. PASCAL → COCO and COCO → PASCAL). We also refer to Figs. 22 and 23 for side-by-side comparisons of the two types of decoders.\n\nObject Localization\n\nSemantic Segmentation (mIoU)\n\nTraining Decoder\n\nCorLoc\n\nDetRate\n\nCOCO Stuff-27 Stuff-171 PASCAL\n\nPASCAL\n\nCOCO\n\nMLP Transformer\n\nMLP Transformer\n\n76.0 ±0.3 69.8 ±4.9\n\n69.3 ±0.5 67.2 ±1.5\n\n56.3 ±0.2 52.1 ±3.9\n\n29.8 ±0.3 31.0 ±0.9\n\n12.5 ±0.4 11.9 ±1.0\n\n12.4 ±0.2 13.9 ±0.4\n\n21.0 ±0.3 22.2 ±0.8\n\n20.3 ±0.2 24.0 ±0.9\n\n11.3 ±0.1 11.6 ±0.6\n\n11.7 ±0.0 13.1 ±0.3\n\n33.7 ±0.9 37.2 ±1.8\n\n32.3 ±0.8 38.6 ±1.1\n\nD.1 CHOICE OF PRE-TRAINING METHOD\n\nWe compare different different backbones and pre-training schemes in Table 11. We list results on object discovery, object localization and unsupervised semantic segmentation for a comprehensive overview. Different from Table 2 in the main part, here we consider the setting with pre-trained, frozen encoders. We also investigate target features from ResNet-50 encoders and differently-sized ViTs. When using ResNet50 encoders, we resorted to the MLP decoder as the Transformer decoder did not yield good performance when reconstructing ResNet features.\n\nAs in Table 2, we find that all self-supervised schemes perform well for the task of object discovery (cf. Fig. 24). In comparison, supervised training via ImageNet classification is a worse source of features. Moreover, there is a gap between ViT and ResNet features: while ARI is high (likely a by-product of using the MLP decoder), mBO is poor. Visual inspection shows that ResNet-trained models seem to rely on a mix of semantic and geometric grouping of the image (cf. Fig. 25). Generally, larger ViT architectures also yield better results. We conclude from this experiment that scaling the architecture used for the encoder and target features yields better results. Furthermore, we find that using self-supervised training is important for real-world grouping to emerge, but that the specific method is less important.\n\nFinally, we also point to Table 12, where we list results for different self-supervised schemes, but training with the MLP decoder instead of the Transformer decoder. This is interesting because MAE, the worst among the self-supervised methods with the Transformer decoder, yields the overall best results in terms of ARI on COCO object discovery: 42.3. This suggests that there are still details about how exactly self-supervised training methods interact with object-centric feature reconstruction left open for investigation.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: Comparing different pre-trained models for feature extraction and reconstruction target (mean ± standard dev., 3 seeds), using 7 slots, the Transformer decoder for ViTs, and the MLP decoder for ResNets. For discovery, results are given on COCO. For localization, results are given on COCO-20k. For segmentation, results are given for object segmentation (COCO) and scene decomposition (Stuff-27, Stuff-171). We refer to Fig. 24 for side-by-side comparisons.\n\nObject Discovery\n\nObject Localization Semantic Segmentation (mIoU)\n\nEncoder\n\nPretraining FG-ARI mBOi mBOc\n\nCorLoc DetRate\n\nCOCO Stuff-27\n\nStuff-171\n\nResNet50 Supervised 36.0 ±0.3 22.1 ±0.1 24.2 ±0.1 36.0 ±0.5 22.9 ±0.4 25.1 ±0.4 ResNet50 DINO ResNet50 MoCo-v3 33.7 ±0.5 21.1 ±0.2 23.2 ±0.2 ViT-S/16 Supervised 26.0 ±0.2 24.3 ±0.1 30.2 ±0.1 ViT-S/16 DINO 36.9 ±0.6 29.7 ±0.7 33.9 ±1.0 ViT-B/16 Supervised 32.3 ±0.8 26.3 ±0.3 31.5 ±0.3 34.1 ±1.0 31.6 ±0.7 39.7 ±0.9 ViT-B/16 DINO 35.2 ±0.2 31.4 ±0.2 38.5 ±0.5 ViT-B/16 MoCo-v3 35.2 ±0.3 31.1 ±0.2 37.2 ±0.4 ViT-B/16 MSN 32.8 ±3.7 30.2 ±1.8 33.2 ±1.8 ViT-B/16 MAE 34.3 ±0.5 32.3 ±0.4 38.8 ±0.4 ViT-S/8 DINO\n\n44.2 ±0.6 15.0 ±0.3 44.3 ±2.9 15.4 ±1.3 35.2 ±1.0 11.2 ±0.5 56.2 ±0.7 24.5 ±0.3 68.6 ±1.9 30.2 ±1.3 59.1 ±1.1 25.5 ±0.4 67.2 ±1.5 31.1 ±0.9 70.0 ±0.5 32.3 ±0.1 69.7 ±0.7 31.9 ±0.4 68.9 ±5.2 29.8 ±3.0 73.0 ±0.2 33.8 ±0.0\n\n9.1 ±0.1 13.7 ±0.3 7.0 ±0.1 12.8 ±0.1 6.3 ±0.0 11.9 ±0.2 18.2 ±0.2 14.4 ±0.5 11.2 ±0.1 20.3 ±0.1 18.6 ±0.2 13.3 ±0.2 13.9 ±0.4 24.0 ±0.9 14.3 ±0.2 22.4 ±0.5 13.3 ±0.2 21.8 ±0.3 9.2 ±0.4 11.2 ±0.7 13.4 ±0.1 23.7 ±0.3\n\n7.7 ±0.1 6.4 ±0.1 5.4 ±0.1 13.5 ±0.1 10.6 ±0.2 13.5 ±0.1 13.1 ±0.3 13.4 ±0.2 12.2 ±0.2 7.3 ±0.2 13.2 ±0.1\n\nD.2 COMPARISON OF RESNET AND PRE-TRAINED VIT ENCODERS\n\nIn this experiment, we compare training a ResNet34 encoder from scratch vs. using a pre-trained, frozen ViT encoder (i.e. our primary setup). As the reconstruction target, we still use features from a pre-trained ViT. Notably, we were not able to optimize the ResNet from scratch using the Transformer decoder, as the model degraded to only use a single slot for the full image. Instead, here we use the MLP decoder with both ResNet34 and pre-trained ViT encoders.\n\nTable 12 lists the results. Example predictions on COCO are included in Table 25 and Table 26. We find that training from scratch is a viable alternative to using a pre-trained encoder, in some cases improving over the pre-trained encoder. We think this is because training from scratch allows adaptation to the domain, whereas the pre-trained encoder may to some degree be misaligned. On COCO, the improvement may also partially stem from the ResNet’s higher spatial resolution (28×28) — although the decoder’s mask resolution is the same (14 × 14). However, using a frozen encoder has computational advantages: no backward pass is needed, and only one forward pass through the encoder is needed (instead of one pass through the encoder being trained, and one pass through the encoder providing the target features). Finally, we find that pre-trained ResNet features are a worse reconstruction target then ViT features. Overall, this experiment shows that the crucial element making our approach work is feature reconstruction. While the type of encoder and the use of pre-trained weights can yield further improvements, they are less important compared to the use of feature reconstruction.\n\nD.3 NUMBER OF SLOTS ANALYSIS\n\nD.3.1 TRAINING SENSITIVITY\n\nMOVi-E (Table 13) On the synthetic MOVi-E dataset, we found that changing the number of slots can greatly impact performance. We attribute this to the strong impact object splitting (distributing an object over two or more slots) can have on the ARI score. While setting the number of slots to the maximum number of objects that can appear in the scene (23 on MOVi-E) allows the model to fully cover crowded scenes, most scenes have fewer objects, leading to impaired performance on those. We show examples on MOVi-E with 11 slots for all tested methods in Fig. 21.\n\nPASCAL (Table 14) & COCO (Fig. 14, Fig. 15) On both PASCAL VOC 2012 and COCO, our method performs well over a range of slots, as long as a minimum number of slots is given. Slot Attention, in contrast, fails to produce meaningful results for all tested number of slots. For our method on COCO, the analysis shows that there are different sweet spots for instance-level (9–11 slots) and class-level segmentation (7 slots). Visual inspection (see Fig. 13) suggests that the optimal number of slots (unsurprisingly) depends on the complexity of the specific image: using few slots tends to lead to grouping of related objects, whereas using many slots leads to oversegmentation of objects into\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nTable 12: Training a ResNet34 encoder from scratch vs. using a pre-trained frozen ViT encoder on object discovery (mean ± standard dev., 3 seeds). Both options use the same target features for reconstruction and the MLP decoder. Training from scratch yields overall similar results compared to using a pre-trained encoder. We refer to Figs. 25 and 26 for side-by-side comparisons.\n\nDataset\n\nTarget Features\n\nEncoder Training\n\nFG-ARI\n\nMOVi-C DINO\n\nViT-S/8\n\nMOVi-E\n\nDINO\n\nViT-S/8\n\nPASCAL DINO\n\nViT-B/16\n\nDINO\n\nCOCO\n\nMoCo-v3\n\nResNet50\n\nViT-B/16\n\nResNet50\n\nViT-B/16\n\nMSN\n\nViT-B/16\n\nMAE\n\nViT-B/16\n\nResNet34 Scratch Frozen ViT-S/8\n\nResNet34 Scratch Frozen ViT-S/8\n\nResNet34 Scratch ViT-B/16 Frozen\n\nResNet34 Scratch ResNet50 Frozen\n\nResNet34 Scratch ViT-B/16 Frozen\n\nResNet34 Scratch ResNet50 Frozen\n\nResNet34 Scratch ViT-B/16 Frozen\n\nResNet34 Scratch ViT-B/16 Frozen\n\nResNet34 Scratch ViT-B/16 Frozen\n\n65.1 ±2.1 67.2 ±0.3\n\n64.5 ±0.6 64.7 ±0.7\n\n27.4 ±0.7 24.6 ±0.2\n\n36.6 ±0.3 36.0 ±0.5\n\n40.9 ±0.2 40.5 ±0.0\n\n31.9 ±1.1 33.7 ±0.5\n\n40.4 ±0.6 39.8 ±0.2\n\n40.7 ±0.3 39.0 ±0.1\n\n37.7 ±0.1 42.3 ±0.3\n\nmBO(i,c)\n\n29.1 ±2.0 38.6 ±0.1\n\n36.5 ±0.1 34.1 ±0.1\n\n38.3 ±0.5 39.2 ±0.5 39.5 ±0.1 40.9 ±0.1\n\n26.2 ±0.2 28.8 ±0.1 22.9 ±0.4 25.1 ±0.4\n\n27.9 ±0.0 31.1 ±0.1 27.7 ±0.2 30.9 ±0.2\n\n23.9 ±0.5 26.7 ±0.4 21.1 ±0.2 23.2 ±0.2\n\n28.1 ±0.2 31.1 ±0.1 27.4 ±0.1 30.4 ±0.1\n\n27.6 ±0.1 30.7 ±0.1 28.1 ±0.0 31.6 ±0.1\n\n28.1 ±0.1 31.7 ±0.0 29.1 ±0.1 32.2 ±0.2\n\nparts. Choosing the number of slots per-dataset rather than per-image thus only optimizes results for the average scene. An interesting question for future work is how models could deal with the variance of visual complexity and number of instances that they are faced with in real-world situations.\n\n5 slots\n\n7 slots\n\n9 slots\n\n11 slots\n\n13 slots\n\n15 slots\n\nFigure 13: Three scenes of low/medium/high complexity on COCO 2017, varying the number of slots the model is trained with, using a ViT-B/16 encoder and the Transformer decoder. For an appropriate grouping to emerge, the number of slots has to fit the complexity of the scene. However, the groupings found by our method are still semantically meaningful, grouping together related objects (too few slots) or separating objects into their constituents parts (too many slots).\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nTable 13: Number of slots analysis on MOVi-E object discovery (mean over 3 seeds). All methods perform better when training with less slots.\n\n11 slots\n\n24 slots\n\nARI mBO\n\nARI mBO\n\nBlock Masks DINO K-Means Slot Attention SLATE Ours (ViT-S/8) Ours (ViT-B/8)\n\n48.9 50.1 52.6 49.4 76.7 79.3\n\n14.7 35.2 16.3 16.6 29.7 32.7\n\n45.4 50.2 45.0 44.4 64.7 65.1\n\n24.3 23.6 24.0 23.6 34.1 35.5\n\nTable 14: Number of slots analysis on PASCAL VOC 2012 object discovery (mean ± standard dev., 3 seeds), using a ViT-B/16 encoder and the Transformer decoder.\n\nSlots\n\nFG-ARI\n\nmBOi\n\nmBOc\n\n4 5\n6 7\n9\n\n29.6 ±1.4 26.6 ±1.1 24.8 ±2.2 19.1 ±1.3 20.5 ±0.7\n\n34.2 ±1.0 39.1 ±0.6 44.0 ±1.9 42.7 ±0.7 43.5 ±0.3\n\n40.4 ±1.1 45.9 ±0.7 51.2 ±1.9 47.9 ±0.6 47.4 ±0.4\n\nFigure 14: Varying the number of slots the model is trained with on COCO object discovery (mean ± standard dev., 3 seeds), using a ViT-B/16 encoder and the Transformer decoder. Our method shows a small trade-off between instance- (more slots) and class-level segmentation (less slots). Slot attention fails to produce meaningful masks over a range of slots.\n\nD.3.2 TEST-TIME ROBUSTNESS\n\nIn the last section, we found that the quality of results is affected by the number of slots the model is trained with. An attractive quality of Slot Attention-based models3 is that the number of slots can be changed after the model is trained (Locatello et al., 2020), and we inherit this feature. However, this constitutes a form of distribution shift for the model, and so it is important to know how robust the model is to such changes. To this end, we compare models on trained on a certain number of slots, with models trained on a different number of slots, but tested with that number of slots. If the performance is similar, we have evidence for the robustness of the model to this type of shift.\n\n3This is only true when using randomly sampled, and not fixed, learned slot initializations. Recent models (Kipf et al., 2022; Elsayed et al., 2022) depart from the original random sampling design, as it appears to be easier to train the model with fixed initializations on more complex data. We found no such difficulties with DINOSAUR.\n\nFigure 15: Varying the number of slots the model is trained with on object localization and unsupervised semantic segmentation (mean ± standard dev., 3 seeds), using a ViT-B/16 encoder and the Transformer decoder. The models are trained on COCO and tested on the respective datasets.\n\n27\n\n5101520Slots10203040FG-ARI5101520Slots10203040mBOi5101520Slots10203040mBOcBlock MasksDINO K-MeansSlot AttentionSLATEDINOSAURPASCAL VOC 2012COCO-20k020406080CorLocObject LocalizationCOCOCOCO Stuff-27COCO Stuff-1710510152025mIoUUnsupervised Semantic SegmentationSlots57911131520Published as a conference paper at ICLR 2023\n\nTable 16 shows the results of this experiment on MOVi-E, PASCAL VOC 2012, and COCO. We find that the model is fairly robust to changing the number of slots at test time. Interestingly, in many instances, there is even a large improvement from changing the number of slots (e.g. on MOVi-E from 11 to 24 slots, or on PASCAL, from 6 to 4 slots. This can be explained by our observations in Sec. D.3.1: depending on the number of slots at training time, the model learns to focus on object of different scales. Some object scales fit better to the dataset annotations than others. Applying a better model with a different number of slots can thus improve over models that did not perform well with the original slot configuration. We can also explain the cases where performance drops from changing the number of slots (e.g. on MOVi-E from 24 to 11 slots, or on COCO from 20 slots) with this: those configurations did not work well in the first place, so they also do not transfer well to a different number of slots. Overall, this result shows that DINOSAUR generalizes well when applied with a different number of slots.\n\nFigure 16: Varying the test time number of slots at on object discovery (mean ± standard dev., 3 seeds). We plot the difference between testing at a certain number of slots and directly training at that number of slots, for models trained with different number of slots. We list FG-ARI for MOVi-E, and mBOi for COCO and PASCAL VOC 2012. For MOVi-E, we use a ViT-B/8 encoder with the MLP decoder. For COCO and PASCAL VOC 2012, we use a ViT-B/16 encoder with the Transformer decoder. The performance of the original models can be gauged from Table 13, Table 14 and Fig. 14.\n\nD.4 HOW DOES DECODER SCALE INTERACT WITH OBJECT DISCOVERY?\n\nExtending our decoder analysis from Sec. 4.3, we investigate how the capacity of the decoder and the slot dimensionality affects the object discovery performance on COCO. In particular, we compare the MLP with the Transformer decoder: for the MLP decoder, we use a 4-layer MLP and test hidden layer dimensionalities 1024, 2048, and 4096. For the Transformer decoder, we test different layer numbers 2, 3, and 4. For each decoder setting, we test it under slot dimensionalities 64, 128, 256, and 512.\n\nWe show the results in Fig. 17. The general trend of Table 3 stays unchanged: the MLP decoder has higher ARI (indicating better instance grouping), whereas the Transformer decoder has higher mBO (indicating better mask quality). For the MLP decoder, we find that scaling the MLP decoder has slight benefits in terms of ARI, and that it is robust to the slot dimensionality (with the exception of slot dimensionality 512). For the Transformer decoder, scaling appears to have little impact. However, we find that the Transformer decoder is sensitive to the slot dimensionality: setting it too low (64, 128) can cause the model to collapse to just use one or two slots to model the whole image. Setting it too high (512) causes training instabilities (i.e. NaNs or loss spikes). These instabilities under slot dimensionality 512 also occur with the MLP decoder. We conjecture this might be related to the use of 16-bit mixed precision and the attention mechanism of slot attention, but we did not investigate further.\n\nD.5 THE TRANSFORMER DECODER PERFORMS POOR WITH LARGER VITS\n\nIn Table 15, we compare the Transformer decoder with different ViT encoders on COCO object discovery. We find that scaling up the encoder leads to better results. However, specifically combining the Transformer decoder with the ViT-B/8 encoder leads to poor results – the MLP decoder has no such issues (cf. the results on the MOVi datasets in Fig. 3). Visual inspection shows that the model collapses to use just one or two slots for the whole image. We attribute this to the high capacity of the decoder, which leads to it being able to reconstruct the token map well while using the conditioning information from only a few slots. Note that the capacity of the decoder is related to the encoder through the token dimensionality, and the number of tokens (which has strong influence on a\n\n28\n\n1124Test Slots105051015 FG-ARIMOVi-E45679Test Slots2048 mBOiPASCAL VOC 201257911131520Test Slots42024COCOTraining with6 Slots7 Slots9 Slots11 Slots20 Slots24 SlotsPublished as a conference paper at ICLR 2023\n\nFigure 17: COCO object discovery performance when varying decoder capacity and slot dimensionality, using 7 slots (mean ± standard dev., 3 seeds). For the MLP decoder, we vary the dimensionality of a 4-layer MLP, whereas for the Transformer decoder, we vary the number of layers. The dotted line represents the block masks baseline. The analysis reveals instabilities of the Transformer decoder: for too small or too large slot dimensionalities, the performance degrades below the block masks baseline. The MLP decoder is generally robust, but also becomes unstable at slot dimensionality 512.\n\nTransformer’s capacity as it directly influences the amount of memory and computation available throughout the blocks). Indeed, when reducing the decoder’s capacity by either reducing the feature dimensionality (ViT-S/8) or the number of tokens the decoder operates on (using bicubic interpolation on the feature map to reconstruct), the problem disappears. In principle, the issue could also arise from optimization becoming unstable due to switching to a larger model, but the loss curves do not suggest such instabilities.\n\nTable 15: Comparing different encoders with the Transformer decoder on COCO object discovery. For ViT-B/8, we also list results for scaling the target token map by a factor of 0.5 (from 784 to 196 tokens) and by a factor of 0.75 (to 441 tokens). Object-centricness does no longer emerge for ViT-B/8, which we attribute to the high capacity of the decoder: when reducing the capacity (by reducing the feature dimensionality or the number of tokens), the problems disappear.\n\nEncoder\n\nTarget Tokens Dfeat\n\nFG-ARI\n\nmBOi\n\nmBOc\n\nViT-S/16 ViT-B/16 ViT-S/8 ViT-B/8 ViT-B/8 ViT-B/8\n\n196 196 784 196 (scaled) 441 (scaled) 784\n\n384 768 384 768 768 768\n\n36.9 ±0.6 34.1 ±1.0 34.3 ±0.5 35.8 ±0.7 20.1 ±9.8 13.1 ±3.5\n\n29.7 ±0.7 31.6 ±0.7 32.3 ±0.4 32.0 ±0.6 19.1 ±8.6 16.6 ±3.9\n\n33.9 ±1.0 39.7 ±0.9 38.8 ±0.4 38.6 ±0.2 23.5 ±9.9 21.3 ±5.1\n\nD.6 MASK TYPE ANALYSIS\n\nAs noted in Sec. 3, the Transformer decoder offers a choice between attention masks from the slot attention module, and attention masks from the decoder. In Table 16, we compare the two types of masks and find that decoder attention masks are consistently better on all tasks, and especially outperform the masks from slot attention on segmentation. Note that we could in principle also use masks from slot attention with the MLP decoder. We point to Singh et al. (2022b), who analyze this choice and find that alpha masks produced by reconstruction are superior to attention masks.\n\nTable 16: Comparing masks from Slot Attention and the Transformer decoder on COCO (mean ± standard dev., 5 seeds), using a ViT-B/16 encoder. For segmentation, we list results for object segmentation (COCO) and scene decomposition (Stuff-27, Stuff-171).\n\nObject Discovery\n\nObject Localization\n\nSegmentation (mIoU)\n\nMask Type\n\nFG-ARI mBOi mBOc\n\nCorLoc DetRate\n\nCOCO Stuff-27 Stuff-171\n\nSlot Attention 31.7 ±1.4 30.3 ±0.8 38.1 ±1.1 34.1 ±1.0 31.6 ±0.7 39.7 ±0.9 Decoder\n\n66.0 ±1.5 30.2 ±0.8 67.2 ±1.5 31.1 ±0.9\n\n12.0 ±0.2 21.1 ±0.6 11.7 ±0.2 13.9 ±0.4 24.0 ±0.9 13.1 ±0.3\n\n29\n\n64128256512Slot Dimensionality02040ARI64128256512Slot Dimensionality02040mBOi64128256512Slot Dimensionality02040mBOcDecodersMLP, 1024 DimsMLP, 2048 DimsMLP, 4096 DimsTransf., 2 LayersTransf., 3 LayersTransf., 4 LayersPublished as a conference paper at ICLR 2023\n\nD.7 DOES DINOSAUR PERFORM SEMANTIC GROUPING ON MOVI?\n\nOn the MOVi datasets, DINOSAUR seemingly separates objects into slots, i.e. instance grouping. As DINOSAUR exhibits some bias towards semantic grouping on real-world datasets, an interesting question is whether our approach does in fact also perform semantic grouping on MOVi. If this would be the case, it would be hard to detect by evaluating on the original MOVi datasets, because objects of the same category rarely appear on the same image on those datasets.\n\nTo answer this question, we designed two variants of the MOVi-C dataset. In the first, only objects of the same semantic category appear in each image. In the second, each image contains the exact same object several times. All other characteristics of the dataset stay unchanged. If our method would perform semantic grouping on MOVi, objects would be grouped together on those datasets. In Table 17, we list the results for training on MOVi-C and testing on the new “semantic” variants. We find that the results do not change significantly compared to evaluating on the original MOVi-C dataset, indicating that our method does in fact perform instance grouping on the MOVi datasets.\n\nTable 17: Testing a model trained on MOVi-C on two variants of MOVi-C: in “semantic objects”, each image only contains objects of the same semantic category; in “duplicate objects”, each image only contains the same object, but several times. We show results for our method with ViT-B/8 encoder and MLP decoder, with mean ± standard dev. over 5 seeds. We find that the results do not change significantly, indicating that DINOSAUR does not apply semantic grouping on MOVi.\n\nDataset\n\nFG-ARI\n\nmBO\n\nMOVi-C Semantic Objects Duplicate Objects\n\n68.6 ±0.4 68.6 ±0.4 68.1 ±0.2\n\n40.8 ±0.2 39.1 ±0.2 39.0 ±0.2\n\nc i\nt\n\nn a\n\nm\n\ne S\n\ne t\na c\ni l\n\np u\nD\n\nE IMPLEMENTATION DETAILS\n\nE.1 ARCHITECTURE AND HYPERPARAMETERS\n\nIn the following, we describe implementation details of the DINOSAUR architecture. The hyperparameters for our experiments are given in Table 18.\n\nViT Encoder We use the Vision Transformer implementation and provided by the timm library (Wightman, 2019)4. Depending on the experiment, we use the following configurations: ViT-S/16 (token dimensionality 384, 6 heads, patch size 16), ViT-S/8 (token dimensionality 384, 6 heads, patch size 8), ViT-B/16 (token dimensionality 768, 12 heads, patch size 16), ViT-B/8 (token dimensionality 768, 12 heads, patch size 8). The specific timm model names (for DINO weights) used are vit small patch16 224 dino (ViT-S/16), vit small patch8 224 dino (ViTS/8), vit base patch16 224 dino (ViT-B/16), vit base patch8 224 dino (ViT-B/8). All models use 12 Transformer blocks, linear patch embedding and additive positional encoding. The output of the last block (not applying the final layer norm) is passed on to the Slot Attention module and used in the feature reconstruction loss, after removing the entry corresponding to the CLS token. We always use images of size 224 × 224 as input to the ViT; this yields N = 142 = 196 patches for patch size 16 and N = 282 = 784 patches for patch size 8. If the image size is not equal to 224, we use bicubic interpolation for resizing. For the pre-trained weights, we use the timm library for DINO and third-party releases for MoCo-v35, MSN6, and MAE7.\n\n4https://github.com/rwightman/pytorch-image-models, v0.6.7 5https://github.com/facebookresearch/moco-v3 6https://github.com/facebookresearch/msn 7https://github.com/facebookresearch/mae\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nResNet34 Encoder We use a ResNet34 using the modifications suggested by Kipf et al. (2022) and Elsayed et al. (2022). That is, we modify the basic ResNet34 architecture (resnet34 in the timm library) to use a stride of 1 in the first convolution, and replace batch normalization with group normalization. This results in a 512-dimensional feature map of size 28 × 28 for images of size 224 × 224. Like Kipf et al. (2022), we add a linear positional encoding to the feature map passed to the Slot Attention module (coordinates in the four cardinal directions scaled to [−1, 1], linearly projected to the feature dimension and added to the features).\n\nSlot Attention The Slot Attention module largely follows the original formulation from Locatello et al. (2020). After applying a layer norm, the set of N input features is transformed to slot dimensionality Dslots by a two-layer MLP with hidden size equal to the feature size Dfeat, followed by another layer norm. An initial set of K slots is randomly sampled from a normal distribution parametrized by learnable mean μ ∈ RDslots and log standard deviation log σ ∈ RDslots . The slots are then updated over several iterations, where each iteration involves a layer norm step on the current slots, an attention step where slots compete for input features, updating the slots using a GRU (Cho et al., 2014), and applying a residual two-layer MLP. The query/key/value projections’ size is the same as the slot dimensionality Dslots; they do not use a bias. The key-value dot product is scaled by D−0.5 slots , and an epsilon of 10−8 is added to the softmax weights for stability reasons. The MLP has a hidden size equal to 4 · Dslots.\n\nOn MOVi-C and MOVi-E, we use 11 and 24 slots respectively, which corresponds to the maximal number of objects that can appear in the scene plus one slot that can capture the background. On PASCAL VOC and COCO, we use 6 and 7 slots respectively, as we found those values to perform quantitatively well and give visually appealing results (cf. App. D.3).\n\nMLP Decoder We use a four-layer MLP with ReLU activations, with output dimensionality Dfeat + 1, where the last dimension is used for the alpha value. The MLP has hidden layer sizes of 1024 for the MOVi datasets, and 2048 for COCO. A learnable positional encoding of size N × Dslots is added to the slots after broadcasting them to the number of patches N .\n\nTransformer Decoder We use the standard decoder design from Vaswani et al. (2017) with pre-normalization (Xiong et al., 2020), conditioning on the set of slots output by the slot attention module. Similar to ImageGPT (Chen et al., 2020a), the decoder autoregressively reconstructs N patch features of dimensionality Dfeat starting from the top-left and proceeding row-by-row over the image (“raster order”). To this end, its initial input is the set of target features shifted back by one, removing the last feature and inserting a learned “beginning-of-sentence” feature at the start. Each Transformer block then consists of self-attention on the set of tokens (using causal masking to prevent attending to future tokens), cross-attention with the set of slots, and a residual two-layer MLP with hidden size 4 · Dfeat. Before the Transformer blocks, both the initial input and the slots are linearly transformed to Dfeat, followed by a layer norm. The Transformer thus operates on tokens of dimensionality Dfeat. The output of the last Transformer block is directly used as the reconstruction y. We do not apply dropout as we did not find it to be beneficial. Also, we do not add positional encoding as the target features forming the input already contain positional information (from the ViT encoder).\n\n√\n\n√\n\nMasks for Evaluation As mentioned in the main text, we use the attention mask from the decoder as the slot mask used for evaluation. In particular, the cross attention step involves a softmax over the slots for each of the N tokens, resulting in N attention weights per slot, which can be reshaped to N mask. Intuitively, each mask entry measures how important the corresponding slot form a was for reconstructing the patch feature at that position. For the evaluation, this mask is bilinearly resized to the image size. We use the attention mask from the last decoder block, which we found to perform best against the alternatives of using masks from earlier blocks or averaging the masks across blocks. The soft slot masks are converted to hard masks using the arg max.\n\nN ×\n\nDetails for Specific Experiments Here we list settings for experiments aside from our main experiments.\n\n• Insufficiency of Image Reconstruction (Sec. 4.3). In this experiment, we train on COCO using image reconstruction and combine a ViT-B/16 encoder with Slot Attention’s spatial broadcast decoder (as detailed in App. E.2). Image reconstruction uses 128 × 128 as the target resolution. For finetuning, we use a lower learning rate of 4 · 10−5 for the encoder, compared to 4 · 10−4 for Slot Attention and the decoder. All models use 7 slots. Other training and architecture settings are the same as for the main experiments.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\n• Choice of Self-Supervised Method (Sec. 4.3) and Choice of Pre-Training Method In these experiments, we use different pre-trained networks for fea- (Sec. D.1). ture extraction and reconstruction. The specific timm model names used are resnet50 (ResNet50, supervised), vit small patch16 224 (ViT-S/16, supervised), vit base patch16 224 (ViT-B/16, supervised), vit small patch16 224 dino (ViT-S/16, DINO), vit base patch16 224 dino (ViT-B/16, DINO). For MoCo-v3, MSN and MAE, we use the ViT-B/16 implementation of timm with third-party weight releases (referenced above). All networks were pre-trained on the ImageNet dataset. The ResNet50 encoder uses the features output from the second last block (feature level 3 in timm), resulting in a 1024-dimensional feature map of size 14 × 14. Like Kipf et al. (2022), we add linear positional encoding to this feature map before Slot Attention. The ViT encoders use the Transformer decoder whereas for the ResNet encoder, we use the MLP decoder as training with the Transformer decoder did not yield good results. All models use 7 slots.\n\n• Choice of Decoder (Sec. 4.3). This experiment uses 11 slots for MOVi-C, 6 slots for\n\nPASCAL VOC 2012, and 7 slots for COCO.\n\n• Object Property Prediction (Sec. B.3). We train the downstream models for 15 000 steps with the Adam optimizer with a starting learning rate of 10−3 and batch size 64. The learning rate is halved every 2 000 training steps. The non-linear predictor is a one-hidden layer MLP with 256 hidden neurons and leaky ReLU activation (slope 0.01). Continuous targets are standardized by computing mean and variance over the training dataset. We use center crops as input and filter objects that are not contained in the center crop.\n\n• Comparison of ResNet and Pre-Trained ViT Encoders (Sec. D.2). We train the ResNet models for 300k steps (instead of 500k steps as the models in our main experiments) due to computational constraints, but we found that they can also benefit from longer training.\n\nTable 18: Hyperparameters for DINOSAUR used for the results on MOVi-C, MOVi-E, PASCAL VOC 2012, COCO and KITTI datasets.\n\nDataset\n\nTraining Steps Batch Size LR Warmup Steps Peak LR Exp. Decay Half-Life ViT Architecture Patch Size Feature Dim. Dfeat Gradient Norm Clipping\n\nImage/Crop Size Cropping Strategy Augmentations Image Tokens\n\nDecoder\n\nSlot Attention\n\nType Layers Heads MLP Hidden Dim.\n\nSlots Iterations Slot Dim. Dslots MLP Hidden Dim.\n\nMOVi-C MOVi-E\n\nPASCAL\n\nCOCO KITTI\n\n500k 64 10000 0.0004 100k ViT-B 8\n768 1.0\n\n224 Full –\n784\n\n250k 64 10000 0.0004 100k ViT-B 16 768 1.0\n\n500k 64 10000 0.0004 100k ViT-B 16 768 1.0\n\n500k 64 10000 0.0004 100k ViT-B 8\n768 1.0\n\n224 Random\n\n224\n\n224 Center Random\n\nRandom Horizontal Flip 196\n\n196\n\n784\n\nMLP Transformer Transformer 4\n8 3072\n\n4 8\n3072\n\n4 –\n1024\n\n24 3\n128 512\n\n6 3\n256 1024\n\n7 3\n256 1024\n\nMLP 4\n– 2048\n\n9 3\n32 128\n\n500k 64 10000 0.0004 100k ViT-B 8\n768 1.0\n\n224 Full –\n784\n\nMLP 4\n– 1024\n\n11 3\n128 512\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nE.2 BASELINES\n\nBlock Masks The block mask patterns are generated by equally dividing the image into a specified number of columns, then equally dividing the columns such that a target number of slot masks is reached (see Fig. 18). For less than 9 masks, 2 columns are used, for 9–15 masks, 3 columns, and for more than 15 masks, 4 columns are used. For the block masks results given in the main text, MOVi-C uses 11 masks, MOVi-E uses 24 masks, PASCAL VOC 2012 uses 6 masks and COCO uses 7 masks. The number of masks is consistent with the number of slots used by the other methods on the respective datasets.\n\nFigure 18: Block masks for 11 and 24 slots.\n\nDINO K-Means We run the K-Means algorithm on the patch tokens output by the last DINO ViT block (before the final layer normalization). For the MOVi experiments, we use ViT-B/8, whereas for the PASCAL VOC 2012 and COCO experiments, we use ViT-B/16. The resulting low-dimensional clustering masks are resized to the target mask resolution required for evaluation, where we treat each cluster as an object instance. For K-Means, we use the standard scikit-learn implementation (sklearn.cluster.KMeans) with default parameters (10 repetitions, k-means++ init, maximum of 300 iterations). We only vary the number of clusters: for the results given in the main text, MOVi-C uses 11 clusters, MOVi-E uses 24 clusters, PASCAL VOC 2012 uses 6 clusters and COCO uses 7 clusters. The number of clusters is consistent with the number of slots used by the other methods on the respective datasets.\n\nSlot Attention The hyperparameters used to run the experiments can be found in Table 19. For the encoder, we use the ResNet34 as described above. The slot attention module largely follows the description given in App. E.1 for the DINOSAUR architecture. However, like Kipf et al. (2022), we add a linear positional encoding to the feature map (coordinates in the four cardinal directions scaled to [−1, 1], linearly projected to the feature dimension and added to the features). The number of slots was set to 11 and 24 for MOVi-C and MOVi-E, whereas for PASCAL and COCO we used 6 and 7 slots, to be consistent with DINOSAUR. For PASCAL and COCO, we verified that Slot Attention does not succeeded in successfully discovering objects over a range of number of slots (cf. Fig. 14). For the decoder, we use the same spatial broadcast decoder as (Kipf et al., 2022), i.e. 5 transposed convolutions with 5 × 5 kernels, channel size 64 and ReLU activations, upsampling from size 8 × 8 to 128 × 128. Input and target images are scaled to range [−1, 1].\n\nTable 19: Hyperparameters used for Slot Attention training on MOVi-C, MOVi-E, PASCAL VOC 2012 and COCO datasets.\n\nDataset\n\nTraining Steps Batch Size LR Warmup Steps Peak LR Annealing Gradient Norm Clipping\n\nImage/Crop Size Cropping Strategy Augmentations\n\nSlot Attention\n\nMOVi-C MOVi-E\n\nPASCAL VOC 2012 & COCO\n\n250k 64 2500 0.0002 Cosine 1.0\n\n128 Full –\n\n11 3\n256 512\n\n250k 64 2500 0.0002 Cosine 1.0\n\n250k 64 2500 0.0002 Cosine 1.0\n\n128 128 Full Random (PASCAL) / Center (COCO) Random Horizontal Flip\n\n–\n\n24 3\n256 512\n\n6 & 7 3\n256 512\n\nSlots Iterations Slot Dim. MLP Hidden Dim.\n\nSLATE We use the official SLATE implementation8. The hyperparameters are close to the ones used in SLATE (Singh et al., 2022a) and STEVE (Singh et al., 2022b) and are presented in Table 20.\n\n8https://github.com/singhgautam/slate\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nWe noticed that SLATE’s performance metrics began to degrade significantly after some training time; we picked the training time such that best performance was obtained.\n\nTable 20: Hyperparameters used for SLATE training on MOVi-C, MOVi-E, PASCAL VOC 2012 and COCO datasets.\n\nDataset\n\nTraining Steps Batch Size LR Warmup Steps Peak LR Dropout Gradient Norm Clipping\n\nDVAE\n\nImage Size Image Tokens\n\nTransformer Decoder\n\nSlot Attention\n\nMOVi-C MOVi-E\n\nPASCAL VOC 2012 & COCO\n\n40k 64 30000 0.0001 0.1 1.0\n\n40k 64 30000 0.0001 0.1 1.0\n\nVocabulary Size Temp. Cooldown Temp. Cooldown Steps LR\n\n4096 1.0 to 0.1 30000 0.0003\n\n4096 1.0 to 0.1 30000 0.0003\n\nLayers Heads Hidden Dim.\n\nSlots Iterations Slot Dim.\n\n128 1024\n\n8 4\n192\n\n11 3\n192\n\n128 1024\n\n8 4\n192\n\n24 3\n192\n\n50k & 40k 64 30000 0.0001 0.1 1.0\n\n4096 1.0 to 0.1 30000 0.0003\n\n128 1024\n\n8 4\n192\n\n6 & 7 3\n192\n\nSTEVE We use the official STEVE implementation9. We use the proposed configuration for the MOVi datasets and only change the number of slots to 11 (for MOVi-C) and 24 (for MOVi-E). We train the model for 20 000 steps.\n\nSAVi++ We use the official SAVi++ implementation10. We use the proposed configuration for MOVi-E and only change it to use fixed, learned slots instead of first-frame conditioning, and train the model with batch size 32 for 30 000 steps. We train with both image reconstruction and optical flow reconstruction.\n\nSTEGO To evaluate STEGO on the COCO-Stuff 171 dataset (Sec. C.2), we modified the official STEGO implementation11 to use batch k-means clustering with 172 clusters. Other parameters were not changed from the STEGO configuration on the COCO-Stuff 27 dataset as the training data is the same for those two datasets.\n\nFor the experiment evaluating STEGO on the MOVi datasets (Sec. C.3), we run a DINO ViT-B with patch size 8, using 22 and 27 clusters, and train for 20 000 steps. We manually tested different hyperparameters and settled on λself = 0.6, λknn = 0.3, λrand = 0.6, bself = 0.26, bknn = 0.21, brand = 0.4 for MOVi-C, and λself = 0.6, λknn = 0.3, λrand = 0.6, bself = 0.22, bknn = 0.17, brand = 0.4 for MOVi-E. We refer to Hamilton et al. (2022) for the meaning of those parameters.\n\nF DATASET AND EVALUATION SETTINGS\n\nIn this section, we provide details about the datasets, evaluation settings and metrics used in this work. An overview over which datasets are used for which task can be found in Table 21.\n\nMOVi As the MOVi datasets are video datasets, we convert them into an image dataset by sampling 9 random frames per clip. This yields 87 633 training images for MOVi-C and 87 741 images on\n\n9https://github.com/singhgautam/steve 10https://github.com/google-research/slot-attention-video/ 11https://github.com/mhamilton723/STEGO\n\n34\n\nGreff et al. (2022) Greff et al. (2022) Greff et al. (2022) Greff et al. (2022) Everingham et al. (2012) Everingham et al. (2012) and Hariharan et al. (2011) Everingham et al. (2012) Lin et al. (2014) Lin et al. (2014)\n\nCaesar et al. (2018), e.g. Ji et al. (2019) Caesar et al. (2018), new Geiger et al. (2013) Alhaija et al. (2018)\n\nPublished as a conference paper at ICLR 2023\n\nTable 21: Overview over tasks and datasets used in this work. For training, we only use images of the respective datasets, and no labels. For central crops, we first resize the mask such that the short side is 320 pixels long, then take the most centered crop of size 320 × 320.\n\nDataset\n\nImages Description\n\nCitation\n\nMOVi-C MOVi-C validation MOVi-E MOVi-E validation VOC 2012 validation VOC 2012 trainaug\n\nVal. split w. instance segm. labels\n\n87 633 Train split w. videos 6 000 87 741 Train split w. 6 000 1 449 10 582 Train split w. instance segm. labels\n\nVal. split w. instance segm. labels Val. split w. instance segm. labels\n\nimages\n\nVOC 2012 trainval COCO 2017 COCO 2017 validation 5 000 COCO-20k COCO-27\n\n11 540 Detection train and val. splits 118 287 Train split w. instance segm. labels\n\nVal split w. instance segm. labels\n\n20 000 COCO 2014 train& val subset w. det. labels e.g. Simeoni et al. (2021) 5 000\n\nSegm. labels from COCO-Stuff\n\nCOCO-171 KITTI KITTI segmentation\n\n5 000\n\n200\n\nSegm. labels from COCO-Stuff Raw videos Images w. instance segm. labels\n\nTask\n\nTraining Dataset\n\nEval Dataset\n\nEval Resolution Crop Type\n\nObject Discovery\n\nMOVi-C validation MOVi-E validation\n\nMOVi-C MOVi-E VOC 2012 trainaug VOC 2012 validation COCO 2017 KITTI\n\nCOCO 2017 validation KITTI segmentation\n\nObject Localization\n\nVOC 2012 trainaug VOC 2012 trainval COCO 2017\n\nCOCO-20k\n\nObject Segmentation\n\nVOC 2012 trainaug VOC 2012 validation COCO 2017\n\nCOCO 2017 validation\n\n128 × 128 128 × 128 320 × 320 320 × 320 1242 × 375\n\nOriginal Original\n\nOriginal Original\n\nFull Full Central Central Full\n\nFull Full\n\nFull Full\n\nScene Segmentation\n\nCOCO 2017 COCO 2017\n\nCOCO-27 COCO-171\n\n320 × 320 320 × 320\n\nCentral Central\n\nMOVi-E. For evaluation, we use all frames from the 250 clips, yielding 6 000 images. Note that we use the validation split provided by the MOVi dataset for evaluation, and not the test split containing out-of-distribution objects and backgrounds. This is in line with prior work (Kipf et al., 2022; Elsayed et al., 2022).\n\nFor the “semantic” MOVi-C variants used in Sec. D.7, we generated 250 videos using the Kubric simulator (Greff et al., 2022), yielding 2 250 images. For the “semantic objects” variant, after sampling the first object in each video, only objects of the same category were allowed to be sampled. The categories are: “Action Figures”, “Bag”, “Board Games”, “Bottles and Cans and Cups”, “Camera”, “Car Seat”, “Consumer Goods”, “Hat”, “Headphones”, “Keyboard”, “Legos”, “Media Cases”, “Mouse”, “Shoe”, “Stuffed Toys”, “Toys”. Objects that do not have one of those categories assigned were not considered. For the “duplicate objects” variant, the exact same object was resampled multiple times. Otherwise, the same settings as for MOVi-C were used.\n\nPASCAL VOC 2012 We train on the “trainaug” variant with 10 582 images. The trainaug variant is an unofficial split that has been used by prior work (Van Gansbeke et al., 2021; Melas-Kyriazi et al., 2022). It consists of 1 464 images from the segmentation train set, and 9 118 images from the SBD dataset (Hariharan et al., 2011). For object discovery and segmentation, we evaluate on the official segmentation validation set, with 1 449 images. For object localization, we evaluate on the detection “trainval” split (joint official training and validation splits), with 11 540 images, as standard for this task (Simeoni et al., 2021; Melas-Kyriazi et al., 2022). For unsupervised object segmentation, we use the 20 object classes plus one background class. For object discovery and segmentation, unlabeled pixels are ignored during evaluation.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\nCOCO We train on the COCO 2017 dataset with 118 287 images, and evaluate on the validation set with 5 000 images. For object discovery, we use both instance and segmentation masks, converting instance masks into segmentation masks using a per-pixel arg max over classes. Overlaps between instances are ignored during metric evaluation, and crowd instance annotations are not used. For unsupervised semantic segmentation, we use the COCO-Stuff annotations (Caesar et al., 2018), which contain the original 80 “things” classes from COCO, plus 91 “stuff” class covering background. In this labeling, overlaps have been manually resolved. We ignore pixels belonging to crowd instances during evaluation. For unsupervised object segmentation, we use the 80 “things” classes, and treat the “stuff” classes as background. For unsupervised scene decomposition, we use the COCO-Stuff 27 labeling (Ji et al., 2019; Cho et al., 2021), resulting from merging the 80 “things” classes into 12, and the 91 “stuff” classes into 15 super-classes, and the COCO-Stuff 171 labeling, using all “things” and “stuff” classes. For object localization, we evaluate on COCO-20k, with 19 817 images. COCO-20k is a subset of COCO 2014 training and validation images, filtered to remove images with only crowd instances and removing crowd annotations.\n\nEvaluation Settings For scene decomposition on COCO-Stuff 27 and COCO-Stuff 171, we take a center crop of the image after resizing the minor axis to 320 pixels and evaluate the masks at 320×320 resolution to be consistent with prior work (Ji et al., 2019; Hamilton et al., 2022). For object discovery on PASCAL VOC 2012 and COCO, we use the same settings. For object discovery on MOVi, we use the full image (at 128×128 resolution). Again to be consistent with prior work, for object localization on PASCAL VOC 2012 and COCO-20k as well as object segmentation on PASCAL VOC 2012 and COCO, we evaluate the full image after resizing to 224 × 224, ignoring aspect ratio. Our model proved to be robust to the aspect ratio distortion to some degree; however, we believe that results can be further improved, e.g. by taking the distortion into account during training, or by using multi-crop evaluation. Predicted soft probability masks are converted to hard masks using the arg max.\n\nFor foreground adjusted rand index (FG-ARI), we use object/instance masks and only Metrics evaluate pixels in the foreground. We treat unlabeled pixels (e.g. on PASCAL) and pixels from overlapping instance masks (e.g. on COCO) as background (i.e. they are not evaluated).\n\nFor mean best overlap (mBO), we use the object masks on MOVi, and both instance and segmentation masks on PASCAL/COCO. Each ground truth mask (excluding the background mask) is assigned the predicted mask with the largest overlap in terms of IoU. mBO is computed as the average IoU over the assigned mask pairs. We do not count unlabeled pixels (e.g. on PASCAL) and pixels from overlapping instance masks (e.g. on COCO) when computing IoU.\n\nG ADDITIONAL EXAMPLES\n\nWe include additional mask predictions of our model for MOVi-C (Fig. 19), for MOVi-E with 24 (Fig. 20) and 11 slots (Fig. 21), for PASCAL VOC 2012 (Fig. 22) and COCO (Fig. 23, Fig. 24, Fig. 25, Fig. 26).\n\n36\n\nPublished as a conference paper at ICLR 2023\n\nImage\n\nMLP Decoder\n\nTransformer Decoder\n\nImage\n\nMLP Decoder\n\nTransformer Decoder\n\nFigure 19: Masks on MOVi-C produced by our method, using 11 slots and a ViT-B/16 encoder. We show predictions from the MLP and the Transformer decoder.\n\ne g\na\n\nm\n\nI\n\nR U\nA S\nO N\nI D\n\nn o\n\ni t\n\nn e\nt t\n\nA\n\nt\n\no\n\nl\n\nS\n\nE T\nA L\nS\n\nFigure 20: Masks on MOVi-E using 24 slots. Our method uses a ViT-B/8 encoder and the MLP decoder.\n\n37\n\nPublished as a conference paper at ICLR 2023\n\ne g\na\n\nm\n\nI\n\nR U\nA S\nO N\nI D\n\nn o\n\ni t\n\nn e\nt t\n\nA\n\nt\n\no\n\nl\n\nS\n\nE T\nA L\nS\n\nFigure 21: Masks on MOVi-E using 11 instead of 24 slots (quantitative results in Table 13). Our method uses a ViT-B/8 encoder and the MLP decoder.\n\n38\n\nPublished as a conference paper at ICLR 2023\n\nGround Truth\n\nMLP Decoder\n\nTransformer Decoder\n\nGround Truth\n\nMLP Decoder\n\nTransformer Decoder\n\nFigure 22: Masks on PASCAL VOC 2012 produced by our method, using 6 slots and a ViT-B/16 encoder. We show predictions from the MLP and the Transformer decoder.\n\n39\n\nPublished as a conference paper at ICLR 2023\n\nGround Truth\n\nMLP Decoder\n\nTransformer Decoder\n\nGround Truth\n\nMLP Decoder\n\nTransformer Decoder\n\nFigure 23: Masks on COCO 2017 produced by our method, using 7 slots and a ViT-B/16 encoder. We show predictions from the MLP and the Transformer decoder.\n\n40\n\nPublished as a conference paper at ICLR 2023\n\nViT-B/16 Supervised\n\nViT-S/16 DINO\n\nViT-B/16 DINO\n\nViT-S/8 DINO\n\nViT-B/16 MoCo-v3\n\nViT-B/16 MSN\n\nViT-B/16 MAE\n\nFigure 24: Masks on COCO 2017 produced by our method, using different encoders and pre-training schemes on COCO 2017 (cf. Table 11). All models use 7 slots and the Transformer decoder.\n\n41\n\nPublished as a conference paper at ICLR 2023\n\nRN50 Supervised\n\nRN50 DINO\n\nRN34 (S) DINO RN50\n\nRN50 MoCo-v3\n\nRN34 (S) MoCo-v3 RN50\n\nViT-B/16 DINO\n\nRN34 (S) DINO ViT\n\nFigure 25: Masks on COCO 2017 produced by our method, using different encoders and pre-training schemes on COCO 2017. ResNet34s (RN34) were trained from scratch (“S”), by reconstruction of ResNet50 (RN50) or ViT-B/16 features (cf. Table 12). All models use 7 slots and the MLP decoder.\n\n42\n\nPublished as a conference paper at ICLR 2023\n\nViT-B/16 MoCo-v3\n\nRN34 (S) MoCo-v3 ViT\n\nViT-B/16 MSN\n\nRN34 (S) MSN ViT\n\nViT-B/16 MAE\n\nRN34 (S) MAE ViT\n\nFigure 26: Masks on COCO 2017 produced by our method, using different encoders and pre-training schemes on COCO 2017. ResNet34s (RN34) were trained from scratch (“S”), by reconstruction of ViT-B/16 features (cf. Table 12). All models use 7 slots and the MLP decoder.\n\n43",
    "reference": "# Summary Of The Paper\n\nThe paper proposes an object-centric representation learning model that targets large-scale real-world datasets. To do so, they propose to learn the object-centric representations by reconstructing the features of a pre-trained encoder (DINO). The hypothesis is that the features pre-trained on large-scale datasets contain semantically meaningful information which provides stronger semantical bias than the low-level features for object grouping. To evaluate the method, the paper provides quantitative and qualitative evaluations of segmentation quality on synthetic datasets with object-centric baselines and real-world datasets with unsupervised semantic segmentation baselines.\n\n# Strength And Weaknesses\n\nStrengths:\n\n- The topic of developing scalable object-centric representation learning models is of significance to the community.\n- As the first paper to test an object-centric model on large-scale real-world datasets like COCO, it is a promising endeavor in this direction.\n- The experiment results show that the proposed method achieves improvement over previous object-centric learning models (though containing questionable results) in segmentation and object detection.\n\nWeaknesses:\n\n- Lack of evaluation of the representation quality\n    - The proposed method is only evaluated on object detection and segmentation tasks. The quality of the learned representations, which is one key aspect of representation learning, is not evaluated. Additional downstream tasks using the slot representations, such as properties prediction, should be included to address this problem.\n- The quantitative results of the baseline model SLATE [1] are questionable.\n    - The experiment results on the baseline model SLATE show very poor performance on object segmentation compared to other models including slot attention. In the MOVI series dataset, SLATE’s mBO score is even close to or worse than the Block Pattern baseline which uses predefined regular block masks. This is questionable, as we have also conducted a similar experiment on various synthetic and real-world datasets, and SLATE outperforms slot attention on all datasets for object segmentation and could achieve higher scores than that reported in the paper for the MOVI datasets. Some investigation might be needed to address this question.\n    - The corresponding qualitative evaluations are also missing. Please include them in the qualitative evaluation.\n- The claim of the method is to achieve object-centric learning in real-world images. However, the method seems to provide only semantic-level segmentation on real-world (or possibly all) datasets.\n    - The experiments show that the method tends to provide semantic-level segmentation on real-world datasets while instance-level segmentation on synthetic datasets (MOVI). Since the MOVI dataset rarely contains multiple same-class objects in one image, i.e., instance-level segmentation and semantic-level segmentation might have similar results. Therefore, the model could actually be doing semantic-level grouping on all datasets. Please verify this part.\n- This is important. Semantic segmentation baselines should also be included in the MOVI experiment\n    - Given the fact that the proposed method does semantic-level grouping in real-world datasets, it seems necessary to include some semantic segmentation baselines in the object-centric learning tasks. More specifically, please provide the set of MOVI series evaluation tasks on some of the unsupervised semantic segmentation baselines in Table 4 with proper slots/clusters number.\n    - This is important because it demonstrates if the existing unsupervised semantic segmentation models already solve the object grouping problem in the multi-object datasets. This might reveal some limitations of existing synthetic multi-object datasets commonly used in evaluating object-centric learning models. The comparison results will also be insightful in explaining the difference between the method and the unsupervised semantic segmentation baselines.\n\n[1] Singh, G., Deng, F., & Ahn, S. (2021). Illiterate DALL-E Learns to Compose. *arXiv*. https://doi.org/10.48550/arXiv.2110.11405\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The paper is easy to follow for the most part.\n- The model design is limited in novelty. The slots encoder and autoregressive decoder are both proposed in prior work. The idea of reconstructing features instead of pixels using slot is novel.\n- The paper seems to provide enough information to reproduce the results.\n\nAdditional question:\n\n- Is there a reason why mBO is used instead of mIoU which involves object matching and could potentially provide a more accurate evaluation of the segmentation quality?\n\n# Summary Of The Review\n\nThis paper aims to develop scalable object-centric learning models for real-world scenes. The topic is important, and the proposed model is the first attempt in this direction. Experiments also seem to suggest that the method provides an improvement over the prior methods. However, as explained in the weakness section, the evaluation contains key problems to be addressed. I am willing to amend my score, if the questions are properly addressed.\n\n---\n\n## Post-rebuttal updates:\n\nSince the rebuttal has partially addressed my concerns, I am now more inclined to accept the paper and have raised my **Recommendation** score to 6. However, due to some of the issues outlined in my response to the rebuttal, I still find this paper on the borderline. See [here](https://openreview.net/forum?id=b9tUk-f_aG&noteId=8vp4F8U2You) for details.\n\nThis paper presents an interesting topic, yet further investigation is needed to answer the lingering questions. The authors should ensure a more comprehensive exploration in the camera-ready version.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDISTRIBUTIONAL REINFORCEMENT LEARNING VIA SINKHORN ITERATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDistributional reinforcement learning (RL) is a class of state-of-the-art algorithms that estimate the entire distribution of the total return rather than only its expectation. The empirical success of distributional RL is determined by the representation of return distributions and the choice of distribution divergence. In this paper, we propose a new class of Sinkhorn distributional RL (SinkhornDRL) algorithm that learns a finite set of statistics, i.e., deterministic samples, from each return distribution and then uses Sinkhorn iterations to evaluate the Sinkhorn distance between the current and target Bellmen distributions. Sinkhorn divergence features as the interpolation between the Wasserstein distance and Maximum Mean Discrepancy (MMD). SinkhornDRL finds a sweet spot by taking advantage of the geometry of optimal transport-based distance and the unbiased gradient estimate property of MMD. Finally, compared to state-of-the-art algorithms, SinkhornDRL’s competitive performance is demonstrated on the suite of 55 Atari games.\n\n1\n\nINTRODUCTION\n\nClassical reinforcement learning (RL) algorithms are normally based on the expectation of discounted cumulative rewards that an agent observes while interacting with the environment. Recently, a new class of RL algorithms called distributional RL estimates the full distribution of total returns and has exhibited the state-of-the-art performance in a wide range of environments (Bellemare et al., 2017a; Dabney et al., 2018b;a; Yang et al., 2019; Zhou et al., 2020; Nguyen et al., 2020).\n\nIn distributional RL literature, it is easily recognized that algorithms based on either Wasserstein distance or MMD have gained great attention due to their superior performance. Their mutual connection from the perspective of mathematical properties intrigues us to explore further in order to design new algorithms. Particularly, Wasserstein distance, long known to be a powerful tool to compare probability distributions with non-overlapping supports, has recently emerged as an appealing contender in various machine learning applications. It is known that Wasserstein distance was long disregarded because of its computational burden in its original form to solve an expensive network flow problem. However, recent works (Sinkhorn, 1967; Genevay et al., 2018) have shown that this cost can be largely mitigated by settling for cheaper approximations through strongly convex regularizers. The benefit of this regularization has opened the path to wider applications of the Wasserstein distance in relevant learning problems, including the design of distributional RL algorithms.\n\nThe Sinkhorn divergence (Sinkhorn, 1967) introduces the entropic regularization on the Wassertein distance, allowing it tractable for the evaluation especially in high-dimensions. It has been successfully applied in numerous crucial machine learning developments, including the SinkhornGAN (Genevay et al., 2018) and Sinkhorn-based adversarial training (Wong et al., 2019). More importantly, it has been shown that Sinkhorn divergence interpolates Wasserstein ditance and MMD, and their equivalence form can be well established in the limit cases (Feydy et al., 2019; Ramdas et al., 2017; Nguyen et al., 2020). However, a Sinkhorn-based distributional RL algorithm has not yet been formally proposed and its connection with algorithms based on Wasserstein distance and MMD is also less studied. Therefore, a natural question is can we design a new class of distributional RL algorithms via Sinkhorn divergence, thus bridging the gap between existing two main branches of distributional RL algorithms? Moreover, the dominant quantile regression-based algorithms, e.g., QR-DQN (Dabney et al., 2018b), aimed at approximating Wasserstein distance, suffers from\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe non-crossing issue in the quantile estimation (Zhou et al., 2020), while sample-based Sinkhorn distributional algorithm can naturally circumvent this problem.\n\nIn this paper, we propose a novel distributional RL family based on Sinkhorn divergence. Firstly, we show key roles of distribution divergence and value distribution representation in the design of distributional RL algorithms. After a detailed introduction of our proposed SinkhornDRL algorithm, with a non-trivial proof, we theoretically analyze the convergence property of distributional Bellman operators under Sinkhorn divergence. A regularized MMD equivalence with Sinkhorn divergence is also established, interpreting its empirical success in real applications. Finally, we compare the performance of our SinkhornDRL algorithm with typical baselines on 55 Atari games, verifying the competitive performance of our proposal. Our method inspires researchers to find a trade-off that simultaneously leverages the geometry of the Wasserstein distance and the favorable unbiased gradient estimate property of MMD while designing new distributional RL algorithms in the future.\n\n2 PRELIMINARY KNOWLEDGE\n\n2.1 DISTRIBUTIONAL REINFORCEMENT LEARNING\n\nIn classical RL, an agent interacts with an environment via a Markov decision process (MDP), a 5-tuple (S, A, R, P, γ), where S and A are the state and action spaces, respectively. P is the environment transition dynamics, R is the reward function and γ ∈ (0, 1) is the discount factor.\n\nFrom Value function to Value distribution. Given a policy π, the discounted sum of future rewards is a random variable Z π(s, a) = (cid:80)∞ t=0 γtR(st, at), where s0 = s, a0 = a, st+1 ∼ P (·|st, at), and at ∼ π(·|st). In the control setting, expectation-based RL is based on the action-value function Qπ(s, a), which is the expectation of Z π(s, a), i.e., Qπ(s, a) = E [Z π(s, a)]. By contrast, distributional RL focuses on the action-value distribution, the full distribution of Z π(s, a). The incorporation of additional distributional knowledge intuitively interprets its empirical success.\n\nDistributional Bellman operators. For the policy evaluation in expectation-based RL, the actionvalue function is updated via Bellman operator T πQ(s, a) = E[R(s, a)] + γEs(cid:48)∼p,π [Q (s(cid:48), a(cid:48))]. In distributional RL, the distribution of Z π(s, a) is updated via the distributional Bellman operator Tπ\n\nTπZ(s, a) :D= R(s, a) + γZ (s(cid:48), a(cid:48)) ,\n\n(1)\n\nwhere s(cid:48) ∼ P (·|s, a) and a(cid:48) ∼ π (·|s(cid:48)). The equality implies random variables of both sides are equal in distribution. The distributional Bellman operator Tπ is contractive under certain distribution divergence metrics. We provide a detailed discussion about more related works in Appendix A.\n\n2.2 DIVERGENCES BETWEEN MEASURES\n\nOptimal Transport (OT) and Wasserstein Distance. The optimal transport (OT) metric between two probability measures (μ, ν) is defined as the solution of the linear program:\n\n(cid:90)\n\nmin Π∈Π(μ,ν)\n\nc(x, y)dΠ(x, y),\n\n(2)\n\nwhere c is the cost function and Π is the joint distribution with marginals (μ, ν). Wasserstein distance (a.k.a. earth mover distance) is a special case of optimal transport with the Euclidean norm as the cost function. In particular, given two scalar random variables X and Y , p-Wasserstein metric Wp between the distributions of X and Y can be simplified as\n\nWp(X, Y ) =\n\n(cid:18)(cid:90) 1\n\n0\n\n(cid:12) (cid:12)F −1\n\nX (ω) − F −1\n\nY (ω)(cid:12)\n\n(cid:12)\n\n(cid:19)1/p\n\n,\n\np\n\ndω\n\n(3)\n\nwhere F −1 is the inverse cumulative distribution function of a random variable. The desirable geometric property of Wasserstein distance allows it to recover full support of measures, but it suffers from the curse of dimension (Genevay et al., 2019; Arjovsky et al., 2017).\n\nMaximum Mean Discrepancy. The squared Maximum Mean Discrepancy (MMD) MMD2 the kernel k is formulated as MMD2\n\nk = E [k (X, X (cid:48))] + E [k (Y, Y (cid:48))] − 2E [k(X, Y )] ,\n\nk with\n\n(4)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nwhere k(·, ·) is a continuous kernel on X . X (cid:48) (resp. Y (cid:48)) is a random variable independent of X (resp. Y ). If k is a trivial kernel, MMD degenerates to the energy distance. Mathematically, the “flat” geometry that MMD induces on the space of probability measures does not faithfully lift the ground distance (Feydy et al., 2019), but MMD is cheaper to compute than OT and has a smaller sample complexity, i.e., approximating the distance with samples of measures (Genevay et al., 2019). We provide the detailed introduction of more distribution divergences in Appendix B.\n\n3 ROLES OF DISTRIBUTION DIVERGENCE AND REPRESENTATION\n\n3.1 DISTRIBUTIONAL RL: FROM NEURAL FITTED Q TO Z ITERATION\n\nNeural Fitted Q-Iteration. It is known that Deep Q Networks (Mnih et al., 2015) can be simplified into Neural Fitted Q-Iteration (Fan et al., 2020) under tricks of experience replay and the target network, where we update Qθ(s, a) parameterized by θ in each iteration k:\n\nQk+1\n\nθ = argmin\n\nQθ\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:2)yi − Qk\n\nθ (si, ai)(cid:3)2\n\n,\n\n(5)\n\nwhere the target yi = r(si, ai) + γ maxa∈A Qk i, a) is fixed within every Ttarget steps to update target network Qθ∗ with parameters θ∗ by letting θ∗ = θ and the experience buffer induces independent samples {(si, ai, ri, s(cid:48) i)}i∈[n]. In an ideal case that neglects the non-convexity and TD approximation errors, we have Qk+1 θ , which is exactly equivalent to the updating rule under Bellman optimality operator.\n\nθ = T Qk\n\nθ∗ (s(cid:48)\n\nNeural Fitted Z-Iteration. Analogous to Neural Fitted Q-iteration, we can also simplify valuebased distributional RL methods based on a parameterized Zθ into a Neural Fitted Z-Iteration as\n\nZ k+1\n\nθ = argmin\n\nZθ\n\n1 n\n\nn (cid:88)\n\ni=1\n\ndp(Yi, Z k\n\nθ (si, ai)),\n\n(6)\n\nwhere the target Yi = R(si, ai) + γZ k fixed within every Ttarget steps to update target network Zθ∗ , and dp is the distribution divergence.\n\ni, πZ(s(cid:48))) with πZ(s(cid:48)) = argmaxa(cid:48) E (cid:2)Z k\n\nθ∗ (s(cid:48), a(cid:48))(cid:3) is\n\nθ∗ (s(cid:48)\n\n3.2 KEY ROLES OF dp AND Zθ\n\nWithin the Neural Fitted Z-Iteration framework proposed in Eq. 6, we observe that the choice of representation manner on Zθ and the metric dp are pivotal for the distributional RL algorithms. For instance, QR-DQN (Dabney et al., 2018b) approximates Wasserstein distance Wp, which leverages quantiles to represent the distribution of Zθ. C51 (Bellemare et al., 2017a) represents Zθ via a categorical distribution under the convergence of Cram ́er distance (Bellemare et al., 2017b; Rowland et al., 2018), while MMD distributional RL (MMDDRL) (Nguyen et al., 2020) learns samples to represent the distribution of Zθ based on MMD. We compare characteristics of these distribution divergence, including the convergence rate and sample complexity, in Table 1. Theoretical results regarding Sinkhorn divergence is based on (Genevay et al., 2019) and the detailed convergence proof of other distances is also provided in Appendix B. In summary, we argue that dp and Zθ are two crucial factors in distributional RL design, based on which we introduce Sinkhorn distributional RL.\n\ndp Distribution Divergence Representation Zθ Convergence Rate of Tπ\n\nSample Complexity of dp\n\nAlgorithm C51 QR-DQN MMDDRL\n\nSinkhornDRL (ours)\n\nCram ́er distance Wasserstein distance MMD\n\nHistogram Quantiles Samples\n\nSinkhorn divergence\n\nSamples\n\n√\n\nγ\n\nγ γα/2 with kernel kα γ (ε → 0) γα/2 (ε → ∞)\n\nO(n− 1 d ) O(1/n)\n\nκ ε\ne ε(cid:98)d/2(cid:99)\n\n√\n\nO(n\n\nn ) (ε → 0)\n\nO(n− 1\n\n2 ) (ε → ∞)\n\nTable 1: Comparison between typical distributional RL algorithms under different distribution divergences and represention of Zθ. kα = −(cid:107)x − y(cid:107)α in MMDDRL, d is the sample dimension and κ = 2βd + (cid:107)c(cid:107)∞, where the cost function c is β-Lipschitz (Genevay et al., 2019). Sample complexity of MMD can be improved to O(1/n) using kernel herding technique (Chen et al., 2012).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n4 SINKHORN DISTRIBUTIONAL RL (SINKHORNDRL)\n\nIn this section, we firstly introduce Sinkhorn divergence and apply it in distributional RL. Next, we conduct a theoretical analysis about the convergence and a new moment matching manner of our algorithm under the Sinkhorn divergence. Finally, a practical Sinkhorn iteration algorithm is introduced to evaluate the Sinkhorn divergence.\n\n4.1 SINKHORN DIVERGENCE AND GENETIC ALGORITHM\n\nWe design Sinkhorn distributional RL algorithm via Sinkhorn divergence. Sinkhorn divergence (Sinkhorn, 1967) is a tractable loss to approximate the optimal transport problem by leveraging an entropic regularization to turn the original Wasserstein distance into a differentiable and more robust quantity. The resulting loss can be computed using Sinkhorn fixed point iterations, which is naturally suitable for modern deep learning frameworks. In particular, the entropic smoothing generates a family of losses interpolating between MMD. As such, it allows us to find a sweet trade-off that simultaneously leverages the geometry of Wasserstein distance on the one hand, and the favorable high-dimensional sample complexity and unbiased gradient estimates of MMD. We introduce the entropic regularized Wassertein distance Wc,ε(μ, ν) as\n\n(cid:90)\n\nmin Π∈Π(μ,ν)\n\nc(x, y)dΠ(x, y) + εKL(Π|μ ⊗ ν),\n\n(7)\n\nwhere KL(Π|μ ⊗ ν) = (cid:82) log dΠ(x, y) is a strongly convex regularization. The impact of this entropy regularization is similar to (cid:96)2 ridge regularization in linear regression. Next, the sinkhorn loss (Feydy et al., 2019; Genevay et al., 2018) between two measures μ and ν is defined as\n\ndμ(x)dν(y)\n\n(cid:16) Π(x,y)\n\n(cid:17)\n\nW c,ε(μ, ν) = 2Wc,ε(μ, ν) − Wc,ε(μ, μ) − Wc,ε(ν, ν).\n\n(8)\n\nAs demonstrated by (Feydy et al., 2019), the Sinkhorn divergence W c,ε(μ, ν) is convex, smooth and positive definite that metrizes the convergence in law. In statistical physics, Wc,ε(μ, ν) can be re-factored as a projection problem:\n\nWc,ε(μ, ν) := min\n\nΠ∈Π(μ,ν)\n\nKL (Π|K) ,\n\n(9)\n\nwhere K is the Gibbs distribution and its density function satisfies dK(x, y) = e− c(x,y) ε dμ(x)dν(y). This problem is often referred to as the “static Schr ̈odinger problem” (L ́eonard, 2013; R ̈uschendorf & Thomsen, 1998) as it was initially considered in statistical physics.\n\nDistributional RL with Sinkhorn Divergence and Particle Representation. The key of applying Sinkhorn divergence in distributional RL is to simply leverage the Sinkhorn loss W c,ε to measure the distance between the current action-value distribution Zθ(s, a) and the target distribution TπZθ(s, a), yielding W c,ε(Zθ(s, a), TπZθ(s, a)) for each s, a pairs. In terms of the representation for Zθ(s, a), we employ the unrestricted statistics, i.e., deterministic samples, due to its superiority in MMDDRL (Nguyen et al., 2020), instead of using predefined statistic functionals, e.g., quantiles in QR-DQN (Dabney et al., 2018b) or categorical distribution in C51 (Bellemare et al., 2017a).\n\nAlgorithm 1 Generic Sinkhorn distributional RL Update Require: Number of generated samples N , the cost function c and hyperparameter ε. Input: Sample transition (s, a, r(cid:48), s(cid:48))\n\na∗ ∼ π(·|s(cid:48)).\n\n1: if Policy evaluation then 2: 3: else 4: 5: end if 6: TZi ← r + γZθ∗ (s(cid:48), a∗)i , ∀1 ≤ i ≤ N {Zθ(s, a)i}N Output: W c,ε\n\na∗ ← arg maxa(cid:48)∈A\n\n(cid:80)N\n\n1 N\n\n(cid:16)\n\ni=1 Zθ (s(cid:48), a(cid:48))i\n\ni=1 , {TZθ(s, a)j}N\n\nj=1\n\n(cid:17)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nMore concretely, we use neural networks to generate samples that approximate the value distribution. This can be expressed as Zθ(s, a) := {Zθ(s, a)i}N i=1, where N is the number of generated samples. We refer to the samples {Zθ(s, a)i}N i=1 as particles. Then we leverage the Dirac mixture 1 i=1 δZθ(s,a)i to approximate the true density function of Z π(s, a), thus minimizing the Sinkhorn divergence between the approximate distribution and its distributional Bellman target. A detailed and generic distributional RL algorithm with Sinkhorn divergence and particle representation is provided in Algorithm 1.\n\n(cid:80)N\n\nN\n\nRemark. From the general algorithm framework in Algorithm 1, our SinkhornDRL generally modifies the distribution divergence comparing the state-of-the-art MMDDRL (Nguyen et al., 2020), but SinkhornDRL fundamentally falls into Wasserstein distance-based distributional RL family as discussed in Appendix A. As such, QR-DQN and MMDDRL are direct counterparts for SinkhornDRL, and the follow-up works IQN (Dabney et al., 2018a) and FQF (Yang et al., 2019) can naturally extend both MMDDRL and SinkhornDRL as discussed in (Nguyen et al., 2020).\n\n4.2 THEORETICAL ANALYSIS UNDER SINKHORN DIVERGENCE\n\nConvergence. Firstly, we denote the supreme form of Sinkhorn divergence as W\n\n∞\n\nc,ε(μ, ν):\n\nW\n\n∞\n\nc,ε(μ, ν) =\n\nsup (x,a)∈S×A\n\nW c,ε(μ(x, a), ν(x, a)).\n\n(10)\n\nWe will use W\n\n∞\n\nc,ε(μ, ν) to establish the convergence of Tπ in Theorem 1.\n\nTheorem 1. If we leverage Sinkhorn loss W c,ε(μ, ν) in Eq. 8 as the distribution divergence in distributional RL, and choose the unrectified kernel kα := −(cid:107)x − y(cid:107)α as −c (α > 0), it holds that\n\n(1) (ε → 0) W c,ε(μ, ν) → 2Wα(μ, ν). When ε = 0, Tπ is a γ-contraction under W\n\n∞\n\nc,ε.\n\n(2) (ε → +∞) W c,ε(μ, ν) → MMD2 kα\n\n(μ, ν). When ε = +∞, Tπ is γα/2-contractive under W\n\n∞\n\nc,ε.\n\n(3) (ε ∈ (0, +∞)), Tπ is a contractive operator under W c,ε. The related non-constant contraction factor ∆(γ, α) < 1 also depends on the distribution sequence in distributional Bellman iterations.\n\n∞\n\nWe provide the long yet rigorous proof of Theorem 1 in Appendix C. Theorem 1 (1) and (2) are follow-up conclusions in terms of the convergence behavior of Tπ based on the interpolation relationship between Sinkhorn divergence with Wasserstein distance and MMD (Genevay et al., 2018). Our key theoretical contribution is for the general ε ∈ (0, ∞), in which we conclude that Tπ is a contractive operator. The crux of the proof is two-fold. Firstly, we show the a variant of scale sensitive property of Sinkhorn divergence when c = −κα, where the resulting non-constant scaling factor is also determined by the specified two probability measures. Next, we propose a new distribution Contraction mapping theorem in Theorem 2 of Appendix C, based on which we eventually arrive at the convergence of distributional Bellman operator under W c,ε. Intriguingly yet reasonably, the contraction factor ∆(γ, α) is non-constant but a function less than 1 that also depends on the distribution sequence while iteratively applying distribution Bellman updates. Our non-trivial proof about Sinkhorn divergence can even contribute to the optimal transport literature.\n\n∞\n\nConsistency with Related Conclusions. As Sinkhorn divergence interpolates between Wasserstein distance and MMD, its contraction property when the cost function holds c = −kα for the general ε ∈ [0, ∞] is intuitive. Note that if we choose Gaussian kernels as the cost function, there will be no concise and consistent contraction results as Theorem 1 (3). This conclusion is also consistent with MMDDRL (Nguyen et al., 2020), where Tπ is generally not a contraction operator under MMD equipped with Gaussian kernels as a counterexample has been pointed out in MMDDRL (when ε → +∞). To be consistent with the contraction property analyzed in our theory (Theorem 1 (3)), we employ the rectified kernel kα as the cost function in our experiments and set α = 2, under which SinkhornDRL suggests a favorable performance in Section 5.\n\nRegularized Moment Matching under Sinkhorn Divergence Associated with Gaussian Kernels. We further examine the potential connection between SinkhornDRL with existing distributional RL families. Inspired by the similar manner in MMDDRL (Nguyen et al., 2020), we find that the Sinkhorn divergence with the Gaussian kernel can also promote to match all moments be-\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ntween two distributions. More specifically, the Sinkhorn divergence can be rewritten as a regularized moment matching form in Proposition 1.\n\nProposition 1. For ε ∈ (0, +∞), Sinkhorn divergence W c,ε(μ, ν) associated with Gaussian kernels k(x, y) = exp(−(x − y)2/(2σ2)) as −c, is equivalent to\n\nW c,ε(μ, ν) :=\n\n∞ (cid:88)\n\nn=0\n\n1 σ2nn!\n\n(cid:16) ̃Mn(μ) − ̃Mn(ν)\n\n(cid:17)2\n\n+ εE\n\n(cid:20)\n\nlog\n\n(Π∗\n\nε(X, Y ))2\n\nΠ∗\n\nε(X, X (cid:48))Π∗\n\nε(Y, Y (cid:48))\n\n(cid:21)\n\n,\n\n(11)\n\nε denotes the optimal Π determined by ε by evaluating the Sinkhorn divergence via\n\nwhere Π∗ minΠ∈Π(μ,ν) W c,ε(μ, ν). ̃Mn(μ) = Ex∼μ\n\n(cid:104)\n\ne−x2/(2σ2)xn(cid:105)\n\n, and similarly for ̃Mn(ν).\n\nWe provide the proof of Proposition 1 in Appendix D. Similar to MMDDRL associated with a Gaussian kernel (Nguyen et al., 2020), Sinkhorn divergence approximately performs a regularized moment matching scaled by e−x2/(2σ2).\n\nEquivalence to Regularized MMD Distributional RL. Based on Proposition 1, we can immediately establish the connection between Sinkhorn divergence and MMD in Corollary 1, indicating that minimizing Sinkhorn divergence between two distributions is equivalent to minimizing a regularized squared MMD. Corollary 1. For ε ∈ (0, +∞) and denote Π∗ gence, it holds that\n\nε as the optimal Π by evaluating the Sinkhorn diver-\n\nW c,ε := MMD2\n\n−c(μ, ν) + εE\n\n(cid:20)\n\nlog\n\nwhere we use W c,ε to replace W c,ε(μ, ν) for short.\n\n(Π∗\n\nε(X, Y ))2\n\nΠ∗\n\nε(X, X (cid:48))Π∗\n\nε(Y, Y (cid:48))\n\n(cid:21)\n\n,\n\n(12)\n\nProof of Corollary 1 is provided in Appendix D. It is worthy of noting that this equivalence is established for the general case when ε ∈ (0, +∞), and it does not hold in the limit cases when ε → 0 or +∞. For example, when ε → +∞, the second part including ε in Eq. 12 is not expected to dominate. This is owing to the fact that the regularization term would be 0 as Π∗ ε → μ ⊗ ν when ε → +∞. In summary, even though the Sinkhorn divergence was initially proposed to serve as an entropy regularized Wasserterin distance when the cost function c = κα, it turns out that it is equivalent to a regularized MMD if associated with Gaussian kernels, as revealed in Corollary 1.\n\n4.3 DISTRIBUTIONAL RL VIA SINKHORN ITERATIONS\n\nThe theoretical analysis in Section 4.2 sheds light on the behavior of distributional RL with Sinkhorn divergence, but another crucial issue we need to address is how to evaluate the Sinkhorn loss effectively. Due to the advantages of Sinkhorn divergence that both enjoys geometry property of optimal transport and the computational effectiveness of MMD, we can utilize Sinkhorn’s algorithm, i.e., Sinkhorn Iterations (Sinkhorn, 1967; Genevay et al., 2018), to evaluate the Sinkhorn loss. Notably, Sinkhorn iteration with L steps yields a differentiable and solvable efficiently loss function as the main burden involved in it is the matrix-vector multiplication, which streams well on the GPU with simply adding extra differentiable layers on the typical deep neural network, such as a DQN architecture. Specifically, given two sample sequences {Zi}N the optimal transport distance is equivalent to the form:\n\nj=1 in the distributional RL algorithm,\n\ni=1 , {TZj}N\n\n(cid:8)(cid:104)P, ˆc(cid:105); P 1N = 1N , P (cid:62)1N = 1N\n\n(cid:9) ,\n\nmin P ∈RN ×N\n\n+\n\n(13)\n\nwhere the empirical cost function ˆci,j = c(Zi, TZj). By adding entropic regularization on optimal transport distance, Sinkhorn divergence can be viewed to restrict the search space of P in the following scaling form:\n\nPi,j = aiKi,jbj,\n\n(14)\n\nwhere Ki,j = e−ˆci,j /ε is the Gibbs kernel defined in Eq. 9. This allows us to leverage iterations regarding the vectors a and b. More specifically, we initialize b0 = 1N , and then the Sinkhorn\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Sinkhorn Iterations to Approximate W c,ε\n\n(cid:16)\n\n{Zi}N\n\ni=1 , {TZj}N\n\nj=1\n\n(cid:17)\n\nInput: Two samples sequences {Zi}N rameter ε.\n\ni=1 , {TZj}N\n\nj=1, number of Sinkhorn iterations L and hyperpa-\n\n1: ˆci,j = c(Zi, TZj) for ∀i = 1, ..., N, j = 1, ..., N 2: Ki,j = exp(−ˆci,j/ε) 3: b0 ← 1N 4: for l = 1, 2, ..., L do 5: 6: end for (cid:16) 7: (cid:99)W c,ε\n\n= (cid:104)(K (cid:12) ˆc)b, a(cid:105)\n\n, bl ← 1N Kal\n\nal ← 1N\n\ni=1 , {TZj}N\n\nKbl−1\n\n(cid:17)\n\nj=1\n\n{Zi}N (cid:16)\n\nReturn: (cid:99)W c,ε\n\n{Zi}N\n\ni=1 , {TZj}N\n\nj=1\n\n(cid:17)\n\niterations are expressed as\n\nal+1 ←\n\n1N Kbl\n\nand\n\nbl+1 ←\n\n1N K(cid:62)al+1\n\n,\n\n(15)\n\nwhere · · indicates an entry-wise division. It has been proven that Sinkhorn iteration asymptotically converges to the true loss in a linear rate (Genevay et al., 2018; Franklin & Lorenz, 1989; Cuturi, 2013; Jason Altschuler, 2017). We provide a detailed algorithm description of Sinkhorn iterations in Algorithm 2. With the efficient and differential Sinkhorn iterations, we can easily evaluate the Sinkhorn divergence and thus let our algorithm enjoy its theoretical advantages. In practice, we need to choose L and ε, and we conduct a rigorous sensitivity analysis in Section 5.\n\n5 EXPERIMENTS\n\nWe demonstrate the effectiveness of SinkhornDRL as described in Algorithm 1 on the full 55 Atari 2600 games. Specifically, we leverage the same architecture as QR-DQN (Dabney et al., 2018b), and replace the quantiles output with N particles, i.e., samples. In contrast to MMDDRL, SinkhornDRL only changes the distribution divergence from MMD to Sinkhorn divergence, and therefore the potential superiority in the performance can be attributed to the advantages of Sinkhorn divergence.\n\nBaselines. Due to the interpolation feature of Sinkhorn divergence between Wassertein distance and MMDDRL, we choose three typical distributional RL algorithms as classic baselines, including QR-DQN (Dabney et al., 2018b) that approximates the Wasserstein distance, C51 (Bellemare et al., 2017a) and MMDDRL (Nguyen et al., 2020), as well as DQN (Mnih et al., 2015). MMDDRL algorithm is implemented with the same architecture as QRDQN, and leverages Gaussian kernels kh(x, y) = exp(−(x−y)2/h) with the kernel mixture trick covering a range of bandwidths h, which is same as the basic setting in the original MMDDQN paper (Nguyen et al., 2020). We deploy all algorithms on 55 Atari 2600 games, and reported results are averaged over 3 seeds with the shade indicating the standard deviation. We runs 10M time steps (40M frames) for the computation cost reason, but we report learning curves across all games to make results convincing enough.\n\nHyperparameter settings. For a fair comparison with QR-DQN, C51 and MMDDRL, we used the number of generated samples N = 200, Adam optimizer with the same hyperparamters: lr = 0.00005, (cid:15)Adam = 0.01/32. We used a target network to compute the distributional Bellman target, which fits well in the Neural Fitted Z-Iteration framework. In addition, we choose number of Sinkhorn iterations L = 10 and smoothing hyperparameter ε = 10.0 in Section 5.1 as they are not sensitive within a proper interval as demonstrated in Section 5.2. We choose the unrectified kernel as the cost function, i.e.,−c = kα, and select α = 2 in kα in our SinkhornDRL algorithm.\n\n5.1 PERFORMANCE OF SINKHORNDRL\n\nFigure 1 illustrates that SinkhornDRL can achieve the competitive performance across 55 Atari games compared with various baseline algorithms with different metrics dp and representation manners on Zθ. On a large number of games, e.g., Tennis, Seaquest and Atlantis, SinkhornDRL can\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Learning curves of SinkhornDRL algorithm compared with DQN, C51, QR-DQN and MMD, on nine typical Atari games over 3 seeds.\n\nsignificantly outperform other baselines, especially on Tennis where other algorithms even fail to converge. The improvement of SinkhornDRL over MMDDRL empirically verifies the regularization advantage of the Sinkhorn as analyzed in Corollary 1. On some games, e.g., Breakout, Pong and SpaceInvaders, SinkhornDRL is on par with MMDDRL and other baselines, while on the last row in Figure 1, SinkhornDRL is slightly inferior to the state-of-the-art algorithm. We provide learning curves of all typical distributional RL algorithms on all 55 Atari games in Appendix F, where SinkhornDRL still achieves the competitive performance in general.\n\n(a) Sinkhorn vs QRDQN\n\n(b) Sinkhorn vs MMDDRL\n\nFigure 2: Ratio improvement of return for Sinkhorn distributional RL algorithm over QRDQN (left) and MMDDRL (right) over 3 seeds. For example, the ratio improvement is calculated by (Sinkhorn - QRDQN) / QRDQN in the left.\n\n8\n\n7LPH6WHSVH$YHUDJH5HWXUQ7HQQLV'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ6HDTXHVW'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQH$WODQWLV'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ%UHDNRXW'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ3RQJ'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ6SDFH,QYDGHUV'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ1DPH7KLV*DPH'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ3KRHQL['41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ:L]DUG2I:RU'41&45'4100'6LQNKRUQ9HQWXUH6HDTXHVW'HPRQ$WWDFN7HQQLV3KRHQL[=D[[RQ$WODQWLV*RSKHUHQGXUR%HU]HUN6SDFH,QYDGHUV5LYHUUDLG6RODULV7LPH3LORW*UDYLWDU6WDU*XQQHU&KRSSHU&RPPDQG$OLHQ.XQJ)X0DVWHUSRQJEUHDNRXW8S1'RZQ5RDG5XQQHU6NLLQJ)UHHZD\\%R[LQJ$VWHUL[.UXOO5RERWDQN0V3DFPDQ%HDP5LGHU$PLGDU<DUV5HYHQJH%DWWOH=RQH%DQN+HLVW1DPH7KLV*DPH)LVKLQJ'HUE\\&HQWLSHGH7XWDQNKDP:L]DUG2I:RU+HUR3ULYDWH(\\H.DQJDURR4EHUW,FH+RFNH\\$VVDXOW&UD]\\&OLPEHU)URVWELWH9LGHR3LQEDOO$VWHURLGV-DPHVERQG%RZOLQJ0RQWH]XPD5HYHQJH'RXEOH'XQN5DWLR,PSURYPHQW6LQNKRUQ45'419HQWXUH$WODQWLV3ULYDWH(\\H7HQQLV=D[[RQ*RSKHU6HDTXHVW6SDFH,QYDGHUV*UDYLWDU5RERWDQN5RDG5XQQHU<DUV5HYHQJH.XQJ)X0DVWHU)LVKLQJ'HUE\\%DWWOH=RQH7LPH3LORW5LYHUUDLGHQGXUR.UXOO0V3DFPDQ-DPHVERQGSRQJ)UHHZD\\&KRSSHU&RPPDQG%HU]HUNEUHDNRXW%R[LQJ6WDU*XQQHU,FH+RFNH\\$PLGDU$OLHQ+HUR%DQN+HLVW$VVDXOW:L]DUG2I:RU1DPH7KLV*DPH6NLLQJ.DQJDURR&HQWLSHGH3KRHQL[%HDP5LGHU)URVWELWH9LGHR3LQEDOO$VWHURLGV$VWHUL[7XWDQNKDP6RODULV8S1'RZQ4EHUW'HPRQ$WWDFN&UD]\\&OLPEHU%RZOLQJ0RQWH]XPD5HYHQJH'RXEOH'XQN5DWLR,PSURYPHQW6LQNKRUQ00'Under review as a conference paper at ICLR 2023\n\nWe conduct a ratio improvement comparison across 55 Atari games between SinkhornDRL with QRDQN and MMDDRL, respectively. Figure 2 showcases that by comparing with QRDQN (left), SinkhornDRL achieves better performance across almost half of considered games and the superiority of SinkhornDRL is significant across a large amount of games, including Venture, Seaquest, Tennis and Phoenix. This empirical outperformance verifies the effectiveness of smoothing Wassertein distance in distributional RL. In contrast with MMDDRL, the advantage of SinkhornDRL is reduced with the performance improvement on a smaller proportion of games, but a remarkable performance improvement for SinkhornDRL on a large amount of games can be easily observed. We also report mean and median of best human-normalized scores in Table 2 of Appendix E, where SinkhornDRL achieves almost state-of-the-art performance as MMDDRL on average.\n\nTherefore, we conclude that SinkhornDRL is competitive with the state-of-the-art distributional RL algorithms, e.g., MMDDRL, and can be extremely superior over existing algorithms on a large proportion of games. This empirical success can be owing to theoretical advantage of Sinkhorn divergence that simultaneously makes full use of the data geometry from Wasserstein distance and the unbiased gradient estimate property from MMD, which coincides with results in Theorem 1.\n\n(a) Hyper-parameter ε\n\n(b) Number of Samples\n\n(c) Sinkhorn Iterations L\n\nFigure 3: Sensitivity analysis of SinkhornDRL on Breakout regarding ε, number of samples, and number of iteration L. Learning curves are reported over 3 seeds.\n\n5.2 SENSITIVITY ANALYSIS AND COMPUTATIONAL COST\n\nThe limit behavior connection in Theorem1 (1) and (2) between SinkhornDRL with QR-DQN and MMDDRL may not be rigorously verified in numerical experiments as an overly large or small ε will lead to numerical instability of Sinkhorn iterations in Algorithm 2, worsening its performance, as shown in Figure 3 (a). In practice, we choose a proper ε = 10 across all games. SinkhornDRL also requires a proper number of iterations L and samples N . For example, a small N , e.g., N = 2 in Seaquest in Figure 3 (b) leads to the divergence of algorithms, while an overly large N can degrade the performance and meanwhile increases the computational burden (Appendix G). We conjecture that using larger networks to represent more samples is more likely to suffer from the overfitting issue, yielding the instability in the RL training (Bjorck et al., 2021). Therefore, we choose N = 200 to attain an appealing performance with the computational effectiveness. For the computation cost (Appendix G), SinkhornDRL increases around 50% computation cost compared with QR-DQN and C51, but only slightly increases the overhead (by around 20%) in contrast to MMDDRL. Please refer to Appendix G for more detailed results and discussion.\n\n6 DISCUSSIONS AND CONCLUSION\n\nTo extend our algorithm for better performance, implicit generative models, including parameterizing the cost function in Sinkhorn loss, can be further incorporated. We leave it as the future work. Moreover, other divergences, e.g., those that can also smooth Wassertein distance, can also be applied into the design of distributional RL algorithms in the future.\n\nIn this paper, a novel family of distributional RL algorithms based on Sinkhorn divergence is proposed that accomplishes a competitive performance compared with the-state-of-the-art distributional RL algorithms on 55 Atari games. Theoretical analysis about the convergence and moment matching behavior is provided along with a rigorous empirical verification. Sinkhorn distributional RL will lead to an important contribution among the research community.\n\n9\n\n7LPH6WHSVH$YHUDJH5HWXUQ6HDTXHVW    45'4100'7LPH6WHSVH$YHUDJH5HWXUQ6HDTXHVW6DPSOHV 6DPSOHV 6DPSOHV 6DPSOHV 6DPSOHV 7LPH6WHSVH$YHUDJH5HWXUQ%UHDNRXW/ / / / / Under review as a conference paper at ICLR 2023\n\nEthics Statement. Our study is about the design of distributional RL algorithms, which is not involved with any ethics issue.\n\nReproducibility Statement. Our results is based on the public implementation released in (Zhang, 2018) with necessary implementation details given in Appendix F. We also provide the detailed proof from Appendix C to Appendix D.\n\nREFERENCES\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks.\n\nIn International conference on machine learning, pp. 214–223. PMLR, 2017.\n\nMarc G Bellemare, Will Dabney, and R ́emi Munos. A distributional perspective on reinforcement\n\nlearning. International Conference on Machine Learning (ICML), 2017a.\n\nMarc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan Hoyer, and R ́emi Munos. The cramer distance as a solution to biased wasserstein gradients. arXiv preprint arXiv:1705.10743, 2017b.\n\nNils Bjorck, Carla P. Gomes, and Kilian Q. Weinberger. Towards deeper deep reinforcement learning\n\nwith spectral normalization. Advances in neural information processing systems, 34, 2021.\n\nPablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Bellemare. Dopamine: A research framework for deep reinforcement learning. CoRR abs/1812.06110, 2018.\n\nYutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. UAI, 109–116.\n\nAUAI Press, 2012.\n\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural\n\ninformation processing systems, 26, 2013.\n\nWill Dabney, Georg Ostrovski, David Silver, and R ́emi Munos.\n\nImplicit quantile networks for distributional reinforcement learning. International Conference on Machine Learning (ICML), 2018a.\n\nWill Dabney, Mark Rowland, Marc G Bellemare, and R ́emi Munos. Distributional reinforcement learning with quantile regression. Association for the Advancement of Artificial Intelligence (AAAI), 2018b.\n\nOdin Elie and Charpentier Arthur. Dynamic Programming in Distributional Reinforcement Learn-\n\ning. PhD thesis, Universit ́e du Qu ́ebec `a Montr ́eal, 2020.\n\nJianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-\n\nlearning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020.\n\nJean Feydy, Thibault S ́ejourn ́e, Franc ̧ois-Xavier Vialard, Shun-ichi Amari, Alain Trouv ́e, and Gabriel Peyr ́e. Interpolating between optimal transport and mmd using sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2681–2690. PMLR, 2019.\n\nJoel Franklin and Jens Lorenz. On the scaling of multidimensional matrices. Linear Algebra and its\n\napplications, 114:717–735, 1989.\n\nAude Genevay, Gabriel Peyr ́e, and Marco Cuturi. Learning generative models with sinkhorn divergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608–1617. PMLR, 2018.\n\nAude Genevay, L ́enaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyr ́e. Sample complexity In The 22nd International Conference on Artificial Intelligence and\n\nof sinkhorn divergences. Statistics, pp. 1574–1583. PMLR, 2019.\n\nPhilippe Rigollet Jason Altschuler, Jonathan Weed. Near-linear time approximation algorithms for\n\noptimal transport via sinkhorn iteration, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nChristian L ́eonard. A survey of the schr\\” odinger problem and some of its connections with optimal\n\ntransport. arXiv preprint arXiv:1308.0215, 2013.\n\nYudong Luo, Guiliang Liu, Haonan Duan, Oliver Schulte, and Pascal Poupart. Distributional reinforcement learning with monotonic splines. In International Conference on Learning Representations, 2021.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\nThanh Tang Nguyen, Sunil Gupta, and Svetha Venkatesh. Distributional reinforcement learning with maximum mean discrepancy. Association for the Advancement of Artificial Intelligence (AAAI), 2020.\n\nNguyen-Tang. Distributional reinforcement learning via moment matching. https://github.\n\ncom/thanhnguyentang/mmdrl, 2021.\n\nAaditya Ramdas, Nicol ́as Garc ́ıa Trillos, and Marco Cuturi. On wasserstein two-sample testing and\n\nrelated families of nonparametric tests. Entropy, 19(2):47, 2017.\n\nMark Rowland, Marc Bellemare, Will Dabney, R ́emi Munos, and Yee Whye Teh. An analysis In International Conference on Artificial\n\nof categorical distributional reinforcement learning. Intelligence and Statistics, pp. 29–37. PMLR, 2018.\n\nLudger R ̈uschendorf and Wolfgang Thomsen. Closedness of sum spaces andthe generalized\n\nschr ̈odinger problem. Theory of Probability & Its Applications, 42(3):483–494, 1998.\n\nRichard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The\n\nAmerican Mathematical Monthly, 74(4):402–405, 1967.\n\nG ́abor J Sz ́ekely. E-statistics: The energy of statistical samples. Bowling Green State University,\n\nDepartment of Mathematics and Statistics Technical Report, 3(05):1–18, 2003.\n\nEric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via projected sinkhorn iterations. In International Conference on Machine Learning, pp. 6808–6817. PMLR, 2019.\n\nDerek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully parameterized quantile function for distributional reinforcement learning. Advances in neural information processing systems, 32:6193–6202, 2019.\n\nShangtong Zhang. Modularized implementation of deep rl algorithms in pytorch. https://\n\ngithub.com/ShangtongZhang/DeepRL, 2018.\n\nFan Zhou, Jianing Wang, and Xingdong Feng. Non-crossing quantile regression for distributional\n\nreinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.\n\nFlorian Ziel.\n\nThe energy distance for ensemble and scenario reduction.\n\narXiv preprint\n\narXiv:2005.14670, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA RELATED WORK\n\nBased on the choice of distribution divergence and the distribution representation manner of Zθ, distributional RL algorithms can be mainly categorized into three classes, including categorical, Wasserstein Distance and MMD distributional RL. Finally, we discuss their relationships with our proposed SinkhornDRL.\n\ni=1 piδzi, where {zi}N\n\nCategorical Distributional RL. As the first successful distributional RL family, categorical distributional RL (Bellemare et al., 2017a) represents the value distribution η by the categorical distribution ˆη = (cid:80)N i=1 (z1 < ... < zN ) is the fixed supports within the pre-specified interval [l, u] and pi is the approximated categorical probability in each bin, respectively. Within this algorithm family, C51 (Bellemare et al., 2017a) leverages a neural network to approximate the categorical probabilities pi and apply a projected KL divergence between the target and current categorical value distributions. C51 has also been shown the theoretical contraction under the Cram ́er distance (Bellemare et al., 2017b; Rowland et al., 2018), and empirically performs favorably in the suite of Atari games.\n\nWasserstein Distance Distributional RL. As directly solving wasserstein distance in Eq. 16 is tricky, QR-DQN (Dabney et al., 2018b) firstly proposed to use quantile regression to approximate Wasserstein distance Wp. QR-DQN leverages quantiles to represent the distribution η of Zθ, i.e., ˆη = (cid:80)N i=1 δzi , where {zi}N i=1 is the learnable support atoms as the quantile values of a fixed quantile { 2i−1 2N }N i=1. Implicit Quantile Networks (IQN) utilizes an implicit model to output quantile values {zi}N i=1 more expressively, instead of the given ones in QRDQN. IQN also incorporates the risk measure in the framework of distributional RL. A follow-up work Fully parameterized Quantile Function (FQF) (Yang et al., 2019) improves IQN by proposing a more expressive quantile network, achieving better performance on Atari games. Non-crossing issue in quantile-regression has been raised and addressed properly in (Zhou et al., 2020) that further improves QR-DQN. The monotonic rational-quadratic splines are also used to learn smooth continuous quantile functions (Luo et al., 2021).\n\nMMD Distributional RL. MMD distributional RL (MMDDRL) (Nguyen et al., 2020) learns samples to represent the value distribution of Zθ based on maximum mean discrepancy (MMD) in Eq. 20, achieving the state-of-the-art performance on Atari games.\n\nDiscussion about SinkhornDRL. As a complementary Wasserstein distance-based distributional RL, our SinkhornDRL allows to solve Wasserstein Distance by incorporating the entropic regularization and circumstance the non-crossing issue in quantile regression intrinsically. Moreover, the cost function in SinkhornDRL can be further parameterized similar to IQN and FQF, which can intuitively achieve better performance. We leave the investigation towards this direction as future works. Meanwhile, SinkhornDRL is also closely linked with MMDDRL as Sinkhorn divergence interpolates between Wasserstein distance and MMD, and also learns the unrestricted statistics, i.e., samples, akin to MMDDRL.\n\nB DEFINITION OF DISTANCES AND CONTRACTION\n\nDefinition of distances. Given two random variables X and Y , p-Wasserstein metric Wp between the distributions of X and Y is defined as\n\nWp(X, Y ) =\n\n(cid:18)(cid:90) 1\n\n0\n\n(cid:12) (cid:12)F −1\n\nX (ω) − F −1\n\nY (ω)(cid:12)\n\n(cid:12)\n\n(cid:19)1/p\n\np\n\ndω\n\n= (cid:107)F −1\n\nX − F −1\n\nY (cid:107)p,\n\n(16)\n\nwhich F −1 is the inverse cumulative distribution function of a random variable with the cumulative distribution function as F . Further, (cid:96)p distance (Elie & Arthur, 2020) is defined as\n\n(cid:96)p(X, Y ) :=\n\n(cid:18)(cid:90) ∞\n\n−∞\n\n|FX (ω) − FY (ω)|p dω\n\n(cid:19)1/p\n\n= (cid:107)FX − FY (cid:107)p\n\n(17)\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nThe (cid:96)p distance and Wassertein metric are identical at p = 1, but are otherwise distinct. Note that when p = 2, (cid:96)p distance is also called Cram ́er distance (Bellemare et al., 2017b) dC(X, Y ). Also, the Cram ́er distance has a different representation given by\n\n1 2\nwhere X (cid:48) and Y (cid:48) are the i.i.d. copies of X and Y . Energy distance (Sz ́ekely, 2003; Ziel, 2020) is a natural extension of Cram ́er distance to the multivariate case, which is defined as\n\ndC(X, Y ) = E|X − Y | −\n\nE |X − X (cid:48)| −\n\nE |Y − Y (cid:48)| ,\n\n(18)\n\n1 2\n\ndE(X, Y) = E(cid:107)X − Y(cid:107) −\n\n1 2\nwhere X and Y are multivariate. Moreover, the energy distance is a special case of the maximum mean discrepancy (MMD), which is formulated as\n\nE(cid:107)X − X(cid:48)(cid:107) −\n\nE(cid:107)Y − Y(cid:48)(cid:107),\n\n(19)\n\n1 2\n\nMMD(X, Y; k) = (E [k (X, X(cid:48))] + E [k (Y, Y(cid:48))] − 2E[k(X, Y)])1/2\n\n(20)\n\nwhere k(·, ·) is a continuous kernel on X . In particular, if k is a trivial kernel, MMD degenerates to energy distance. Additionally, we further define the supreme MMD, which is a functional P(X )S×A × P(X )S×A → R defined as\n\nMMD∞(μ, ν) =\n\nsup (x,a)∈S×A\n\nMMD∞(μ(x, a), ν(x, a))\n\n(21)\n\nWe further present the convergence rate under different distribution divergences.\n\n• T π is γ-contractive under the supreme form of Wassertein distance Wp. • T π is γ1/p-contractive under the supreme form of (cid:96)p distance. • T π is γα/2-contractive under MMD∞ with the kernel kα(x, y) = −(cid:107)x − y(cid:107)α, ∀α > 0.\n\nProof of Contraction.\n\n• Contraction under supreme form of Wasserstein diatance is provided in Lemma 3 (Belle-\n\nmare et al., 2017a).\n\n• Contraction under supreme form of (cid:96)p distance can refer to Theorem 3.4 (Elie & Arthur,\n\n2020).\n\n• Contraction under MMD∞ is provided in Lemma 6 (Nguyen et al., 2020).\n\nC PROOF OF THEOREM 1\n\nProof. 1. ε → 0 and c = −kα It is obvious to observe that Sinkhorn loss degenerates to the wasserstein distance. We also have the conclusion that the distributional Bellman operator Tπ is γ-contractive under the supreme form of Wasserstein diatance, the proof of which is provided in Lemma 3 (Bellemare et al., 2017a). Since the above conclusion is made directly based on the limiting case when ε = 0, for an unspecified ε, we need a more rigorous proof. We show that their distance difference is at most an infinitesimal δ.\n\nFirstly, as Wc,ε → Wα and the regularization term is non-negative, using the language of (ε, δ) definition, we have: for ∀δ, there exists a small positive constant a, such that Wc,ε − Wα < δ when (cid:15) ≤ a. Based on that, we have the contraction conclusion:\n\nW\n\n∞\n\n−κα,ε(TπZ1, TπZ2) = W\n\n∞\n\n−κα,ε(TπZ1, TπZ2) − W ∞ α (TπZ1, TπZ2),\n\n≤ δ + W ∞\n\nα (TπZ1, TπZ2) + W ∞\n\nα (TπZ1, TπZ2)\n\nwhere the second term W ∞ difference from the limting ε = 0 is an infinitesimal δ, which will vanish as ε → 0 or a → 0.\n\n(22) α (TπZ1, TπZ2) is contractive, and thus for the unspecified ε, the only\n\n2. ε → ∞. Our complete proof is inspired by (Ramdas et al., 2017; Genevay et al., 2018). Recap the definition of squared MMD is\n\nE [k (X, X(cid:48))] + E [k (Y, Y(cid:48))] − 2E[k(X, Y)]\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nWhen the kernel function k degenerates to a unrectified kα(x, y) := −(cid:107)x − y(cid:107)α for α ∈ (0, 2), the squared MMD would degenerate to\n\nE(cid:107)X − X(cid:48)(cid:107)α + E(cid:107)Y − Y(cid:48)(cid:107)α − 2E(cid:107)X − Y(cid:107)α\n\nOn the other hand, we have the Sinkhorn loss as\n\nW c,∞(μ, ν) = 2Wc,∞(μ, ν) − Wc,∞(ν, ν) − Wc,∞(μ, ν)\n\nDenoting Πε be the unique minimizer for W c,ε, it holds that Πε → μ ⊗ ν as ε → ∞. That being said, Wc,∞(μ, ν) → (cid:82) c(x, y)dμ(x)dν(y) + 0 = (cid:82) c(x, y)dμ(x)dν(y). If c = −kα = (cid:107)x − y(cid:107)α, we eventually have W−kα,∞(μ, ν) → (cid:82) (cid:107)x − y(cid:107)αdμ(x)dν(y) = E(cid:107)X − Y(cid:107)α. Finally, we can have\n\nW −kα,∞ → 2E(cid:107)X − Y(cid:107)α − E(cid:107)X − X(cid:48)(cid:107)α − E(cid:107)Y − Y(cid:48)(cid:107)α\n\nwhich is exactly the form of squared MMD. Now the key is prove that Πε → μ ⊗ ν as ε → ∞. Firstly, it is apparent that Wc,ε(μ, ν) ≤ (cid:82) c(x, y)dμ(x)dν(y) as μ⊗ν ∈ Π(μ, ν). Let {εk} be a positive sequence that diverges to ∞, and Πk be the corresponding sequence of unique minimizers for Wc,ε. According to the optimality condition, it must be the case that (cid:82) c(x, y)dΠk + εkKL(Πk, μ ⊗ ν) ≤ (cid:82) c(x, y)dμ ⊗ ν + 0 (when Π(μ, ν) = μ ⊗ ν). Thus,\n\nKL (Πk, μ ⊗ ν) (cid:54) 1 εk\n\n(cid:18)(cid:90)\n\nc dμ ⊗ ν −\n\n(cid:90)\n\n(cid:19)\n\nc dΠk\n\n→ 0.\n\nBesides, by the compactness of Π(μ, ν), we can extract a converging subsequence Πnk → Π∞. Since KL is weakly lower-semicontinuous, it holds that\n\nKL (Π∞, μ ⊗ ν) (cid:54) lim inf\n\nk→∞\n\nKL (Πnk , μ ⊗ ν) = 0\n\nHence Π∞ = μ⊗ν. That being said that the optimal coupling is simply the product of the marginals, indicating that Πε → μ ⊗ ν as ε → ∞. As a special case, when α = 1, W −k1,∞(u, v) is equivalent to the energy distance\n\ndE(X, Y) := 2E(cid:107)X − Y(cid:107) − E(cid:107)X − X(cid:48)(cid:107) − E(cid:107)Y − Y(cid:48)(cid:107).\n\n(23)\n\nIn summary, if the cost function is the rectified kernel kα, it is the case that W −kα,ε converges to the squared MMD as ε → ∞. According to (Nguyen et al., 2020), Tπ is γα/2-contractive in the supreme form of MMD with the rectified kernel kα.\n\nFor the unspecified ε, we can get the similar result to the case of ε → 0. For ∀δ, there exists a large positive constant M , such that MMD2 − Wc,ε < δ when (cid:15) ≥ M . Based on that, we have the contraction conclusion:\n\nkα\n\nW\n\n∞\n\n−κα,ε(TπZ1, TπZ2) = W\n\n∞\n\n−κα,ε(TπZ1, TπZ2) − MMD2 ∞(TπZ1, TπZ2) − δ,\n\n≤ MMD2\n\n∞(TπZ1, TπZ2) + MMD2\n\n∞(TπZ1, TπZ2)\n\n(24) where the first term MMD2 2 -contractive, and thus for the unspecified ε, the only difference from the limiting ε = ∞ is an infinitesimal δ, which will vanish as ε → +∞ or (M → +∞).\n\n∞(TπZ1, TπZ2) is γ α\n\n3. For ε ∈ (0, +∞), the contraction property needs a long proof. The proof pipeline is firstly we prove the three properties of Sinkhorn divergence, and then we show the contraction of distributional Bellman operator under Sinkhorn divergence based on its properties. Most importantly, we analyzed the contraction under a new non-constant factor.\n\n3.1 Properties of Sinkhorn Divergence. We recap three crucial properties of a divergence metric. The first is scale sensitive (S) (of order β, β > 0), i.e., dp(cX, cY ) ≤ |c|βdp(X, Y ). The second property is shift invariant (I), i.e., dp(A + X, A + Y ) ≤ dp(X, Y ). The last one is unbiased gradient (U). A key observation for the analysis is that the Sinkhorn divergence would degenerate to a two-dimensional KL divergence, and therefore embraces a similar convergence behavior to KL divergence. Concretely, according to the equivalent form of Wc,ε(μ, ν) in Eq. 9, it can be expressed\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nas the KL divergence between an optimal joint distribution and a Gibbs distribution associated with the cost function:\n\nWc,ε(μ, ν) := KL (Π∗(μ, ν)|K(μ, ν)) ,\n\n(25)\n\nwhere Π∗ is the optimal joint distribution. Thus, the total Sinkhorn divergence is expressed as\n\nW c,ε(μ, ν) := 2KL (Π∗(μ, ν)|K(μ, ν)) − KL (Π∗(μ, μ)|K(μ, μ)) − KL (Π∗(ν, ν)|K(ν, ν)) .\n\n(26)\n\nDue to the form of W c,ε(μ, ν), the convergence behavior is determined by Wc,ε(μ, ν), which is similar to the behavior of KL divergence. Thus, we will focus on the convergence analysis of Wc,ε(μ, ν). According to the fact that KL divergence has unbiased gradient estimates (U) and shift invariant (I), and Sinkkhorn divergence can be viewed as a two-dimensional KL divergence, both properties of U and I can be extended to Sinkhorn divergence. However, we find the non scale sensitive (S) property can not directly apply to Sinkhorn divergence due to the minimum nature of Wc,ε(μ, ν) as the optimal joint distribution Π∗(μ, ν) could be different from Π0(aμ, aν) where a is the scale factor. We need a new rigorous proof of scale sensitive property as follows.\n\n3.2 Scale Sensitive Property of Sinkhorn Divergence.\n\nWe show Sinkhorn divergence satisfies a variant of scale sensitive property when c = −kα that corresponds to a non-constant scale factor ∆(a, α) that is not only a function of the vanilla scale factor a and α in kα, but also the two specified probability measures (U, V ). By definition, the pdf of K(U, V ) ∝ e μ(x)ν(y). After a scaling transformation, the pdf of aU and aV with respect a ν( y to x and y would be 1 a μ( x a ). We denote Π∗ and Π0 as the optimal joint distribution of Wc,ε(μ, ν) and Wc,ε(aμ, aν).\n\na ). Thus K(aU, aV ) ∝ e\n\na ) and 1\n\na μ( x\n\na ν( y\n\n−c(x,y) ε\n\n−c(x,y) ε\n\na ) 1\n\n1\n\nWc,ε(aU, aV ) =\n\n≤\n\n(cid:90)\n\n(cid:90)\n\nc(x, y)dΠ0(x, y) + εKL(Π0|aμ ⊗ aν)\n\nc(x, y)dΠ∗(x, y) + εKL(Π∗|aμ ⊗ aν)\n\n(cid:90)\n\n(cid:90)\n\nc=−kα=\n\n= |a|α\n\n(x − y)α 1\n\na2 π∗(\n\nx a\n\n,\n\ny a\n\n)dxdy + ε\n\n(cid:90) 1\n\na2 π∗(\n\nx a\n\n(x − y)απ∗(x, y)dxdy + ε\n\n(cid:90)\n\nπ∗(x, y) log\n\n,\n\n) log\n\ny a\nπ∗(x, y) μ(x)ν(y) (cid:90)\n\n1\n\na2 π∗( x a2 μ( x\n\na , y a ) a )ν( y a )\n\n1\n\ndxdy\n\ndxdy\n\n(x − y)απ∗(x, y)dxdy\n\n(cid:90)\n\n=\n\n(x − y)απ∗(x, y)dxdy + εKL(Π∗|μ ⊗ ν) − (1 − |a|α)\n\n= Wc,ε(U, V ) − (1 − |a|α)\n\n(cid:90)\n\n(x − y)αdΠ∗(x, y)\n\n= ∆U,V (a, α)Wc,ε(U, V )\n\n(27) where ∆U,V (a, α) = 1 − (1−|a|α) (cid:82) (x−y)αdΠ∗(x,y) ∈ (0, 1) for ε ∈ (0, +∞) and a < 1 due to the fact that 0 < (1 − |a|α) (cid:82) (x − y)αdΠ∗(x, y) < (cid:82) (x − y)αdΠ∗(x, y) < Wc,ε(U, V ). ∆U,V (a, α) is function less than 1 that depends on the two margin distributions and the scale factor a. The result implies that we have a new variant of scale sensitive property of Sinkhorn divergence with a non-constant factor ∆U,V (a, α) < 1 when we choose c = −kα and |a| < 1.\n\nWc,ε(U,V )\n\n3.3 A New Contraction Mapping Theorem.\n\nWe derive a new contraction mapping theorem based on the distribution distance d in order to prove the convergence in 3.4.\n\nTheorem 2. (Distribution Contraction Mapping Theorem with a Non-constant Factor) Consider a distribution distance d and a function g : P → P. The mapping d is a contraction: There exists a function q(X, Y ) < 1 such that for ∀ distributions X and Y :\n\nd(g(X), g(Y )) ≤ q(X, Y )d(X, Y )\n\n(28)\n\nThen there exists a unique distribution X ∗ with g(X ∗) = X ∗.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nProof. We consider the convergence of the distribution sequence X k. We have the updating rule as\n\nd(X k+1, X k) = d(g(X k), g(X k−1)) ≤ qk,k−1d(X k, X k−1),\n\nwhere we use qk,k−1 = q(X k, X k−1) for short. Hence, we have\n\nd(X k+1, X k) ≤ Πk\n\ni=1qi,i−1d(X 1, X 0).\n\nLet d0 = d(X 1, X 0). From the triangle inequality, we have\n\nd(X k+l, X k) ≤ d(X k+1, X k) + ... + d(X k+l, X k+l−1),\n\n≤ Πk\n\n≤ Πk\n\n≤ Πk\n\ni=1qi,i−1d0 + .. + Πk+l−1 i=1qi,i−1(1 + qk+1,k + ... + Πk+l−1 i=1qi,i−1(1 + qk+1,k + ... + Πk+l−1\n\nqi,i−1d0\n\ni=1\n\ni=k+1qi,i−1)d0 i=k+1qi,i−1 + ...)d0\n\n(29)\n\n(30)\n\n(31)\n\nFor the infinite series 1 + qk+1,k + ... + Πk+l−1 according to the ratio convergence judgment method of infinite series, limk→∞ infinite series is convergent. Due to the fact Πk as k → ∞. Therefore, it must converge to a limit distribution X ∗ that satisfies g(X ∗) = X ∗.\n\ni=k+1qi,i−1 + ..., which we denote as ui for i-the term, < 1. Thus, the i=1qi,i−1 → 0 as k → ∞, we have d(X k+1, X k) → 0\n\nui+1 ui\n\n3.4 Contraction of Distributional Bellman Operator under Sinkhorn Divergence.\n\nAccording to the equation of W c,ε, it holds the same properties as Wc,ε, i.e., shift invariant and scale sensitive. Thus, we derive the convergence of distributional Bellman operator Tπ under the supreme form of W c,ε, i.e., W\n\n∞\n\nc,ε:\n\nW\n\n∞\n\nc,ε(TπZ1, TπZ2)\n\n= sup s,a\n\nW c,ε(TπZ1(s, a), TπZ2(s, a))\n\n= W c,ε(R(s, a) + γZ1(s(cid:48), a(cid:48)), R(s, a) + γZ2(s(cid:48), a(cid:48))) c=−kα\n\n≤ ∆Z1(s(cid:48),a(cid:48)),Z2(s(cid:48),a(cid:48))(γ, α)W c,ε(Z1(s(cid:48), a(cid:48)), Z2(s(cid:48), a(cid:48)))\n\n≤ sup s(cid:48),a(cid:48)\n\n∆Z1(s(cid:48),a(cid:48)),Z2(s(cid:48),a(cid:48))(γ, α) sup\n\ns(cid:48),a(cid:48)\n\nW c,ε(Z1(s(cid:48), a(cid:48)), Z2(s(cid:48), a(cid:48)))\n\n(32)\n\n≤ ∆Z1,Z2 (γ, α) sup s(cid:48),a(cid:48) ∞\n\n= ∆Z1,Z2 (γ, α)W\n\n−kα,ε(Z1, Z2)\n\nW −kα,ε(Z1(s(cid:48), a(cid:48)), Z2(s(cid:48), a(cid:48)))\n\nwhere the first inequality comes from the scale sensitive property proof of Sinkhorn divergence and we let ∆Z1,Z2 (γ, α) = sups(cid:48),a(cid:48) ∆Z1(s(cid:48),a(cid:48)),Z2(s(cid:48),a(cid:48))(γ). If ∆Z1,Z2 (γ, α) is only a constant function in terms of γ and α, we can directly arrive the conclusion that distributional Bellman operator is ∆Z1,Z2(γ, α)-contractive based on existing Banach fixed point theorem. However, the fact is that ∆Z1,Z2(γ, α) also depends on Z1 and Z2, and thus we need a new contraction mapping theorem to guarantee the convergence of fixed distribution iteration. According to Theorem 2 in 3.3 that we specifically figure out for the our contraction proof, we have W c,ε can guarantee the convergence via distributional Bellman iterations. In summary, we conclude that Tπ is a contractive operator when we use the −kα as the cost function and γ ≤ 1, while the contraction factor, which is short for ∆(γ, α) < 1, is not only a function of α and γ, but also depends on distribution sequence in the while iterations.\n\n∞\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nD PROOF OF PROPOSITION 1 AND COROLLARY 1\n\nProof. As we leverage Π∗ to denote the optimal Π by evaluating the Sinkhorn divergence via minΠ∈Π(μ,ν) W c,ε(μ, ν; k), the Sinkhorn divergence can be composed in the following form:\n\nW c,ε(μ, ν; k) = 2KL (Π∗(μ, ν)|K−k(μ, ν)) − KL (Π∗(μ, μ)|K−k(μ, μ)) − KL (Π∗(ν, ν)|K−k(ν, ν))\n\n= 2(EX,Y [log Π∗(μ, ν)]) +\n\n1 ε\n\nEX,X (cid:48) [c(X, Y )]) − (EX,X (cid:48) [log Π∗(μ, ν)]) +\n\n1 ε\n\nEX,Y [c(X, Y )])\n\n− (EY,Y (cid:48) [log Π∗(ν, ν)]) +\n\nEY,Y (cid:48) [c(Y, Y (cid:48))])\n\n1 ε\n\n= EX,X (cid:48),Y,Y (cid:48)\n\n= EX,X (cid:48),Y,Y (cid:48)\n\n(cid:20)\n\nlog\n\n(cid:20)\n\nlog\n\n(Π∗(X, Y ))2 Π∗(X, X (cid:48))Π∗(Y, Y (cid:48)) (Π∗(X, Y ))2 Π∗(X, X (cid:48))Π∗(Y, Y (cid:48))\n\n(cid:21)\n\n(cid:21)\n\n+\n\n+\n\n1 ε\n\n1 ε\n\n(EX,X (cid:48) [k(X, X (cid:48))] + EY,Y (cid:48) [k(Y, Y (cid:48))] − 2EX,X (cid:48) [k(X, Y )])\n\nMMD2\n\n−c(μ, ν)\n\n(33) where the cost function c in the Gibbs distribution K is minus Gaussian kernel, i.e., c(x, y) = −k(x, y) = e−(x−y)/(2σ2). Till now, we have shown the result in Corollary 1.\n\nNext, we use Taylor expansion to prove the moment matching of MMD. Firstly, we have the following equation:\n\nMMD2\n\n−c(μ, ν) = EX,X (cid:48) [k(X, X (cid:48))] + EY,Y (cid:48) [k(Y, Y (cid:48))] − 2EX,X (cid:48) [k(X, Y )] (cid:2)φ(Y )(cid:62)φ(Y (cid:48))(cid:3) − 2EX,X (cid:48)\n\n(cid:2)φ(X)(cid:62)φ(X (cid:48))(cid:3) + EY,Y (cid:48)\n\n= EX,X (cid:48) = E(cid:107)φ(X) − φ(Y )(cid:107)2\n\n(cid:2)φ(X)(cid:62)φ(Y )(cid:3)\n\n(34)\n\nWe expand the Gaussian kernel via Taylor expansion, i.e.,\n\nk(x, y) = e−(x−y)2/(2σ2) = e− x2\n\n2σ2 e− y2\n\n2σ2 e\n\nxy σ2\n\n= e− x2\n\n2σ2 e− y2\n\n2σ2\n\n∞ (cid:88)\n\nn=0\n\n1 √\n\nn!\n\n(\n\nx σ\n\n)n 1\n\n√\n\nn!\n\n(\n\ny σ\n\n)n\n\n=\n\n∞ (cid:88)\n\nn=0\n\ne− x2\n\n2σ2\n\n1 √\n\nn!\n\n(\n\nx σ\n\n)ne− y2\n\n2σ2\n\n1 √\n\nn!\n\n(\n\ny σ\n\n)n\n\n= φ(x)(cid:62)φ(y)\n\nTherefore, we have\n\nMMD2\n\n−c(μ, ν) =\n\n=\n\n∞ (cid:88)\n\nn=0 ∞\n(cid:88)\n\nn=0\n\n(cid:16)\n\n1 σ2nn!\n\nEx∼μ\n\n(cid:104)\n\ne−x2/(2σ2)xn(cid:105)\n\n− Ex∼ν\n\n(cid:104)\n\ne−y2/(2σ2)yn(cid:105)(cid:17)2\n\n1 σ2nn!\n\n(cid:16) ̃Mn(μ) − ̃Mn(ν)\n\n(cid:17)2\n\n(35)\n\n(36)\n\n ̃Mn(μ) = Ex∼μ\n\n(cid:104)\n\ne−x2/(2σ2)xn(cid:105)\n\n, and similarly for ̃Mn(ν). The conclusion is the same as the\n\nmoment matching in (Nguyen et al., 2020). Finally, due to the equivalence of W c,ε(μ, ν) after multiplying ε, we have\n\nW c,ε(μ, ν; k) := MMD2\n\n−c(μ, ν) + εE\n\n(cid:20)\n\n(Π∗(X, Y ))2 Π∗(X, X (cid:48))Π∗(Y, Y (cid:48))\n\n(cid:21)\n\n=\n\n∞ (cid:88)\n\nn=0\n\n1 σ2nn!\n\n(cid:16) ̃Mn(μ) − ̃Mn(ν)\n\n(cid:17)2\n\n+ εE\n\n(cid:20)\n\n(Π∗(X, Y ))2 Π∗(X, X (cid:48))Π∗(Y, Y (cid:48))\n\n(cid:21)\n\n,\n\n(37)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nThis result is also equivalent to Theorem 1, where Π∗ would degenerate to μ ⊗ ν as ε → +∞. In that case, the first regularization term would vanish, and thus the Sinkhorn divergence degrades to a MMD loss, i.e., MMD2\n\n−c(μ, ν).\n\nE HUMAN-NORMALIZED SCORES\n\nMean Median > Human >DQN\n\nDQN C51 QR-DQN-1 MMDDRL QRDQN(tf) MMDDRL(tf) SinkhornDRL\n\n438.7 %\n\n43.6 % 1043.4 % 103.7 % 1286.4 % 108.6 % 924.6 % 117.5 % 535.1 % 108.2 % 99.8 % 665.0 % 1435.8 % 113.0 %\n\n17 26 31 27 28 27 27\n\n0 42 47 43 40 39 42\n\nTable 2: Mean and median of best human-normalized scores across 55 Atari 2600 games. The results for all considered algorithms are averaged over 3 seeds. DQN, C51, QR-DQN-1, MMDDRL are based on our Pytorch implementation after 10M time steps adapted from (Zhang, 2018), while QRDQN(tf) and MMDDRL(tf) are training results also after 10M time steps from the tensorflow Dopamine framework of MMDDRL (Nguyen et al., 2020) released in (Nguyen-Tang, 2021).\n\nHuman normalized score equation is (algorithm - randomplay) / (human - randomplay). Our implementation of DQN, QRDQN-1, C51, MMDDRL, Sinkhorn is based on (Zhang, 2018) and all the experimental settings, including parameters are identical to the distributional RL baselines implemented by (Zhang, 2018). The results about mean and median human-normalized scores of all considered distributional RL algorithms are reported in Table 2. We also compare the performance of QRDQN(tf) and MMDDRL(tf) after the same 10M time steps based on tensorflow implementation on Dopamine framework (Castro et al., 2018). These results are averaged over training data released in (Nguyen-Tang, 2021). We argue that Human-normalized scores may be limited to evaluate the superiority of algorithms as mean can be highly affected by the performance on games with high-level returns. For example, in Figure 2, MMDDRL is superior to QR-DQN as Sinkhorn outperforms MMDDRL on a smaller portion of games compared with QRDQN, but the mean score for MMDDRL in Table 2 is lower than QRDQN. By contrast, SinkhornDRL is superior in terms of mean score and competitive in terms of median. We also provide all average results in Table 3 of Appendix H and all learning curves of our implemented algorithms in Appendix F.\n\nF MORE EXPERIMENTAL RESULTS\n\nWe provide learning curves of DQN, QRDQN, C51, MMD and SinkhornDRL algorithms on all 55 Atari games in Figures 4 5 6 7 8 9. It illustrates that SinkhornDRL dramatically surpasses the other distributional RL algorithms on a large amount of environments, e.g., Venture, Atlantis, Tennis and SpaceInvader, and presents competitive performance or is only slightly inferior as opposed to the state-of-the-art baselines on other games. Note that the average improvement of SinkhornDRL on Venture game is significant owing to one to two times convergence of SinkhornDRL algorithm over 3 seeds, while the other baselines do not converge over the considered seeds. Although this improvement may also suffer from the instability issue, its occasional success for our SinkhornDRL algorithm also presents huge potential on some complicated environments. We leave the further exploration on the advantage and potential of SinkhornDRL algorithm as the future work.\n\nG SENSITIVITY ANALYSIS AND COMPUTATIONAL COST\n\nG.1 MORE RESULTS IN SENSITIVITY ANALYSIS\n\nDecreasing ε. We argue that the limit behavior connection as stated in Theorem 1 may not be able to be verified rigorously via numeral experiments due to the numerical instability of Sinkhorn\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Performance of SinkhornDRL compared with DQN, C51, QRDQN and MMD on Breakout, Enduro, Pong, YarRevenge, Alien, BattleZone, Berzerk, Qbert and SpaceInvader.\n\n19\n\n7LPH6WHSVH$YHUDJH5HWXUQ%UHDNRXW'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ(QGXUR'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ3RQJ'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ<DUV5HYHQJH'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ%DWWOH=RQH'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ%HU]HUN'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ4EHUW'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ6SDFH,QYDGHUV'41&45'4100'6LQNKRUQUnder review as a conference paper at ICLR 2023\n\nFigure 5: Performance of SinkhornDRL compared with DQN, C51, QRDQN and MMD on UpNDown, Asterix, Asteriods, BeamRider, Centipede, FishingDerby, Frostbite and Riverraid.\n\n20\n\n7LPH6WHSVH$YHUDJH5HWXUQ8S1'RZQ'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ$VWHUL['41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ$VWHURLGV'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ%HDP5LGHU'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ&HQWLSHGH'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ&UD]\\&OLPEHU'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ)LVKLQJ'HUE\\'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ)URVWELWH'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ5LYHUUDLG'41&45'4100'6LQNKRUQUnder review as a conference paper at ICLR 2023\n\nFigure 6: Performance of SinkhornDRL compared with DQN, C51, QRDQN and MMD on TimePilot, StarGuner, Seaquest, NameThisGame, Phoenix, Tennix, Tutankham, Venture and VideoPinball.\n\n21\n\n7LPH6WHSVH$YHUDJH5HWXUQ7LPH3LORW'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ6WDU*XQQHU'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ6HDTXHVW'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ1DPH7KLV*DPH'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ3KRHQL['41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ7HQQLV'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ7XWDQNKDP'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ9HQWXUH'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ9LGHR3LQEDOO'41&45'4100'6LQNKRUQUnder review as a conference paper at ICLR 2023\n\nFigure 7: Performance of SinkhornDRL compared with DQN, C51, QRDQN and MMD on RoadRunner, Jamesbond, IceHockey, Hero, BankHeist, Atlantis, WizardOfWor, Amidar and Assault.\n\n22\n\n7LPH6WHSVH$YHUDJH5HWXUQ5RDG5XQQHU'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ-DPHVERQG'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ,FH+RFNH\\'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ+HUR'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ%DQN+HLVW'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQH$WODQWLV'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ:L]DUG2I:RU'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ$PLGDU'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ$VVDXOW'41&45'4100'6LQNKRUQUnder review as a conference paper at ICLR 2023\n\nFigure 8: Performance of SinkhornDRL compared with DQN, C51, QRDQN and MMD on Bowling, Boxing, DoubleDunk, Freeway, Gravitar, Kangaroo, Krull, KunFuMaster and MontezumaRevenge.\n\n23\n\n7LPH6WHSVH$YHUDJH5HWXUQ%RZOLQJ'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ%R[LQJ'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ'RXEOH'XQN'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ)UHHZD\\'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ.UXOO'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ.XQJ)X0DVWHU'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ0RQWH]XPD5HYHQJH'41&45'4100'6LQNKRUQUnder review as a conference paper at ICLR 2023\n\nFigure 9: Performance of SinkhornDRL compared with DQN, C51, QRDQN and MMD on MsPacman, Pitfall, PrivateEye, Robotank, Skiing, Solaris, Zaxxon, ChopperCommand, Gopher and DemonAttack.\n\n24\n\n7LPH6WHSVH$YHUDJH5HWXUQ3LWIDOO'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ3ULYDWH(\\H'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ5RERWDQN'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ6NLLQJ'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ6RODULV'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ=D[[RQ'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ*RSKHU'41&45'4100'6LQNKRUQ7LPH6WHSVH$YHUDJH5HWXUQ'HPRQ$WWDFN'41&45'4100'6LQNKRUQUnder review as a conference paper at ICLR 2023\n\n(a) Small ε in SinkrhornDRL vs QRDQN\n\n(b) Large ε in SinkrhornDRL vs MMDDRL\n\n(c) ε on Breakout\n\n(d) ε on Seaquest\n\nFigure 10: (a) Sensitivity analysis w.r.t. a small level of ε SinkhornDRL to compare with QR-DQN that approximates Wasserstein distance on Breakout. (b) Sensitivity analysis w.r.t. a large level of ε SinkhornDRL algorithm to compare with MMDDRL on Breakout. All learning curves are reported over 2 seeds. (c) and (d) are results for a general ε on Breakout and Seaquest, respectively.\n\nIteration in Algorithm 2. From Figure 10 (a), we can observe that if we gradually decline ε to 0, SinkhornDRL’s performance tends to degrade and approach to QR-DQN. Note that an overly small ε will lead to a trivial almost 0 Ki,j in Sinkhorn iteration in Algorithm 2, and will cause 1 0 numerical instability issue for al and bl in Line 5 of Algorithm 2. In addition, we also conducted experiments on Seaquest, the similar result is also observed in Figure 10 (d). As shown in Figure 10 (d), the performance of SinkhornDRL is robust when ε = 10, 100, 500, but a small (cid:15) = 1 tends to worsen the performance.\n\nIncreasing ε. Moreover, for breakout, if we increase ε, the performance of SinkhornDRL tends to degrade and be close to MMDDRL as suggested in Figure 10 (b). It is also noted that an overly large ε will let the Ki,j explode to ∞. This also leads to numerical instability issue in Sinkhorn iteration in Algorithm 2.\n\nSamples N . We find that SinkhornDRL requires a proper number of samples N to perform favorably, and the sensitivity w.r.t N depends on the environment. As suggested in Figure 11 (a), a smaller N , e.g., N = 2 on breakout has already achieved favorably performance and even accelerates the convergence in the early phase, while N = 2 on Seaquest will lead to the divergence issue. Meanwhile, an overly large N worsens the performance across two games. We conjecture that using larger network networks to generate more samples may suffer from the overfitting issue, yielding the training instability (Bjorck et al., 2021). In practice, we choose a proper number of sample, i.e., N = 200 across all games.\n\n25\n\n7LPH6WHSVH$YHUDJH5HWXUQ%UHDNRXW     45'417LPH6WHSVH$YHUDJH5HWXUQ%UHDNRXW   00'7LPH6WHSVH$YHUDJH5HWXUQ%UHDNRXW    7LPH6WHSVH$YHUDJH5HWXUQ6HDTXHVW    45'4100'Under review as a conference paper at ICLR 2023\n\n(a) Samples on Breakout\n\n(b) Samples on Seaquest\n\nFigure 11: Sensitivity analysis of Sinkhorn in terms of the number of samples N on Breakout (a) and Seaquest (b).\n\nG.2 COMPARISON WITH THE COMPUTATIONAL COST\n\nWe evaluate the computational time every 10,000 iterations across the whole training process of all considered distributional RL algorithms and make a comparison in Figure 12. It suggests that SinkhornDRL indeed increases around 50% computation cost compared with QR-DQN and C51, but only slightly increases the the cost in contrast to MMDDRL on both Breakout and Qbert games. We argue that this additional computational burden can be tolerant in view of the significant outperformance of SinkhornDRL in a large amount of environments.\n\nIn addition, we also find that the number of Sinkhorn iterations L is negligible to the computation cost, while an overly large samples N , e.g., 500, will lead to a large computational burden as illustrated in Figure 13. This can be intuitively explained as the computation complexity of the cost function ci,j is O(N 2) in SinkhornDRL, which is particularly heavy in computation if N is large enough.\n\nFigure 12: Average computational cost per 10,000 iterations of all considered distributional RL algorithm, where we select ε = 10, L = 10 and number of samples N = 200 in SinkhornDRL algorithm.\n\nH RAW SCORE TABLES ACROSS ALL ATARI GAMES AFTER 10\n\nTIMESTEPS (40M FRAMES)\n\nFor distributional Rl algorithms, results are averaged over 3 seeds after 10 timeSteps, i.e., 40M Frames. Results of DQN, C51, QRDQN, MMD and Sinkhorn are based on our implementation on Pytorch, which is adapted from the reliable open-source implementation (Zhang, 2018). QRDQN(tf) and MMD(tf) are from training results after 40M frames from the implementation of MMDDRL (Nguyen et al., 2020) in (Nguyen-Tang, 2021).\n\n26\n\n7LPH6WHSVH$YHUDJH5HWXUQ%UHDNRXW6DPSOHV 6DPSOHV 6DPSOHV 6DPSOHV 6DPSOHV 6DPSOHV 7LPH6WHSVH$YHUDJH5HWXUQ6HDTXHVW6DPSOHV 6DPSOHV 6DPSOHV 6DPSOHV 6DPSOHV '4145'41&00'6LQNKRUQ$YHUDJH&RPSXWDWLRQ7LPH%UHDNRXW'4145'41&00'6LQNKRUQ$YHUDJH&RPSXWDWLRQ7LPH4EHUWUnder review as a conference paper at ICLR 2023\n\nFigure 13: Average computational cost per 10,000 iterations of SinkhornDRL algorithm over different samples.\n\n27\n\n6LQNKRUQVDPSOHV 6LQNKRUQVDPSOHV 6LQNKRUQVDPSOHV 6LQNKRUQVDPSOHV $YHUDJH&RPSXWDWLRQ7LPHV%UHDNRXW6LQNKRUQVDPSOHV 6LQNKRUQVDPSOHV 6LQNKRUQVDPSOHV 6LQNKRUQVDPSOHV $YHUDJH&RPSXWDWLRQ7LPHV4EHUWUnder review as a conference paper at ICLR 2023\n\nGAMES Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert RiverRaid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon\n\nRANDOM HUMAN\n\n211.9 2.34 283.5 268.5 1008.6 22188 14 3000 414.3 165.6 23.48 -0.69 1.5 2064.77 794 8043 162.25 -18.14 0.01 -93.06 0.01 73.2 364 226.5 551 -10.3 27 54 1,566.59 451 0.0 242.6 2404.9 757.2 -265 -20.34 34.49 188.75 1575.4 7\n2.24 88.2 -16267.9 2346.6 136.15 631 -23.92 3682 15.56 604.7 0.0 15720.98 534 3271.42 8\n\n7,127.7 1,719.5 742.0 8,503.3 47,388.7 29,028.1 753.1 37,187.5 16,926.5 2,630.4 160.7 12.1 30.5 12,017.0 7,387.8 35,829.4 1,971.0 -16.4 860.5 -38.7 29.6 4,334.7 2,412.5 3,351.4 30,826.4 0.9 302.8 3,035.0 2,665.5 22,736.3 4,753.3 6,951.6 8,049.0 7,242.6 6,463.7 14.6 69,571.3 13,455.0 17,118.0 7,845.0 11.9 42,054.7 -4,336.9 12,326.7 1,668.7 10,250.0 -8.3 5,229.2 167.6 11,693.2 1,187.5 17,667.9 4,756.5 54,576.9 9,173.3\n\nDQN 1334.0 400.2 5651.8 5490.0 1246.0 18990.0 657.0 22100.0 9519.0 746.0 29.6 96.0 313.4 4548.1 2780.0 15960.0 58324.5 0.2 1961.3 15.8 30.9 1767.0 7058.0 110.0 4657.5 -13.0 320.0 660.0 9191.1 62800.0 1.0 3230.0 4702.0 5398.0 1.0 20.0 100.0 8150.0 8350.0 44950.0 13.2 1444.0 -13340.4 582.0 1005.0 1270.0 -5.7 1420.0 206.6 19145.0 1.0 270050.9 1440.0 12507.9 1.0\n\nC51 1946.0 354.5 3368.1 31860.0 826.0 1490040.0 948.0 28400.0 13069.2 824.0 30.3 91.8 373.0 6090.9 4360.0 158070.0 41656.5 0.6 1507.5 26.0 32.6 3317.0 9314.0 325.0 8098.0 -11.4 625.0 9870.0 9366.9 55060.0 1.0 2168.0 6278.0 12043.0 1.0 20.7 100.0 16575.0 10232.0 54490.0 22.5 10666.0 -19040.3 192.0 1725.5 22600.0 -1.5 3260.0 186.0 16046.0 1.0 477206.8 1620.0 15954.4 5910.0\n\nQRDQN QRDQN(tf) MMD MMD(tf)\n\n1625.0 554.6 7593.6 7660.0 1660.0 2520080.0 1000.0 37800.0 8043.8 928.0 35.5 98.3 361.4 5508.0 5490.0 69430.0 63889.0 -0.4 2832.5 33.4 34.0 4487.0 6466.0 565.0 11673.5 -3.6 1995.0 13440.0 9918.7 36020.0 1.0 2673.0 11739.0 12324.0 1.0 20.8 100.0 13830.0 8714.0 54620.0 48.1 2640.0 -29970.3 956.0 1826.5 38380.0 -11.9 6030.0 178.3 17074.0 1.0 388106.7 4890.0 17593.8 7410.0\n\n1425.8 870.2 7931.8 17126.8 1610.7 859419.7 936.7 25223.2 9728.9 766.3 33.8 97.9 391.0 5866.0 3575.7 85543.6 94433.0 -15.1 1912.4 18.9 33.1 3359.6 3854.7 509.5 9779.7 -3.7 694.0 14398.2 9293.1 27786.9 0.0 2557.7 11161.7 21813.7 -37.4 20.5 62.1 12307 10102.4 43574.9 52.4 4424.3 -27543.7 1220.5 1794.6 52937.6 -11.3 6338.7 153.8 14402.2 3.9 44599.9 8670.1 12336.1 9077.0\n\n2218.0 706.4 6001.5 15890.0 1095.0 80920.0 1034.0 28400.0 14072.6 959.0 60.0 96.9 405.9 5152.0 6760.0 112130.0 437760.5 -0.4 3248.2 24.5 33.6 2874.0 6412.0 345.0 7215.0 -4.5 480.0 14720.0 8732.7 36940.0 1.0 2568.0 12394.0 32086.0 1.0 20.9 100.0 15782.5 9350.0 42530.0 34.4 11685.0 -8983.3 3336.0 1216.0 52050.0 -1.5 7900.0 205.2 44746.0 1.0 288137.2 4480.0 8516.8 4640.0\n\n1683.1 694.7 8947.5 36411.5 1460.9 935273.3 982.8 20089.1 16889.1 802.3 37.3 92.3 486.9 5885.6 2465.8 115997.2 98849.8 -14.9 1839.5 21.6 33.2 3671.9 4966.6 547.7 8382.4 -2.4 657.4 8548.1 6069.8 28394.6 0.0 3244.8 10859.2 30561.7 -82.8 20.3 284.2 13325.7 10975.4 37909.9 48.4 1745.2 -23701.6 996.8 991.8 46658.3 -13.7 4516.2 190.5 19350.3 39.3 152686.0 5095.3 13420.7 8557.9\n\nSinkhorn 1873.0 506.7 3771.0 7610.0 624.0 3417430.0 849.0 27000.0 9865.6 1029.0 12.6 96.7 402.5 4952.2 6520.0 16000.0 195827.0 -2.2 4272.0 24.6 34.0 2632.0 15168.0 470.0 7476.0 -4.6 450.0 10680.0 9549.0 42600.0 0.0 2568.0 9200.0 18558.0 0.0 21.0 100.0 6530.0 11998.0 52600.0 48.1 14795.0 -29970.3 792.0 2302.5 43820.0 13.3 7060.0 202.8 20063.0 1370.0 164597.3 3250.0 13507.3 10320.0\n\nTable 3: Scores of all algorithms averaged over 3 seeds across 55 Atari games.\n\n28",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new DRL method leveraging Sinkhorn divergence. Analogous to DQN, they consider learning the action value distribution by minimizing the divergence between the current distribution and the target distribution. The main difference with previous DRL methods is they replace the divergence to be Sinkhorn divergence. Theoretically, they study the convergence of the distributional\nBellman operator, the moment matching meaning of Sinkhorn divergence, and the connection with MMD DRL. Although the paper is generally well-written, the experimental results does not show significant improvements over the current state-of-the art methods.\n\n# Strength And Weaknesses\n\nStrengths: 1. The new contributions of this paper is to replace the divergence in DRL to be Sinkhorn divergence, which can be viewed\nas an interpolation between Wasserstein distance and MMD.\n\nWeakness: 1. \"The main limitation of our proposal is that the superiority over existing state-of-the-art algorithms\nmay not be sufficiently significant.\" The authors themselves point this out. So what is the advantage of using Sinkhorn iterations?\n2. Figure 3 (b) Why does the performance degrade with increase in sample size? Increasing the samples should better approximate the return distribution.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written. \n\nI consider it as a \"new\" work. The new contributions of this paper is to replace the divergence in DRL to be Sinkhorn divergence, which can be viewed as an interpolation between Wasserstein distance and MMD.\n\nI am not convinced with average over only 3 seeds. At least a few experiments should be presented with more number of seeds.\n\n# Summary Of The Review\n\nThis paper proposes a new DRL method leveraging Sinkhorn divergence. Analogous to DQN, they consider learning the action value distribution by minimizing the divergence between the current distribution and the target distribution. The main difference with previous DRL methods is they replace the divergence to be Sinkhorn divergence. Theoretically, they study the convergence of the distributional\nBellman operator, the moment matching meaning of Sinkhorn divergence, and the connection with MMD DRL. Although the paper is generally well-written, the experimental results does not show significant improvements to the current state-of-the art methods.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSELF-CONSISTENT LEARNING: COOPERATION BETWEEN GENERATORS AND DISCRIMINATORS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nUsing generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a self-consistent learning framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple rounds of alternating training until a scoring consensus is reached. This framework proves to be easy to train and free from instabilities such as mode collapse and non-convergence. Extensive experiments on sentence semantic matching demonstrate the effectiveness of the proposed framework: the discriminator achieves 10+ AP of improvement on the zero-shot setting and new state-of-the-art performance on the full-data setting.\n\n1\n\nINTRODUCTION\n\nThe advance of Pre-trained Language Models (PLMs) (Brown et al., 2020; Chowdhery et al., 2022) has substantially improved the performance of deep neural networks across a variety of Natural Language Processing (NLP) tasks. Various language models, based on the Transformer (Vaswani et al., 2017) architecture, have been proposed, leading to state-of-the-art (SOTA) performance on the fundamental discrimination tasks. These models are first trained with self-supervised training objectives (e.g., predicting masked tokens according to surrounding tokens) on massive unlabeled text data, then fine-tuned on annotated data to adapt to downstream tasks of interest. However, annotated data is usually limited to a wide range of downstream tasks, which results in overfitting and a lack of generalization to unseen data.\n\nOne straightforward way to deal with this data scarcity problem is data augmentation (Xie et al., 2020), and incorporating generative models to perform data augmentation has been widely adopted recently (Carlini et al., 2021; Gangal et al., 2022). Despite its popularity, the generated text can easily deviate from the real data distribution without exploiting any of the signals passed back from the discrimination task. In previous studies, generative data augmentation and discrimination have been well studied as separate problems, but it is less clear how these two can be leveraged in one framework and how their performances can be improved simultaneously.\n\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014; Gulrajani et al., 2017) are good attempts to couple generative and discriminative models in an adversarial manner, where a twoplayer minimax game between learners is carefully crafted. GANs have achieved tremendous success in domains such as image generation (Denton et al., 2015), and related studies have also shown their effectiveness in semi-supervised learning (Salimans et al., 2016; Kumar et al., 2017). However, GANs are notoriously difficult to train, most training objectives work well for only one model, either the discriminator or the generator, so rarely both learners can be optimal at the same time (Arjovsky & Bottou, 2017; Wiatrak et al., 2019). This essentially arises from the adversarial nature of GANs,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthat during the process, optimizing one learner can easily destroy the learning ability of the other, making GANs fail to converge.\n\nAnother limitation of simultaneously optimizing the generator and the discriminator comes from the discrete nature of text in NLP, as no gradient propagation can be done from discriminators to generators. One theoretically sound attempt is to use reinforcement learning (RL), but the sparsity and the high variance of the rewards in NLP make the training particularly unstable (Caccia et al., 2020).\n\nTo address these shortcomings, we novelly introduce a self-consistent learning framework based on one generator and one discriminator: the generator and the discriminator are alternately trained by way of cooperation instead of competition, and the samples are used as the medium to pass the feedback signal from the discriminator. Specifically, in each round of training, the samples generated by the generator are synthetically labeled by the discriminator, and then only part of them would be selected based on dynamic thresholds and used for the training of the discriminator and the generator in the next round. Several benefits can be discovered from this cooperative training process. First, a closed-loop form of cooperation can be established so that we can get the optimal generator and discriminator at the same time. Second, this framework helps improve the generation quality while ensuring the domain specificity of generator, which in turn contributes to training. Third, a steady stream of diverse synthetic samples can be added to the training in each round and lead to continuous improvement of the performance of all learners. Finally, we can start the training with only domainrelated corpus and obtain strong results, while these data can be easily sampled with little cost or supervision. Also, the performance on labeled datasets can be further boosted based on the SOTA level. As an example to demonstrate the effectiveness of our framework, we examine it on the task of sentence semantic matching. The experiments show that our method significantly improves over standalone state-of-the-art discriminative models on zero-shot and full-data settings.\n\nOur contributions are summarized as follows,\n\n• We propose a self-consistent learning framework that incorporates the generator and the discriminator, in which both achieve remarkable performance gains simultaneously.\n\n• We propose a dynamic selection mechanism such that cooperation between the generator and the discriminator drives the convergence to reach their scoring consensus.\n\n• Experimental results show that our proposed framework significantly outperforms the state-of-theart methods for the task of sentence semantic matching.\n\n2 RELATED WORKS\n\nTo alleviate the lack of annotated data in supervised learning in NLP, semi-supervised learning (SSL) has been a popular line of research (Van Engelen & Hoos, 2020). The sources of the unlabeled data required by SSL are either collected from the domains or generated by generative language models. Then NLU models can learn from the unlabeled data by pseudo-labeling (Arazo et al., 2020; Banitalebi-Dehkordi & Zhang, 2021) and consistent regularization (Jeong et al., 2019; Sohn et al., 2020). However, collecting unlabeled data comes at a cost(though smaller than labeling data), and the total amount is limited. Even with generative models, there is no guarantee of the quality of the generated samples, because the model cannot tune the generating results based on the performance of the downstream tasks. In contrast, our method usually includes a continuously updated generative model, which dynamically adjusts its generation according to the performance of downstream tasks.\n\nIn GANs, the generator is adversarially trained with the discriminator. Unlike conventional GANs in continuous domains, language GANs usually employ Gumbel-Softmax differentiation (Jang et al., 2017; Yin et al., 2020), Reinforcement Learning (RL) (Yu et al., 2017; Wu et al., 2021), or modified training objectives (Montahaei et al., 2021) to update the generator, to use the non-differential signals from the discriminator. However, language GANs are often criticized for underperforming Maximum likelihood estimation (MLE) and are very difficult to train, even the single optimality of either the generator or the discriminator cannot be guaranteed (Alvarez-Melis et al., 2022). In comparison, our proposed framework allows us to cooperatively couple the generator and the discriminator, leading to continuous improvement for both learners.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 METHODOLOGY\n\n3.1 COOPERATIVE OR ADVERSARIAL\n\nFollowing the principle of self-consistency outlined in Ma et al. (2022), a closed-loop training needs to be built between the generator and the discriminator, either cooperatively or adversarially. GANs are typical examples of adversarial learning, but training GANs remains quite unstable. Let us consider an extreme case to show the possible instability: the discriminator can perfectly distinguish real data and fake data generated by the generator, and the generator can fully reproduce the real data distribution. Then the discriminator has only a 50% probability of selecting all samples that are generated by the generator. Therefore, any further updates to the generator parameters based on the feedback from the discriminator deviate the generator from the optimum. Neither the generator nor the discriminator can likely be optimal (Arjovsky & Bottou, 2017; Lamprier et al., 2022). In practice, a very delicate balance needs to be maintained between the discriminator and the generator to keep the training stable. In terms of cooperatively closed-loop learning, as discussed below, it does not suffer from instability: the generator and the discriminator usually enhance each other.\n\n3.2 SELF-CONSISTENT LEARNING FRAMEWORK\n\nIn this section, we introduce our self-consistent learning (SCL) framework.\n\nFigure 1: Overview of the flow chart for the SCL framework.\n\nAs shown in Figure 1, our framework, similar to the GANs, consists of a generator and a discriminator model. However, contrasting to the GANs, these two parts in our framework work cooperatively to enhance each other. Specifically, for any given class k, the generator G now become a conditional generator that takes in an input sentence sa k. The discriminator D is then responsible for discriminating the sentence using a dynamic threshold εD. The discriminated sentence is used as positive or negative data for that specific class to continue the training process. Once the new discriminator is trained, the sentence is discriminated again by the new discriminator with a different dynamic threshold εG. This time only the positive data is passed to the generator as the training data for the new round. In this way, a closed loop of cooperation is formed.\n\nk and generate an output sentence sb\n\nIn the above closed-loop training, we propose a selection mechanism that uses dynamic thresholds to filter samples. This mechanism is empirically shown to play a critical role in closing the gap between the generator and the discriminator, and thus makes this cooperation loop a virtuous circle. Specifically, as shown in Equation 1, the output probability pD(y = k|sb k} belongs to class k is calculated from the embedding representation H1 of {sb\n\nk) that the sentence {sb\n\nk},\n\npD(y = k|sb\n\nk) = softmax(MLP(H))\n\n(1)\n\n1We follow Reimers & Gurevych (2019) and use the embedding representation of CLS-token as the sen-\n\ntence representation H .\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nwhere y represents the class label. Then, through the filtering function filter(t) k (·) in round t for the k-th class in Equation 2, we keep samples whose output probability is not less than threshold εt,k, while other generated samples whose confidence is lower than threshold εt,k are discarded.\n\nfilter(t)\n\nk (sb\n\nk) ≜ pD(k|sb\n\nk) ≥ εt,k\n\n(2)\n\nwhere εt,k represents the dynamic threshold for accepting {sb the t-th round. The generalized threshold function for εt,k is defined as,\n\nk} as negative or positive samples in\n\nεt,k = f (t, Lt−1,k, εt−1,k)\n\n(3)\n\nwhere Lt−1,k and εt−1,k represent the discriminator loss and threshold for round t − 1, respectively. L0,k is set as 0 and ε0,k = λ, where λ represents a hyperparameter.\n\nTheorem 1 At round t, given the previous round discriminator Dt−1 generator Gt\n\nθ boils down to:\n\nφ , the aim of optimizing the\n\nmin θ\n\nDKL(pk\n\nDt−1\n\nφ\n\n(·), pk Gt θ\n\n(·))\n\nwhere DKL is the standard KL divergence, pk (·) refers to the degree of confidence that the Gt θ\nsentences generated by the generator belongs to a given class k (we can either train the generator to express its confidence in the generated sentences(Lin et al., 2022) or use a fixed third-party model to score them(Gao et al., 2021)), and pk (·) the probability of being classified into class k given\n\nDt−1\n\nφ\n\nby the discriminator.\n\nTheorem 1 shows that the generator at round t is encouraged to approximate the probability distribution given by the previous round discriminator. The proof is given in Appendix A. In particular, on the basis of a well-pretrained discriminator, the generated distribution of the generator can be guaranteed to be faithful to the real data distribution.\n\nWhy Cooperative, Not Adversarial? (1) the generator is no longer a challenger to the discriminator that only provides negative data points to fool it, but now serves as a data augmenter to provide both positive and negative data points to enhance the discriminator; (2) the generator no longer updates its parameters through the policy gradients guided by the signals from the discriminator, but rather by utilizing the filtered data points to further improve its conditional generation quality. Note that by deliberately choosing the conditional generation paradigm along with the selection mechanism, we not only make the training more stable due to the different training goals, but also bypass the mode collapse problem of GANs (see Section 4.6 for further discussion). Besides, by iterating through the loops, our framework achieves self-consistency by honing the domain specificity of the generator and increasing the domain data exposure of the discriminator.\n\n3.3 SENTENCE SEMANTIC MATCHING\n\nWe leverage the sentence semantic matching task (i.e. k = 2) as an example to demonstrate the effectiveness of our method. At this time, corresponding to Equation 2, k = 1/0 represents the positive/negative class, and filter(t) 1/0 represents the filter function in round t for the positive/negative class respectively. First, let us introduce the formal definition of this task. Given two sentences sa = {wa j represent the i-th and j-th lb tokens in the sentences, and la and lb indicate the length of sa and sb. The goal of this task is to learn a discriminator D to precisely predict the label y = D(sa, sb), where y ∈ Y = {0, 1} indicates whether the two sentences are similar.\n\n} and sb = {wb\n\n}, where wa\n\ni and wb\n\n2 , ..., wa\n\n2, ..., wb\n\n1 , wa\n\n1, wb\n\nla\n\nIn our task, G is trained to generate a similar sentence sb from any given sentence sa and D is trained to predict label y from any given sentence pair {sa, sb}. As demonstrated in Figure 1, there are mainly two training processes in the entire framework: fix G to train D and fix D to train G. We introduce the two training procedures in detail with the t-th round training.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nt. Next, we feed sentence pair {sa\n\nt to Gt Training D: We first randomly sample sa t} into Dt−1 to predict the label yt−1, and filter to generate sb {sa D . Finally, we train Dt−1 on the selected data and pre-training data P to get an improved discriminator Dt. Note that the filtered data have both positive and negative samples. The update process of D seeks to minimize the cross-entropy loss over all instances:\n\nt from domain-related corpus C, and then input sa\n\nt, yt−1} using threshold εt−1\n\nt , sb\n\nt , sb\n\nLD(s, y) =\n\n1 |s|\n\n|s| (cid:88)\n\ni=1\n\n−[yi · log pD(yi = 1|sa\n\ni , sb\n\ni ) + (1 − yi) · log(1 − pD(yi = 1|sa\n\ni , sb\n\ni ))]\n\n(4)\n\nt , sb\n\nt} into Dt to predict new labels yt, and Training G: We feed the generated sentence pairs {sa t, yt} using threshold εt then filter {sa G and additional rules 2. Note that the filtered data has only positive samples. For the filtered data, we supplement it with the pre-training data P to update Gt to Gt+1 3 We also take out sb t from the filtered data and add them to the domain-related corpus. The expanded domain corpus are used to sample conditional sentences in the next round of generation. The update procedure of G employs the negative log-likelihood function over all instances:\n\nt , sb\n\nLG(sa, sb) = −\n\n1 |sb|\n\n|sb| (cid:88)\n\nt=1\n\nlog pG(sb\n\nt|sb\n\n<t, sa)\n\nFor the selection mechanism, we adopt the form εt = m ∗ t + λ after comparing the effects of different functions through experiments according to Equation 3. where m is the increment of the threshold for each round, λ is the initial threshold, and εt is the threshold for rounds t.\n\nIn the process of training G, since the sentences generated in each round are added to the domainrelated corpus, the source of domain-specific data is thus monotonically expanding by iterating the self-consistent learning loop. The formalized process is shown in Algorithm 1.\n\nAlgorithm 1 Self-consistent Learning (SCL)\n\nRequire: Generator G; Discriminator D; Domain-Related Corpus C; Pre-training Data P .\n\nif Two consecutive rounds of discriminator still improve then\n\n1: Initialize G0 and D0 with pre-trained language models; 2: Warm-up G0 and D0 with pre-training data P to get G1 and D1; 3: for each round i ∈ [1, n] do 4: 5: 6: 7: 8: 9: end if 10: 11: end for\n\nGenerate similar sentences sb ∼ pGi (·|sa) from sampled sentences sa from C; Predict pseudo-labels yi ∼ pDi(·|sa, sb); Use threshold εi Predict pseudo-labels yi+1 ∼ pDi+1(·|sa, sb); Use threshold εi\n\nD to select data on {sa, sb, yi} to train Di+1;\n\nG and additional rules to select data on {sa, sb, yi+1} to train Gi+1;\n\n4 EXPERIMENTS\n\n4.1 TASKS DESIGN\n\nIn our experiments, the pre-training datasets are used to warm up the discriminator and generator, and the domain-related corpus is a set of independent sentences. To avoid label leakage, none of the\n\n2The additional rules are used to exclude sentences which are too long, too short, or too similar according\n\nto the longest common substring algorithm.\n\n3Note that the pre-training data P is used to warm up G and D. Although pre-training data is not mandatory in subsequent training, we empirically found that including it when training G can prevent language degeneration and improve downstream performances.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ntraining datasets participate in the pre-training of the generator and discriminator. In other words, the datasets in pre-training and self-consistent training are two non-overlapped datasets.\n\nZero-Shot Baseline: We utilize the best-performing Chinese model RoBERTa-wwm-ext-large (Cui et al., 2020; 2021) and English model ALBERT-xxlarge-v2 (Lan et al., 2020) as the base discriminators in our self-consistent learning framework.\n\nFine-Tune Baseline: We compare our model with several state-of-the-art semantic matching models, including the following. Chinese models MacBERT (Cui et al., 2020), StructBERT (Wang et al., 2020), RoFormer (Su et al., 2021; Su et al., 2022), XLNet (Xu et al., 2020), ELECTRA (Cui et al., 2020), ALBERT (Lan et al., 2020), RoBERTa (Cui et al., 2020; 2021) and English models BERT (Devlin et al., 2019b), XLM-RoBERTa (XLM-R) (Conneau et al., 2020), XLNet (Yang et al., 2019b), ELECTRA (Clark et al., 2020), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019). For a fair comparison, we use models that are as close in size as possible.\n\n4.2 EXPERIMENTS SETUP\n\n4.2.1 DATASETS\n\nWe conduct experiments on three Chinese semantic matching datasets AFQMC (Xu et al., 2020), CHIP-STS (Zhang et al., 2022a), Chinese-QQP (Wang et al., 2019) and an English semantic matching dataset MRPC (Wang et al., 2019). More details about the datasets are given in Appendix E.\n\n4.2.2 MODEL PRE-TRAINING\n\nWe adopt the well-established Transformer-XL (Dai et al., 2019)/OPT (Zhang et al., 2022b) architectures as the Chinese/English generator. To enable the generator to generate similar sentences with better linguistic quality, we pre-train a Transformer-XL model with 5.0 billion parameters and incrementally pre-train an OPT model with 2.7 billion parameters on the corpus consisting of plain texts and similar sentence pairs. Cleaned large-scale Chinese corpus WuDaoCorpora (Yuan et al., 2021) and English corpus WikiText (Merity et al., 2017) are used as plain texts. Similar sentence pairs that do not overlap with downstream datasets are used in the pre-training, and the designed prompts are employed to guide the generation of similar sentences. More details regarding model pre-training can be found in Appendix D.\n\n4.3 ZERO-SHOT RESULTS\n\nTable 1 shows how the F1 score of the discriminator varies with the number of self-consistent learning rounds on different datasets in the zero-shot task. According to Algorithm 1, the training is stopped when the discriminator no longer improves for two consecutive rounds. In addition, these four datasets are collected from different domains to further reflect the generality of our method in different domains. Specific training settings are recorded in Appendix F.\n\nTable 1: Zero-Shot Performance of the Discriminator. (F1 score (%))\n\nRound Num\n\nAFQMC (Financial)\n\nCHIP-STS (Medical)\n\nChinese-QQP (Common)\n\nMRPC (News)\n\nbaseline 1\n2 3\n4 5\n6 7\n8 9\n10\n\n38.25 39.61 44.98 45.99 45.71 48.01 50.41 50.68 51.36 -\n-\n\n58.82 62.89 67.24 71.38 71.45 74.06 74.08 76.66 76.30 76.67 77.42\n\n57.88 60.08 58.57 60.30 61.31 64.47 66.44 63.88 65.46 68.08 70.51\n\n68.54 75.47 76.63 83.00 83.90 84.24 84.50 84.32 84.61 -\n-\n\n+13.11\n\n+18.60\n\n+12.63\n\n+16.07\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nThe scores in the last line of Table 1 give the improvement of our discriminator in the last round relative to the first round. We can see that the F1 score gradually increases after each training round, eventually reaching a 10+ absolute percentage (AP) improvement. We believe what drives the improvement of the discriminator is the self-consistency, which it acquires with the generator step by step during the loop.\n\nTo verify that the generator also improves after self-consistent training, we adopt Perplexity and Bertscore (Zhang et al., 2020) to measure the language fluency and the semantic similarity (i.e. domain specificity) respectively. For different generators in different rounds, we first select sa in similar sentence pairs from the same test set as the original sentences input, and generate similar sentences sb with greedy search. The reason for not using other sampling methods is to ensure reproducibility. Given the generated sentences, we introduce an additional GPT2 4 model to calculate the perplexity of generated similar sentences, and use a third-party library 5 to calculate the bertscore between the original and generated similar sentences. The results are shown in Table 2.\n\nTable 2: Zero-Shot Performance of the Generator.\n\nAFQMC CHIP-STS\n\nChinese-QQP MRPC\n\nPerplexity -first round Perplexity -last round\n\nBertscore -first round Bertscore -last round\n\n10.13\n\n8.43\n\n0.79\n\n0.80\n\n6.86\n\n5.97\n\n0.84\n\n0.85\n\n12.94\n\n12.27\n\n0.87\n\n0.89\n\n28.71\n\n17.56\n\n0.94\n\n0.97\n\nWe can see that the perplexity/bertscore of the last round in Table 2 has decreased/improved compared to the first round. Note that a lower perplexity indicates a more fluent sentence, while a higher bertscore indicates a more similar sentence. It suggests that after self-consistent training, the generator is gradually improved in language fluency and semantic similarity (i.e. domain specificity). The reason why the improvement of the generator is not as obvious as that of the discriminator is that the size of the generator is several times that of the discriminator, and the total number of training samples is limited. In Appendix G, the generated samples of the generator in different rounds are given to show the changes in the generation.\n\n4.4 FINE-TUNE RESULTS\n\nOur method not only works well in the zero-shot case, but also achieves good results in the full-data case. For the sake of a fair comparison, we reproduce several state-of-the-art semantic matching models on the four training sets, and their performances on the test sets are shown in Table 3.\n\nOur approach uses the best-performing model on a single test set as the base discriminator for selfconsistent learning. The bold scores in the last line of Table 3 show that our method outperforms the SOTA results (shaded in gray) by 1 to 2 AP on all four test datasets, indicating the potential of self-consistent learning to further improve the model performance and establish new SOTA.\n\n4.5 EVALUATING SELF-CONSISTENCY\n\nIn this section, we evaluate the consistency between the generator and the discriminator as the learning loop unfolds. We follow the same method used in Section 4.3 and use greedy search to generate similar sentences on the same test set. Then we take the confidence of the discriminator RD as the score of the discriminator, which is calculated for the original sentences sa and the generated similar sentences sb according to Equation 5.\n\n4Wenzhong-GPT2-110M (Wang et al., 2022) for Chinese data, and GPT2-base (Radford et al., 2019) for\n\nEnglish data.\n\n5https://pypi.org/project/bert-score/\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: F1 Score(%) of Different Discriminators on the Test Datasets.\n\n#Param(zh/en) AFQMC CHIP-STS Chinese-QQP MRPC\n\nAVG\n\nBERTlarge XLM-Rbase\n\nMacBERTlarge StructBERTlarge RoFormerlarge\n\nXLNetlarge ELECTRAlarge ALBERTlarge RoBERTalarge\n\n-/335M -/278M\n\n326M/- 326M/- 316M/-\n\n360M/360M 324M/334M 221M/223M 326M/355M\n\nOur Method\n\n-\n\n- -\n\n61.11 60.56 64.19\n\n50.31 54.59 56.87 57.29\n\n66.59\n\n- -\n\n85.94 85.17 84.16\n\n82.97 84.97 86.32 86.93\n\n88.39\n\n- -\n\n72.94 76.33 76.56\n\n64.96 71.81 70.52 74.58\n\n78.43\n\n82.51 84.27\n\n- -\n\n- -\n-\n\n79.51 89.64 91.21 90.24\n\n92.78\n\n73.33 74.02 74.97\n\n69.44 75.25 76.23 77.26\n\n81.55\n\nRD = pD(y+|sa, sb)\n\n(5)\n\nwhere y+ represents a positive label.\n\nHowever, for the generator, to the best of our knowledge, there is no reliable way to measure how similar sa and sb are by using the generator itself. Therefore, to quantify this similarity, we introduce a third-party static model SimCSE 6 to get the embedding representation A, B of sentences sa, sb. The cosine similarity RG between A and B is then calculated according to Equation 6 to approximate the score of the generator.\n\nA, B = Encoder(sa), Encoder(sb)\n\nRG =\n\nA · B ∥A∥2 ∗ ∥B∥2\n\n(6)\n\nwhere A and B both represent the embedding representation at the [CLS] position. Note that the original sentence sa remains unchanged in each round, while the generated sentence sb changes.\n\nD and Rt\n\nFinally, for the trained discriminator and generator in each round t, we can obtain two score distributions Rt G correspondingly. According to Theorem 1, we draw the curves of KL divergence between Rt G in each round for the four datasets: AFQMC, CHIP-STS, Chinese-QQP, and MRPC. As illustrated in Figure 2, all the curves show a clear downward trend, indicating that the distance between the two score distributions decreases with the increase in the number of training rounds until a score consensus is reached. Table 4 shows the values of KL divergence in the first and last rounds. Numerically, it is more evident that the distances are significantly reduced on the four datasets.\n\nD and Rt\n\n4.6 EFFECT OF PRE-TRAINING DATA AND SELECTION MECHANISM\n\nWe perform ablation experiments on the pre-training data and the selection mechanism in the zeroshot case. As described in Section 4.1, the pre-training data is used to pre-train the generator and discriminator, completely independent of the experimental datasets in self-consistent training.\n\nTo explore the influence of pre-training data on self-consistent training, we no longer add it in each round when training the discriminator, and only the generated data is used. But when the generator is trained, pre-training data is still retained to prevent language degeneration and lack of expressive diversity of the generation. The result of removing pre-training data is shown as the green curves in Figure 3. With all other training parameters being the same, after the same number of training rounds, the discriminator is slightly worse compared to the original method (red curves in Figure 3).\n\n6We use SimCSE-BERT-base to calculate scores on Chinese datasets and sup-SimCSE-BERT-base-uncased\n\non English datasets. (Gao et al., 2021)\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nKL Divergence -first round\n\nKL Divergence -last round\n\nAFQMC CHIP-STS Chinese-QQP MRPC\n\n0.29 0.16 0.18 0.22\n\n0.26 0.08 0.06 0.02\n\nTable 4: The KL divergence in the first and last rounds.\n\nFigure 2: The KL Divergence between the score distributions of the Discriminator and the Generator.\n\nHowever, the green curves maintain an upward trend and are very close to the red curves in all datasets except CHIP-STS. This shows that the generated data plays a key role in continuously improving the discriminator, while the pre-training data has a limited role.\n\nFigure 3: Results of ablation experiments on pre-training data and selection mechanism. Results of the proposed method, results without pre-training data, and results without the selection mechanism are given in red, green, and blue, respectively.\n\nIn order to explore the effect of the selection mechanism on training the discriminator, we remove the selection mechanism when training the discriminator, while the training of the generator remains unchanged. The blue curves in Figure 3 depict the performance of the discriminator in each round after removing the selection mechanism. Compared to the original method (red curves), the discriminator only improves in the first round after removing the selection mechanism, which demonstrates the importance of the selection mechanism on the discriminator for the convergence of the selfconsistent learning framework.\n\n5 CONCLUSION\n\nIn this paper, we propose a self-consistent learning framework to enable cooperative training of the generator and the discriminator. During the training process, the generator and the discriminator continuously enhance each other until reaching a score consensus. This framework can utilize both limited labeled data and large-scale unlabeled domain-related corpus. Experimental results on four Chinese/English sentence semantic matching datasets demonstrate that as a form of closedloop training, our proposed framework can achieve new state-of-the-art results with continuously improved generators and discriminators.\n\nFor future work, we will explore the effectiveness of our self-consistent learning framework on more NLP tasks, since the framework is straightforward and has no additional requirements on generators and discriminators.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY STATEMENT\n\nWe now discuss the efforts that have been made to ensure the reproducibility of our work. We have packaged the executable code and data into supplementary materials, which can be downloaded and run directly. In addition, we also provide detailed experimental parameters in the appendix to reproduce the experimental results. All datasets and code platforms (Pytorch) we use are public.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDavid Alvarez-Melis, Vikas Garg, and Adam Tauman Kalai. Why gans are overkill for nlp. arXiv\n\npreprint arXiv:2205.09838, 2022. URL https://arxiv.org/abs/2205.09838.\n\nEric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2020.\n\nMart ́ın Arjovsky and L ́eon Bottou. Towards principled methods for training generative adversarial In 5th International Conference on Learning Representations, ICLR 2017, Toulon, networks. France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=Hk4_qw5xe.\n\nAmin Banitalebi-Dehkordi and Yong Zhang. Repaint:\n\nImproving the generalization of downstream visual tasks by generating multiple instances of training examples. In 32nd British Machine Vision Conference 2021, BMVC 2021, Online, November 22-25, 2021, pp. 122. BMVA Press, 2021. URL https://www.bmvc2021-virtualconference.com/assets/ papers/0068.pdf.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent CharIn 8th International Conference on Learning Representalin. Language gans falling short. tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=BJgza6VtPB.\n\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633–2650, 2021.\n\nJing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu, and Buzhou Tang. The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4946–4951, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/ v1/D18-1536. URL https://aclanthology.org/D18-1536.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm ́an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. UnIn Proceedings of the 58th Annual supervised cross-lingual representation learning at scale. Meeting of the Association for Computational Linguistics, pp. 8440–8451, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https: //aclanthology.org/2020.acl-main.747.\n\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pretrained models for Chinese natural language processing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 657–668, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.58. URL https://aclanthology. org/2020.findings-emnlp.58.\n\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang. Pre-training with whole word masking for chinese bert. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3504–3514, 2021. doi: 10.1109/TASLP.2021.3124365.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285.\n\nDeep generative Emily L. Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. image models using a laplacian pyramid of adversarial networks. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 1486–1494, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/ aa169b49b583a2b5af89203c2b78c67c-Abstract.html.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019a. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019b. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\n\nVarun Gangal, Steven Y Feng, Malihe Alikhani, Teruko Mitamura, and Eduard Hovy. Nareor: The narrative reordering problem. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 10645–10653, 2022.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6894–6910, 2021.\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https: //arxiv.org/abs/1406.2661.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nIshaan Gulrajani, Faruk Ahmed, Mart ́ın Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5767–5777, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=rygGQyrFvH.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumble-softmax. In International Conference on Learning Representations (ICLR 2017). OpenReview. net, 2017.\n\nJisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object detection. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 10758–10767, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/d0f4dae80c3d0277922f8371d5827292-Abstract.html.\n\nAbhishek Kumar, Prasanna Sattigeri, and Tom Fletcher. Semi-supervised learning with gans: Manifold invariance with improved inference. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5534–5544, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ d3d80b656929a5bc0fa34381bf42fbdd-Abstract.html.\n\nSylvain Lamprier, Thomas Scialom, Antoine Chaffin, Vincent Claveau, Ewa Kijak, Jacopo Staiano, and Benjamin Piwowarski. Generative cooperative networks for natural language generation. arXiv preprint arXiv:2201.12320, 2022. URL https://arxiv.org/abs/2201.12320.\n\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id= H1eA7AEtvS.\n\nStephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in\n\nwords. arXiv preprint arXiv:2205.14334, 2022.\n\nXin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, and Buzhou Tang. LCQMC:a large-scale Chinese question matching corpus. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 1952–1962, Santa Fe, New Mexico, USA, 2018. Association for Computational Linguistics. URL https://aclanthology.org/ C18-1166.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv e-prints, pp. arXiv–1907, 2019.\n\nYi Ma, Doris Tsao, and Heung-Yeung Shum. On the principles of parsimony and self-consistency for the emergence of intelligence. Frontiers of Information Technology & Electronic Engineering, pp. 1–26, 2022.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture In 5th International Conference on Learning Representations, ICLR 2017, Toulon, models. France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=Byj72udxe.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nEhsan Montahaei, Danial Alihosseini, and Mahdieh Soleymani Baghshah. Dgsan: discrete genera-\n\ntive self-adversarial network. Neurocomputing, 448:364–379, 2021.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERTIn Proceedings of the 2019 Conference on Empirical Methods in Natural Lannetworks. guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982–3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/ D19-1410.\n\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2226–2234, 2016. URL https://proceedings.neurips.cc/ paper/2016/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html.\n\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596–608, 2020.\n\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv e-prints, art. arXiv:2104.09864, 2021. URL https://arxiv.org/abs/2104.09864.\n\nJianlin Su, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformerv2: A faster and better roformer - zhuiyiai. Technical report, 2022. URL https://github.com/ZhuiyiTechnology/ roformer-v2.\n\nJesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine Learn-\n\ning, 109(2):373–440, 2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= rJ4km2R5t7.\n\nJunjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, Chongpei Chen, Ruyi Gan, and Jiaxing Zhang. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. arXiv preprint arXiv:2209.02970, 2022. URL https://arxiv.org/abs/2209.02970.\n\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, and Luo Si. Structbert: Incorporating language structures into pre-training for deep language understanding. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id= BJgQ4lSFPH.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nMaciej Wiatrak, Stefano V Albrecht, and Andrew Nystrom. Stabilizing generative adversarial networks: A survey. arXiv preprint arXiv:1910.00927, 2019. URL https://arxiv.org/abs/ 1910.00927.\n\nQingyang Wu, Lei Li, and Zhou Yu. Textgail: Generative adversarial imitation learning for text In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.\n\ngeneration. 14067–14075, 2021.\n\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems, 33:6256–6268, 2020.\n\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 4762–4772, Barcelona, Spain (Online), 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.419. URL https://aclanthology.org/ 2020.coling-main.419.\n\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3687–3692, Hong Kong, China, 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1382. URL https://aclanthology. org/D19-1382.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5754–5764, 2019b. URL https://proceedings.neurips.cc/paper/ 2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html.\n\nHaiyan Yin, Dingcheng Li, Xu Li, and Ping Li. Meta-cotgan: A meta cooperative training paradigm for improving adversarial text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 9466–9473, 2020.\n\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In Satinder P. Singh and Shaul Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pp. 2852–2858. AAAI Press, 2017. URL http://aaai.org/ocs/index. php/AAAI/AAAI17/paper/view/14344.\n\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open, 2:65–68, 2021. ISSN 2666-6510. doi: https://doi.org/10.1016/j.aiopen. 2021.06.001. URL https://www.sciencedirect.com/science/article/pii/ S2666651021000152.\n\nNingyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan Liang, Lei Li, Xin Shang, Kangping Yin, Chuanqi Tan, Jian Xu, Fei Huang, Luo Si, Yuan Ni, Guotong Xie, Zhifang Sui, Baobao Chang, Hui Zong, Zheng Yuan, Linfeng Li, Jun Yan, Hongying Zan, Kunli Zhang, Buzhou Tang, and Qingcai Chen. CBLUE: A Chinese biomedical language understanding evaluation benchmark. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7888–7915, Dublin, Ireland, 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.544. URL https://aclanthology.org/2022. acl-long.544.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022b. URL https://arxiv.org/ abs/2205.01068.\n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: EvalIn 8th International Conference on Learning Representauating text generation with BERT. tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS\n\nA.1 PROOF OF THEOREM 1\n\nTheorem 1 At round t,given the previous round discriminator Dt−1 the generator Gt\n\nθ, boils down to,\n\nφ , the aim of the optimization of\n\nmin θ\n\nDKL(pk\n\nDt−1\n\nφ\n\n(·), pk Gt θ\n\n(·))\n\nwhere DKL is the standard KL divergence, pk (·) refers to the degree of confidence that the senGt θ\ntences generated by the generator belong to a given class k (we can either train the generator to express its confidence in the generated sentences(Lin et al., 2022) or use a fixed third-party model to score them(Gao et al., 2021)), and pk (·) the probability of being classified into class k given\n\nDt−1\n\nφ\n\nby the discriminator.\n\nProof. We use the previous round generator Gt−1 previous round discriminator Dt−1 of the current round generator Gt probability that the generated samples pass the discrimination for the fixed Dt−1 k, we have\n\nto generate samples, and filter them using the φ with a threshold εt−1, then these samples are used for training θ will tend to maximize the φ . For a given class\n\nθ. Therefore, the optimization of Gt\n\nθ\n\nmax θ\n\nE\n\nx∼pk G\n\nt−1 θ\n\npk Gt θ\n\n(x) s.t. filter(t−1)\n\nk\n\n(x) = 1\n\nwhere the definition of function filter(t−1)\n\nk\n\n(·) has been given in Equation 2.\n\nThe above objective is equivalent to sampling from the generator being optimized in round t and making these samples pass the discrimination in round t − 1 as much as possible, which gives\n\nmax θ\n\n(x) is fixed.\n\nwhere pk\n\nDt−1\n\nφ\n\nE\n\nx∼pk\n\nGt θ\n\npk\n\nDt−1\n\nφ\n\n(x)\n\nA further transformation of the formula shows that\n\npk\n\nDt−1\n\nφ\n\n(x)\n\nx∼pk\n\nGt θ\n\nE\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\nmax θ\n\n(i) ⇒ max\n\nθ\n\n(ii) ⇒ max\n\nθ\n\n(iii) ⇒ max\n\nθ\n\ndθθθ∇θθθE\n\nx∼pk\n\nGt θ\n\npk\n\nDt−1\n\nφ\n\n(x)\n\ndθθθE\n\nx∼pk\n\nGt θ\n\n∇θθθ log pk Gt θ\n\n(x)pk\n\nDt−1\n\nφ\n\n(x)\n\ndθθθ∇θθθ\n\n1 N\n\nN (cid:88)\n\ni=1\n\n{log pk Gt θ\n\n(xi)pk\n\nDt−1\n\nφ\n\n(xi) − log pk\n\nDt−1\n\nφ\n\n(xi)pk\n\nDt−1\n\nφ\n\n(xi)}\n\n(iv) ⇒ min\n\nθ\n\nDKL(pk\n\nDt−1\n\nφ\n\n(·), pk Gt θ\n\n(·))\n\nwhere (i) uses the integral property that integrating the derivative of a function gives the original function along with a constant, (ii) takes advantage of the derivative property of the logarithmic function, (iii) approximates the expectation of the probability distribution pk (·) by using averagGt θ\ning on N samples sampling from pk Gt θ\nsummation would not change its derivative, and (iv) cancels out the integral and the derivative and uses the definition of KL divergence. The above concludes our proof.\n\n(·), and adding a constant term with respect to θ under the\n\nB EXPERIMENTS ON DIFFERENT FILTER FUNCTIONS\n\nTo compare the effect of different filter functions on the final result, we use four type of functions, including oscillatory function (cosine), constant function and monotonically increasing functions\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(quadratic and linear). For the fairness of comparison, we keep the maxima and minima the same for all functions(except for the constant threshold), and the values are given in Appendix F. In addition, the number of training rounds for different functions on the same dataset remains the same.\n\nIn the results below, the best results and the second-best results are bold and underlined, respectively.\n\nTable 5: Performance of Different Filter Functions in Zero-Shot Setting. (F1 Score(%))\n\nAFQMC CHIP-STS Chinese-QQP MRPC\n\nAVG\n\nBaseline Cosine Constant Quadratic\n\nLinear\n\n38.25 47.38 47.06 51.75\n\n51.36\n\n58.82 74.26 74.15 73.09\n\n77.42\n\n57.88 64.39 68.67 70.85\n\n70.51\n\n68.54 83.48 84.11 83.48\n\n84.61\n\n55.87 67.38 68.50 69.79\n\n70.98\n\nAs can be seen from the Table 5, in the zero-shot setting, the chosen linear function outperforms the other functions, and all the filter functions show an averaging 10+ AP improvement relative to the baseline. Therefore, the self-consistent learning framework makes it easy to choose a certain threshold function and perform well, and the results are not so sensitive to the choice of the functions.\n\nFigure 4: Results of contrast experiments on Cosine(green), Constant(orange), Quadratic(blue) and Linear(red) function.\n\nFigure 4 dipicts the comparison results in each round. The linear function (red line) is significantly better than the other functions on both CHIP-STS and MRPC datasets. In the AFQMC and ChineseQQP datasets, the quadratic function (blue line) is slightly more effective than the linear function. In general, we can intuitively see that all functions show a significant increase relative to the starting point.\n\nTable 6: Performance of Different Filter Functions in Fine-Tune Setting. (F1 Score(%))\n\nAFQMC CHIP-STS Chinese-QQP MRPC\n\nAVG\n\nBaseline Cosine Constant Quadratic\n\nLinear\n\n64.19 66.43 66.57 66.37\n\n66.59\n\n86.93 88.01 88.15 87.76\n\n88.39\n\n76.56 77.33 78.45 79.26\n\n78.43\n\n91.21 92.63 92.51 92.75\n\n92.78\n\n79.72 81.10 81.42 81.54\n\n81.55\n\nTable 6 shows the effects of different filter functions in the fine-tune experiment. It can be seen that all functions have a 1 ∼ 2 AP increase relative to the baseline, and the chosen linear function achieves the best performance on all datasets except Chinese-QQP.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nC CONTRASTIVE EXPERIMENTS WITH ADVERSARIAL TRAINING\n\nIn this section, We further demonstrate the superiority of the cooperative approach by comparing the results with adversarial experiments. All experimental settings independent of the training method remain the same in the adversarial training.\n\nDuring the experiments, the generator is no longer trained using the samples filtered by the discriminator, but the rewards passed by the discriminator assist the training. All generated samples are treated as negative samples when training the discriminator.\n\nSpecifically, G takes the prompt ’ ”sa” is similar to ” ’ and the first M tokens of sb as input to get M sentence pairs < sa, sb m >, where m is from 1 to M . Note that we repeat the process of generating sentences N times to reduce the negative impact caused by the large variance of the rewards.7 The sentence pair gn\n\nm of the m-th token at the n-th time is formalized as\n\nm ←< sa, sb gn\n\nm >= Gθ(sb\n\nm|sb\n\n<m, sa; N )\n\nOnce the M ∗ N sentence pairs gn m are generated, they are passed as input to the D to obtain the m over N as the reward ̄Qm probability score Qn corresponding to the m-th token. If the sentence length of sb is greater than M , the rewards of the remaining tokens are all the same as those of the M -th token. Taking the m-th token as an example, the rewards ̄Qm can be formalized as\n\nm for each of them. We take the average of Qn\n\n ̄QGθ Dφ\n\n(m) =\n\n(cid:26) 1 N\n\n(cid:80)N\n\nn=1 Dφ(gn ̄Q(M )\n\nm) m ≤ M m > M\n\nTherefore, the objective function for training the generator G is,\n\nLG(sa, sb) = −\n\n1 |sb|\n\n|sb| (cid:88)\n\nt=1\n\nlog(pG(sb\n\nt|sb\n\n<t, sa) ∗ ̄Qt)\n\nThe loss function of training the discriminator remains the same as Equation 4, but differing from cooperative training, the generated samples are regarded as negative samples to the discriminator, and the training target for the discriminator can be given by\n\nmin φ\n\n−EX∼pdata [log Dφ(X)] − EX∼pGθ\n\n[log (1 − Dφ(X))]\n\nThe results of zero-shot and fine-tune on the four datasets are shown in Tables 7 and 8.\n\nAs can be seen from Table 7, in the zero-shot setting, training in an adversarial manner does not give any improvement over the baseline. Because the initial discriminator in the zero-shot setting is very weak in distinguishing positive and negative samples, it is reasonable to believe that if all generated samples are considered negative samples from the very beginning, it is difficult for the discriminator to know how to distinguish positive samples. As a result, the F1 scores on both AFQMC and CHIP-STS datasets end up being 0, while the scores on the Chinese-QQP and MRPC datasets fluctuate intensively with the number of rounds, which further validates the instability of the adversarial training in the zero-shot setting.\n\nFor the fine-tune experiments, Table 8 shows that training in an adversarial manner can slightly improve the performance on the Chinese-QQP and MRPC datasets, but is still worse than the cooperative training. On the AFQMC and CHIP-STS dataset, adversarial training makes it even worse relative to the baseline. It is worth noting that the whole process of adversarial training is so unstable and it is easy to collapse after a few training rounds.\n\n7In practice, we take M = 5, N = 5 for ease of calculation.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Zero-Shot Performance of Adversarial Trained Discriminator. (F1 score (%))\n\nRound Num\n\nAFQMC (Financial)\n\nCHIP-STS (Medical)\n\nChinese-QQP (Common)\n\nMRPC (News)\n\nbaseline 1\n2 3\n4 5\n6 7\n8 9\n10\n\n38.25 0.0 0.02 0.0 0.0 0.0 0.0 0.0 0.0 -\n-\n\n58.82 8.73 7.13 0.29 1.09 0.10 0.39 0.20 0.20 0.20 0.20\n\n57.88 21.71 49.30 42.94 41.13 43.10 34.30 42.62 34.95 41.81 40.00\n\n68.54 4.19 7.06 5.32 0.0 1.72 67.38 48.31 37.97 -\n-\n\nTable 8: Fine-Tune Performance of the Discriminator. (F1 score (%))\n\nAFQMC CHIP-STS\n\nChinese-QQP MRPC\n\nAVG\n\nBaseline Adversarial Cooperative (Our Method)\n\n64.19 58.37\n\n66.59\n\n86.93 80.46\n\n88.39\n\n76.56 77.93\n\n78.43\n\n91.21 92.18\n\n92.78\n\n79.72 77.24\n\n81.55\n\nD MODEL DETAILS\n\nThe 5.0B Transformer-XL is pre-trained on 32 A100s with 40G memory for 45 days, the batch size is set to 32*8=256. After running 445k steps, the final validation loss reduces to about 2.4. The 2.7B OPT is incrementally trained on the basis of the open-source model.\n\nDuring the pre-training of the generator model, we utilize the memory-cache mechanism of Transformer-XL and design a special attention mask to concatenate the multiple input sentences into one sample, to reduce the number of the padding token in a batch and therefore increase the number of effective tokens. To make the generation more robust, we add noise to the original sentences by randomly replacing or discarding tokens with a 5% probability. In addition, the prompts that we use for Chinese generation and English generation are as follows,\n\n• Chinese prompt: “sa”的相似句是“sb” (en: A similar sentence to “sa” is “sb”.)\n\n• English prompt: “sa” is similar to “sb”\n\nWhen training the discriminator, following the usage of special tokens in BERT (Devlin et al., 2019a), we use [SEP ] to concatenate two sentences and take the embedding at the [CLS] position to represent the whole sentence to predict the label. Moreover, we utilize the mask method in BERT to randomly mask 15% of the input tokens.\n\nE DATASET DETAILS\n\nThe statistics of the experimental datasets are reported in Table 9.\n\nOther Chinese datasets (LCQMC (Liu et al., 2018), OPPO, PAWS-X-zh (Yang et al., 2019a), BQ (Chen et al., 2018), CCKS, Chinese-STS-B (Wang et al., 2019)) and English datasets (QQP (Wang et al., 2019), STS-B (Wang et al., 2019), PAWS-X-en (Yang et al., 2019a)) are collected and used as the corpus of similar sentence pairs for pre-training the generator.\n\nThe Chinese-QQP dataset contains 9000 pieces of data randomly selected and translated from the English QQP dataset, which is then divided into training set and test set in a ratio of 3:2.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Statistics of Experimental Datasets\n\nAFQMC CHIP-STS Chinese-QQP MRPC(en)\n\nZero-Shot\n\nFine-Tune\n\ndomain-related corpus test dataset\n\ntraining dataset test dataset\n\n68668\n\n4316\n\n34334 4316\n\n32000\n\n4000\n\n16000 4000\n\n10800\n\n3600\n\n5400 3600\n\n8,152\n\n1725\n\n4076 1725\n\nF PARAMETER SETTINGS\n\nThe training parameters of zero-shot are shown in Table 10. The three thresholds are used to select positive and negative examples for training the discriminator and positive examples for training the generator, respectively. We adopt cosine annealing learning rate decay strategy during training.\n\nTable 10: Parameter Settings of Zero-Shot.\n\nAFQMC CHIP-STS Chinese-QQP MRPC\n\nMaximum Threshold -negative Minimum Threshold -negative Maximum Threshold -positive Minimum Threshold -positive Maximum Threshold -generator Minimum Threshold -generator Threshold Increase Sentence Num Learning Rate Warm Up Steps Early Stopping Patience Generator Batch Size -training Generator Batch Size -predicting\n\nDiscriminator Batch Size -training Discriminator Batch Size -predicting\n\n0.8\n\n0.6\n\n0.8\n\n0.6\n\n0.6\n\n0.6\n\n0.9\n\n0.7\n\n0.9\n\n0.7\n\n0.9\n\n0.7\n\n0.07 6000\n\n0.1 6000\n\n2e-5 40 1\n\n0.95\n\n0.8\n\n0.95\n\n0.8\n\n0.95\n\n0.8\n\n0.05 3000\n\n2(concatenate 30 samples)\n\n512\n\n64\n\n384\n\n0.95\n\n0.8\n\n0.95\n\n0.8\n\n0.95\n\n0.8\n\n0.05 4000\n\n24\n\n100\n\n32\n\n96\n\nThe training parameters of fine-tuning are shown in Table 11.\n\nG GENERATE SAMPLES\n\nThe generators use nucleus sampling (Holtzman et al., 2020) to generate similar sentences. Generated examples in English are shown in Table 12 and in Chinese in Table 13.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Parameter Settings of Fine-Tune.\n\nAFQMC CHIP-STS Chinese-QQP MRPC\n\nMaximum Threshold -negative Minimum Threshold -negative Maximum Threshold -positive Minimum Threshold -positive Maximum Threshold -generator Minimum Threshold -generator Threshold Increase Sentence Num Learning Rate Warm Up Steps Early Stopping Patience Generator Batch Size -training Generator Batch Size -predicting\n\nDiscriminator Batch Size -training Discriminator Batch Size -predicting\n\n32\n\n256\n\n0.98\n\n0.9\n\n0.98\n\n0.9\n\n0.98\n\n0.9\n\n0.07 6000\n\n0.98\n\n0.7\n\n0.98\n\n0.7\n\n0.98\n\n0.7\n\n0.07 6000\n\n0.84\n\n0.6\n\n0.98\n\n0.9\n\n0.98\n\n0.9\n\n0.07 3000\n\n5e-6 40 1\n\n2(concatenate 30 samples)\n\n512\n\n64\n\n384\n\n32\n\n256\n\n0.8\n\n0.6\n\n0.8\n\n0.6\n\n0.8\n\n0.6\n\n0.2 3000\n\n24\n\n100\n\n32\n\n96\n\nTable 12: Generated Samples in English\n\nMRPC(News) origin: Hong Kong was flat, Australia, Singapore and South Korea lost 0.2-0.4 percent. first round: the market is flat, the US is flat, Europe is flat, Japan is flat, ... Malaysia is flat, Philippines is flat, Vietnam is flat, Singapore is last round: Hong Kong was unchanged, Australia, Singapore and South Korea were down 0.2-0.4 percent. origin: Ballmer has been vocal in the past warning that Linux is a threat to Microsoft. first round: Microsoft is a threat to Linux. last round: Ballmer has been vocal in the past about the threat Linux poses to Microsoft. origin: Last year, Congress passed similar, though less expensive, buyout legislation for peanut farmers, ending that Depression-era program. first round: The government is going to buy out your farm and give you a check for $1,000,000.00. You can’t refuse. last round: Congress passed a similar bill last year to end a Depression-era program for peanut farmers, ending that program. origin: Licensing revenue slid 21 percent, however, to $107.6 million. first round: The company’s net income fell to $1.1 million, or 2 cents per share, in the quarter ended Sept. 30, from $1.3 million, or 3 cents per share, a year earlier. last round: Licensing revenue fell 21 percent to $107.6 million in the third quarter. origin: SCO says the pricing terms for a license will not be announced for weeks. first round: we’re not going to tell you how much it costs to buy a car, but we’re going to tell you how much it costs to rent one. last round: The pricing terms for a license for the SCO software will not be announced for several weeks.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nAFQMC (Financial)\n\nCHIP-STS (Medical)\n\nChinese-QQP (Common)\n\nTable 13: Generated Samples in Chinese.\n\norigin:可以用自己的支付宝为别人的花呗还款吗 (Can I use my Alipay to repay someone else’s Huabei) first round:花呗的钱可以让其他人代还吗 (Can someone else pay for the money from Huabei) last round:花呗能不能帮别人还 (Can my Huabei help others repay?) origin:花呗怎么还欠费 (How does Huabei pay off the arrears) first round:花呗怎么还有电费 (Why does Huabei still have electricity bills) last round:花呗还款怎么还 (How to pay off Huabei) origin:我的花呗收款二维码 (QR code for my Huabei payment) first round:商家花呗的二维码 (Merchant Huabei’s QR code) last round:花呗收款二维码在哪里 (Where is the QR code for Huabei payment) origin:艾滋病的病因是什么 (What is the cause of AIDS) first round:艾滋病毒是什么? (What is HIV?) last round:艾滋病是什么原因引起的? (What causes AIDS?) origin:高血压总是流口水是怎么回事 (High blood pressure is always drooling what is going on) first round:高血压怎么回事 (What about high blood pressure) last round:高血压为什么会流口水? (Why does high blood pressure cause drooling?) origin:得了糖尿病,现在越来越瘦了怎么回事 (Why am I getting thinner and thinner now that I have diabetes) first round:糖尿病现在怎么回事? (What’s going on with diabetes now?) last round:糖尿病患者为什么会瘦? (Why do people with diabetes lose weight?) origin:如何从此网站删除我的帐户? (How do I delete my account from this site?) first round:怎么删除网站 (How to delete a website) last round:如何才能删除我的帐户? (How can I delete my account?) origin:关于电子产品的一些好书是什么? (What are some good books on electronics?) first round:有什么好的电子产品推荐 (Any good electronics recommendations) last round:有哪些关于电子产品的好书? (What are some good books about electronics?) origin:为什么没有人看到无尽和无限之间的区别? (Why does no one see the difference between endless and infinite?) first round:为什么宇宙中没有极限的存在 (Why is there no limit in the universe) last round:为什么没有人知道无限和有限之间的区别? (Why does no one know the difference between infinite and finite?)\n\n23",
    "reference": "# Summary Of The Paper\n\nThe work introduced a self-consistent learning framework, where a generator and a discriminator are trained cooperatively and alternatively. A selection mechanism is proposed to filter samples in the training process. The sentence semantic matching task is demonstrated as an example to support the effectiveness of the proposed framework. The results show the framework outperforms standalone discriminative models on four datasets in both zero-shot and full-data settings.\n\n# Strength And Weaknesses\n\nStrength: The paper is well-organized, and the main ideas are interesting, and easy to understand. The proposed method is evaluated on multiple datasets with good performance in sentence sematic matching. \n\nWeakness: 1. The work included both positive and negative samples in the training of discriminator, and claimed cooperative setting can lead to better training stability than adversarial setting. The idea makes sense but it is not clear to me how this point is supported either in theoretical or experimental aspects. 2. The definition of the output probability for generator $p_G$ cannot be found. 3. It is mentioned in Eq (1) that $p_D$ depends on the embedding representation H of generated samples. Since it is shown in the paper that the selective mechanism is important to the performance, I think it would be helpful to provide more details on this embedding process, and some evidence or explanation to justify the resulting output probability is somehow “good enough” for selective mechanism. 4. It is mentioned in page 4 that both positive and negative samples would be included in the training of discriminator, (which is a key difference between this method and GAN.) Due to the lack of information on the embedding representation, I fail to see why Eq (2) can make sure of this.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIn my opinion, the work is written in good quality and clear in general. Some questions mentioned above may need some clarifications. Codes were provided in supplementary materials, so it should be reproducible.\n\n# Summary Of The Review\n\nThe paper includes interesting ideas and shows the proposed approach can improve performance for sentence sematic matching task. It may be more influential if fundamental evidences or analysis are available to compare cooperative and adversarial setting.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMINI-BATCH k-MEANS TERMINATES WITHIN O(d/ε) ITERATIONS\n\nGregory Schwartzman Japan Advanced Institute of Science and Technology (JAIST) greg@jaist.ac.jp\n\nABSTRACT\n\nWe answer the question: \"Does local progress (on batches) imply global progress (on the entire dataset) for mini-batch k-means?\". Specifically, we consider minibatch k-means which terminates only when the improvement in the quality of the clustering on the sampled batch is below some threshold.\n\nAlthough at first glance it appears that this algorithm might execute forever, we answer the above question in the affirmative and show that if the batch is of size ̃Ω((d/ε)2), it must terminate within O(d/ε) iterations with high probability, where d is the dimension of the input, and ε is a threshold parameter for termination. This is true regardless of how the centers are initialized. When the algorithm is initialized with the k-means++ initialization scheme, it achieves an approximation ratio of O(log k) (the same as the full-batch version). Finally, we show the applicability of our results to the mini-batch k-means algorithm implemented in the scikit-learn (sklearn) python library.\n\n1\n\nINTRODUCTION\n\nThe mini-batch k-means algorithm (Sculley, 2010) is one of the most popular clustering algorithms used in practice (Pedregosa et al., 2011). However, due to its stochastic nature, it appears that if we do not explicitly bound the number of iterations of the algorithm, then it might never terminate. We show that, when the batch size is sufficiently large, using only an \"early-stopping\" condition, which terminates the algorithm when the local progress observed on a batch is below some threshold, we can guarantee a bound on the number of iterations that the algorithm performs which is independent of input size.\n\nProblem statement We consider the following optimization problem. We are given an input (dataset), X = {xi}n i=1 ⊆ [0, 1]d, of size n of d-dimensional real vectors and a parameter k. Note that the assumption that X ⊆ [0, 1]d is standard in the literature (Arthur et al., 2011), and is meant to simplify notation (otherwise we would have to introduce a new parameter for the diameter of X). Our goal is to find a set C of k centers (vectors in [0, 1]d) such that the following goal function is minimized:\n\n1 n\n\n(cid:88)\n\nx∈X\n\nmin c∈C\n\n∥c − x∥2\n\nUsually, the 1/n factor does not appear as it does not affect the optimization goal, however, in our case, it will be useful to define it as such.\n\nLloyd’s algorithm The most popular method to solve the above problem is Lloyd’s algorithm (often referred to as the k-means algorithm) (Lloyd, 1982). It works by randomly initializing a set of k centers and performing the following two steps: (1) Assign every point in X to the center closest to it. (2) Update every center to be the mean of the points assigned to it. The algorithm terminates when no point is reassigned to a new center. This algorithm is extremely fast in practice but has a worst-case exponential running time (Arthur & Vassilvitskii, 2006; Vattani, 2011).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nMini-batch k-means To update the centers, Lloyd’s algorithm must go over the entire input at every iteration. This can be computationally expensive when the input data is extremely large. To tackle this, the mini-batch k-means method was introduced by Sculley (2010). It is similar to Lloyd’s algorithm except that steps (1) and (2) are performed on a batch of b elements sampled uniformly at random with repetitions, and in step (2) the centers are updated slightly differently. Specifically, every center is updated to be the weighted average of its current value and the mean of the points (in the batch) assigned to it. The parameter by which we weigh these values is called the learning rate, and its value differs between centers and iterations. In the original paper by Sculley, there is no stopping condition similar to that of Lloyd’s algorithm, instead, the algorithm is simply executed for t iterations, where t is an input parameter.\n\nIn practice (for example in sklearn (Pedregosa et al., 2011)), together with an upper bound on the number of iterations to perform there are several \"early stopping\" conditions. We may terminate the algorithm when the change in the locations of the centers is sufficiently small or when the change in the goal function for several consecutive batches does not improve. We note that in both theory (Tang & Monteleoni, 2017; Sculley, 2010) and practice (Pedregosa et al., 2011) the learning rate goes to 0 over time. That is, over time the movement of centers becomes smaller and smaller, which guarantees termination for most reasonable early-stopping conditions at the limit.\n\nOur results are the first to show extremely fast termination guarantees for mini-batch k-means with early stopping conditions. Surprisingly, we need not require the learning rate to go to 0.\n\nRelated work Mini-batch k-means was first introduced by Sculley (2010) as a natural generalization to online k-means (Bottou & Bengio, 1994) (here the batch is of size 1). We are aware only of a single paper that analyzes the convergence rate of mini-batch k-means (Tang & Monteleoni, 2017). It is claimed in (Tang & Monteleoni, 2017) that under mild assumptions the algorithm has O(1/t) convergence rate. That is, after t iterations it holds that the current value of the goal function is within an additive O(1/t) factor from the value of the goal function in some local optimum of Lloyd’s algorithm. However, their asymptotic notation subsumes factors that depend on the size of the input. Taking this into account, we get a convergence rate of Ω(n2/t), which implies, at best, a quadratic bound on the execution time of the algorithm. This is due to setting the learning rate at iteration t to O(1/(n2 + t)). Our results do not guarantee convergence to any local-minima, however, they guarantee an exponentially faster runtime bound.\n\nOur results We analyze the mini-batch k-means algorithm described above (Sculley, 2010), where the algorithm terminates only when the improvement in the quality of the clustering for the sampled batch is less than some threshold parameter ε. That is, we terminate if for some batch the difference in the quality of the clustering before the update and after the update is less than ε. Our stopping condition is slightly different than what is used in practice. In sklearn termination is determined based on the changes in cluster centers. In Section 5 we prove that this condition also fits within our framework.\n\nOur main goal is to answer the following theoretical question: \"Does local progress (on batches) imply global progress (on the entire dataset) for mini-batch k-means, even when the learning rate does not go to 0?\". Intuitively, it is clear that the answer depends on the batch size used by the algorithm. If the batch is the entire dataset the claim is trivial and results in a termination guarantee of O(d/ε) iterations1. We show that when the batch size exceeds a certain threshold, indeed local progress implies global progress and we achieve the same asymptotic bound on the number of iterations as when the batch is the entire dataset. We present several results:\n\nWe start with a warm-up in Section 3, showing that when b = ̃Ω(kd3ε−2) we can guarantee termination within O(d/ε) iterations2 w.h.p (with high probability)3. We require the additional assumption that every real number in the system can be represented using O(1) bits (e.g., 64-bit floats). The above bound holds regardless of how cluster centers are initialized or updated. That is, this bound holds for any center update rule, and not only for the \"standard\" center update rule described above. Our proof uses elementary tools and is presented to set the stage for our main result.\n\n1This holds because the maximum value of the goal function is d (Lemma 2.1). 2Throughout this paper the tilde notation hides logarithmic factors in n, k, d, ε. 3This is usually taken to be 1 − 1/np for some constant p ≥ 1. For our case, it holds that p = 1, however,\n\nthis can be amplified arbitrarily by increasing the batch size by a multiplicative constant factor.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nIn Section 4 we show that using the standard update rule, we can achieve the same termination time with a much smaller batch size. Specifically, a batch size of Ω((d/ε)2 log(nkd/ε)) = ̃Ω((d/ε)2) is sufficient to guarantee termination within O(d/ε) iterations. This holds regardless of how centers are initialized and does not require any assumption on the number of bits required to represent real numbers. Our proof makes use of the fact that the standard update rule adds additional stability to the stochastic process when the learning rate is sufficiently small (but need not go to 0). Finally, in Section 5, we show that our main result also holds for the early stopping condition used in sklearn (with our learning rate). However, this results in a larger batch size and slower termination. Specifically if b = ̃Ω((d/ε)3k) we terminate within O((d/ε)1.5\n\nk) iterations w.h.p.\n\n√\n\nNote that for the batch size to be reasonable, we must require that b ≤ n, which implies that (d/ε)2 log(nkd/ε) = O(n). Thus, our results only hold for a certain range of values for k, d, ε. This is reasonable, as in practice it is often the case that ε = O(1), d ≪ n and the dependence on the rest of the parameters is logarithmic.\n\nSolution quality Applying the k-means++ initialization scheme to our results we achieve the same approximation ratio, O(log k) in expectation, as the full-batch algorithm. The approximation guarantee of k-means++ is guaranteed already in the initialization phase (Theorem 3.1 in Arthur & Vassilvitskii (2007)), and the execution of Lloyd’s algorithm following initialization can only improve the solution. We show that w.h.p the global goal function is decreasing throughout our execution which implies that the approximation guarantee remains the same.\n\n2 PRELIMINARIES\n\nThroughout this paper we work with ordered tuples rather than sets, denoted as Y = (yi)i∈[l], where [l] = {1, . . . , l}. To reference the i-th element we either write yi or Y [i]. It will be useful to use set notations for tuples such as x ∈ Y ⇐⇒ ∃i ∈ [l], x = yi and Y ⊆ Z ⇐⇒ ∀i ∈ [l], yi ∈ Z. When summing we often write (cid:80)\n\nx∈Y g(x) which is equivalent to (cid:80)l\n\ni=1 g(Y [i]).\n\nWe borrow the following notation from (Kanungo et al., 2004). For every x, y ∈ Rd let ∆(x, y) = ∥x − y∥2. For every finite tuple S ⊆ Rd and a vector x ∈ Rd let ∆(S, x) = (cid:80)\n\ny∈S ∆(y, x).\n\nk-means We are given an input X = (xi)n tuple C ⊆ Rd of k centers such that the following goal function is minimized:\n\ni=1 ⊆ [0, 1]d and a parameter k. Our goal is to find a\n\n1 n\n\n(cid:88)\n\nx∈X\n\nmin C∈C\n\n∆(x, C)\n\nLet us define for every x ∈ X the function fx : Rk·d → R where fx(C) = minC∈C ∆(x, C). We can treat Rk·d as the set of k-tuples of d-dimensional vectors. We also define the following function for every tuple A = (ai)l\n\ni=1 ⊆ X:\n\nfA(C) =\n\n1 l\n\nl (cid:88)\n\ni=1\n\nfai(C)\n\nNote that fX is our original goal function. We state the following useful lemma: Lemma 2.1. For any tuple of k centers C ⊂ [0, 1]d it holds that fX (C) ≤ d.\n\nProof. Because X, C ⊂ [0, 1]d it holds that ∀x ∈ X, fx(C) ≤ maxC∈C ∆(x, C) ≤ d. Therefore fX (C) = 1\n\nx∈X fx(C) ≤ 1\n\nn · nd = d.\n\n(cid:80)\n\nn\n\nWe state the following well known theorems: Theorem 2.2 (Hoeffding (1963)). Let Y1, ..., Ym be independent random variables such that ∀1 ≤ i ≤ m, E[Yi] = μ and Yi ∈ [amin, amax]. Then\n\nP r\n\n(cid:32)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 m\n\nm (cid:88)\n\ni=1\n\n(cid:12) (cid:12) (cid:12) Yk − μ (cid:12) (cid:12)\n\n(cid:33)\n\n≥ δ\n\n3\n\n≤ 2e−2mδ2/(amax−amin)2\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 2.3 (Jensen (1906)). Let φ be a convex function, y1, . . . , yn numbers in its domain and weights a1, . . . , an ∈ R+. It holds that: (cid:18) (cid:80)n (cid:80)n\n\n(cid:80)n\n\n≤\n\n(cid:19)\n\nφ\n\ni=1 aiyi i=1 ai\n\ni=1 aiφ(yi) (cid:80)n i=1 ai\n\n3 WARM-UP: A SIMPLE BOUND\n\nLet us first show a simple convergence guarantee which makes no assumptions about how the centers are updated. This will set the stage for our main result in Section 4, where we consider the standard update rule used in mini-batch k-means (Sculley, 2010; Pedregosa et al., 2011).\n\nAlgorithm We analyze a generic variant of the mini-batch k-means algorithm, presented in Algorithm 1. Note that it a very broad class of algorithms (including the widely used algorithm of Sculley (2010)). The only assumptions we make are:\n\n1. The centers remain within [0, 1]d (the convex hull bounding X).\n\n2. Batches are sampled uniformly at random from X with repetitions.\n\n3. The algorithm terminates when updating the centers does not significantly improve the\n\nquality of the solution for the sampled batch.\n\nItems (1) and (2) are standard both in theory and practice (Sculley, 2010; Pedregosa et al., 2011; Tang & Monteleoni, 2017). Item (3) is usually referred to as an \"early-stopping\" condition. Early stopping conditions are widely used in practice (for example in sklearn (Pedregosa et al., 2011)), together with a bound on the number of iterations. However, our early-stopping condition is slightly different than the one used in practice. We discuss this difference in Section 5.\n\nAt first glance, guaranteeing termination for any possible way of updating the centers might seem strange. However, if the update procedure is degenerate, it will make no progress, at which point the algorithm terminates.\n\nAlgorithm 1: Generic mini-batch k-means\n\n1 C1 ⊆ [0, 1]d is an initial tuple of centers 2 for i = 1 to ∞ do\n\n3\n\n4\n\n5\n\nSample b elements, Bi = (y1, . . . , yb), uniformly at random from X (with repetitions) Update Ci+1 (such that Ci+1 ⊆ [0, 1]d) if fBi(Ci) − fBi(Ci+1) < ε then Return Ci+1\n\nTermination guarantees for Algorithm 1 To bound the number of iterations of such a generic algorithm we require the following assumption: every real number in our system can be represented using q = O(1) bits. This implies that every set of k centers can be represented using qkd bits. This means that the total number of possible solutions is bounded by 2qkd. This will allow us to show that when the batch is sufficiently large, the sampled batch acts as a sparsifier for the entire dataset. Specifically, it means that for any tuple of k centers, C, it holds that |fBi(C) − fX (C)| < ε/4. This implies that, for a sufficiently large batch size, simply sampling a single batch and executing Lloyd’s algorithm on the batch will be sufficient, and executing mini-batch k-means is unnecessary. Nevertheless, this serves as a good starting point to showcase our general approach and to highlight the challenges we overcome in Section 4 in order to reduce the required batch size without compromising the running time.\n\nLet us assume that the algorithm executes for at least t iterations. That is, the termination condition does not hold for the first t iterations. Our goal is to upper bound t.\n\nParameter range Let us first define the range of parameter values for which the results for this section hold. Recall that n is the size of the input, k is the number of centers, d is the dimension, ε is\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nthe termination threshold. For the rest of this section assume that b = Ω((d/ε)2(kd+log(nt))). Later we show that t = O(d/ε), which will imply that b = ̃Ω(kd3ε−2) is sufficient for our termination guarantees to hold.\n\nWe state the following useful lemma which guarantees that fB(C) is not too far from fX (C) when the batch size is sufficiently large and C is fixed (i.e., independent of the choice of Bi). Lemma 3.1. Let B be a tuple of b elements chosen uniformly at random from X with repetitions. For any fixed tuple of k centers, C ⊆ [0, 1]d, it holds that: P r[|fB(C) − fX (C)| ≥ δ] ≤ 2e−2bδ2/d2 .\n\nProof. Let us write B = (y1, . . . , yb), where yi is a random element selected uniformly at random from X with repetitions. For every such yi define the random variable Zi = fyi(C). These new random variables are IID for any fixed C. It also holds that ∀i ∈ [b], E[Zi] = 1 x∈X fx(C) = fX (C) and that fB(C) = 1\n\nx∈B fx(C) = 1\n\ni=1 Zi.\n\n(cid:80)b\n\n(cid:80)\n\n(cid:80)\n\nn\n\nb\n\nb\n\nApplying a Hoeffding bound (Theorem 2.2) with parameters m = b, μ = fX (C), amax − amin ≤ d we get that: P r[|fB(C) − fX (C)| ≥ δ] ≤ 2e−2bδ2/d2\n\n.\n\nUsing the above we can show that every Bi is a sparsifier for X. Lemma 3.2. It holds w.h.p that for every i ∈ [t] and for every set of k centers, C ⊂ [0, 1]d, that |fBi(C) − fX (C)| < ε/4.\n\nProof. Using Lemma 3.1, setting δ = ε/4 and using the fact that b = Ω((d/ε)2(kd + log(nt))), we get: P r[|fB(C) − fX (C)| ≥ δ] ≤ 2e−2bδ2/d2 Taking a union bound over all t iterations and all 2qkd configurations of centers, we get that the probability is bounded by 2−Ω(kd+log(nt)) · 2qkd · t = O(1/n), for an appropriate constant in the asymptotic notation for b.\n\n= 2−Θ(bδ2/d2) = 2−Ω(kd+log(nt)).\n\nThe lemma below guarantees global progress for the algorithm. Lemma 3.3. It holds w.h.p that ∀i ∈ [t], fX (Ci) − fX (Ci+1) ≥ ε/2.\n\nProof. Let us write (the notation ±x means that we add and subtract x):\n\nfX (Ci) − fX (Ci+1) = fX (Ci) ± fBi(Ci) ± fBi(Ci+1) − fX (Ci+1) ≥ ε/2 Due to Lemma 3.2 it holds that w.h.p fX (Ci) − fBi(Ci) > −ε/4 and fBi(Ci+1) − fX (Ci+1) > −ε/4. Finally due to the termination condition it holds that fBi (Ci) − fBi(Ci+1) ≥ ε. This completes the proof.\n\nAs fX is upper bounded by d, it holds that we must terminate within O(d/ε) iterations w.h.p when b = Ω(kd3ε−2 log(nd/ε)). We state our main theorem for this Section. Theorem 3.4. For b = ̃Ω(kd3ε−2), Algorithm 1 terminates within O(d/ε) iterations w.h.p.\n\nTowards a smaller batch size Note that the batch size used in this section is about a kd factor larger than what we require in Section 4. This factor is required for the union bound over all possible sets of k centers in Lemma 3.2. However, when actually applying Lemma 3.2, we only apply it for two centers in iteration i, setting B = Bi and C = Ci, Ci+1. A more direct approach would be to apply Lemma 3.1 only for Ci, Ci+1, which would get rid of the extra kd factor. This will work when C = Ci as Bi is sampled after Ci is determined, but will fail for C = Ci+1 because Ci+1 may depend on Bi. In the following section, we show how to use the fact that the learning rate is sufficiently small in order to overcome this challenge.\n\n4 MAIN RESULTS\n\nIn this section, we show that we can get a much better dependence on the batch size when using the standard center update rule. Specifically, we show that a batch of size ̃Ω((d/ε)2) is sufficient to guarantee termination within O(d/ε) iterations. We also do not require any assumption about the number of bits required to represent a real number.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n|S|\n\n(cid:80)\n\nSection preliminaries Let us define for any finite tuple S ⊂ Rd the center of mass of the tuple x∈S x. For any tuple S ⊂ Rd and some tuple of cluster centers C = (Cl)l∈[k] it as cm(S) = 1 implies a partition (Sl)l∈[k] of the points in S. Specifically, every Sl contains the points in S closest to Cl and every point in S belongs to a single Cl (ties are broken arbitrarily). We state the following useful observation: Observation 4.1. Fix some A ⊆ X. Let C be a tuple of k centers, S = (Sl)l∈[k] be the partition j=1 ∆(Sj, Cj) ≤ of A induced by C and S = (S (cid:80)k\n\n)l∈[k] be any other partition of A. It holds that (cid:80)k\n\nl\n\nj\n\nj=1 ∆(S\n\n, Cj).\n\ni denote the location of the j-th center in the beginning of the i-th iteration. Let (Bl\n\ni )l∈[k] be\n\nLet Cj the partition of Bi induced by Ci and let (X l\n\ni )l∈[k] be the partition of X induced by Ci.\n\ni is the learning rate. Note that Bj\n\ni + αj We analyze Algorithm 1 when clusters are updated as follows: Cj i ), where αj i ) is undefined, however, the learning rate is chosen such that αj i ). Note that the learning rate may take on different values for different centers, and may change between iterations. In the standard mini-batch k-means algorithm (Sculley, 2010; Pedregosa et al., 2011) the learning rate goes to 0 over time. This guarantees termination for most reasonable stopping conditions.\n\ni )Cj i may be empty in which case cm(Bj\n\ni = 0 in this case (Cj\n\ni+1 = (1 − αj\n\ni+1 = Cj\n\ni cm(Bj\n\nAs before, we assume that the algorithm executes for at least t iterations and upper bound t. We show that the learning rate need not go to 0 to guarantee termination when the batch size is sufficiently large. (cid:12) Specifically, we set αj (cid:12) (cid:12), and we require that b = Ω((d/ε)2 log(ndtk)).\n\ni /b, where bj bj\n\n(cid:12)Bj\n\ni =\n\ni =\n\n(cid:113)\n\n(cid:12) (cid:12)\n\ni\n\nProof outline In our proof, we use the fact that a sufficiently small learning rate enhances the stability of the algorithm, which in turn allows us to use a much smaller batch size compared to Section 3. Let us define the auxiliary value C i ). This is the j-th center at step i + 1 if we were to use the entire dataset for the update, rather than just a batch. Note that this is only used in the analysis and not in the algorithm.\n\ni+1 = (1 − αj\n\ni cm(X j\n\ni + αj\n\ni )Cj\n\nj\n\nRecall that in the previous section we required a large batch size because we could not apply Lemma 3.1 when B = Bi and C = Ci+1 because Ci+1 may depend on Bi. To overcome this challenge we use Ci+1 instead of Ci+1. Note that Ci+1 only depends on Ci, X and is independent of Bi (i.e., we can fix its value before sampling Bi). We show that for our choice of learning rate it holds that Ci+1, Ci+1 are sufficiently close, which implies that fX (Ci+1), fX (Ci+1) and fBi(Ci+1), fBi(Ci+1) are also sufficiently close. This allows us to use a similar proof to that of Lemma 3.3 where Ci+1 acts as a proxy for Ci+1. We formalize this intuition in what follows.\n\nFirst, we state the following useful lemmas: Lemma 4.2 (Kanungo et al. (2004)). For any set S ⊆ Rd and any C ∈ Rd it holds that ∆(S, C) = ∆(S, cm(S)) + |S| ∆(C, cm(S)). Lemma 4.3. For any S ⊆ X and C, C ′ ∈ [0, 1]d, it holds that: 2\n\n|∆(S, C ′) − ∆(S, C)| ≤\n\nd |S| ∥C − C ′∥.\n\n√\n\nProof. Using Lemma 4.2 we get that ∆(S, C) = ∆(S, cm(S)) + |S| ∆(cm(S), C) and that ∆(S, C ′) = ∆(S, cm(S)) + |S| ∆(cm(S), C ′). Thus, it holds that |∆(S, C ′) − ∆(S, C)| = |S|·|∆(cm(S), C ′) − ∆(cm(S), C)|. Observe that for two vectors x, y ∈ Rd it holds that ∆(x, y) = (x − y) · (x − y). Let us switch to vector notation and bound |∆(cm(S), C ′) − ∆(cm(S), C)|.\n\n|∆(cm(S), C ′) − ∆(cm(S), C)| = |(cm(S) − C ′) · (cm(S) − C ′) − (cm(S) − C) · (cm(S) − C)| = |−2cm(S) · C ′ + C ′ · C ′ + 2cm(S) · C − C · C| = |2cm(S) · (C − C ′) + (C ′ − C) · (C ′ + C)| = |(C − C ′) · (2cm(S) − (C ′ + C))|\n\n≤ ∥C − C ′∥∥2cm(S) − (C ′ + C)∥ ≤ 2\n\n6\n\n√\n\nd∥C − C ′∥\n\nPublished as a conference paper at ICLR 2023\n\nWhere in the last transition we used the Cauchy-Schwartz inequality.\n\nFirst, we show that due to our choice of learning rate Cj\n\ni+1, C\n\nj\n\ni+1 are sufficiently close.\n\nLemma 4.4. For it holds w.h.p that ∀i ∈ [t], j ∈ [k], ∥Cj\n\ni+1 − C\n\nj\n\ni+1∥ ≤ ε\n\n√\n\n10\n\n.\n\nd\n\nj\n\ni+1 − C\n\ni+1 = αj\n\nProof. Note that Cj i )). Let us fix some iteration i and center j. To simplify notation, let us denote: X ′ = X j i . Although b′ is a random variable, in what follows we treat it as a fixed value (essentially conditioning on its value). As what follows holds for all values of b′ it also holds without conditioning due to the law of total probabilities.\n\ni ) − cm(X j\n\ni , B′ = Bj\n\ni , α′ = αj\n\ni , b′ = bj\n\ni (cm(Bj\n\nFor the rest of the proof, we assume b′ > 0 (if b′ = 0 the claim holds trivially). Let us denote by {Yl}b′ l=1 the sampled points in B′. Note that a randomly sampled element from X is in B′ if and only if it is in X ′. As batch elements are sampled uniformly at random with repetitions from X, conditioning on the fact that an element is in B′ means that it is distributed uniformly over X ′. Thus, it holds that ∀l, E[Yl] = 1 l=1 E[Yl] = cm(X ′). Our goal is to bound P r[∥cm(B′) − cm(X ′)∥ ≥ ], we note that it is sufficient to bound the deviation of every coordinate by ε/(10α′d), as that will guarantee that:\n\nx∈X ′ x = cm(X ′) and E[cm(B′)] = 1\n\nε 10α′\n\n(cid:80)b′\n\n|X ′|\n\n(cid:80)\n\n√\n\nb′\n\nd\n\n∥cm(B′) − cm(X ′)∥ =\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nd (cid:88)\n\nl=1\n\n(cm(B′)[l] − cm(X ′)[l])2 ≤\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nd (cid:88)\n\nl=1\n\n(\n\nε 10α′d\n\n)2 =\n\nε\n\n10α′\n\n√\n\nd\n\nWe note that for a single coordinate, l, we can apply a Hoeffding bound with parameters μ = cm(X ′)[l], amax − amin ≤ 1 and get that:\n\nP r[|cm(B′)[l] − cm(X ′)[l]| ≥\n\nε 10α′d\n\n] ≤ 2 · e− 2b′ ε2\n\n100(α′)2d2\n\nTaking a union bound we get that\n\nP r[∥cm(B′) − cm(X ′)∥ ≥\n\nε\n\n10α′\n\n√\n\n]\n\nd\n\n≤ P r[∃l, |cm(B′)[l] − cm(X ′)[l]| ≥\n\nε 10α′d\n\n] ≤ 2d · e− 2b′ ε2\n\n100(α′ )2d2\n\nUsing the fact that α′ = (cid:112)b′/b together with the fact that b = Ω((d/ε)2 log(ntkd)) (for an appropriate constant) we get that the above is O(1/ntk). Finally, taking a union bound over all t iterations and all k centers per iteration completes the proof.\n\nLet us now use the above lemma to bound the goal function when cluster centers are close. Lemma 4.5. Fix some A ⊆ X. It holds w.h.p that ∀i ∈ [t], (cid:12) (cid:12) ≤ ε/5\n\n(cid:12)fA(Ci+1) − fA(Ci+1)(cid:12)\n\nProof. Let S = (Sl)l∈[k], S = (S expand the expression:\n\nl\n\n)l∈[k] be the partitions induced by Ci+1, Ci+1 on A. Let us\n\nfA(Ci+1) − fA(Ci+1) =\n\n1 |A|\n\nk (cid:88)\n\nj=1\n\n∆(S\n\nj\n\n, C\n\nj\n\ni+1) − ∆(Sj, Cj\n\ni+1)\n\n≤\n\n≤\n\n1 |A|\n\n1 |A|\n\nk (cid:88)\n\nj=1\n\nk (cid:88)\n\nj=1\n\n∆(Sj, C\n\nj\n\ni+1) − ∆(Sj, Cj\n\ni+1)\n\n√\n\n2\n\nd (cid:12)\n\n(cid:12)Sj(cid:12)\n\n(cid:12) ∥C\n\nj\n\ni+1 − Cj\n\ni+1∥ ≤\n\n1 |A|\n\nk (cid:88)\n\nj=1\n\n(cid:12)Sj(cid:12) (cid:12)\n\n(cid:12) ε/5 = ε/5\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nWhere the first inequality is due to Observation 4.1, the second is due Lemma 4.3 and finally we use Lemma 4.4 together with the fact that (cid:80)k (cid:12) = |A|. Using the same argument we also get that fA(Ci+1) − fA(Ci+1) ≤ ε/5, which completes the proof.\n\n(cid:12)Sj(cid:12) (cid:12)\n\nj=1\n\nFrom here our proof is somewhat similar to that of Section 3. Let us state the following useful lemma.\n\nLemma 4.6. It holds w.h.p that for every i ∈ [t] :\n\nfX (Ci+1) − fX (Ci+1) ≥ −ε/5\n\nfBi(Ci+1) − fBi (Ci+1) ≥ −ε/5 fX (Ci) − fBi(Ci) ≥ −ε/5 fBi(Ci+1) − fX (Ci+1) ≥ −ε/5\n\n(1)\n\n(2)\n\n(3)\n\n(4)\n\nProof. The first two inequalities follow from Lemma 4.5. The last two are due to Lemma 3.1 by setting δ = ε/5, B = Bi:\n\nP r[|fBi(C) − fX (C)| ≥ δ] ≤ 2e−2bδ2/d2\n\n= e−Θ(bε2/d2) = e−Ω(log(nt)) = O(1/nt)\n\nWhere the last inequality is due to the fact that b = Ω((d/ε)2 log(nt)) (for an appropriate constant). The above holds for either C = Ci or C = Ci+1. Taking a union bound over all t iterations we get the desired result.\n\nPutting everything together We wish to lower bound fX (Ci) − fX (Ci+1). We write the following:\n\nfX (Ci) − fX (Ci+1) = fX (Ci) ± fBi(Ci) − fX (Ci+1) ≥ fBi(Ci) − fX (Ci+1) − ε/5 = fBi (Ci) ± fBi (Ci+1) − fX (Ci+1) − ε/5 ≥ fBi(Ci+1) − fX (Ci+1) + 4ε/5 = fBi(Ci+1) ± fBi (Ci+1) ± fX (Ci+1) − fX (Ci+1) + 4ε/5 ≥ ε/5\n\nWhere the first inequality is due to inequality (3) in Lemma 4.6 (fX (Ci) − fBi(Ci) ≥ −ε/5), the second is due to the stopping condition of the algorithm (fBi(Ci) − fBi (Ci+1) > ε), and the last is due to the remaining inequalities in Lemma 4.6. The above holds w.h.p over all of the iterations of the algorithms.\n\nAs in Section 3, we conclude that w.h.p it holds that t = O(d/ε), which implies that b = Ω((d/ε)2 log(knd/ε)) is sufficient. We state our main theorem.\n\nTheorem 4.7. For b = ̃Ω((d/ε)2) and αj (1 − αj\n\ni = i ), terminates within O(d/ε) iterations w.h.p.\n\ni cm(Bj\n\ni + αj\n\ni )Cj\n\ni /b, Algorithm 1 with center update Cj bj\n\ni+1 =\n\n(cid:113)\n\n5 APPLICATION TO SKLEARN\n\nIn this section, we show the relevance of our results to the algorithm implementation of sklearn. The main differences in sklearn are the learning rate and stopping condition. The termination condition4 depends on the movement of the centers in the iteration, rather than the value of fBi. Specifically, we continue as long as (cid:80) i ) ≥ ε for some tolerance parameter ε. The learning rate is set as αj l=1 bj of the algorithm in the limit.\n\ni+1, Cj . Roughly speaking, this implies that αj\n\ni → 0 over time, and guarantees termination\n\nj∈[k] ∆(Cj\n\ni = bj\n\n(cid:80)i\n\nl\n\ni\n\nHowever, for our convergence guarantee, we only require αj bj i /b which need not go to 0 over time. We show that with our learning rate and the termination condition of sklearn, the proof from Section 4 still implies termination, although at a slower rate and requires a larger batch size.\n\ni =\n\n(cid:113)\n\n4The exact parameters of this algorithm were extracted directly from the code (the relevant function is _mini_batch_convergence): https://github.com/scikit-learn/scikit-learn/blob/ baf828ca1/sklearn/cluster/_kmeans.py#L1502.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nk) iterations w.h.p if the batch size is ̃Ω(k(d/ε)3). Specifically, we terminate within O((d/ε)1.5 Note that this result is not subsumed by the result in Section 3 because the stopping condition is different.\n\n√\n\nj∈[k] ∆(Cj\n\nBelow we show that as long as the termination condition in sklearn does not hold ((cid:80) i ) ≥ ε), our stopping condition also does not hold for an appropriate parameter ( fBi(Ci) − fBi(Ci+1) > ε′ where ε′ = ε1.5/ Lemma 5.1. Let x, y ∈ Rd, α ∈ [0, 1]. It holds that ∆(x, (1 − α)x + αy) = α2∆(x, y).\n\nkd). We state the following useful lemma:\n\ni+1, Cj\n\n√\n\nProof. ∆(x, (1 − α)x + αy) = ∥x − (1 − α)x + αy∥2 = ∥αx − αy∥2 = α2∆(x, y).\n\nBelow is our main lemma for this section: Lemma 5.2. If it holds that (cid:80) j∈[k] ∆(Cj\n\ni+1, Cj\n\ni ) > ε then fBi (Ci) − fBi(Ci+1) > ε1.5\n\n√\n\n.\n\nkd\n\nProof. Recall that Cj\n\ni+1 = (1 − αj\n\ni )Cj\n\ni + αj\n\ni cm(Bj\n\ni ) for αj\n\ni =\n\n(cid:113)\n\nbj i /b. Thus, we get:\n\nε <\n\n(cid:88)\n\nj∈[k]\n\n∆(Cj\n\ni , Cj\n\ni+1) ≤\n\n(cid:88)\n\n(αj\n\ni )2∆(Cj\n\ni , cm(Bj\n\ni )) =\n\nj∈[k]\n\nbj i\nb\n\n(cid:88)\n\nj∈[k]\n\n∆(Cj\n\ni , cm(Bj\n\ni ))\n\n(5)\n\nWhere in the transitions we used Lemma 5.1. Let us fix some j ∈ [k], we can write the following:\n\ni ) − ∆(Bj i , cm(Bj\n\ni , Cj i )) + bj\n\ni+1)\n\n∆(Bj i , Cj = ∆(Bj = bj = bj = bj = (2αj\n\ni (∆(Cj i (∆(Cj i (∆(Cj\n\ni − (αj\n\ni ∆(Cj\n\ni )) − bj\n\ni , cm(Bj\n\ni+1, cm(Bj\n\ni , cm(Bj i+1, cm(Bj\n\ni ∆(Cj i )) − ∆(Cj i )) − ∆((1 − αj i )) − (1 − αj i , cm(Bj i ∆(Cj\n\ni , cm(Bj i , cm(Bj i , cm(Bj i )2)bj transition we apply Lemma 4.2, and in the last we use the fact\n\ni cm(Bj i ))) i , cm(Bj\n\ni )Cj i )2∆(Cj i )) ≥ αj\n\ni ), cm(Bj\n\ni )) − ∆(Bj i ))) i + αj i , cm(Bj i ∆(Cj i bj\n\ni )) =\n\ni )))\n\ni ))\n\ni , cm(Bj\n\ni )) and the fact that ∀, αj\n\ni ∈ [0, 1], 2αj\n\ni − (αj\n\nthat i )2 ≥ αj i .\n\nWhere in the first ∆(Cj i+1, cm(Bj Let us bound fBi (Ci) − fBi(Ci+1):\n\ni )) = (αj\n\ni )2∆(Cj\n\nfBi(Ci) − fBi(Ci+1) ≥\n\n1 b\n\nk (cid:88)\n\nj=1\n\n(∆(Bj\n\ni , Cj\n\ni ) − ∆(Bj\n\ni , Cj\n\ni+1))\n\n≥\n\nk (cid:88)\n\nj=1\n\nαj i bj b\n\ni\n\n∆(Cj\n\ni , cm(Bj\n\ni )) =\n\n(cid:33)1.5\n\n(cid:32)\n\nk (cid:88)\n\nj=1\n\nbj i\nb\n\n∆(Cj\n\ni , cm(Bj\n\ni ))\n\ni ) − ∆(Bj\n\nWhere the first inequality is due to Observation 4.1, the second is due to the fact that ∀j ∈ i , Cj [k], ∆(Bj i , cm(Bj i )), and in the last equality we simply plug i = bj in αj b combined with. We complete the proof by applying Jensen’s inequality, with parameters: φ(x) = x1.5, yj = bj\n\ni+1) ≥ αj\n\ni ∆(Cj\n\ni , Cj\n\ni bj\n\ni , cm(Bj\n\ni\n\ni /b and aj = ∆(Cj \n\ni )), combined with inequality (5). \n\n\n\n∆(Cj\n\ni , cm(Bj\n\ni )) ≥\n\n\n\n∆(Cj\n\ni , cm(Bj\n\ni ))\n\n ·\n\n\n\n(cid:33)1.5\n\n(cid:32)\n\nk (cid:88)\n\nj=1\n\nbj i\nb\n\n≥\n\n(cid:113)(cid:80)k\n\nε1.5 j=1 ∆(Cj\n\ni , cm(Bj\n\ni ))\n\nk (cid:88)\n\nj=1\n\n≥\n\nε1.5 √\n\nkd\n\nbj\n\n(cid:80)k\n\nj=1\n\ni\n\nb ∆(Cj j=1 ∆(Cj\n\ni , cm(Bj i )) i , cm(Bj i ))\n\n(cid:80)k\n\n\n\n1.5\n\n\n\nFinally, plugging ε′ = ε1.5 iterations is bounded by O((d/ε)1.5\n\nkd\n\n√\n\n√\n\ninto our bounds, we conclude that if b = ̃Ω(ε−3d3k) then the number of\n\nk) w.h.p.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe author would like to thank Ami Paz, Uri Meir and Giovanni Viglietta for reading preliminary versions of this work.\n\nThis work was supported by JSPS KAKENHI Grant Numbers JP21H05850, JP21K17703, JP21KK0204.\n\nREFERENCES\n\nDavid Arthur and Sergei Vassilvitskii. How slow is the k-means method? In SCG, pp. 144–153.\n\nACM, 2006.\n\nDavid Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In SODA, pp.\n\n1027–1035. SIAM, 2007.\n\nDavid Arthur, Bodo Manthey, and Heiko Röglin. Smoothed analysis of the k-means method. J. ACM,\n\n58(5):19:1–19:31, 2011.\n\nLéon Bottou and Yoshua Bengio. Convergence properties of the k-means algorithms. In NIPS, pp.\n\n585–592. MIT Press, 1994.\n\nWassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the\n\nAmerican Statistical Association, 58(301):13–30, 1963.\n\nJohan Ludwig William Valdemar Jensen. Sur les fonctions convexes et les inégalités entre les valeurs\n\nmoyennes. Acta mathematica, 30(1):175–193, 1906.\n\nTapas Kanungo, David M. Mount, Nathan S. Netanyahu, Christine D. Piatko, Ruth Silverman, and Angela Y. Wu. A local search approximation algorithm for k-means clustering. Comput. Geom., 28(2-3):89–112, 2004.\n\nStuart P. Lloyd. Least squares quantization in PCM. IEEE Trans. Inf. Theory, 28(2):129–136, 1982.\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\n\nD. Sculley. Web-scale k-means clustering. In WWW, pp. 1177–1178. ACM, 2010.\n\nCheng Tang and Claire Monteleoni. Convergence rate of stochastic k-means. In AISTATS, volume 54\n\nof Proceedings of Machine Learning Research, pp. 1495–1503. PMLR, 2017.\n\nAndrea Vattani. k-means requires exponentially many iterations even in the plane. Discret. Comput.\n\nGeom., 45(4):596–616, 2011.\n\n10",
    "reference": "# Summary Of The Paper\n\nThe paper analyzes the convergence rate of mini-batch k-means, namely, running Lloyd's iteration with a uniform sample of points from the data set, rather than using the entire set in each iteration. It gives strong results: with a sample size nearly quadratic in the dimension, the number of steps needed is linear in the dimension (and independent of the size of the data set). This requires a stopping condition that deviates from practice, and somewhat weaker bounds are shown for the conventional stopping condition.\n\n# Strength And Weaknesses\n\nThis is an appealing result, the best in my small pile, and should definitely be accepted.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written and easy to follow. It uses standard notation.\n\n# Summary Of The Review\n\nThe convergence bound is strong, and the paper actually indicates a modification in the standard implementation that could result is superior performance in practice.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nONLINE CONTINUAL LEARNING WITH FEEDFORWARD ADAPTATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecently deep learning has been widely used in time-series prediction tasks. Although a trained deep neural network model typically performs well on the training set, performance drop significantly in a test set under slight distribution shifts. This challenge motivates the adoption of online test-time adaptation algorithms to update the prediction models in real time to improve the prediction performance. Existing online adaptation methods optimize the prediction model by feeding back the latest prediction error computed with respect to the latest observation. However, the feedback based approach is prone to forgetting past information. In this work, we propose an online adaptation method with feedforward compensation, which uses critical data samples from a memory buffer, instead of the latest samples, to optimize the prediction model. We prove that the proposed approach has a smaller error bound than previously used approaches in slow time-varying systems. The experiments on several time-series prediction tasks show that the proposed feedforward adaptation outperforms previous adaptation methods by 12%. In addition, the proposed feedforward adaptation method is able to estimate an uncertainty bound of the prediction that is agnostic from specific optimizers, while existing feedback adaptation could not.\n\n1\n\nINTRODUCTION\n\nTime-series prediction (or forecasting) has been widely studied in many fields, including control, energy management, and financial investment Box et al. (2015); Brockwell & Davis (2002). Among the research applications, acquiring future trends and tendencies of the time-series data is one of the most important subjects. With the emergence of deep learning, many deep neural network models have been proposed to solve this problem Lim & Zohren (2021), e.g., Recurrent Neural Networks Lai et al. (2018) and Temporal Convolutional Networks Bai et al. (2018). Inspired by the great success of Transformer in the NLP and CV community Vaswani et al. (2017); Dosovitskiy et al. (2020), Transformer-style methods have been introduced to capture long-term dependencies in time series prediction tasks Zhou et al. (2021). Benefiting from the self-attention mechanism, Transformers obtain a great advantage in modeling long-term dependencies for sequential data Brown et al. (2020). Although a trained Transformer model (or other big deep neural network models) typically performs well on the training set, performance can significantly drop in a slightly different test domain or under a slightly different data distribution Popel & Bojar (2018); Si et al. (2019).\n\nIn practical time-series prediction problems, there are often significant distributional discrepancies between the offline training set and the real-time testing set. These differences may be attributed to multiple factors. In some cases, it is too expensive to collect large unbiased training datasets, e.g., for weather prediction or medical time-series prediction. In other cases, it may be difficult to obtain the training instances from a specific domain. For example, in human-robot collaboration, it is hard to collect data from all potential future users. In these cases, adaptation techniques are applied to deal with the distribution mismatch between offline training and real-time testing Blum (1998). Besides, some tasks require the system to adapt itself after every observation. For example, in human-robot collaboration, the robot needs to continually adapt its behaviors to different users. In these scenarios, online adaptation techniques are often embraced Abuduweili et al. (2019).\n\nOnline adaptation is a special case of online continual learning, which continually learns from real-time streaming data. In online adaptation, a prediction model receives sequential observations,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nand then an online optimization algorithm (e.g. SGD) updates the prediction model according to the prediction loss measured by the observed data. The goal of online adaptation is to improve prediction accuracy in subsequent rounds. Online adaptation is currently applied to many kinds of research like time-series prediction Pastor et al. (2011); Abuduweili & Liu (2020), image recognition Lee & Kriegman (2005); Chen et al. (2022), and machine translation Mart ́ınez-G ́omez et al. (2012). In this paper, we mainly focus on time-series prediction tasks, but the proposed methods also can be used for other online adaptation (or online learning) tasks.\n\nMost existing online adaptation approaches are based on feedback compensation Tonioni et al. In feedback adaptation, a prediction model only utilizes (2019), analogous to feedback control. the latest received data. After observing a new sample, the online optimization algorithm updates the prediction model according to the prediction loss measured between the last prediction and the latest ground truth. However, this kind of passive feedback compensation is not efficient.\n\nIn this work, we propose feedforward compensation in online adaptation to maximize information extraction from existing data, especially those that are more critical. A critical sample is more helpful to reduce the objective (loss) of the model when the sample is selected for training. In the proposed feedforward adaptation, we will not only have forgetting as is done in conventional online adaptation Paleologu et al. (2008), but also enable recalling to compensate for potential shortsighted behaviors due to forgetting. There is a balance between forgetting and recalling. On the one hand, to rapidly learn the new function value in a time-varying system, we need to forget some of the old data. On the other hand, too much forgetting may cause unstable and incorrect predictions when we encounter a similar pattern with historical data. To achieve the balance between forgetting and recalling, we design a novel mechanism for feedforward compensation using a memory buffer similar to the functionality of the Hippocampus in the human brain Barron (2021). We will maintain the memory buffer by storing recent L-steps observations (or hidden features) of samples. When the prediction model experiences similar observations, it will pull the corresponding data (critical sample) from the memory buffer to enhance learning. For example, in human behavior prediction tasks, a human subject may exhibit similar behavior patterns on different days. These would be extremely difficult to discover if we only learn from the most recent data like conventional online adaptation but can be identified using the feedforward adaptation methods with memory buffer. We can also use the related information between the current sample and critical samples to measure the uncertainty bound to the current prediction. Our main contributions can be summarized in the following points. • By summarizing feedforward and feedback adaptation methods, we provide a general online\n\ntest-time adaptation framework and prove its error bound.\n\n• We propose a feedforward compensation for online test-time adaptation problems. We prove that the proposed feedforward adaptation method has a smaller error bound than previously used feedback methods.\n\n• We propose an uncertainty-bound estimation related to the feedforward approach, which is ag-\n\nnostic from specific optimizers.\n\n• We conduct extensive experiments to show that the proposed feedforward adaptation is superior\n\nto conventional feedback adaptation.\n\n2 PROBLEM OVERVIEW\n\n2.1 TIME-SERIES PREDICTION\n\nThe time series prediction problem is to make inferences on future time-series data given the past and current observations. We consider a multi-step prediction problem: using recent I steps’ observations to predict future O steps’ data. Assume the transition model is composed of a feature extractor (or Encoder) E and a predictor (or decoder) f . At time step t, the input to the model is Xt = [xt−I+1, xt−I+2, · · · , xt], which denotes the stack of I-step recent observations. The output of the model is Yt+1 = [yt+1, yt+2, · · · , yt+O], which denotes the stack of O-step future predictions. The observations xt, yt are vectors that may contain trajectory or feature, and xt = yt for some cases (e.g. univariate prediction). The transition model for time series prediction can be formulated as\n\nZt = E(Xt), Yt+1 = ft(Zt), (2) where Zt is a hidden feature representation of input Xt. Feature extractor E does not change over time, while predictor ft changes over time. Let ft denote the ground-truth predictor, that generates\n\n(1)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nground-truth output Yt+1. In online adaptation, we use the parameterized model (e.g. Neural Networks) ˆf (θt, Zt) with learnable parameters θt to estimate the ground-truth predictor ft(Zt). This paper assumes that the encoder E is fixed in online adaptation, which could be trained offline before online adaptation or be a non-parametric feature extractor1.\n\n2.2 ONLINE ADAPTATION\n\nDue to the temporal nature of time series prediction, the output (future) space in prediction tasks is not fixed. Since the train set and test set are split in chronological order, train-test distribution mismatch is very common in time-series prediction. Thus, online test-time adaptation is crucial to overcome the distribution mismatch problems and make the prediction robust to time-varying and heterogeneous behaviors.\n\nOnline adaptation also can be called adaptable prediction, since it makes an inference concurring with updating model parameters. Online adaptation explores local overfitting to minimize the prediction error: at time step t, the prediction error et+1 is to be minimized. The optimization objective is shown below\n\nLerr = min\n\nθt\n\net+1 = min\n\nθt\n\n∥Yt+1 − ˆf (θt, Zt)∥p,\n\n(3)\n\nwhere ft(Zt) = Yt+1 = [yt+1, yt+2, · · · , yt+O] is the ground truth observation (to be received in the future) and ˆf (θt, Zt)) = ˆYt+1 = [ˆyt+1, ˆyt+2, · · · , ˆyt+O] is the predicted outcome from the learned model parameter θt. The adaptation objective can be in any lp norm.\n\nIn conventional (feedback) online adaptation, the objective of minimizing the prediction error in the future Eq. (3) can be approximated by minimizing the fitting error in the past, as shown below\n\nLf b = min\n\nθt\n\n1 t\n\nt (cid:88)\n\ni=1\n\nλt−i∥Yi − ˆf (θt, Zi)∥p,\n\n(4)\n\nwhere 0 < λ ≤ 1 is a forgetting factor. The model parameter θt is updated iteratively when new observations are received. Then a new prediction is made using the new estimate. In the next time step, the estimate will be updated again given a new observation, and the process repeats. It is worth noting that the observation we received at time t is yt. The other terms in Yt remain unknown. This paper focused on adaptation methods using one-step-ahead observation. It is also possible to conduct online adaptation with multi-step ahead observations Abuduweili & Liu (2021).\n\nIn this paper, we propose a feedforward adaptation method, whose objective is different from feedback adaptation. In the feedforward adaptation, the objective of minimizing the prediction error in the future Eq. (3) can be approximated by minimizing the upper bound of the prediction error in the future, as shown below\n\nLf f = min\n\nθt\n\nBound[et+1] = min\n\nθt\n\nBound[∥Yt+1 − ˆf (θt, Zt)∥p].\n\n(5)\n\nWe will show the algorithms and effectiveness of the feedforward adaptation in section 3. In specific adaptation algorithms, the feedback and feedforward approach differ by sample selection strategy.\n\nAlgorithm 1 General Online Adaptation Framework (Adaptable Prediction)\n\nInput: Initial predictor f (θ0, :) with parameters θ0, Feature Extractor E, Optimizer O(:, :, :) Output: Sequence of predictions { ˆYt+1}T 1: for t = 1, 2, · · · , T do 2: 3:\n\nReceive the ground truth observation values xt, yt Find the critical input-output pairs (Zs, ys+1) for 1 ≤ s < t, where Zs = E(Xs) Adaptation: θt = O(θt−1, ˆys+1, ys+1), where ˆYs+1 = [ˆys+1, · · · , ˆys+O] = f (θt−1, Zs) ˆYt+1 = [ˆyt+1, · · · , ˆyt+O] = f (θt, Zt), where Zt = E(Xt) Prediction:\n\nt=1\n\n4:\n\n5: 6: end for\n\n1In a special case, the feature extractor can be an identity mapping E(X) = X, then we adapt the end-to-\n\nend neural network model in online adaptation\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nWe provide a general online adaptation framework as shown in algorithm 1, by incorporating conventional feedback with the proposed feedforward adaptation. At time step t, after receiving the current observations (xt, yt), we select the critical input-output pair (Zs, ys+1) from historical observations. The critical pair was used to adjust the parameters of the prediction model by an online optimizer (e.g. SGD). Then we obtain the current prediction result with the adapted model. The main difference between different adaptation algorithms lies in the critical pair selection strategy (line 3 in algorithm 1). In feedback adaptation, the critical input-output pairs are composed by the latest observations Zs = Zt−1. In random adaptation, the critical input-output pairs are randomly sampled from historical observations Zs ∼ [Z1, · · · , Zt−1]. We will show in section 3, in feedforward adaptation, the critical input-output pairs are the most similar samples to the current observation Zs = arg minZ ∥Zt − Z∥.\n\n3 FEEDFORWARD ADAPTATION APPROACH\n\n3.1 ERROR BOUND FOR GENERAL ONLINE ADAPTATION\n\nThe theoretical analysis of the paper is based on two basic conditions about the local smoothness property of ground-truth predictors ft.\n\nK-Lipschitz continuity condition. For a time-step t and ∀s ∈ [t − L, t − 1], we have local K Lipschitzness for ground-truth prediction function ft and recent L steps input data:\n\n∥ft(Zt) − ft(Zs)∥ ≤ K∥Zt − Zs∥,\n\n(6)\n\nwhere K is the bound (real number) for the change of the value of the function over input space. Intuitively, a Lipschitz continuous function is limited in how fast function value can change over input space. It is proven that every function that has bounded first derivatives is Lipschitz continuous Sohrab (2003). Similar to Eq. (6), we assume our parameterized function ˆft(θt, :) (e.g. Neural Networks) has Lipschitz continuity with constant value ˆK:\n\n∥ ˆf (θt, Zs) − ˆf (θt, Zt)∥ ≤ ˆK∥Zt − Zs∥.\n\n(7)\n\nK-Lipschitz continuity is common in machine learning because neural networks have bounded first derivatives by proper training.\n\nδ time-varying condition. For a time-step t and ∀s ∈ [t − L, t − 1], assume the ground-truth prediction functions ft has bounded changes within recent L steps under the same input Zs:\n\n∥ft(Zs) − fs(Zs)∥ ≤ δ,\n\n(8)\n\nwhere δ is the bound (real number) for change of the value of the function sequences over time on a fixed input. A bounded δ time-varying condition is a common condition in time-series tasks because we obtain similar future time-series signals with the same input in most cases.\n\nTheorem 1 (Error Bound of Online Adaptation). Under the K-Lipschitz continuity condition (Eq. (6) and (7)) and δ time-varying condition (Eq. (8)), the (prior) prediction error et+1 of general online adaptation (algorithm 1) has the following upper bound:\n\net+1 ≤ K∥Zt − Zs∥ + δ + ∥Ys+1 − ˆf (θt, Zs)∥ + ∥ ˆf (θt, Zs) − ˆf (θt, Zt)∥\n\n≤ (K + ˆK)∥Zt − Zs∥ + δ + ∥Ys+1 − ˆf (θt, Zs)∥\n\n(9)\n\n(10)\n\nPlease check the proof in appendix C.1.\n\n3.2 FEEDFORWARD ADAPTATION\n\nAs discussed in section 2.2, the goal of the online adaptation is to minimize the prediction error in the future Eq. (3). Due to the lack of ground-truth value in the current steps, it is not feasible to directly minimize the prediction error. Conventional online adaptation methods approximate the original objective by minimizing the fitting error in the past Eq. (4). In this work, we approximate the original objective by minimizing the bound of the prediction error Eq. (5). Which is equivalent to optimizing the worst-case scenarios. The worst-case performance analysis is really useful in\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nreal-world applications Roughgarden (2021). We will show that, under the following assumption, feedforward adaptation provides better results than feedback adaptation.\n\nTemporarily Slow Time-varying Assumption. For the δ time-varying function ft, assume δ is time-independent or it slowly changes within recent L-steps: ∂δ\n\n∂t ≈ 0.\n\nNote that, we only assume the ground-truth function ft varying slowly (locally) within recent Lsteps not (globally) for every step. Thus the assumption is reasonable because the major changes in function value are caused by the change of inputs instead of the time-dependency in many realworld prediction problems. In addition in some cases, δ is caused by random noises, then which still follow our assumptions. Under the above assumption, we can ignore the term δ in optimization. Then the optimization objective for the error bound is shown below:\n\nLf f = min\n\nθt\n\nBound[et+1] =\n\nmin θt,s∈[t−L,t−1]\n\n(K + ˆK)∥Zt − Zs∥ + ∥Ys+1 − ˆf (θt, Zs)∥\n\n(11)\n\nHowever, we still can not directly minimize the above objective, because we do not know K, ˆK. Then a simplification is applied to the above objective to make it applicable. We change the joint minimization over s and θt to a bi-level optimization which first minimizes the first term of objective over sampling time-step s, then minimizes the second term of objective over parameter θt. Thus, the simplified objective function is shown below:\n\nLf inal = min\n\n∥Ys⋆+1 − ˆf (θt, Zs⋆ )∥\n\nθt s⋆ = arg min\n\ns.t.\n\n∥Zt − Zs∥\n\n(12)\n\n(13)\n\ns∈[t−L,t−1]\n\nIn a summary, the proposed feedforward adaptation method selects the most similar samples to the current observation as critical pair (Zs⋆ , ys⋆ ) by Eq. (13). Then using the critical pair to optimize the prediction model by Eq. (12).\n\n3.3 UNCERTAINTY ESTIMATION\n\nThe error bound Eq. (9) provides uncertainty estimation of the prediction results. Here we use estimation of ̃Kt and ̃δ to approximate real K and δ in Eq. (9). We use confidence factor σ ∈ (0, 1] to decay the error bound . The uncertainty estimation ˆUt+1 for prediction ˆYt+1 is shown below:\n\nˆUt+1 = σ ·\n\n(cid:16) ̃Kt∥Zt − Zs∥ + ̃δ + ∥Ys+1 − ˆf (θt, Zs)∥ + ∥ ˆf (θt, Zs) − ˆf (θt, Zt)∥\n\n(cid:17)\n\n(14)\n\nWhere s = t−1 for feedback adaptation and s = s⋆ (Eq. (13)) for feedforward adaptation. The confidence factor σ is a predefined hyperparameter, e.g. σ = 0.9 for 90% confidence of uncertainty. ̃δ and ̃Kt are also predefined hyperparameters, e.g. ̃δ = 0.001, ̃K = 1. For more accurate uncertainty estimation, we can set ̃δ as a small fixed value, and iteratively update ̃Kt according to the estimated uncertainty ˆUt and real error et. If the previous uncertainty estimation is much larger than the real error then we shrink the ̃Kt value. If the previous uncertainty estimation is much smaller than the real error then we enlarge the ̃Kt value. The ̃Kt estimation criteria are shown below.\n\n ̃Kt =\n\n ̃Kt−1 1.5\n\nif ˆUt ≥ 2et, ̃Kt = 1.5 ̃Kt−1 if ˆUt ≤ 0.5et, ̃Kt = ̃Kt−1 if 0.5et < ˆUt < 2et\n\n(15)\n\nIn adaptation and prediction, feedforward adaptation does not require K and δ values. In the uncertainty estimation, we need to approximate K and δ or serve these as predefined hyperparameters, but an accurate approximation is not necessary. Since we mainly consider the relative uncertainty estimation between different samples or different optimizers. As shown in Eq. (14), the inaccuracy of ̃K and ̃δ mainly raise a data-independent error, which is no impact on the data-selection strategy and optimizers. The overall feedforward adaptation algorithm is shown in algorithm 2. The 1 ∼ 5 lines of algorithm 2 is corresponding to the 1 ∼ 5 lines of algorithm 1. Buffer B is used to store recent L-step observations.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Online Adaptation with Feedforward Compensation\n\nt=1 and estimated uncertainty { ˆUt+1}T\n\nInput: Initial predictor f (θ0, :) with parameters θ0, Feature Extractor E, Optimizer O(:, :, :) Input: Empty L-size buffer B ; Output: Sequence of predictions { ˆYt+1}T 1: for t = 1, 2, · · · , T do 2: 3:\n\nReceive the ground truth observation values xt, yt; Construct input Xt = [xt−I+1, · · · , xt] Find the critical (similar) input-output pairs (Zs⋆ , ys⋆+1) from buffer B by Eq. (13) Adaptation by Eq. (12): θt = O( ̃θt, ˆys⋆+1, ys⋆+1), where [ˆys⋆+1, · · · , ˆys⋆+O] = f ( ̃θt, Zs⋆ ) Prediction: Uncertainty ˆUt+1 Estimation by Eq. (14) and Eq. (15) Add current data to buffer: B.append(Zt, yt) if size(B) > L then\n\nˆYt+1 = [ˆyt+1, · · · , ˆyt+O] = f (θt, Zt), where Zt = E(Xt)\n\n4:\n\n5:\n\nt=1\n\nB ← keep more recent samples (B, L)\n\n6: 7: 8: 9: end if 10: 11: end for\n\n3.4 EFFECTIVENESS OF FEEDFORWARD ADAPTATION\n\nAs described in section 3.2, under the temporarily slow time-varying assumption, feedforward adaptation achieves a smaller error bound than feedback adaptation. In this section, we analyze more theoretical results on the online adaptation of some specific functions.\n\nLinear time-variant function. Consider a function ft which can be separated into time-invariant function g(zt) and linearly time-dependent part αt. Input zt is a random variable from the uniform distribution U(0, 1).\n\nft(zt) = g(zt) + αt,\n\n(16) ˆf (θ, :) to learn the linear time-variant function Eq. (16). Let K Lemma 1. We use neural network be a Lipschitz constant for the ground-truth function ft, and ˆK is a Lipschitz constant for neural e denote the prediction error bound for feedback and feedforward network adaptation. Then we have the following results.\n\nˆf (θ, :). Let Bf b\n\ne and Bf f\n\nzt ∼ U(0, 1)\n\n(a) Expectation of the error bound for feedback adaptation is E[Bf b (b) Expectation of the error bound for feedforward adaptation is\n\ne ] = K+ ˆK\n\n3 + α.\n\nE[Bf f\n\ne ] ≤ (K + ˆK) · max( 1 t ,\n\n1\n\nL+1 ) + α · min( t\n\n2 , L+1\n\n2 ). With L =\n\n− 1, we achieve\n\nthe smallest final error bound for feedforward adaptation: E[Bf f\n\ne ] ≤\n\n2α(K + ˆK).\n\n(cid:113)\n\n(cid:113)\n\n2(K+ ˆK) α\n\n(c) If time-varying factor α is smaller, specifically α ≤ α⋆ = 4−\n\ntion has smaller error bound than feedback adaptation.\n\n√\n\n6\n\n12\n\n(K + ˆK), feedforward adapta-\n\nTime-invariant polynomial function. Consider a time-invariant polynomial ground-truth function ft(zt) with the input of random variable sampled from the uniform distribution U(0, 1).\n\nyt+1 = ft(zt) =\n\nN (cid:88)\n\ni=1\n\nWizi t,\n\nzt ∼ U(0, 1)\n\n(17)\n\nLemma 2. We use linear projection ˆf (Vt, zt) = Vtzt to learn the time-invariant polynomial function Eq. (17). Then we have the following results.\n\n(a) Expectation of the prediction error for feedback adaptation is E[ef b (b) Expectation of the prediction error for feedforward adaptation is\n\nt+1] ≥ 1\n\n3\n\nN −1 N 2\n\n(cid:80)N −1\n\ni=1 Wi+1.\n\nE[ef f\n\nt+1] ≤ 1\n\n2 max( 1 t ,\n\n1\n\nL+1 ) (cid:80)N −1\n\ni=1 Wi+1. The final error converges limL→∞,t→∞ E(ef f\n\nt+1) = 0.\n\n(c) Feedforward adaptation is provably better than feedback adaptation.\n\nThe proof of Lemma 1 and Lemma 2 are provided in appendix C.2. Thus feedforward adaptation can be used to learn time-invariant functions and slow time-varying systems. These kinds of systems are very common in the world. For example, the exchange rate and incidence rate for common diseases have a slow time-dependent shift over time.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\n4.1 SYNTHETIC EXPERIMENTS\n\nTo evaluate Lemma 1 in section 3.4, we consider the following time-varying function.\n\nyt+1 = ft(zt) = sin zt + αt,\n\nzt ∼ U(0, 1)\n\n(18)\n\nAccording to Lemma 1, we derive the error bound for feedback adaptation Bf b adaptation Bf f\n\nis shown below (please check details in appendix D.1).\n\ne\n\ne , and feedforward\n\nE[Bf b\n\ne ] =\n\nE[Bf f\n\ne ] ≤\n\n5 12 (cid:114) 5 2\n\n+ α, E[Bf f\n\ne ] ≤\n\n5 4(L + 1)\n\n+ α\n\nL + 1 2\n\nα with L =\n\n(cid:114) 5 2α\n\n− 1\n\n(19)\n\n(20)\n\nThen we calculate the threshold α⋆. If α ≤ α⋆, feedforward adaptation has a smaller error bound.\n\nα⋆ ≈ 0.1, L⋆ =\n\n(cid:114) 5\n\n2α⋆ − 1 ≈ 3\n\n(21)\n\n(a) Average error\n\n(b) Prediction output\n\nFigure 1: (a) Average error of feedback and feedforward adaptation with different buffer size L and time-varying factor α. (b) Prediction output of feedforward (blue curve) and feedback adaptation (green curve) with buffer size L = 100 and time-varying factor α = 10−3 and α = 10−2.\n\nFigure 1a shows the average prediction error of feedback and feedforward adaptation under different time-varying factors α. Note that, feedforward adaptation with L = 1 is the same as feedback adaptation. We will show that the results in Fig. 1a correspond to the theoretical results from Lemma 1. (1) As can be seen, for a smaller time-varying factor α, feedforward adaptation with a larger buffer size L achieves a smaller prediction error. Besides, if α is larger, the performance of the feedforward adaptation drops. These are consistent with Eq. (20). For example, feedforward adaptation with buffer size L = 100 achieves the smallest prediction error when α < 10−3. (2) If α < 0.1, feedforward adaptation with buffer size L = 3 performs better than feedback adaptation. But if α > 0.1, feedback adaptation is better. The threshold α⋆ ≈ 0.1 is consistent with Eq. (21).\n\nFigure 1b illustrate the comparison between feedforward and feedback adaptation. The first row shows the prediction results for α = 10−3 target function of feedforward adaptation with L = 100 (b.1 subfigure) and feedback adaptation (b.3 subfigure). As can be seen, feedforward adaptation can learn the ground truth more precisely. The second row shows the prediction results for α = 10−2 target function. For this case, prediction with feedforward adaptation (b.2 subfigure) has a significant shift from the ground truth, although it is still able to learn some input-dependent details. Compared to feedback adaptation, feedforward adaptation is more focused on the input-dependent part of the ground truth function and pays less attention to the time-varying shift of the function. These empirical results validate the conclusions in section 3.4. The experimental evaluation of Lemma 2 is shown in appendix D.2.\n\n7\n\n1041031021010.10.20.30.40.5Average ErrorFeedbackFeedforward (L=3)Feedforward (L=5)Feedforward (L=10)Feedforward (L=100)(b.1)(b.2)(b.3)(b.4)Under review as a conference paper at ICLR 2023\n\n4.2 EXPERIMENTS ON REAL WORLD DATA\n\nIn this section, we evaluate the proposed feedforward adaptation on four real-world benchmarks, including ETT (Electricity Transformer Temperature), Exchange-Rate, ILI (Influenza-like Illness), and THOR human motion trajectory dataset. We include two models as our learnable prediction function ˆf (θ, :): Informer Zhou et al. (2021) and simple MLP (multi-layer perception).\n\nExperimental Design. First we offline train models (Informer and MLP) on a train set. After training, we will incrementally receive the data point from the test set like in real-world applications. At each time step, we conduct an online adaptation to optimize the model with selected previous observations by feedback or feedforward compensation. Then the prediction output is inferred from the updated model. We evaluate the prediction results with mean squared error (MSE) and mean absolute error (MAE). In the experiments, we only adapt the decoder of the model and make the encoder fixed. Please check the detailed experimental design in appendix D.3.\n\nBaselines. We compare the proposed method with four baselines. 1) w/o adapt directly conduct prediction without adaptation. Which is a lower bound for adaptation methods. 2) Feedback adaptation is the most important baseline to us. 3) Random adaptation is a method that selects the critical pair from the L-size buffer with random sampling. 4) Full adaptation is a method that uses all samples from the buffer to adapt the model, which is similar to offline training.\n\nResults. The detailed results of the experiment on SGD optimizer are shown in table 1. In table 1, the last row denotes the performance gain of the proposed feedforward adaptation over feedback adaptation in terms of average result. Feedforward adaptation achieves the best results on all four datasets. Specifically, feedforward adaptation outperforms feedback adaptation by 12.6%, 7.4%, 22.34%, and 8.1% in terms of average MSE on four datasets respectively. Feedforward adaptation is better than full adaptation, which means adapting models with more samples is not effective, because online adaptation focuses on the ability to rapidly learn and adapt in the presence of non-stationarity instead of generalization ability like offline learning. Feedforward adaptation is better than feedback and random adaptation, which means the sample selection strategy based on sample similarity in feedforward adaptation is more critical than the time-based sample selection and random sampling.\n\nTable 1: Performance comparison between the proposed feedforward adaptation method and other baselines. Avg denotes the average results of two models (MLP and Informer). The last row denotes the performance gain of feedforward adaptation over feedback adaptation. We use boldface and underline for the best and second-best average results.\n\nMethod \\ Dataset\n\nETTh1\n\nExchange\n\nILI\n\nTHOR\n\nAdaptation\n\nw/o Adaptation\n\nFull Adaptation\n\nRandom Adaptation\n\nFeedback Adaptation\n\nFeed Forward Adaptation\n\nModel MLP Informer Avg MLP Informer Avg MLP Informer Avg MLP Informer Avg MLP Informer Avg\n\nGain of Feedforward Over Feedback\n\nMSE MAE MSE MAE MSE MAE MSE MAE 0.117 0.195 0.171 0.211 0.144 0.203 0.150 0.142 0.287 0.146 0.219 0.144 0.148 0.145 0.271 0.131 0.210 0.138 0.119 0.153 0.173 0.116 0.146 0.135 0.108 0.128 0.156 0.109 0.118 0.132 12.6% 7.2% 8.0% 8.2% 22.3% 16.1% 8.1% 9.6%\n\n0.371 0.389 0.380 0.311 0.307 0.309 0.312 0.283 0.298 0.317 0.269 0.293 0.286 0.259 0.272\n\n1.413 1.531 1.472 1.340 1.252 1.296 1.347 1.380 1.364 1.334 1.411 1.373 1.193 1.312 1.252\n\n0.540 0.858 0.699 0.443 0.567 0.505 0.445 0.553 0.499 0.442 0.461 0.452 0.432 0.399 0.415\n\n0.549 1.128 0.839 0.362 0.503 0.433 0.365 0.497 0.431 0.349 0.326 0.338 0.347 0.276 0.311\n\n4.348 4.942 4.645 3.844 3.413 3.628 3.865 3.867 3.866 3.868 4.383 4.123 3.041 3.363 3.202\n\n0.135 0.137 0.136 0.127 0.213 0.170 0.132 0.204 0.168 0.112 0.134 0.123 0.102 0.125 0.113\n\nWe found another interesting result by comparison between Informer and MLP on table 1. Without adaptation, the simple MLP outperforms the Informer on average. This is caused by the overfitting of the Informer on the training set. However, by online adaptation, especially with feedforward adaptation, the performance of Informer is greatly increased. For example, the performance of Informer on the ETTh1 dataset increased from 0.211 to 0.109 by 48% and outperform the MLP model. That shows Informer has greater representation ability but may perform poorly on the test\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nset. Considering this case, it is embarrassing to directly use the big Transformer-like models on real-world time-series prediction. But online adaptation may make the big Transformer-like models powerful again. This phenomenon emphasizes the importance of online adaptation in real-world time-series prediction.\n\nFigure 2: Experiments on ETTh1 data. (a) Time series output; (b) FFT period analysis; (c) Time shift t − s between current sample Zt and selected sample Zs in feedback and random adaptation; (d) Time shift t − s in feedforward adaptation; (e) Online prediction error. (f) Sample difference ∥Zt − Zs∥ between current sample Zt and adapted sample Zs.\n\nDiscussion. We will show that the sample selection strategy in the proposed feedforward adaptation method could intrinsically mine the periodicity of the input data. Figure 2(a) shows the ETTh1 timeseries data, and Fig. 2(b) is the FFT (Fast Fourier Transformation) period analysis of the ETTh1 data. As can be seen ETTh1 has roughly T ≈ 500 repetition periods. Random adaptation randomly selects samples from the buffer, then the time shift between current time-step t and the time-step s for selected samples is (t − s) ∼ [1, L]; Feedback adaptation only selects the last time sample to optimization, then (t − s) = 1. This can be found from the distribution of time shift (t − s) in Fig. 2(c). For the proposed feedforward adaptation in Fig. 2(d), many samples were selected from (t − s) ≈ 500 steps earlier, which corresponds to the repetition period of T ≈ 500. Because feedforward adaptation selects the most similar samples to the current sample, which helps to extract the hidden periodicity of the input signal over time. Thus the distribution of t − s is similar to FFT period analysis. Experiments on different datasets are shown in appendix D.5. According to theorem 1 (Eq. (10)), the error bound is related to the sample difference ∥Zt −Zs∥. Figure 2(e) shows the real prediction error for different adaptation methods over time. As can be seen, feedforward adaptation has the smallest prediction error, because feedforward adaptation has the smallest sample difference ∥Zt − Zs∥ during adaptation, as shown in Fig. 2(f).\n\nOne of the advantages of the proposed feedforward adaptation is that it could provide uncertainty estimation. The results of prediction output and uncertainty are shown in appendix D.6.\n\n5 CONCLUSION\n\nThis paper studies an effective feedforward adaptation algorithm for time-series prediction tasks. Firstly we propose the general framework for online adaptation which includes feedback and feedforward adaptation. Then we propose the feedforward adaptation algorithm by selecting the most similar critical samples for optimization. We prove that, in a time-invariant or slow time-varying system, the feedforward adaptation has a smaller error bound than conventional feedback adaptation. In the experiments, we empirically validate the effectiveness of the proposed algorithm both on synthetic and real-world data. In the end, we show that online adaptation can greatly improve the performance of Informer or other models.\n\nREFERENCES\n\nAbulikemu Abuduweili and Changliu Liu. Robust online model adaptation by extended kalman filter with exponential moving average and dynamic multi-epoch strategy. In Learning for Dynamics and Control, pp. 65–74. PMLR, 2020.\n\n9\n\n(a)(b)(c)(d)(e)(f)Under review as a conference paper at ICLR 2023\n\nAbulikemu Abuduweili and Changliu Liu. Robust nonlinear adaptation algorithms for multitask International Journal of Adaptive Control and Signal Processing, 35(3):\n\nprediction networks. 314–341, 2021.\n\nAbulikemu Abuduweili, Siyan Li, and Changliu Liu. Adaptable human intention and trajectory\n\nprediction for human-robot collaboration. arXiv preprint arXiv:1909.05089, 2019.\n\nNesreen K Ahmed, Amir F Atiya, Neamat El Gayar, and Hisham El-Shishiny. An empirical comparison of machine learning models for time series forecasting. Econometric reviews, 29(5-6): 594–621, 2010.\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019.\n\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional\n\nand recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\n\nHelen C Barron. Neural inhibition for continual learning and memory. Current opinion in neurobi-\n\nology, 67:85–94, 2021.\n\nShubhendu Bhasin, Rushikesh Kamalapurkar, Huyen T Dinh, and Warren E Dixon. Robust identification-based state derivative estimation for nonlinear systems. IEEE Transactions on Automatic Control, 58(1):187–192, 2012.\n\nAvrim Blum. On-line algorithms in machine learning. Online algorithms, pp. 306–325, 1998.\n\nGeorge EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis:\n\nforecasting and control. John Wiley & Sons, 2015.\n\nPeter J Brockwell and Richard A Davis. Introduction to time series and forecasting. Springer, 2002.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nDian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 295–305, June 2022.\n\nYujiao Cheng, Weiye Zhao, Changliu Liu, and Masayoshi Tomizuka. Human motion prediction using semi-adaptable neural networks. In 2019 American Control Conference (ACC), pp. 4884– 4890, 2019. doi: 10.23919/ACC.2019.8814980.\n\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nAndrew H Jazwinski. Stochastic processes and filtering theory. Courier Corporation, 2007.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\n\nJyrki Kivinen, Alexander J Smola, and Robert C Williamson. Online learning with kernels. IEEE\n\ntransactions on signal processing, 52(8):2165–2176, 2004.\n\nJogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4544–4553, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nGuokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 95–104, 2018.\n\nKuang-Chih Lee and David Kriegman. Online learning of probabilistic appearance manifolds for video-based recognition and tracking. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), volume 1, pp. 852–859. IEEE, 2005.\n\nTimoth ́ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia D ́ıaz-Rodr ́ıguez. Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges. Information fusion, 58:52–68, 2020.\n\nRui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9641–9650, 2020.\n\nBryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philosophical\n\nTransactions of the Royal Society A, 379(2194):20200209, 2021.\n\nShizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2021.\n\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.\n\nAdvances in neural information processing systems, 30, 2017.\n\nZheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. Neurocomputing, 469:28–51, 2022.\n\nPascual Mart ́ınez-G ́omez, Germ ́an Sanchis-Trilles, and Francisco Casacuberta. Online adaptation strategies for statistical machine translation in post-editing scenarios. Pattern Recognition, 45(9): 3193–3203, 2012.\n\nConstantin Paleologu, Jacob Benesty, and Silviu Ciochina. A robust variable forgetting factor reIEEE Signal Processing Letters, 15:\n\ncursive least-squares algorithm for system identification. 597–600, 2008.\n\nPeter Pastor, Ludovic Righetti, Mrinal Kalakrishnan, and Stefan Schaal. Online movement adapIn 2011 IEEE/RSJ International Conference on\n\ntation based on previous sensor experiences. Intelligent Robots and Systems, pp. 365–371. IEEE, 2011.\n\nMartin Popel and Ondˇrej Bojar.\n\nTraining tips for the transformer model.\n\narXiv preprint\n\narXiv:1804.00247, 2018.\n\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.\n\nicarl: In Proceedings of the IEEE conference on\n\nIncremental classifier and representation learning. Computer Vision and Pattern Recognition, pp. 2001–2010, 2017.\n\nTim Roughgarden. Beyond the worst-case analysis of algorithms. Cambridge University Press,\n\n2021.\n\nDennis W. Ruck, Steven K. Rogers, Matthew Kabrisky, Peter S. Maybeck, and Mark E. Oxley. Comparative analysis of backpropagation and the extended kalman filter for training multilayer IEEE transactions on pattern analysis & machine intelligence, 14(06):686–691, perceptrons. 1992.\n\nAndrey Rudenko, Tomasz P Kucner, Chittaranjan S Swaminathan, Ravi T Chadalavada, Kai O Arras, and Achim J Lilienthal. Th ̈or: Human-robot navigation data collection and accurate motion trajectories dataset. IEEE Robotics and Automation Letters, 5(2):676–682, 2020.\n\nWenwen Si, Tianhao Wei, and Changliu Liu. Agen: Adaptable generative prediction networks for In 2019 IEEE Intelligent Vehicles Symposium (IV), pp. 281–286. IEEE,\n\nautonomous driving. 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nHoushang H Sohrab. Basic real analysis, volume 231. Springer, 2003.\n\nAlessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, and Luigi Di Stefano. Real-time self-adaptive deep stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 195–204, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully\n\ntest-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.\n\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419–22430, 2021.\n\nAiling Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\n\nforecasting? arXiv preprint arXiv:2205.13504, 2022.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11106–11115, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA RELATED WORKS\n\nTime series prediction. Time-series prediction historically has very diverse applications across various domains Lim & Zohren (2021). These applications trigger many methods for time-series prediction. Modern machine learning methods provide a purely data-driven manner without domain knowledge Ahmed et al. (2010). In recent years, deep neural networks becomes the dominant approach inspired by notable achievements of deep learning in computer vision Dosovitskiy et al. (2020) and natural language processing Vaswani et al. (2017). Among deep neural networks, Transformer-style models are becoming more and more popular in time-series prediction tasks, since which has the potential to capture long-term dependencies Brown et al. (2020). Most Transformerstyle models focus on long-term time series forecasting problem, including Informer Zhou et al. (2021), Autoformer Wu et al. (2021), and Pyraformer Liu et al. (2021).\n\nContinual Learning also known as Lifelong learning, aims at providing incrementally updated knowledge in an ever-changing environment. From the data statistics perspective, continual learning studies the problem of learning from non-i.i.d data, with the goal of preserving and extending the acquired knowledge. Conventional continual learning assumes the new data arrive one task at a time, and the data distribution for each task is stationary De Lange et al. (2021). Which is also called offline continual learning. Different from the batch training on offline continual learning, Online Continual Learning focuses on the more realistic problem, where data arrive one tiny batch at a time and previously seen batches from the current or the previous tasks are not accessible Mai et al. (2022). Therefore, online continual learning is required to efficiently learn from a single sample from the online data stream in a single step. Recent works of continual learning can be roughly divided into four families: dynamic architectures, regularization-based, rehearsal, and generative replay approach Lesort et al. (2020). Among these methods, Rehearsal-based methods are more related to our work, which use memory replays to enhance the knowledge from the previous data or processes such as iCaRL Rebuffi et al. (2017), GEM Lopez-Paz & Ranzato (2017). However, most of these methods focus on offline continual learning.\n\nTest-time Adaptation is a special setting of domain adaptation and continual learning where a trained model on the training domain has to adapt to the testing domain without accessing the training set Kundu et al. (2020). Most test-time adaptation methods for image data utilize unsupervised domain adaptation algorithms to improve the performance on the test set such as entropy minimization Wang et al. (2020), pseudo-labeling Li et al. (2020). For the time-series prediction task, the input signal at the current step might be the ground-truth label of the previous step. So (partially) supervised learning can be used for test-time adaptation on time-series tasks Abuduweili & Liu (2021). In many real-world applications, online test time adaptation (online adaptation for short) is more reasonable than offline adaptation. For example, in automotive driving, we only receive an observation (sample) in an online manner at each time step, instead of batch data of offline setting.\n\nOnline Adaptation continually learns from real-time streaming data. Online adaptation is a special case of online continual learning in that the model is always offline trained on a training dataset (which can be very small), and then incrementally adapt its models in an online test dataset Cheng et al. (2019). Unlike works that focus on avoiding catastrophic forgetting in offline continual learning Kirkpatrick et al. (2017), online adaptation focuses on the ability to rapidly learn and adapt in the presence of non-stationarity. Most existing online adaptation approaches are based on feedback compensation with SGD (Stochastic Gradient Descent) or EKF (Extended Kalman Filter) optimizers Bhasin et al. (2012).\n\nB ONLINE OPTIMIZERS\n\nIn algorithm 1, an online optimizer was used to adjust the model with the selected critical samples: θt = O(θt−1, ˆys+1, ys+1). Unlike offline machine learning, online optimizers need to handle iterative input-output pairs with a single sample. We assume our objective function (loss function) has suitable smoothness properties (e.g. differentiable and locally Lipschitz continuous). SGD and EKF are two widely used optimizers for online adaptation.\n\nStochastic Gradient Descent (SGD) is one of the widely used optimizers in online adaptation Kivinen et al. (2004). SGD is memory efficient and stable. The update formula of the SGD is shown\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nbelow:\n\nθt = θt−1 − η\n\n(cid:18) ∂ ∂θ\n\n∥ys+1 − ˆf (θ, Zs)∥2\n\n(cid:19)\n\n|θ=θt−1,\n\n(22)\n\nwhere η is the learning rate.\n\nExtended Kalman Filter (EKF) is another strong optimizer in online adaptation Jazwinski (2007). In the original EKF, the object being estimated is the state value of a dynamic system. In online adaptation, we can apply the EKF approach to adapt model parameters by regarding model parameters as system states. The EKF approach has a faster convergence rate than SGD, but it is very memoryexhaustive. When optimizing small neural networks, EKF approach has been demonstrated to be superior to the SGD-based algorithms Ruck et al. (1992); Abuduweili & Liu (2021). If the adapted model size is not big, EKF is a strong optimizer for online adaptation. The update formula of the EKF is shown below:\n\nHt =\n\n∂ ˆf (θ, Zs) ∂θ\n\n|θ=θt−1\n\nt · (Ht · Pt−1 · HT\n\nGt = Pt−1 · HT θt = θt−1 + Gt · (ys+1 − ˆf (θt−1, Zs)) Pt = λ−1(Pt−1 − Gt · Ht · Pt−1)\n\nt + λI)−1\n\n(23)\n\n(24)\n\n(25)\n\n(26)\n\nwhere λ ∈ (0, 1] is a forgetting factor, Ht is the gradient matrix, Gt is the Kalman gain, and Pt is a matrix representing the uncertainty in the estimates of parameters. The initial value of the matrix Pt can be set as a diagonal matrix P0 = p0 · diag(1, 1, · · · , 1) for p0 > 0.\n\nC THEORETICAL RESULTS ON SECTION 3\n\nC.1 THEOREM 1 (ERROR BOUND OF ONLINE ADAPTATION)\n\nBound of ground-truth difference. If the sereis of ground-truth functions ft within recent L steps follows K-Lipschitz continuity Eq. (6) and δ time-varying conditions Eq. (8), then the ground-truth value Yt+1 and Ys+1 has the following property:\n\n∥Yt+1 − Ys+1∥ ≤ K∥Zt − Zs∥ + δ\n\n(27)\n\nThe proof is shown below:\n\n∥Yt+1 − Ys+1∥ = ∥ft(Zt) − fs(Zs)∥\n\n= ∥ft(Zt) − ft(Zs) + ft(Zs) − fs(Zs)∥ ≤ ∥ft(Zt) − ft(Zs)∥ + ∥ft(Zs) − fs(Zs)∥ ≤ K∥Zt − Zs∥ + ∥ft(Zs) − fs(Zs)∥ ≤ K∥Zt − Zs∥ + δ\n\n(δ time varying)\n\n(triangle inequality)\n\n(K Lipschitzness)\n\n(28)\n\nError Bound of Online Adaptation. For time step t, the (prior) prediction error et+1 has the following inequality:\n\net+1 = ∥Yt+1 − ˆYt+1∥ = ∥Yt+1 − ˆf (θt, Zt)∥\n\n= ∥Yt+1 − Ys+1 + Ys+1 − ˆf (θt, Zs) + ˆf (θt, Zs) − ˆf (θt, Zt)∥ ≤ ∥Yt+1 − Ys+1∥ + ∥Ys+1 − ˆf (θt, Zs)∥ + ∥ ˆf (θt, Zs) − ˆf (θt, Zt)∥ ≤ K∥Zt − Zs∥ + δ + ∥Ys+1 − ˆf (θt, Zs)∥ + ∥ ˆf (θt, Zs) − ˆf (θt, Zt)∥\n\n(triangle inequality)\n\n(Eq. (27))∥\n\n(29)\n\nThe first two terms come from the difference between ground-truth Yt+1 − Ys+1, the third term is a (posterior) fitting error for input-output tuple (Zs, Ys+1), and the last term is the difference between two predictions. Combining Eq. (29) with Eq. (7), we obtain the error bound for general online adaptation is shown below:\n\net+1 ≤ (K + ˆK)∥Zt − Zs∥ + δ + ∥Ys+1 − ˆf (θt, Zs)∥\n\n(30)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC.2 EFFECTIVENESS OF FEEDFORWARD ADAPTATION\n\nC.2.1 ERROR BOUND OF ONLINE ADAPTATION ON LINEAR TIME-VARIANT FUNCTION\n\nIn this section, we compare the error bound of feedforward and feedback adaptation methods on linear time-variant systems (functions). Consider a function ft which can be separated into timeinvariant function g(zt) and linearly time-dependent part αt. Input zt is a random variable from the uniform distribution U(0, 1).\n\n(31)\n\nzt ∼ U(0, 1) ft(zt) = g(zt) + αt\n\n(32) ˆf (θ, :) to learn the above function. Let K be a Lipschitz constant for ft We use neural network (it is also equal to the Lipschitz constant for the function g), and ˆK is a Lipschitz constant for ˆf (θ, :). The δ time-varying condition becomes ∥ft(zs) − fs(zs)∥ = α∥t − s∥, and neural network time-varying factor δ = α∥t − s∥. Then we have an error bound from Eq. (10): et+1 ≤ (K + ˆK)∥zt − zs∥ + α∥t − s∥ + ∥ys+1 − ˆf (θt, zs)∥ (33) Now we consider the comparison of the error bound between feedforward and feedback adaptation. Note that, the last term of the above equation ∥ys+1 − ˆf (θt, zs)∥ is a fitting error on input-output tuple (zs, ys+1), and the fitting error is irrelevant to feedback or feedforward compensation strategy. Besides, in over-parameterized neural networks, the fitting error is very small, even can be zero Allen-Zhu et al. (2019). Thus we ignore the fitting error in comparison. We now compare the expectation of the error bound. Let Be denote the prediction error bound for online adaptation, Be := Bound[et+1]. Then we have the expectation of the error bound for online adaptation as shown below.\n\nE[Be] = (K + ˆK)E[∥zt − zs∥] + αE[|t − s|] (34) Then we use Eq. (34) to derive the expectation of the error bound for feedback and feedforward adaptation methods. The core of the proof is to estimate E[∥zt − zs∥] and E[|t − s|].\n\nIn the following sections, we compare the error bounds of four methods. 1) Feedback adaptation is the most important baseline that selects the latest observations to optimize the model. 2) Random adaptation is a method that randomly selects samples from the L-size buffer to optimize the model. 3) Full adaptation is a method that uses all samples from the buffer to adapt the model, which is similar to offline training. 4) Feedforward adaptation is the proposed method, that selects the most similar samples to optimize the model.\n\nExpectation of the error bound for feedback adaptation. In feedback adaptation, the selected input-output pairs are the latest observations zs = zt−1 and s = t − 1. The current sample zt and last sample zt−1 are independent random variables from U(0, 1). The expectation of the distance between these two independent variables is 1 3 . We have the expectation for the error bound of feedback adaptation Bf b\n\n3 , then E[∥zt − zs∥] = E[∥zt − zt−1∥] = 1 e as shown: e ] = (K + ˆK)E[∥zt − zs∥] + αE[|t − s|]\n\nE[Bf b\n\n= (K + ˆK) ·\n\n1 3\n\n+ α · 1 =\n\nK + ˆK 3\n\n+ α\n\n(35)\n\nExpectation of the error bound for feedforward adaptation. In feedforward adaptation, the selected input-output pairs are the most similar samples to the current observation z⋆ s = arg minzs ∥zt − zs∥ from L-size buffer, and s⋆ = arg mins∈[t−L,t−1] ∥Zt − Zs∥. The expectation E[∥zt − zs∥] represents the average minimum distance between current sample zt and previous samples. If t ≤ L, E[∥zt − zs∥] considers the minimum distance for t samples, which is no greater t . If t > L, E[∥zt − zs∥] considers the minimum distance for L + 1 samples in the buffer, than 1 L+1 . Similarly, E[∥t − s∥] is an average distance between t indices for which is no greater than t ≤ L, which is t 2 . We have the expectation for the error bound of feedforward adaptation Bf f\n\n2 , or average distance from L + 1 indices for t ≥ l , which is L+1\n\nas shown:\n\n1\n\ne\n\nE[Bf f\n\ne ] = (K + ˆK)E[∥zt − zs∥] + αE[|t − s|]\n\n≤ (K + ˆK) · max(\n\n1 t\n\n,\n\n1 L + 1\n\n) + α · min(\n\nt 2\n\n,\n\nL + 1 2\n\n)\n\n(36)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nWe mainly consider the final performance t > L, then the above equation can be simplified as:\n\nE[Bf f\n\ne ] ≤\n\nK + ˆK L + 1\n\n+ α\n\nL + 1 2\n\nWith L =\n\n(cid:113)\n\n2(K+ ˆK) α\n\n− 1, we achieve the smallest error bound for feedforward adaptation:\n\nE[Bf f\n\ne ] ≤\n\n(cid:113)\n\n2α(K + ˆK)\n\n(37)\n\n(38)\n\nExpectation of the error bound for random adaptation. In random adaptation, the critical inputoutput pairs are randomly sampled from historical observations zs ∼ [zt−L, · · · , zt−1] from L size buffer. The current input zt and selected input zs are independent random variables from U(0, 1). Similar to feedback adaptation E[∥zt −zs∥] = 1 3 . For the term E[∥t−s∥], it is similar to feedforward adaptation, because s is randomly sampled from [t − L, · · · , t − 1]. We have that E[∥t − s∥] = t 2\nif t ≤ L, and E[∥t − s∥] = L+1 if t > L. We have the expectation for the error bound of random adaptation Brnd\n\nas shown:\n\n2\n\ne\n\nE[Brnd\n\ne\n\n] = (K + ˆK)E[∥zt − zs∥] + αE[|t − s|]\n\n≤ (K + ˆK) ·\n\n1 3\n\n+ α · min(\n\nt 2\n\n,\n\nL + 1 2\n\n)\n\nWe mainly consider the final performance t > L, then the above equation can be simplified as:\n\nE[Brnd\n\ne\n\n] ≤\n\nK + ˆK 3\n\n+ α\n\nL + 1 2\n\nIf L ≥ 2:\n\nK + ˆK L + 1\n\n+ α\n\nL + 1 2\n\n≤\n\n+ α\n\nK + ˆK 3\n\nL + 1 2\ne ] ≤ E[Brnd\n\ne\n\nAccording to Eq. (37), (40) and (41) , we have E[Bf f ]. Actually if L = 1, feedforward adaptation is the same as random adaptation, because the buffer only has one sample. Thus, feedforward adaptation has no greater error bound than random adaptation.\n\nExpectation of the error bound for full adaptation. The full adaptation uses all samples from the buffer [zt−L, · · · , zt−1] to adapt the model. Considering t > L, for an arbitrary sample zs ∼ [zt−L, · · · , zt−1], we have E[∥zt − zs∥] = 1 3 , because all samples in the buffer are independent uniformly-random variables. Considering the final performance t > L, we derive the expectation for the error bound of full adaptation Bf l\n\ne as shown:\n\n(39)\n\n(40)\n\n(41)\n\nt−1 (cid:88)\n\n|t − s|]\n\ns=t−L\n\n1 L\n\nL (cid:88)\n\ni=1\n\ni]\n\nE[Bf l\n\ne ] = (K + ˆK)E[\n\n1 L\n\nt−1 (cid:88)\n\ns=t−L\n\n∥zt − zs∥] + αE[\n\n1 L\n\nt−1 (cid:88)\n\ns=t−L\n\nt−1 (cid:88)\n\ns=t−L\n\nt−1 (cid:88)\n\ns=t−L\n\n= (K + ˆK) ·\n\n= (K + ˆK) ·\n\n≤ (K + ˆK) ·\n\n≤ (K + ˆK) ·\n\n1 L\n\n1 L\n\n1 L\n\n1 3\n\nE[∥zt − zs∥] + αE[\n\nE[∥zt − zs∥] + α\n\nL + 1 2\n\n1 3\n\n+ α\n\nL + 1 2\n\n+ α\n\nL + 1 2\n\n(42)\n\nAs can be seen, the expected error bound of full adaptation is the same as the error bound of random adaptation. It is reasonable because full adaptation usually averages out individual differences between samples. Thus we have a similar conclusion as random adaptation, that feedforward adaptation has no greater error bound than full adaptation. We have a conclusion below.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 3. Given a linear time-variant function (may not be a slowly varying function) Eq. (32), the expected error bound of the feedforward adaptation is smaller than the full adaptation and random adaptation, with the proper buffer size (e.g. L > 2).\n\nTherefore, in the following comparison, we only compare the feedforward and feedback adaptation.\n\nWhen is feedforward better than feedback. Similar to the worst-case analysis, we derive the conditions that feedforward adaptation has a smaller error bound than feedback adaptation. To find the condition for E[Bf f\n\ne ], we solve the following inequality\n\ne ] ≤ E[Bf b\n\n(cid:113)\n\n2α(K + ˆK) ≤\n\nK + ˆK 3\n\n+ α and L =\n\n2(K + ˆK) α\n\n− 1 ≥ 1\n\n(43)\n\n(cid:115)\n\nThen we obtain a δ time condition on linear time-variant function for feedforward adaptation:\n\nα ≤ α⋆ =\n\n4 −\n\n12\n\n(K + ˆK) ≈ 0.089(K + ˆK)\n\n(44)\n\n√\n\n6\n\n√\n\nAs summary, if time-varying factor α of linear time-variant function Eq. (32) is smaller, specifically (K + ˆK), feedforward adaptation has smaller error bound than feedback adaptation. α ≤ 4− 6\n(K + ˆK) is a threshold for selecting whether feedforward or feedback. Thus, we α⋆ = 4− proved the Lemma 1 in section 3.4.\n\n12 √\n\n12\n\n6\n\n∂t = ∂h\n\nWe can extend the above conclusion to a slightly larger set of functions. α can be time-dependent, but the supremum of it needs to be small. Consider the function ft(zt) = g(zt) + h(t), and let αt = ∂ft ∂t . Similar to the above analysis on linear time-variant function Eq. (32), we have a conclusion below. Lemma 4. Given a function ft, let αt = ∂ft truth ft and learnable function (e.g. neural networks) ˆf (θ). If sup(αt) ≤ 4− proper buffer size, feedforward adaptation achieves smaller error bound than feedback adaptation.\n\n∂t . Let K and ˆK denote the Lipschitz constant for ground- (K + ˆK), with the\n\n12\n\n√\n\n6\n\nCombing Lemma 3 with Lemma 4, we have a conclusion about the effectiveness of the feedforward adaptation method.\n\nLemma 5. Given a function ft, let αt = ∂ft ground-truth ft and learnable function (e.g. neural networks) ˆf (θ). (a) With a buffer size L > 2, feedforward adaptation always has a smaller error bound than full\n\n∂t . Let K and ˆK denote the Lipschitz constant for\n\nadaptation and random adaptation.\n\n(b) If sup(αt) ≤ 4−\n\n12\n\n(K + ˆK), with the proper buffer size L =\n\nadaptation achieves a smaller error bound than feedback adaptation.\n\n√\n\n6\n\n(cid:113)\n\n2(K+ ˆK) α\n\n− 1, feedforward\n\nC.2.2 PREDICTION ERROR OF ONLINE ADAPTATION ON TIME INVARIANT POLYNOMIAL\n\nIn this section, we compare the error (not error bound) of feedforward and feedback adaptation methods on time-invariant polynomial systems (functions). Consider a time-invariant polynomial ground-truth function ft(zt) with the input of random variable sampled from the uniform distribution U(0, 1).\n\nzt ∼ U(0, 1)\n\nyt+1 = ft(zt) =\n\nN (cid:88)\n\ni=1\n\nWizi\n\nt\n\nAssume our parameterized prediction model is a linear projection:\n\nˆyt = ˆf (Vt, zt) = Vtzt\n\n(45)\n\n(46)\n\n(47)\n\nIn online adaptation, we use the critical pair (zs, ys+1) to optimize the linear prediction model at time step t. Then we have a parameter of\n\nVt = arg min\n\nV\n\n∥V zs − ys+1∥ =\n\nys+1 zs\n\n=\n\n17\n\n(cid:80)N\n\ni=1 Wizi zs\n\ns\n\n=\n\nN −1 (cid:88)\n\ni=0\n\nWi+1zi\n\ns\n\n(48)\n\nUnder review as a conference paper at ICLR 2023\n\nThen the absolute prediction error (l1 norm) of online adaptation is shown below:\n\net+1 = |yt+1 − ˆf (Vt, zt)| = |yt+1 − Vtzt| = |\n\nN (cid:88)\n\ni=1\n\nWizi\n\nt −\n\nN −1 (cid:88)\n\ni=0\n\nWi+1zi\n\ns · zt|\n\n= |zt| · |\n\nN −1 (cid:88)\n\ni=0\n\nWi+1(zi\n\nt − zi\n\ns)| = |zt| · |\n\nN −1 (cid:88)\n\ni=0\n\ni−1 (cid:88)\n\nWi+1(\n\nj=0\n\nzi−j−1\n\nt\n\nzj s)(zt − zs)|\n\n= |zt| · |zt − zs| · |\n\nN −1 (cid:88)\n\ni=0\n\nWi+1\n\ni−1 (cid:88)\n\nj=0\n\nzi−j−1\n\nt\n\nzj s|\n\n(49)\n\nWe consider the expectation of the prediction error. We have the following property in statistics:\n\nE(zn\n\nt ) =\n\n1 n + 1\n\nfor zt ∼ U(0, 1)\n\nFor simplicity, assume Wi > 0. The expectation of the prediction error is shown below.\n\nE(et+1) = E(zt) · E(|zt − zs|) ·\n\nN −1 (cid:88)\n\ni=0\n\nWi+1\n\ni−1 (cid:88)\n\nj=0\n\nE[zi−j−1\n\nt\n\nzj s]\n\n(50)\n\n(51)\n\nThen we use Eq. (51) to derive the expectation of the error for feedback and feedforward adaptation methods. The core is measurement of E[∥zt − zs∥] and E[zi−j−1\n\nzj s].\n\nt\n\nExpectation of the prediction error for feedback adaptation. In feedback adaptation, zs = zt−1 the current sample zt and the selected sample zs are independent random variables from U(0, 1). We have E(|zt − zs|) = 1 3 (Eq. (35)). Then we derive an expectation of the prediction error for feedback adaptation:\n\nE(ef b\n\nt+1) =\n\n=\n\n≥\n\n1 2\n\n1 6\n\n1 6\n\n·\n\n1 3\n\nN −1 (cid:88)\n\ni=0\n\nWi+1\n\ni−1 (cid:88)\n\nj=0\n\nE[zi−j−1\n\nt\n\nxj\n\ns] =\n\n1 6\n\nN −1 (cid:88)\n\ni=0\n\nWi+1\n\ni−1 (cid:88)\n\nj=0\n\nE[zi−j−1\n\nt\n\n] · E[zj s]\n\nN −1 (cid:88)\n\ni=0\n\nN −1 (cid:88)\n\ni=0\n\nWi+1\n\nWi+1\n\ni−1 (cid:88)\n\nj=0\n\ni−1 (cid:88)\n\nj=0\n\n1 i − j\n\n1 j + 1\n\n2\n\n(i + 1)2 =\n\n1 3\n\nN −1 (cid:88)\n\ni=0\n\nWi+1\n\ni (i + 1)2\n\n≥\n\n1 3\n\nN − 1 N 2\n\nN −1 (cid:88)\n\ni=1\n\nWi+1\n\n(52)\n\nAs can be seen, the prediction error of feedback adaptation cannot converge. The expected prediction error E(ef b\n\nt+1) in each step is lower bounded by a non-zero constant.\n\nExpectation of the prediction error for feedforward adaptation. In feedforward adaptation, the selected samples zs are the most similar samples to the current observation zt. We have E(|zt − zs|) ≤ max( 1 L+1 ) (Eq. (37)). Then we derive an expectation of the prediction error t ,\n\n1\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nfor feedforward adaptation:\n\nE(ef f\n\nt+1) ≤\n\n≤\n\n≤\n\n≤\n\n≤\n\n1 2\n\n1 2\n\n1 2\n\n1 2\n\n1 2\n\nmax(\n\nmax(\n\nmax(\n\nmax(\n\nmax(\n\n1 t\n\n1 t\n\n1 t\n\n1 t\n\n1 t\n\n,\n\n,\n\n,\n\n,\n\n,\n\n1 L + 1\n\n)\n\n1 L + 1\n\n)\n\n1 L + 1\n\n)\n\n1 L + 1\n\n)\n\n1 L + 1\n\n)\n\nN −1 (cid:88)\n\ni=0\n\nN −1 (cid:88)\n\ni=0\n\nN −1 (cid:88)\n\ni=0\n\nN −1 (cid:88)\n\ni=0\n\nN −1 (cid:88)\n\ni=1\n\nWi+1\n\nWi+1\n\nWi+1\n\nWi+1\n\nWi+1\n\ni−1 (cid:88)\n\nj=0\n\ni−1 (cid:88)\n\nj=0\n\ni−1 (cid:88)\n\nj=0\n\ni−1 (cid:88)\n\nj=0\n\nE[zi−j−1\n\nt\n\nzj s]\n\nE[zi−j−1\n\nt\n\nzj t ]\n\nE[zi−1\n\nt\n\n]\n\n1 i\n\n(53)\n\nWhen we consider the final performance,\n\nlim L→∞,t→∞\n\nE(ef f\n\nt+1) ≤\n\nlim L→∞,t→∞\n\n1 2\n\nmax(\n\n1 t\n\n,\n\n1 L + 1\n\n)\n\nN −1 (cid:88)\n\ni=1\n\nWi+1 = 0\n\n(54)\n\nAs can be seen, the expectation of the prediction error of feedforward compensation converges to zero with a large buffer size L. Thus, we proved the Lemma 2 in section 3.4.\n\nExpectation of the prediction error for full (or random) adaptation. For full adaptation or random adaptation, we have E(|zt − zs|) = 1 3 (Eq. (42)), which is the same as feedback adaptation. Then we can see that the expectation of the prediction error for full (or random) adaptation is the same as feedback adaptation:\n\nE(ef l\n\nt+1) =\n\n1 2\n\n·\n\n1 3\n\nN −1 (cid:88)\n\ni=0\n\nWi+1\n\ni−1 (cid:88)\n\nj=0\n\nE[zi−j−1\n\nt\n\nxj s]\n\n≥\n\n1 3\n\nN − 1 N 2\n\nN −1 (cid:88)\n\ni=1\n\nWi+1\n\n(55)\n\nAs can be seen, the prediction error of full adaptation cannot converge. Thus, feedforward adaptation is provably better than full adaptation in the time-invariant polynomial system.\n\nWhen is feedforward better than feedback. In the previous appendix C.2.1, we already know that feedforward adaptation has a smaller error bound than feedback adaptation on approximate time-invariant system (α ≈ 0). In this section, we furtherly prove that for the time-invariant polynomial system, feedforward adaptation has a smaller expected prediction error (not an error bound) than feedback adaptation, full adaptation, and random adaptation methods. As the result, feedforward adaptation is better than feedback adaptation (or full adaptation, random adaptation) for time-invariant or slowly time-variant systems.\n\nD ADDITIONAL EXPERIMENTS\n\nD.1 SYNTHETIC EXPERIMENTS: LINEAR TIME-VARYING FUNCTION\n\nTo evaluate Lemma 1 in section 3.4, we consider the following time-varying function.\n\nzt ∼ U(0, 1)\n\nyt+1 = ft(zt) = sin zt + αt\n\n(56) (57)\n\nOur parameterized prediction model is a one-layer perception with Sigmoid activation function.\n\nˆyt = ˆf (Vt, bt; zt) = S(Vtzt) + bt =\n\n1 1 + e−Vtzt\n\n+ bt\n\n(58)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nWe have The Lipschitz constant K and ˆK for the ground-truth function ft and the one-layer perception ˆf :\n\nK = sup |\n\n| = sup | cos(zt)| = 1\n\n∂ft ∂zt ∂ ˆf ∂zt\n\nˆK = sup(|\n\n|) = sup |Vt · S(Vtzt) · (1 − S(Vtzt)) | = 0.25 sup |Vt|\n\n(59)\n\n(60)\n\nWe use SGD as an optimizer in feedback and feedforward adaptation. During training, we keep the ∥Vt∥ bounded, i.e. ∥Vt∥ ≤ 1, then ˆK = 1 4 . We use Lemma 1 to calculate the error bound for feedback and feedforward adaptation:\n\nE[Bf b\n\ne ] =\n\nE[Bf f\n\ne ] ≤\n\n5 12\n\n+ α\n\n5 4(L + 1)\n\n+ α\n\nL + 1 2\n\nIn feedforward adaptation, when L =\n\n(cid:113) 5\n\n2α − 1, we get the minimum error bound:\n\nE[Bf f\n\ne ] ≤\n\n(cid:114) 5 2\n\nα with L =\n\n(cid:114) 5 2α\n\n− 1\n\n(61)\n\n(62)\n\n(63)\n\nThen we calculate the threshold α⋆. If α ≤ α⋆, feedforward adaptation has a smaller error bound.\n\n(K + ˆK) ≈ 0.089(K + ˆK) ≈ 0.11\n\n√\n\n12\n\n4 −\n\nα⋆ =\n\nL⋆ =\n\n6 (cid:114) 5\n\n2α⋆ − 1 ≈ 3\n\n(64)\n\n(65)\n\nThe experimental evaluation of the above theoretical results is shown in section 4.1.\n\nD.2 SYNTHETIC EXPERIMENTS: TIME-INVARIANT QUADRATIC FUNCTION\n\nTo evaluate Lemma 2 in section 3.4, we consider the following quadratic function with random input.\n\nzt ∼ U(0, 1) yt+1 = ft(zt) = z2\n\nt\n\nOur parameterized prediction model is a linear projection:\n\nˆyt = ˆf (Vt, zt) = Vtzt\n\n(66)\n\n(67)\n\n(68)\n\nSet the buffer size L = 99. According to Lemma 2, the expected prediction error for feedback and feedforward adaptation are:\n\nE[ef b\n\nt+1] ≥\n\n1 3\n\nN − 1 N 2\n\nN −1 (cid:88)\n\ni=1\n\nWi+1 =\n\n1 12\n\nE(ef f\n\nt+1) ≤\n\n1 2\n\nmax(\n\n1 t\n\n,\n\n1 L + 1\n\n)\n\nN −1 (cid:88)\n\ni=1\n\nWi+1 =\n\n1 2\n\nmax(\n\n1 t\n\n,\n\n1 L + 1\n\n) = max(\n\n1 2t\n\n,\n\n1 200\n\n)\n\n(69)\n\n(70)\n\nFig. 3a shows the prediction error for feedback and feedforward adaptation. As can be seen, the error curve for feedback adaptation is not converged. While the error curve for feedforward adaptation converges by the trend of 1 2t . The experimental results are consistent with Eq. (69) and (70). Fig. 3a shows the prediction results for feedforward adaptation. The dashed blue region denotes the estimated uncertainty in section 3.3. In the figure, the ground truth is always within the estimated uncertainty, which validifies the proposed uncertainty estimation.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Error Curve\n\n(b) Prediction of feedforward adaptation\n\nFigure 3: Error curve and prediction results on quadratic function.\n\nD.3 EXPERIMENTAL DESIGN FOR REAL-WORLD EXPERIMENTS\n\nDataset.\n\n• ETT Zhou et al. (2021) dataset contains the data collected from electricity transformers, including load and oil temperature that are recorded every 15 minutes between July 2016 and July 2018. Which consists of two hourly-level datasets (ETTh) and two 15-minute-level datasets (ETTm). In our experiments, we used the first hourly-level dataset ETTh1 as a univariance prediction task. • Exchange-Rate Lai et al. (2018) records the daily exchange rates of eight countries from 1990 to\n\n2016.\n\n• ILI 2 describes the ratio of patients seen with ILI and the total number of patients. Which includes the weekly recorded influenza-like illness (ILI) patients data from the Centers for Disease Control and Prevention of the United States between 2002 and 2021.\n\n• THOR Rudenko et al. (2020) is a public dataset of human motion trajectories, recorded in a controlled indoor experiment. Which includes the motion trajectories with diverse and accurate social human motion data in a shared indoor environment. In our experiments, we use No. 2 ∼ 6 agent’s trajectory as a train set, No. 7 ∼ 8 as a validation set, and No. 9 ∼ 10 agent’s trajectory as a test set.\n\nBackbone models. We include two models as our learnable prediction function ˆf (θ, :): Informer Zhou et al. (2021) and simple MLP (multi-layer perception).\n\n• Informer Zhou et al. (2021) is a widely used transformer-based time-series prediction model.\n\nWhich extends the Transformer with KL-divergence based ProbSparse attention.\n\n• MLP is a simple but robust baseline for time-series prediction. Our MLP consists of 2 layers. The first layer can be considered as Encoder Zt = W · Xt. After the encoder, MLP has a layer normalization, activation function and a final linear projection Yt+1 = V ·Relu(LayerNorm(Zt)). The layer normalization and the final projection can be served as a decoder. Note that, we did not flatten the input for MLP, the expression Zt = W · Xt is a linear layer along the temporal axis. Hyperparameters. For offline training, we follow the strategy in Zeng et al. (2022). In adaptation, we set the learning rate of SGD as η = 0.1 and set the EKF hyperparameters as p0 = 0.1, λ = 1. Buffer size for feedforward adaptation is L = 1000. For ETTh1 dataset, we use I = 192 step recent observations to predict future O = 192 step output data. For the Exchange-Rate dataset, we use I = 96 step recent observations to predict future O = 192 step output data. For the ILI dataset, we use I = 36 step recent observations to predict future O = 36 step output data. For the THOR dataset, we use I = 20 step recent observations to predict future O = 20 step output data. For uncertainty estimation, we set ̃δ = 0, ̃K = 1.\n\nD.4 EXPERIMENTAL RESULTS ON EKF OPTIMIZERS\n\nWe conduct experiments on SGD and EKF as optimizers in adaptation. The descriptions of SGD and EKF are shown in appendix B. In this section, we report the detailed results of the experiment on the\n\n2https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\n\n21\n\n020406080Time Step0.00.10.20.30.40.50.60.7Prediction ErrorFeedbackfeedforward1/2t020406080Time Step0.20.00.20.40.60.81.01.2TargetGroundTruthPredictionUnder review as a conference paper at ICLR 2023\n\nEKF optimizer, as shown in table 2. Similar to results on the SGD on table 1, feedforward adaptation still achieves the best results on all four datasets with EKF optimizer. Specifically, feedforward adaptation outperforms feedback adaptation by 5.9%, 15.1%, 24.1%, and 2.6% in terms of average MSE on four datasets respectively. The performance gain on the ILI dataset is the most obvious. It is reasonable because the incidence rate of common diseases in a region is a slow time-varying system, the future value is mostly dependent on the current input.\n\nTable 2: Performance comparison between the proposed feedforward adaptation method and other baselines with EKF optimization. Avg denotes the average results of two models (MLP and Informer). The last row denotes the performance gain of feedforward adaptation over feedback adaptation.\n\nMethod \\Dataset\n\nETTh1\n\nExchange\n\nILI\n\nTHOR\n\nAdaptation\n\nw/o Adaptation\n\nRandom Adaptation\n\nFeedback Adaptation\n\nFeed Forward Adaptation\n\nModel MLP Informer Avg MLP Informer Avg MLP Informer Avg MLP Informer Avg\n\nGain of Feedforward Over Feedback\n\nMSE MAE MSE MAE MSE MAE MSE MAE 0.117 0.195 0.171 0.211 0.144 0.203 0.149 0.174 0.313 0.191 0.231 0.182 0.106 0.129 0.168 0.109 0.137 0.119 0.103 0.123 0.167 0.102 0.112 0.135 5.9% 1.5% 15.1% 15.6% 24.1% 12.5% 2.6% 1.5%\n\n0.549 1.128 0.839 0.404 0.847 0.625 0.356 0.359 0.357 0.352 0.254 0.303\n\n0.135 0.137 0.136 0.123 0.277 0.200 0.107 0.124 0.116 0.102 0.124 0.113\n\n4.348 4.942 4.645 3.318 3.573 3.445 3.873 4.320 4.110 2.847 3.395 3.121\n\n0.540 0.858 0.699 0.468 0.684 0.576 0.459 0.462 0.460 0.450 0.327 0.388\n\n0.371 0.389 0.380 0.335 0.352 0.343 0.288 0.249 0.268 0.280 0.248 0.264\n\n1.413 1.531 1.472 1.262 1.328 1.295 1.341 1.401 1.371 1.154 1.244 1.199\n\nD.5 STUDY OF THE SAMPLE SELECTION STRATEGY OF DIFFERENT ADAPTATION METHODS\n\nWe will show that the sample selection strategy in the proposed feedforward adaptation method could intrinsically extract the periodicity of the input data. The experiments on the ETTh1 dataset were shown in Fig. 2. In this section, we show the results of ILI, Exchange, THOR datasets. In our experiment, we set the buffer size L = 1000. We use SGD optimizer to adapt the MLP model with different adaptation methods.\n\nFigure 4: Experiments on ILI data. (a) Time series output; (b) FFT period analysis; (c) Time shift t−s between current sample Zt and selected sample Zs in feedback and random adaptation; (d) Time shift t − s in feedforward adaptation; (e) Online prediction error. (f) Sample difference ∥Zt − Zs∥ between current sample Zt and adapted sample Zs.\n\nILI dataset. Figure 4(a) shows the ILI time-series data (1st dimension of the output), and Fig. 4(b) is the FFT (Fast Fourier Transformation) period analysis of the ILI data. As can be seen ETTh1 has roughly T ≈ 50 repetition periods. Random adaptation randomly selects samples from the\n\n22\n\n(a)(b)(c)(d)Under review as a conference paper at ICLR 2023\n\nbuffer, then the time shift between current time-step t and the time-step for selected samples s is (t − s) ∼ [1, L]; Feedback adaptation only selects the last time sample to optimization, then (t − s) = 1. This can be found from the distribution of time shift (t − s) in Fig. 4(c). For the proposed feedforward adaptation in Fig. 4(d), many samples were selected from (t − s) ≈ 50 steps earlier, which corresponds to the repetition period of T ≈ 50. Because feedforward adaptation selects the most similar samples to the current sample, which helps to extract the hidden periodicity of the input signal over time. Thus the distribution of t − s is similar to FFT period analysis.\n\nFigure 5: Experiments on Exchange data. (a) Time series output; (b) FFT period analysis; (c) Time shift t − s between current sample Zt and selected sample Zs in feedback and random adaptation; (d) Time shift t − s in feedforward adaptation; (e) Online prediction error. (f) Sample difference ∥Zt − Zs∥ between current sample Zt and adapted sample Zs.\n\nExchange dataset. Figure 5(a) shows the Exchange time-series data (6th dimension of the output), and Fig. 5(b) is the FFT (Fast Fourier Transformation) period analysis of the Exchange data. As can be seen Exchange has no apparent periodicity. (The intensity of the FFT signal is too low, for example, the intensity of at the T ≈ 90 is about 0.02). In this case, Feedforward adaptation in Fig. 5(d), is likely to select the recent samples like feedback adaptation in Fig. 5(c). Because the most recent sample is a similar sample itself. Even for this kind of no-periodic system, feedforward adaptation still outperforms feedback adaptation, because it has more freedom to select samples. At least feedforward adaptation can select samples exactly the same as feedback adaptation.\n\nFigure 6: Experiments on THOR data. (a) Time series output; (b) FFT period analysis; (c) Time shift t − s between current sample Zt and selected sample Zs in feedback and random adaptation; (d) Time shift t − s in feedforward adaptation; (e) Online prediction error. (f) Sample difference ∥Zt − Zs∥ between current sample Zt and adapted sample Zs.\n\nTHOR dataset. Figure 6(a) shows the THOR time-series data (1st dimension of the output), and Fig. 6(b) is the FFT (Fast Fourier Transformation) period analysis of the THOR data. As can be seen\n\n23\n\n(a)(b)(c)(d)(a)(b)(c)(d)Under review as a conference paper at ICLR 2023\n\nTHOR has roughly T ≈ 1200 repetition periods. But the buffer size is L = 1000 < T . In this case, we only store the recent L = 1000 samples in the buffer, but the period is more largerT ≈ 1200 > L, so the feedforward adaptation cannot extract the periodicity. In this case, Feedforward adaptation Fig. 6(d), is likely to select the recent samples like feedback adaptation in Fig. 6(c). Because the most recent sample is a similar sample itself within the buffer. In the future, we will investigate the more efficient sampling and buffer storing strategy to extract the very long time-dependency and periods.\n\nD.6 PREDICTION OUTPUT AND UNCERTAINTY\n\n(a) Prediction on ETTh1 dataset\n\n(b) Prediction on ILI dataset\n\n(c) Prediction on Exchange dataset\n\n(d) Prediction on THOR dataset\n\nFigure 7: Prediction output and uncertainty estimation of the proposed feedforward adaptation on (a) ETTh1 dataset, (b) ILI dataset (1st dimension of the output), (c) Exchange dataset (6th dimension of the output), and (d) THOR dataset (1st dimension of the output). The blue dashed region represents the estimated uncertainty of the prediction.\n\nOne of the advantages of the proposed feedforward adaptation is it could provide uncertainty estimation as shown in algorithm 2. Figure 7 shows the prediction output (blue curve), ground truth label (red curve), and uncertainty estimation (blue dashed region) on four different datasets. As can be seen, in most cases, the estimated uncertainty covers the real ground truth value. Which validates the effectiveness of the proposed uncertainty estimation.\n\n24\n\n5001000150020002500Time step2.52.01.51.00.50.0Prediction OutputGroundTruthPrediction20406080100120140Time step420246Prediction OutputGroundTruthPrediction20040060080010001200Time step10123Prediction OutputGroundTruthPrediction200040006000800010000Time step105051015Prediction OutputGroundTruthPrediction",
    "reference": "# Summary Of The Paper\n\nThis paper studies the problem of using online adaptation method in time-series prediction tasks. Existing approaches that feedback based on the latest prediction errors have the risk of forgetting past information. To address this challenge, the authors of this paper propose an approach that uses the critical data samples from the memory buffer to optimize the prediction model. The authors derive a bound of the prediction error, which gives the insight that the proposed approach will perform well if the ground-truth prediction functions are time-varying at a very slow rate. They also use experiments to show the proposed method performs better than existing approaches with and without adaptation.\n\n# Strength And Weaknesses\n\nStrength:\n\nThis paper studies an interesting question that is impactful for the fields of online learning and online control.  The intuition behind the proposed change to existing online adaptation methods is convincing, and some intuitions have been verified in the first synthetic experiment. Specifically, I like the plot in Figure 1 (a) that shows how the performance of the proposed approach depends on the time-varying shift of the function.\n\nWeakness:\n\nI feel the major weakness of this work is on the theoretical part. If I understand correctly, the error bound in equation (8) is just an upper bound of the prediction error $e_{t+1}$. I don’t think one can claim feedforward is better than feedback because the $E[B_e^{ff}] \\leq E[B_e^{fb}]$, because such claim can only be made when an upper bound is smaller than a lower bound. And it is also worth noticing that the proof only works for a specific special case. These two factors make me feel the theoretical guarantee is very weak.\n\nAnother weakness of this work is on the algorithm design. It has been shown both from the theory and the simulation that the proposed feedforward approach is prone to $\\delta$, which characterize how fast the ground-truth prediction function changes. In other words, one needs to decide whether to use the feedforward approach based on $\\delta$. However, the value of $\\delta$ is not known before the online prediction process starts. Even if one can switch algorithms in the middle based on the historical experience, it is unclear how to estimate $\\delta$ because the ground-truth prediction functions are unknown.\n\nFor the experiments, I’m uncertain about whether the improvement made on the real-world datasets are significant.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is well-written and easy to follow in general. However, a clarity issue in Algorithm 1 is that $s$ in line 4 is not defined. The authors might want to add a clarification that $s$ is chosen based on some rules.\n\n# Summary Of The Review\n\nIn summary, I feel the contributions made in this work is not significant enough for a conference publication. The theoretical contributions are not very meaningful, and the novelty of algorithm design is limited. I'm uncertain about how significant the empirical improvements on the real-world datasets are. Therefore, I would vote for reject with a low confidence score.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nGENERALIZING AND DECOUPLING NEURAL COLLAPSE VIA HYPERSPHERICAL UNIFORMITY GAP\n\nWeiyang Liu1,2,*, Longhui Yu3,*, Adrian Weller2,4, Bernhard Schölkopf1 1Max Planck Institute for Intelligent Systems - Tübingen 3Peking University\n\n4The Alan Turing Institute\n\n2University of Cambridge\n\nABSTRACT\n\nThe neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both crossentropy loss and mean square error can provably lead to NC. We remove NC’s key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective – hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity. HUG not only provably converges to GNC, but also decouples GNC into two separate objectives. Unlike cross-entropy loss that couples intra-class compactness and inter-class separability, HUG enjoys more flexibility and serves as a good alternative loss function. Empirical results show that HUG works well in terms of generalization and robustness.\n\n1\n\nINTRODUCTION\n\nRecent years have witnessed the great success of deep representation learning in a variety of applications ranging from computer vision [37], natural language processing [16] to game playing [55, 64]. Despite such a success, how deep representations can generalize to unseen scenarios and when they might fail remain a black box. Deep representations are typically learned by a multi-layer network with cross-entropy (CE) loss optimized by stochastic gradient descent. In this simple setup, [86] has shown that zero loss can be achieved even with arbitrary label assignment. After continuing to train the neural network past zero loss with CE, [60] discovers an intriguing phenomenon called neural collapse (NC). NC can be summarized as the following characteristics:\n\n• Intra-class variability collapse: Intra-class variability of last-layer features collapses to zero,\n\nindicating that all the features of the same class concentrate to their intra-class feature mean.\n\n• Convergence to simplex ETF: After being centered at their global mean, the class-means are both linearly separable and maximally distant on a hypersphere. Formally, the class-means form a simplex equiangular tight frame (ETF) which is a symmetric structure defined by a set of maximally distant and pair-wise equiangular points on a hypersphere.\n\n• Convergence to self-duality: The linear classifiers, which live in the dual vector space to that of\n\nthe class-means, converge to their corresponding class-mean and also form a simplex ETF.\n\n• Nearest decision rule: The linear classifiers behave like nearest class-mean classifiers.\n\nThe NC phenomenon suggests two general principles for deeply learned features and classifiers: minimal intra-class compactness of features (i.e., features of the same class collapse to a single point), and maximal inter-class separability of classifiers / feature mean (i.e., classifiers of different classes have maximal angular margins). While these two principles are largely independent, popular loss functions such as CE and square error (MSE) completely couple these two principles together. Since there is no trivial way for CE and MSE to decouple these two principles, we identify a novel quantity – hyperspherical uniformity gap (HUG), which not only characterizes intra-class feature compactness and inter-class classifier separability as a whole, but also fully decouples these two\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nprinciples. The decoupling enables HUG to separately model intra-class compactness and inter-class separability, making it highly flexible. More importantly, HUG can be directly optimized and used to train neural networks, serving as an alternative loss function in place of CE and MSE for classification. HUG is formulated as the difference between inter-class and intra-class hyperspherical uniformity. Hyperspherical uniformity [48] quantifies the uniformity of a set of vectors on a hypersphere and is used to capture how diverse these vectors are on a hypersphere. Thanks to the flexibility of HUG, we are able to use many different formulations to characterize hyperspherical uniformity, including (but not limited to) minimum hyperspherical energy (MHE) [45], maximum hyperspherical separation (MHS) [48] and maximum gram determinant (MGD) [48]. Different formulations yield different interpretation and optimization difficulty (e.g., HUG with MHE is easy to optimize, HUG with MGD has interesting connection to geometric volume), thus leading to different performance.\n\nSimilar to CE loss, HUG also provably leads to NC under the setting of unconstrained features [53]. Going beyond NC, we hypothesize a generalized NC (GNC) with hyperspherical uniformity, which extends the original NC to the scenario where there is no constraint for the number of classes and the feature dimension. NC requires the feature dimension no smaller than the number of classes while GNC no longer requires this. We further prove that HUG also leads to GNC at its objective minimum.\n\nAnother motivation behind HUG comes from the classic Fisher discriminant analysis (FDA) [19] where the basic idea is to find a projection matrix T that maximizes between-class variance and minimizes within-class variance. What if we directly optimize the input data (without any projection) rather than optimizing the linear projection in FDA? We make a simple derivation below:\n\nProjection FDA: max\n\nT ∈Rd×r\n\n(cid:18)(cid:16)\n\ntr\n\nT ⊤SwT\n\n(cid:17)−1\n\nT ⊤SbT\n\n(cid:19)\n\nData FDA:\n\nmax x1,··· ,xn∈Sd−1\n\ntr (Sb) − tr (Sw)\n\ni=1\n\n(cid:80)\n\nwhere the between-class scatter matrix is Sw = (cid:80)C (xj − μi)(xj − μi)⊤, the within-class scatter matrix is Sb = (cid:80)C i=1 ni(μi − ̄μ)(μi − ̄μ)⊤, ni is the number of samples in the i-th class, n is the total number of samples, μi = n−1 j=1 xj is the global mean. By considering class-balanced data on the unit hypersphere, optimizing data FDA is equivalent to simultaneously maximizing tr(Sb) and minimizing tr(Sw). Maximizing tr(Sb) encourages inter-class separability and is a necessary condition for hyperspherical uniformity.1 Minimizing tr(Sw) encourages intra-class feature collapse, reducing intra-class variability. Therefore, HUG can be viewed a generalized FDA criterion for learning maximally discriminative features.\n\nxj is the i-th class-mean, and ̄μ = n−1 (cid:80)n\n\nj∈Ac\n\nj∈Ac\n\n(cid:80)\n\ni\n\nHowever, one may ask the following questions: Why is HUG useful if we already have the FDA criterion? Could we simply optimize data FDA? In fact, the FDA criterion has many degenerate solutions. For example, we consider a scenario of 10-class balanced data where all features from the first 5 classes collapse to the north pole on the unit hypersphere and features from the rest 5 classes collapse to the south pole on the unit hypersphere. In this case, tr(Sw) is already minimized since it achieves the minimum zero. tr(Sb) also achieves its maximum n at the same time. In contrast, HUG naturally generalizes FDA without having these degenerate solutions and serves as a more reliable criterion for training neural networks. We summarize our contributions below:\n\n• We decouple the NC phenomenon into two separate learning objectives: maximal inter-class separability (i.e., maximally distant class feature mean and classifiers on the hypersphere) and minimal intra-class variability (i.e., intra-class features collapse to a single point on the hypersphere).\n\n• Based on the two principled objectives induced by NC, we hypothesize the generalized NC which generalizes NC by dropping the constraint on the feature dimension and the number of classes.\n\n• We identify a general quantity called hyperspherical uniformity gap, which well characterizes both inter-class separability and intra-class variability. Different from the widely used CE loss, HUG naturally decouples both principles and thus enjoys better modeling flexibility.\n\n• Under the HUG framework, we consider three different choices for characterizing hyperspherical uniformity: minimum hyperspherical energy, maximum hyperspherical separation and maximum Gram determinant. HUG provides a unified framework for using different characterizations of hyperspherical uniformity to design new loss functions.\n\n1We first obtain the upper bound n of tr(Sb) from tr(Sb) = (cid:80)C\n\ni=1 ni∥μi∥ · ∥ ̄μ∥ ≤ n. i=1 achieving hyperspherical uniformity has Eμ1,··· ,μn {∥ ̄μ∥} → 0 (as n grows Because a set of vectors {μi}n larger) [20]. Then we have that tr(Sb) attains n. Therefore, vectors achieving hyperspherical uniformity are one of its maximizers. tr(Sw) can simultaneously attain its minimum if intra-class features collapse to a single point.\n\ni=1 ni∥μi − ̄μ∥2\n\nF ≤ (cid:80)C\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 ON GENERALIZING AND DECOUPLING NEURAL COLLAPSE\n\nNC describes an intriguing phenomenon for the distribution of last-layer features and classifiers in overly-trained neural networks, where both features and classifiers converge to ETF. However, ETF can only exist when the feature dimension d and the number of classes C satisfy d ≥ C − 1. This is not always true for deep neural networks. For example, neural networks for face recognition are usually trained by classifying large number of classes (e.g., more than 85K classes in [23]), and the feature dimension (e.g., 512 in SphereFace [43]) is usually much smaller than the number of classes. In general, when the number of classes is already large, it is prohibitive to use a larger feature dimension. Thus a question arises: what will happen in this case if a neural network is fully trained?\n\nMotivated by this question, we conduct a simple experiment to simulate the case of d ≥ C − 1 and the case of d < C − 1. Specifically, we train a convolutional neural network (CNN) on MNIST with feature dimension 2. For the case of d ≥ C − 1, we use only 3 classes (digit 0,1,2) as the training set. For the case of d < C − 1, we use all 10 classes as the training set. We visualize the learned features of both cases in Figure 1. The results verify the case of d ≥ C − 1 indeed approaches to NC, and ETF does not exist in the case of d < C − 1. Interestingly, one can observe that learned features in both cases approach to the configuration of equally spaced frames on the hypersphere. To accommodate the case of d < C − 1, we extend NC to the generalized NC by hypothesizing that last-layer inter-class features and classifiers converge to equally spaced points on the hypersphere, which can be characterized by hyperspherical uniformity.\n\nFigure 1: 2D learned feature visualization on MNIST. The features are inherently 2-dimensional and are plotted without visualization tools. (a) Case 1: d = 2, C = 3; (b) Case 2: d = 2, C = 10.\n\nGeneralized Neural Collapse (GNC) We define the feature global mean as μG = Avei,cxi,c where xi,c ∈ Rd is the last-layer feature of the i-th sample in the c-th class, the feature class-mean as μc = Aveixi,c for different classes c ∈ {1, · · · , C}, the feature within-class covariance as ΣW = Avei,c(xi,c − μc)(xi,c − μc)⊤ and the feature between-class covariance as ΣB = Avec(μc − μG)(μc − μG)⊤. GNC states that\n\n• (1) Intra-class variability collapse: Intra-class variability of last-layer features collapse to zero, indicating that all the features of the same class converge to their intra-class feature mean. Formally, GNC has that Σ†\n\nBΣW → 0 where † denotes the Moore-Penrose pseudoinverse.\n\n• (2) Convergence to hyperspherical uniformity: After being centered at their global mean, the class-means are both linearly separable and maximally distant on a hypersphere. Formally, the class-means converge to equally spaced points on a hypersphere, i.e.,\n\n(cid:88)\n\nc̸=c′\n\nK( ˆμc, ˆμc′ ) → min\n\nˆμ1,··· , ˆμC\n\n(cid:88)\n\nc̸=c′\n\nK( ˆμc, ˆμc′ ),\n\n∥μc − μG∥ − ∥μc′ − μG∥ → 0, ∀c ̸= c′\n\n(1)\n\nwhere ˆμi = ∥μi − μG∥−1(μi − μG) and K(·, ·) is a kernel function that models pairwise interaction. Typically, we consider Riesz s-kernel Ks( ˆμc, ˆμc′) = sign(s) · ∥ ˆμc − ˆμc′∥−s or logarithmic kernel Klog( ˆμc, ˆμc′) = log ∥ ˆμc − ˆμc′∥−1. For example, the Riesz s-kernel with s = d − 2 is a variational characterization of hyperspherical uniformity (e.g., hyperspherical energy [45]) using Newtonian potentials. In the case of d = 3, s = 1, the Riesz kernel is called Coulomb potential and the problem of finding minimal coulomb energy is called Thomson problem [70].\n\n• (3) Convergence to self-duality: The linear classifiers, which live in the dual vector space to that of the class-means, converge to their corresponding class-means, leading to hyperspherical uniformity. Formally, GNC has that ∥wc∥−1wc − ˆμc → 0 where wc ∈ Rd is the c-th classifier. • (4) Nearest decision rule: The learned linear classifiers behave like the nearest class-mean\n\nclassifiers. Formally, GNC has that arg maxc⟨wc, x⟩ + bc → arg minc ∥x − μc∥.\n\nIn contrast to NC, GNC further considers the case of d < C − 1 and hypothesizes that both feature class-means and classifiers converge to hyperspherically uniform point configuration that minimizes some form of pairwise potentials. Similar to how NC connects tight frame theory [74] to deep learning, our GNC hypothesis connects potential theory [3] to deep learning, which may shed new light on understanding it. We show in Theorem 1 that GNC reduces to NC in the case of d ≥ C − 1.\n\n3\n\n(a) 2D feature with 3 classes(b) 2D feature with 10 classes-6-4-20246-6-4-20246-6-4-202-6-4-2020120123456789Published as a conference paper at ICLR 2023\n\nTheorem 1 (Regular Simplex Optimum for GNC) Let f : (0, 4] → R be a convex and decreasing function defined at v = 0 by limv→0+ f (v). If 2 ≤ C ≤ d + 1, then we have that the vertices of regular (C − 1)-simplices inscribed in Sd−1 with centers at the origin (equivalent to simplex ETF) minimize the hyperspherical energy (cid:80) c̸=c′ K( ˆμc, ˆμc′) on the unit hypersphere Sd−1 (d ≥ 3) with the kernel as K( ˆμc, ˆμc′) = f (∥ ˆμc − ˆμc′∥2). If f is strictly convex and strictly decreasing, then these are the only energy minimizing C-point configurations. Thus GNC reduces to NC when d ≥ C − 1.\n\nWe note that Theorem 2 guarantees the simplex ETF as the minimizer of a general family of hyperspherical energies (as long as f is convex and decreasing). This suggests that there are many possible kernel functions K(·, ·) in GNC that can effectively generalize NC. The case of d < C − 1 is where GNC really gets interesting but complicated. Other than the regular simplex case, we also highlight a special uniformity case of 2d = C. In this case, we can prove in Theorem 2 that GNC(2) converges to the vertices of a cross-polytope as hyperspherical energy gets minimized. As the number of classes gets infinitely large, we show in Theorem 3 that GNC(2) leads to a point configuration that is uniformly distributed on Sd−1. Additionally, we show a simple yet interesting result in Proposition 1 that the last-layer classifiers are already initialized to be uniformly distributed on the hypersphere in practice.\n\nFigure 2: Geometric illustration in R3 of (a) regular simplex optimum (equivalent to simplex ETF in NC) and (b) cross-polytope optimum in GNC.\n\nTheorem 2 (Cross-polytope Optimum for GNC) If C = 2d, then the vertices of the cross-polytope are the minimizer of the hyperspherical energy in GNC(2).\n\nThe cross-polytope optimum for GNC(2) is in fact quite intuitive, because it corresponds to the Cartesian coordinate system (up to a rotation). For example, the vertices of the unit cross-polytope in R3 are (±1, 0, 0), (0, ±1, 0), (0, 0, ±1). These 6 vectors minimize the hyperspherical energy on S2. We illustrate both the regular simplex and cross-polytope cases in Figure 2. For the other cases of d < C − 1, there exists generally no simple and universal point structure that minimizes the hyperspherical energy, as heavily studied in [12, 27, 38, 63]. For the point configurations that asymptotically minimize the hyperspherical energy as C grows larger, Theorem 3 can guarantee that these configurations asymptotically converge to a uniform distribution on the hypersphere.\n\nTheorem 3 (Asymptotic Convergence to Hyperspherical Uniformity) Consider a sequence of point configurations { ˆμC C=2 that asymptotically minimizes the hyperspherical energy on Sd−1 as C → ∞, then { ˆμC C}∞\n\nC=2 is uniformly distributed on the hypersphere Sd−1.\n\nC}∞ 1 , · · · , ˆμC\n\n1 , · · · , ˆμC\n\nProposition 1 (Minimum Energy Initialization) With zero-mean Gaussian initialization (e.g., [22, 28]), the C last-layer classifiers of neural networks are initialized as a uniform distribution on the hypersphere. The expected initial energy is C(C − 1) (cid:82) Sd−1 ∥ ˆμc − ˆμc′∥−2dσd−1( ˆμc)dσd−1( ˆμc′).\n\nSd−1\n\n(cid:82)\n\nWith Proposition 1, one can expect that the hyperspherical energy of the last-layer classifiers will first increase and then decrease to a lower value than the initial energy. To validate the effectiveness of our GNC hypothesis, we conduct a few experiments to show how both class feature means and classifiers converge to hyperspherical uniformity (i.e., minimizing the hyperspherical energy), and how intra-class feature variability collapses to almost zero. We start with an intuitive understanding about GNC from Figure 1. The results are directly produced by the learned features without any visualization tool (such as t-SNE [73]), so the feature distribution can reflect the underlying one learned by neural networks. We observe that GNC is attained in both d < C − 1 and d ≥ C − 1, while NC is violated in d < C − 1 since the learned feature class-means can no longer form a simplex ETF. To see whether the same conclusion holds for higher feature dimensions, we also train two CNNs on CIFAR-100 with feature dimension as 64 and 128, respectively. The results are given in Figure 3.\n\nFigure 3 shows that GNC captures well the underlying convergence of the neural network training. Figure 3(a,c) shows that the hyperspherical energy of feature class-means and classifiers converge to a small value, verifying the correctness of GNC(2) and GNC(3) which indicate both feature class-means and classifiers converge to hyperspherical uniformity. More interestingly, in the MNIST experiment, we can compute the exact minimal energy on S1: 2 in the case of d = 2, C = 3 (1/3 for average energy) and ≈ 82.5 in the case of d = 2, C = 10 (≈ 0.917 for average energy). The final average energy in Figure 3(a) matches our theoretical minimum well. From Figure 3(c), we observe that the classifier energy stays close to its minimum at the very beginning, which matches our Proposition 1\n\n4\n\nv1v2v3-v3-v2-v1v1v2v3v4(a) Regular simplex(b) Cross-polytopePublished as a conference paper at ICLR 2023\n\nFigure 3: Training dynamics of hyperspherical energy (which captures inter-class separability) and hyperspherical reverse-energy (which captures intra-class variability). (a,b) MNIST with d = 2, C = 10 and d = 2, C = 3. (c,d) CIFAR-100 with d = 64, C = 100 and d = 128, C = 100.\n\nthat vectors initialized with zero-mean Gaussian are uniformly distributed over the hypersphere (this phenomenon becomes more obvious in higher dimensions). To evaluate the intra-class feature variability, we consider a hyperspherical reverse-energy Er = (cid:80) ∥ ˆxi − ˆxj∥ where ˆxi = xi ∥xi∥ and Ac denotes the sample index set of the c-th class. The smaller this reverse-energy gets, the less intra-class variability it implies. Figure 3(b,d) shows that the intra-class feature variability approaches to zero, as GNC(1) suggests. Details and more empirical results on GNC are in given Appendix A.\n\ni̸=j∈Ac\n\nNow we discuss how to decouple the GNC hypothesis and how such a decoupling can enable us to design new objectives to train neural networks. GNC(1) and GNC(2) suggest to minimize intra-class feature variability and maximize inter-class feature separability, respectively. GNC(3) and GNC(4) are natural consequences if GNC(1) and GNC(2) hold. It has long been discovered in [42, 68, 82] that last-layer classifiers serve as proxies to represent the corresponding class of features, and they are also an approximation to the feature class-means. GNC(3) indicates the classifiers converge to hyperspherical uniformity, which, together with GNC(1), implies GNC(4).\n\nUntil now, it has been clear that GNC really boils down to two decoupled objectives: maximize inter-class separability and minimize intra-class variability, which again echos the goal of FDA. The problem reduces to how to effectively characterize these two objectives while being decoupled for flexibility (unlike CE or MSE). In the next section, we propose to address this problem by characterizing both objectives with a unified quantity - hyperspherical uniformity.\n\n3 HYPERSPHERICAL UNIFORMITY GAP\n\n3.1 GENERAL FRAMEWORK\n\nAs GNC(2) suggests, the inter-class separability is well captured by hyperspherical uniformity of feature class-means, so it is natural to directly use it as a learning target. On the other hand, GNC(1) does not suggest any easy-to-use quantity to characterize intra-class variability. We note that minimizing intra-class variability is actully equivalent to encouraging features of the same class to concentrate on a single point, which is the opposite of hyperspherical uniformity. Therefore, we can unify both intra-class variability and inter-class separability with a single characterization of hyperspherical uniformity. We propose to maximize the hyperspherical uniformity gap:\n\nmax { ˆxj }n\n\nj=1\n\nLHUG := α ·\n\nHU(cid:0){ ˆμc}C (cid:124)\n\nc=1\n\n(cid:1)\n\n(cid:123)(cid:122) Tb: Inter-class Hyperspherical Uniformity\n\n(cid:125)\n\n−β ·\n\nC (cid:88)\n\nc=1\n\nHU(cid:0){ ˆxi}i∈Ac (cid:124) (cid:123)(cid:122) Tw: Intra-class Hyperspherical Uniformity\n\n(cid:1)\n\n(cid:125)\n\n(2)\n\nc∈Ac\n\nwhere α, β are hyperparameters, ˆμc = μc ∥μc∥ is the feature class-mean projected on the unit hypersphere, μc = (cid:80) xc is the feature class-mean, xi is the last-layer feature of the i-th sample and Ac denotes the sample index set of the c-th class. HU({vi}m i=1) denotes some measure of hyperspherical uniformity for vectors {v1, · · · , vm}. Eq. 2 is the general objective for HUG. Without loss of generality, we assume that the larger it gets, the stronger hyperspherical uniformity we have. We mostly focus on supervised learning with parameteric class proxies2 where the CE loss is widely used as a de facto choice, although HUG can be used in much broader settings as discussed later. In the HUG framework, there is no longer a clear notion of classifiers (unlike the CE loss), but we still can utilize class proxies (i.e., a generalized concept of classifiers) to facilitate the optimization.\n\nWe observe that Eq. 2 directly optimizes the feature class-means for inter-class separability, but they are intractable to compute during training (we need to compute them in every iteration). Therefore it is nontrivial to optimize the original HUG for training neural networks. A naive solution is to\n\n2Parametric class proxies are a set of parameters used to represent a group of samples in the same class.\n\nTherefore, these proxies store the information about a class. Last-layer classifiers are a typical example.\n\n5\n\nIterationAverage Hyperspherical Energy(a) Inter-class separability on MNISTIterationAverage Reverse-Energy(b) Intra-class variability on MNISTIterationAverage Hyperspherical Energy(c) Inter-class separability on CIFAR-100IterationAverage Reverse-Energy(d) Intra-class variability on CIFAR-1000500100015002000051015Classifier EnergyClass-mean Energy05010015020025001020Classifier EnergyClass-mean Energyd=2, C=10d=2, C=310002000300040005000600000.20.40.6Feature Reverse-energyFeature-mean Reverse-energy50010001500200000.20.4Feature Reverse-energyFeature-mean Reverse-energy0500100015000.511.52Classifier EnergyClass-mean Energy020040060080010000.511.52Classifier EnergyClass-mean Energy10002000300040005000600070000.40.60.811.2Feature Reverse-energyFeature-mean Reverse-energy10002000300040005000600070000.40.60.81Feature Reverse-energyFeature-mean Reverse-energyd=2, C=10d=2, C=3d=128, C=100d=64, C=100d=128, C=100d=64, C=100Published as a conference paper at ICLR 2023\n\napproximate feature class-mean with a few mini-batches such that the gradients of Tb can be still back-propagated to the last-layer features. However, it may take many mini-batches in order to obtain a sufficiently accurate class-mean, and the approximation gets much more difficult with large number of classes. To address this, we employ parametric class proxies to act as representatives of intra-class features and optimize them instead of feature class-means. We thus modify the HUG objective as\n\nmax\n\n{ ˆxj }n\n\nj=1,{ ˆwc}C\n\nc=1\n\nLP-HUG := α ·\n\nHU(cid:0){ ˆwc}C (cid:124)\n\nc=1\n\n(cid:1)\n\n(cid:123)(cid:122) Inter-class Hyperspherical Uniformity\n\n(cid:125)\n\n−β ·\n\nC (cid:88)\n\nc=1\n\nHU(cid:0){ ˆxi}i∈Ac , ˆwc (cid:124) (cid:123)(cid:122) Intra-class Hyperspherical Uniformity\n\n(cid:1)\n\n(cid:125)\n\n(3)\n\nwhere ˆwc ∈ Sd−1 is the parametric proxy for the c-th class. The intra-class hyperspherical uniformity term connects the class proxies with features by minimizing their joint hyperspherical uniformity, guiding features to move towards their corresponding class proxy. When training a neural network, the objective function in Eq. 3 will optimize network weights and proxies together. There are alternative ways to design the HUG loss from Eq. 2 for different learning scenarios, as discussed in Appendix C.\n\nLearnable proxies. We can view the class proxy ˆμi as learnable parameters and update them with stochastic gradients, similarly to the parameters of neural networks. In fact, learnable proxies play a role similar to the last-layer classifiers in the CE loss, improving the optimization by aggregating intra-class features. The major difference between learnable proxies and moving-averaged proxies is the way we update them. As GNC(3) implies, class proxies in HUG can also be used as classifiers.\n\nStatic proxies. Eq. 3 is decoupled into maximal inter-class separability and minimal intra-class variability. These two objects are independent and do not affect each other. We can thus optimize them independently. This suggests a even simpler way to assign class proxies – initializing class proxies with prespecified points that have attained hyperspherical uniformity, and fixing them in the training. There are two simple ways to obtain these class proxies: (1) minimizing their hyperspherical energy beforehand; (2) using zero-mean Gaussian to initialize the class proxies (Proposition 1). After initialization, class proxies will stay fixed and the features are optimized towards their class proxies.\n\nPartially learnable proxies. After the class proxies are initialized using the static way above, we can increase its flexibility by learning an orthogonal matrix for the class proxies to find a suitable orientation for them. Specifically, we can learn this orthogonal matrix using methods in [47].\n\n3.2 VARIATIONAL CHARACTERIZATION OF HYPERSPHERICAL UNIFORMITY\n\nWhile there exist many ways to measure hyperspherical uniformity, we seek variational characterization due to simplicity. As examples, we consider minimum hyperspherical energy [45] that is inspired by Thomson problem [66, 70] and minimizes the potential energy, maximum hyperspherical separation [48] that is inspired by Tammes problem [69] and maximizes the smallest pairwise distance, and maximum gram determinant [48] that is defined by the volume of the formed parallelotope.\n\nMinimum hyperspherical energy. MHE seeks to find an equilibrium state with minimum potential energy that distributes n electrons on a unit hypersphere as evenly as possible. Hyperspherical uniformity is characterized by minimizing the hyperspherical energy for n vectors Vn = {v1, · · · , vn ∈ Rd}:\n\nmin { ˆv1,··· , ˆvn∈Sd−1}\n\n(cid:26)\n\nEs( ˆVn) :=\n\nn (cid:88)\n\nn (cid:88)\n\n(cid:27)\n\nKs(ˆvi, ˆvj)\n\n, Ks(ˆvi, ˆvj) =\n\ni=1\n\nj=1,j̸=i\n\n(cid:26) ∥ˆvi − ˆvj∥−s, s > 0\n\n−∥ˆvi − ˆvj∥−s, s < 0\n\n, (4)\n\nwhere ˆvi := vi apply MHE to HUG and formulate the new objective as follows (sb = 2, sw = −1):\n\n∥vi∥ is the i-th vector projected onto the unit hypersphere. With HU( ˆV ) = −Es( ˆV ), we\n\nmin\n\n{ ˆxj }n\n\nj=1,{ ˆwc}C\n\nc=1\n\nLMHE-HUG := α · Esb\n\n(cid:0){ ˆwc}C\n\nc=1\n\n(cid:1) − β ·\n\nC (cid:88)\n\nc=1\n\nEsw\n\n(cid:0){ ˆxi}i∈Ac , ˆwc\n\n(cid:1)\n\n(5)\n\nwhich can already be used as to train neural networks. The intra-class variability term in Eq. 5 can be relaxed to a upper bound such that we can instead minimize a simple upper bound of LMHE-HUG:\n\nL′\n\nMHE-HUG := α ·\n\n(cid:88)\n\nc̸=c′\n\n∥ ˆwc − ˆwc′ ∥−2 + β′ ·\n\n(cid:88)\n\n(cid:88)\n\nc\n\ni∈Ac\n\n∥ ˆxi − ˆwc∥ ≥ LMHE-HUG\n\n(6)\n\nwhich is much more efficient to compute in practice and thus can serve as a relaxed HUG objective. Moreover, LMHE-HUG and L′ MHE-HUG share the same minimizer. Detailed derivation is in Appendix H.\n\nMaximum hyperspherical separation. MHS uses a maximum geodesic separation principle by maximizing the separation distance θ( ˆVn) (i.e., the smallest pairwise distance in Vn = {v1, · · · , vn ∈\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nRd}): max ˆV {θ( ˆVn) := mini̸=j ∥ˆvi − ˆvj∥}. Because θ( ˆVn) is another variational definition, we cannot naively set HU(·) = θ(·). We define θ−1( ˆVn) := maxi̸=j ∥ˆvi − ˆvj∥ and HUG becomes\n\nmax\n\n{ ˆxj }n\n\nj=1,{ ˆwc}C\n\nc=1\n\nLMHS-HUG := α · θ(cid:0){ ˆwc}C\n\nc=1\n\n(cid:1) − β ·\n\nC (cid:88)\n\nc=1\n\nθ−1(cid:0){ ˆxi}i∈Ac , ˆwc\n\n(cid:1),\n\nwhich, by replacing intra-class variability with its surrogate, results in a more efficient form:\n\nL′\n\nMHS-HUG := α · min c̸=c′\n\n∥ ˆwc − ˆwc′ ∥ − β ·\n\n(cid:88)\n\nc\n\nmax i∈Ac\n\n∥ ˆxi − ˆwc∥\n\n(7)\n\n(8)\n\nwhich is a max-min optimization with a simple nearest neighbor problem inside. We note that LMHS-HUG and L′\n\nMHS-HUG share the same maximizer. Detailed derivation is given in Appendix H.\n\nMaximum gram determinant. MGD characterizes the uniformity by computing a proxy to the volume of the parallelotope spanned by the vectors. MGD is defined with kernel gram determinant:\n\nmax { ˆv1,··· , ˆvn∈Sd−1}\n\nlog det (cid:0)G := (cid:0)K(ˆvi, ˆvj)(cid:1)n\n\ni,j=1\n\n(cid:1), K(ˆvi, ˆvj) = exp (cid:0) − ε2∥ˆvi − ˆvj∥2(cid:1)\n\n(9)\n\nwhere we use a Gaussian kernel with parameter ε and G( ˆVn) is the kernel gram matrix for ˆVn = {ˆv1, · · · , ˆvn}. With HU( ˆVn) = det G( ˆVn), minimizing intra-class uniformity cannot be achieved by minimizing det G( ˆVn), since det G( ˆVn) = 0 only leads to linear dependence. Then we have\n\nmax\n\n{ ˆxj }n\n\nj=1,{ ˆwc}C\n\nc=1\n\nLMGD-HUG := α · log det (cid:0)G({ ˆwc}C\n\nc=1)(cid:1) + β′ ·\n\n(cid:88)\n\n(cid:88)\n\nc\n\ni∈Ac\n\n∥ ˆxi − ˆwc∥\n\n(10)\n\nwhere we directly use the surrogate loss from Eq. 6 as the intra-class variability term. With MGD, HUG has interesting geometric interpretation – it encourages the volume spanned by class proxies to be as large as possible and the volume spanned by intra-class features to be as small as possible.\n\n3.3 THEORETICAL INSIGHTS AND DISCUSSIONS\n\nThere are many interesting theoretical questions concerning HUG, and this framework is highly related to a few topics in mathematics, such as tight frame theory [74], potential theory [39], sphere packing and covering [3, 18, 25]. The depth and breath of these topics are beyond imagination. In this section, we focus on discussing some highly related yet intuitive theoretical properties of HUG.\n\nTheorem 4 (Order of Minimum Hyperspherical Energy) If d − 1 > s > 0 or 0 > s > −2 and d ∈ Es( ˆVn)} = c(s, d) where c(s, d) is a constant involving s, d. N, we have that limn→∞{n−2 · min ˆVn\n\nThe result above shows that the leading term of the minimum energy grows of order O(n2) as n → ∞. Theorem 4 generally holds with a wide range of s for the Riesz kernel in hyperspherical energy. Moreover, the following result shows that MHS is in fact a limiting case of MHE as s → ∞.\n\nProposition 2 (MHS is a Limiting Case of MHE) Let n ∈ N, n ≥ 2 be fixed and (Sd−1, L2) be a s = (max ˆVn⊂Sd−1 θ( ˆVn))−1. compact metric space. We have that lims→∞(min ˆVn⊂Sd−1 Es( ˆVn)) 1\n\nProposition 3 The HUG objectives in both Eq. 5 and Eq. 6 converge to simplex ETF when 2 ≤ C ≤ d + 1, converge to cross-polytope when C = 2d and asymptotically converge to GNC as C → ∞.\n\nProposition 3 shows that HUG not only decouples GNC but also provably converges to GNC. Since GNC indicates that the CE loss eventually approaches to the maximizer of HUG, we now look into how the CE loss implicitly maximizes the HUG objective in a coupled way.\n\ni=1 log(1 + (cid:80)C Proposition 4 The CE loss is LCE = (cid:80)n exp(⟨wj, xi⟩ − ⟨wyi, xi⟩)) where n is the number of samples, xi is the i-th sample with label yi and wj is the last-layer linear classifier for the j-th class. Bias is omitted for simplicity. LCE is bounded by (ρ = C − 1)\n\nj̸=yi\n\nn (cid:88)\n\nC (cid:88)\n\n⟨wj, xi⟩\n\n− ρ\n\nj̸=yi\n\ni=1 (cid:124) (cid:125) (cid:123)(cid:122) Q1: Coupled IS and IV\n\nn (cid:88)\n\n⟨wyi , xi⟩\n\ni=1\n\n(cid:124)\n\n(cid:123)(cid:122) Q2: Inter-class Variability\n\n(cid:125)\n\n≤ LCE ≤ log (cid:0)1 +\n\nn (cid:88)\n\nC (cid:88)\n\nexp(⟨wj, xi⟩)\n\n+ ρ\n\ni=1 (cid:124)\n\nj̸=yi\n\n(cid:123)(cid:122) Q3: Coupled IS and IV\n\n(cid:125)\n\nn (cid:88)\n\ni=1\n\nexp(−⟨wyi , xi⟩)\n\n(cid:1).\n\n(cid:124)\n\n(cid:123)(cid:122) Q4: Inter-class Variability\n\n(cid:125)\n\nWe show in Proposition 4 that CE inherently optimizes two independent criterion: intra-class variability (IV) and inter-class separability (IS). With normalized classifiers and features, we can see that Q1 and Q3 have similar minimum where xi = wyi and wi, ∀i attain hyperspherical uniformity.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nWe show that CE is lower bounded by the gap of inter-class and intra-class hyperspherical uniformity: n\n(cid:88)\n\nn (cid:88)\n\nn (cid:88)\n\nn (cid:88)\n\nC (cid:88)\n\nn (cid:88)\n\n(cid:88)\n\n2\n\nLCE ≥\n\nlog\n\nexp(ρ2\n\nljc⟨xi, xj⟩) − ρ3\n\nlicxi\n\n− ρ1\n\n⟨xi, xj⟩\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ni=1 (cid:124)\n\nc=1\n\nj=1\n\ni=1\n\n(cid:123)(cid:122) Inter-class Hyperspherical Uniformity\n\n(cid:125)\n\ni=1\n\nj∈Ayi (cid:123)(cid:122) Intra-class Hyperspherical Uniformity\n\n(cid:124)\n\n(cid:125)\n\n(11)\n\nwhere ρ1, ρ2, ρ3 are constants and lic is the softmax confidence of xi for the c-th class (Appendix L). This result [4] implies that minimizing CE effectively minimizes HUG. [50] proves that the minimizer of the normalized CE loss converges to hyperspherical uniformity. We rewrite their results below:\n\nTheorem 5 (CE Asymptotically Converges to HUG’s Maximizer) Considering unconstrained features of C classes (each class has the same number of samples), with features and classifiers normalized on some hypersphere, we have that, for the minimizer of the CE loss, classifiers converge weakly to the uniform measure on Sd−1 as C → ∞ and features collapse to their corresponding classifiers. The minimizer of CE also asymptotically converges to the maximizer of HUG.\n\nTheorem 5 shows that the minimizer of the CE loss with unconstrained features [53] asymptotically converges to the maximizer of HUG (i.e., GNC). Till now, we show that HUG shares the same optimum with CE (with hyperspherical normalization), while being more flexible for decoupling inter-class feature separability and intra-class feature variability. Therefore, we argue that HUG can be an excellent alternative for the widely used CE loss in classification problems.\n\nHUG maximizes mutual information. We can view HUG as a way to maximize mutual information I(X; Y ) = H(X) − H(X|Y ), where X denotes the feature space and Y is the label space. Maximizing H(X) implies that the feature should be uniform over the space. Minimizing H(X|Y ) means that the feature from the same class should be concentrated. This is nicely connected to HUG.\n\nThe role of feature and class proxy norm. Both NC and GNC do not take the norm of feature and class proxy into consideration. HUG also assume both feature and class proxy norm are projected onto some hypersphere. Although dropping these norms usually improves generalizability [8, 9, 14, 43, 76], training neural networks with standard CE loss still yields different class proxy norms and feature norms. We hypothesize that this is due to the underlying difference among training data distribution of different classes. One empirical evidence to support this is that average feature norm of different classes is consistent across training under different random seeds (e.g., average feature norm for digit 1 on MNIST stays the smallest in different run). [36, 46, 52] empirically show that feature norm corresponds to the quality of the sample, which can also viewed as a proxy to sample uncertainty. [56] theoretically shows that the norm of neuron weights (e.g., classifier) matters for its Rademacher complexity. As a trivial solution to minimize the CE loss, increasing the classifier norm (if the feature is correctly classified) can easily decrease the CE loss to zero for this sample, which is mostly caused by the softmax function. Taking both feature and class proxy norm into account greatly complicates the analysis (e.g., it results in weighted hyperspherical energy where the potentials between vectors are weighted) and seem to yield little benefit for now. We defer this issue to future investigation.\n\nHUG as a general framework for designing loss functions. HUG can be viewed as an inherently decoupled way of designing new loss functions. As long as we design a measure of hyperspherical uniformity, then HUG enables us to effortlessly turn it into a loss function for neural networks.\n\n4 EXPERIMENTS AND RESULTS\n\nOur experiments aims to demonstrate the empirical effectiveness of HUG, so we focus on the fair comparison to the popular CE loss under the same setting. Experimental details are in Appendix N.\n\n4.1 EXPLORATORY EXPERIMENTS AND ABLATION STUDY\n\nDifferent HUG variants. We compare different HUG variants and the CE loss on CIFAR-10 and CIFAR-100 with ResNet-18 [29]. Specifically, we use Eq. 6, Eq. 6 and Eq. 10 for MHE-HUG, MHS-HUG and MGD-HUG, respectively. The results are given in Table 1. We can observe that all HUG variants outperform the CE loss. Among all, MHE-HUG achieves the best testing accuracy with considerable improvement over the CE loss. We note that all HUG variants are used without the CE loss. The performance gain of HUG are actually quite significant, since the CE loss is currently a default choice for classification problems and serves as a very strong baseline.\n\nTable 1: Testing error (%) of HUG variants on CIFAR-10 and CIFAR-100.\n\nCE Loss MHE-HUG MHS-HUG MGD-HUG\n\nCIFAR-10 CIFAR-100\n\n24.38 24.59\n\n24.90 23.50\n\n5.09 5.38\n\n5.45 5.03\n\nMethod\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nCIFAR-10 CIFAR-100\n\nDifferent methods to update proxies. We also evaluate how different proxy update methods will affect the classification performance. We use the same setting as Table 1. For all the proxy update methods, we apply them to MHE-HUG (Eq. 6) under the same setting. The results are given Table 2. We can observe that all the propose proxy update methods work reasonably well. More interestingly, static proxies work surprisingly well and outperform the CE loss even when all the class proxies are randomly initialized and then fixed throughout the training. The reason the static proxies work for MHE-HUG is due to Proposition 1. This result is significant since we no longer have to train class proxies in HUG (unlike CE). When trained with large number of classes, it is GPU-memory costly for learning class proxies, which is also known as one of the bottlenecks for face recognition [1]. HUG could be a promising solution to this problem.\n\nCE Loss Fully learnable Static (random) Static (optimized) Partially learnable\n\nTable 2: Testing error (%) of different proxy update methods on CIFAR-10 and CIFAR-100.\n\n24.23 24.02 23.89\n\n5.19 5.12 5.08\n\n24.90 23.50\n\n5.45 5.03\n\nLoss landscape and convergence. We perturb neuron weights (refer to [40]) to visualize the loss landscape of HUG and CE in Figure 4. We use MHE in HUG here. The results show that HUG yields much flatter local minima than the CE loss in general, implying that HUG has potentially stronger generalization [34, 57]. We show more visualizations and convergence dynamics in Appendix O.\n\nFigure 4: Loss landscape visualization. (b,c,d) show L′\n\nMHE-HUG, Tb and Tw, respectively.\n\nLearning with different architectures. We evaluate HUG with different network architectures such as VGG-16 [65], ResNet-18 [29] and DenseNet-121 [31]. Results in Table 3 (Left number: CIFAR-10, right number: CIFAR-100) show that HUG is agnostic to different network architectures and outperforms the CE loss in every case. Although HUG works well on its own, any other methods that improve CE can also work with HUG.\n\nCE Loss 5.45 / 24.90 5.28 / 22.99 5.03 / 23.50 5.19 / 22.77 HUG\n\nTable 3: Testing error (%) with different architectures.\n\n5.04 / 21.47 4.85 / 21.30\n\nVGG-16 DenseNet-121\n\nMethod ResNet-18\n\n4.2 GENERALIZATION AND ROBUSTNESS UNDER DIFFERENT LEARNING SCENARIOS\n\nLong-tailed recognition. We consider the task of long-tailed recognition, where the data from different classes are imbalanced. The settings generally follow [6], and the dataset gets more imbalanced if the imbalance ratio (IR) gets smaller. The potential of HUG in imbalanced classification is evident, as the inter-class separability in the HUG is explicitly modeled and can be easily controlled. Experimental results in Table 4 show that HUG can consistently outperform the CE loss in the challenging long-tailed setting under different imbalanced ratio.\n\nCE 66.74 62.31 48.79 43.82 90.29 87.85 79.17 74.11 HUG 67.83 63.33 50.48 45.63 90.41 88.20 79.88 75.14\n\nTable 4: Testing accuracy (%) of long-tailed recognition.\n\nCIFAR-100 0.02 0.1\n\nCIFAR-10 0.02 0.1\n\n0.01\n\n0.01\n\n0.2\n\n0.2\n\nIR\n\nContinual learning. We demonstrate the potential of HUG in the class-continual learning setting, where the training data is not sampled i.i.d. but comes in class by class. Since training data is highly biased, hyperspherical uniformity among class proxies is crucial. Due to the decoupled nature of HUG, we can easily increase the importance of inter-class separability, unlike CE. We use a simple continual learning method – ER [62] where the CE loss with memory is used. We replace it with HUG. Table 5 shows HUG consistently improves ER under different memory size.\n\n22.14 31.02 43.54 49.07 61.58 76.89 23.52 31.92 43.92 53.74 62.67 77.21\n\nTable 5: Final testing accuracy (%) of continual learning.\n\nER + CE ER + HUG\n\nMemory size\n\n2000\n\n2000\n\nCIFAR-100 500\n\nCIFAR-10 500\n\n200\n\n200\n\nAdversarial robustness. We further test HUG’s adversarial robustness. In our experiments, we consider the classical white-box PGD attack [51] on ResNet18. The PGD attack iteration is set as 100 and the attack strength level is set as 2/255, 4/255, 8/255 in l∞ norm. All networks are naturally training with either HUG or CE loss. Results in Table 6 demonstrate that HUG yields consistently stronger adversarial robustness than the CE loss.\n\nTable 6: Testing accuracy (%) under adversarial attacks.\n\n5.03 / 23.50 15.24 / 5.26 3.45 / 1.24 1.76 / 0.44\n\nCE Loss 5.45 / 24.90 7.94 / 2.12\n\nl∞=4/255 l∞=8/255\n\nl∞=2/255\n\nMethod\n\n0.61 / 0\n\nClean\n\nHUG\n\n0 / 0\n\nNLP tasks. As an exploration, we evaluate HUG on some simple NLP classification tasks. Our experiments follow the same settings as [32] and finetune the BERT model [15] in these tasks. Table 7 shows that HUG yields better generalizability than CE, demonstrating its potential for NLP.\n\nTask\n\nMRPC SST-2 WNLI\n\nCE Loss\n\nHUG\n\n84.8\n\n85.8\n\n91.6\n\n91.8\n\n33.8\n\n34.0\n\nTable 7: NLP testing accuracy (%)\n\n9\n\n(a) CE Loss(b) HUG: Both losses(c) HUG: Intra-class Variability(d) HUG: Inter-class SeparabilityPublished as a conference paper at ICLR 2023\n\n5 RELATED WORK AND CONCLUDING REMARKS\n\nWe start by generalizing and decoupling the NC phenomenon, obtaining two basic principles for loss functions. Based on these principles, we identify a quantity hyperspherical uniformity gap, which not only decouples NC but also provides a general framework for designing loss functions. We demonstrate a few simple HUG variants that outperform the CE loss in terms of generalization and adversarial robustness. There is a large body of excellent work in NC that is related to HUG, such as [26, 33, 71, 89]. [88] extends the study of NC to more practical loss functions (e.g., focal loss and losses with label smoothing). Different from existing work in hyperspherical uniformity [41, 45, 48] and generic diversity (decorrelation) [2, 7, 11, 54, 77, 83], HUG works as a new learning target (used without CE) rather than acting as a regularizer for the CE loss (used together with CE). Following the spirit of [32], we demonstrate the effectiveness and potential of HUG as a valid substitute for CE.\n\nRelevant theoretical results. [87] has discussed NC under the case of d < C − 1, and shown that the global solution in this case yields the best rand-d approximation of the simplex ETF. Along with [87], GNC gives a more profound characterization of the convergence of class-means. We show a special case of d = 2, C = 4. It is easy to see that hyperspherical uniformity in this case forms four vectors with adjacency ones being perpendicular. This is also the case captured by the best rank-2 approximation (i.e., a 2-dimensional hyperplane with simplex ETF projected onto it). Figure 5 gives a geometric interpretation for the connection between [87] and GNC. [3] provides an in-depth introduction and comprehensive theoretical analysis for the energy minimization problem, which significantly benefits this work.\n\nFigure 5: Geometric connection between GNC and [87].\n\nConnection to contrastive learning. The goal of contrastive learning [8, 10, 24, 30, 72, 78, 85] is to learn discriminative features through instance-wise discrimination and contrast. Despite the lack of class labels, [78] discovers that contrastive learning performs sample-wise alignment and sample-wise uniformity, sharing a similar high-level spirit to intra-class variability and inter-class separability. [35] adapts contrastive learning to the supervised settings where labeled samples are available, which also shares conceptual similarity to our framework and settings.\n\nRelated work on (deep) metric learning. Metric learning also adopts similar idea where similar samples are pulled together and dissimilar ones are pushed away. HUG has intrinsic connections to a number of loss functions in metric learning [4, 17, 21, 24, 58, 59, 61, 67, 68, 79, 79–81, 84].\n\n6 BROADER IMPACT AND FUTURE WORK\n\nOur work reveals the underlying principle – hyperspherical uniformity gap, for classification loss function, especially in the context of deep learning. We provide a simple yet effective framework for designing decoupled classification loss functions. Rather than previous objective functions that are coupled and treated as a black-box, our loss function has clear physical interpretation and is fully decoupled for different functionalities. These characteristics may help neural networks to identify intrinsic structures hidden in data and true causes for classifying images. HUG may have broader applications in interpretable machine learning and fairness / bias problems.\n\nOur work is by no means perfect, and there are many aspects that require future investigation. For example, the implicit data mining in CE [49] is missing in the current HUG design, current HUG losses are more sensitive to hyperparameters than CE (the flexibility of decoupling also comes at a price), current HUG losses could be more unstable to train (more difficult to converge) than CE, and it requires more large-scale experiments to fully validate the superiority of current HUG losses. We hope that our work can serve as a good starting point to rethink classification losses in deep learning.\n\nACKNOWLEDGEMENT\n\nThe authors would like to sincerely thank the anonymous reviewers for all the detailed and valuable suggestions that have significantly improved the paper. This work is supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A, 01IS18039B; and by the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645. AW acknowledges support from a Turing AI Fellowship under EPSRC grant EP/V025279/1, and the Leverhulme Trust via CFI.\n\n10\n\n2D subspaceSimplex ETF (NC)d=3, C=43D spaceHyperspherical Uniformity (GNC)d=2, C=4Published as a conference paper at ICLR 2023\n\nREFERENCES\n\n[1] Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang,\n\nDebing Zhang, et al. Partial fc: Training 10 million identities on a single machine. In ICCV, 2021.\n\n[2] Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regularizations\n\nin training deep networks? In NeurIPS, 2018.\n\n[3] Sergiy V Borodachov, Douglas P Hardin, and Edward B Saff. Discrete energy on rectifiable sets. Springer,\n\n2019.\n\n[4] Malik Boudiaf, Jérôme Rony, Imtiaz Masud Ziko, Eric Granger, Marco Pedersoli, Pablo Piantanida, and Ismail Ben Ayed. A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses. In ECCV, 2020.\n\n[5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920–15930, 2020.\n\n[6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets\n\nwith label-distribution-aware margin loss. In NeurIPS, volume 32, 2019.\n\n[7] Tianlong Chen, Zhenyu Zhang, Yu Cheng, Ahmed Awadallah, and Zhangyang Wang. The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy. In CVPR, 2022.\n\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, 2020.\n\n[9] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at\n\nfew-shot classification. arXiv preprint arXiv:1904.04232, 2019.\n\n[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021.\n\n[11] Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in\n\ndeep networks by decorrelating representations. In arXiv preprint arXiv:1511.06068, 2015.\n\n[12] Henry Cohn and Abhinav Kumar. Universally optimal distribution of points on spheres. Journal of the\n\nAmerican Mathematical Society, 20(1):99–148, 2007.\n\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\n\nimage database. In CVPR, 2009.\n\n[14] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for\n\ndeep face recognition. In CVPR, 2019.\n\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\n\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\n\ntional transformers for language understanding. In NAACL-HLT, 2019.\n\n[17] Ismail Elezi, Sebastiano Vascon, Alessandro Torcinovich, Marcello Pelillo, and Laura Leal-Taixé. The\n\ngroup loss for deep metric learning. In ECCV, 2020.\n\n[18] D Jack Elzinga and Donald W Hearn. The minimum covering sphere problem. Management science, 19\n\n(1):96–104, 1972.\n\n[19] Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7(2):\n\n179–188, 1936.\n\n[20] Eduardo García-Portugués and Thomas Verdebout. An overview of uniformity tests on the hypersphere.\n\narXiv preprint arXiv:1804.00286, 2018.\n\n[21] Weifeng Ge. Deep metric learning with hierarchical triplet loss. In ECCV, 2018.\n\n[22] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural\n\nnetworks. In AISTATS, 2010.\n\n[23] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and\n\nbenchmark for large-scale face recognition. In ECCV, 2016.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\n[24] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\n\nIn CVPR, 2006.\n\n[25] Thomas C Hales. The sphere packing problem. Journal of Computational and Applied Mathematics, 44\n\n(1):41–76, 1992.\n\n[26] XY Han, Vardan Papyan, and David L Donoho. Neural collapse under mse loss: Proximity to and dynamics\n\non the central path. arXiv preprint arXiv:2106.02073, 2021.\n\n[27] Doug P Hardin, Edward B Saff, et al. Discretizing manifolds via minimum energy points. Notices of the\n\nAMS, 51(10):1186–1194, 2004.\n\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\n\nhuman-level performance on imagenet classification. In ICCV, 2015.\n\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\n\nIn CVPR, 2016.\n\n[30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\n\nvisual representation learning. In CVPR, 2020.\n\n[31] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\n\nconvolutional networks. In CVPR, 2017.\n\n[32] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy\n\nin classification tasks. In ICLR, 2021.\n\n[33] Like Hui, Mikhail Belkin, and Preetum Nakkiran. Limitations of neural collapse for understanding\n\ngeneralization in deep learning. arXiv preprint arXiv:2202.08384, 2022.\n\n[34] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.\n\n[35] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,\n\nCe Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020.\n\n[36] Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In\n\nCVPR, 2022.\n\n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional\n\nneural networks. Communications of the ACM, 60(6):84–90, 2017.\n\n[38] Arno Kuijlaars and E Saff. Asymptotics for minimal discrete energy on the sphere. Transactions of the\n\nAmerican Mathematical Society, 350(2):523–538, 1998.\n\n[39] NS Landkof. Foundations of modern potential theory, volume 180. Springer, 1972.\n\n[40] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of\n\nneural nets. In NeurIPS, 2018.\n\n[41] Rongmei Lin, Weiyang Liu, Zhen Liu, Chen Feng, Zhiding Yu, James M Rehg, Li Xiong, and Le Song.\n\nRegularizing neural networks via minimizing hyperspherical energy. In CVPR, 2020.\n\n[42] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional\n\nneural networks. In ICML, 2016.\n\n[43] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep\n\nhypersphere embedding for face recognition. In CVPR, 2017.\n\n[44] Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep\n\nhyperspherical learning. In NIPS, 2017.\n\n[45] Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning towards\n\nminimum hyperspherical energy. In NeurIPS, 2018.\n\n[46] Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and Le Song.\n\nDecoupled networks. In CVPR, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\n[47] Weiyang Liu, Rongmei Lin, Zhen Liu, James M Rehg, Liam Paull, Li Xiong, Le Song, and Adrian Weller.\n\nOrthogonal over-parameterized training. In CVPR, 2021.\n\n[48] Weiyang Liu, Rongmei Lin, Zhen Liu, Li Xiong, Bernhard Schölkopf, and Adrian Weller. Learning with\n\nhyperspherical uniformity. In AISTATS, 2021.\n\n[49] Weiyang Liu, Yandong Wen, Bhiksha Raj, Rita Singh, and Adrian Weller. Sphereface revived: Unifying hyperspherical face recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(02): 2458–2474, 2023.\n\n[50] Jianfeng Lu and Stefan Steinerberger. Neural collapse under cross-entropy loss. Applied and Computational\n\nHarmonic Analysis, 59:224–241, 2022.\n\n[51] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\n\ndeep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\n[52] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface: A universal representation for face\n\nrecognition and quality assessment. In CVPR, 2021.\n\n[53] Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. Sampling\n\nTheory, Signal Processing, and Data Analysis, 20(2):1–13, 2022.\n\n[54] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for\n\ngenerative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.\n\n[55] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\n[56] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.\n\nIn COLT, 2015.\n\n[57] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in\n\ndeep learning. In NIPS, 2017.\n\n[58] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured\n\nfeature embedding. In CVPR, 2016.\n\n[59] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep metric learning via facility\n\nlocation. In CVPR, 2017.\n\n[60] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of\n\ndeep learning training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020.\n\n[61] Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin. Softtriple loss: Deep metric learning\n\nwithout triplet sampling. In ICCV, 2019.\n\n[62] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018.\n\n[63] Edward B Saff and Amo BJ Kuijlaars. Distributing many points on a sphere. The mathematical intelligencer,\n\n19(1):5–11, 1997.\n\n[64] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.\n\n[65] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\n\ntion. arXiv preprint arXiv:1409.1556, 2014.\n\n[66] Steve Smale. Mathematical problems for the next century. The mathematical intelligencer, 20(2):7–15,\n\n1998.\n\n[67] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NIPS, 2016.\n\n[68] Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and Yichen Wei.\n\nCircle loss: A unified perspective of pair similarity optimization. In CVPR, 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n[69] Pieter Merkus Lambertus Tammes. On the origin of number and arrangement of the places of exit on the\n\nsurface of pollen-grains. Recueil des travaux botaniques néerlandais, 27(1):1–84, 1930.\n\n[70] Joseph John Thomson. On the structure of the atom: an investigation of the stability and periods of oscillation of a number of corpuscles arranged at equal intervals around the circumference of a circle; with application of the results to the theory of atomic structure. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 7(39):237–265, 1904.\n\n[71] Christos Thrampoulidis, Ganesh R Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting\n\nneural-collapse geometry. arXiv preprint arXiv:2208.05512, 2022.\n\n[72] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for\n\ngood views for contrastive learning? In NeurIPS, 2020.\n\n[73] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning\n\nresearch, 9(11), 2008.\n\n[74] Shayne FD Waldron. An introduction to finite tight frames. Springer, 2018.\n\n[75] Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng. Additive margin softmax for face verification.\n\narXiv preprint arXiv:1801.05599, 2018.\n\n[76] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu.\n\nCosface: Large margin cosine loss for deep face recognition. In CVPR, 2018.\n\n[77] Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal convolutional neural\n\nnetworks. In CVPR, 2020.\n\n[78] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment\n\nand uniformity on the hypersphere. In ICML, 2020.\n\n[79] Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Romain Garnier, and Neil M Robertson. Ranked\n\nlist loss for deep metric learning. In CVPR, 2019.\n\n[80] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss with\n\ngeneral pair weighting for deep metric learning. In CVPR, 2019.\n\n[81] Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin nearest neighbor\n\nclassification. Journal of machine learning research, 10(2), 2009.\n\n[82] Yandong Wen, Weiyang Liu, Adrian Weller, Bhiksha Raj, and Rita Singh. Sphereface2: Binary classifica-\n\ntion is all you need for deep face recognition. In ICLR, 2022.\n\n[83] Pengtao Xie, Aarti Singh, and Eric P Xing. Uncorrelation and evenness: a new diversity-promoting\n\nregularizer. In ICML, 2017.\n\n[84] Baosheng Yu and Dacheng Tao. Deep metric learning with tuplet margin loss. In ICCV, 2019.\n\n[85] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\n\nlearning via redundancy reduction. In ICML, 2021.\n\n[86] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep\n\nlearning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.\n\n[87] Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features. In ICML, 2022.\n\n[88] Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu. Are all losses\n\ncreated equal: A neural collapse perspective. arXiv preprint arXiv:2210.02192, 2022.\n\n[89] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric\n\nanalysis of neural collapse with unconstrained features. In NeurIPS, 2021.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nTable of Contents\n\nA Empirical Results on Generalized Neural Collapse\n\n. A.1 Detailed Metric Definition . A.2 Empirical Results of GNC on ImageNet\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\nB 2D MNIST Feature Visualization\n\nC Other Variants in the HUG Framework .\n.\n\n. Proxy-free HUG . C.1 C.2 Coupled HUG . .\n. C.3 HUG without Hyperspherical Normalization .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\nD Proof of Theorem 1\n\nE Proof of Theorem 2\n\nF Proof of Theorem 3\n\nG Proof of Proposition 1\n\nH Derivation of HUG Surrogate for MHE and MHS\n\nI\n\nProof of Proposition 2\n\nJ Proof of Proposition 3\n\nK Proof of Proposition 4\n\nL Derivation of CE’s Lower Bound\n\nM Proof of Theorem 5\n\nN Experimental Details\n\nO Additional Experimental Results\n\n15\n\n16 16 16\n\n17\n\n20 20 20 21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nA EMPIRICAL RESULTS ON GENERALIZED NEURAL COLLAPSE\n\nA.1 DETAILED METRIC DEFINITION\n\nWe consider four metrics: average classifier energy (ACE), average class-mean energy (ACME), average feature reverse-energy (AFRE) and average feature-mean reverse-energy (AFMRE) in the paper. Their definitions are given below:\n\nEACE =\n\n1 C(C − 1)\n\n(cid:88)\n\ni̸=j\n\n∥ ˆwi − ˆwj∥−2\n\nEACME =\n\n1 C(C − 1)\n\n(cid:88)\n\ni̸=j\n\n∥ ˆμi − ˆμj∥−2\n\nEAFRE =\n\n1 C\n\nC (cid:88)\n\nc=1\n\n1 |Ac| · (|Ac| − 1)\n\n(cid:88)\n\ni̸=j∈Ac\n\n∥ ˆxi − ˆxj∥\n\nEAFMRE =\n\n1 C\n\nC (cid:88)\n\nc=1\n\n1 |Ac|\n\n(cid:88)\n\ni∈Ac\n\n∥ ˆxi − ˆμc∥\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\nwhere |Ac| denotes the cardinality of the set Ac, ˆμc is the normalized feature mean of the c-th class and ˆwc denotes the normalized class proxy of the c-th class.\n\nA.2 EMPIRICAL RESULTS OF GNC ON IMAGENET\n\nWe find that the GNC hypothesis remains valid and informative even under the scenario of large number of classes (we use the 1000-class ImageNet-2012 dataset [13] here). Experimental results with ResNet-18 [29] (feature dimension as 512) are given in Figure 6. Experimental results with ResNet-50 [29] (feature dimension as 2048) are given in Figure 7.\n\nFigure 6: Training dynamics of hyperspherical energy (which captures inter-class separability) and hyperspherical reverse-energy (which captures intra-class variability). ImageNet-2012 [13] with ResNet-18 [29] (d = 512, C = 1000).\n\nFigure 7: Training dynamics of hyperspherical energy (which captures inter-class separability) and hyperspherical reverse-energy (which captures intra-class variability). ImageNet-2012 [13] with ResNet-50 [29] (d = 2048, C = 1000).\n\n16\n\n1020304050607080Number of Epochs0.50.510.520.530.540.550.56Average Hyperspherical EnergyClass-mean EnergyClassifier Energy1020304050607080Number of Epochs0.550.60.650.70.750.80.850.90.95Average Reverse-EnergyFeature-mean Reverse-EnergyFeature Reverse-Energy1020304050607080Number of Epochs0.50.510.520.530.540.550.56Average Hyperspherical EnergyClass-mean EnergyClassifier Energy1020304050607080Number of Epochs0.550.60.650.70.750.80.850.90.95Average Reverse-EnergyFeature-mean Reverse-EnergyFeature Reverse-EnergyPublished as a conference paper at ICLR 2023\n\nB 2D MNIST FEATURE VISUALIZATION\n\nWe also visualize the 2D MNIST feature in Figure 8, Figure 9 and Figure 10, which is done by directly setting the output feature dimension as 2. Different color denotes different class and black arrow denotes the class proxy. We compare the difference between the CE loss and the HUG-MHE loss (with either independently optimized proxies or fully learnable proxies). Specifically, for the HUG-MHE loss with independently optimized proxies, we use the following form:\n\nmax\n\n{ ˆxj }n\n\nj=1,{ ˆwc}C\n\nc=1\n\nLP-HUG := α ·\n\nHU(cid:0){ ˆwc}C (cid:124)\n\nc=1\n\n(cid:1)\n\n(cid:123)(cid:122) Inter-class Hyperspherical Uniformity\n\n(cid:125)\n\n−β ·\n\nC (cid:88)\n\nc=1\n\nHU(cid:0){ ˆxi}i∈Ac , SG( ˆwc)(cid:1) (cid:124) (cid:125) (cid:123)(cid:122) Intra-class Hyperspherical Uniformity\n\n(16)\n\nwhere we stop the gradient for the class proxies in the intra-class hyperspherical uniformity term. Form the results, we observe that the our HUG losses generally learns better representations than the CE loss, and moreover, HUG learns more aligned class proxy and class feature-mean than CE.\n\nFigure 8: 2D MNIST feature visualization for the CE loss at 1,5,10,15,20 epochs (top left - top right - middle left - middle right -bottom).\n\n17\n\n4030201001020305040302010010200123456789402002040302010010203001234567893020100102030302010010200123456789403020100102030403020100102030012345678940302010010203030201001020300123456789Published as a conference paper at ICLR 2023\n\nFigure 9: 2D MNIST feature visualization for the HUG loss (randomly initialized and then optimized proxies) at 1,5,10,15,20 epochs (top left - top right - middle left - middle right -bottom).\n\n18\n\n4202464202401234567896420246642024601234567896420246642024601234567893210123442024012345678943210123321012340123456789Published as a conference paper at ICLR 2023\n\nFigure 10: 2D MNIST feature visualization for the HUG loss (fully learnable proxies) at 1,5,10,15,20 epochs (top left - top right - middle left - middle right -bottom).\n\n19\n\n20244202401234567894202466420240123456789420246420240123456789420244202401234567893210123321012340123456789Published as a conference paper at ICLR 2023\n\nC OTHER VARIANTS IN THE HUG FRAMEWORK\n\nThere are plenty of interesting and useful instantiations for the loss function under the HUG framework. In this section, we discuss a few highly relevant and natural ones.\n\nC.1 PROXY-FREE HUG\n\nWe have the following general HUG objective function:\n\nmax { ˆxj }n\n\nj=1\n\nLHUG := α ·\n\nHU(cid:0){ ˆμc}C (cid:124)\n\nc=1\n\n(cid:1)\n\n(cid:123)(cid:122) Tb: Inter-class Hyperspherical Uniformity\n\n(cid:125)\n\n−β ·\n\nC (cid:88)\n\nc=1\n\nHU(cid:0){ ˆxi}i∈Ac (cid:124) (cid:123)(cid:122) Tw: Intra-class Hyperspherical Uniformity\n\n(cid:1)\n\n(cid:125)\n\n(17)\n\nwhere we can have many possible instantiations. Other than the proxy-based form proposed in the main paper, we can also have a proxy-free version:\n\nmax { ˆxj }n\n\nj=1\n\nLPF-HUG := α ·\n\nHU(cid:0){ ˆxi∈Ac}C (cid:124)\n\n(cid:123)(cid:122) Inter-class Hyperspherical Uniformity\n\n(cid:125)\n\nc=1\n\n(cid:1)\n\n−β ·\n\nC (cid:88)\n\nc=1\n\nHU(cid:0){ ˆxi}i∈Ac (cid:123)(cid:122) (cid:124) Intra-class Hyperspherical Uniformity\n\n(cid:1)\n\n(cid:125)\n\n(18)\n\nwhere { ˆxi∈Ac}C c=1 denotes a set of vectors that consist of one random sample per class. This is essentially to replace the class proxy with a random sample from this class. The proxy-free HUG loss can be used in the scenario where extremely large amount of classes exist and storing class proxies can be very expensive, or in the scenario of self-supervised contrastive learning where each instance and its augmentations are viewed as one class. A MHE-based instantiation of Eq. 18 is given by\n\nLMHE-PF-HUG := α · Esb\n\n(cid:0){ ˆxi∈Ac}C\n\nc=1\n\n(cid:1) − β ·\n\nmin { ˆxj }n\n\nj=1\n\nC (cid:88)\n\nc=1\n\nEsw\n\n(cid:0){ ˆxi}i∈Ac\n\n(cid:1)\n\n(19)\n\nwhich can be similarly relaxed to\n\nL′\n\nMHE-PF-HUG = α ·\n\n(cid:88)\n\nc̸=c′\n\n∥ ˆxi∈Ac − ˆxj∈Ac′ ∥−2 + β′ ·\n\n(cid:88)\n\n(cid:88)\n\n∥ ˆxi − ˆxj∥\n\n(20)\n\nc\n\ni∈Ac,j∈Ac,i̸=j\n\nwhere ˆxi∈Ac denotes a randomly selected sample from the c-th class. The first term in Eq. 20 can also be viewed as a scalable stochastic approximation to the first term in the following loss function:\n\nL′′\n\nMHE-PF-HUG = α ·\n\n(cid:88)\n\n∥ ˆxi − ˆxj∥−2 + β′ ·\n\n(cid:88)\n\n(cid:88)\n\n∥ ˆxi − ˆxj∥\n\n(21)\n\ni∈Ac,j∈Ac′ ,c̸=c′\n\nc\n\ni∈Ac,j∈Ac,i̸=j\n\nwhich is typically optimized by stochastic gradients (samples come as a mini batch) in practice.\n\nC.2 COUPLED HUG\n\nOne advantage of HUG is that it decouples intra-class variability and inter-class separability. However, coupling may also bring some benefits (e.g., robustness on hyperparameters, stability in training). To this end, we also propose a coupled loss function using the HUG framework:\n\nmax { ˆxj }n\n\nj=1\n\nLPF-HUG := α·\n\nn (cid:88)\n\nHU(cid:0){ ˆwc}C\n\nc=1,c̸=yi , ˆxi\n\n(cid:1)\n\n(cid:123)(cid:122) Coupled Intra-class and Inter-class Hyperspherical Uniformity\n\n(cid:125)\n\ni=1 (cid:124)\n\n−β·\n\nC (cid:88)\n\nc=1\n\nHU(cid:0){ ˆxi}i∈Ac (cid:123)(cid:122) (cid:124) Intra-class Hyperspherical Uniformity\n\n(cid:1)\n\n(cid:125)\n\nwhich can be turned into a MHE-based instantiation:\n\nL′′\n\nMHE-C-HUG = α ·\n\nn (cid:88)\n\n(cid:88)\n\ni=1\n\nc̸=yi\n\n∥ ˆxi − ˆwc∥−2 + β′ ·\n\n(cid:88)\n\n(cid:88)\n\n∥ ˆxi − ˆxj∥\n\n(23)\n\nc\n\ni∈Ac,j∈Ac,i̸=j\n\nwhere the first term itself couples intra-class and inter-class hyperspherical uniformity. Although the coupled HUG drops the flexibility that the original HUG framework brings, it may introduce extra advantages (e.g., training stability).\n\n(22)\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nC.3 HUG WITHOUT HYPERSPHERICAL NORMALIZATION\n\nWhile the CE loss does not necessarily require hyperspherical normalization for the proxies and features (but hyperspherical normalization does improve CE’s generalizability [44, 75]), we also consider the HUG framework without hyperspherical normalization here. We note that this issue remains an open challenge and we only aim to provide some simple yet natural designs.\n\nThe obvious problem to remove hyperspherical normalization is that HUG has a trivial way to decrease its loss – simply increasing the magnitude of features and proxies. A naive way to address this is to introduce magnitude penalty terms for the features and proxies. This results in\n\nmax\n\n{xj }n\n\nj=1,{wc}C\n\nc=1\n\nLUN-P-HUG :=α ·\n\nHU(cid:0){wc}C (cid:124)\n\nc=1\n\n(cid:123)(cid:122) Inter-class Hyperspherical Uniformity\n\n(cid:125)\n\n(cid:1)\n\n−β ·\n\nC (cid:88)\n\nc=1\n\nHU(cid:0){xi}i∈Ac , wc (cid:123)(cid:122) (cid:124) Intra-class Hyperspherical Uniformity\n\n(cid:1)\n\n(cid:125)\n\n− λ1 ·\n\nC (cid:88)\n\n∥wc − s∥2\n\n−λ2 ·\n\nn (cid:88)\n\n∥xi − s∥2\n\ni=1 (cid:124)\n\n(cid:123)(cid:122) Soft Magnitude Constraint on Features\n\n(cid:125)\n\nc=1 (cid:124)\n\n(cid:123)(cid:122) Soft Magnitude Constraint on Proxies\n\n(cid:125)\n\nwhere s denotes the magnitude hyperparameter.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nD PROOF OF THEOREM 1\n\nWe first let ˆVC = {ˆv1, · · · , ˆvC} be an arbitrary vector configuration in Sd−1. Then we will have that\n\nΛ( ˆVC) :=\n\n=\n\nC (cid:88)\n\nC (cid:88)\n\ni=1\n\nj=1\n\nC (cid:88)\n\nC (cid:88)\n\ni=1\n\nj=1\n\n∥ˆvi − ˆvj∥2\n\n(2 − 2ˆvi · ˆvj)\n\n(24)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nC (cid:88)\n\ni=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nˆvi\n\n=2C 2 − 2\n\n≤2C 2\n\nwhich holds if and only if (cid:80)C satisfy this condition. With the properties of the potential function f , we have that\n\ni=1 ˆvi = 0. The vertices of a regular (n − 1)-simplex at the origin well\n\nEf (ˆvC) :=\n\nC (cid:88)\n\n(cid:88)\n\ni=1\n\nj:j̸=i\n\nf (cid:0) ∥ˆvi − ˆvj∥2 (cid:1)\n\n≥C(C − 1)f\n\n≥C(C − 1)f\n\n(cid:18) Λ(ˆvC)\n\n(cid:19)\n\nC(C − 1)\n\n(cid:18) 2C\n\n(cid:19)\n\nC − 1\n\n(25)\n\nwhich holds true if all pairwise distance ∥ˆvi − ˆvj∥ are equal for i ̸= j and the center of mass is at the origin (i.e., (cid:80)C C which contains the vertices of a regular (C − 1)-simplex inscribed in Sd and centered at the origin, we have that for 2 ≤ C ≤ d + 1\n\ni=1 ˆvi = 0). Therefore, for the vector configuration ˆV ∗\n\nEf ( ˆV ∗\n\nn ) = C(C − 1)f\n\n(cid:18) 2C\n\n(cid:19)\n\nC − 1\n\n≤ Ef ( ˆVC).\n\n(26)\n\nIf f is strictly convex and strictly decreasing, then Ef ( ˆVC) ≥ C(C − 1)f ( 2C ˆV ∗ C is a regular (C − 1)-simplex inscribed in Sd−1 and centered at the origin.\n\nC−1 ) holds only when ■\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nE PROOF OF THEOREM 2\n\nThis result comes as a natural conclusion from [12] where they prove that any sharp code is a minimal hyperspherical f -energy N -point configuration for any interaction potential f that is absolutely monotone on [−1, 1] including all Riesz s-potentials f (t) = 2(t − 2t)−s/2 for s > 0.\n\nBefore we move on, we need to introduce the definition of sharp code:\n\nDefinition 1 Let ˆVN = {ˆv1, · · · , ˆvN } be a N -point configuration on Sd′\n\n.\n\n• If for every (d′ + 1)-variate polynomial P of degree at most m,\n\n(cid:90)\n\nSd′\n\nP dσd′ =\n\n1 N\n\nN (cid:88)\n\ni=1\n\nP (ˆvi)\n\nthen ˆVN is called a spherical m-design.\n\n• If ˆVN is a configuration of N distinct points such that the set of inner products between\n\ndistinct points in ˆVN has cardinality k, then ˆVN is called a spherical k-distance set.\n\n• The configuration ˆVN is a sharp code if it is both a k-distance set and a spherical (2k − 1)-\n\ndesign.\n\nThe Cohn-Kumar Universal Optimality theorem [12] states that any sharp code is universally optimal. By universal optimality, we mean that\n\nDefinition 2 An N -point configuration ˆVN on Sd′\n\nis called universally optimal if\n\nEf ( ˆVN ) :=\n\n(cid:88)\n\nˆv1,ˆv2∈ ˆVN ,ˆv1̸=ˆv2\n\nf (ˆv⊤\n\n1 ˆv2) = min\n\nˆVN ⊂Sd′\n\nEf ( ˆVN )\n\nholds for any absolutely monotone function f : [−1, 1) → R.\n\nThen formally, Cohn-Kuma Universal Optimality Theorem states:\n\nTheorem 6 If ˆVN is a sharp code on Sd′\n\n, then ˆVN is universally optimal.\n\nBecause the vertices of the cross-polytope are a sharp code, then this vertex set (2d′ + 2 points in total) is universally optimal, which implies that\n\nEf ( ˆWN ) = min\n\nˆWN ⊂Sd′\n\nEf ( ˆWN )\n\n(27)\n\nwhere ˆWN denote the vertex set of the cross-polytope. Then we let s = 2 for the f -energy and ■\nd′ = d − 1, and we prove our theorem.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nF PROOF OF THEOREM 3\n\nThis theorem is in fact a well-known result (see [3, 27, 38, 63]). This general result is stated as\n\nTheorem 7 If A ⊂ Rp is compact with dim A > 0 and 0 < s < dim A, then\n\nlim N→∞\n\nεs(A, N ) N 2\n\n= Ws(A),\n\nwhere εs(A, n) := min ˆWn⊂A Es( ˆWn) and Ws(A) is Wiener constant. Moreover, the equilibrium measure μs,A on A is unique for the Riesz s-kernel when 0 < s < dim A. Finally, any sequence {ˆvN\n\nN =2 of asympototically s-energy minimizing N -point configuration on A satisfies\n\n1 , · · · , ˆvN\n\nN }∞\n\nv({ˆvN\n\n1 , · · · , ˆvN\n\nN }) →weak μs,A, N → ∞\n\nFrom the theorem above, with s = 2, d − 1 > s, N = C and A = Sd−1, we have that Ws(Sd−1) is a constant term, and most importantly, we have that these point sequences { ˆμC C} asymptotically minimizes the hyperspherical energy on Sd−1.\n\n1 , · · · , ˆμC\n\nMoreover, the same theorem also gives that the leading term of the minimum hyperspherical energy ■\nis of order O(n2) as n → ∞.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nG PROOF OF PROPOSITION 1\n\nWe show that zero-mean equal-variance Gaussian distributed vectors (after normalized to norm 1) are uniformly distributed over the unit hypersphere with Theorem 8.\n\nLemma 1 Let x be a n-dimensional random vector with distribution N (0, 1) and U ∈ Rn×n be an orthogonal matrix (U U ⊤ = U ⊤U = I). Then Y = U x also has the distribution of N (0, 1).\n\nProof G.1 For any measurable set A ⊂ Rn, we have that\n\nP (Y ∈ A) = P (X ∈ U ⊤A) (cid:90)\n\n=\n\nU ⊤A\n\n=\n\n=\n\n(cid:90)\n\n(\n\nA\n\n(cid:90)\n\n(\n\nA\n\n√\n\n√\n\n√ (\n1\n\n2π)n 1\n\n2π)n\n\n1\n\n2π)n\n\ne− 1\n\n2 ⟨x,x⟩\n\ne− 1\n\n2 ⟨U x,U x⟩\n\ne− 1\n\n2 ⟨x,x⟩\n\nbecause of orthogonality of U . Therefore the lemma holds.\n\n(28)\n\n■\n\nTheorem 8 The normalized vector of Gaussian variables is uniformly distributed on the sphere. Formally, let x1, x2, · · · , xn ∼ N (0, 1) and be independent. Then the vector\n\nx =\n\n(cid:20) x1 z\n\n,\n\nx2 z\n\n, · · · ,\n\n(cid:21)\n\nxn z\n\n(29)\n\nfollows the uniform distribution on Sn−1, where z = (cid:112)x2\n\n1 + x2\n\n2 + · · · + x2\n\nn is a normalization factor.\n\nProof G.2 A random variable has distribution N (0, 1) if it has the density function\n\nf (x) =\n\n1 √\n\n2π\n\ne− 1\n\n2 x2\n\n.\n\n(30)\n\nA n-dimensional random vector x has distribution N (0, 1) if the components are independent and have distribution N (0, 1) each. Then the density of x is given by\n\nf (x) =\n\n1\n\n2π)n\n\n√\n\n(\n\ne− 1\n\n2 ⟨x,x⟩.\n\n(31)\n\nThen we use Lemma 1 about the orthogonal-invariance of the normal distribution.\n\nBecause any rotation is just a multiplication with some orthogonal matrix, we know that normally distributed random vectors are invariant to rotation. As a result, generating x ∈ Rn with distribution N(0, 1) and then projecting it onto the hypersphere Sn−1 produces random vectors U = x ∥x∥ that are ■\nuniformly distributed on the hypersphere. Therefore the theorem holds.\n\nThe above results indicate that as long as class proxies are initialize with zero-mean Gaussian, they ■\nare uniformly distributed over the hypersphere in a probabilistic sense.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nH DERIVATION OF HUG SURROGATE FOR MHE AND MHS\n\nThe derivation of L′\n\nMHE-HUG is as follows:\n\nLMHE-HUG := α · Esb\n\n(cid:0){ ˆwc}C\n\nc=1\n\n(cid:1) − β ·\n\nC (cid:88)\n\nc=1\n\nEsw\n\n(cid:0){ ˆxi}i∈Ac, ˆwc\n\n(cid:1)\n\n∥ ˆwc − ˆwc′∥−2 + β ·\n\n(cid:88)\n\n(cid:0) (cid:88)\n\n∥ ˆxi − ˆxj∥ + 2 ·\n\n∥ ˆwc − ˆwc′∥−2 + β ·\n\nc (cid:88)\n\ni,j∈Ac,i̸=j\n\n(cid:0) (cid:88)\n\nc\n\ni,j∈Ac,i̸=j\n\n∥ ˆxi − ˆwc∥(cid:1)\n\n(cid:88)\n\ni∈Ac\n\n∥ ˆxi − ˆwc + ˆwc − ˆxj∥\n\n+ 2 ·\n\n(cid:88)\n\ni∈Ac\n\n∥ ˆxi − ˆwc∥(cid:1)\n\n(32)\n\n∥ ˆwc − ˆwc′∥−2 + β ·\n\n(cid:88)\n\n(cid:0) (cid:88)\n\n(∥ ˆxi − ˆwc∥ + ∥ ˆwc − ˆxj∥)\n\nc\n\ni,j∈Ac,i̸=j\n\n+ 2 ·\n\n(cid:88)\n\ni∈Ac\n\n∥ ˆxi − ˆwc∥(cid:1)\n\n∥ ˆwc − ˆwc′∥−2 + β′ ·\n\n(cid:88)\n\n(cid:88)\n\nc\n\ni∈Ac\n\n∥ ˆxi − ˆwc∥ =: L′\n\nMHE-HUG\n\n= α ·\n\n= α ·\n\n(cid:88)\n\nc̸=c′ (cid:88)\n\nc̸=c′\n\n≤ α ·\n\n(cid:88)\n\nc̸=c′\n\n= α ·\n\n(cid:88)\n\nc̸=c′\n\nThe derivation of L′\n\nMHS-HUG is as follows:\n\nLMHS-HUG := α · θ(cid:0){ ˆwc}C\n\nc=1\n\n(cid:1) − β ·\n\nC (cid:88)\n\nc=1\n\nθ(cid:0){ ˆxi}i∈Ac, ˆwc\n\n(cid:1)\n\n= α · min c̸=c′\n\n∥ ˆwc − ˆwc′∥ − β ·\n\n≤ α · min c̸=c′\n\n∥ ˆwc − ˆwc′∥ − β ·\n\n(cid:88)\n\nc (cid:88)\n\nc\n\nmax u,v∈{{ ˆxi}i∈Ac , ˆwc},u̸=v\n\n∥u − v∥\n\n(33)\n\nmax i∈Ac\n\n∥ ˆxi − ˆwc∥ =: L′\n\nMHS-HUG.\n\nMost importantly, (cid:80) MHS-HUG share the same minimizer (minimum is 0, which happens when intra-class feature collapse to its class proxy). Therefore, L′ MHS-HUG can be viewed as a surrogate loss for LMHS-HUG.\n\nc maxu,v∈{{ ˆxi}i∈Ac , ˆwc},u̸=v ∥u − v∥ in LMHS-HUG and L′\n\nMHS-HUG and LMHS-HUG share the same maximizer, and L′\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nI PROOF OF PROPOSITION 2\n\nFor notational convenience, we first define εs(Sd−1, n) := min ˆVn⊂Sd−1 Es( ˆVn) and δρ n(Sd−1) := max ˆVn⊂Sd−1 θ( ˆVn). We then define that ˆV s n is a s-energy minimizing n-point configuration on Sd−1 if 0 < s < ∞ (i.e., MHE configuration) and ˆV ∞ n denotes a best-packing configuration on Sd−1 if s = ∞ (i.e., MHS configuration). Since we are considering s > 0, we only need to discuss the case of Ks(ˆvi, ˆvj) = ρ(ˆvi, ˆvj)−s. Then we will have the following equation:\n\nεs(Sd−1, n)\n\n1\n\ns = Es( ˆV s n )\n\n1\n\ns ≥\n\n1 n( ˆV s δρ n )\n\n≥\n\n1 δρ n(Sd−1)\n\n.\n\nMoreover, we have that\n\nεs(Sd−1, n)\n\n1\n\ns ≤ Es( ˆV ∞ n )\n\n1 s\n\n=\n\n≤\n\n1 δρ( ˆV ∞ n )\n\n1 δρ( ˆV ∞ n )\n\n(cid:18) (cid:88)\n\n1≤i̸=j≤N\n\n(cid:0) δρ( ˆV ∞ n ) , ˆv∞ ρ(ˆv∞ j )\n\ni\n\n(cid:1)s(cid:19) 1\n\ns\n\n(cid:0)n(n − 1)(cid:1) 1\n\ns\n\nTherefore, we will end up with\n\nsup εs(Sd−1, n)\n\n1\n\ns ≤\n\nlim s→∞\n\n1 δρ( ˆV ∞ n )\n\n=\n\n1 δρ n(Sd−1)\n\n.\n\nThen we take both Eq. 34 and Eq. 36 into consideration and have that\n\nεs(Sd−1, n)\n\n1\n\ns =\n\nlim s→∞\n\n1 δρ n(Sd−1)\n\nwhich concludes the proof.\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\n■\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nJ PROOF OF PROPOSITION 3\n\nWe write down the formulation of the HUG objectives (with MHE):\n\nmin\n\n{ ˆxi}n\n\ni=1,{ ˆwc}C\n\nc=1\n\nLMHE-HUG := α · Esb\n\n(cid:0){ ˆwc}C\n\nc=1\n\n(cid:1) − β ·\n\nC (cid:88)\n\nc=1\n\nEsw\n\n(cid:0){ ˆxi}i∈Ac , ˆwc\n\n(cid:1)\n\n= α ·\n\n(cid:88)\n\nc̸=c′\n\n∥ ˆwc − ˆwc′∥−2 + β ·\n\n(cid:88)\n\n(cid:0) (cid:88)\n\n∥ ˆxi − ˆxj∥\n\n(38)\n\nc\n\ni,j∈Ac,i̸=j\n\n+ 2 ·\n\n(cid:88)\n\ni∈Ac\n\n∥ ˆxi − ˆwc∥(cid:1)\n\nL′\n\nMHE-HUG = α ·\n\n(cid:88)\n\nc̸=c′\n\n∥ ˆwc − ˆwc′∥−2 + β′ ·\n\n(cid:88)\n\n(cid:88)\n\nc\n\ni∈Ac\n\n∥ ˆxi − ˆwc∥\n\n(39)\n\nmin\n\n{ ˆxi}n\n\ni=1,{ ˆwc}C\n\nc=1\n\nFor both objectives, we can see that the minimizer of the second term (i.e., the intra-class variability term) is all intra-class feature collapse to their class proxy and therefore the second term achieves the global minimum 0.\n\nFor the first term of both objectives, the global minimizer can be obtain directly from Theorem 1, Theorem 2 and Theorem 3. It is easy to see that the global minimizer of the inter-class separability term and the intra-class variability term does not contradict with each other and can be achieved ■\nsimultaneously.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nK PROOF OF PROPOSITION 4\n\n(40)\n\n(41)\n\n■\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\nlog(1 +\n\nC (cid:88)\n\nj=1̸=yi\n\nexp(⟨wj, xi⟩ − ⟨wyi, xi⟩))\n\nC (cid:88)\n\nlog(1 + exp(⟨wj, xi⟩ − ⟨wyi, xi⟩))\n\ni=1\n\nj=1̸=yi\n\nn (cid:88)\n\nC (cid:88)\n\ni=1\n\nj=1̸=yi\n\n(⟨wj, xi⟩ − ⟨wyi, xi⟩)\n\nn (cid:88)\n\nC (cid:88)\n\n⟨wj, xi⟩\n\n− (C − 1)\n\nn (cid:88)\n\n⟨wyi, xi⟩\n\nj̸=yi\n\ni=1 (cid:125) (cid:123)(cid:122) (cid:124) Q1: Coupling IS and IV\n\n(cid:124)\n\ni=1 (cid:123)(cid:122) Q2: Inter-class Variability\n\n(cid:125)\n\n≥\n\n≥\n\n=\n\nC (cid:88)\n\nlog(1 +\n\nexp(⟨wj, xi⟩ − ⟨wyi, xi⟩))\n\nj=1̸=yi\n\nn (cid:88)\n\nC (cid:88)\n\ni=1\n\nj=1̸=yi\n\nn (cid:88)\n\nC (cid:88)\n\ni=1\n\nj=1̸=yi\n\nexp(⟨wj, xi⟩ − ⟨wyi, xi⟩))\n\n(exp(⟨wj, xi⟩) + exp(−⟨wyi, xi⟩)))\n\nn (cid:88)\n\ni=1\n\n≤ log(1 +\n\n≤ log(1 +\n\n= log (cid:0)1 +\n\nn (cid:88)\n\nC (cid:88)\n\nexp(⟨wj, xi⟩)\n\n+ (C − 1)\n\nn (cid:88)\n\nexp(−⟨wyi, xi⟩)\n\n(cid:1)\n\nj̸=yi\n\ni=1 (cid:124)\n\n(cid:123)(cid:122) Q3: Coupling IS and IV\n\n(cid:125)\n\n(cid:124)\n\ni=1\n\n(cid:123)(cid:122) Q4: Inter-class Variability\n\n(cid:125)\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nL DERIVATION OF CE’S LOWER BOUND\n\nThe derivation is actually very simple and this result is originally given by [4]. We find that it naturally matches the intuition behind HUG. For our paper to be self-contained, we briefly give the simple derivation below. For the details, please refer to Proposition 1 in [4].\n\nWe start by rewriting the CE loss as\n\nn (cid:88)\n\ni=1\n\nLCE = −\n\n(cid:124)\n\n⟨wyi, xi⟩ +\n\nλn 2\n\n(cid:123)(cid:122) Q1(w)\n\nC (cid:88)\n\nc=1\n\n⟨wc, wc⟩\n\n+\n\n(cid:125)\n\nlog\n\nC (cid:88)\n\nc=1\n\nn (cid:88)\n\ni=1 (cid:124)\n\nexp(⟨wc, xi⟩) −\n\n(cid:123)(cid:122) Q2(w)\n\nλn 2\n\nC (cid:88)\n\nc=1\n\n⟨wc, wc⟩\n\n(42)\n\n(cid:125)\n\nwhere λ can be chosen such that both Q1(w) and Q2(w) become convex functions with respect to w. Taking advantage of the convexity, we can separately set the gradient of Q1(w) and Q2(w) with respect to w as 0 and compute their minima. Specifically, we end up with\n\nQ1(w) ≥ Q1(w∗\n\nQ1\n\n) = −\n\n1 2λn\n\nn (cid:88)\n\n(cid:88)\n\ni=1\n\nj∈Ayi\n\n⟨xi, xj⟩,\n\n(43)\n\nQ2(w) ≥ Q2(w∗\n\nQ2\n\n) =\n\nn (cid:88)\n\nlog\n\nC (cid:88)\n\nexp\n\ni=1\n\nc=1\n\n(cid:18) 1 λn\n\nn (cid:88)\n\nj=1\n\n(cid:19)\n\nljc⟨xi, xj⟩\n\n−\n\nn 2λ\n\nC (cid:88)\n\nc=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\nlicxi\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n,\n\n(44)\n\nj exp(⟨wj ,xi⟩) denotes the softmax confidence. Combining the two lower bounds above,\n\n(cid:80)\n\nwhere lic = exp(⟩wc,wi⟩) we can have that LCE ≥ Q1(w∗\n\n) + Q2(w∗\n\nQ1\n\n)\n\nQ2\n\nn (cid:88)\n\n=\n\nlog\n\nC (cid:88)\n\nexp\n\ni=1\n\nc=1\n\n(cid:18) 1 λn\n\nn (cid:88)\n\nj=1\n\n(cid:19)\n\nljc⟨xi, xj⟩\n\n−\n\nn 2λ\n\nC (cid:88)\n\nc=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\nlicxi\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n−\n\n1 2λn\n\nn (cid:88)\n\ni=1\n\n(cid:88)\n\n⟨xi, xj⟩\n\nj∈Ayi\n\n(45) where the first two terms encourage larger inter-class hyperspherical uniformity, and the last term promotes smaller intra-class hyperspherical uniformity.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nM PROOF OF THEOREM 5\n\nThis theorem follows naturally from the main result in [50]. [50] has proved that the minimizer of a simplified form of the cross-entropy loss is the simplex ETF when 2 ≤ C ≤ d + 1 and the minimizer also asymptotically converges to uniform measure on the hypersphere. More formally, we have\n\nTheorem 9 ([50]) Consider the following variational problem\n\nmin u\n\nLα(u) :=\n\nn (cid:88)\n\ni=1\n\nlog\n\n(cid:18) (cid:80)n\n\nj=1 exp(⟨uj, ui⟩) exp(⟨ui, ui⟩)\n\n(cid:19)\n\ns.t. ui ∈ Rd, ∥ui∥ = 1, ∀i\n\nLet μn be the probability measure on Sd generated by a minimizer\n\nμn =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nδui ,\n\n(46)\n\n(47)\n\nthen for any α > 0, μn converges weakly to the unform measure on Sd−1 as n → ∞.\n\nFrom Theorem 3, we know that HUG with specific potential energy also converges to the uniform measure on Sd−1. Combining the results above, we can conclude that HUG and CE share the same ■\nminimizer.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nN EXPERIMENTAL DETAILS\n\nGeneral settings. For MHE-HUG and MHS-HUG, α and β are set as 0.15 and 0.015, respectively. For MGD-HUG, α and β are set as 0.15 and 0.03, respectively. We train the model for 200 epochs with 512 batchsize for both the cross-entropy (CE) loss and HUG. We use the stochastic gradient descent with momentum 0.9 and weight decay 2 × 10−4. The initial learning rate is set as 0.1 for both CIFAR-100 and CIFAR-10 and is divided by 10 at 60, 120, 180 epoch. For the general classification experiments, we use multiple architectures, including ResNet-18, VGG16 and DenseNet121. we use the simple data augmentation: 4 pixels are padded on each side, and image is randomly cropped.\n\nLong-tailed recognition. We follow LDAM [6] to obtain imbalanced CIFAR-10 and CIFAR-100 datasets with different imbalanced ratio. Following LDAM, we use ResNet-32 as our base network. The other setting is the same as our general setting.\n\nContinual learning. We follow DER [5] to construct our continual learning experiments. We split both the CIFAR-10 and CIFAR-100 training set into 5 tasks. Each task has 2 classes and 20 classes for CIFAR-10 and CIFAR-100, respectively. The training batchsize is set as 64, where there are 32 incoming samples and 32 replayed samples. Different size of memory buffer is also studied.\n\nAdversarial robustness. For the experiments of adversarial robustness, we first obtain the model trained with CE and HUG. With the information of the attacked model, PGD [51] generates some adversarial examples to mislead the attacked model. The test accuracy in the experiments of adversarial robustness shows the accuracy of the perturbed samples.\n\nVisualizing loss landscape. We perturb neuron weights to visualize the loss landscape, as proposed in [40]. For details, we perturb the model weight with 400 interpolation points in two random vectors around the current model weight minima. The visualization method is also the same as [47].\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nO ADDITIONAL EXPERIMENTAL RESULTS\n\nTraining convergence. We observe the training convergence of HUG on CIFAR-10 and CIFAR-100. Both the evaluation accuracy and the training loss, including the overall losses, the intra-class loss and the inter-class loss, are shown in Figure 11. For both the CIFAR-10 and CIFAR-100, the inter-class uniformity loss remains relatively small, which is consistent with the empirical finding in [41, 47]. Moreover, we find that the intra-class uniformity loss (i.e., intra-class variability) dominates the overall loss on CIFAR-100 dataset and it is relatively difficult to optimize when the class number becomes large.\n\nFigure 11: HUG’s training loss and testing accuracy (%) on CIFAR-10 (left) and CIFAR-100 (right).\n\n2D loss contour. We also utilize the method in [40] to visualize the 2D loss landscape, which is more easy to visualize the flatness of the loss landscape. As shown in Figure 12, the 2D loss landscape of our HUG loss is flatter than the widely used CE loss, showing that HUG yields a flat minima which may have better generalization ability.\n\nFigure 12: The 2D Loss Contour of different loss objective. From left to the right: (1). CE loss. (2). HUG overall loss. (3). intra-class loss. (4). inter-class loss.\n\nThe ablation of α and β. In our HUG framework, we introduce two scaling hyperparameters, α for the inter-class hyperspherical uniformity, β for the intra-class hyperspherical uniformity. We investigate the effect of the two hyperparameters for the model performance. As shown in Table 8, HUG is not sensitive to α, as the inter-class hyperspherical uniformity is always easy to optimize. HUG is also not sensitive to β in a wide range. The ablations are conducted on CIFAR-100. α is set as 0.15 when we perform ablation on β. β is set as 0.015 when doing ablation on α.\n\nα\n\n0.0003 0.0015 0.015\n\n0.05\n\n0.15\n\n0.5\n\n1.5\n\n5.0\n\nAccuracy\n\n76.31\n\n75.99\n\n76.28 76.16 76.48 76.32 76.1 76.03\n\nβ\n\n0.005 0.015\n\n0.05\n\n0.15\n\n0.3\n\n0.5\n\n1.5\n\n5.0\n\nAccuracy 74.15 76.48 76.12 75.87 75.59 75.24 74.81 74.00\n\nTable 8: Effect of hyperparameters α and β.\n\n33\n\n050100150200Epoch0.0000.0250.0500.0750.1000.1250.150LossOverallIntra-classInter-class020406080Top1Top1050100150200Epoch0.00.20.40.60.81.01.21.4LossOverallIntra-classInter-class010203040506070Top1Top10.40.30.20.10.00.10.20.30.40.40.30.20.10.00.10.20.30.40.3940.5260.6570.7890.9201.0521.1831.3151.4461.4461.5781.5781.7091.7091.8411.8411.8411.9721.9721.9722.1042.1042.2352.2352.3662.3662.4982.4982.6292.6292.6292.7612.7612.8922.8923.0243.0243.1553.2873.4183.5503.6813.8133.9444.0764.2074.3390.40.30.20.10.00.10.20.30.40.40.30.20.10.00.10.20.30.40.0180.0240.0300.0360.0360.0420.40.30.20.10.00.10.20.30.40.40.30.20.10.00.10.20.30.40.0260.0340.0430.0430.0430.40.30.20.10.00.10.20.30.40.40.30.20.10.00.10.20.30.4-0.004-0.004-0.004-0.004",
    "reference": "# Summary Of The Paper\n\nInspired by the recently observed neural collapse (NC) phenomena in deep learning classifiers where the feature dimension is often large than the number of classes, this paper considers a more general format of neural collapse (GNC) to include the case where feature dimension $d$ is smaller than the number of class $C$. Then the authors decouple NC into two objectives: minimizing intra-class variability and maximizing inter-class separability on the hypersphere via the proposed hyperspherical uniformity. After decoupling these two objectives, the authors then propose a new objective, called hyperspherical uniform gap (HUG),  with three choices of measurement (MHE, MHS, and MGD) as examples. The proposed HUG-based loss shows some improvements in long-tailed recognition, continual learning, and adversarial robustness.\n\n# Strength And Weaknesses\n\n## Strength:\n- Overall, the paper is well-written and the results are clearly presented.\n- The authors slightly relax the definition of neural collapse to include the case where the feature dimension $d$ is smaller than the number of class $C$.\n- The paper proposes a new class of objective functions based on the hyperspherical uniform gap (HUG), which captures the difference between inter-class and intra-class hyperspherical uniformity. \n- The proposed loss improves upon CE in long-tailed recognition, continual learning, and adversarial robustness.\n\n## Weakness:\n- For the case the feature dimension $d$ is smaller than the number of class $C$, it seems that characterizing the geometry of maximally distant features on the unit sphere is still a challenging problem.  What is the minimum hyperspherical energy could be in this case? In other words, how could one tell if the features converge to GNC since the smallest hyperspherical energy is not zero?\n- It was also observed in [A] that when the feature dimension $d$ is smaller than the number of class $C$, the features are still well separated via CE loss, but not for MSE loss. But there is no analysis for CE loss. Under the setting of unconstrained features, could the new losses or CE be proved to converge to a solution of the generalized NC for that case (i.e., feature dimension $d$ is much smaller than the number of classes (C))?\n\n- I am curious whether normalization is necessary for the new objective, hyperspherical uniform gap. In other words, could the objective be extended to the case without normalization of the features?\n- Typo: Caption of figure 3 c: is it about inter-class separability?\n- Typo: Should -2<s<0 in Theorem 4?\n\n[A] Zhou et al., On the Optimization Landscape of Neural Collapse under MSE Loss: Global Optimality with Unconstrained Features, ICML 2022.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Overall, the paper is well organized and most parts are very clear. \n- Quality: this is well prepared as an academic work including theoretical proof and support experiments.\n- The paper provides a generalized neural collapse definition and new loss functions, which appear to be new. \n- In terms of reproducibility, the paper provides hyper-parameter settings for each experiment, but no code files or links are provided.\n\n# Summary Of The Review\n\nThis paper introduces a more general NC definition and introduces a new objective (HUG) by decoupling the objectives into inter-class variability and inter-class separability. Some theoretical analyses are provided for both GNC and HUG. The proposed new loss functions can achieve better performance in long-tailed recognition, continual learning, and adversarial robustness. Overall, the results could be of interest to researchers working in these areas.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTARGETED ADVERSARIAL SELF-SUPERVISED LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecently, unsupervised adversarial training (AT) has been extensively studied to attain robustness with the models trained upon unlabeled data. To this end, previous studies have applied existing supervised adversarial training techniques to selfsupervised learning (SSL) frameworks. However, all have resorted to untargeted adversarial learning as obtaining targeted adversarial examples is unclear in the SSL setting lacking of label information. In this paper, we propose a novel targeted adversarial training method for the SSL frameworks, especially for positive-pairs in SSL framework. Specifically, we propose a target selection algorithm for the adversarial SSL frameworks; it is designed to select the most confusing sample for each given instance based on similarity and entropy, and perturb the given instance toward the selected target sample. Our method significantly enhances the robustness of a positive-only SSL model without requiring large batches of images or additional models, unlike existing works aimed at achieving the same goal. Moreover, our method is readily applicable to general SSL frameworks that only uses positive pairs. We validate our method on benchmark datasets, on which it obtains superior robust accuracies, outperforming existing unsupervised adversarial training methods.\n\n1\n\nINTRODUCTION\n\nEnhancing the robustness of deep neural networks (DNN) is a critical challenge for their real-world applications. DNNs have been known to be vulnerable to adversarial attacks using imperceptible perturbations (Goodfellow et al., 2015), corrupted images (Hendrycks & Dietterich, 2019), and images with shifted distributions (Koh et al., 2021), which cause the attacked DNN models to perform incorrect predictions. A vast volume of prior studies has proposed to leverage adversarial training (AT) (Madry et al., 2018); AT explicitly uses generated adversarial examples with specific types of perturbations (e.g., l∞-norm attack) when training a DNN model. Most of these previous AT studies have considered supervised learning settings (Madry et al., 2018; Zhang et al., 2019; Wu et al., 2020; Wang et al., 2019) in which we can utilize class label information to generate adversarial examples. On the other hand, achieving robustness in a self-supervised learning (SSL) setting has been relatively understudied despite the recent success of SSL in a variety of tasks and domains.\n\nSSL frameworks (Dosovitskiy et al., 2015; Zhang et al., 2016; Tian et al., 2020b; Chen et al., 2020; He et al., 2020; Grill et al., 2020; Chen & He, 2021) have been proposed to learn transferable visual representations by solving for pretext tasks constructed out of the training data (Dosovitskiy et al., 2015; Zhang et al., 2016). A popular SSL approach is contrastive learning (e.g., SimCLR (Chen et al., 2020), MoCo (He et al., 2020)), which learns to maximize the similarity across positive pairs, each of which contains differently augmented samples of the same instance, while minimizing the similarity across different intances. Recently, to establish robustness in these SSL frameworks, RoCL (Kim et al., 2020) and ACL (Jiang et al., 2020) have proposed adversarial SSL methods based on contrastive learning frameworks. They have demonstrated improved robustness without leveraging any labeled data. However, both of these adversarial SSL frameworks are inefficient as they require a large batch size in order to attain good performances either on clean or adversarial samples.\n\nRecent SSL frameworks (Grill et al., 2020; Chen & He, 2021; Zbontar et al., 2021) mostly resort to maximizing the consistency across two differently augmented samples of the same instance, using an additional momentum encoder (Grill et al., 2020), without any negative pairs or additional\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Motivation. In supervised adversarial learning (a), perturbation is generated to maximize the crossentropy loss, which will push adversarial examples to the decision boundaries of other classes. In adversarial contrastive SSL (b), perturbation is generated to minimize the similarity (red line) between positive pairs while maximizing the similarity (blue lines) between negative pairs. Then, the adversarial examples may be pushed to the space of other classes as negative samples may mostly contain samples from other classes. However, in positive-only adversarial SSL (c), minimizing the similarity (red) between positive pairs have weaker constraints in generating effective adversarial examples than supervised AT or contrastive-based SSL. To overcome such a limitation, we suggest a selective targeted attack that maximizes the similarity (blue) to the most confusing target instance (yellow square in (c)).\n\nnetworks (Chen & He, 2021; Zbontar et al., 2021). Such non-contrastive SSL frameworks using only positive pairs are shown to obtain representations with superior generalization performance compared to contrastive counterparts in a more efficient manner. However, leveraging untargeted adversarial attacks in these SSL frameworks results in a suboptimal performance. BYORL (Gowal et al., 2021a), an adversarial SSL framework using only positive pairs, obtains much lower robust accuracies than those of adversarial contrastive-learning SSL methods on the benchmark datasets (Table 3). Then, what is the cause of such suboptimal robustness in a non-contrastive adversarial SSL framework?\n\nWe observe that this limited robustness mainly comes from the suboptimality of untargeted attacks; adversarial examples generated by the deployed untargeted attacks are ineffective in improving robustness in non-contrastive adversarial SSL frameworks. As shown in Figure 1, the attack in the inner loop of the adversarial training loss, designed to minimize the distance between two differently augmented samples, perturbs a given example to a random position in the latent space. Thus, the generated adversarial samples have little impact on the final decision boundaries. Contrarily, in contrastive SSL frameworks, the samples are perturbed toward negative samples to maximize the instance classification loss, most of which belong to different classes. Thus, the ineffectiveness of the untargeted attacks in non-contrastive SSL frameworks mostly comes from their inconsideration of other instances.\n\nTo tackle this issue, we propose Targeted Attack for RObust Self-Supervised learning (TAROSS). TAROSS is designed to enhance robustness of a non-contrastive SSL framework with only positive pairs, such as BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2021), by conducting targeted attacks, which perturbs the given instance toward a target. However, this leads to the question of which direction we want to perform the targeted attack to, that is unclear in unsupervised adversarial learning without class labels. To address this point, we consider attacking the instance toward another instance, and further perform an empirical study of which target instances help enhance robustness as opposed to randomly selected target instances, in a targeted attack. Based on our observation, we propose a simple yet effective target selection algorithm based on the similarity and entropy between instances.\n\nOur main contributions can be summarized as follows:\n\n• We demonstrate that achieving comparable robustness in the positive-only self-supervised learning (SSL) with contrastive-based SSL is difficult due to ineffective adversarial inputs generated by untargeted attacks.\n\n• We perform an empirical study of different targeted attacks for non-contrastive adversarial SSL frameworks using only positive pairs. Then, based on the observation, we propose a novel targeted adversarial attack scheme which perturbs the target sample in the direction to the most confusing instance to it, based on similarity and entropy.\n\n• We experimentally confirm that the proposed targeted adversarial SSL framework is able to obtain significantly high robustness, outperforming the state-of-the-art contrastive- and positive-only adversarial SSL methods.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of TAROSS on SimSiam. Our approach consists of three parts: target selection, targeted attack, and adversarial self-supervised learning (SSL). We propose a simple and effective similarity- and entropy-based target selection algorithm that selects the maximum score target based on score function (S) (left). When we select the target image (xt) for each instances, we conduct a targeted attack, which maximizes the similarity between the instance (xi) and the targeted (xt) (middle). Then, we train the model with targeted adversarial examples (right).\n\n2 RELATED WORK\n\nAdversarial Training Szegedy et al. (2013) showed that imperceptible perturbation to an input image may lead a DNN model to misclassify a given input into a false label, demonstrating the vulnerabilities of DNN models against adversarial attacks. Goodfellow et al. (2015) proposed the Fast Gradient Sign Method (FGSM), which perturbs a given input to add imperceptible noise in the gradient direction of decreasing the loss of a target model. They also demonstrated that training a DNN model over perturbed as well as clean samples improves the robustness of the model against FGSM attacks. Follow-up works (Kurakin et al., 2016; Carlini & Wagner, 2017) proposed diverse gradient-based strong attacks, and Madry et al. (2018) proposed a projected gradient descent (PGD) attack and a robust training algorithm leveraging a minimax formulation; it finds an adversarial example that achieves a high loss while minimizing the adversarial loss over given data points. Due to a surge of interest in achieving robustness, various defense mechanisms (Song et al., 2017; Buckman et al., 2018; Dhillon et al., 2018) have been proposed.\n\nHowever, Athalye et al. (2018) showed that many of the previous studies depend on gradient masking, which results in obfuscated gradients in the representation space. At the same time, this renders a target model highly vulnerable to stronger attacks that circumvent obfuscated gradients. TRADES (Zhang et al., 2019) proposed to minimize the Kullback-Leibler divergence (KLD) over clean examples and their adversarial counterparts, thus enforcing consistency between their predictions. They further showed that there is a theoretical trade-off relationship in achieving both clean accuracy and robustness. Recently, leveraging additional unlabeled data (Carmon et al., 2019) and conducting additional attacks (Wu et al., 2020) have been proposed. Carmon et al. (2019) proposed using Tiny ImageNet (Le & Yang, 2015) as pseudo labels, and Gowal et al. (2021b) proposed using generated images from generative models to learn richer representations with additional data.\n\nSelf-Supervised Learning Due to the high annotation cost of labeling data, SSL has recently gained a large attention (Dosovitskiy et al., 2015; Zhang et al., 2016; Tian et al., 2020a;b; Zbontar et al., 2021). Previously, SSL focused on solving a pre-task problem of collaterally obtaining visual representation, such as solving the jigsaw puzzle (Noroozi & Favaro, 2016), predicting the relative position of two regions (Dosovitskiy et al., 2015), or impainting the masked area (Pathak et al., 2016). Contrastive SSL coined with data augmentation (Chen et al., 2020; He et al., 2020) has achieved impressive performance in SSL. On the other hand, other previous studies employed a momentum network to learn visual differences between two augmented images (Grill et al., 2020) or to mimic the momentum network with stop-gradient (Chen & He, 2021).\n\nAdversarial Self-Supervised Learning The first adversarial SSL method employed contrastive learning to achieve a high level of robustness (Kim et al., 2020; Jiang et al., 2020) without any class labels. Adversarial self-supervised contrastive learning (Kim et al., 2020; Jiang et al., 2020) generated an instance-wise adversarial example that maximizes the contrastive loss against their positive and negative samples by conducting untargeted attacks. Both methods achieved robustness\n\n3\n\nEncoder )Predictor hProjector 'max1z%,z2z%z2Encoder)Projector 'Targeted attackAdversarial Self-supervisedlearning#'#-!!#.*+,!\"#.*+,!!(#.)Predictor hEncoder )Encoder )Projector 'Projector 'similarityXXTarget selection based on score function (*)#'#-B!\"=#!⋅#\"#!#\"+&\"log&\"+FG('!,'\")B%&B%'B%(B%)B%*B%+H34⋯⋯B!5Base imageTarget imageUnder review as a conference paper at ICLR 2023\n\nTable 1: Experimental results against PGD l∞ attacks on ResNet18 where all models are trained on the CIFAR-5 for simple observation. The CIFAR-5 includes airplane, automobile, bird, cat, and deer from CIFAR-10.\n\nSelf-supervised framework Method\n\nContrastive based approach\n\nSimCLR SimCLR\n\nACL Jiang et al. (2020) RoCL Kim et al. (2020)\n\nNon-contrastive based approach with untargeted attack\n\nBYOL SimSiam\n\nUntargeted attack Untargeted attack\n\nNon-contrastive based approach with targeted attack (Ours)\n\nBYOL BYOL SimSiam SimSiam\n\nRandomly targeted attack Entropy and Similarity based targeted attack Randomly targeted attack Entropy and Similarity based targeted attack\n\nClean\n\nPGD l∞\n\n80.84 87.74\n\n75.40 66.36\n\n83.50 87.08 77.08 79.54\n\n39.16 47.60\n\n21.00 36.53\n\n36.50 35.18 47.58 49.50\n\nwith the cost of requiring large computation power due to a large batch size for contrastive learning. On the other hand, Gowal et al. (2021a) utilized only positive samples to obtain adversarial examples by maximizing the similarity loss between the latent vectors from the online and target networks, enabling this method free for batch size. However, it exhibited relatively poorer robustness than that of self-supervised contrastive learning, even with an advanced SSL framework. Despite this advanced framework, i.e., non-contrastive SSL, that employs only positive pairs, robustness is not guaranteed with a simple combination of untargeted adversarial learning and advanced SSL. To overcome such vulnerability in non-contrastive SSL, we propose a targeted attack leveraging a novel score function designed to improve robustness.\n\n3 TARGETED ADVERSARIAL SELF-SUPERVISED LEARNING\n\nAdversarial SSL has been proposed to apply existing supervised adversarial learning to models trained with SSL; this method generates adversarial examples in an untargeted manner and leverages these examples to improve robustness. Despite previous works being rather a straightforward combination of two approaches, we argue that leveraging untargeted adversarial attacks still leaves large room for robustness to improve.\n\n3.1 ATTACK IN ADVERSARIAL SELF-SUPERVISED LEARNING\n\nAdversarial supervised training. To explain adversarial SSL, we first recap adversarial supervised training with our notations. We denote the dataset D = {X, Y }, where x ∈ X is a training example, and y ∈ Y is its corresponding label. In this supervised learning task, a model is fθ : X → Y , where θ is a set of the model parameters to train.\n\nGiven D and fθ, an adversarial attack generates an adversarial example by adding a perturbation to a given source image that maximizes the loss within a certain radius from it (e.g., l∞ norm balls). We define this adversarial l∞ attacks as follows:\n\nδt+1 = ΠB(0,ε)\n\n(cid:16)\n\nδt + αsign\n\n(cid:16)\n\n∇δtLCE\n\n(cid:0)f (θ, x + δt), y(cid:1)(cid:17)(cid:17)\n\n,\n\n(1)\n\nwhere B(0, ε) is the l∞ norm-ball with radius ε, Π is the projection function to the norm-ball, α is the step size of the attacks, and sign(·) is the sign of the vector. Also, the perturbation δ is the accumulated perturbations by αsign(·) over multiple iterations t, and LCE is the cross-entropy loss. In the case of PGD (Madry et al., 2018), the attack starts from a random point within the epsilon ball and performs t gradient steps, to obtain a perturbed sample xadv.\n\nAdversarial training. (AT) is a straightforward way to improve the robustness of the model by minimizing the training loss embedding the adversarial described above as an inner loop, as follows:\n\nLAT = max\n\nδ∈B(x,ε)\n\nLCE\n\n(cid:0)f (θ, x + δ), y(cid:1).\n\n(2)\n\nUntargeted adversarial self-supervised learning. Previously proposed adversarial SSL methods (Jiang et al., 2020; Kim et al., 2020; Gowal et al., 2021a) naturally combined the adversarial supervised training and SSL frameworks. Therefore, all previous works design the inner loop of\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Targeted Attack Robust Self-Supervised Learning (TAROSS)\n\nRequire: Dataset D, transformation function T, model f , parameter of model θ, target score function\n\nS, constant w for iter ∈ number of iteration do\n\nfor xi ∈ miniBatch B = {x1, . . . , xm} do\n\nfor r in 2 do\n\ntransform input Tr(xi) Find target images tr(xj) = S(Tr(xi), B) Generate targeted adversarial examples Tr(xi)adv = Tr(xi) + δt δt+1 = ΠB(0,ε)\n\nδt + αsign\n\n∇δtLtarget\n\n(cid:16)\n\n(cid:16)\n\n(cid:0)f (θ, Tr(xi) + δt), f (θ, Tr(xj)))(cid:1)(cid:17)(cid:17)\n\nend for Calculate training loss LTAROSS = Lselfsup(T1(xi), T1(xi)adv) + w · Lselfsup(T1(xi)adv, T2(xi)adv)\n\nend for Optimize the weight θ over LTAROSS\n\nend for\n\nadversarial attack with untargeted attack as follows:\n\nδt+1 = ΠB(0,ε)\n\n(cid:16)\n\nδt + αsign\n\n(cid:16)\n\n∇δtLloss\n\n(cid:0)f (θ, T1(x) + δt), f (θ, T2(x))(cid:1)(cid:17)(cid:17)\n\n,\n\n(3)\n\nwhere perturbation is generated to maximize the self-supervised loss Lloss that minimizes the similarity between positive pairs, and maximize the similarity between negative pairs if negative pairs exists (Jiang et al., 2020; Kim et al., 2020).\n\nTargeted adversarial self-supervised learning We argue that leveraging untargeted adversarial attacks in an SSL framework using positive-only pairs still leaves large room for better robustness. We can simply improve robustness in non-contrastive SSL by changing the inner loop to a randomly targeted attack as follows:\n\nδt+1 = ΠB(0,ε)\n\n(cid:16)\n\nδt + αsign\n\n(cid:16)\n\n∇δtLtarget\n\n(cid:0)f (θ, x + δt), f (θ, x′))(cid:1)(cid:17)(cid:17)\n\n,\n\n(4)\n\nwhere Ltarget = −Lloss, x′ is a randomly selected target within the batch. To precisely elaborate our observation in the latter section, we introduce a score function (S(x, batch)) to select the target (x′) within the batch. The score function (S) can be a random-sampling function or a designed function as Eq. 6. In other words, the score function outputs the target (x′) corresponding to the base image (x), then the targeted attack generates perturbation that maximizes the similarity to target x′.\n\nIn Table 1, an untargeted attack makes the model more vulnerable when the non-contrastive SSL frameworks utilize only positive pairs, such as in BYOL (Grill et al., 2020), and SimSiam (Chen & He, 2021). However, when we modify the inner loop of the untargeted attack to the targeted attack with the simple random sampling score function as Eq. 4., non-contrastive SSL achieves better robustness.\n\n3.2 OBSERVATIONS OF TARGET SELECTION BASED ON SCORE FUNCTION\n\nIn the previous section, we found that the targeted attack can improve robustness even with randomly selected target images in the batch. However, we argue there exist effective target images for each base input x that can further improve robustness. However, selecting effective target instances for the targeted attack is not trivial. To enable this, we propose leveraging the K-means clustering-based and similarity-based score functions. We mainly observe our score function on SimSiam (Chen & He, 2021) and futher describe the details of our method for observations in Appendix C.\n\nK-means clustering-based target selection. Intuitively, perturbing the most confusing class can be a straightforward way of implementing a strong adversarial attack for better robustness. To validate our assumption, we first design the score function based on labels from K-means clustering. To find the most confusing instance, we label the pseudo class yi on the source images xi with K-means clustering and then find the most closest cluster yt to the pseudo class yi based on the centroid vector of each cluster. Accordingly, we can filter the instances that are labeled as yt from the batch.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAmong those, the score function selects the instance that is closest to the source images xi.\n\nTable 2: Results of our TAROSS with K-means clustering based-, and similarity based- selection for targeted attack on CIFAR5.\n\nClean\n\nSelection\n\nAs shown in Table 2, the K-means clustering target selection score function shows better robustness than the random selection when performing targeted attacks. However, K-means clustering requires a lot of computation resources to derive a pseudo class label at every iteration, and it is vague to set the K value when there is no label information. Moreover, we cannot guarantee that the closest instance in the confused cluster is always the best candidate to improve robustness (Zhang et al., 2020) becase the closest cluster to an instance itself may differ from the closest cluster to the pseudo class cluster. For example, when the cat class cluster is close to the dog class cluster, there could be a cat instance close to horse images on the other side. Therefore, we also test the target selection score function based on the distance of each instance.\n\nUntargeted* Random Similarity K-means clustering\n\n66.36 77.08 76.04 75.86\n\n36.53 47.58 48.73 49.39\n\nPGD l∞\n\nSimilarity-based target selection. In SSL, the similarity between given instances is an important metric to learn a good visual representation without any class label information (Chen et al., 2020; Tian et al., 2020b). Therefore, we leverage the similarity score to find optimal target images in the adversarial SSL. Before designing the score function based on the similarity score, we first observe the influence scores (Koh & Liang, 2017) of training points that are generated by the targeted attacks with randomly selected targets. The influence score is designed to approximate the influence function in DNN to compute the degree to which the training points are responsible for a given prediction (Koh & Liang, 2017). We followed Koh & Liang (2017) to calculate the influence scores of the adversarial examples that are generated by the targeted attack with a variety of target images. However, since this approximation requires a lot of computations, and the possible pair for the targeted attack is squared of the batch size, we sampled 60,000 training points to observe the correlation between the similarity and influence scores.\n\nWe found that the input pairs having the similarity of around 0.0 have relatively high influence scores (Figure 7). To empirically verify whether the instances with the high influence score actually act as an effective target to show better robustness in adversarial SSL, we design an experiment in which we select target images based on the similarity. We experiment with five cases that have a different range of the similarity: r1: −1.0 ∼ −0.5, r2: −0.5 ∼ −0.25, r3: −0.25 ∼ 0.0, r4: 0.0 ∼ 0.25, r5: 0.25 ∼ 0.5, and r6: 0.5 ∼ 1.0. As shown in Figure 3, when we select the target images between −0.25 ∼ 0.25, it improves robustness compared to that using the random selection algorithm.\n\nFigure 3: Robustness of the model that conduct targeted attack based on similarity score which selecting the target image that are in pre-defined range (r1, · · · , r6).\n\n3.3 TARGETED ATTACK WITH ENTROPY- AND SIMILARITY-BASED SCORE FUNCTION\n\nIn this section, we describe our overall framework to achieve robustness in non-contrastive SSL performing targeted attacks wherein targets are selected by leveraging our proposed score function. The target score function is designed based on the observation in the previous section.\n\nPositive-paired self-supervised learning. Our targeted attack is designed for an SSL framework that only employs positive pairs of transformed images. Therefore, we mainly describe our method on the SimSiam (Chen & He, 2021).\n\nWe first describe an SSL method, SimSiam (Chen & He, 2021), that learns visual representations with only positive pairs using a stop-gradient. Let us denote the dataset D = {X}, and transformation set T that augments the images x ∈ X. SimSiam consists of the encoder f , followed by the projector g, and the predictor h; each of g and h is a multi-layer perceptron (MLPs). To learn visual representations, SimSiam maximizes the cosine similarity between the positive pairs as follows:\n\nLSimSiam(T1(x), T2(x)) = −\n\n1 2\n\np1 ||p1||2\n\n·\n\nz2 ||z2||2\n\n−\n\n1 2\n\np2 ||p2||2\n\n·\n\nz1 ||z1||2\n\n,\n\n(5)\n\n6\n\nCleanPGDAutoAttackAccuracy (%)020406080r1r2r3r4r5r6Under review as a conference paper at ICLR 2023\n\nwhere pi = h(g(f (Ti(x)))) and zi = g(f (Ti(x))) are output vectors of the predictor h and the projector g, respectively, where i stands for index of the differently augmented two images. Before calculating the loss, SimSiam stops the gradient on the z. Stop-gradient helps the model to form proper visual representations without any momentum network but makes the single model acts like a momentum network.\n\nEntropy and similarity based target selection. From the observation in the previous paragraph, we argue that selecting target instances that are most confused and are adequately close to the base instance is effective for targeted attacks. For this, we design the score function based on the similarity and entropy values; it does not need any class information, as follows:\n\nS(T1(xi), T2(xj)) =\n\nei ||ei||2\n\n·\n\nej ||ej||2\n\n+ (pj/τ ) log (cid:0)pj/τ ),\n\n(6)\n\nwhere pj = h(g(f (T1(xj)))) and ei = f (T1(xi)) are output vectors of predictor h and encoder f , respectively. Overall, the score function S consists of a cosine similarity term, and an entropy term. The cosine similarity is calculated between features of base images and candidate images in the differently augmented batch (T2). The entropy is calculated with the assumption that the vector p is an instance’s logit as Caron et al. (2021). We design the score function to work as a combination of selection algorithms based on K-means clustering and similarity values. It naturally selects the target that is mostly confused with base images (Figure 1). We also verify our score function selects targets as our intention in the experiment section (Figure 4b).\n\nRobust self-supervised learning with target attack (TAROSS). In non-contrastive SSL, we make a positive pair, i.e., T1(x), T2(x), with differently transformed augmentation. As shown in Figure 2, to generate adversarial examples, we first select the target images T1(x)′, T2(x)′ for each base image T1(x), T2(x), respectively, which have the maximum score from score function (S). Here, we employ score function (S) in Eq. 6. Then, we generate adversarial examples, i.e., T1(x)adv, T2(x)adv, for each transformed input with our suggested targeted attack (Eq. 4) where Ltarget = −LSimSiam. Finally, we maximize the agreement between adversarial images T1(x)adv, and T2(x)adv with clean image T1(x) as follows:\n\nLTAROSS = L(T1(x), T1(x)adv) + w · L(T1(x)adv, T2(x)adv) (7) where L is Eq. 5. Since all three instances have the same identity, we maximize the similarity between the clean image and adversarial examples. Overall, TAROSS is summarized in Algorithm 1.\n\n4 EXPERIMENT\n\nIn this section, we validate our TAROSS on non-contrastive adversarial SSL frameworks (Section 4.1). Moreover, we evaluate the robustness of the representation with different datasets and in transferlearning tasks (Section 4.2). Finally, we analyze why our targeted attacks help achieve better robustness compared to models using untargeted attacks (Section 4.3).\n\nExperimental setup. We validate our TAROSS on top of BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2021). To compare the effectiveness of TAROSS, we also implement noncontrastive adversarial SSL using untargeted attacks in each SSL framework. All the models are trained on ResNet18 with l∞ PGD attacks with an attack step of 10 with epsilon 8/255. We evaluate the robustness against AutoAttack (Croce & Hein, 2020a) and l∞ PGD attacks with epsilon 8/255 using an attack step of 20 iterations. We further describe the details of our experimental settings in Appendix B. The code will be available in Anonymous.\n\n4.1 EVALUATING ROBUSTNESS\n\nRobustness with targeted/untargeted attack. In Table 3, as we observed in the previous section, when replacing untargeted attacks with targeted attacks in positive-only SSL (BYOL, SimSiam), it obtains robustness comparable to contrastive adversarial SSL (RoCL, ACL), which verifies our motivation. Therefore, it is feasible to improve robustness even in positive-only SSL while attaining the pros of positive-only SSL frameworks. Interestingly, the targeted attacks helped improve the clean accuracy in SSL when compared with those using untargeted attacks in positive-only SSL. We conjecture that untargeted attacks are not only inadequate to learn robust features but also hinder to learn good visual representation of natural images.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Experimental results against white box attacks on ResNet18 trained on the CIFAR10. We evaluate adversarial SSL with linear evaluation and robust linear evaluation. Clean is the accuracy of clean images. All models are evaluated with PGD l∞ with 20 steps of ε = 0.0314 and AutoAttack (Croce & Hein, 2020a). Untargeted attack maximizes the training loss between the differently augmented but the same instances to generate adversaries. To see the effectiveness, we test TAROSS on positive-only SSL, i.e., BYOL, SimSiam.\n\nTrain type\n\nSelf-supervised framework Method\n\nSelf-supervised linear evaluation\n\nSelf-supervised robust linear evaluation\n\nContrastive\n\nPositive-only\n\nContrastive\n\nPositive-only\n\nSimCLR SimCLR\n\nBYOL BYOL SimSiam SimSiam\n\nSimCLR SimCLR\n\nBYOL BYOL SimSiam SimSiam\n\nRoCL (Kim et al., 2020) ACL (Jiang et al., 2020)\n\nUntargeted attack TAROSS Untargeted attack TAROSS\n\nRoCL (Kim et al., 2020) ACL (Jiang et al., 2020)\n\nUntargeted attack TAROSS Untargeted attack TAROSS\n\nClean\n\n78.14 79.96\n\n72.65 84.52 71.78 74.06\n\n76.53 77.17\n\n54.01 74.33 68.88 76.19\n\nPGD l∞ AutoAttack\n\n42.89 39.37\n\n16.20 31.19 37.28 44.71\n\n47.51 40.67\n\n27.24 40.84 37.84 45.57\n\n27.19 35.97\n\n0.01 21.01 32.41 36.39\n\n30.22 39.13\n\n4.49 29.91 31.44 37.25\n\nTable 4: Results of linear evaluation in STL10 and CIFAR10. Rob. stands for robust accuracy against PGD l∞ attack with 20 steps and ε = 0.0314.\n\nTable 5: Results of adversarial transfer learning to CIFAR10 and STL10 from CIFAR100. Rob. stands for robust accuracy against l∞ PGD attack.\n\nSTL10\n\nCIFAR100\n\nMethod\n\nAttack\n\nClean\n\nRob.\n\nClean\n\nRob.\n\nUntarget RoCL Untarget ACL BYOL Untarget SimSiam Untarget SimSiam TAROSS\n\n52.63 54.14 61.02 43.88 46.38\n\n19.19 19.69 12.14 12.70 21.32\n\n45.99 41.09 53.43 27.53 36.02\n\n17.17 15.31 14.81 14.20 22.18\n\nCIFAR100 → Method\n\nCIFAR10\n\nSTL10\n\nAttack\n\nClean\n\nRob.\n\nClean\n\nRob.\n\nRoCL ACL BYOL SimSiam SimSiam\n\nUntarget Untarget Untarget Untarget TAROSS\n\n73.93 61.68 71.06 50.05 50.50\n\n18.62 15.66 16.57 25.43 25.44\n\n74.06 59.56 63.53 34.54 43.13\n\n19.06 13.53 12.93 24.82 22.46\n\nRobustness compared to contrastive-based approaches. We compare our TAROSS to contrastive adversarial SSL methods to show that our approach could make positive-only SSL to achieve robustness comparable to previous works. To this end, we evaluate adversarial SSL with linear evaluation and robust linear evaluation against AutoAttack (Croce & Hein, 2020a) and PGD (Madry et al., 2018) l∞ attacks. Notably, as shown in Table 3, the targeted attacks allow the positive-only SSL model to have better robustness than that of contrastive adversarial SSL (RoCL, ACL) in linear evaluation. Specifically, when we train the fc layer, our model achieves 36.39% of robustness against strong AutoAttack which shows the effectiveness of our proposed method. Interestingly, the SimSiam-based model has already obtained good robustness in the linear evaluation compared to the robust linear evaluation which takes ×4 times to train. We conjecture that SimSiam-based TAROSS already has a robust well-cluster after the robust pretraining which does not need to find robust decision boundaries with the adversarial examples. Since the SimSiam directly maximizes the similarity between adversary and clean images only between the positive pairs with the single models, the gap between the representation of adversaries and clean examples may reduce. Moreover, our approach is also applicable to positive pairs in the contrastive-based approaches. As shown in Table 6, our TAROSS also could improve both clean and robustness of contrastive-based approaches. This verifies that targeted attack generates effective adversaries for self-supervised learning frameworks which is model agnostic approaches.\n\n4.2 EVALUATING THE QUALITY OF ROBUST REPRESENTATION\n\nRobustness on multiple benchmarks datasets. We validate our method on multiple benchmark datasets, such as CIFAR100 and STL10. In Table 4, our TAROSS consistently shows good robustness across different benchmark datasets when comparing with adversarial SSL frameworks using untargeted attacks. Especially, when the dataset becomes larger, such as CIFAR100, our method achieves even better robustness than contrastive learning-based approaches, e.g., RoCL, and ACL, where these approaches are sensitive to the number of same class samples in the single batch size. From this result, we argue that our targeted adversarial SSL contributes to learning better robustness in the larger dataset than contrastive-based SSL.\n\nTransfer to different data. SSL is effective to utilize in several downstream tasks with a pretrained encoder. Therefore, we also evaluate how our robust pretrained features help in the different datasets in transfer tasks, which demonstrates the quality of our robust pretrained features in a different dataset.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nWe followed the experimental setting as supervised adversarial transfer learning tasks (Shafahi et al., 2020) which freeze the encoder and train only the fc layer. We pretrained the model on CIFAR100 and evaluate the robust transferability to STL10, and CIFAR10. Once our model is trained, we transfer the robust features to different kinds of datasets with fewer epochs and parameters, which is efficient in that we do not need to conduct adversarial training from scratch. In Table 5, our model also shows impressive transferability both in CIFAR10 and STL10 compare to untargeted adversarial SSL. Moreover, while our model shows relatively low clean accuracy, TAROSS could obtain about 10% gain in robustness compared to contrastive-based adversarial SSL. From these results, we confirm that our methods generate robust representations that can be transferred to several downstream tasks, which previous adversarial SSL studies addressed.\n\n4.3 ANALYSIS OF TARGETED ATTACKS\n\nIn this section, we further analyze the targeted attacks in adversarial SSL to uncover evidence for why our proposed method is effective in adversarial SSL. We analyze which class is frequently selected when using our score function. Then, we visualize the difference between an untargeted attack and a targeted attack in the representation space.\n\nAnalyze the distribution of the target class. We examine which class is selected as a target by the score function (S). We test the selected target of the single class (airplane) to the adversarial supervised trained models (Madry et al., 2018) to obtain the probability and prediction for analyzing the class distribution. As shown in Figure 4b, (a) Mean predict probahalf of the target images are the same class as bility of base images the base image. And the next most frequently Figure 4: Analysis of target from score function (S) selected class was the class with the second highest probability that the images of the corresponding class were most confused (Figure 4a). The score function we proposed selects images of the same class when the image in the center of the cluster, and images near the decision boundary that have a high probability to predict to a wrong class select images of a different class. Through these results, it can be confirmed that we designed a score function that appropriately uses K-means clustering to always select a different class from itself and heuristic target selection, which performs an instance-wise attack close to a similarity value, depending on the case, as we intended.\n\n(b) Distribution of class of targeted images\n\nVisualization of embedding space. To observe the differences between images that are generated with targeted versus untargeted attacks, we visualize the embedding space of targeted attack examples and untargeted attack examples. In Figure 5, adversarial examples are denoted with dark blue, and clean examples are denoted with light blue; both are instances of the same class. As shown in Figure 5a, untargeted adversarial examples are located near clean examples. On the other hand, targeted adversarial examples are located near the boundary of the cluster (Figure 5b). This visualization shows that our targeted attack generates relatively more effective adversarial examples than untargeted attacks, which is likely to push the decision boundary to have a better robust representation.\n\nFigure 5: Visualize embedding\n\n(a) Untargeted attack\n\n(b) Targeted attack\n\n5 CONCLUSION\n\nIn this paper, we showed that a simple combination of supervised adversarial training with SSL is highly suboptimal due to the ineffectiveness of adversarial examples generated by the untargeted adversarial attacks perturbed to random places without the consideration of decision boundaries. Based on this observation, we propose an instance-wise targeted attack scheme for an adversarial SSL framework. This scheme selects the target instance based on the similarity and entropy, such that the given instance is perturbed to be confused with the selected target. Our targeted adversarial SSL framework obtains representation that achieves better robustness than the state-of-the-art adversarial SSL frameworks, including contrastive ones, without using any negative pairs or additional networks. We believe that our work opens a door to future research on the search for more effective targeted attacks, for adversarial SSL.\n\n9\n\nMean Probabilty (%)00.20.4planeshipcartruckbirdcatdeerdogfroghorseAll samplesMisclassified samplesNumber of Samples0200400planeshipcartruckbirdcatdeerdogfroghorseUnder review as a conference paper at ICLR 2023\n\nReproducibility Statement\n\n• Datasets. We use CIFAR10, CIFAR100, STL10, and CIFAR5 datasets for our experiment.\n\nTo see more details, please see Supplementary B.2.\n\n• Baselines. We use following models as our baseline: RoCL (Kim et al., 2020), ACL (Jiang et al., 2020), and BYORL (Gowal et al., 2021a). To see the training details, please see Supplementary A\n\n• Robustness Test. We test our model and baselines against PGD attacks (Madry et al., 2018) and AutoAttack (Croce & Hein, 2020a). To see the evaluation details for robustness, please see Section 4. Experimental setup., and Supplementary B.4.\n\n• TAROSS. To train our model, please see Supplementary B.3.\n\n• Table 1. To reproduce the results in Table 1, please see Section 3.1.\n\n• Table 2. To reproduce the results in Table 2, please see Section 3.2. K-means clustering-\n\nbased target selection., and Supplementary C\n\n• Figure 3. To reproduce the results in Figure 3, please see Section 3.2. Similarity-based\n\ntarget selection, and Supplementary C\n\n• Figure 4, and 5. To reproduce results of Figure 4, and 5, please see Supplementary D.\n\nREFERENCES\n\nMaksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, pp. 484–501. Springer, 2020.\n\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning, 2018.\n\nJacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. In International Conference on Learning Representations, 2018.\n\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017\n\nIEEE symposium on security and privacy (sp), pp. 39–57. IEEE, 2017.\n\nYair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data improves adversarial robustness. Advances in Neural Information Processing Systems, 2019.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 9650–9660, 2021.\n\nDavid M Chan, Roshan Rao, Forrest Huang, and John F Canny. Gpu accelerated t-distributed stochastic neighbor embedding. Journal of Parallel and Distributed Computing, 131:1–13, 2019.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\n\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International Conference on Machine Learning, pp. 2206–2216. PMLR, 2020a.\n\nFrancesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive boundary attack. In International Conference on Machine Learning, pp. 2196–2205. PMLR, 2020b.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nGuneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. International Conference on Learning Representations, 2018.\n\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE transactions on pattern analysis and machine intelligence, 38(9):1734–1747, 2015.\n\nLijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, and Chuang Gan. When does contrastive learning preserve adversarial robustness from pretraining to finetuning? Advances in Neural Information Processing Systems, 34, 2021.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In International Conference on Learning Representations, 2015.\n\nSven Gowal, Po-Sen Huang, Aaron van den Oord, Timothy Mann, and Pushmeet Kohli. Selfsupervised adversarial robustness for the low-label, high-data regime. In International Conference on Learning Representations, 2021a.\n\nSven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A Mann. Improving robustness using generated data. Advances in Neural Information Processing Systems, 34:4218–4233, 2021b.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. Advances in Neural Information Processing Systems, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019.\n\nZiyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Robust pre-training by adversarial\n\ncontrastive learning. In Advances in Neural Information Processing Systems, 2020.\n\nMinseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning.\n\nAdvances in Neural Information Processing Systems, 2020.\n\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In\n\nInternational conference on machine learning, pp. 1885–1894. PMLR, 2017.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 5637–5664. PMLR, 18–24 Jul 2021.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097–1105, 2012.\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.\n\narXiv preprint arXiv:1607.02533, 2016.\n\nYa Le and X. Yang. Tiny imagenet visual recognition challenge. In TinyImageNet, 2015.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw\n\npuzzles. In European Conference on Computer Vision, pp. 69–84. Springer, 2016.\n\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2536–2544, 2016.\n\nAli Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, and Tom Goldstein. Adversarially robust transfer learning. International Conference on Learning Representations, 2020.\n\nYang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. International Conference on Learning Representations, 2017.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding.\n\nIn European\n\nConference on Computer Vision, 2020a.\n\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. In Advances in Neural Information Processing Systems, 2020b.\n\nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In International Conference on Learning Representations, 2019.\n\nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-\n\nization. Advances in Neural Information Processing Systems, 33, 2020.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St ́ephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pp. 12310– 12320. PMLR, 2021.\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, 2019.\n\nJingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In International Conference on Machine Learning, pp. 11278–11287. PMLR, 2020.\n\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization.\n\nIn European\n\nConference on Computer Vision, pp. 649–666. Springer, 2016.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nSupplementary Material\n\nTargeted Adversarial Self-Supervised Learning\n\nA BASELINES.\n\n• RoCL (Kim et al., 2020). RoCL is SimCLR (Chen et al., 2020) based adversarial selfsupervised learning methods. We experiment with the official code1. To make a fair comparison, we set the attack step to 10 as other baselines. We train the model with 1,000 epochs under the LARS optimizer with weight decay 2e−6 and momentum with 0.9. For the learning rate schedule, we also followed linear warmup with cosine decay scheduling. We set a batch size of 512 for all datasets (CIFAR10, CIFAR100, STL10). For data augmentation, we use a random crop with 0.08 to 1.0 size, horizontal flip with a probability of 0.5, color jitter with a probability of 0.8, and grayscale with a probability of 0.2 for RoCL training.\n\n• ACL (Jiang et al., 2020) ACL is SimCLR (Chen et al., 2020) based adversarial selfsupervised learning methods. We conduct the experiment with the official code2. To make a fair comparison, we set the attack step to 10 as other baselines. We train the model with 1,000 epochs. We set a batch size of 512 for STL10 dataset. For CIFAR10, and CIFAR100, we use the official pretrained checkpoints. For data augmentation, we use a random crop with 0.08 to 1.0 size, horizontal flip with a probability of 0.5, color jitter with a probability of 0.8, and grayscale with a probability of 0.2 for ACL training. We set PGD dual mode which calculates both clean and adversarial during the training.\n\n• BYORL (Gowal et al., 2021a) BYORL is BYOL (Grill et al., 2020) based adversarial selfsupervised learning methods for low label regime. Since there is no official code for BYORL we implement the BYORL by ourselves. Though we mentioned as BYOL with untargeted attack in the Table 1, 3, 4 and 5, please note that BYOL with untargeted approach stands for Gowal et al. (2021a). We implement based on BYOL from a self-supervised learning library3. We use the same CIFAR-10 setting in the library except for normalization. We exclude normalization in the data augmentation. To make a fair comparison, we implement on the ResNet18 with attack step 10 of PGD. As shown in supplementary materials in Gowal et al. (2021a), when the model is trained with 10 steps in ResNet34 it shows 37.88% of robustness. We conjecture that we have a different performance from the original paper because the original paper employs 40 steps of PGD in WideResNet34 to obtain the reported robustness which requires extraordinary computation power.\n\n• AdvCL (Fan et al., 2021) AdvCL is SimCLR (Chen et al., 2020) based adversarial selfsupervised learning which employ pseudo labels from the model that is pretrained on ImageNet (Krizhevsky et al., 2012) data. Even though the outstanding performance of AdvCL, we exclude this model as our baseline because the proposed methods require the model that is trained with the labels of ImageNet which we assume to have no label information for pretraining the model.\n\nB DETAILED DESCRIPTION OF EXPERIMENTAL SETUPS\n\nB.1 RESOURCE DESCRIPTION.\n\nAll experiments are conducted with a two NVIDIA RTX 2080 Ti, except for the experiments with CIFAR100 experiments. For CIFAR100 experiments, two NVIDIA RTX 3080 are used. All experiments are processed in Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz.\n\n1https://github.com/Kim-Minseon/RoCL 2https://github.com/VITA-Group/Adversarial-Contrastive-Learning 3https://github.com/vturrisi/solo-learn\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB.2 DATASET DESCRIPTION.\n\nFor experiments, we use CIFAR 10, CIFAR 100, CIFAR 5, and STL10. CIFAR 10 and CIFAR 1004 consist of 50,000 training images and 10,000 test images with 10 and 100 classes, respectively. All CIFAR images are 32×32×3 resolution (width, height, and channel). CIFAR 5 is a subset of CIFAR 10 which contains all images from 5 classes: airplane, automobile, bird, cat, and deer. We also test with determined classes. The STL105 consists of 5,000 training images and 8,00 test images with 10 classes. All STL10 images are 96×96×3 resolution (width, height, and channel.) However, for our experiment in Table 4, 5, we resize the images into 32×32×3 resolution (width, height, and channel).\n\nB.3 TRAINING DETAIL.\n\nFor all methods, we train on the ResNet18 (He et al., 2016) with l∞ attacks with the attack strength of ε = 8/255 and the step size of α = 2/255, with the number of inner maximization iteration set to K = 10. For the optimization, we train every model for 800 epochs using the SGD optimizer with the learning rate of 0.05, weight decay of 5e−4, and the momentum of 0.9. For data augmentation, we use a random crop with 0.08 to 1.0 size, horizontal flip with a probability of 0.5, color jitter with a probability of 0.8, and grayscale with a probability of 0.2. We exclude normalization for adversarial training. We set the weight of adversarial similarity loss w as 2.0. We use batch size 512 with two GPUs.\n\nIn the score function, we calculate the similarity score term and entropy term as shown in Equation 6. First, to exclude the positive pairs’ similarity score we set the similarity score between positive pairs to −1. Then, to calculate the overall score, after obtaining the similarity score and entropy of each sample, we normalize each component with Euclidean normalization to balance each component to score function.\n\nB.4 EVALUATION DETAILS.\n\nPGD l∞ attack. For all PGD l∞ attacks used in the test time, we use the projected gradient descent (PGD) attack with the strength of ε = 8/255, with the step size of α = 8/2550, and with the number of inner maximization iteration set to K = 20 with the random start.\n\nAutoAttack. We further test against strong gradient based attack, i.e., AutoAttack (AA) Croce & Hein (2020a). AutoAttack is an ensemble attack of four different attacks (APGD-CE, APGD-T, FABT (Croce & Hein, 2020b), and Square (Andriushchenko et al., 2020)). AGPD-CE is an untargeted attack, APGD-T and FAB-T are targeted attacks. The Square is a black box attack. We use official code to test models6.\n\nSelf-supervised learning. For self-supervised learning, we denote linear evaluation when we use only clean images to train the fully connected (fc) layer after the pretraining phase. When we denote robust linear evaluation, we train the fc layer with adversarial examples. While ACL uses partial fine-tuning to obtain their reported accuracy and robustness, to make a fair comparison we freeze the encoder and train only the fc layer. Robust fine-tuning is training all parameters including parameters of the encoder with adversarial examples. For linear evaluation, we followed baselines hyperparameters for each model. We train the baseline models with 150 epochs, 25 epochs, and 50 epochs for RoCL, ACL, and BYORL, respectively. We also followed their learning rate 0.1, 0.1, and 2 × 10−3 for RoCL, ACL, and BYORL, respectively. On the other hand, we train our model with 100 epochs with a learning rate of 0.5 for linear evaluation. We use AT loss for robust linear evaluation except for ACL. For ACL, we use TRADES loss as the official code.\n\nC EXPERIMENT DETAILS OF OUR TARGETED ATTACK\n\nK-means cluster-based TAROSS Intuitively, perturbing to the most confusing class is a straightforward way of implementing a strong adversarial attack for better robustness. To validate our\n\n4http://www.cs.toronto.edu/ ̃kriz/cifar.html. 5https://ai.stanford.edu/ ̃acoates/stl10/ 6https://github.com/fra31/auto-attack\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nassumption, we design the score function based on labels from K-means clustering. We set K as 5 for CIFAR 5 in Table 2.\n\nAs show in Figure 6, we label the pseudo-class yi on the source images xi with the feature zi. Then find the closest cluster yt to pseudo class yi based on the centroid vector c of each cluster. We use the cosine similarity function to measure the distance between the latent vector z and centroid vector c. To calculate the K-means cluster for each batch, K-means cluster-based TAROSS takes high computation.\n\nFigure 6: K-means clustering target selection\n\nSimilarity score based TAROSS To calculate the influence score during the training, we employ underfitted adversarial selfsupervised model which learns visual representation a little with 400 epochs. To calculate all pairs of the training set, it requires O(D2). Moreover, the computation of influence score (Koh & Liang, 2017) for single pair also takes a lot due to approximation. Therefore, we randomly sampled a subset of the dataset from the training set. We use official code of influence score7. As shown in Figure 3, samples that are in −0.25 ∼ 0.25 shows relatively high influence score.\n\n(a) Histogram of cosine similarity between sampled pairs.\n\n(b) Scatter of influence score based on similarity\n\nFigure 7: Analysis with influence score.\n\nD EXPERIMENTAL DETAILS OF ANALYSIS\n\nAnalysis the distribution of target class To analyze the target from the score function (S), we employ an adversarially supervised trained model. We calculate the score function that is trained with our TAROSS on SimSiam. We use a train set. For each class, we calculate the mean predict probability which is the average of all softmax outputs of target images from the supervised trained model. Further, we also count the number of samples that are predicted for each class. In Figure 4, the results are target images of the airplane as a base image. There is a similar tendency even though we change the base class to other classes as shown in the following Figure 8.\n\n(a) Distribution of class of target of automobile.\n\n(b) Mean predict probability of automobile\n\n(c) Distribution of class of target of deer.\n\n(d) Mean predict probability of deer\n\nFigure 8: Analysis of target distribution in different class\n\nVisualization of embedding space To visualize the embedding of our targeted attack and untargeted attack, we use t-Distributed Stochastic Neighbor Embedding (t-SNE) (Chan et al., 2019) with the cosine similarity metric. Our TAROSS model is trained on CIFAR10 as a feature extractor. We sample a few examples and conduct two types of attacks, untargeted attack, and targeted attack. To visualize more effectively we ignore the other seven classes in CIFAR10. We visualize clean examples from three classes and then visualize adversaries that are generated with our targeted attack and untargeted attack, respectively with dark blue.\n\n7https://github.com/kohpangwei/influence-release\n\n15\n\nHistogram of similarityNumber of samples0100020003000Similarity−1.0−0.500.51.0influence scoreInfluence Score−4×106−2×10602×1064×1066×106Similarity−0.500.5All samplesMisclassified samplesNumber of Samples0200400airplaneautomobilebirdcatdeerdogfroghorseshiptruckMean Probabilty (%)00.20.4airplaneautomobilebirdcatdeerdogfroghorseshiptruckAll samplesMisclassified samplesNumber of Samples0200400airplaneautomobilebirdcatdeerdogfroghorseshiptruckMean Probabilty (%)00.20.4airplaneautomobilebirdcatdeerdogfroghorseshiptruckUnder review as a conference paper at ICLR 2023\n\nE ADDITIONAL EXPERIMENT\n\nE.1 CONTRASTIVE BASED ADVERSARIAL SELF-SUPERVISED LEARNING WITH TAROSS.\n\nOur TAROSS could be also applied to positive pairs in contrastive-based adversarial self-supervised learning (e.g., RoCL (Kim et al., 2020), ACL (Jiang et al., 2020)). We applied our TAROSS in instance-wise attack of the contrastive-based approaches as follow,\n\nLattack = Lnt-xent(x, {xpos}, {xneg}) + Lsimilarity(x, {xjTAROSS })\n\n(8)\n\nwhere attack loss is consists of original attack loss nt-xent loss (Chen et al., 2020) and similarity loss. The similarity loss additionally constrains the positive pairs as the TAROSS that maximize the similarity between the x with the jth index images which is searched by our TAROSS score function. Overall, we generate adversarial examples that maximizes the Lattack loss. Surprisingly, when we apply TAROSS on the contrastive learning based approach, previous work could achieve marginally better clean accuracy and robustness (Table 6). This shows that our empirical assumption also holds on contrastive-based SSL but since there is (1/batch size) effects on the total loss the gain could be marginal.\n\nTable 6: Results of contrastive learning approach with TAROSS.\n\nLinear evaluation Robust linear evaluation Clean\n\nClean\n\nPGD\n\nPGD\n\nRoCL RoCL +TAROSS\n\nACL ACL +TAROSS\n\n78.14 78.43\n\n79.96 80.02\n\n42.89 43.91\n\n39.37 40.18\n\n76.53 80.11\n\n77.17 79.84\n\n47.51 46.47\n\n40.67 42.29\n\nE.2 ABLATION EXPERIMENT ON THE SCORE FUNCTION\n\nOur score function consists of two terms: entropy term and cosine similarity term. While our score function is designed based on our observation in Section 3.2, we also empirically validate each term with the ablation experiment. We only use each term as the score function during the adversarial training as follow,\n\nTable 7: Results of ablation study on score function.\n\nClean\n\nPGD AutoAttack\n\nSentropy Ssimilarity STAROSS\n\n78.43 72.90 74.06\n\n40.35 44.59 44.71\n\n32.51 36.12 36.39\n\nSentropy(T1(xi), T2(xj)) = (pj/τ ) log (cid:0)pj/τ ), .\nSsimilarity(T1(xi), T2(xj)) =\n\n·\n\nei ||ei||2\n\nej ||ej||2\n\n(9)\n\n(10)\n\nThrough the experimental results in Table 7, the entropy term leverage to have good clean accuracy while similarity term focus on to have better robust performance. With combined score function, our model could have good robustness while having good clean accuracy.\n\nE.3 ROBUSTNESS AGAINST BLACK BOX ATTACK\n\nWe conduct black box attack to verify our model is robust to gradient free attacks. We generate black box adversaries with AT (Madry et al., 2018) model, RoCL (Kim et al., 2020) model and our models. Then, we test adversaries to each other. As show in the table, our model is able to defend the black box attack from AT model than the RoCL model. Moreover, our model generates stronger black box adversaries than RoCL since AT model shows more weak robustness.\n\nTable 8: Results of black box attack. Models on the row are the tested models. Models on the columns are the source models to generate black box adversaries.\n\nAT\n\n-\n\nAT\n\nRoCL 70.40 69.97 Ours\n\nRoCL Ours\n\n59.73 -\n54.99\n\n60.92 57.98 -\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper aims to improve the robustness of adversarial self-supervised learning (SSL) by leveraging the proposed targeted adversarial data for adversarial SSL. The targeted adversarial data is generated by updating the natural data towards a targeted sample selected by K-mean or similarity. The paper empirically validates that target attacks can improve the adversarial robustness of several previous adversarial SSL methods including BYORL and SimSiam.\n\n# Strength And Weaknesses\n\nStrength:\n[Motivation for target attacks is clear.] Figure 1 clearly illustrates that targeted adversarial data can help find effective adversarial data in the setting of adversarial SSL with only positive pairs.\n\nWeakness:\n[Writing is poor.]\nThe definitions of some terms in the abstract and introduction like “positive-only adversarial SSL” are unclear to readers. In addition, Authors cannot assume the reader fully read “BYORL”. The paper does not introduce BYROL in detail. The paper should be self-contained. Besides, please write a full name before using the abbreviations. \n\n[Motivation is unclear.]\nThis paper tries to propose a method to improve adversarial self-supervised learning. There already exists effective adversarial contrastive learning based on SimCLR. However, this paper tries to improve a suboptimal method BYORL. \n\n[Effectiveness of the proposed method is not well validated.]\n-In the introduction, it seems that Authors tried to improve non-contrastive methods since non-contrastive methods are more efficient. However, Authors do not show the efficiency of the proposed targeted SSL compared to BYORL. Observed that TAROSS spends almost twice of the training time of ACL from the results in Author’s response (https://openreview.net/forum?id=wwRjJScpsOO&noteId=BU4byVrzGsk), it seems that the proposed targeted SSL is more inefficiency.\n-The scalability and efficiency of the targeted SSL on the large-scale datasets (e.g., ImageNet) are unknown. It would be more difficult to select an appropriate target sample in a large-scale dataset, which could degrade the efficiency and scalability of the proposed method.\n-The clean accuracy of the proposed targeted SSL is significantly degraded on some datasets (such as STL10 and CIFAR100 in Table 4). The degradation of clean accuracy is even higher than the improvement in robust accuracy. Therefore, targeted SSL could be problematic.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n[Clarity is poor] As stated in Weakness, Authors do not provide the definitions of some terms and some important preliminaries.\n\n[Quality is poor] As stated in Weakness, the effectiveness of the proposed method is not well validated. In addition, the target selection method is a bit tricky and lacks theoretical justification, which makes its quality further degrade.\n\n[Novelty is marginal]: The paper replaces the label with a carefully-selected target data for generating targeted adversarial data in the setting of adversarial SSL, which is a marginal improvement in novelty.  \n\n[Reproducibility seems to be good.] The paper provides experimental details in the section of Reproducibility Statement.\n\n# Summary Of The Review\n\nThis paper proposes a target attack for improving the robustness of adversarial SSL. However, the paper’s weighting is poor, its motivation is unclear, and it lacks theoretical justification and enough empirical results to validate its effectiveness.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nn/a"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLOGICAL MESSAGE PASSING NETWORKS WITH ONE-HOP INFERENCE ON ATOMIC FORMULAS\n\nZihao Wang & Yangqiu Song CSE, HKUST Hong Kong SAR {zwanggc,yqsong}@cse.ust.hk\n\nGinny Y. Wong & Simon See NVIDIA AI Technology Center (NVATIC), NVIDIA Santa Clara, USA {gwong,ssee}@nvidia.com\n\nABSTRACT\n\nComplex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct one-hop inferences on atomic formulas, the results of which are regarded as the messages passed in LMPNN. The reasoning process over the overall logical formulas is turned into the forward pass of LMPNN that incrementally aggregates local information to finally predict the answers’ embeddings. The complex logical inference across different types of queries will then be learned from training examples based on the LMPNN architecture. Theoretically, our query-graph representation is more general than the prevailing operator-tree formulation, so our approach applies to a broader range of complex KG queries. Empirically, our approach yields a new state-of-the-art neural CQA model. Our research bridges the gap between complex KG query answering tasks and the long-standing achievements of knowledge graph representation learning. Our implementation can be found at https://github.com/HKUST-KnowComp/LMPNN.\n\n1\n\nINTRODUCTION\n\nKnowledge Graphs (KG) are essential sources of factual knowledge supporting downstream tasks such as question answering (Zhang et al., 2018; Sun et al., 2020; Ren et al., 2021). Answering logical queries is a complex but important task to utilize the given knowledge (Ren & Leskovec, 2020; Ren et al., 2021). Modern Knowledge Graphs (KG) (Bollacker et al., 2008; Suchanek et al., 2007; Carlson et al., 2010), though on a great scale, is usually considered incomplete. This issue is well known as the Open World Assumption (OWA) (Ji et al., 2021). Representation learning methods are employed to mitigate the incompleteness issue by learning representations from the observed KG triples and generalizing them to unseen triples (Bordes et al., 2013; Trouillon et al., 2016; Sun et al., 2018; Zhang et al., 2019; Chami et al., 2020). When considering logical queries over incomplete knowledge graphs, the query answering models are required to not only predict the unseen knowledge but also execute logical operators, such as conjunction, disjunction, and negation (Ren & Leskovec, 2020; Wang et al., 2021b).\n\nRecently, neural models for Complex Query Answering (CQA) have been proposed to complete the unobserved knowledge graph and answer the complex query simultaneously. These models aim to address complex queries that belong to an important subset of the first-order queries. Formally speaking, the complex queries are Existentially quantified First Order queries and has a single free\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nvariable (EFO-1) (Wang et al., 2021b) containing logical conjunction, disjunction, and negation (Ren & Leskovec, 2020). The EFO-1 queries are transformed in the forms of operator trees, e.g., relational set projection, set intersection, set union, and set complement (Wang et al., 2021b). The key idea of these approaches is to represent the entity set into specific embedding spaces (Ren & Leskovec, 2020; Zhang et al., 2021; Chen et al., 2022). Then, the set operators are parameterized by neural networks (Ren & Leskovec, 2020; Amayuelas et al., 2022; Bai et al., 2022). The strict execution of the set operations can be approximated by learning and conducting continuous mappings over the embedding spaces.\n\nIt is observed by experiments that classic KG representation (Trouillon et al., 2016) can easily outperform the neural CQA models in one-hop queries even though the neural CQA models model the one-hop projection with complex neural networks (Ren & Leskovec, 2020; Amayuelas et al., 2022; Bai et al., 2022). One possible reason is that the neural set projection is sub-optimal in modeling the inherent relational properties, such as symmetry, asymmetry, inversion, composition, etc, which are sufficiently discussed in KG completion tasks and addressed by KG representations (Trouillon et al., 2016; Sun et al., 2018). On the other hand, Continuous Query Decomposition (CQD) (Arakelyan et al., 2021) method searches for the best answers with a pretrained KG representation. The logical inference step is modeled as an optimization problem where the continuous truth value of an Existential Positive First Order (EPFO) query is maximized by altering the variable embeddings. However, the speed and the performance of inference heavily rely on the optimization algorithm. It also assumes that the embeddings of entities and relations can reflect higher-order logical relations, which is not generally assumed in existing knowledge graph representation models. Moreover, it is unclear whether CQD can achieve good performance on complex queries with negation operators 1.\n\nIn this paper, we aim to answer complex EFO-1 queries by equipping pretrained KG representations with logical inference power. First, we formulate the EFO-1 KG queries as Disjunctive Normal Form (DNF) formulas and propose to represent the conjunctive queries in the form of query graphs. In the query graph, each edge is an atomic formula that contains a predicate with a (possible) negation operator. For each one-hop atomic formula, we use the pretrained KG representation to infer the intermediate embeddings given the neighboring entity embedding, relation embedding, direction information, and negation information. We show that the inference can be analytically derived for the KG representation formulation. The results of one-hop atomic formula inference are interpreted as the logical messages passed from one node to another. Based on this mechanism, we propose a Logical Message Passing Neural Network (LMPNN), where node embeddings are updated by one Multi-Layer Perceptron (MLP) based on aggregated logical messages. LMPNN coordinates the local logical message by pretrained knowledge graph representations and predicts the answer embedding for a complex EFO-1 query. Instead of performing on-the-fly optimization over the query graph as CQD (Arakelyan et al., 2021), we parameterize the query answering process as the forward pass of LMPNN which is trained from the observed KG query samples.\n\nExtensive experiments show that our approach is a new state-of-the-art neural CQA model, in which only one MLP network and two embedding vectors are trained. Interestingly, we show that the optimal number of layers of the LMPNN is the largest distance between the free variable node and the constant entity nodes. This makes it easy to generalize our approach to complex queries of arbitrary complexity. Hence, our approach bridges the gap between complex KG query answering tasks and the long-standing achievements of knowledge graph representation learning.\n\n2 RELATED WORKS\n\nKnowledge graph representation. Representing relational knowledge is one of the long-standing topics in representation learning. Knowledge graph representations aim to predict unseen relational triples by representing the discrete symbols in continuous spaces. Various algebraic structures (Bordes et al., 2013; Trouillon et al., 2016; Sun et al., 2018; Ebisu & Ichise, 2018; Zhang et al., 2019) are applied to represent the relational patterns (Sun et al., 2018) and different geometric spaces (Chami et al., 2020; Cao et al., 2022) are explored to capture the hierarchical structures in knowledge graphs. Therefore, entities and relations in large knowledge graphs can be efficiently represented in a continuous space.\n\n1Existing empirical evaluations are all conducted on queries without negation (Arakelyan et al., 2021)\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nNeural complex query answering. Most existing works treat the complex queries as operator trees (Ren et al., 2020; Ren & Leskovec, 2020; Wang et al., 2021b). The query types that can be answered are extended from existential positive first-order (EPFO) queries (Ren et al., 2020; Choudhary et al., 2021; Arakelyan et al., 2021) to the existential first-order (Ren & Leskovec, 2020; Zhang et al., 2021; Bai et al., 2022), or more specifically EFO-1 queries (Wang et al., 2021b). In a neural CQA model, the entity sets are represented by various forms, including probabilistic distributions (Ren & Leskovec, 2020; Choudhary et al., 2021; Bai et al., 2022), geometric shapes (Ren et al., 2020; Zhang et al., 2021), and fuzzy-logic-inspired representations (Chen et al., 2022). In contrast to knowledge graph representations, the relation projections between sets are usually modeled by complex neural networks, including multi-layer perceptron (Ren & Leskovec, 2020), MLPmixer (Amayuelas et al., 2022), or even transformers (Bai et al., 2022). However, their performances on one-hop queries are shown to be worse than the state-of-the-art but simple knowledge graph representation (Trouillon et al., 2016). Other works compiled the queries into the graphs and then solve queries with graph neural networks (Daza & Cochez, 2020; Liu et al., 2022). In contrast to this work, existing investigations only focused on EPFO queries and require to train the entire GNN from zero. Notably, knowledge graph representations also provide effective signals when answering complex queries. Specifically, CQD (Arakelyan et al., 2021) uses the KG representation to calculate the continuous truth value of an EPFO logical formula with the logical t-norms. Then, the embeddings are optimized to maximize the continuous truth value. The optimization can be applied in the embedding space as well as the symbolic space. Our experiments show that this method performs badly on complex queries with logical negation, see Section 4.3.\n\n3 PRELIMINARIES\n\nIn this section, we formally introduce the knowledge graph and related model-theoretic concepts. These concepts are helpful when we define the DNF formulation of EFO-1 queries in Section 4. Then, we introduce the abstract interface of knowledge graph representation, which is useful in defining one-hop inference in Section 5.\n\nModel-theoretic concepts for knowledge graphs. A first-order language L is specified by a triple (F, R, C) where F, R, and C are sets of symbols for functions, relations, and constants, respectively. A knowledge graph is specified under the language LKG, where function symbol set F = ∅ and relation symbols in R denote binary relations. A knowledge graph KG is an LKG-structure given the entity set V, where each constant c ∈ C = V is also an entity and each relation r ∈ R is a set r ⊆ V × V. We say r(t1, t2) = True when (t1, t2) ∈ r. A knowledge graph is usually defined by the relation triple set E = {(h, r, t)}, where h and t are entities such that (h, t) ∈ r. The Open World Assumption (OWA) means only a subset of E can be observed. The observed knowledge graph is denoted by KGobs. A term is either a constant or a variable. And an atomic formula is either r(t1, t2) or ¬r(t1, t2) where t1 and t2 are terms and r is a relation. In the following parts, we use a· to denote an atomic formula. Then the first order formula can be inductively defined by adding connectives (conjunction ∧, disjunction ∨, and negation ¬) to atomic formulas and quantifiers (existential ∃ and universal ∀) to variables. The formal definition of first-order formulas can be found in Marker (2006). A variable is bounded when associated with a quantifier, otherwise, it is free.\n\n3.1 KNOWLEDGE GRAPH REPRESENTATIONS\n\nOur approach relies upon the following abstract interface of knowledge graphs. Given the head entity embedding h, relation embedding r, and tail entity embedding t, a knowledge graph representation is able to produce a continuous truth value ψ(h, r, t) in [0, 1] of the embedding triple (h, r, t). In the symbolic space, whether (h, t) ∈ r is indicated by the {0, 1} truth value of r(h, t). In the embedding space, ψ(h, r, t) indicates the “probability” that (h, t) ∈ r. Hence, this definition is a continuous relaxation of the {0, 1} truth value.\n\nEach knowledge graph representation has a scoring function φ(h, r, t), which can be based on a similarity function or a distance function. It is easy to convert such functions into ψ by applying the Sigmoid function with necessary scaling and shift. For example, the scoring function of ComplEx (Trouillon et al., 2016) embedding is.\n\nφ(h, r, t) = Re(⟨h ⊗ r, ̄t⟩),\n\n(1)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: The operator tree representation (a) and query graph representation (b) of an examplar complex query in Ren & Leskovec (2020). The logical formula of this query is given by r1(x, c1) ∧ ¬r2(c2, x) ∧ r3(y, x). For shorthand, this query is denoted as INP. The symbols about relations and terms are consistent in the query graph representation. In the operator tree representation, c1 and c2 are represented by anchor node operators e1 and e2. Relation r1 and r3 are represented by projection node p1 and p3. Relation r2 is jointly represented by projection node p2 and negation node pn. The fact that x is connected to all other nodes is represented by intersection operator i.\n\nwhere ⊗ denotes the element-wise complex number multiplication and ⟨x, y⟩ is the complex inner product. Re extracts the real part of a complex number. Then, the truth value of which can be computed by\n\nψ(h, r, t) = σ(φ(h, r, t)),\n\n(2)\n\nwhere σ is the sigmoid function. This truth value function is used in Arakelyan et al. (2021) with logic t-norms. In the context of knowledge graph representation learning, the entity embeddings h, t are usually related to specific entity symbols in a look-up table. In this work, we assume the embedding vector is related to not only specific entities but also variables.\n\n4 EFO-1 QUERIES AND QUERY GRAPHS\n\nWithout loss of generality, we consider the logical formulas under the disjunctive normal form. Then, we define the Existential First Order queries with a single free variable (EFO-1).\n\nDefinition 1. Given a knowledge graph KG, an EFO-1 query Q is formulated as the first-order formula in the following disjunctive normal form,\n\nQ(y, x1, ..., xm) = ∃x1, ..., ∃xm [a11 ∧ a12 ∧ · · · ∧ a1n1 ] ∨ · · · ∨ (cid:2)ap1 ∧ ap2 ∧ · · · ∧ apnp\n\n(cid:3) ,\n\n(3)\n\nwhere y is the only free variable and xi, 1 ≤ i ≤ m are m existential variables. aij, 1 ≤ i ≤ p, 1 ≤ j ≤ np are atomic formulas with constants and variables y, x1, . . . , xm. aij can be either negated or not.\n\nTo answer the EFO-1 queries, one is expected to identify the answer set A[Q, KG]. A[Q, KG] is the set of entities such that a ∈ A[q, KG] if and only if Q(y = a, x1, ..., xm) = True.\n\nMoreover, since Q is given in the disjunctive normal form, let us consider\n\nQ(y, x1, ..., xm) = CQ1(y, x1, ..., xm) ∨ · · · ∨ CQp(y, x1, ..., xm),\n\n(4)\n\nwhere CQi = ∃x1, ...∃xmai1 ∧ ai2 ∧ · · · ∧ aini is a conjunctive query. A[Q, KG] = ∪p sets for all conjunctive queries.\n\nIt is easy to see that i=1A[CQi, KG]. Therefore, solving A[Q, KG] is equivalent to solving the answer\n\n4.1 QUERY GRAPH FOR CONJUNCTIVE QUERIES\n\nFor each conjunctive query, the constant entities and variables are closely related by the atomic formulas. To emphasize the dependencies between entities and variables, we propose to use the query graph where the terms are nodes connected by the atomic formulas. Each node in the query graph is either a constant symbol or a free or existential variable. Each edge in the query graph represents an atomic formula containing both relation and negation information.\n\nFigure 1 shows our query graph representation and the operator tree representation (Wang et al., 2021b) for a typical query type defined in Ren & Leskovec (2020). We see that the query graph is more concise than the operator tree. First, we can see that the nodes and edges have different meanings in operator trees and query graphs. In the operator trees representation, each node is an operator denoting a set operation, whose output can be fed into other set operators. When using the complex query answering models with the operator tree, the information flows from leaf to root,\n\n4\n\n e! e\" n i p# c! c\" x y(a)OperatorTree(b) QueryGraph p\" p!r!¬r\"r#Published as a conference paper at ICLR 2023\n\nwhich is unidirectional. However, for the query graph, the messages are passed bi-directionally through each edge as we will show in Section 6. In Figure 1, the central node x receives messages from all neighbor nodes.\n\n4.2 EXPRESSIVENESS OF DEFINITION 1\n\nOur definition is theoretically broader than all existing discussions. The definition in (Wang et al., 2021b), though widely adapted and discussed in the existing literature, has implicit assumptions because they are proposed to predict the answers by neural operators. It is assumed that (1) the Skolemization process can always convert the query into a tree of set operators, and (2) all leaves of the operator tree are entities rather than variables. A counterexample that can be expressed by Definition 1 but cannot be represented by operator trees is shown in Appendix A.\n\n4.3 LIMITATION OF OPTIMIZATION-BASED METHODS FOR NEGATED QUERIES\n\nOur definition accepts the atomic formulas with negation operation. Therefore, It can be seen as a natural extension of the definitions in CQD (Arakelyan et al., 2021). Moreover, we extended CQD to negation queries with the continuous truth value with fuzzy logical negator (see Appendix B). The extended method is named CQD(E), and the results are compared in Table 1. We could see that the performance of CQD(E) is much less effective on negation queries. We conjecture that the landscape of the objective function, i.e., the continuous truth values of the complex formula with negation, can be non-convex. So the optimization problem is inherently harder. The non-convexity objective function is discussed in Appendix B.1.\n\n5 ONE-HOP INFERENCE ON ATOMIC FORMULAS\n\nAs shown in Figure 1, each edge in a query graph is an atomic formula containing the information of neighboring entities, relation, and logical negation, which are all crucial for predicting the answers. We propose to encode such entity, relation, and logical negation information by one-hop inference that maximizes the continuous truth value of the (negated) atomic formula. Let ρ be the logical message encoding function of four input parameters, including neighboring entity embedding, relation embedding, direction information (h2t or t2h), and logical negation information (0 for no negation and 1 for with negation). The goal of this section is to properly define ρ.\n\nMoreover, inference on one-hop atomic formula is much easier compared to that on the entire complex EFO-1 query graph, as discussed in Section 4.3. We also provide the closed-form expression of ρ for the knowledge graph embedding we used in this paper.\n\n5.1 ONE-HOP INFERENCE IN NON-NEGATED ATOMIC FORMULAS\n\nThe first situation is to infer the head embedding ˆh given the tail embedding t and relation embedding r on a non-negated atomic formula. We formulate the inference task in the form of continuous truth value maximization:\n\nˆh = ρ(t, r, t2h, 0) := arg max\n\nψ(x, r, t),\n\nx∈D\n\n(5)\n\nwhere D is the search domain for the embedding. Similarly, the tail embedding ˆt can be infered given head embedding h and relation embedding r, that is,\n\nˆt = ρ(h, r, h2t, 0) := arg max\n\nψ(h, r, x).\n\nx∈D\n\n(6)\n\n5.2 ONE-HOP INFERENCE IN NEGATED ATOMIC FORMULAS\n\nTo extend the definition for non-negated atomic formulas to negated formulas, one only need to compute the continuous truth value of a negated atomic formula by the fuzzy logic negator (H ́ajek,\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: An illustration of the two-stage procedures of logical message passing neural networks: (a) passing the logical messages across the graph; (b) updating the node embedding with the aggregated information with an MLP network.\n\n2013), that is, ψ(h, ¬r, t) = 1 − ψ(h, r, t). Then the estimation of head and tail embeddings is related to the following inference problems\n\nˆh = ρ(t, r, t2h, 1) := arg max\n\nψ(x, ¬r, t) = arg max\n\n[1 − ψ(x, r, t)] ,\n\nx∈D\n\nx∈D\n\nˆt = ρ(h, r, h2t, 1) := arg max\n\nψ(h, ¬r, x) = arg max\n\n[1 − ψ(h, r, x)] .\n\nx∈D\n\nx∈D\n\n(7)\n\n(8)\n\nThis optimization-based approach is similar to CQD discussed in Section 4.3, but it is more reliable since atomic formulas are what we used to train the knowledge graph representation. Specifically, the objectives in Eq. (5) and Eq. (6) are eventually the likelihood of positive samples and those in Eq. (7) and Eq. (8) are the likelihood of negative samples. These objectives are widely used to learn the representations with negative sampling.\n\nClosed-form message encoding function ρ with pretrained KG representation. We have already defined ρ with optimization problems. Moreover, the closed-form expression of ρ can be (approximately) derived in many cases given two facts about the knowledge graph representations: (1) the scoring function of knowledge graph representation is usually as simple as the inner product or distance. More details about constructing closed-form ρ for these two types of scoring functions are discussed in Appendix D.1; (2) the sigmoid function outside the scoring function φ makes the final truth value zero or one only if the output of the scoring function is sufficiently small or large. We identify the closed-form approximation of ρ for ComplEx (Trouillon et al., 2016) and other five different KG representations in Appendix C and D, which allows fast computation logical messages used in Section 6.\n\n6 LOGICAL MESSAGE PASSING NEURAL NETWORKS\n\nIn this section, we propose a Logical Message Passing Neural Network (LMPNN) to bridge the one-hop inference proposed in Section 5 and complex query answering defined in Section 4. As a variation of the message-passing neural network (Gilmer et al., 2017; Xu et al., 2018), LMPNN has two stages: (1) each node passes a message to all its neighbors; (2) each node aggregates the received messages and updates its latent embedding. Figure 2 illustrates how those two stages work. Then the final layer embedding for the free variable node can be used to predict the answer entities.\n\n6.1 LOGICAL MESSAGE PASSING OVER THE QUERY GRAPH\n\nWe use the message encoding function ρ to compute the messages passed from node to node. Figure 2 (a) demonstrates the logical message passing with blue arrows. Each node receives the message from all its neighboring nodes.\n\n6.2 NODE EMBEDDINGS IN QUERY GRAPH AND UPDATING SCHEME\n\nLet n be a node and z(l) from the input layer l = 0 to latent layers l > 0. When l = 0, z(0)\n\nn be the embedding of n at the l-th layer. We discuss how to compute the z(l) n falls into one of three situations.\n\nn\n\n6\n\n c! c\" x yr!¬r\"r#ρy,r#,h2t,0ρc\",r\",h2t,1ρc!,r!,t2h,0(a)LogicalMessagePassingwithOneHopInference(QueryGraphView)ρx,r!,h2t,0ρx,r\",t2h,1ρx,r#,t2h,0 z(%) z(%’!)(b) LogicalMessagePassingNetworks(NodeEmbeddingView)AggregateLogicalMessagesMLPUpdatewithEquation(9)Published as a conference paper at ICLR 2023\n\n(1) For an entity node e, z(0) (2) For an existential variable node xi, we assign an learnable embedding z(0) free variable node y, we assign another learnable embedding z(0) variables xi share one vx, for simplicity.\n\nis looked up from the pretrained knowledge graph representation. xi = vx. (3) For a y = vy. We set that all existential\n\ne\n\nAt the l-th layer, z(l) n can be computed by updating the aggregated information from the (l − 1)-th layer. Specifically, let N (n) be the neighbor set of node n in the query graph. For each neighbor node v ∈ N (n), one can obtain its embedding z(l−1) ∈ D, the relation rv→n ∈ R, the direction Dv→n ∈ {h2t, t2h}, and the negation indicator Negv→n ∈ {0, 1}. Then, the embedding z(l) n , l ≥ 1 is computed by an MLP network after the summation of the aggregated information, that is,\n\nv\n\nz(l)\n\nn = MLP(l)\n\n εz(l−1)\n\nn\n\n+\n\n(cid:88)\n\n(cid:16)\n\nρ\n\nz(l−1)\n\nv\n\nv∈N (n)\n\n, rv→n, Dv→n, Negv→n\n\n(cid:17)\n\n\n\n ,\n\n(9)\n\nwhere ε is a hyperparameter. To feed the complex vector of ComplEx (Trouillon et al., 2016) into the MLP network, the real and imaginary vectors of one complex embedding are concatenated and regarded as one feature vector. The formulation in Eq. (9) is similar to the Graph Isomorphic Networks (Xu et al., 2018) except that the logical messages passed are encoded by ρ from the pretrained KG representation. Trainable vx and vy are unrelated to any specific entity.\n\n6.3 LEARNING LMPNN FOR COMPLEX QUERY ANSWERING\n\nTo train the neural network, we apply the Noisy Contrastive Estimation (NCE) loss for ranking tasks proposed in (Ma & Collins, 2018). Let {(ai, qi)}n i=1 be the positive data samples, where ai ∈ A[qi, KG]. Our optimization involves K uniformly sampled noisy answers from the entity set. The NCE objective is:\n\nLN CE =\n\n(cid:34)\n\nlog\n\n1 n\n\nn (cid:88)\n\ni=1\n\nexp [cos(ai, z(qi))/T ]\n\nexp [cos(ai, z(qi))/T ] + (cid:80)K\n\nk=1 exp [cos(zk, z(qi))/T ]\n\n(cid:35)\n\n,\n\n(10)\n\nwhere ai is the embedding of positive answer ai and zk is the embedding of the noisy entity samples. z(qi) indicates the embedding of the free variable in qi at the final layer of LMPNN. T is a hyperparameter. This objective is optimized by stochastic gradient descent.\n\n6.4 ANSWERING COMPLEX QUERIES WITH LMPNN\n\nWe discuss two ways to retrieve answers for general DNF queries in Definition 1: (a) A two-step approach as the previous works (Ren et al., 2020; Ren & Leskovec, 2020), where the free variable embedding for each sub conjunctive query are estimated, the answer entities are then ranked by the minimal distance (or maximal similarity) against free variable embeddings from multiple sub (2) We transform all disjunctions in the formula to conjunctions, then one conjunctive queries. query graph is sufficient for solving the transformed query. The answer set of a transformed query is a strict subset of the originanl answer. For simplicity, we use the second way to solve disjunctive queries in this paper, though it may lead to sub-optimal performance. Then, we discuss how to answer conjunctive queries with LMPNN.\n\nConjunctive query graph of arbitrary size. We apply LMPNN to the query graph of a given conjunctive query Q. A sufficient condition to produce a correct answer is that the free variable node has received messages from all the entity nodes after the forward passing through LMPNN layers. Let the largest distance between entity nodes and the free variable node be L. Then, we apply the LMPNN layers L times to ensure all messages from entity nodes are successfully received by the free variable node. The prediction of answer embedding z(Q) is given by the free variable embedding at the final layer, i.e., z(Q) = z(L) . We propose to use the cosine similarity between z(Q) and the pretrained entity embeddings to rank the entities and then retrieve answers.\n\ny\n\nSince L is not determined, we assume all L layers share the same MLP layer. Hence, the only trainable parameter in LMPNN is one MLP network and two embeddings for existential and free variables. Our experiments on different query types show that the single MLP network has strong generalizability to LMPNN of different depths.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: MRR results of different CQA models on three KGs. AP represents the average score of EPFO queries and AN represents the average score of queries with negation. The boldface indicates the best results for each KG.\n\nKG\n\nModel\n\n1P\n\n2P\n\n3P\n\n2I\n\n3I\n\nPI\n\nIP\n\n2U\n\nUP\n\n2IN\n\n3IN\n\nINP\n\nPIN\n\nPNI AP AN\n\nBetaE ConE Q2P\n\n65.1 73.3 82.6\n\n25.7 33.8 30.8\n\n24.7 29.2 25.5\n\n(Using pretrained KG representation) CQD(E) LMPNN\n\n89.4 85.0\n\n15.1 28.6\n\n27.6 39.3\n\nBetaE ConE Q2P\n\n39.0 41.8 39.1\n\n10.9 12.8 11.4\n\n10.0 11.0 10.1\n\n(Using pretrained KG representation) CQD(E) LMPNN\n\n46.7 45.9\n\n6.5 10.3\n\n10.3 13.1\n\nBetaE ConE Q2P\n\n53.0 53.1 56.5\n\n13.0 16.1 15.2\n\n11.4 13.9 12.5\n\nFB15K\n\nFB15K -237\n\nNELL\n\n(Using pretrained KG representation) CQD(E) LMPNN\n\n60.8 60.6\n\n13.2 17.5\n\n18.3 22.1\n\n55.8 64.4 65.1\n\n63.0 68.2\n\n28.8 32.6 32.3\n\n23.1 34.8\n\n37.6 40.0 35.8\n\n66.5 73.7 74.7\n\n65.5 76.5\n\n42.5 47.3 47.7\n\n29.8 48.9\n\n47.5 50.8 48.7\n\n43.9 50.9 49.5\n\n46.0 46.7\n\n22.4 25.5 24.0\n\n22.1 22.7\n\n24.1 26.3 22.6\n\n28.1 35.7 34.9\n\n35.2 43.0\n\n12.6 14.0 14.3\n\n16.3 17.6\n\n14.3 17.5 16.1\n\n40.1 55.7 32.1\n\n42.9 36.7\n\n12.4 14.5 8.7\n\n14.2 13.5\n\n12.2 15.3 11.1\n\n25.2 31.4 26.2\n\n23.2 31.4\n\n9.7 10.8 9.1\n\n8.9 10.3\n\n8.5 11.3 10.4\n\n36.5 40.1\n\n43.0 50.3\n\n30.0 28.4\n\n22.5 24.9\n\n17.6 17.2\n\n13.7 15.7\n\n14.3 17.9 21.9\n\n0.2 29.1\n\n5.1 5.4 4.4\n\n0.2 8.7\n\n5.1 5.7 5.1\n\n0.1 8.5\n\n14.7 18.7 20.8\n\n0.2 29.4\n\n7.9 8.6 9.7\n\n0.2 12.9\n\n7.8 8.1 7.4\n\n11.5 12.5 12.5\n\n6.5 9.8 8.9\n\n12.4 15.1 17.1\n\n4.0 14.9\n\n0.1 10.2\n\n18.4 16.4\n\n7.4 7.8 7.5\n\n2.1 7.7\n\n10.0 10.8 10.2\n\n3.6 4.0 4.6\n\n0.1 4.6\n\n3.1 3.5 3.3\n\n0.0 3.9\n\n3.4 3.6 3.8\n\n6.1 5.2\n\n3.5 3.9 3.4\n\n5.2 4.8\n\n41.6 49.8 46.8\n\n45.3 50.6\n\n20.9 23.4 21.9\n\n19.8 24.1\n\n24.6 27.2 25.5\n\n28.4 30.7\n\n11.8 14.8 16.4\n\n4.6 20.0\n\n5.4 5.9 6.0\n\n1.7 7.8\n\n5.9 6.4 6.0\n\n1.9 8.0\n\n0.1 10.8\n\n4.0 12.2\n\n7 EXPERIMENTS\n\nIn this section, we compare LMPNN with existing neural CQA methods and justify the important features of LMPNN with ablation studies. Our results show that LMPNN is a very strong method for answering complex queries.\n\n7.1 EXPERIMENTAL SETTINGS\n\nBaselines. We consider the neural complex query answering models for EFO-1 queries in recent three years, including BetaE (Ren & Leskovec, 2020), ConE (Zhang et al., 2021), and Q2P (Bai et al., 2022). The baseline results are obtained by training models with the code released by the authors under the suggested hyperparameters. Neural-symbolic ensemble models are implemented with the negation. Moreover, we also implement and report CQD (Arakelyan et al., 2021) with the same pretrained knowledge graph representation. We also compare more neural CQA models in Appendix E.\n\nDatasets. We consider the widely used training and evaluation dataset in (Ren & Leskovec, 2020). It allows us to compare our results with existing methods directly. We compare the results on FB15k (Bordes et al., 2013), FB15k-237 (Toutanova et al., 2015), and NELL (Carlson et al., 2010).\n\nEvaluations. The evaluation metric follows the previous works (Ren & Leskovec, 2020). For each query instance, we first rank all entities except those observed as easy answers based on their cosine similarity with the free variable embedding estimated by LMPNN. The rankings of hard answers are used to compute MRR for the given query instance. Then, we average the metrics from all query instances. In this paper, MRR is reported and compared.\n\nLMPNN Setting. We use the ComplEx (Trouillon et al., 2016) checkpoints released by Arakelyan et al. (2021) in LMPNN to make a fair comparison to CQD (Arakelyan et al., 2021). More results about LMPNN with other six kinds of pretrained KG representations are also presented in the Appendix D. The rank of ComplEx is 1,000, and the epoch for the checkpoint is 100. For LMPNN, we use AdamW to train the MLP network. The learning rate is 1e-4, and the weight decay is 1e-4. The batch size is 1,024, and the negative sample size is 128, selected from {32, 128, 512}. The MLP network has one hidden layer whose dimension is 8,192 for NELL and FB15k, and 4,096 for FB15k-237. T in the training objective is chosen as 0.05 for FB15k-237 and FB15k and 0.1 for NELL. ε in Eq (9) is chosen to be 0.1. Reported results are averaged from 3 random experimental trials. All experiments of LMPNN are conducted on a single V100 GPU (16GB).\n\n7.2 MAJOR RESULTS\n\nTable 1 presents the MRR results of LMPNN and neural CQA baselines on answering EFO-1 queries over three KGs. It is found that LMPNN reaches the best performance on average for both EPFO and\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: MRR results of different hyperparameter settings compared to the best combination.\n\nModel\n\nKGE CAT\n\nε = 0 ε = 0.5\n\nL − 1 L + 1 L + 2 L + 3\n\nT = 0.01 T = 0.1\n\nBEST CHOICE\n\n1P\n\n30.5\n\n45.5 43.9\n\n45.5 45.3 45.6 45.4\n\n43.9 45.6\n\n45.9\n\n2P\n\n6.8\n\n12.6 11.9\n\n6.8 12.6 13.0 13.0\n\n12.1 12.7\n\n13.1\n\n3P\n\n6.9\n\n9.7 9.7\n\n7.8 10.1 10.1 10.1\n\n9.7 9.9\n\n10.3\n\n2I\n\n7.8\n\n33.6 30.5\n\n34.1 33.1 32.9 32.4\n\n31.9 33.7\n\n34.8\n\n3I\n\n7.8\n\n47.1 42.3\n\n47.5 46.4 45.8 45.2\n\n46.5 47.1\n\n48.9\n\nPI\n\n6.2\n\n11.1 18.2\n\n11.6 20.4 21.3 22.1\n\n18.3 22.0\n\n22.7\n\nIP\n\n6.4\n\n17.1 14.8\n\n6.3 16.3 17.7 17.4\n\n15.2 17.1\n\n17.6\n\n2U\n\n8.4\n\n14.0 13.8\n\n13.9 13.7 13.4 13.3\n\n13.5 14.0\n\n13.5\n\nUP\n\n6.3\n\n10.0 9.6\n\n6.1 10.0 10.0 10.0\n\n10.2 10.0\n\n10.3\n\n2IN\n\n3.0\n\n8.8 7.3\n\n8.6 8.0 7.9 7.7\n\n6.9 8.8\n\n8.7\n\n3IN\n\n2.8\n\n11.6 10.3\n\n11.7 10.9 10.9 10.9\n\n11.2 11.5\n\n12.9\n\nINP\n\nPIN\n\nPNI\n\n5.6\n\n7.3 7.0\n\n5.6 7.5 7.7 7.7\n\n7.2 7.3\n\n7.7\n\n2.3\n\n3.5 3.8\n\n2.9 4.3 4.2 4.2\n\n4.6 4.4\n\n4.6\n\n1.7\n\n2.7 5.4\n\n3.9 5.2 5.2 5.2\n\n4.3 5.4\n\n5.2\n\nAP\n\n9.7\n\n22.3 21.6\n\n20.0 23.1 23.3 23.2\n\n22.4 23.6\n\n24.1\n\nAN\n\n3.1\n\n6.8 6.7\n\n6.5 7.2 7.2 7.1\n\n6.8 7.5\n\n7.8\n\nnegation queries. Our results on negation queries indicate that the embedding estimation formulation with negated atomic formula proposed in Section 6.1 produces meaningful features.\n\nInterestingly, LMPNN performs much better than the CQD on both EPFO and negation queries with the same pretrained knowledge graph representation. Our results show that the LMPNN is stronger than CQD in more complex queries, especially those with logical negation. Notably, our approach does not require any optimization in the inference time as in Arakelyan et al. (2021). It confirms again that LMPNN successfully leverages the representation power of knowledge graph representation simply by training an MLP.\n\n7.3 ABLATION STUDY\n\nIn the ablation study, we conduct extensive experiments to justify the effects of four key factors of LMPNN, including (1) the logical message passing by one-hop inference; (2) the hyperparameter ε at each LMPNN layer; (3) the depth of LMPNN; (4) the hyperparameter T at the noisy contrastive learning loss. All experiments of the ablation study are conducted on queries at FB15K-237.\n\nTo justify the effect of one-hop inference, we compare a baseline with logical messages computed by a linear transformation of the concatenation of the entity embedding, the relation embedding, a binary indicator for h2t and t2h, and a binary indicator for negation. For example, for ComplEx embedding in 1,000-dimensional complex vector space, there are 2,000 parameters for entity embedding and 2,000 for relation embedding. The concatenation produces a feature of 4,002 dimensions. Then we use a linear transformation to transform this feature to 2000 dimensions so that the logical message can be used in Eq. (9). This baseline is denoted as KGE CAT. To justify the effect of the depth of LMPNN, we alter the depth of LMPNN based on its original depth L into L − 1, L + 1, L + 2, and L + 3. The value L is computed from the maximal distances between the free variable node and constant entity nodes in the query graph. Even in the L − 1 case, we keep the depth of LMPNN at least 1 to ensure the logical message is passed between nodes.\n\nTable 2 shows the results of the ablation study, where the setting reported in Table 1 is indicated by BEST CHOICE. We note that BEST CHOICE uses one-hop inference on atomic formulas, L LMPNN layers, ε = 0.1, and T = 0.05 for FB15k-237. We find that KGE CAT performs poorly even though it contains the pretrained KG information, which indicates that one-hop inference is essential to answer complex queries. Meanwhile, L − 1 performs worse than BEST CHOICE since the information is not fully passed to the free variable node. And the worse performances of L + 1, L + 2, and L + 3 cases indicate that our definition for L is reasonable. Moreover, ε and T are also important to the best performance. Overall, one-hop inference on atomic formula is the most critical factor in the learning and inference process of LMPNN.\n\n8 CONCLUSION\n\nIn this paper, we present LMPNN to answer complex queries, especially EFO-1 queries, over knowledge graphs. LMPNN achieves a strong performance by training one MLP network to aggregate the logical messages passed over the query graph. In the ablation study, we identify that the one-hop inference on atomic formulas based on a pretrained knowledge graph is critical to answering complex queries. Our research effectively bridges the gap between EFO-1 query answering tasks and the long-standing achievements of knowledge graph representation. In future work, our method can be combined with stronger knowledge graph representation techniques, as well as with neural-symbolic ensembles.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n9 ACKONWLEDGEMENT\n\nThe authors of this paper were supported by the NSFC Fund (U20B2053) from the NSFC of China, the RIF (R6020-19 and R6021-20) and the GRF (16211520 and 16205322) from RGC of Hong Kong, the MHKJFS (MHP/001/19) from ITC of Hong Kong and the National Key R&D Program of China (2019YFE0198200) with special thanks to HKMAAC and CUSBLT, and the Jiangsu Province Science and Technology Collaboration Fund (BZ2021065). We also thank the support from NVIDIA AI Technology Center (NVAITC) and the UGC Research Matching Grants (RMGS20EG01-D, RMGS20CR11, RMGS20CR12, RMGS20EG19, RMGS20EG21, RMGS23CR05, RMGS23EG08).\n\nREFERENCES\n\nAlfonso Amayuelas, Shuai Zhang, Xi Susie Rao, and Ce Zhang. Neural methods for logical reasoning over knowledge graphs. In International Conference on Learning Representations, 2022.\n\nErik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. Complex query answering\n\nwith neural link predictors. International Conference on Learning Representations, 2021.\n\nJiaxin Bai, Zihao Wang, Hongming Zhang, and Yangqiu Song. Query2particles: Knowledge graph reasoning with particle embeddings. Findings of the Association for Computational Linguistics: NAACL 2022, 2022.\n\nKurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pp. 1247–1250, 2008.\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26, 2013.\n\nZongsheng Cao, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. Geometry interaction knowledge graph embeddings. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.\n\nAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. Toward an architecture for never-ending language learning. In Proceedings of the AAAI conference on artificial intelligence. AAAI Press, 2010.\n\nInes Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher R ́e. Lowdimensional hyperbolic knowledge graph embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6901–6914, 2020.\n\nXuelu Chen, Ziniu Hu, and Yizhou Sun. Fuzzy logic based logical query answering on knowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 3939– 3948, 2022.\n\nNurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chandan Reddy. Probabilistic entity representation model for reasoning over knowledge graphs. Advances in Neural Information Processing Systems, 34:23440–23451, 2021.\n\nWilliam Cohen, Fan Yang, and Kathryn Rivard Mazaitis. Tensorlog: A probabilistic database implemented using deep-learning infrastructure. Journal of Artificial Intelligence Research, 67: 285–325, 2020.\n\nDaniel Daza and Michael Cochez. Message passing query embedding.\n\narXiv preprint\n\narXiv:2002.02406, 2020.\n\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTakuma Ebisu and Ryutaro Ichise. Toruse: Knowledge graph embedding on a lie group. In Pro-\n\nceedings of the AAAI conference on artificial intelligence, 2018.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\nPetr H ́ajek. Metamathematics of fuzzy logic, volume 4. Springer Science & Business Media, 2013.\n\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems, 33(2):494–514, 2021.\n\nTimoth ́ee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for knowledge base completion. In International Conference on Machine Learning, pp. 2863–2872. PMLR, 2018.\n\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-bert: Enabling language representation with knowledge graph. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 2901–2908, 2020.\n\nXiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, and Jie Tang. Mask and reason: Pre-training knowledge graph transformers for complex logical In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and queries. Data Mining, pp. 1120–1130, 2022.\n\nZhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3698–3707, 2018.\n\nDavid Marker. Model theory: an introduction, volume 217. Springer Science & Business Media,\n\n2006.\n\nMaximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning\n\non multi-relational data. In International Conference on Machine Learning, 2011.\n\nFabio Petroni, Tim Rockt ̈aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, In Proceedings of the 2019 and Alexander Miller. Language models as knowledge bases? Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, 2019.\n\nH Ren, W Hu, and J Leskovec. Query2box: Reasoning over knowledge graphs in vector space using\n\nbox embeddings. In International Conference on Learning Representations, 2020.\n\nHongyu Ren and Jure Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge\n\ngraphs. Advances in Neural Information Processing Systems, 33:19716–19726, 2020.\n\nHongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure Leskovec, and Denny Zhou. Lego: Latent execution-guided reasoning for multi-hop question answering on knowledge graphs. In International Conference on Machine Learning, pp. 8959–8970. PMLR, 2021.\n\nHongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Denny Zhou, Jure Leskovec, and Dale Schuurmans. Smore: Knowledge graph completion and multi-hop reasoning in massive knowledge In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and graphs. Data Mining, pp. 1472–1482, 2022.\n\nDaniel Ruffinelli, Samuel Broscheit, and Rainer Gemulla. You can teach an old dog new tricks! on training knowledge graph embeddings. In International Conference on Learning Representations, 2020.\n\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European semantic web conference, pp. 593–607. Springer, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nFabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pp. 697–706, 2007.\n\nHaitian Sun, Andrew Arnold, Tania Bedrax Weiss, Fernando Pereira, and William W Cohen. Faith-\n\nful embeddings for knowledge base queries. volume 33, pp. 22505–22516, 2020.\n\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In International Conference on Learning Representations, 2018.\n\nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. In Proceedings of the 2015 conference on empirical methods in natural language processing, pp. 1499–1509, 2015.\n\nTh ́eo Trouillon, Johannes Welbl, Sebastian Riedel, ́Eric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International conference on machine learning, pp. 2071–2080. PMLR, 2016.\n\nHongwei Wang, Hongyu Ren, and Jure Leskovec. Relational message passing for knowledge graph completion. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 1697–1707, 2021a.\n\nXiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In Proceedings of the 16th international conference on World Wide Web, pp. 2022–2032, 2019.\n\nZihao Wang, Hang Yin, and Yangqiu Song. Benchmarking the combinatorial generalizability of complex query answering on knowledge graphs. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b.\n\nRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. Representation learning of knowledge graphs with entity descriptions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016a.\n\nRuobing Xie, Zhiyuan Liu, Maosong Sun, et al. Representation learning of knowledge graphs with hierarchical types. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, volume 2016, pp. 2965–2971, 2016b.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? International Conference on Learning Representations, 2018.\n\nZezhong Xu, Wen Zhang, Peng Ye, Hui Chen, and Hua zeng Chen. Neural-symbolic entangled framework for complex query answering. In Advances in Neural Information Processing Systems, 2022.\n\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. International Conference on Learning Representations, 2014.\n\nShuai Zhang, Yi Tay, Lina Yao, and Qi Liu. Quaternion knowledge graph embeddings. Advances in\n\nneural information processing systems, 32, 2019.\n\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. Variational reasoning for question answering with knowledge graph. In Proceedings of the AAAI conference on artificial intelligence, 2018.\n\nZhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. Cone: Cone embeddings for multi-hop reasoning over knowledge graphs. Advances in Neural Information Processing Systems, 34:19172–19183, 2021.\n\nZhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. Advances in Neural Information Processing Systems, 34:29476–29490, 2021.\n\nZhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. Neural-symbolic models for logical\n\nqueries on knowledge graphs. In International Conference on Machine Learning, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA A COUNTEREXAMPLE FOR THE EXPRESIVENESS OF OPERATOR TREE\n\nREPRESENTATION\n\nExample 1. Given a citation network with authors, papers, and conferences, one query wants to find ICLR authors with at least one collaborator. It can be expressed in the format in Definition 1 as\n\nq(a1, a2, p1, p2) = ∃a2∃p1∃p2IsAuthor(a1, p1) ∧ InConf(p1, ICLR)\n\n∧ IsAuthor(a1, p2) ∧ IsAuthor(a2, p2) ∧ ¬(a1 = a2).\n\nWe see that if we take a1 as the answer node. a2 and p2 are leaves but not anchor entities. In this way, this query cannot be represented by the operator tree anchor nodes. However, this query can be represented in the query graph, see Figure 3.\n\nFigure 3: The query graph for the query in Example 1.\n\nThen we discuss how to answer this query with LMPNN. It is easy to see that LMPNN can be applied to the query graph in Figure 3 once the ̸= is considered as the combination of predicate eq (equality) and negation ¬.\n\nTo include eq, we only need to define the logical messages ρ(a1, eq, 1) and ρ(a2, eq, 1). According to the Proposition 2 in Appendix D.1, these two problems boil down to defining ρ(a, eq, 0) = f (a, eq). By the semantics of “equality”, equal terms shares the equal embedding. Therefore, the entity embedding which is equal to a given embedding a is just f (a, eq) = a. Then, ρ(a, eq, 0) = f (a, eq) = a and ρ(a1, eq, 1) = −a1, ρ(a2, eq, 1) = −a2. In this way, LMPNN is the first actionable approach to address the queries in Example 1.\n\nB A NATURAL EXTENSION OF COMPLEX QUERY DECOMPOSITION (CQD)\n\nTO ANSWER NEGATION QUERIES\n\nIn this paper, we compare the optimization-based approach CQD (Arakelyan et al., 2021) by extending existing CQD with fuzzy logic negator. The extended version is denoted as CQD(E). For example, consider the logical formula INP query in the Figure 1, we could estimate the continuous truth value of the given logical formula r1(x, c1) ∧ ¬r2(c2, x) ∧ r3(y, x) as follows\n\nT VCQD(E)(x, y|INP) = ψr1(x, c1)⊤ [1 − ψr2(c2, x)] ⊤ψr3(y, x),\n\n(11)\n\nwhere φ· are the continuous value of relations r1, r2, and r3 and ⊤ is a t-norm. Then CQD(E) maximizes the continuous truth value T VCQD(E)(x, y|INP) to obtain the “best” variable embeddings x and y as Arakelyan et al. (2021).\n\nB.1 NON-CONVEX LANDSCAPE OF NEGATED COMPLEX QUERIES\n\nIn this part, we show that the negator in fuzzy logic introduces non-convexity. Let x be an optimizable variable in the 1D interval I and φ1(x) and φ2(x) be two continuous truth value of two atomic\n\n13\n\nICLRp!a!InConfIsAuthorp\"a\"IsAuthorIsAuthor≠Published as a conference paper at ICLR 2023\n\nformula a1 and a2, respectively. They are convex functions over I. Consider the conjunctive query a1 ∧ ¬a2. The continuous truth value is\n\nJ(x) = φ1(x)⊤[1 − φ2(x)]. (12) Consider an example with convex φ1(x) and φ2(x). Let ⊤ is the product t-norm, and φ1(x) = 1−x2 and φ2(x) = 1 − (x − 0.3)2 for x ∈ [0, 1]. Then J(x) turns to be non-convex as shown in Figure 4.\n\nC CLOSED-FORM LOGICAL MESSAGE BY COMPLEX\n\nIn this section, we derive the closed-form logical message encoding function for ComplEx embedding (Trouillon et al., 2016). The scoring function of ComplEx is φ(h, r, t) = Re(⟨h ⊗ r, ̄t⟩).\n\n(13)\n\nWe expand the complex embeddings to real vectors h = hr + ihi, r = rr + iri, t = tr + iti. Then the scoring function is\n\nφ(h, r, t) = Re(⟨h ⊗ r, ̄t⟩)\n\n= ⟨rr ⊗ hr − ri ⊗ hi, tr⟩ + ⟨rr ⊗ hi + ri ⊗ hr, ti⟩ = ⟨rr ⊗ tr + ri ⊗ ti, hr⟩ + ⟨rr ⊗ ti − ri ⊗ tr, hi⟩.\n\nSince −ri = ̄ri under the complex conjugate, then,\n\nφ(h, r, t) = ⟨rr ⊗ tr − ̄ri ⊗ ti, hr⟩ + ⟨rr ⊗ ti + ̄ri ⊗ tr, hi⟩\n\n= Re(⟨t ⊗ ̄r, ̄h⟩).\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\nThen, we optimize the continuous truth value of ComplEx given in Eq. (2) to derive the closed-form estimation of Eq. (5). We note that the embedding used in ComplEx is not strictly restricted in a domain set D. Instead, the N3 regularization (Lacroix et al., 2018) is applied to the embedding as a soft constraint. Therefore, in our derivation of the close form solution, we also employ N3 regularization rather than hard constraint. Our first result is the following proposition. Proposition 1. For ComplEx embedding, the logical message encoding function has the following closed form with respect to the complex embedding r and t, ̄r ⊗ t (cid:112)3λ∥r ⊗ ̄t∥\n\nρ(t, r, t2h, 0) =\n\n(19)\n\n.\n\nProof. We expand the optimization problem as follows, (cid:8)Re(⟨ ̄r ⊗ t, ̄x⟩) − λ∥x∥3(cid:9)\n\nρ(t, r, t2h, 0) = arg max\n\nx∈Cd\n\n(cid:26)\n\n= arg max\n\nx∈Cd\n\n⟨rr ⊗ tr − ̄ri ⊗ ti, xr⟩ + ⟨rr ⊗ ti + ̄ri ⊗ tr, xi⟩ − λ\n\n(20)\n\n(cid:16)(cid:112)⟨xr, xr⟩ + ⟨xi, xi⟩\n\n(cid:17)3(cid:27)\n\n.\n\n(21)\n\nFigure 4: The landscape of continuous truth value becomes non-convex after negation.\n\n14\n\n0.00.20.40.60.81.0x0.000.020.040.060.08J(x)Published as a conference paper at ICLR 2023\n\nNotice that rr ⊗ tr − ̄ri ⊗ ti and rr ⊗ ti + ̄ri ⊗ tr are the real and imaginary part of ̄r ⊗ t. Let s = [rr ⊗ tr − ̄ri ⊗ ti, rr ⊗ ti + ̄ri ⊗ tr] be the real vector concatenated by the real and imageinary part of ̄r ⊗ t. Also, let ̃x = [xr, xi] be the real vector concatenated by the real and imageinary part of x. Then the Eq. (21) is equivalent to the following optimization problem in the real space:\n\nmax ̃x∈R2d\n\n⟨s, ̃x⟩ − λ∥ ̃x∥3 2\n(cid:124) (cid:125) (cid:123)(cid:122) :=J\n\n.\n\n(22)\n\nWe note that J is convex function over ̃x. To optimize ̃x, we optimize the unit direction v and length η of ̃x, with rewriting ̃x = ηv. When η is fixed the second term is also fixed, it is easy to see that the v∗ = s/∥s∥2 maximizes the first term. Then we find the optimal η by minizing the following objective for η > 0:\n\nBy letting dJ\n\ndη = ∥s∥2 − 3λη2 = 0, we derive the optimal η∗ =\n\n(cid:113) ∥s∥2\n\n3λ . Then, we have\n\nJ = ∥s∥2η − λη3.\n\n ̃x∗ = η∗v∗ =\n\ns (cid:112)3λ∥s∥2\n\n.\n\n(23)\n\n(24)\n\nThen, we identify the optimal real and imaginary part of x∗ from ̃x∗, and thus recover the optimal x∗.\n\nSimilarly, we derive the optimal closed-form expression of ρ in all other cases:\n\nρ(h, r, h2t, 0) =\n\nρ(t, r, t2h, 1) =\n\nρ(h, r, h2t, 1) =\n\nr ⊗ h (cid:112)3λ∥r ⊗ h∥ − ̄r ⊗ t (cid:112)3λ∥ ̄r ⊗ t∥ −r ⊗ h (cid:112)3λ∥r ⊗ h∥\n\n,\n\n,\n\n.\n\n(25)\n\n(26)\n\n(27)\n\nWe note that the value of λ is not determined. On the one hand, it can be of course a hyperparameter to discuss. In LMPNN application, we just let 3λ∥ · ∥ = 1 and then all denominators in the closedform expression are 1.\n\nD CLOSED-FORM LOGICAL MESSAGES FOR KG REPRESENTATIONS\n\nWe demonstrate two general ways to construct closed-form logical messages function ρ for LMPNN in the Appendix D.1. Then, we show six examples to illustrate how our approach constructs the closed form ρ for various KG representations in the Appendix D.2.\n\nSpecifically, our constructions apply to two types of KG representations characterized by their scoring functions. The first type of KG representations uses inner-product-based scoring functions while the second type of KG representations uses distance-based scoring functions. Moreover, we provide six examples of KG representations, including RESCAL (Nickel et al., 2011), TransE (Bordes et al., 2013), DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), and RotatE (Sun et al., 2018).\n\nD.1 TWO CONSTRUCTIONS\n\nAs discussed in Section 5, the closed-form logical message encoding function ρ is the result of the closed-form solution of four one-hop inference problems (estimating the head or tail entity embedding with or without logical negation, see Eq. (5-8). This leads to four construction tasks. The major result of Appendix D.1 is Proposition 2. It shows that, with our constructions of two types of scoring\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nfunctions, closed-form solutions for four one-hop inference problems are actually dependent. Once one of four one-hop inference problems are approximately solved in the closed form, the other three one-hop inference problems are also solved approximately in the closed form.\n\nSimplification with reciprocal relations. We simplify the four construction tasks into two tasks by introducing reciprocal relations. For each relation r ∈ R, the reciprocal relation is r−1 ∈ R−1 but in the reversed direction. By introducing reciprocal relations r−1 and training their embeddings r−1, the one-hop inference in the tail-to-head direction can be rewritten in the head-to-tail direction. Specifically, we have\n\nρ(t, r, t2h, 0) = ρ(t, r−1, h2t, 0), ρ(t, r, t2h, 1) = ρ(t, r−1, h2t, 1).\n\n(28)\n\n(29)\n\nIntroducing reciprocal relations is shown to improve the performances of the link prediction tasks (Ruffinelli et al., 2020). We assume that the reciprocal relation embedding can be obtained, irrespective of being separately trained or analytically derived from the original relation embedding, such as ComplEx discussed in Appendix C. Then, it suffices to construct the closed-form solution for ρ(h, r, h2t, 0) and ρ(h, r, h2t, 1), and the rest two types of logical messages are naturally defined with reciprocal relation embeddings.\n\nThen we construct the closed-form ρ(h, r, h2t, 0) and ρ(h, r, h2t, 1) for two types of KG embeddings, characterized by their scoring functions. We emphasize that the derivations below are only approximate estimations to keep the closed-form expression as simple as possible. However, empirical results show that these simple and approximate closed-form solutions can already be used in LMPNN.\n\nType 1: inner-product-based scoring function. The inner-product-based scoring function for a triple of embeddings (h, r, t) is ⟨f (h, r), t⟩, where f is a binary function of the entity and relation embeddings and ⟨·, ·⟩ is the inner product. The inner-product ⟨·, ·⟩ can be defined in real or complex vector spaces. This scoring function is used in RESCAL (Nickel et al., 2011) DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), etc. When optimizing the embeddings, lq 2020). Then we consider the following optimization problem:\n\n2 regularizations (q = 2, 3) are usually applied (Ruffinelli et al.,\n\nρ(h, r, h2t, 0) = arg max\n\nx\n\nσ (⟨f (h, r), x⟩) − λ∥x∥p 2\n(cid:124) (cid:125) (cid:123)(cid:122) :=J1\n\n,\n\n(30)\n\nwhere hyperparameter λ > 0 is a regularization coefficient, σ is the sigmoid function.\n\nWe note that J1 is just the Lagrangian of the following maximization problem, and the λ is the Langrangian multiplier\n\nmax\n\n∥x∥q\n\n2<δ\n\nσ (⟨f (h, r), x⟩) ,\n\nwhere x is restricted inside a δ\n\n1\n\nq -ball. Then we could conclude that\n\narg max ∥x∥q 2<δ\n\nσ (⟨f (h, r), x⟩) = arg max 2<δ\n\n∥x∥q\n\n⟨f (h, r), x⟩ = δ\n\n1 q\n\nf (h, r) ∥f (h, r)∥2\n\n.\n\nBy altering the hyperparameter δ = ∥f (h, r)∥p\n\n2, we could derive a simple result\n\narg max x\n\nσ (⟨f (h, r), x⟩) − λ∥x∥p\n\n2 ≈ f (h, r).\n\nTherefore, we define\n\nρ(h, r, h2t, 1) := f (h, r).\n\nSimilarly, for the ρ(h, r, h2t, 1), we have\n\nρ(h, r, h2t, 1) = arg max\n\nx\n\n= arg max\n\nx\n\n[1 − σ (⟨f (h, r), x⟩)] − λ∥x∥p\n\n2\n\nσ (⟨−f (h, r), x⟩) − λ∥x∥p\n\n2\n\n≈ arg max ∥x∥p 2 <δ\n\n⟨−f (h, r), x⟩.\n\n16\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\nPublished as a conference paper at ICLR 2023\n\nWe conclude the closed-form solution as\n\nρ(h, r, h2t, 0) := −f (h, r).\n\n(38)\n\nType 2: distance-based scoring function. Another type of scoring functions for a triple of embeddings (h, r, t) is γ − ∥f (h, r) − t∥, where f follows the definition above and γ is a margin. This scoring function is used in TransE (Bordes et al., 2013), RotatE (Sun et al., 2018), etc. Similarly, the ∥x∥q\n\n2 regularizations can also be considered (Ruffinelli et al., 2020).\n\nρ(h, r, h2t, 0) can be computed by\n\nρ(h, r, h2t, 0) = arg max\n\nσ (γ − ∥f (h, r) − x∥) − λ∥x∥p 2.\n\n(39)\n\nx With similar tricks, we transform the “soft” regularization into the “hard” constraint.\n\narg max x\n\nσ (γ − ∥f (h, r) − x∥) − λ∥x∥q\n\n2 ≈ arg max ∥x∥q 2<δ where δ is another hyperparameter. We set δ > ∥f (h, r)∥q which summarizes\n\n[γ − ∥f (h, r) − x∥] ,\n\n(40)\n\n2, then the optimal solution is f (h, r),\n\nFor the negated head-to-tail direction, the one-hop inference problem is\n\nρ(h, r, h2t, 0) := f (h, r).\n\nFor simplicity, we choose\n\narg max ∥x∥q 2<δ\n\n= arg max ∥x∥q 2<δ\n\n[1 − σ (γ − ∥f (h, r) − x∥)]\n\n∥f (h, r) − x∥ = −δ\n\n1\n\nq f (h, r).\n\nρ(h, r, h2t, 1) := −f (h, r).\n\n(41)\n\n(42)\n\n(43)\n\n(44)\n\nOur constructions for two types of KG representations share a unified closed-form logical message once the function f (h, r) is given. In the following part f is named as “forward” estimation function since it estimate the tail embeddings based on head and relation embedding in a forward direction. Therefore, we summarize the four types of logical messages used in LMPNN in the following proposition: Proposition 2. For a KG representation of either Type 1 and Type 2, we could define four closedform logical encoding functions with (1) relation embedding r and the corresponding reciprocal relation embedding r−1 and (2) the forward estimation function f as follows:\n\nρ(h, r, h2t, 0) = f (h, r),\n\nρ(h, r, h2t, 1) = −f (h, r), ρ(t, r, t2h, 0) = f (t, r−1), ρ(t, r, t2h, 1) = −f (t, r−1).\n\n(45)\n\n(46)\n\n(47)\n\n(48)\n\nD.2 SIX KG REPRESENTATION EXAMPLES\n\nNow it is ready to apply the Proposition 2 to six KG representations. For each KG representation, it is important to state its scoring function for triple (h, r, t) and the relation parameterization. We assume the reciprocal relation embeddings are already trained.\n\nTable 3 summarizes the information for RESCAL (Nickel et al., 2011) TransE (Bordes et al., 2013), DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), and RotatE (Sun et al., 2018). We list the relation parameter r, the essential one-hop inference function ρ(h, r, h2t, 0) = f (h, r), and the scoring function for each KG representation.\n\nThe scoring function of ComplEx (Trouillon et al., 2016) is not the exact inner-product in the complex vector space, but it can be reduced to the inner product in the real vector space and has already been discussed in Appendix C. We see that the Proposition 2 and Table 3 covers the results in Appendix C by letting the reciprocal embedding of r−1 be the complex conjugate ̄r of the original embedding r.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Closed-form forward estimation function f for six KG representations. Closed-form logical message encoding function ρ can be easily constructed with the closed-form f .\n\nKG Embedding\n\nr parameters\n\nf (h, r)\n\nRESCAL (Nickel et al., 2011) TransE (Bordes et al., 2013) DistMult (Yang et al., 2014) ComplEx (Trouillon et al., 2016) ConvE (Dettmers et al., 2018) RotatE (Sun et al., 2018)\n\nWr r\nr r\nω, W cos θ + i sin θ\n\nWrh r + h r ⊗ h r ⊗ h ReLU(vec(ReLU([eh; er] ∗ ω))W ) (cos θ + i sin θ) ⊗ h\n\nScoring function\n\n⟨f (h, r), t⟩ γ − ∥f (h, r) − t∥ ⟨f (h, r), t⟩ Re⟨f (h, r), ̄t⟩ ⟨f (h, r), t⟩ γ − ∥f (h, r) − t∥\n\nTable 4: Properties of six backbone KG representations on FB15-237. The data is released by Ruffinelli et al. (2020) on https://github.com/uma-pi1/kge. The dimension of each KG representation is listed in the bracket. We note that the dimensions for the complex vector embeddings indicates the trainable parameters. For example, each ComplEx embedding of 256D is a complex vector in C128 with 256 trainable parameters. KG Repr.\n\nMRR Hits@1 Hits@3 Hits@10 Config file\n\nCheckpoint\n\nRESCAL (128D) TransE (128D) DistMult (256D) ComplEx (256D) ConvE (256D) RotatE (256D)\n\n0.356 0.313 0.343 0.348 0.339 0.333\n\n0.263 0.221 0.250 0.253 0.248 0.240\n\n0.393 0.347 0.378 0.384 0.369 0.368\n\n0.541 0.497 0.531 0.536 0.521 0.522\n\n[download link] [download link] [download link] [download link] [download link] [download link]\n\n[download link] [download link] [download link] [download link] [download link] [download link]\n\nD.3 PERFORMANCES OF LMPNN WITH DIFFERENT BACKBONE KG REPRESENTATIONS\n\nThe performances of LMPNN with six backbone KG representations are presented in Table 5. The LMPNN is trained in the suggested setting in the Section 7.3. The pretrain checkpoints of six backbone KG representations are obtained from Ruffinelli et al. (2020). The information of the performances of each KG representation is listed in Table 4.\n\nIt could be found that, LMPNN achieves descent performances with simple KG backbones of relatively low dimensions (128D and 256D). ConvE (256D) (Dettmers et al., 2018), DistMult (256D) (Yang et al., 2014), and ComplEx (256D) Trouillon et al. (2016) outperform BetaE (800D) on both EPFO and negation queries. All KG representation except TransE (128D) (Bordes et al., 2013) could outperform BetaE (800D) (Ren & Leskovec, 2020) on negation queries. We note that adjust the hyperparameters, i.e., embedding dimensions, to obtain more powerful KG representations could improve the results. However, this is beyond the scope of this paper.\n\nE NEURAL CQA BENCHMARK\n\nIn this section, we show that LMPNN (with ComplEx 2000D pretrained by (Arakelyan et al., 2021)) is the new state-of-the-art method among all neural CQA models. We include the following neural CQA baselines that can address the EFO-1 queries. Other models that cannot answer EFO-1 queries\n\nTable 5: Comparison of LMPNN with different pretrained backbone KG representations on FB15k237 queries.\n\nModel\n\nTransE (128D) RESCAL (128D) ConvE (256D) RotatE (256D) DistMult (256D) ComplEx (256D)\n\n1P\n\n39.9 43.6 42.5 43.8 43.6 44.4\n\n2P\n\n9.2 11.9 12.3 11.2 11.2 11.7\n\n3P\n\n8.4 9.9 10.5 8.9 9.5 9.3\n\n2I\n\n23.5 33.7 30.6 30.4 32.2 32.4\n\n3I\n\n36.2 48.0 43.8 44.5 46.3 46.4\n\nPI\n\n8.2 9.8 9.6 11.0 18.1 18.1\n\nIP\n\n10.0 16.2 13.0 15.0 15.1 15.7\n\n18\n\n2U\n\n10.1 12.1 11.8 13.0 13.0 13.0\n\nUP\n\n5.4 9.9 7.6 8.6 9.3 9.4\n\n2IN\n\n3.4 4.2 5.2 7.0 6.1 6.0\n\n3IN\n\n6.9 10.3 9.8 10.5 10.5 10.7\n\nINP\n\nPIN\n\nPNI\n\n5.8 7.0 7.1 6.3 6.6 6.8\n\n3.0 3.5 3.8 3.7 4.1 4.1\n\n2.2 2.5 3.5 3.2 4.2 4.0\n\nAP\n\n16.8 21.7 20.2 20.7 22.0 22.3\n\nAN\n\n4.2 5.5 5.9 6.2 6.3 6.4\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Benchmark comparison with neural CQA models on FB15k-237 queries.\n\nModel\n\nBETAE ConE MLP-Mix Q2P CQD LMPNN\n\n1P\n\n39.0 41.8 43.4 39.1 46.7 45.9\n\n2P\n\n10.9 12.8 12.6 11.4 10.3 13.1\n\n3P\n\n10.0 11.0 10.4 10.1 6.5 10.3\n\n2I\n\n28.8 32.6 33.6 32.3 23.1 34.8\n\n3I\n\n42.5 47.3 47.0 47.7 29.8 48.9\n\nPI\n\n22.4 25.5 14.9 24.0 22.1 22.7\n\nIP\n\n12.6 14.0 25.7 14.3 16.3 17.6\n\n2U\n\n12.4 14.5 14.2 8.7 14.2 13.5\n\nUP\n\n2IN\n\n9.7 10.8 10.2 9.1 8.9 10.3\n\n5.1 5.4 6.6 4.4 0.2 8.7\n\n3IN\n\n7.9 8.6 10.7 9.7 0.2 12.9\n\nINP\n\nPIN\n\nPNI\n\n7.4 7.8 8.1 7.5 2.1 7.7\n\n3.6 4.0 4.7 4.6 0.1 4.6\n\n3.4 3.6 4.4 3.8 6.1 5.2\n\nAP\n\n20.9 23.4 23.6 21.9 19.8 24.1\n\nAN\n\n5.5 5.9 6.9 6.0 1.7 7.8\n\nTable 7: Benchmark comparison with neural CQA models on FB15k queries.\n\nModel\n\nBETAE ConE MLP-Mix Q2P CQD LMPNN\n\n1P\n\n65.1 73.3 71.9 82.6 89.4 85.0\n\n2P\n\n25.7 33.8 32.1 30.8 27.6 39.3\n\n3P\n\n24.7 29.2 27.1 25.5 15.1 28.6\n\n2I\n\n55.8 64.4 59.9 65.1 63.0 68.2\n\n3I\n\n66.5 73.7 70.5 74.7 65.5 76.5\n\nPI\n\n43.9 50.9 33.7 49.5 46.0 46.7\n\nIP\n\n28.1 35.7 48.4 34.9 35.2 43.0\n\n2U\n\n40.1 55.7 40.4 32.1 42.9 36.7\n\nUP\n\n25.2 31.4 28.4 26.2 23.2 31.4\n\n2IN\n\n14.3 17.9 17.2 21.9 0.2 29.1\n\n3IN\n\n14.7 18.7 17.8 20.8 0.2 29.4\n\nINP\n\n11.5 12.5 13.5 12.5 4.0 14.9\n\nPIN\n\n6.5 9.8 9.1 8.9 0.1 10.2\n\nPNI\n\n12.4 15.1 15.2 17.1 18.4 16.4\n\nAP\n\n41.7 49.8 45.8 46.8 45.3 50.6\n\nAN\n\n11.9 14.8 14.6 16.2 4.6 20.0\n\nare not compared (Ren et al., 2020; Choudhary et al., 2021; Liu et al., 2022). We tried to reproduce the results reported, and we note that different model applies to different knowledge graphs.\n\nBetaE (Ren & Leskovec, 2020): Results are reproduced for FB15k-237, FB15k, and NELL. ConE (Zhang et al., 2021): Results are reproduced for FB15k-237, FB15k, and NELL. MLP-Mix (Amayuelas et al., 2022): Results are reproduced for FB15k-237, FB15k, and NELL. Q2P (Bai et al., 2022): Results are reproduced for FB15k-237, FB15k, and NELL. FuzzQE (Chen et al., 2022): Results on FB15k are missing. Results on FB15k-237 are not reproducible with the given code and suggested hyperparameters. Results on NELL are partially reproduced, so we report the results in the paper and reproduced by us.\n\nCQD (Arakelyan et al., 2021): Results are reproduced on FB15k-237, FB15k, and NELL.\n\nThe results of FB15k-237, FB15k, and NELL are shown in Table 6, Table 7, and Table 8, respectively. We can see that LMPNN achieves the best performance among all neural complex query answering models.\n\nF COMPARE TO SYMBOLIC INTEGRATION METHODS\n\nContextualized and symbolic information are shown to be effective to improve the neural models for both knowledge graph representation and complex query answering. For knowledge graph representation, neighboring information (Schlichtkrull et al., 2018; Wang et al., 2019; 2021a; Zhu et al., 2021) aggregated by graph neural networks of KG, external information (Xie et al., 2016a;b) by annotations, or even information from language models (Petroni et al., 2019; Liu et al., 2020) are also leveraged to make the knowledge graph representation more informative and effective. For complex query answering, neural models are enhanced with symbolic resoning (Zhu et al., 2022; Xu et al., 2022) that heavily search over the original symbolic space (Zhu et al., 2022) or its approximations (Cohen et al., 2020; Xu et al., 2022). Unlike neural CQA models whose operations are always in the embedding space of fixed size, the size of the intermediate states for symbolic reasoning grows with the number of the entity sets, such as the fuzzy sets used in (Zhu et al., 2022; Xu et al., 2022), and the beam-search variation of CQD (Arakelyan et al., 2021).\n\nWe refer to two methods with symbolic integration. We cannot reproduce the results since the codes for those two methods have not been released. However, since symbolic integration can also be applied to improve the LMPNN, we also list their results to show the potential.\n\nGNN-QE Zhu et al. (2022): This requires 4 V100 GPU (32G), which is 8 times larger than the resources required by LMPNN. The official implementation has not been released.\n\nENeSy (Xu et al., 2022): The official implementation has not been released.\n\nTable 9 shows that LMPNN is also compatible even with the symbolic integrated models at EPFO queries with only 1% trainable parameters at NELL and 10% trainable parameters at FB15k-237. For FB15k-237, there are still gaps between the neural CQA models and the models with symbolic\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Benchmark comparison with neural CQA models on NELL queries.\n\nModel\n\nBETAE ConE MLP-Mix Q2P FuzzQE (ours) FuzzQE (reported) CQD LMPNN\n\n1P\n\n53.0 53.1 55.6 56.5 55.5 58.1 60.8 60.6\n\n2P\n\n13.0 16.1 16.3 15.2 16.8 19.3 18.3 22.1\n\n3P\n\n11.4 13.9 14.9 12.5 14.4 15.7 13.2 17.5\n\n2I\n\n37.6 40.0 38.5 35.8 37.3 39.8 36.5 40.1\n\n3I\n\n47.5 50.8 49.5 48.7 46.9 50.3 43.0 50.3\n\nPI\n\n24.1 26.3 17.1 22.6 24.0 28.1 30.0 28.4\n\nIP\n\n14.3 17.5 23.7 16.1 19.1 21.8 22.5 24.9\n\n2U\n\n12.2 15.3 14.6 11.1 15.0 17.3 17.6 17.2\n\nUP\n\n2IN\n\n8.5 11.3 11.0 10.4 11.7 13.7 13.7 15.7\n\n5.1 5.7 5.1 5.1 7.3 8.3 0.1 8.5\n\n3IN\n\n7.8 8.1 8.0 7.4 9.1 10.2 0.1 10.8\n\nINP\n\n10.0 10.8 10.0 10.2 11.1 11.5 4.0 12.2\n\nPIN\n\nPNI\n\n3.1 3.5 3.6 3.3 4.1 4.6 0.0 3.9\n\n3.5 3.9 3.6 3.4 4.9 5.4 5.2 4.8\n\nAP\n\n24.6 27.1 26.8 25.4 26.7 29.3 28.4 30.7\n\nAN\n\n5.9 6.4 6.1 5.9 7.3 8.0 1.9 8.0\n\nTable 9: Comparison between LMPNN and symbolic integration methods. The number in brackets indicate the order of trainable parameters.\n\nModel\n\n1P\n\n2P\n\n3P\n\n2I\n\n3I\n\nPI\n\nIP\n\n2U\n\nUP\n\n2IN\n\n3IN\n\nINP\n\nPIN\n\nPNI\n\nAP\n\nAN\n\n(FB15k-237) LMPNN (107) GNN-QE (108) ENeSy (108)\n\n(NELL) LMPNN (107) GNN-QE (109) ENeSy (109)\n\n45.9 42.8 44.7\n\n60.6 53.3 59.0\n\n13.1 14.7 11.7\n\n22.1 18.9 18.0\n\n10.3 11.8 8.6\n\n17.5 14.9 14.0\n\n34.8 38.3 34.8\n\n40.1 42.4 39.6\n\n48.9 54.1 50.4\n\n50.3 52.5 49.8\n\n22.7 31.1 27.6\n\n28.4 30.8 29.8\n\n17.6 18.9 19.7\n\n24.9 18.9 24.8\n\n13.5 16.2 14.2\n\n17.2 15.9 16.4\n\n10.3 13.4 8.4\n\n15.7 12.6 13.1\n\n8.7 10.0 10.1\n\n8.5 9.9 11.3\n\n12.9 16.8 10.4\n\n10.8 14.6 8.5\n\n7.7 9.3 7.6\n\n12.2 11.4 11.6\n\n4.6 7.2 6.1\n\n3.9 6.3 8.6\n\n5.2 7.8 8.1\n\n4.8 6.3 8.8\n\n24.1 26.8 24.5\n\n30.7 28.9 29.4\n\n7.8 10.2 8.5\n\n8.0 9.7 9.8\n\nintegrations. These results suggest that neural models can be potentially improved with symbolic integration. The additional cost is the larger computational cost.\n\nWe noticed that the task of answering logical queries are investigated over larger knowledge graphs (Ren et al., 2022). When considering larger knowledge graphs, neural CQA methods (discussed in the Appendix E) and symbolic integrated methods (discussed in this part) have different scalabilities. For neural CQA models, the intermediate embeddings are of fixed dimensions, while the sizes of intermediate fuzzy sets used in the symbolic integration methods grow linearly with the size of the knowledge graph. Such difference makes neural-symbolic methods more resource demanding and they may suffer from the scalabilities issues.\n\nThe differences between NELL and FB15k-237 can be explained by the quality of the ground knowledge graphs. However, integrating the symbolic method into neural CQA models and investigating the fundamental impact of ground KGs are beyond the scope of this paper. Our work connects the KG representation and neural CQA, which could also be combined with context and symbolic information. These extensions are left for future work and are expected to bring additional improvements.\n\n20",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a Logical Message Passing Neural Network (LMPNN), which relies on pre-trained knowledge graph embeddings and MLP-based local one-hop inference to perform the Complex Query Answering (i.e., EFO-1) task. Compared to the prior work CQD [1], which formalizes the KG reasoning as an optimization problem, this paper proposes to use the closed-form solution of ComplEX to approximate the single-hop inference and use an MLP-based multi-layer GNN to perform the multi-hop reasoning.\n\n[1]: Arakelyan, Erik, Daniel Daza, Pasquale Minervini, and Michael Cochez. \"Complex Query Answering with Neural Link Predictors.\" In International Conference on Learning Representations. 2020.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. LMPNN is quite parameter-efficient, where only MLP and the embeddings of existential variable and free variable are learned.\n\n2. Due to the novel design of closed-form approximation of single-hop ComplEx, LMPNN is inference-efficient (in a GNN manner) compared to optimization-based methods like CQD [1].\n\n3. LMPNN achieves competitive or better performance on FB15k, FB15k-237, and NELL datasets.\n\n4. Compared to previous work CQD, LMPNN can additionally model the negation operation.\n\nWeaknesses:\n\n1. My main concern is on the comparison with the optimization-based method (i.e., CQD). The author claims that \"Moreover, it is unclear whether CQD can be applied to complex queries with negation operators\". Could I interpret it as the CQD is evaluated without the negation operations for INP, PIN, PNI, etc settings? Would it be possible to just use Eq. 7&8 in the paper to get an optimization-based baseline? Since one of the contributions in the paper is a GNN-like inference, I believe a fair comparison with optimization-based methods is necessary.\n\n2. The method is motivated by how to better perform one-hop queries, however, the experimental results show that LMPNN is usually outperformed by CQD in the 1P setting. Since in 1P, the one-hop ComplEx closed-form approximation should lead to the same solution as CQD, could the author explain the reasons?\n\n3. I feel the claim of bridging the gap between EFO-1 query answering and \"the long-standing achievements of KGR\" is a little over-claimed, given that previous work like CQD already uses the exactly same pre-trained KGE model (ComplEx).\n\nTypos:\n\nPage 5, \" In Figure 1, the central node x receives messages from all neighbor nodes, suo.\" what does \"suo\" mean?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is written clearly and technically sound. The proposed method (w.r.t one-hot inference-based GNN) is novel.\n\n# Summary Of The Review\n\nThe proposed LMPNN is 1) parameter-efficient, 2) inference-efficient (compared to optimization-based methods like CQD), and 3) achieves competitive or better performance on three EFO-1 datasets. My main concern is about the fair comparison with optimization-based methods with negations. Therefore, I can only recommend a weak accept for the paper.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nAGNOSTIC LEARNING OF GENERAL RELU ACTIVATION USING GRADIENT DESCENT\n\nPranjal Awasthi Google Research pranjalawasthi@google.com\n\nAlex Tang⇤ Northwestern University alextang@u.northwestern.edu\n\nAravindan Vijayaraghavan⇤ Northwestern University aravindv@northwestern.edu\n\nABSTRACT\n\nWe provide a convergence analysis of gradient descent for the problem of agnostically learning a single ReLU function under Gaussian distributions. Unlike prior work that studies the setting of zero bias, we consider the more challenging scenario when the bias of the ReLU function is non-zero. Our main result establishes that starting from random initialization, in a polynomial number of iterations gradient descent outputs, with high probability, a ReLU function that achieves an error that is within a constant factor of the optimal i.e., it is guaranteed to achieve an error of O(OP T ), where OP T is the error of the best ReLU function. This is a significant improvement over existing guarantees for gradient descent, which only guarantee error of O(pd OP T ) even in the zero-bias case (Frei et al., 2020). We also provide finite sample guarantees, and obtain similar guarantees for a broader class of marginal distributions beyond Gaussians.\n\n·\n\n1\n\nINTRODUCTION\n\nGradient descent forms the bedrock of modern optimization algorithms for machine learning. Despite a long line of work in understanding and analyzing the gradient descent iterates, there remain several In this outstanding questions on whether they can provably learn important classes of problems. work we study one of the simplest learning problems where the properties of gradient descent are not well understood, namely agnostic learning of a single ReLU function. More formally, let ̃D be a distribution over Rd where ̃w x with a fixed coordinate 1 as x = ( by appending induced by ̃D. We define the loss incurred at w = ( ̃w, bw) to be\n\nR. A ReLU function is parameterized by w = ( ̃w, bw) R. For notational convenience, we will consider the points to be in Rd+1 R\n\nx, 1). Let D be the distribution over Rd+1\n\nRd and bw 2\n\n⇥\n\n⇥\n\n2\n\ne L(w) =\n\n1 2\n\n(\n\nE x,y)\n\n⇠\n\n ̃D\n\nh\n\n(( ̃w>\n\nx + bw)\n\ne y)2\n\n=\n\n1 2\n\nE (x,y)\n\n⇠\n\nD\n\nh\n\ni\n\n((w>x)\n\ny)2\n\n.\n\ni\n\nHere (x) = max(x, 0) is the standard rectified linear unit popularly used in deep learning. The goal in agnostic learning of a ReLU function (or agnostic ReLU regression) is to design a polynomial time learning algorithm that takes as input i.i.d. samples from D and outputs w = ( ̃w, bw) such that L(w) compares favorably with OP T that is given by\n\ne\n\ne\n\nOP T :=\n\nmin w=( ̃w,bw)\n\nH\n\n2\n\n1 2\n\nE (x,y)\n\n⇠\n\nD\n\n[((w>x)\n\ny)2].\n\nHere the hypothesis set H that algorithm competes with is the set of ReLU units with parameters w = ( ̃w, bw) with the relative bias k2 being bounded. This is a non-trivial and interesting regime; when the bias is too large in magnitude the optimal ReLU function fitting the data is either the constant zero function almost everywhere, or a linear function almost everywhere.\n\nbw|\n\n ̃w\n\nk\n\n/\n\n|\n\nThis agnostic learning problem has been extensively studied and polynomial time learning algorithms x] is given by exists for a variety of settings. This includes the noisy teacher setting where E[y\n\n|\n\n⇤The last two authors are supported by the National Science Foundation (NSF) under Grant No. CCF-1652491\n\nand CCF 1934931. The last author was also funded by a Google Research Scholar award.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\na ReLU function Kakade et al. (2011); Mukherjee & Muthukumar (2020) and the fully agnostic setting where no assumption on y is made (Goel & Klivans, 2019; Diakonikolas et al., 2020). In a recent work (Frei et al., 2020) analyzed the properties of gradient descent for the above agnostic learning problem when the bias term is assumed to be zero. The gradient descent based learning algorithm corresponds to the following sequence of updates starting from a suitable initializer w0: L(wt). The work of Frei et al. (2020) proved that starting from zero initialization wt+1 = wt  and for distributions where the marginal of x satisfies some mild assumptions , gradient descent iterates produce, in polynomial time, a point wT such that L(wT ) = O(pOP T ) when the domain for x is bounded (it is instructive for this bound to think of OP T < 1; the general expression is more complicated with some additive terms and dependencies on problem-dependent quantities).\n\nr\n\n⌘\n\nWhile the above provides the first non-trivial learning guarantees for gradient descent in the case of agnostic ReLU learning, it suffers from a few key limitations. The result of Frei et al. (2020) only applies in the setting when the distribution has a bounded domain and when the bias terms are zero. When the distribution is not bounded, the error of O(pOP T ) also includes some dimension-dependent OP T ) error. terms; e.g., when the marginal of Moreover, there is a natural question of improving the bound of O(pOP T ) on the error of gradient 1). This is particularly descent (since the most interesting regime of parameters is when OP T intriguing given the recent result of Diakonikolas et al. (2020) that shows that, assuming zero bias, gradient descent on a convex surrogate for L(w) achieves O(OP T ) error. This raises the question of whether the same holds for gradient descent on L(w) itself. In another recent work, the authors in Vardi et al. (2021) are able to provide convergence guarantees for gradient descent in the presence of bias terms, but under the strong realizability assumption, i.e, assuming that OP T = 0.\n\nx is a standard Gaussian\n\nd), it gives a O(pd\n\n(0, Id\n\n⌧\n\nN\n\ne\n\n⇥\n\n·\n\nTo summarize the existing guarantees, to the best of our knowledge, (i) there are no existing guarantees for any polynomial time algorithm (including gradient descent) for agnostic learning of a ReLU function with bias, and (ii) even in the zero bias case, there is no existing guarantee for gradient descent (on the standard squared loss) that achieves O(OP T ) error.\n\n1.1 OUR RESULTS\n\nIn this work we make progress on both these fronts, by improving the state of the art of guarantees for gradient descent for agnostic ReLU regression. In particular, we show that when the marginal of x is a Gaussian, gradient descent on L(w) achieves an error of O(OP T ), even under the presence of bias terms that are bounded. The O(OP T ) guarantee that we get even in the zero bias case answers an open question raised in the work of Frei et al. (2020). There are also no additional dependencies on the dimension. Given the recent statistical query lower bound of Goel & Klivans (2019) that rules out an additive guarantee of OP T + \" for agnostic ReLU regression, our result shows that vanilla gradient descent on the target loss already achieves near optimal error guarantees. Below we state our H such ̃v main theorem. For convenience we assume that 2\nthat L(v) = OP T ), is a constant; Appendix C shows why this is without loss of generality. 1, C2 > 0, c3 > 0 be absolute constants. Let D be a distribution over Theorem 1.1. Let C1  w = (\nx, y) 2\n, and consider population gradient descent iterates: wt+1 = ( ̃w, bw) : k\nL(wt). For a suitable constant learning rate ⌘, when starting from w0 = ( ̃w0, 0) where ̃w0 is wt  e\nrandomly initialized from a radially symmetric distribution, with at least constant probability c3 > 0 one of the iterates wT of gradient descent after poly(d, 1\n\nk2 (the optimal weight, i.e. v = (\n\ne (0, I). Let H =\n\n\" ) steps satisfies L(wT ) = O(OP T ) + \".\n\nR where the marginal over\n\nx is the standard Gaussian\n\n[1/C1, C1],\n\nbw|\n\n⇥ k 2\n\nRd ̃w\n\nC2}\n\nv, bv)\n\nr\n\nN\n\ne\n\nk\n\n{\n\n⌘\n\n|\n\nPlease see Section 4 for the more formal statement and proof. Note that the above guarantee applies to one of the intermediate iterates produced by gradient descent within the first poly(d, 1/\") iterations. This is consistent with other convergence guarantees for gradient descent in non-realizable settings where last iterate guarantees typically do not exist Frei et al. (2020). One can always pick the iterate among the first poly(d, 1/\") steps that has the smallest loss on an independent sample from the distribution D.\n\nThe above theorem proves that gradient descent obtains a bound of O(OP T ) when the relative bias k2 = ⇥(1) for the optimal classifier without of the optimal ReLU function is bounded (recall that loss of generality from Proposition C.1). Note that we do not constrain the gradient updates to remain in the set H. This result significantly improves upon the existing state-of-the-art guarantees Frei et al.\n\n ̃v\n\nk\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n·\n\n(2020) of O(pd OP T ) for gradient descent even when specialized to the case of ReLU activations with no bias. Further this gives the first provable guarantees in the setting with non-zero bias. Our improved bound of O(OP T ) error even with non-zero bias involves several new ideas. At a high level there are two main ingredients that allow us to do beyond the previous work: (1) an improved analysis for gradient descent in the agnostic case that in particular avoids any dimension-dependent factors, and (2) a new “multiscale” random initialization scheme with a stronger guarantee for the initializer. We outline these in more detail in Section 4 and Section 5 respectively.\n\nWe remark that some of the assumptions in Theorem 1.1 are made with a view towards a clearer exposition, and similar guarantees hold in more general settings. While the above theorem gives guarantees for gradient descent on the population loss function L(w) (as in Vardi et al. (2021)), we also prove guarantees for the empirical loss function in Section D. Moreover while the above Theorem 1.1 assumes Gaussian marginals (as this already illustrates the improvements guarantees in a basic and well-studied setting), these techniques extend to a broader class of distributions that we describe next.\n\n1.2 GUARANTEES BEYOND GAUSSIAN MARGINALS\n\nThe above algorithmic result can be generalized to a broader class of marginals than Gaussians, that we call O(1)-regular marginals.\n\nO(1)-regular marginals: Assumptions about the marginals over assumptions about the marginal distribution\n\n1, 02, 2, 3, 4, 5 > 0 and 0 : R+ !\n\nDx over R+, such that e\n(i) Approximate isotropicity and bounded fourth moments: for every unit vector u\n\nx We make the following there exists absolute constants\n\nRd:\n\n2\n\nx\n\ne\n\ne\n\n[\n\nu,\n\nx\n\n2]\n\n[1/02, 2], and E\n\n[\n\nu,\n\nx\n\n4]\n\n4.\n\nh (ii) Anti-concentration: there exists an absolute constant 3 > 0 such that for every unit vector\n\nDx\n\nDx\n\n\n\n2\n\n⇠\n\n⇠\n\nh\n\ni\n\ni\n\nRd,\n\n2\n\nx\n\ne\n\ne\n\ne\n\nE\n\nx\n\ne\n\n ̃u\n\ne 2\n\nRd and > 0, e\n\n(iii) Spread out: there exists 0 : R+ !\n\ne\n\nconstant, and\n\ne\n\nsup t\n2R\n\nP Dx ⇠\n\nx\n\nh\n\nh\n\n ̃u,\n\nx\n\n(t\n\n, t + )\n\nmin\n\n3, 1\n\n{\n\n.\n\n}\n\ni 2\n\ni R+ such that 0(\n\ne\n\n\n\nbv|\n\n|\n\n) > 0 is a constant when\n\nis a\n\nbv|\n\n|\n\nSd\n\n1,\n\n ̃v\n\n8\n\n2\n\nE Dx ⇠\n\nx\n\n( ̃v>\n\nx + bv)\n\n0(\n\n).\n\nbv|\n\n|\n\nh (iv) 2-D projections: In every 2-dimensional subspace of Rd spanned by orthonormal unit\n\ne\n\ni\n\ne\n\ne\n\nvectors ̃u1, ̃u2 2\n\nRd, we have a set G ̃u1, ̃u2 ⇢\n\nR such that ,\n\n(1)\n\n(2)\n\n[ ̃u>2\n\nx\n\n2\n\nG ̃u1, ̃u2 ] = 1\n\no(1), and\n\nP Dx ⇠\n( ̃u>1\n\ne\n\nx\n\ne\n\nh\n\n \n\ne\n\nt\n\n8\n\n2\n\nG ̃u1, ̃u2 ,\n\nE Dx ⇠\n\nx\n\nx)\n\ne\n\n ̃u>2\n\nx = t\n\n5 ·\n\nE Dx ⇠\n\nx\n\ni\n\n⇥\n\n( ̃u>1\n\nx)\n\n.\n\n⇤\n\ne\n\nIn other words, the conditional expectation of ( ̃u>1 on the projection in an orthogonal direction ̃u2, for most values of ̃u>2 Gaussian N (0, I), the r.v.s ̃u>1 and G ̃u1, ̃u2 = R.\n\nx) is not much smaller after conditioning x. Note that for a x are independent, so this condition holds with 5 = 1\n\nx, ̃u>2\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\ne\n\nN\n\nN (0,1)[(g + bv)] > 0 for all bv 2\n\nWe remark that Gaussian distribution 2, and 0(bv) = Eg\n\ne (0, I) is O(1)-regular i.e., all the constants 1, 2, 02, 5 = ); in fact 0 is an increasing ,\n1, 3  1 function that is 0 only at . We also note that assumptions of this flavor have also been used in prior works including Vardi et al. (2021), which inspired parts of our analysis. In particular, Vardi et al. (2021) assume a lower-bound on the density for any 2-dimensional marginal; our assumption (4) on the 2-dimensional marginals is qualitatively weaker (it is potentially satisfied by even discrete distributions), and moreover we only need the condition to be satisfied for a large fraction of values of ̃u>2\n\nx (and not all). See Section B for the generalized version of our main theorem.\n\n⇠ 1\n\n1\n\n(\n\ne\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\n|\n\nThe agnostic ReLU regression problem that we consider has been studied in a variety of settings. In the realizable setting or when the noise is stochastic with zero mean, i.e., E[y x] is a ReLU function, the learning problem is known as isotonic regression and can be solved efficiently via the GLM-tron algorithm (Kakade et al., 2011; Kalai & Sastry, 2009). Distributions generated by a 1-layer ReLU neural network under the realizable setting can also be learned efficiently (Wu et al., 2019). In the absence of any assumptions on the distribution of y x, the work of Goel & Klivans (2019) provided an efficient algorithm that achieves O(OP T 2/3) + \" error under Gaussian and log-concave marginals in the zero-bias setting. The authors also show that it is hard to achieve an additive bound of OP T + \" via statistical query (SQ) algorithms Kearns & Valiant (1994). For the case of zero bias and any marginal over the unit sphere, the work of Goel et al. (2017) provides agnostic learning algorithms for the ReLU regression problem that run in time exponential in 1/\" and achieve an error bound of OP T + \". The recent work of Diakonikolas et al. (2020) improved the upper bound of Goel & Klivans (2019) to O(OP T ) + \" via designing an efficient algorithm that performs gradient descent on a convex surrogate for the loss L(w); very recently they also obtained near optimal sample complexity with a regularized loss (Diakonikolas et al., 2022). Note that all of the above works that study the fully agnostic setting consider the setting where the bias terms are not present.\n\n|\n\nRecent works of Frei et al. (2020); Vardi et al. (2021) consider analyzing gradient descent for the ReLU regression problem. Frei et al. (2020) provides an O(pOP T ) guarantee (along with some additional problem-dependent terms) for the case of zero bias and bounded distributions. When (0, I) the bound of Frei et al. (2020) incurs considering distributions such as the standard Gaussian a dimension dependent term of the form O(pd pOP T ) in the error bound. Vardi et al. (2021) ·\nprovides a tighter analysis that also extends to the case of non-zero bias. However the analysis only applies in the realizable setting, i.e., when OP T is zero. Our main result provides improved bounds over these works by providing a dimension independent error bound that applies to the case of non-zero bias as well.\n\nN\n\nThere is also a long line of work analyzing gradient descent for broader settings. The works of Ge et al. (2015; 2018); Jin et al. (2017); Anandkumar & Ge (2016); Soltanolkotabi (2017) show convergence of gradient descent updates to approximate stationary points in non-convex settings under suitable assumptions on the function being optimized. Another line of work considers the global convergence properties of gradient descent. These works establish that gradient descent on highly overparameterized neural networks converges to the global optimum of the empirical loss over a finite set of data points (Allen-Zhu et al., 2019; Du et al., 2019; Jacot et al., 2021; Zhong et al., 2017; Chizat & Bach, 2018; Lee et al., 2019; Arora et al., 2019). Yet another line of work considers the realizable setting where data is generated from an unknown small depth and width neural network. These works analyze the local convergence properties of gradient descent when starting from a suitably close initial point (Bartlett et al., 2018; Zou et al., 2020).\n\n3 PRELIMINARIES\n\nWe consider agnostically learning a single ReLU neuron with bias through gradient descent under Rd+1 follows the the supervised learning setting. We assume we are given data (x, y), where x (0, I) in the first d dimensions and the d + 1’th dimension being a standard Gaussian distribution R are arbitrarily correlated with x and (w>x). constant 1. We also assume the labels y\n\nN\n\n2\n\n2\n\nNote that throughout the paper, we will use\n\nw,\n\nv,\n\nrespectively, with the last dimension of w being bw 2\n\nfact\n\nw>\n\nx + bw.\n\nx to denote the first d dimensions of w, v, x R). Therefore, w>x is in R (similarly for bv 2\n\ne\n\ne\n\ne\n\nIn the analysis, we will compare the current iterate w to any optimizer of the loss L(w).\n\ne\n\ne\n\nv := arg min H\n\nw\n\n2\n\nL(w), where L(w) =\n\n1 2\n\nE (x,y)\n\n⇠\n\nD\n\n((w>x)\n\nh\n\ni\n\ny)2\n\n,\n\n(3)\n\n[ 1 and the hypothesis set H = C1 absolute constants. This is to ensure that the relative bias Appendix C allows us to assume\n\n, C1], /\nbw| , C1] without loss of generality.\n\nbw| , where C1 and C2 are k2 is bounded; as described earlier\n\nw = ( ̃w, bw) :\n\nC2)\n\nk 2\n\n| ̃w\n\n ̃w\n\n ̃w\n\n{\n\nk\n\nk\n\n}\n\n|\n\n[ 1 C1\n\nk\n\nk 2\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAs we are in the agnostic setting, there may be no w that achieves zero loss. We can split the loss function L(w) into two components, one of which is F (w) defined by\n\nF (w) :=\n\n1 2\n\nE\n\n((w>x)\n\n(v>x))2\n\n,\n\nh\n\nF (w) := E\n\n((w>x)\n\n(v>x))0(w>x)x\n\n.\n\n(4)\n\nr\n\ni\n\nh\n\ni\n\nWe will often refer to F (w) as the realizable loss, since it captures the difference between w and v; in the realizable setting L(w) = F (w). Note that F (v) = 0.\n\nGradient of the Loss. The gradient of L(w) with respect to w is\n\nr\n\nL(w) = E\n\n((w>x)\n\nh\n\ny)0(w>x)x\n\ni\n\n(5)\n\n) is the derivative of (\n\n. Note that the ReLU function (z) where 0( is differentiable everywhere except at z = 0. Following standard convention in this literature, we define 0(0) = 1. Note that the exact value of 0(0) will have no effect on our results.\n\n), defined as 0(z) =\n\n{\n\n}\n\n0\n\nz\n\n·\n\n·\n\nWe can also decompose\n\nL(w) as\n\nr\n\nL(w) = E\n\n((w>x)\n\nL(w) =\n\nh F (w) + E\n\nr\n\nr\n\nr\n\nTherefore,\n\n(v>x))0(w>x)x\n\n+ E\n\n((v>x)\n\n((v>x)\n\nh\n\ny)0(w>x)x\n\ni\n\nh\n\ni\n\ny)0(w>x)x\n\n(6)\n\n(7)\n\ni\n\nGradient Descent. Finally, our paper focuses on the standard gradient descent algorithm with a fixed learning rate ⌘> 0. We initialize at some point w0 2 N we F (wt). We do not optimize the iteration count in this paper; hence it will be have wt+1 = wt  instructive to think of ⌘ as a non-negligible parameter that can be set to be sufficiently small (e.g., an inverse polynomial for polynomial time guarantees).\n\nRd+1, and at each iteration t\n\nr\n\n2\n\n⌘\n\nk2 = 1; the same analysis goes Simplification. For sake of exposition we will assume that ̃v through when k2 is normalized is without loss of generality. Note that we cannot make such a simplifying assumption about the vectors wt = ( ̃wt, bw) in the intermediate iterations.\n\n[1/C1, C1] as well. Moreover Proposition C.1 shows that assuming that\n\n ̃v k2 2\n\n ̃v\n\nk\n\nk\n\nk\n\nFinally, please see Section B for the weaker distributional guarantees and guarantees.\n\n4 OVERVIEW OF THE ANALYSIS (PROOF OF THEOREM 1.1)\n\nWe now provide an overview of our analysis. For complete proofs of the lemmas and propositions, please refer to the supplementary material (Appendix A). Recall that our goal throughout the learning Rd+1 such that L(w) achieves a comparable performance to OP T = L(v). process is to find a w In order to accomplish this, we aim to find w such that it is close to v, i.e. is small. Note that approximating v suffices to achieve an error close to OP T , since we can upper-bound L(w) as\n\nw\n\n2\n\nk\n\nk\n\nv\n\nL(w) =\n\nE\n\n((w>x)\n\nE\n\n((w>x)\n\n(v>x) + (v>x)\n\n1 2\n\ny)2\n\n=\n\n1 2\n\ny)2\n\n2\n\n1 2\n\nh ((w>x)\n\nh 1\n2 through Young’s inequality. The realizable portion of the loss F (w) becomes O(OP T ) when O(pOP T ) (see Lemma 4.4 for a proof), and as a consequence we will get O(OP T )\n\n= 2F (w) + 2OP T\n\ni (v>x))2\n\n((v>x)\n\ny)2\n\n+ 2\n\n\n\nw\n\nE\n\nE\n\ni\n\ni\n\nh\n\ni\n\nh\n\n·\n\n·\n\nv k\nerror in total.\n\nk \n\nTo formalize our intuition above, we adopt a similar proof strategy used in Frei et al. (2020). Namely, we argue that when optimizing with respect to the agnostic loss L(wt), we are always making some and due to a decrease in F (wt) (which is just the non-trivial progress due to a decrease in k\nrealizable portion of the loss). Moreover, whenever we stop making progress, we will argue that at O(pOP T ); in both cases, this iterate O(pOP T ) or this point either already achieves an error of O(OP T ) due to Lemma 4.4 and Lemma 4.3.\n\nwt \n\nwt \n\nF (wt)\n\nk \n\nk \n\nkr\n\nk\n\nk\n\nv\n\nv\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAt a high-level the analysis of gradient descent follows a Challenges in arguing progress. similar approach to Frei et al. (2020) which only handles zero bias. Yet there are several new ideas needed to obtain the stronger O(OP T ) guarantee even for the zero-bias case. Moreover, allowing non-zero bias terms imposes extra technical challenges. For example, the probability measure of under Gaussian distributions, which is vital to deriving the gain in each {\ngradient descent step, does not have a closed-form expression when bias is present. Furthermore we cannot afford to lose any dimension dependent factors or assume boundedness. Thus, to address these difficulties, more detailed analyses (e.g. Lemma 4.1, 4.2) are needed to facilitate our argument.\n\n0, v>x\n\nw>x\n\n}\n\n0\n\nMoreover tackling non-zero bias terms requires additional assumptions when initializing w0 as well. The initializer finds a w0 such that F (w0) is strictly less than F (0) by a constant amount > 0 (this is inspired by Vardi et al. (2021), however  in their case can have an inverse-polynomial dependence on the dimension ). In fact our multiscale random initialization and the improved analysis is crucial to obtaining a dimension-independent bound on the error. The high-level intuition behind why this property is useful is that it ensures that gradient descent does not get trapped around a highly non-smooth region (e.g. when w = 0) by making it start at somewhere better than it, so that w keeps moving closer to v. Moreover, in our case the analysis is more challenging to implement compared to Vardi et al. (2021) because of the agnostic setting. This is because Vardi et al. (2021) heavily relies on the realizability assumption to simplify its analysis.\n\nWe also highlight our improvements on the dependency of the dimension d. In previous works, the guarantees of the algorithm has a dependence on d either explicitly or implicitly. For instance, in Frei et al. (2020) the O(pOP T ) guarantee for ReLU neurons includes a coefficient in terms of BX (the ), which for Gaussian inputs is in fact pd; or for example in Vardi et al. (2021), upper-bound for the gain for each gradient descent iteration  comes with a dependency on c (the upper-bound for 4. In contrast, we avoid such dependencies on the dimension d\n\n8, which for Gaussian is d\n\n) of c\n\nx\n\nx\n\nk\n\nk\n\nk\n\nk in order to obtain our guarantees.\n\nWe first establish two important lemmas we will later utilize in proving progress in each iteration. k2 = 1. The first lemma As stated in the preliminaries, we assume in the rest of the section that gives a lower bound on the measure of the region where both (v>x) and (w>t x) are non-zero. Our inductive hypotheses will ensure that this lower bound is a constant (if\n\nis a constant).\n\n ̃v\n\nk\n\nLemma 4.1 (Lower bound on the measure of the intersection). Suppose the marginal distribution x is O(1)-regular. There exists an absolute constant c > 0 such that for all > 0, if Dx over F (0)\n\n then\n\nbv|\n\n|\n\nF (w) e\n\n\n\ne\n\nP[w>x\n\n0, v>x\n\n0]\n\n2 4\nk\n\n2k\n\nv\n\n4 2\n\nk\n\n=\n\n2 2(1 +\n\n4 k\n\nc\n\nw\n\nk\n\n2)2 .\n\nbv|\n\n|\n\nc\n\nw\n\nk\n\n(8)\n\nWith Lemma 4.1, the following lemma allows us to get an improvement on the realizable portion of the loss function as long as the gradient is non-negligible. We state and prove this lemma for the general case of O(1)-regular marginal distributions.\n\nLemma 4.2 (Improvement from the first order term). Suppose the marginal over There exists absolute constants c1, c2 > 0 such that for any > 0, if k2, 2, where  = c19 B28 . F (w) k\n\nF (w), w\n\n, then\n\nF (0)\n\ni \n\nhr\n\n\n\nw\n\nk\n\nk\n\nv\n\nv\n\nv\n\nk e\n\nx is O(1)-regular. B and\n\nw\n\nk2 \n\nThe constants c1, c2 depend on the constants 1, 02, 2, 4 etc. in the regularity assumption of Dx. We remark that for our setting of parameters  = ⌦(1) and B = O(1), and hence we will conclude that\n\n2). Please refer to Appendix A for all the complete proofs.\n\nF, w\n\n⌦(\n\nw\n\nv\n\nv\n\ne\n\nhr\n\ni \n\nk\n\n2 k\n\n4.1 MAIN PROOF STRATEGY\n\nWith these two key lemmas, we are now ready to discuss the proof overview of the main theorem (Theorem 1.1). We inductively maintain two invariants in every iteration of the algorithm:\n\n(A)\n\nwt \n\nk\n\nv\n\nk2 \n\nO(1),\n\nand\n\n(B) F (0)\n\nF (wt) = ⌦(1).\n\nThese two invariants are true at t = 0 due to our initialization w0. Lemma B.3 guarantees with at least constant probability ⌦(1), both the invariants hold for w0. The proof that both the invariants\n\n6\n\nPublished as a conference paper at ICLR 2023\n\ncontinue to hold follows from the progress made by the algorithm due to a decrease in both k\nand F (wt) (note that we only need to show they do not increase to maintain the invariant).\n\nwt \n\nv\n\nk2\n\nThe argument consists of two parts. First, assuming F (wt) > 0), we establish that whenever k\n2 v\nwt  wt+1  always makes progress i.e. k\nk initialized such that F (w0) F (0) F (wt) always decreases, i.e. the inequality F (wt)\n\n holds (for some constant 2 >OP T for some constant > 0, gradient descent 2 is lower bounded. Next, we argue that if w0 is  k  for some constant > 0, then throughout gradient descent\n\nwt \n\nF (w0)\n\nF (0)\n\n\n\n\n\nk\n\nk\n\nv\n\nv\n\n\n\nF (0)\n\n\n\n always holds.\n\nHowever, unlike Vardi et al. (2021) where they focus on the realizable setting, analyzing gradient L(w) and not descent on the agnostic loss L(w) is more challenging, since the update depends on F (w). In fact, the additional term from the “non-realizable” portion of the loss L(w) can overwhelm\n\nr the contribution from the realizable loss when either k2  O(pOP T ). The following two lemmas argue that in both of these cases, the current iterate already achieves O(OP T ) error (and this iterate will be the T that satisfies the guarantee of Theorem 1.1). O(pOP T )). Suppose B,  > 0 are constants such that . Then there exists a constant CG > 0, such that if O(pOP T ).\n\nkr B and F (w) CGpOP T then\n\nLemma 4.3 (Success if k2  k \n\nO(pOP T ) or\n\nw k\nF (w)\n\nF (0) v\n\n k2 \n\nwt \n\nk kr\n\nk2 \n\n w\nk\n\nk2,\n\nk \n\nkr\n\nr\n\nF\n\nF\n\nk\n\nv\n\nv\n\nProof. We can first apply Lemma 4.2 to conclude that constant > 0 (since B,  > 0 are constants), hence we have v\nv\n\n2. Thus\n\nhr\n\nw\n\nw\n\nv\n\ni \n\nk\n\nk\n\nk\n\nk2 = O(pOP T ) which implies the lemma. O(pOP T ), then F (wt)\n\nv We now argue that if this is stated and proven for O(1)-regular distributions.\n\nwt \n\nk \n\nk\n\n\n\nF (w), w\n\nv\n\n F (w)\n\ni  w\n\nkk\n\nkr\n\nv\n\nw\n\n2 for some \nk k\n F (w), w v\nk  hr\n\nO(OP T ) through the following lemma;\n\nLemma 4.4 (Small defined above. If\n\nwt \n\nk\n\nv\n\nv\n\nk k2 \n\nwt \n\nk\n\nimplies small F (wt)). Assume O(pOP T + \") for some \"> 0, then F (wt)\n\nDx is O(1)-regular with parameters\n\nO(OP T + \").\n\n\n\nProof. Since ReLU function is 1-Lipschitz (i.e.\n\n(z)\n\n|\n\n(z0)\n\nz\n\nz0\n\n|\n\n),\n\nF (wt) =\n\n1 2\n\nE\n\n((w>t x)\n\n(v>x))2\n\n1 2\n\n\n\ni\n\nE\n\n(w>t x\n\nh\n\n= k\n\nwt \n\n2\n\n2\n\nv\n\nk\n\nE\n\n(u>x)2\n\nh\n\ni\n\ni\n\n, hence the last equation. Now, notice by using Young’s inequality, we\n\nh\n\nwhere we defined u = wt wt get\n\nk\n\nv v\n\nk\n\n(u>x)2\n\nE\n\n= E\n\n(\n\nu>\n\nx + bu)2\n\n2 E\n\n(\n\nu>\n\nx)2\n\n+ 2b2\n\nu \n\n22 + 2b2\n\nu \n\nO(1)\n\ndue to the regularity assumption on\n\ni\n\nh\n\nh\n\n\n\ni\n\nDx. Hence\n\nh\n\ni\n\ne\n\ne\n\ne\n\ne\n\nwt \n\nk\n\n2\n\nv\n\n2 k\ne\n\nO(1)\n\n·\n\nO(\n\nwt \n\nk\n\nv\n\n2)\n\n2 k\n\n\n\n\n\nO(OP T + \")\n\ne\n\n|| v>x)2\n\nF (wt)\n\n which concludes the proof.\n\nProving progress in lemma. Lemma 4.5 (Decrease in\n\nk\n\nwt \n\nv\n\nk\n\nand F (wt). To show\n\nwt \n\nk\n\nv\n\nk\n\ndecreases, we establish the following\n\nv\n\nand\n\nwt  Dx is O(1)-regular. For constants ⌘ = 0.05\n\n , Cp = 1 9 ( 19.8/2 where  is defined as in Lemma 4.2, if for some \"> 0 2\nthen\n\n). Assume at time t, F (wt)\n\n⌘C 0(OP T + \").\n\n· d2\n\nk\n\nk\n\nv\n\nv\n\ne k\n\nwt+1 \n\n2 k\n\nwt \n\n k\n\nk\n\nF (0)\n\n 1002\n\n where > 0 is a constant 2 /2+90 2/ v\n\n ), C 0 = p (OP T + \"), q\n\n+ 10 1C 2\n\n2 > \n\n2\n\nq k\n\nwt \n\nk\n\nAs a direct consequence of Lemma 4.5, we obtain the following inductive statement: for every t, 2\n⌘C (OP T + \") is true for some constant C > 0 or (b) either (a) k\n1(OP T + \")) holds. Observe that when (b) holds Lemma 4.4 implies the loss is wt  O( k\nO(OP T ); hence we need only assume at time t (b) does not hold yet, thus it suffices focusing on showing (a) is true. Additionally, note at each timestep t,\n\nwt+1 \n\nwt \n\nk 2\nk\n\n k\n\n\n\nk\n\nv\n\nv\n\nv\n\n2\n\nwt \n\nk\n\nv\n\n2 k\n\nwt+1 \n\nv\n\n k\n\n2 = 2⌘\n\nk\n\nL(wt), wt \n\nv\n\nhr\n\ni \n\n⌘2\n\nkr\n\nL(wt)\n\n2 k\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n2 L(wt), wt  v\nv wt  Therefore, to lower-bound k\ni k\n2. To show that F (wt) decreases we show that at time t, if gradient L(wt) and an upper bound for k\nkr descent continues to make progress towards v, then F (wt+1) . The progress in F (wt) F (w) follows crucially relies on Lemma 4.2. Please see Appendix A in the supplementary material for the detailed proofs.\n\n2, we will give a lower bound for\n\nwt+1 \n\nF (0)\n\nk\n\nhr\n\n\n\n\n\nk\n\nv\n\n5 RANDOM INITIALIZATION\n\n2\n\nRd which is\n\nWe now prove the initialization lemma assuming weak conditions on the marginal distribution over Dx (recall that the standard Gaussian N (0, I) also satisfies all of the properties). x\nWe will initialize w = ( ̃w, bw) with bw = 0 and ̃w drawn from a spherical symmetric distribution D⇢ so that it has a non-negligible probability in any Dw. The length is chosen from the distribution k2) where a2 > a1 > 0 are constants: our specific choice constant length interval (a1k\n\npicks the correct length scale with non-negligible probability, and is reasonably spread out.\n\nk2, a2k\n\ne\n\ne\n\nv\n\nv\n\nOur new random initialization and the improved analysis are crucial in obtaining the O(OP T ) guarantee even with non-zero bias. Our multiscale random initialization scheme tries out different length scales and ensures that with non-negligible probability we get an initializer that satisfies k2 (up to a factor of 2), our the required property. For the correct guess of length scale of improved analysis (see (10)) shows that the random spherically symmetric initialization with constant probability produces an initializer w with F (w) 2). When we have unknown [1/M, M ], the random initialization can try out the different length scales length scale in geometric progression i.e., the length scale ⌧ is chosen uniformly at random from Z,\n\n ̃v k2 2 j\n\nF (0) =\n\nlog M\n\nlog M\n\nj : j\n\n2 k\n\n2\n\n⌦(\n\n2\n\n ̃v\n\n ̃v\n\nk\n\n{\n\nk\n\nk\n\n\n\n\n\n. }\n\nlog M , 2log M ] Multiscale random initialization We are given a parameter M such that (note that M can have large dependencies on d and other parameters; our guarantees will be polynomial in log M ). A random initializer w = ( ̃w, 0) is drawn from\n\nk2 2\n\n[2\n\nk\n\nv\n\nDunknown(M ) as follows:\n\n1. Pick j uniformly at random from\n\nlog M\n\n,\n\nlog M\n\n+ 1, . . . ,\n\n1, 0, 1, . . . ,\n\nlog M\n\n d\n\ne\n\nd\n\ne\n\nD⇢ as follows: we first pick1 g\n\n| ⇠\nRd is drawn and we output ̃w = ⇢ ˆw. The initializer is\n\n|\n\nd\n\nN (0, 1) and set ⇢ = 2j\n\n.\n\n.\n\ne g\n\n2. ⇢\n\nR+ is drawn according to\n\n3. A uniformly random unit vector ˆw\n\n2\n\n( ̃w, 0).\n\n2\n\nWe prove the following claim about the multiscale random initializer.\n\nLemma 5.1. There exists c1(v), c2(v), c3(v) > 0 which only depend on bv/ dimension), and are both absolute constants when When w = ( ̃w, bw = 0) is drawn according to the distribution v\nsome given M\n\nk2 (and not on the k2 = O(1), such that the following holds. Dunknown(M ) described above for\n\n[1/M, M ]. Then with probability at least c2(v)/ log M ,\n\n1 satisfying\n\nbv|\n\n ̃v\n\n ̃v\n\nk\n\n/\n\nk\n\n|\n\nk2 2\n\nk F (0)\n\nF (w)\n\n\n\nc1(v)2\n\n ̃v\n\n2 k\n\nk\n\n2, and\n\nw\n\nk\n\nv\n\nk \n\nc3(v)\n\n ̃v k2\n\nk\n\n(9)\n\nIn the above lemma, if become much simpler, as described in Section B.3. The guarantees for the multiscale random initialization scheme follows from the analysis of random initialization when the length scale of\n\nDx is a standard Gaussian N (0, I), the descriptions of these above constants\n\n ̃v\n\nk ⇥(1)). For convenience, we will set N (0, 2) with  2\nare constants when\n\nk2 = 1 is known. Without loss of generality (see Section C, we can assume that\n\nk2 = 1 (or D⇢ to be the absolute value of a standard Gaussian N (0, 1) (or [1, 2]. In this setting, we can show for constants c1(v), c2(v), c3(v) > 0 (these bv|\n\nk2 is bounded), we have with probability at least c2(v) > 0\n\n ̃v\n\n ̃v\n\n/\n\nk\n\n|\n\ne\n\n2 k\nWe remark that for random initialization to work, we only need the probability of success ⌘ \nc2(v) > 0 to be non-negligible (e.g., at least an inverse polynomial). We can try O(1/⌘) many random initializers, and amplify the success probability to be very close to 1.\n\nc1(v)2\n\n2, and\n\nc3(v)\n\nk2.\n\nF (0)\n\n(10)\n\nk \n\n\n\nw\n\n ̃v\n\n ̃v\n\nk\n\nk\n\nk\n\nv\n\nk F (w)\n\n1One can pick many other spread out distributions in place of the absolute value of a Gaussian.\n\n8\n\n Published as a conference paper at ICLR 2023\n\n2\n\nk\n\nOverview of the proof of Lemma 5.1 We now outline the argument of Lemma 5.1. Please refer v := to Section B.3 and Section B.4 for the full proofs. For convenience we define ̃v k2, so they are normalized w.r.t. the length of ̃v. The conditions of the lemma assume that v/ = O(1). The multiscale random initialization finds the correct length scale with probability at k2 = 1 is known; without k2 = 1. By definition, the distribution of\n\nbv| |\nleast 1/(log M ). For the rest of the overview we assume that the length loss of generality (see Section C), we can assume that b\n ̃w\n\nRd is spherically symmetric.\n\nbv := bv/\n\nk2,\n\nb\n\nb\n\n ̃v\n\n ̃v\n\n ̃v\n\nk\n\nk\n\nk\n\nF (w)\n\nF (0) =\n\n=\n\n1 E\n2 x\n⇢2\n\nh 2\n ̃v 2\nk k\n2\n\n(( ̃w>x)\n\n( ̃v>x + bv))2\n\n((\n\nw>x)2\n\nE x\n\nh\n\ni\n\ni 2\n2 E k\nx\n\n ̃v\n\n⇢\n\nk\n\nh\n\n1 2\n\n( ̃v>x + bv))2\n\nE x\n\nh w>x)(\n\nv>x +\n\ni bv))\n\n,\n\n(\n\nwhere ̃w = ⇢ picked along a uniformly random direction i.e.,\n\nw being the unit vector along ̃w. For a fixed ⇢ 1. Hence for x w\n\nw with\n\n ̃v k2\n\nb\n\nb\n\nk\n\n⇠U Sd\n\ni\n\nw (and hence ̃w) is\n\nb\n\nR+, b\n2 Dx, ⇠\n\nE ⇠Sd\n\nw\n\n1\n\nb\n\nb w, 0))\n\n[F ((⇢\n\nb\n\nb\n\nF (0)] =\n\n⇢2\n\n2 2\nk\n\n ̃v k\n2\n\nw\n\nE b\n⇠U Sd\n\n1 E\n\nx\n\n⇢\n\n ̃v\n\n2 2\nk\n\nk\n\nE ⇠U Sd\n\nw\n\n1 E\n\nb\n\nx\n\nDx\n\n⇠\n\n(\n\nh\n\n((\n\nw>x)2\n\ne\n\nDx\n\n⇠ h\nw>x)( e\n\ni bv))\n\n=\n\nv>x +\n\nb\n\nb\n\n(11)\n\nc0⇢2\n\n2c3(v)⇢\n\n ̃v\n\n2 2\nk\n\nk\n\nb\n\nwhere c0 > 0 is a universal constant based on our assumptions about ⇠\nN (0, I)). One technical portion of the argument is to derive an expression for c3(v), and prove that it is a constant independent of the dimension. This forms the bulk of the argument and requires symmetrization and careful use of anti-concentration bounds. Once we establish this, we need to prove that the first part (10) holds with non-negligible probability. From (11), we note that for any ⇢\n\nDx (c0 = 0.5 for x\n\n, we have that\n\ne\n\nb\n\nb\n\ne\n\ni\n\nb\n\nc3(v) 2c0\n\n, c3(v) c0\n\n2\n\n⇥\n\n⇤\n\n[F ((⇢\n\nw, 0))]\n\nE ⇠U Sd\n\nw\n\n1\n\nF (0)\n\n ̃v\n\n2 2\nk\n\n k\n\n\n\nc3(v)2 2c0\n\n.\n\nb\n\nb\n\nMoreover ⇢ is distributed as the absolute value of a standard normal with variance in [1, 4]; so we get from anti-concentration bounds that ⇢ is in the right interval with probability at least c5(v) > 0, . For which is constant when a fixed ⇢ in this interval, let Z be a r.v. that captures the distribution of F ((⇢ w\nis drawn uniformly from the unit sphere Sd\n\nis a constant. Now we condition on this event that ⇢ ̃v ⇥\nk 2c3(v)2/2c0. 2\nk 24 +\n\nc3(v) 2c0 2\nw, 0)) k\n\n1. Note that E[Z]\n\n, c3(v) c0\n\nw, 0))2]\n\nF (0) as\n\nVar[Z]\n\nO(1)\n\nbv|\n\nb4\n\nb\n\nb\n\nb\n\n ̃v\n\n⇤\n\n|\n\n.\n\nE[F ((⇢\n\nv\n\n\n\n\n\n·k\n\nFurther for  = \nfor some absolute constant c6 > 0\n\nE[Z]/2, we have from the Cantelli-Chebychev one-sided tail inequality we have\n\n ̃v k2\n\nk\n\nb\n\n k 4\n ̃v k\n\n⇣\n\nP\n\nZ\n\nE[Z]/2\n\n\n\nh\n\ni\n\nE[Z]2\n\n4 Var[Z] + E[Z]2 \n\nn\n\nmin\n\nc6c3(v)2/(4 +\n\nb4 v),\n\n=: c6(v),\n\nwhere c6(v) is a constant when 2) with probability at least c5(v) ⌦( k\nk ̃v v\nw k2  k \n\nk2 +\n\nw\n\nb\n\n ̃v\n\nk\n\n·\n\nk bounded by assumption. See Sections B.3 and B.4 for the full proofs.\n\nk2 is upper bounded just because of our choice of ⇢ and\n\nbv is a constant. This allows us to conclude that F (w) < F (0)\n\nb c6(v) which is a constant when\n\n bv is a constant. Finally k2 being upper\n\n ̃v\n\nk\n\nb\n\n⌘\n\nb\n\n1 2\n\no\n\n6 CONCLUSION\n\nIn this paper, we provided a convergence analysis of gradient descent for learning a single neuron with general ReLU activations (with non-zero bias terms) and gave improved guarantees under comparable assumptions also made in previous works. We addressed multiple challenges for analyzing general ReLU activations with non-zero bias terms throughout our analyses that may lead to better understanding of the dynamics of gradient descent when learning ReLU neurons. However, our analysis does not apply to modern neural networks that have multiple nodes and layers. The major open direction is to generalize current performance guarantees for networks of multiple neurons and higher depth.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019.\n\nAnimashree Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points\n\nin non-convex optimization. In Conference on learning theory, pp. 81–102. PMLR, 2016.\n\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322–332. PMLR, 2019.\n\nPeter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International conference on machine learning, pp. 521–530. PMLR, 2018.\n\nLénaïc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 3040–3050, Red Hook, NY, USA, 2018. Curran Associates Inc.\n\nIlias Diakonikolas, Daniel M Kane, Vasilis Kontonis, and Nikos Zarifis. Algorithms and sq lower bounds for pac learning one-hidden-layer relu networks. In Conference on Learning Theory, pp. 1514–1539. PMLR, 2020.\n\nIlias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning a single neuron with adversarial label noise via gradient descent, 2022. URL https://arxiv.org/abs/ 2206.08918.\n\nSimon S. Du, Xiyu Zhai, Barnabás Póczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https: //openreview.net/forum?id=S1eK3i09YQ.\n\nSpencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient\n\ndescent. Advances in Neural Information Processing Systems, 33:5417–5428, 2020.\n\nRong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor decomposition. In Conference on learning theory, pp. 797–842. PMLR, 2015.\n\nRong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape In International Conference on Learning Representations, 2018. URL https://\n\ndesign. openreview.net/forum?id=BkwHObbRZ.\n\nSurbhi Goel and Adam R Klivans. Learning neural networks with two nonlinear layers in polynomial\n\ntime. In Conference on Learning Theory, pp. 1470–1499. PMLR, 2019.\n\nSurbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial\n\ntime. In Conference on Learning Theory, pp. 1004–1042. PMLR, 2017.\n\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and In Proceedings of the 53rd Annual ACM generalization in neural networks (invited paper). SIGACT Symposium on Theory of Computing, STOC 2021, pp. 6, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380539. doi: 10.1145/3406325.3465355. URL https://doi.org/10.1145/3406325.3465355.\n\nChi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International Conference on Machine Learning, pp. 1724–1732. PMLR, 2017.\n\nSham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. Advances in Neural Information Processing Systems, 24, 2011.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAdam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression.\n\nIn COLT. Citeseer, 2009.\n\nMichael Kearns and Leslie Valiant. Cryptographic limitations on learning boolean formulae and\n\nfinite automata. Journal of the ACM (JACM), 41(1):67–95, 1994.\n\nSamuel Kutin. Extensions to mcdiarmid’s inequality when differences are bounded with high\n\nprobability. 05 2002.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf.\n\nAnirbit Mukherjee and Ramchandran Muthukumar. A study of neural training with non-gradient and\n\nnoise assisted gradient methods. 2020.\n\nShai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to\n\nAlgorithms. Cambridge University Press, USA, 2014. ISBN 1107057132.\n\nMahdi Soltanolkotabi. Learning relus via gradient descent. In NIPS, 2017.\n\nGal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a single neuron with bias using gradient\n\ndescent. Advances in Neural Information Processing Systems, 34, 2021.\n\nShanshan Wu, Alexandros G. Dimakis, and Sujay Sanghavi. Learning distributions generated by one-layer relu networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8105–8115, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ a4d41b834ea903526373a9a1ae2ac66e-Abstract.html.\n\nKai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees for one-hidden-layer neural networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, pp. 4140–4149. JMLR.org, 2017.\n\nDifan Zou, Philip M Long, and Quanquan Gu. On the global convergence of training deep linear\n\nresnets. arXiv preprint arXiv:2003.01094, 2020.\n\n11",
    "reference": "# Summary Of The Paper\n\nThis work studies the convergence of gradient descent in the regime of learning ReLU activations agnostically for a class of distributions that contains the standard high dimensional normal distribution. In contrast with previous results, they consider the case where bias exists and apply gradient descent on the weight vector+bias. Their approach essentially connects the analysis of [1] and [2].\n\n[1]: Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient descent.\n[2]: Gal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a single neuron with bias using gradient descent\n\n# Strength And Weaknesses\n\nPros:\n1. This is an important problem. Previous results have focused on the unbiased case, so this is somehow the first result for biased ReLU activations.\n2. The gradient descent happens in bias and the weight vector, which means that the naive idea of extending the dimension works\n\nCons:\n1.  The authors state in the abstract that this is the first algorithm based on gradient descent that achieves these guarantees when the bias is zero. But [3,4] apply gradient descent and achieve $O(OPT)$. The first work does gradient descent on a surrogate loss which differs from this work, where the gradient descent is applied to the standard objective. Meanwhile, the second work uses gradient descent on the same objective and achieves $O(OPT)$. So, I believe this sentence should be removed from the abstract. \n2.  This work considers the case where the ratio between $||w||_2$ and $b_w$ is bounded by a constant. The title and the abstract leave the impression that this work provides a theorem for any value of bias which is not the case. One of the main difficulties of learning ReLU activations is to sample enough points so that $\\sigma'(w^Tx+b)>0$  and $\\sigma(u^Tx+b')>>0$ ($u,b'$ are the best parameters for this instance) for a large portion of the points (see [5] theorem 4.2 for the intuition); with these assumptions, this is somehow easily satisfied (for standard Gaussian) with a good initialization (Section 5) because the mean and variance of the RV $u^Tx+b'$ is bounded by constants. \n\n[3]: Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R. Klivans, Mahdi Soltanolkotabi. Approximation Schemes for ReLU Regression\n[4]: Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning a single neuron with adversarial label noise via gradient descent\n[5]: Gilad Yehudai, and Ohad Shamir. Learning a Single Neuron with Gradient Methods\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work is sound and well-written.\n\np.9 has a broken reference.\n\n# Summary Of The Review\n\nI think the contributions of this work are not sufficient for acceptance in ICRL.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDIFFERENTIABLE and TRANSPORTABLE STRUCTURE LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDirected acyclic graphs (DAGs) encode a lot of information about a particular distribution in its structure. However, compute required to infer these structures is typically super-exponential in the number of variables, as inference requires a sweep of a combinatorially large space of potential structures. That is, until recent advances made it possible to search this space using a differentiable metric, drastically reducing search time. While this technique— named NOTEARS —is widely considered a seminal work in DAG-discovery, it concedes an important property in favour of differentiability: transportability. To be transportable, the structures discovered on one dataset must apply to another dataset from the same domain. In our paper, we introduce D-Struct which recovers transportability in the discovered structures through a novel architecture and loss function, while remaining completely differentiable. Because D-Struct remains differentiable, our method can be easily adopted in existing differentiable architectures, as was previously done with NOTEARS. In our experiments, we empirically validate D-Struct with respect to edge accuracy and structural Hamming distance in a variety of settings.\n\n1\n\nINTRODUCTION\n\nMachine learning has proven to be a crucial tool in many disciplines. With successes in medicine [1–5], economics [6–8], physics [9–14], robotics [15–18], and even entertainment [19–21], machine learning is transforming the way in which experts interact with their field. These successes are in large part due to increasing accuracy of diagnoses, marketing campaigns, analyses of experiments, and so forth. However, machine learning has much more to offer than improved accuracy alone. Indeed, recent advances seem to support this claim, as machine learning is slowly recognised as a tool for scientific discovery [22–25]. In these successes, machine learning helped to uncover a previously unknown relationships between variables. Discovering such relationships is the first step of the long process of scientific discovery and are the focus of our paper as D-Struct— the model we propose in this paper —aims to help through differentiable and transportable structure learning.\n\n}\n\n:=\n\nX, Y, Z\n\n, governed by a joint-distribution, P\n\n. A DAG interact with each other. Consider a three-variable . A DAG explicitly models variable\n\nThe structures. We focus on discovering directed acyclic graphs (DAGs) in a domain helps us understand how different variables in domain {\nX interactions in P G\nas a DAG. Such a DAG allows useful analysis of dependence and independence of depicts P Z\nvariables in P |\nas X does not give us any additional information on Y once we know Z. While DAGs are the model of choice in causality [28], it is impossible to discover a causal DAG from observational data alone [29–32]. As we only wish to assume access to observational data, our goal is not causal discovery.\n\n, we learn that X does not directly influence Y , and that X\n\n. For example, consider the following DAG:\n\n[26, 27]. From\n\n, where\n\n= X\n\n⊥⊥\n\nX\n\nX\n\nZ\n\nG\n\nG\n\nY\n\nY\n\nX\n\nX\n\nX\n\nX\n\nZ The above forms the basis for conventional DAG-structure learning [33]. In particular, X |\nstrongly limits the possible DAGs that model P . Given more independence statements, we limit X\nthe potential DAGs further. However, independence tests are computationally expensive which is problematic as the number of potential DAGs increases super-exponentially in\n\n[34].\n\n⊥⊥\n\nY\n\nThis limitation strongly impacted the adoption of DAG-learning, until Zheng et al. [35] proposed NOTEARS which incorporates a differentiable metric to evaluate whether or not a discovered structure is a DAG [35, 36]. Using automatic differentiation, NOTEARS learns a DAG-structure in a much more efficient way than earlier methods based on conditional independence tests (CITs).\n\n|X |\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nPopulation of patients.\n\nDifferent hospitals.\n\nDifferent datasets.\n\nThe same DAGs.\n\n(cid:143)\n\n(cid:154) (cid:154)\n\nDifferent distributions.\n\n \n\nThese DAGs are transportable!\n\nFigure 1: Transportability in DAG discovery. Different patients go to different hospitals (left), yet we wish to infer a general structure (right) across hospitals. A structure can only be considered a discovery if it generalizes in distributions over the same domain. For example, the way blood pressure interacts with heart disease is the same for all humans and should be reflected in the structure.\n\nWhile NOTEARS makes DAG inference tractable, we recognise an important limitation in the approach: a discovered DAG does not generalise to equally factorisable distributions, i.e. NOTEARS is not transportable. While we explain why this is the case in Section 2.2 (and confirm it empirically in Section 4), we give a brief description of the problem below, helping us to state our contribution.\n\nTransportability. Consider Fig. 1, depicting two hospitals: (cid:154) and (cid:154), named hospitals A and B onward. Each hospital hosts patients described by the same set of features such as age and gender. However, the hospitals may have different patient-distributions, e.g. patients in A are older compared to B. But their underlying biology remains the same. Using NOTEARS to learn a DAG from data on hospital A, actually does not guarantee the same DAG is discovered from data in hospital B.\n\nLearning from multiple data-sources is not new. In particular, papers focusing on federated structure learning solve a similar objective as described above [37, 38]. However, we believe transportability is a more general property than only training from multiple data sources. Crucially, transportability is very explicit about the domains we learn from, allowing their distributions to vary across domains.\n\nInterestingly, despite computational limitations, transportability is actually guaranteed when using a CIT-based discovery method [39, 40], assuming that patients in both hospitals exhibit the same (in)dependencies in . Being unable to transport findings across distributions is a major shortcoming, as replicating a found discovery is considered a hallmark of the scientific method [41–44].\n\nX\n\nContributions. In this paper, we present D-Struct, the first transportable differentiable structure learner. Transportability grants D-Struct several advantages over the state-of-the art: D-Struct is more robust, even in the conventional single-dataset case (Table 1); D-Struct is fast, in fact, we report time-to-convergence often up to 20 times faster than NOTEARS (Fig. 5); and given its completely differentiable architecture, D-Struct is easily incorporated in existing architectures (e.g. [45–49]).\n\nWhile transportable methods have clear benefits over non-transportable methods in settings with multiple datasets (as illustrated in Fig. 1), we emphasise that our method is not limited to these settings alone. In fact, we find that enforcing transportability significantly increases performance in settings with one dataset, which is arguably most common. In Section 3 we introduce D-Struct and how to use our ideas in the single dataset setting. We then empirically validate D-Struct in Section 4.\n\n2 PRELIMINARIES AND RELATED WORK\n\nOur goal is to build a transportable and differentiable DAG learner. Without loss of generality, we focus our discussion mostly on NOTEARS [35] (and refinements [36, 50–52]) as it is the most adopted differentiable DAG learner. For a more in depth overview of structure learners (CIT-based as well as score-based), we refer to Appendix G or relevant literature [26, 28, 34]. First, we formally introduce transportability, and then explain how NOTEARS works and why it is not transportable.\n\n2.1 TRANSPORTABILITY\n\nFactorisation and independence. Consider a distribution, P\n\n(cid:89)\n\ni\n\nP\n\ni X\n\ni+1:d ,\n\n|X\n\n, which we can factorise into,\n\nX\n\n(1)\n\nwith i long with increasing d as the conditions may contain up to d\n\n[d], where [d] := 1, . . . , d, and\n\ni representing the ith element in\n\n. Eq. (1) may get quite 1 different variables. This becomes\n\nX\n\nX\n\n∈\n\n−\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nrestrictive when estimating and decomposing ˆP independence statements, e.g. version of eq. (1) translates into a smaller Markov blanket and boundary [53] (see Appendix D).\n\nfrom data. Instead, we can simplify eq. (1) using j,k = P j . A simplified\n\nk invokes the equality: P\n\n⊥⊥ X\n\ni X\n\n|X\n\n|X\n\nX\n\nX\n\nX\n\ni\n\ni\n\nNon-symmetrical statements. Contrasting the above, we are interested in directed and acyclic is a set of edges graphical (DAG) structures. Let i) [54]. connecting nodes in\n\n:= (i.e. random variables), with (\n\nbe a DAG, where\n\nE ⊂ X × X\n\nimplying (\n\n{X\n\nGX\n\nE}\n\nj)\n\nj,\n\ni,\n\n,\n\nX\n\nX\n\nX\n\n∈ E\n\nX\n\nX\n\n̸∈ E\n\nWhile independence is symmetric, it is still possible to infer non-symmetric structures with only independence statements using d-separation [28, 30, 55–58]. Given a collection of conditional independence statements, which renders two variables independent given a third, e.g. X Z, d-separation (defined in Def. 4 in Appendix D) helps us narrow down a directed structure from independence statements [59, 60]1. If a set , they are d-separated, .\nd-sep\n\nB d), where blocking is directly related to independence between\n\nd blocks all paths between\n\nand\n\nand\n\n⊥⊥\n\nA\n\nX\n\nY\n\n|\n\n; (\nA\n\nG\n\nB|X\n\nA\n\nB\n\nGX\n\nand P\n\n. Specifically, the conditional independence relationships in\n\nWith d-separation and the common faithfulness assumption (see Appendix G), we have a link behave a one-to-one tween correspondence with those in P denotes independence in . The reverse is not necessarily true as there can be many graphs that correspond with P in terms of (in)dependence— these graphs are termed Markov equivalent [26].\n\nZ then X |\n\nZ, where |\n\n[61], i.e. if X\n\n⊥⊥ S\n\n⊥⊥ G\n\nP Y\n\nGX\n\n⊥⊥\n\nS\n\nY\n\nX\n\nX\n\nThe set of conditional independence assertions in P is denoted as dence statements implied by d-separation in a graph d-sep\n\nG , referred to as the set of global Markov independencies [26, Chapter 3].\n\n(P). Similarly, all independ) : (\nX ⊥⊥ B|X\n\nare denoted as\n\n) =\n\nG\n\nI\n\nI\n\n{\n\n(\n\n; (\nA\n\nG\n\nd)\n\nB|X\n\n}\n\n{\n\n}\n\n∈\n\nX\n\n: n\n\nand\n\n[N ]\n\n[M ]\n\n∈ X\n\nX (m)\n\nInvariance and discovery. Consider two datasets:\n\n{ ∈\nvariables as a sample X (m) from example, if hospital A collected a dataset on its patients in cancer, then if this statement is true this should also be found in data collected by hospital B (\n\nX (n) D2 = : n D1 = . As a sample X (n) from D1 depicts the same D2, both datasets reflect the same underlying mechanisms. For D1) and associated smoking with D2).\n\n, spanning the same space\n\n∈ X\n\nD1 and and P2\n\nD2 come from the same domain\n\n, they may be sampled Of course, while the samples in from different distributions, P1 , respectively. Recall Fig. 1, where hospitals A and B may be located in different regions, resulting in the characteristics of patients to also be different. However, key in a scientific discovery is that it generalises beyond distributions and is carried over through the entire domain almost all distributions Pi (P [26, Theorem 3.5]; if this is not the case, we haven’t discovered anything at all.\n\n. In other words, any structure we may find in (Pi\n\nD1, should also be found in\n\nD2, as for = P\n\nthat factorise over\n\n) where Pi\n\nX ∈ P\n\n(say\n\n) =\n\n) =\n\nX ̸\n\n2,\n\nX\n\nX\n\nX\n\nG\n\nG\n\nI\n\nI\n\nI\n\n}\n\nX\n\nX\n\nX\n\nX\n\nX\n\n(\n\nj\n\nj\n\nDefinition 1 (Transportability). With multiple datasets, domain call a method transportable if it learns a structure that is the same across all datasets: k\n\n{D , sampled from potentially different distributions Pi\n\n[K] = j for all i, j\n\n∼ = P\n\n[K]\n\n: k\n\ns.t.\n\nif i\n\nX ̸\n\n=\n\nX\n\n∈\n\nX j\n\n}\n\nX\n\nover the same [K], we k :\n\n∈ k\n{D\n\n→ G\n\nk\n\nPk\n\n∈\n\n}\n\nG1 =\n\n· · ·\n\nK.\n\nG\n\nD1 is also found in\n\nDef. 1 states that when a DAG found in method proposing it, transportable. Transportability in CIT-based methods is only satisfied when we assume that each distribution contains the same global Markov independencies. This assumption , and is not strict at all, as we are only concerned with distributions that span the same domain thus inherit the same interactions as posed by , i.e. the discoveries to be made. As such, CIT-based methods learn transportable DAGs automatically as transportability is a property directly related to the set of independencies of both distributions and DAGs, which we assume to be the same.\n\nD2, we consider that DAG, and the\n\nX\n\nX\n\nFrom the above, it is clear that transportability is a property of the structure learning method itself. In the case of CIT-based methods, we are guaranteed transportability in our setting, but not so for differentiable structure learnings. However, given that the method is responsible for transportability, we can come up with ways to include this property in differentiable structure learners also.\n\n1Note that in these earlier works, DAGs were named influence diagrams. 2For all distributions except for a set of measure zero in the space of conditional probability distribution\n\nparameterizations [39].\n\n3\n\n̸ Under review as a conference paper at ICLR 2023\n\nSubset Construction (Section 3.2)\n\nP\n\n{\n\n} ∼ P\n\nD Forward pass Backward pass Composes\n\n1 D\n\n2 D\n\n.\n\n.\n\n.\n\nk D\n\n1 G\n\n2 G\n\nk G\n\nL\n\n×\n\n×\n\n×\n\nDSF(\n\ni)\n\nG\n\nMSE(\n\n1, . . . ,\n\nG\n\nk)\n\nG\n\nL\n\nG\n\nTransportable Structure Learning (Section 3.1)\n\nFigure 2: D-Struct architecture. Above architecture enforces transportability. D-Struct is split in two major parts: subset construction (Section 3.2) and the transportable structure learning algorithm MSE. The losses are combined and ,\n(Section 3.1). There are three components: P\n. backpropagated through the architecture. Lastly, all DSFs are merged into a final DAG structure\n\nDSF, and\n\nL\n\nL\n\nG\n\n2.2 DIFFERENTIABLE STRUCTURE LEARNING\n\nX\n\nX\n\nI\n\nG\n\nusing\n\n( G ∈\n\n), where G\n\nwith respect to its factorisation over CIT-based methods evaluate each possible DAG that spans . The P\nmajor issue with this is computation. Essentially, there are two aspects which negatively impact increases super-exponentially computation time: first, the number of to-be-evaluated DAGs in G 1018 possible DAGs [34, 62, 63]); second, simply recovering in requires many independence tests, each with additional compute.\n\n(e.g. 10 variables result in > 4 ) to evaluate each\n\ndenotes the space of all possible DAGs in the domain\n\nI Appendix G includes an overview of the most well known CIT-based (and score-based) methods.\n\n|X | (P X\n\nG ∈\n\nG\n\n×\n\nX\n\nX\n\nX\n\nX\n\nX\n\nDifferentiable score functions. Enter differentiable score functions (DSFs). With DSFs one can traverse G smartly to arrive at a DAG [64, 65] much faster [63]. Furthermore, a differentiable method is straightforward to include in a variety of differentiable architectures, allowing joint optimisation of both the graphical structure as well as the accompanying structural equations or another downstream use.\n\nX\n\nMost notable is NOTEARS by Zheng et al. [35], which proposes the following learning objective:\n\nmin A\n\n∈A\n\nF (A) + λ1∥\n\nA\n\n∥1 +\n\nρ 2 |\n\nh(A) |\n\n2 + λ2h(A),\n\n(2)\n\nd (transformed to binary with a threshold) denotes an adjacency matrix; F (A) is a where A likelihood based loss (like the MSE); ρ and λ1,2 are parameters of the augmented Lagrangian; and\n\nRd\n\n∈\n\n×\n\nh(A) := tr(exp(A\n\nA))\n\nd,\n\n(3)\n\n◦ ) is the matrix trace operator, and is the actual differentiable score function, where tr( is the ·\nelement-wise (Hadamard) product. Importantly, h(A) = 0 indicates A is a DAG. Considering that eq. (3) is differentiable, we can take its derivative with respect to A and minimise eqs. (2) and (3).\n\n−\n\n◦\n\nAs is always the case for gradient-based learning, the random initialisation of A may guide the optimisation in different directions, potentially arriving at a local minimum in the case of non-convex targets. The latter is certainly the case in more recent improvements of NOTEARS as they almost exclusively focus on non-linear structural equations which result in non-convex losses [36, 51, 52].\n\nTransportability of DSFs. Current DSFs are not transportable, due to eq. (2) having conflicting . Essentially, the approxisolutions— contrasting the single solution (set) that transforms mate nature of (stochastic) gradient-based learning can result in conflicting estimate structures given different datasets. In fact, other works recognise this [66]. Rather than trying to recover one true DAG, their objective is to learn multiple (potentially different DAGs) through a multi-task objective. In our work, we assume there does exist one (unique) DAG, which is the one we aspire to recover.\n\n(P) to\n\nG\n\nI\n\n3 D-STRUCT: DIFFERENTIABLE AND TRANSPORTABLE StructURE LEARNING\n\nThe goal of any structure learner (differentiable or not) is to transform finite data into a structure:\n\n,\n\nD → G\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nSample according to\n\nResults in\n\n(a) We are presented with a dataset D over the domain X .\n\n(b) With two distributions P X , we can sample from D. and P\n\n2\n\n1 X\n\n(c) Sampling according to the two distributions results in two subsets D1 ∪ D2 = D.\n\nFigure 3: Differently distributed single-origin data. On the left we illustrate a single-origin dataset , sampled from one distribution. In the middle we illustrate two distributions over the domain of , thereby creating a new multi-origin datasource. , which are used to resample two subsets from\n\nD D\n\nD\n\nand so is the case for D-Struct. We introduce D-Struct in Section 3.1 and provide implementation details using NOTEARS in Section 3.3. While transportability seems most natural in a setting with multiple datasets (e.g. data from two hospitals illustrated in Fig. 1), we show in Section 3.2 how one can leverage transportability in the setting where there is only one dataset. In fact, our experiments in Section 4 show that D-Struct consistently offers more accurate DAGs in this “single-origin” setting.\n\n3.1 D-STRUCT: TRANSPORTABLE STRUCTURE LEARNING\n\nIn order to enforce transportability, D-Struct employs an ensemble-architecture of multiple initialisations of a chosen DSF and their appropriate (differentiable) architecture. Each loss is then combined with a regularisation function based on the D-Struct architecture. Fig. 2 depicts this architecture and highlights how our regularisation scheme is backpropagated throughout the entire network.\n\nD\n\nD1, . . . ,\n\nGiven datasets K, we can use any DSF (e.g. [35, 36, 50–52]) to learn a DAG. Specifically, we let K distinct DSFs learn a DAG from one of the K datasets, agnostic from each other. We consider these learning objectives to be K parallel objectives, as is also illustrated in Fig. 2 in the rightmost part. Crucially, D-Struct does not restrict which type of DSF we can use. In a linear setting one can use vanilla NOTEARS [35], whereas in a non-linear setting, one can use the non-parametric version [36]. Naturally, any restriction posed by the chosen DSF will carryover to D-Struct. We use NOTEARS-MLP [36] in Sections 3.3 and 4, while Appendix A includes pairings with other DSFs.\n\nDSF(\n\nk), which depends on the chosen DSF (illustrated At this point, we identify a first loss term: L\nk) corresponds with eqs. (2) and (3). Whenever in red in Fig. 2). In the case of NOTEARS, data is passed through the architecture– without mixing distinct datasets –we evaluate the discovered structure as . If the chosen DSF requires hyperparameters (such as λ1,2 and ρ in eq. (4)), we have to also include these in D-Struct’s set of required hyperparameters. While it is possible to set different hyperparameter values for each of the DSFs separately (which is potentially helpful when there is a lot of variety in the K distinct datasets), we fix these across DSFs in light of simplicity. A discussion on D-Struct’s hyperparameters can be found in Appendix A.1.\n\nk), where X\n\nG DSF(\n\nDSF(\n\n⊆ D\n\n∼ D\n\nX\n\nL\n\nL\n\nG\n\nG\n\nk\n\n|\n\nG\n\n∈\n\n[K]\n\nGiven\n\nk) : k\n\n{L G\n\nwe enforce transportability across each\n\nDSF( k. We do this by calculating the difference of the adjacency matrices Ak\n\nk by comparing the structures d. SpecifiRd G1, . . . , cally, for each gradient calculation (before we perform a backward pass), we take the (element-wise) mean adjacency matrix, ̄A1:K = 1 k Ak, detach it from the gradient and backpropagate the MSE for each parallel DSF. In particular, we include the following regularisation term in D-Struct’s loss:\n\n(cid:80)\n\nD\n\n∈\n\n}\n\nK\n\n×\n\n∥ Minimising eq. (4) results in transportable structures (see Theorem 1). Note that eq. (4) (green in k) to the DSF loss, Fig. 2) remains differentiable, which was our goal for D-Struct. We add\n\nMSE(\n\n−\n\nL\n\nMSE(Ak) :=\n\nAk\n\n ̄A1:K\n\n2.\n\n2 ∥\n\n(4)\n\nG ) indicates the adjacency matrix of\n\n|D\n\nL\n\nk\n\nk) :=\n\nDSF(\n\nwhere A( , and α is a scalar hyperparameter (refer to ApG pendix A.1 for hyperparameter settings, details, and further insights). Note that the second term in MSE be agnostic to the data makes sense as transportability eq. (5) does not depend on\n\nk. Having\n\nG\n\nk) + α\n\nMSE(A(\n\nk)),\n\nG\n\nL\n\n(5)\n\nG|D\n\nL\n\nG\n\n( L\n\nD\n\nL\n\n5\n\nDP1XP2XD1D2Under review as a conference paper at ICLR 2023\n\nis not a property of the data. Indeed, recall from Section 2.1 that transportability is a property of the structure learner instead. This additional term (based solely on the parallel learners) enforces transportability as the architecture encourages the DSFs to converge to the same adjacency matrix.\n\nTheorem 1 (Minimising eq. (4) yields transportable structures.). The only way in which eq. (4) is equal to 0— for every adjacency matrix Ak —is when A1 = = AK. Even a slight difference in one of the Ak will result in a non-zero equation 4 as ̄A1:K will be affected by this difference. Having every A1 = k— where each Ak is learned from a distinct\n\nk —corresponds with transportable structures as we have defined in Def. 1\n\n= AK and thus equal structures in\n\n· · ·\n\n· · ·\n\nG\n\nD\n\n3.2 D-STRUCT: SUBSET CONSTRUCTION\n\nSo far, we have only discussed the rightmost part of Fig. 2. In Section 3.1, we assumed data is provided in multiple distinct datasets, i.e. they stem from a multiorigin datasource. However, here we explain how even in the single-origin case D-Struct is applicable, irrespective of which DSF we end up choosing. As such, we continue our discussion by explaining the leftmost part of Fig. 2. Pk, this step Naturally, if one already has distinct can be skipped and D-Struct can be used as in Section 3.1.\n\n∼\n\nD\n\nk\n\nFigure 4: K distributions. We have ilDifferent distributions will guide each (distinct) optimilustrated the subset sampling with betasation target into a different direction. Combining their distributions above, for K = 3. For results, will encourage the total model to be more roeach density, we evaluate its PDF for bust and generalisable. However, while a multi-origin each index, normalize it and perform a datasource may be governed by multiple distributions, a Bernoulli experiment. The selected insingle-origin one is not. Our task is clear: from a singledices are plotted below the density funcorigin datasource, we have to mimic a multi-origin datations (black indicates a selected index). source in such a way that we know each subset has a different distribution, yet maintains the properties of the original single-origin-distribution (such as the global Markov independencies). Doing so allows us to enforce transportability through eq. (4).\n\nk\n\nX\n\n}\n\nD\n\nD\n\nD\n\nD\n\nD\n\n:=\n\nX }\n\n∈ P\n\nk =\n\nk D\n\n∈ D\n\nD , and\n\nP1 {\n\n× D → D\n\n, . . . , Pk\n\nusing Pk\n\nk where (cid:83)\n\nwith their values in\n\n∈ P , for each Pk\n\n, according to different distributions\n\nk need not be disjoint but is not equal to\n\nP . In principle, each element X (n)\n\nThe lefthand side of Fig. 2 shows that we handle the single-origin case by transforming it to a multi-origin case. With this constructed multi-origin setup, we can continue using D-Struct as we into subsets have done in Section 3.1. We preface the multi-origin case with a step that divides D\n. Consider Fig. 3 where has a . As such, each distribution leads to a\n\n{D1, . . . , we illustrate how we sample from Pk(X (n)) probability to be sampled from subset Pk In our experiments, we perform this preprocessing step by first correlating the index of each element . Next, we define K distributions over [N ] and then use these distributions in to sample indices. The sampled indices compose the subset. While we have included a detailed description of our implementation in Appendix F, we give a brief step-wise explanation below. ◆ Step 1 Correlating indices and values. covariates in i.e. the order of X’s in the data structure representing ◆ Step 2 Distributions over [N ]. Step 1 allows us to create subsets based on one-dimensional Pk . An added distributions {\n. Of bonus to these one-dimensional distributions is that they easily scale to more dimensions in course, the number of distributions, and consequentially their shape, should change in function of K. Specifically, with higher K, we have to ensure that the probability mass of each distribution is concentrated in different areas of [N ]. As such, we chose to model these as beta-distributions with,\n\naccording to the [N ], where i < j indicates X (i) < X (j),\n\n, rather than much more complicated distributions over\n\n∈ is correlated with the values of the X’s.\n\nSorting and reindexing elements in\n\nensures a dependency between\n\n[N ] : k\n\nand i\n\n[K]\n\nD\n\nD\n\nD\n\nX\n\nX\n\nX\n\nX\n\nX\n\n∈\n\n}\n\n.\n\nα, β\n\n(i, K), (K, K), (K, j) : i\n\n∈ {\n\ninterp(1, K\n\n1), j\n\n−\n\n∈\n\n∈\n\ninterp(K\n\n1, 1)\n\n, }\n\n−\n\nwhere interp(a, b) is a linear interpolation between a and b, used to sample K is even we leave out (K, K) such that the total number of distributions always equals K. ◆ Step 3 Selecting indices.\n\nOur final task is to create K subsets, which due to the first step is\n\ni’s and j’s. When\n\n⌊\n\nK 2 ⌋\n\n6\n\nP1P2P3Selectedindices:P1P2P312345678910Under review as a conference paper at ICLR 2023\n\nsimplified to choosing indices instead. These indices are selected based on the distributions defined in the second step. First, we evaluate each density’s PDF for every index (after normalisation: N ), and normalise the output to be a value between 0 and 1. Once we have K values for each index, we perform a Bernoulli experiment where the output determines whether or not the index is selected to [K]. This process is illustrated in Fig. 4 for K = 3 using beta distributions. be part of the subset k\n\ni\n\nWith each subset, we apply D-Struct as explained in Section 3.1. In our experiments (Section 4 and Appendix A), we show that D-Struct greatly improves performance of non-transportable DSFs. Furthermore, we also empirically validate our subsampling routing compared to random sampling.\n\n∈\n\n3.3 EXAMPLE IMPLEMENTATION USING NOTEARS-MLP\n\nD-Struct works with any DSF, though it is instructive to illustrate this with an example. For this, we chose NOTEARS-MLP [36] which is a non-parametric (cfr. the structural equations) extension of the classic NOTEARS paper [35]. The main challenge to incorporating D-Struct into NOTEARS-MLP is to integrate it into its dual ascent strategy which solves the (non-convex) constrained optimisation problem in eq. (2) [50], solved with an augmented Lagrangian method [67, Chapter 5].\n\nThe constraint in the optimisation problem stems from, for example, knowing that the diagonal of A can only contain zeros [35, 36, 50]. NOTEARS (and its extensions), solve this problem by using the L-BFGS-B optimizer [68] which can handle parameter bounds out-of-the-box, making it a suitable choice to optimise the augmented Lagrangian3. This is made explicit in Algs. 1 and 2.\n\n[K]\n\nInit.: θk for each k Input: htol, ρmax Setup: h ←\nfor maximum amount of epochs do\n\n, ρ1,...,K\n\n← ∞\n\n1, ρ\n\n←\n\n∈\n\n1\n\nfor k\n\n[K] do\n\n∈\n\nfor batch\n\nk do\n\n∼ D\n\ntraining step(θk,\n\nbatch);\n\nh ρ\n\n← ←\n\nmaxk h(A(θk)); mink ρk;\n\nInput: θk, batch while ρ < ρmax do\n\nMSE(θ1, . . . , θK); DSF(batch);\n\nlm ld θ\n\n← L ← L ←\n\nL-BFGS-B.update(lm, ld);\n\nh(A(θk));\n\nh′ if h′ > 0.25h then\n\n← ρk\n\n10ρk;\n\nelse\n\n← break;\n\nAlgorithm 1: Outer-loop of dual ascent procedure for D-Struct(NOTEARS-MLP)\n\nAlgorithm 2: training step for D-Struct(NOTEARS-MLP)\n\nAlgorithms 1 and 2 highlight where D-Struct requires edits in the standard NOTEARS-MLP al- [K], where each gorithm. Most obvious is the creation of multiple parameters, θk for each k θk indicates the set of parameters for one initialisation of NOTEARS-MLP, exactly following the architecture depicted in Fig. 2. The set then denotes the parameters for D-Struct. As ∈\nsuch, the number of parameters for D-Struct scales linearly in K, compared to the used DSFs.\n\nθk : k\n\n[K]\n\n∈\n\n}\n\n{\n\nFrom Algs. 1 and 2, we learn that information across the different NOTEARS-MLPs is shared in training step (corresponding with Alg. 2). Typically, a training step is solely focused on one structure learner leaving the learner unaware of the other DSFs, as is also implied in Alg. 1 which iterates over each learner separately. The sharing of information across each learner though— through MSE(θ1, . . . , θk) computed in the first line in Alg. 2’s while loop —is enforcing transportability.\n\nL D-Struct hardly increases implementation complexity. In fact, besides architectural alterations (as explained in Section 3.1 and Fig. 2), the optimisation strategy is mostly adopted from the underlying DSF. This is an important advantage. In fact, Zheng et al. [35] already state the importance of an easy to implement model; they use approximately 60 lines of code, and we only add approximately 10 lines to that. Furthermore, we also noticed a major improvement in efficiency as D-Struct drastically reduces computation time compared to NOTEARS. We report computation time in the next section.\n\n3This also allows including prior knowledge on I(P). We discuss this in more detail in Appendix E.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Results on Erdos-Reny`ı (ER) graphs. First block: We sample ten different ER random graphs, and accompanying non-linear structural equations as in Zheng et al. [36]. From each system we then sample a varying number of samples, and evaluate NOTEARS-MLP with D-Struct (indicated as “✓”) and without D-Struct (indicated as “✗”). Second block: For each row we sample a new ER graph with a varying degree of connectedness (s indicates the expected number of edges). In both cases, we report the average performance in terms of SHD, FPR, TPR, and FDR, with std in scriptsize. Unless otherwise indicated, n = 1000, d = 5, K = 3, s = 2d.\n\nmetric\n\nD-Struct\n\nSHD (\n\n↓\n\n)\n\n✗\n\n✓\n\nFPR (\n\n) ↓\n\n✗\n\n✓\n\nTPR (\n\n) ↑\n\n✗\n\n✓\n\nFDR (\n\n) ↓\n\n✗\n\n✓\n\nn\n\n200 500 1000 2000\n\ns\n\n0.5d 1d 1.5d 2d\n\nd\n\n5 7\n10\n\n3.60 3.20 2.75 2.66\n\n±\n\n±\n\n±\n\n±\n\n0.27\n\n0.80\n\n0.47\n\n0.80\n\n3.75 3.50 3.00 2.28\n\n1.6\n\n0.86\n\n1.15\n\n0.80\n\n±\n\n±\n\n±\n\n±\n\nvarying sample size\n\n4.20 3.94 2.67 2.09\n\n±\n\n±\n\n±\n\n±\n\n0.44\n\n0.33\n\n0.63\n\n0.31\n\n0.67 0.66 0.75 0.81\n\n±\n\n±\n\n±\n\n±\n\n0.05\n\n0.24\n\n0.08\n\n0.11\n\n2.00 1.20 1.00 1.88\n\n0.67\n\n0.44\n\n0.37\n\n0.67\n\n±\n\n±\n\n±\n\n±\n\n0.64 0.56 0.63 0.75\n\n0.05\n\n0.04\n\n0.13\n\n0.03\n\n±\n\n±\n\n±\n\n±\n\n0.25 0.18 0.18 0.27\n\n0.06\n\n0.05\n\n0.03\n\n0.07\n\n±\n\n±\n\n±\n\n±\n\n4.20 3.94 3.67 3.54\n\n0.44\n\n0.33\n\n0.82\n\n0.16\n\n±\n\n±\n\n±\n\n±\n\n7.33 7.67 5.67 3.67\n\n0.13\n\n0.45\n\n1.75\n\n0.82\n\n±\n\n±\n\n±\n\n±\n\nvarying graph connectedness\n\n0.50 0.55 1.00 1.00\n\n0.25\n\n0.22\n\n0.19\n\n0.32\n\n±\n\n±\n\n±\n\n±\n\n1.05 1.53 1.55 2.67\n\n±\n\n±\n\n±\n\n±\n\n0.02\n\n0.09\n\n0.08\n\n0.63\n\n0.83 0.75 0.89 0.67\n\n±\n\n±\n\n±\n\n±\n\n0.19\n\n0.09\n\n0.07\n\n0.17\n\n0.88 0.46 0.62 0.70\n\n0.04\n\n0.09\n\n0.06\n\n0.09\n\n±\n\n±\n\n±\n\n±\n\n0.42 0.40 0.32 0.11\n\n0.16\n\n0.09\n\n0.05\n\n0.03\n\n±\n\n±\n\n±\n\n±\n\nvarying dimension count\n\n±\n\n2.28 8.67 ±\n19.71\n\n0.80\n\n0.56\n\n3.67 12.9 0.72 30.8\n\n±\n\n0.82\n\n0.15\n\n0.98\n\n±\n\n±\n\n±\n\n1.00 0.72 0.42\n\n0.32\n\n0.05\n\n0.13\n\n±\n\n±\n\n±\n\n2.67 1.07 1.18\n\n±\n\n±\n\n±\n\n0.63\n\n0.01\n\n0.04\n\n0.67 0.96 0.70\n\n±\n\n±\n\n±\n\n0.17\n\n0.02\n\n0.16\n\n0.70 0.83 0.71\n\n0.09\n\n0.01\n\n0.06\n\n±\n\n±\n\n±\n\n0.11 0.49 0.34\n\n0.03\n\n0.01\n\n0.08\n\n±\n\n±\n\n±\n\n0.42 0.44 0.39 0.33\n\n±\n\n±\n\n±\n\n±\n\n0.73 0.77 0.53 0.32\n\n±\n\n±\n\n±\n\n±\n\n0.32 0.63 0.70\n\n±\n\n±\n\n±\n\n0.04\n\n0.04\n\n0.11\n\n0.00\n\n0.01\n\n0.07\n\n0.04\n\n0.08\n\n0.08\n\n0.01\n\n0.02\n\n4 EXPERIMENTS\n\nRecall from Section 3 that D-Struct’s objective is exactly the same as any differentiable structure learner: transform a dataset into a DAG, whilst remaining differentiable. With D-Struct, our aim is to increase performance of any DSF by enforcing transportability on the learner’s outcome structure. As such, the most pressing questions are: (1) Are the discovered structures transportable?, (2) Does D-Struct improve existing learners?, (3) Does D-Struct give up on efficiency?, and (4) Do we really need our subsampling routine? We answer these questions one-by-one below with experiments.\n\n(1) Transportability. Before testing accuracy, we first empirically confirm that NOTEARS is not transportable while D-Struct is. We compare NOTEARS with D-Struct using 1000 samples drawn from an Erdos-Reny`ı (ER) random graph, and split the samples into two equal sized subsets. We evaluate the structural Hamming distance (SHD) between the graphs learned by NOTEARS on each dataset, and the same for the internal graphs learned by D-Struct. The DAGs learnt by D0.27), with only minor Struct are perfectly transportable (SHD= 0) in 8/10 runs (mean SHD 0.46 0.20, only discrepancies in the other cases. Conversely, NOTEARS has a mean SHD of 1.14 displaying transportability in 2 cases. Similar results for other DSFs are reported in Appendix A.\n\n±\n\n±\n\n(2) Accuracy. The most straightforward way to see if D-Struct is better is by repeating the experiments in Zheng et al. [36]. We report only a subset of our outcomes in the main text, mainly on D-Struct’s improvement over NOTEARS-MLP. However, more metrics and experiments on different DSFs can be found in Appendix A. In Table 1 we report the false positive rate (FPR), true positive rate (TPR), false discovery rate (FDR), and structural Hamming distance (SHD) of the estimated DAGs using data sampled from different ER random graphs with varying sample size (n), expected number of edges (s), and dimension count (d). In all cases we find that D-Struct significantly improves NOTEARS-MLP (other DSFs in Appendix A). A similar conclusion can be drawn from Fig. 6, where we report the SHD for more parameters and data from Erdos-Reny`ı as well as Scale Free graphs [35].\n\n8\n\n25x 20x 15x 10x 5x\n\np u\nd e\ne p\nS\n\n200 500\n\n1k\n\n2k\n\nNumber of samples (n)\n\nFigure 5: Speedup of D-Struct over NOTEARS. Difference in computation time between NOTEARS-MLP and DStruct, as a function of n. On average, D-Struct is about 10x quicker.\n\nUnder review as a conference paper at ICLR 2023\n\nvarying n\n\nvarying s\n\nvarying d\n\nvarying K\n\n5\n\n4\n\n3\n\n2\n\n8\n\n6\n\n4\n\n2\n\nD H\nS\n\nD H\nS\n\n10\n\n8\n\n6\n\n4\n\n2\n\n200 500\n\n1,000 n\n\n2,000\n\n0.5\n\n1\n\n1.5\n\nsd\n\n2\n\n10\n\n8\n\n6\n\n4\n\n2\n\n200 500\n\n1,000 n\n\n2,000\n\n0.5\n\n1\n\n1.5\n\nsd\n\n2\n\n30\n\n20\n\n10\n\n0\n\n30\n\n20\n\n10\n\n0\n\n5\n\n5\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n10\n\n8\n\n6\n\n4\n\n2\n\n0\n\n2\n\n3\n\nK\n\n2\n\n3\n\nK\n\n7\n\nd\n\n10\n\n7\n\nd\n\n10\n\nE r\nd o\ns -\n\nR e\nn y\n`ı\n\nS c\na\n\nl e\n\nF r\ne e\n\n5\n\n5\n\nD-Struct(NOTEARS-MLP)\n\nNOTEARS-MLP\n\nFigure 6: Structure recovery. We report the SHD ( ) compared to the true graph. We report performance as a function of four different parameters (changing the properties of the task). The results demonstrate that D-Struct outperforms NOTEARS-MLP in all these settings. Additional results are reported in Appendix A. Unless otherwise indicated, n = 1000, d = 5, K = 3, s = 2d.\n\n↓\n\n(3) Computational efficiency. In Fig. 5 we learn that despite its parallel ensemble architecture, D-Struct is actually much faster than NOTEARS. Note that D-Struct is built on top of NOTEARS, meaning this computational gain is not due to differences in implementation. Instead, we believe computation gains are largely due to D-Struct’s learning scheme. Rather than using the entire dataset at once to learn one (computationally intensive) DSF, D-Struct splits the data and learns multiple DSFs from several smaller datasets. We believe this is an important result: the whole reason for having differentiable structure learners is due to their efficiency gains over CIT-based methods.\n\n(4) Subset construction. A final property we wish to validate is the need for sampling K different subsets using our subsampling routine from Section 3.2. This is an important validation as it shows that D-Struct does not only gain in performance due to its ensemble architecture. For this, we compare D-Struct’s performance with and without our subsampling routine. Using D-Struct without our subsampling routine amounts to providing K random Pk. splits, rather than carefully sampling K distinct Table 2 shows that our subsampling routine does improve D-Struct’s performance as expected, validating our goal to explicitly optimise for transportable structure learners.\n\n∼\n\nD\n\nk\n\nWe believe that these experiments offer a broad view of how D-Struct and transportability can help us create useful structure learners. However, as we were unable to include all our experimental results in the main text of this paper, we refer the interested reader to our Appendix A for completed and additional validation. In our appendices we also include an anonymous link to our code repository, encouraging readers to reproduce our results.\n\n5 DISCUSSION\n\nTable 2: Usefulness of our subsampling routine. We sample ten different ER random graphs like in Table 1. From each system we then sample n = 2000 samples, and evaluate NOTEARS-MLP with our subsampling routine from Section 3.2 (indicated as “✓”) and without the subsampling routine, using random splits instead (indicated as “✗”). For each row we repeat our experiment with different K. We report the average (and std) performance in terms of the SHD.\n\nmetric\n\nSubsample\n\nSHD (\n\n) ↓\n\n✗\n\n✓\n\nK varying amount of splits\n\n2 3\n5\n\n2.80 3.00 2.80\n\n±\n\n±\n\n±\n\n0.53\n\n0.37\n\n0.57\n\n3.40 4.00 4.40\n\n±\n\n±\n\n±\n\n0.58\n\n0.59\n\n1.29\n\nD-Struct advances differentiable structure learning by introducing transportability, a property guaranteed by CIT-based methods. We believe D-Struct can have a positive impact on architectures and problems relying on differentiable structure learners, as well as on general scientific data analysis.\n\nRelating DSFs to causality. As pointed out by Kaiser and Sipos [69] and Reisach et al. [70], DSFs are often wrongly used to recover a causal DAG. While DAGs are indeed the model of choice to describe causal interactions, there is currently no guarantee that a DAG discovered using any DSF can be identified (and thus used) as such. With this we wish to state explicitly that a DSF’s output is not to be interpreted as a causal model. We discuss more about this particular topic in Appendix B.\n\nFuture work. The inability to recover causal structure is a consequence of there existing many more useful properties stemming from a CIT-based approach (multiple books concern this very topic [26, 53, 71, 72]). Bridging the gap between these methods is a clear path forward, hopefully\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nincreasing differentiable structure learners’ potential even further. Specifically, using structure learners to uncover a causal structure from observational data requires stricter assumptions. As such, one particularly interesting avenue of future work is to allow DSFs (not only D-Struct) to adhere to some of these assumptions and use them to guarantee causal discovery, taking DSFs to a next level.\n\nFinally, D-Struct is only the first step in the process of scientific discovery. As D-Struct (or any other DSF) suggests a link between variables, it is still the scientist’s task to confirm this link in the lab.\n\nEthics Statement. We envisage D-Struct as a tool to help the scientific endeavour, however emphasise the discovered structures and links would need to be further verified by a human expert or in an experimental setting. Furthermore, the data used in this work is synthetically generated from given random graphical models, and no human-derived data was used.\n\nReproducibility Statement. To ensure reproducibility, we include experimental details in Appendix A.1. These details include: (1) hyperparameter settings, (2) evaluation metrics, (3) the synthetic data generation procedure, and (4) additional implementation details of D-Struct. Finally, all code is readily available at our anonymous online code repository: https://anonymous. 4open.science/r/d-struct. Beyond documentation and instructions, this code includes benchmark models, synthetic data generation, and our D-Struct implementation.\n\nREFERENCES\n\n[1] Rohan Bhardwaj, Ankita R Nambiar, and Debojyoti Dutta. A study of machine learning In 2017 IEEE 41st Annual Computer Software and Applications Conference\n\nin healthcare. (COMPSAC), volume 2, pages 236–241. IEEE, 2017.\n\n[2] Jeroen Berrevoets, James Jordon, Ioana Bica, Mihaela van der Schaar, et al. Organite: Optimal transplant donor organ offering using an individual treatment effect. Advances in neural information processing systems, 33:20037–20050, 2020.\n\n[3] Mihaela van der Schaar, Ahmed M Alaa, Andres Floto, Alexander Gimson, Stefan Scholtes, Angela Wood, Eoin McKinney, Daniel Jarrett, Pietro Lio, and Ari Ercole. How artificial intelligence and machine learning can help healthcare systems respond to covid-19. Machine Learning, 110(1):1–14, 2021.\n\n[4] Alvin Rajkomar, Jeffrey Dean, and Isaac Kohane. Machine learning in medicine. New England\n\nJournal of Medicine, 380(14):1347–1358, 2019.\n\n[5] Jeroen Berrevoets, Ahmed Alaa, Zhaozhi Qian, James Jordon, Alexander ES Gimson, and Mihaela Van Der Schaar. Learning queueing policies for organ transplantation allocation using interpretable counterfactual survival analysis. In International Conference on Machine Learning, pages 792–802. PMLR, 2021.\n\n[6] Susan Athey et al. The impact of machine learning on economics. The economics of artificial\n\nintelligence: An agenda, pages 507–547, 2018.\n\n[7] Susan Athey and Guido W Imbens. Machine learning methods that economists should know\n\nabout. Annual Review of Economics, 11:685–725, 2019.\n\n[8] Sendhil Mullainathan and Jann Spiess. Machine learning: an applied econometric approach.\n\nJournal of Economic Perspectives, 31(2):87–106, 2017.\n\n[9] Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, and Lenka Zdeborov ́a. Machine learning and the physical sciences. Reviews of Modern Physics, 91(4):045002, 2019.\n\n[10] Alexander Radovic, Mike Williams, David Rousseau, Michael Kagan, Daniele Bonacorsi, Alexander Himmel, Adam Aurisano, Kazuhiro Terao, and Taritree Wongjirad. Machine learning at the energy and intensity frontiers of particle physics. Nature, 560(7716):41–48, 2018.\n\n[11] Sankar Das Sarma, Dong-Ling Deng, and Lu-Ming Duan. Machine learning meets quantum\n\nphysics. arXiv preprint arXiv:1903.03516, 2019.\n\n[12] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422–440, 2021.\n\n[13] Philip G Breen, Christopher N Foley, Tjarda Boekholt, and Simon Portegies Zwart. Newton versus the machine: solving the chaotic three-body problem using deep neural networks. Monthly Notices of the Royal Astronomical Society, 494(2):2465–2470, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\n[14] Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for sym-\n\nbolic regression. Science Advances, 6(16):eaay2631, 2020.\n\n[15] Jan Reinhard Peters. Machine learning of motor skills for robotics. University of Southern\n\nCalifornia, 2007.\n\n[16] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 3803–3810. IEEE, 2018.\n\n[17] Ben Kehoe, Sachin Patil, Pieter Abbeel, and Ken Goldberg. A survey of research on cloud robotics and automation. IEEE Transactions on automation science and engineering, 12(2): 398–409, 2015.\n\n[18] Pieter Abbeel, Adam Coates, and Andrew Y Ng. Autonomous helicopter aerobatics through apprenticeship learning. The International Journal of Robotics Research, 29(13):1608–1639, 2010.\n\n[19] Yanir Kleiman, Simon Pabst, and Patrick Nagle. Boosting vfx production with deep learning.\n\nIn ACM SIGGRAPH 2019 Talks, pages 1–2. 2019.\n\n[20] Dan Ring, Johanna Barbier, Guillaume Gales, Ben Kent, and Sebastian Lutz.\n\nat the deep end: how to experiment with machine learning in post-production software. Proceedings of the 2019 Digital Production Symposium, pages 1–5, 2019.\n\nJumping in In\n\n[21] Yi Wang. Film and television special effects production based on modern technology: from the perspective of statistical machine learning. In 2022 4th International Conference on Smart Systems and Inventive Technology (ICSSIT), pages 833–836. IEEE, 2022.\n\n[22] Alex Davies, Petar Veliˇckovi ́c, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomaˇsev, Richard Tanburn, Peter Battaglia, Charles Blundell, Andr ́as Juh ́asz, et al. Advancing mathematics by guiding human intuition with ai. Nature, 600(7887):70–74, 2021.\n\n[23] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\n[24] Kathryn Tunyasuvunakool, Jonas Adler, Zachary Wu, Tim Green, Michal Zielinski, Augustin ˇZ ́ıdek, Alex Bridgland, Andrew Cowie, Clemens Meyer, Agata Laydon, et al. Highly accurate protein structure prediction for the human proteome. Nature, 596(7873):590–596, 2021.\n\n[25] Kiersten M. Ruff and Rohit V. Pappu. Alphafold and implications for intrinsically disorISSN 0022-2836. dered proteins. doi: https://doi.org/10.1016/j.jmb.2021.167208. URL https://www.sciencedirect. com/science/article/pii/S0022283621004411. From Protein Sequence to Structure at Warp Speed: How Alphafold Impacts Biology.\n\nJournal of Molecular Biology, 433(20):167208, 2021.\n\n[26] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.\n\nMIT press, 2009.\n\n[27] Sewall Wright. The method of path coefficients. The annals of mathematical statistics, 5(3):\n\n161–215, 1934.\n\n[28] Judea Pearl. Causality. Cambridge university press, 2009.\n\n[29] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on\n\ngraphical models. Frontiers in genetics, 10:524, 2019.\n\n[30] Dan Geiger and Judea Pearl. On the logic of causal models. Pattern Recognition, volume 9, pages 3–14. Elsevier, 1990.\n\nIn Machine Intelligence and\n\n[31] Christopher Meek. Strong completeness and faithfulness in bayesian networks. arXiv preprint\n\narXiv:1302.4973, 2013.\n\n[32] Frederick Eberhardt. Introduction to the foundations of causal discovery. International Journal\n\nof Data Science and Analytics, 3(2):81–91, 2017.\n\n[33] Max Chickering, Dan Geiger, and David Heckerman. Learning bayesian networks: Search methods and experimental results. In Proceedings of the fifth international workshop on artificial intelligence and statistics, 1995.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\n[34] Jonas Peters, Dominik Janzing, and Bernhard Sch ̈olkopf. Elements of causal inference: foun-\n\ndations and learning algorithms. The MIT Press, 2017.\n\n[35] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Continuous Optimization for Structure Learning. In Advances in Neural Information Processing Systems, 2018.\n\n[36] Xun Zheng, Chen Dan, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Learning sparse In International Conference on Artificial Intelligence and Statistics,\n\nnonparametric DAGs. 2020.\n\n[37] Erdun Gao, Junjia Chen, Li Shen, Tongliang Liu, Mingming Gong, and Howard Bondell.\n\nFederated causal discovery. arXiv preprint arXiv:2112.03555, 2021.\n\n[38] Ignavier Ng and Kun Zhang. Towards federated bayesian network structure learning with continuous optimization. In International Conference on Artificial Intelligence and Statistics, pages 8095–8111. PMLR, 2022.\n\n[39] Christopher Meek. Strong completeness and faithfulness in bayesian networks. In Proceedings\n\nof the Eleventh Conference on Uncertainty in Artificial Intelligence, 1995.\n\n[40] Ann Becker, Dan Geiger, and Christopher Meek. Perfect tree-like markovian distributions. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, 2000. URL https://arxiv.org/abs/1301.3834.\n\n[41] Monya Baker. 1,500 scientists lift the lid on reproducibility. Nature, 533(7604), 2016.\n\n[42] Colin F Camerer, Anna Dreber, Eskil Forsell, Teck-Hua Ho, J ̈urgen Huber, Magnus Johannesson, Michael Kirchler, Johan Almenberg, Adam Altmejd, Taizan Chan, et al. Evaluating replicability of laboratory experiments in economics. Science, 351(6280):1433–1436, 2016.\n\n[43] Robert K Merton. The sociology of science: Theoretical and empirical investigations. Univer-\n\nsity of Chicago press, 1973.\n\n[44] Victoria Stodden. The scientific method in practice: Reproducibility in the computational\n\nsciences. 2010.\n\n[45] Rohit Bhattacharya, Tushar Nagarajan, Daniel Malinsky, and Ilya Shpitser. Differentiable In International Conference on Artificial\n\ncausal discovery under unmeasured confounding. Intelligence and Statistics, pages 2314–2322. PMLR, 2021.\n\n[46] Trent Kyono, Yao Zhang, and Mihaela van der Schaar. Castle: Regularization via auxiliary causal graph discovery. Advances in Neural Information Processing Systems, 33:1501–1512, 2020.\n\n[47] Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Georgatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from timeseries data. In International Conference on Artificial Intelligence and Statistics, pages 1595– 1605. PMLR, 2020.\n\n[48] Trent Kyono, Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. Miracle: Causally-aware imputation via learning missing data mechanisms. Advances in Neural Information Processing Systems, 34, 2021.\n\n[49] Boris van Breugel, Trent Kyono, Jeroen Berrevoets, and Mihaela van der Schaar. Decaf: Generating fair synthetic data using causally-aware generative networks. Advances in Neural Information Processing Systems, 34, 2021.\n\n[50] S ́ebastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradientbased neural dag learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rklbKA4YDS.\n\n[51] Yue Yu, Tian Gao, Naiyu Yin, and Qiang Ji. Dags with no curl: An efficient dag structure learning approach. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12156–12166. PMLR, 18–24 Jul 2021. URL https://proceedings. mlr.press/v139/yu21a.html.\n\n[52] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings neural networks. of the 36th International Conference on Machine Learning, volume 97 of Proceedings of\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nMachine Learning Research, pages 7154–7163. PMLR, 09–15 Jun 2019. URL https: //proceedings.mlr.press/v97/yu19a.html.\n\n[53] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference.\n\nMorgan kaufmann, 1988.\n\n[54] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and\n\nvariational inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.\n\n[55] Judea Pearl. Fusion, propagation, and structuring in belief networks. Artificial intelligence, 29\n\n(3):241–288, 1986.\n\n[56] Thomas Verma and Judea Pearl. Causal networks: Semantics and expressiveness. In Machine\n\nintelligence and pattern recognition, volume 9, pages 69–76. Elsevier, 1990.\n\n[57] Dan Geiger, Thomas Verma, and Judea Pearl. d-separation: From theorems to algorithms. In Machine Intelligence and Pattern Recognition, volume 10, pages 139–148. Elsevier, 1990.\n\n[58] Dan Geiger, Thomas Verma, and Judea Pearl. Identifying independence in bayesian networks.\n\nNetworks, 20(5):507–534, 1990.\n\n[59] Ronald A Howard and James E Matheson. The principles and applications of decision analysis.\n\nStrategic Decisions Group, Palo Alto, CA, pages 719–762, 1984.\n\n[60] JQ Smith. Influence diagrams for statistical modeling. The Annals of Statistics, 1, 1989.\n\n[61] Dan Geiger and Judea Pearl. Logical and algorithmic properties of conditional independence\n\nand graphical models. The annals of statistics, 21(4):2001–2021, 1993.\n\n[62] Robert W Robinson. Counting unlabeled acyclic digraphs. In Combinatorial mathematics V,\n\npages 28–43. Springer, 1977.\n\n[63] Matthew J Vowels, Necati Cihan Camgoz, and Richard Bowden. D’ya like dags? a survey on\n\nstructure learning and causal discovery. ACM Computing Surveys (CSUR), 2021.\n\n[64] Ignavier Ng, S ́ebastien Lachapelle, Nan Rosemary Ke, Simon Lacoste-Julien, and Kun Zhang. On the convergence of continuous constrained optimization for structure learning. In International Conference on Artificial Intelligence and Statistics, pages 8176–8198. PMLR, 2022.\n\n[65] Dennis Wei, Tian Gao, and Yue Yu. Dags with no fears: A closer look at continuous optimization for learning bayesian networks. Advances in Neural Information Processing Systems, 33: 3895–3906, 2020.\n\n[66] Xinshi Chen, Haoran Sun, Caleb Ellington, Eric Xing, and Le Song. Multi-task learning of order-consistent causal graphs. Advances in Neural Information Processing Systems, 34: 11083–11095, 2021.\n\n[67] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientific, 3rd edition, 2016. ISBN\n\n978-1-886529-05-2.\n\n[68] Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on scientific computing, 16(5):1190–1208, 1995.\n\n[69] Marcus Kaiser and Maksim Sipos. Unsuitability of NOTEARS for causal graph discovery when dealing with dimensional quantities. Neural Processing Letters, pages 1–9, 2022.\n\n[70] Alexander G Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated dag!\n\nvarsortability in additive noise models. arXiv preprint arXiv:2102.13647, 2021.\n\n[71] Michael Irwin Jordan. Learning in graphical models. MIT press, 1999.\n\n[72] Steffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.\n\n[73] Albert-L ́aszl ́o Barab ́asi and R ́eka Albert. Emergence of scaling in random networks. science,\n\n286(5439):509–512, 1999.\n\n[74] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, predic-\n\ntion, and search. MIT press, 2000.\n\n[75] Thomas S. Verma and Judea Pearl. Equivalence and synthesis of causal models. In Proceedings\n\nof the Sixth Conference on Uncertainty in Artificial Intelligence, 1990.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n[76] Ignavier Ng, AmirEmad Ghassami, and Kun Zhang. On the role of sparsity and dag constraints for learning linear dags. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17943–17954. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/ 2020/file/d04d42cdf14579cd294e5079e0745411-Paper.pdf.\n\n[77] Eric Walter. Identifiability of parametric models. Elsevier, 2014.\n\n[78] AmirEmad Ghassami, Negar Kiyavash, Biwei Huang, and Kun Zhang. Multi-domain causal structure learning in linear systems. Advances in neural information processing systems, 31, 2018.\n\n[79] Jonas Peters, Peter B ̈uhlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 78(5):947–1012, 2016. ISSN 13697412, 14679868. URL http://www.jstor.org/stable/44682904.\n\n[80] Biwei Huang, Kun Zhang, Jiji Zhang, Joseph D Ramsey, Ruben Sanchez-Romero, Clark Glymour, and Bernhard Sch ̈olkopf. Causal discovery from heterogeneous/nonstationary data. J. Mach. Learn. Res., 21(89):1–53, 2020.\n\n[81] Jiji Zhang and Peter Spirtes. Detection of unfaithfulness and robust causal inference. Minds\n\nand Machines, 18(2):239–271, 2008.\n\n[82] Markus Kalisch and Peter B ̈uhlman. Estimating high-dimensional directed acyclic graphs with\n\nthe pc-algorithm. Journal of Machine Learning Research, 8(3), 2007.\n\n[83] Jiji Zhang and Peter L Spirtes. Strong faithfulness and uniform consistency in causal inference.\n\narXiv preprint arXiv:1212.2506, 2012.\n\n[84] Caroline Uhler, Garvesh Raskutti, Peter B ̈uhlmann, and Bin Yu. Geometry of the faithfulness\n\nassumption in causal inference. The Annals of Statistics, pages 436–463, 2013.\n\n[85] Dan Geiger and David Heckerman. Learning gaussian networks. In Uncertainty Proceedings\n\n1994, pages 235–243. Elsevier, 1994.\n\n[86] David Heckerman, Christopher Meek, and Gregory Cooper. A bayesian approach to causal\n\ndiscovery. In Innovations in Machine Learning, pages 1–28. Springer, 2006.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAppendix: D-Struct\n\nTable of Contents\n\nA Additional experiments .\n. A.1 Settings and details .\n. A.2 Completed results . .\n. A.3 Other DSFs .\n. A.4 Subsampling datasets .\n. A.5 DAGs: D-Struct vs NOTEARS . A.6 Gains from enforcing transportability .\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n.\n\n.\n\n.\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\nB Causal interpretation and uniqueness\n\nC Transportability in non-overlapping domains\n\nD Definitions\n\nE Incorporating prior knowledge on I(P) using L-BFGS-B\n\nF Additional details on subsampling from different distributions .\n. .\n\nF.1 The general way . .\nF.2 How it’s implemented in D-Struct .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nG CIT-based methods, score-based methods and faithfulness .\n\nG.1 CIT-based methods .\nG.2 (Differentiable) Score-based methods .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n\n15 15 17 18 18 19 19\n\n21\n\n23\n\n24\n\n25\n\n26 26 26\n\n27 27 28\n\nA ADDITIONAL EXPERIMENTS\n\nPlease find our (anonymous) online code repository at:\n\nhttps://anonymous.4open.science/r/d-struct\n\nOur code is based on code provided by Zheng et al. [36], and we annotated our code where we used their implementation.\n\nA.1 SETTINGS AND DETAILS\n\nIn the interest of space, we left out a few details in our main text. Here we discuss hyperparameters (those in addition to the hyperparameters required for the selected DSFs), the evaluation metrics, and how we combine the different parallel DAGs.\n\nHyperparameters. D-Struct inherits hyperparameters from the chosen underlying DSFs. These hyperparameters act in the same way as they would in their original incarnation. For a discussion on these hyperparameters we refer to the relevant literature on these methods specifically.\n\nHowever, D-Struct also adds two additional parameters: K and α. The impact of K is already discussed in the main text, recapitulated as: K implicitly determines the sizes of the subsets used to train the parallel DSFs, as such, for high K we should have high n. With both increasing, we report better performance (particularly in Scale-Free DAGs).\n\nThe impact of α is a bit more subtle, and also a function of K. First, consider Fig. 7, displaying the impact on each evaluation metric as a function of different α. What we find is that setting α is mostly\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\ndependent on K as lower α tend to work better with higher K, and vice versa for lower K. This MSE, resulting in a higher value with more K. If α is large in a setting makes sense as we sum each with large K, the regularisation effect would simply be too large. We set our hyperparameters to those which yielded best performance (deduced from Fig. 7 for α, and K = 3 when not varied over as this yielded most stable results overall).\n\nL\n\nvarying n\n\nvarying s\n\nvarying d\n\nvarying K\n\n6\n\n4\n\n2\n\nD H\nS\n\n6\n\n4\n\n2\n\n200 500\n\n1,000 n\n\n2,000\n\n0.5\n\n1\n\n1.5\n\nsd\n\n2\n\n20\n\n15\n\n10\n\n5\n\n0\n\n6\n\n4\n\n2\n\n5\n\n7\n\nd\n\n10\n\n2\n\n3\n\nK\n\n5\n\nα = 1\n\nα = 2\n\nα = 3\n\nα = 4\n\nα = 5\n\nFigure 7: Results showing the effect of α. Depending on the nature of the problem the degree of regularization imposed by α can vary. This then changes the amount we enforce the similarity between the different D-Struct adjacency. matrices.\n\nEvaluation metrics. The learned graphs from NOTEARS and D-Struct are assessed using four graph metrics namely: (1) Structural Hamming distance (SHD), (2) False discovery rate (FDR), (3) False positive rate (FPR) and (4) True positive rate (TPR). These values are standard when evaluating structure learning methods. We provide some insight into these evaluation metrics below.\n\nStructural Hamming distance (SHD)\n\nSHD is the total number of edge additions, deletions, and reversals needed to convert the estimated DAG into the true DAG. That means that the worst case SHD is d2 d, as we bound the diagonal to be 0 at all times. As such, the reported SHD with varying d is expected to be higher, not due to hardness of the problem, but as a property of the SHD (see for example Fig. 6).\n\n−\n\nFalse discovery rate (FDR)\n\nWhenever an edge is suggested in the estimated DAG, which is incorrect, we add to the falsely discovered edges. As such, the FDR is defined as the number of reversed edges and edges that should not exist, divided by the number of edges in total. Of course, the exception being when no edges are suggested at all (which implies dividing by 0), which naturally has an FDR of zero.\n\nFalse positive rate (FPR)\n\nWe sum the edges that should have been reversed and those that should not exist, and divide by the total number of non-edges in the ground truth DAG. A non-edge is an edge that does not exist. With a more connected ground truth DAG, we expect this number to be lower automatically (as the numerator of the FPR would be higher). This is the reason why we let s be a function of d, as increasing the number of expected edges with d would somewhat counter this effect. Note that, in Table 1 we see the FPR increasing proportionate to the factor multiplied with d, which is as we would expect.\n\nTrue positive rate (TPR)\n\nThis signifies the number of correctly estimated edges, over the number of edges in the true graph. Note that, reversed edges are counted as wrong edges.\n\nCombining graphs. Inference is done by combining the K internal graphs. In our implementation of D-Struct we combine graphs by averaging the adjacency matrices and apply a threshold to convert the average graph into a binary matrix. The latter is a similar strategy to most DSFs’ strategies to convert a continuous matrix into a binary one. This is a relatively simple method with promising results, in line with what is currently done in literature.\n\nHowever, given that D-Struct has multiple graphs, we can actually come up with different strategies (a potential topic for future research). Naturally, this would be more relevant with high K, which in turn requires a larger sample-size, as per our discussion above. Specifically, we enter the domain of ensemble learning. Like D-Struct, ensemble methods need to combine, potentially conflicting, outcomes and provide the user with only one outcome.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Results on Erdos-Reny`ı (ER) graphs. First block: We sample five different ER random graphs, and accompanying non-linear structural equations using an index-model. From each system we then sample a varying number of samples, and evaluate NOTEARS-MLP with D-Struct (indicated as “✓”) and without D-Struct (indicated as “✗”). Second block: For each row we sample a new ER graph with a varying degree of connectedness (s indicates the expected number of edges). In both cases, we report the average performance in terms of SHD, FPR, TPR, and FDR, with std in scriptsize.\n\nmetric\n\nD-Struct\n\nSHD (\n\n↓\n\n)\n\n✗\n\n✓\n\nFPR (\n\n) ↓\n\n✗\n\n✓\n\nTPR (\n\n) ↑\n\n✗\n\n✓\n\nFDR (\n\n) ↓\n\n✗\n\n✓\n\nn\n\n200 500 1000\n\ns\n\n0.5d 1d 1.5d 2d\n\nd\n\n5 7\n10\n\n3.60 3.20 2.75\n\n±\n\n±\n\n±\n\n0.27\n\n0.80\n\n0.47\n\n3.75 3.50 3.00 2.28\n\n1.6\n\n0.86\n\n1.15\n\n0.80\n\n±\n\n±\n\n±\n\n±\n\nvarying sample size\n\n4.20 3.94 2.67\n\n±\n\n±\n\n±\n\n0.44\n\n0.33\n\n0.63\n\n0.67 0.66 0.75\n\n±\n\n±\n\n±\n\n0.05\n\n0.24\n\n0.08\n\n2.00 1.20 1.00\n\n0.67\n\n0.44\n\n0.37\n\n±\n\n±\n\n±\n\n0.64 0.56 0.63\n\n0.05\n\n0.04\n\n0.13\n\n±\n\n±\n\n±\n\n0.25 0.18 0.18\n\n0.06\n\n0.05\n\n0.03\n\n±\n\n±\n\n±\n\n4.20 3.94 3.67\n\n0.44\n\n0.33\n\n0.82\n\n±\n\n±\n\n±\n\n7.33 7.67 5.67 3.67\n\n0.13\n\n0.45\n\n1.75\n\n0.82\n\n±\n\n±\n\n±\n\n±\n\nvarying graph connectedness\n\n0.50 0.55 1.00 1.00\n\n0.25\n\n0.22\n\n0.19\n\n0.32\n\n±\n\n±\n\n±\n\n±\n\n1.05 1.53 1.55 2.67\n\n±\n\n±\n\n±\n\n±\n\n0.02\n\n0.09\n\n0.08\n\n0.63\n\n0.83 0.75 0.89 0.67\n\n±\n\n±\n\n±\n\n±\n\n0.19\n\n0.09\n\n0.07\n\n0.17\n\nvarying dimension count\n\n0.88 0.46 0.62 0.70\n\n0.04\n\n0.09\n\n0.06\n\n0.09\n\n±\n\n±\n\n±\n\n±\n\n0.42 0.40 0.32 0.11\n\n0.16\n\n0.09\n\n0.05\n\n0.03\n\n±\n\n±\n\n±\n\n±\n\n±\n\n2.28 8.67 ±\n19.71\n\n0.80\n\n0.56\n\n3.67 ±\n12.88 0.72 30.82\n\n0.82\n\n1.00 0.15 0.72 0.98 0.42\n\n±\n\n±\n\n±\n\n0.32\n\n0.05\n\n0.13\n\n±\n\n±\n\n±\n\n2.67 1.07 1.18\n\n±\n\n±\n\n±\n\n0.63\n\n0.01\n\n0.04\n\n0.67 0.96 0.70\n\n±\n\n±\n\n±\n\n0.17\n\n0.02\n\n0.16\n\n0.70 0.83 0.71\n\n0.09\n\n0.01\n\n0.06\n\n±\n\n±\n\n±\n\n0.11 0.49 0.34\n\n0.03\n\n0.01\n\n0.08\n\n±\n\n±\n\n±\n\n0.42 0.44 0.39\n\n±\n\n±\n\n±\n\n0.73 0.77 0.53 0.32\n\n±\n\n±\n\n±\n\n±\n\n0.32 0.63 0.70\n\n±\n\n±\n\n±\n\n0.04\n\n0.04\n\n0.11\n\n0.01\n\n0.07\n\n0.04\n\n0.08\n\n0.08\n\n0.01\n\n0.02\n\nOne avenue is to not vote on a per-element basis, but on a per-graph basis. Imagine, two graphs in K that are exactly the same aspire more confidence in their accuracy. We could even relax similarity to an SHD across graphs, where we weight each graph’s “vote” proportionally to their combined SHD. We believe this to be promising area of future research.\n\nExperimental procedure. Here we explain how our experimental setup works, which steps we need to perform before starting an experiment, and which information each model is provided.\n\nThere are two main parts to an experimental setup: (i) we need a structure, (ii) we need a set of structural equations accompanying the structure of step (i).\n\n(i) The structure. In our setup, a structure can only be a DAG. To reduce bias as much as possible, we do not determine structures up front, but sample random structures for each experimental run. Of course, the same random structure is presented for each benchmark. Sampling random structures happens in two ways: either we sample a random Erd ̈os-Renyi graph, which requires a dimension count (d), and an expected number of edges (ds); or we use a scale-free graph which is generated using the process described in Barab ́asi and Albert [73] as was also done in Zheng et al. [36], which needs a parameter β = 1 (the exponent for the preferential attachment process). The expected number of edges in our setup depends on d such that s resembles the ratio of edges versus nonedges in the random graph.\n\n(ii) The equations. With a sampled structure from (i), we can now sample some structural equations. In our paper we use an index model to sample these. In short, an index model is randomly parameterised as: fj(Xpa(j)) = (cid:80)3 pa(j) θjmkXk), where h1 = tanh, h2 = cos, h3 = sin, [0.5, 2]. Exactly as was reported in and each θjmk is drawn uniformly from range [ Zheng et al. [36].\n\nm=1 hm((cid:80)\n\n0.5]\n\n−\n\n−\n\n2,\n\n∪\n\n∈\n\nk\n\nA.2 COMPLETED RESULTS\n\nRecall from Section 4 that we only reported a subset of the results. In Table 5 we report the remainder for NOTEARS-MLP and the D-Struct implementation on scale free graphs.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Results on Scale-Free (SF) graphs. First block: We sample five different SF random graphs, and accompanying non-linear structural equations using an index-model. From each system we then sample a varying number of samples, and evaluate NOTEARS-SOB with D-Struct (indicated as “✓”) and without D-Struct (indicated as “✗”). Second block: For each row we sample a new SF graph with a varying degree of connectedness (s indicates the expected number of edges). Third block: For each row we vary the feature dimension count (d). Fourth block: For each row we vary the number of subsets for D-Struct (s).In all cases, we report the average performance in terms of SHD, FPR, TPR, and FDR, with std in scriptsize.\n\nmetric\n\nD-Struct\n\nSHD (\n\n↓\n\n)\n\n✗\n\n✓\n\nFPR (\n\n) ↓\n\n✗\n\n✓\n\nTPR (\n\n) ↑\n\n✗\n\n✓\n\nFDR (\n\n) ↓\n\n✗\n\n✓\n\nn\n\n200 500 1000\n\ns\n\n0.5d 1d 1.5d 2d\n\nd\n\n5 7\n10\n\nK\n\n2 3\n5\n\n6.00 3.40 2.75\n\n±\n\n±\n\n±\n\n0.69\n\n0.88\n\n0.86\n\n3.8 ±\n4.60 4.33\n\n0.25\n\n0.25\n\n0.50\n\n±\n\n±\n\nvarying sample size\n\n1.27 1.53 1.44\n\n±\n\n±\n\n±\n\n0.08\n\n0.08\n\n0.17\n\n0.49 0.57 0.61\n\n±\n\n±\n\n±\n\n0.12\n\n0.09\n\n0.15\n\n0.83 0.69 0.76\n\n0.03\n\n0.03\n\n0.05\n\n±\n\n±\n\n±\n\n0.63 0.41 0.36\n\n0.08\n\n0.12\n\n0.15\n\n±\n\n±\n\n±\n\n1.8 ±\n0.67 0.58\n\n0.20\n\n0.14\n\n0.22\n\n±\n\n±\n\n14.11 8.11 ±\n15.20 15.20\n\n± 3.96\n\n5.40 39.53 39.46 3.44 38.31 3.44 38.25\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\nvarying graph connectedness\n\n0.37 0.31 0.38 0.13 0.41 0.32 0.44 0.32\n\n0.13\n\n0.09\n\n0.14\n\n0.14\n\n±\n\n±\n\n±\n\n±\n\n0.89 0.89 1.05 1.04\n\n±\n\n±\n\n±\n\n±\n\n0.01\n\n0.01\n\n0.01\n\n0.01\n\n0.22 0.30 0.58 0.58\n\n±\n\n±\n\n±\n\n±\n\n0.14\n\n0.18\n\n0.23\n\n0.23\n\nvarying dimension count\n\n0.20 0.16 0.52 0.50\n\n0.11\n\n0.09\n\n0.07\n\n0.07\n\n±\n\n±\n\n±\n\n±\n\n0.42 0.22 0.40 0.40\n\n0.17\n\n0.15\n\n0.17\n\n0.17\n\n±\n\n±\n\n±\n\n±\n\n±\n\n2.75 8.25 ±\n16.80\n\n0.86\n\n3.09\n\n4.33 ±\n15.00 4.21 35.75\n\n0.50\n\n0.58 0.22 0.55 0.33 0.36\n\n±\n\n±\n\n±\n\n0.22\n\n0.21\n\n0.17\n\n±\n\n±\n\n±\n\n3.00 2.75 2.80\n\n±\n\n±\n\n±\n\n0.42\n\n0.86\n\n0.57\n\n6.00 4.33 5.25\n\n0.30\n\n0.5\n\n0.21\n\n±\n\n±\n\n±\n\n0.53 0.58 0.73\n\n0.21\n\n0.22\n\n0.15\n\n±\n\n±\n\n±\n\n1.44 1.00 0.99\n\n±\n\n±\n\n±\n\n0.17\n\n0.01\n\n0.01\n\n0.61 0.96 0.58\n\n±\n\n±\n\n±\n\n0.15\n\n0.08\n\n0.24\n\nvarying subset count\n\n2.00 1.44 1.75\n\n±\n\n±\n\n±\n\n0.10\n\n0.17\n\n0.07\n\n0.66 0.61 0.74\n\n±\n\n±\n\n±\n\n0.04\n\n0.15\n\n0.09\n\n0.76 0.78 0.67\n\n0.05\n\n0.02\n\n0.03\n\n±\n\n±\n\n±\n\n0.36 0.49 0.42\n\n0.15\n\n0.16\n\n0.17\n\n±\n\n±\n\n±\n\n0.57 0.76 0.68\n\n0.04\n\n0.05\n\n0.03\n\n±\n\n±\n\n±\n\n0.21 0.36 0.31\n\n0.07\n\n0.15\n\n0.07\n\n±\n\n±\n\n±\n\n0.39 0.48 0.44\n\n±\n\n±\n\n±\n\n0.93 0.99 0.98 0.89\n\n±\n\n±\n\n±\n\n±\n\n0.44 0.76 0.85\n\n±\n\n±\n\n±\n\n0.60 0.44 0.53\n\n±\n\n±\n\n±\n\n0.02\n\n0.03\n\n0.05\n\n0.01\n\n0.01\n\n0.01\n\n0.01\n\n0.05\n\n0.01\n\n0.01\n\n0.03\n\n0.05\n\n0.02\n\nA.3 OTHER DSFS\n\nWe repeat the results above for NOTEARS-SOB which is a Sobolev based implementation of NOTEARS, in Table 4. The main difference here with NOTEARS-MLP is the nonparametric estimation of the structural equations in ˆ . Note that, future implementations of DSFs broadly alter G\nthe way in which the structural equations are estimated, and much less on how the proposed structure is evaluated to be a DAG (as they are mostly based on eq. (3)). Overall, we find that NOTEARS-SOB behaves the same as NOTEARS-MLP: D-Struct vastly improves performance.\n\nNote that code to reproduce above results is provided in the online code repository linked to above.\n\nA.4 SUBSAMPLING DATASETS\n\nWe refer to Table 6 for the full results presented originally in Table 2. While FPR may be a little higher, using D-Struct still outperforms not using D-Struct in terms of the FPR– already shown in Table 1. Furthermore, as the subsampling routine forces D-Struct to learn on different distributions, it is possible that this increase in FPR is a result of initially more conflicting DAG structures. When combined, these structures include more edges which in turn result in more potential for a false positive edge discovery. In fact, we observe a lower necessary threshold when using our subsampling routine, necessary to transform the real-values matrix into a binary adjacency matrix.\n\nWe also report the same metrics as a function of the DAG-finding threshold in Fig. 8, where the threshold is applied to the adjacency matrix to produce a binary matrix on which we compute the metrics. Of course, a threshold will be selected in practice; however, we show that for a range of plausible threshold values and all metrics that subsampling with our routine is indeed beneficial, compared to randomized subsampling. From this, it seems that the results we find in Table 6 are consistent even with changing thresholds.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Results on Scale-Free (SF) graphs. First block: We sample five different SF random graphs, and accompanying non-linear structural equations using an index-model. From each system we then sample a varying number of samples, and evaluate NOTEARS-MLP with D-Struct (indicated as “✓”) and without D-Struct (indicated as “✗”). Second block: For each row we sample a new SF graph with a varying degree of connectedness (s indicates the expected number of edges). Third block: For each row we vary the feature dimension count (d). Fourth block: For each row we vary the number of subsets for D-Struct (s).In all cases, we report the average performance in terms of SHD, FPR, TPR, and FDR, with std in scriptsize.\n\nmetric\n\nD-Struct\n\nSHD (\n\n↓\n\n)\n\n✗\n\n✓\n\nFPR (\n\n) ↓\n\n✗\n\n✓\n\nTPR (\n\n) ↑\n\n✗\n\n✓\n\nFDR (\n\n) ↓\n\n✗\n\n✓\n\nn\n\n200 500 1000\n\ns\n\n0.5d 1d 1.5d 2d\n\nd\n\n5 7\n10\n\nK\n\n2 3\n5\n\n2.80 2.20 3.25\n\n±\n\n±\n\n±\n\n3.33 3.33 3.25 2.75\n\n±\n\n±\n\n±\n\n±\n\n0.86\n\n0.80\n\n1.49\n\n0.88\n\n0.89\n\n0.41\n\n1.03\n\nvarying sample size\n\n2.07 2.20 1.78\n\n±\n\n±\n\n±\n\n0.19\n\n0.18\n\n0.20\n\n0.80 0.77 0.68\n\n±\n\n±\n\n±\n\n0.11\n\n0.13\n\n0.15\n\n0.73 0.27 0.75\n\n0.28\n\n0.12\n\n0.43\n\n±\n\n±\n\n±\n\n0.54 0.37 0.66\n\n0.08\n\n0.09\n\n0.08\n\n±\n\n±\n\n±\n\n0.26 0.14 0.29\n\n0.11\n\n0.06\n\n0.18\n\n±\n\n±\n\n±\n\n6.20 7.20 5.33\n\n0.57\n\n0.66\n\n0.61\n\n±\n\n±\n\n±\n\n8.00 8.00 7.67 5.00\n\n0.37\n\n1.00\n\n0.31\n\n1.00\n\n±\n\n±\n\n±\n\n±\n\nvarying graph connectedness\n\n0.50 0.50 0.50 0.33\n\n0.19\n\n0.19\n\n0.07\n\n0.23\n\n±\n\n±\n\n±\n\n±\n\n1.17 1.17 1.17 1.22\n\n±\n\n±\n\n±\n\n±\n\n0.06\n\n0.17\n\n0.04\n\n0.22\n\n0.92 0.92 0.94 0.64\n\n±\n\n±\n\n±\n\n±\n\n0.08\n\n0.08\n\n0.04\n\n0.15\n\n0.38 0.38 0.42 0.50\n\n0.05\n\n0.13\n\n0.06\n\n0.07\n\n±\n\n±\n\n±\n\n±\n\n0.41 0.41 0.43 0.14\n\n0.08\n\n0.08\n\n0.03\n\n0.09\n\n±\n\n±\n\n±\n\n±\n\nvarying dimension count\n\n±\n\n3.25 8.22 ±\n16.80\n\n1.49\n\n1.31\n\n5.33 ±\n15.67 4.21 35.75\n\n0.61\n\n0.75 0.14 0.54 0.33 0.36\n\n±\n\n±\n\n±\n\n0.43\n\n0.09\n\n0.17\n\n±\n\n±\n\n±\n\n2.40 2.00 0.75\n\n±\n\n±\n\n±\n\n0.24\n\n1.04\n\n0.48\n\n6.50 5.33 5.25\n\n0.46\n\n0.6\n\n0.21\n\n±\n\n±\n\n±\n\n0.53 0.33 0.25\n\n0.08\n\n0.47\n\n0.16\n\n±\n\n±\n\n±\n\n1.78 1.04 0.99\n\n±\n\n±\n\n±\n\n0.20\n\n0.01\n\n0.01\n\n0.68 0.98 0.58\n\n±\n\n±\n\n±\n\n0.15\n\n0.02\n\n0.24\n\nvarying subset count\n\n2.16 1.78 2.55\n\n±\n\n±\n\n±\n\n0.15\n\n0.20\n\n0.11\n\n0.83 0.68 1.00\n\n±\n\n±\n\n±\n\n0.05\n\n0.14\n\n0.00\n\n0.66 0.83 0.67\n\n0.08\n\n0.03\n\n0.03\n\n±\n\n±\n\n±\n\n0.29 0.54 0.42\n\n0.18\n\n0.04\n\n0.17\n\n±\n\n±\n\n±\n\n0.50 0.66 0.33\n\n0.06\n\n0.09\n\n0.05\n\n±\n\n±\n\n±\n\n0.21 0.14 0.09\n\n0.03\n\n0.09\n\n0.05\n\n±\n\n±\n\n±\n\n0.62 0.72 0.53\n\n±\n\n±\n\n±\n\n0.82 0.82 0.80 0.48\n\n±\n\n±\n\n±\n\n±\n\n0.53 0.76 0.85\n\n±\n\n±\n\n±\n\n0.65 0.53 0.76\n\n±\n\n±\n\n±\n\n0.06\n\n0.06\n\n0.06\n\n0.03\n\n0.07\n\n0.03\n\n0.12\n\n0.06\n\n0.01\n\n0.01\n\n0.06\n\n0.06\n\n0.03\n\n5\n\n4\n\n3\n\n2\n\n1\n\nSHD (\n\n↓\n\n)\n\n0.7\n\n0.8 threshold\n\n0.9\n\n4\n\n3\n\n2\n\n1\n\n0\n\nFPR (\n\n↓\n\n)\n\n0.7\n\n0.8 threshold\n\n0.9\n\n0.9\n\n0.8\n\n0.7\n\n0.6\n\nTPR (\n\n↑\n\n)\n\n0.7\n\n0.8 threshold\n\n0.9\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\nFDR (\n\n)\n\n↓\n\n0.7\n\n0.8 threshold\n\n0.9\n\nwith subsampling (✓)\n\nwithout subsampling (✗)\n\nFigure 8: Subsampling with different DAG-thresholds. The DAG-threshold transforms the realvalued adjacency matrix, to a binary one. As the threshold increases, the amount edges that remain part of the DAG decreases. The above confirms our findings from Table 6 in different settings.\n\nA.5 DAGS: D-STRUCT VS NOTEARS\n\nWe wish to also highlight that indeed what is recovered by D-Struct is different from NOTEARS. For this we refer to Figs. 9 and 10, each representing an independent run.\n\nA.6 GAINS FROM ENFORCING TRANSPORTABILITY\n\nA key concept of D-Struct is to enforce transportability, which is done using our novel loss function.\n\n( L\n\nk\n\nG\n\nk) :=\n\n|D\n\nL\n\nDSF(\n\nG|D\n\nk) + α\n\nMSE(A(\n\nk)),\n\nG\n\nL\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Usefulness of our subsampling routine. We sample ten different ER random graphs, and accompanying non-linear structural equations as in Zheng et al. [36]. From each system we then sample n = 2000 samples, and evaluate NOTEARS-MLP with our subsampling routine from Section 3.2 (indicated as “✓”) and without the subsampling routine, using random splits instead (indicated as “✗”). For each row we repeat our experiment with different K. In both cases, we report the average performance in terms of SHD, FPR, TPR, and FDR, with std in scriptsize.\n\nmetric\n\nSubsample\n\nSHD (\n\n↓\n\n)\n\n✗\n\n✓\n\nFPR (\n\n) ↓\n\n✗\n\n✓\n\nTPR (\n\n) ↑\n\n✗\n\n✓\n\nFDR (\n\n) ↓\n\n✗\n\n✓\n\nK\n\n2 3\n5\n\n2.80 3.00 2.80\n\n±\n\n±\n\n±\n\n0.53 3.40 0.37 4.00 0.57 4.40\n\n±\n\n±\n\n±\n\nvarying amount of splits\n\n0.58 2.80 0.59 2.00 1.29 1.40\n\n0.53 2.60 0.51 1.60 0.50 0.60\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n0.33 0.80 0.45 0.73 0.26 0.71\n\n0.06 0.71 0.04 0.58 0.06 0.53\n\n0.07 0.28 0.06 0.22 0.15 0.18\n\n±\n\n±\n\n±\n\n0.05 0.30 0.05 0.24 0.06 0.07\n\n±\n\n±\n\n±\n\n0.16\n\n0.17\n\n0.10\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n(a) True DAG.\n\n(b) D-Struct estimated DAG. SHD=2, FDR=0.11, FPR=1.\n\n(c) NOTEARS estimated DAG. SHD=2, FDR=0.2, FPR=2.\n\nFigure 9: First independent run\n\n(a) True DAG.\n\n(b) D-Struct estimated DAG. SHD=2, FDR=0.11, FPR=1.\n\n(c) NOTEARS estimated DAG. SHD=2, FDR=0.2, FPR=2.\n\nFigure 10: Second independent run\n\nThe question is what do we gain from the usage of the α term which is key to enforcing transportability. We conduct an experiment where we set α = 0. This not only assesses the importance MSE this amounts to assessing K independent versions of vanilla of this term, but also without NOTEARS.\n\nL\n\nResults: When we combine the K DAGs by averaging them, the result is NOT a DAG.\n\nThis highlights that indeed that (1) transportability is key as part of this formulation and (2) that simply running parallel versions of NOTEARS is not a sufficient solution.\n\nWe highlight this by showing the independent DAGs discovered without transportability enforced, the average of the DAGs and the true DAG. These results are reported in Figs. 11 and 12\n\n20\n\n01234012340.00.20.40.60.81.001234012340.00.20.40.60.81.001234012340.00.20.40.60.81.001234012340.00.20.40.60.81.001234012340.00.20.40.60.81.001234012340.00.20.40.60.81.0Under review as a conference paper at ICLR 2023\n\n(a) True DAG.\n\n(b) NOTEARS: Mean. This is not a DAG!\n\n(c) NOTEARS: 1st DAG.\n\n(d) NOTEARS: 2nd DAG.\n\n(e) NOTEARS: 3rd DAG.\n\n(f) D-Struct: Mean. This is a DAG!\n\n(g) D-Struct: 1st DAG.\n\n(h) D-Struct: 2nd DAG.\n\n(i) D-Struct: 3rd DAG.\n\nFigure 11: First independent run. Note the differences between the three DAGS on each partition for NOTEARS (Row 1), the average is also not a DAG. Whereas, for D-Struct note the similarities by enforcing transportability, the average is also a DAG.\n\nB CAUSAL INTERPRETATION AND UNIQUENESS\n\nCausality. Causal relationships between variables are often expressed as DAGs [28]. While DStruct is able to recover DAGs more reliably, there is actually no guarantee that the found DAG can be interpreted as a causal DAG. There is a simple reason for this: we do not make any additional identification assumptions on the structural equations when learning DAGs, at least not beyond what is already assumed in the used DSFs. Furthermore, should D-Struct be combined with a DSF that is able to recover a causal DAG4, the way in which the K internal DAGs are combined may violate these assumptions (recall DAG combination from Appendix A.1).\n\nWith D-Struct, we recover a Bayesian network (BN), which is directed, yet the included directions are not necessarily meaningful. The only guarantee we have with BNs is that they resemble a distribution, which express some conditional distributions (as per the independence sets in Section 2.1). Order is not accounted for in these independence sets. For more information regarding this, we refer to Appendix D and Koller and Friedman [26].\n\nHowever, as is indicated in Koller and Friedman [26, Chapter 21], a “good” BN structure should correspond to causality, where edges X Y indicated that X causes Y . Koller and Friedman [26] state that BNs with a causal structure tend to be sparser. Though, if queries remain probabilistic, it doesn’t matter whether or not the structure is causal, the answers will remain the same. Only when we are interested in interventional queries (by using do-calculus) we have to make sure the DAG is a causal one.\n\n→\n\nUniqueness. The above is a pragmatic view. To our knowledge, there is no real proof stating that sparser DAGs are (even more likely to be) causal. However, it could offer guidance to try and recover a causal DAG, assuming it to be sparse [76]. The latter of course is assuming that there exists a unique or correct DAG, which is something we implicitly assume to be true. Naturally, when\n\n4We know of none that is able to.\n\n21\n\n01234012340.00.20.40.60.81.001234012340.00.20.40.60.81.0012340123402468100123401234012345601234012340.80.91.01.11.21.31.401234012340.00.20.40.60.81.001234012340.00.51.01.52.02.53.03.501234012340.00.51.01.52.02.53.03.501234012340.00.51.01.52.02.53.03.5Under review as a conference paper at ICLR 2023\n\n(a) True DAG.\n\n(b) NOTEARS: Mean. This is not a DAG!\n\n(c) NOTEARS: 1st DAG.\n\n(d) NOTEARS: 2nd DAG.\n\n(e) NOTEARS: 3rd DAG.\n\n(f) D-Struct: Mean. This is a DAG!\n\n(g) D-Struct: 1st DAG.\n\n(h) D-Struct: 2nd DAG.\n\n(i) D-Struct: 3rd DAG.\n\nFigure 12: Second independent run. Note the differences between the three DAGS on each partition for NOTEARS (Row 1), the average is also not a DAG. Whereas, for D-Struct note the similarities by enforcing transportability, the average is also a DAG.\n\neasy identification\n\nhard identification\n\nnon-transportable models\n\ntransportable models\n\nidentifiable models\n\ncausal models\n\n◆ DSFs [35, 36, 51, 52]\n\n◆ CIT [74, 75] ◆ D-Struct\n\nFigure 13: Comparison of methods w.r.t. identification and uniqueness. The ultimate goal of structure learning is to come up with unique and correct structures. Once we recover the one true DAG, we may interpret the structure as a causal model. However, discovering a causal structure using only observational data is not possible. Yet, we can approach it with methods that restrict the set of possible DAGs. From this illustration, we gather that D-Struct is an attempt to restrict the solution space of DSFs, going one step further towards unique solutions.\n\n22\n\n01234012340.00.20.40.60.81.001234012340.00.20.40.60.81.001234012340246810121401234012340246810121401234012340.91.01.11.21.31.41.501234012340.00.20.40.60.81.001234012340.00.51.01.52.02.53.03.501234012340.00.51.01.52.02.53.03.501234012340.00.51.01.52.02.53.03.54.0Under review as a conference paper at ICLR 2023\n\naiming to make a discovery, we aim to recover a true DAG, where a truthful DAG is corresponding with a DAG that can be uniquely recovered.\n\nHowever, there is a difference between a unique DAG, and the unique DAG. Where the former is a matter of identifiability (discussed more below), the latter is one of causality. With the latter we mean: “can a method actually recover the unique causal DAG?” From Meek [39] and Meek [39] we learn that, from observational data alone, this is impossible and should thus not be a goal if one is not willing to make additional assumptions.\n\nWe stress that transportability is a weaker goal than identifiability. Enforcing transportability does not guarantee unique or repeatable results. Take CIT-based methods— which we know to be fully transportable. While it is true that the same set of independence statements will always result in the same DAG (i.e. transportability), it is not necessarily true that we will always recover the same independence statements. Depending on which independence test one uses to build the set of independence statements, the resulting DAG may look entirely different. Similar for D-Struct, while D-Struct does encourage similar DAGs (see for example Appendix A), we have no guarantee to recover the same DAG over different runs. The latter is a requirement for identifiability [77] as identifiability requires the model to always converge to the same set of parameters.\n\nHowever, we do believe transportability is a vehicle to bring us closer to unique identification with DSFs. It is clear from our experiments that transportable learners greatly improve edge accuracy. As our synthetic setup is governed by one (and thus unique) graph, having a more accurate learner means a learner that discovers a DAG that is more like the unique, underlying graphical model. Consider Fig. 13 for an illustration comparing the relevant methods in terms of model identification.\n\nC TRANSPORTABILITY IN NON-OVERLAPPING DOMAINS\n\nConsider the multi-origin setting, where we have at least two datasets, each stemming from a different source. It is entirely possible that, given the different sources, these datasets are not comparable in terms of recorded features. We can recognise two major manifestations of this phenomenon: either (i) the supports of the datasets do not match, or (ii) the dimensions do not match.\n\n(i) Different support. Recall from Section 2.1 that DAGs encode a set of independence statements. As such, it is mainly independence that governs structure. Transportability in the setting of conflicting support, thus requires some (mild) assumptions. Specifically, we require that independence holds, regardless of support. The is mostly a pragmatic assumption. If for example, we find that , we usually don’t specify over what i\nX support this independence holds. Implicitly, we assume that independence holds, regardless of what area in\n\nj, where each component denotes a dimension in\n\nwe find ourselves in.\n\n⊥⊥ X\n\nX\n\ni,\n\n{X\n\nj X\n\n}\n\nin Section 3.2 govern the entire domain [N ], and as a conNote that the chosen distributions in sequence . As such, the problem of conflicting support does not manifest in our solution of single-origin D-Struct. In case one chooses distributions that do not cover [N ] equally, we have to assume independence is constant across different supports (i.e. the assumption explained above).\n\nX\n\nP\n\n(ii) Different dimensions. A more difficult setting of conflicting domains, is when we record different variables in each of the multi-origin datasets. In order for a DAG to be transportable, we require the variable sets to correspond. As such, we are only able to work with overlapping intersections of the non-overlapping domains. Doing so requires some additional assumptions on the noise: assuming we record some noise on each variable, we have to make the additional assumption that the noise is independent of the other variables, or at least the variables outside the intersection between domains. The latter is made quite often, and should not limit applicability of D-Struct in this setting too much (recall that applicability of D-Struct is mostly determined by the used DSF). The reason relates to the second assumption, below.\n\nThe second assumption is a bit stricter: any variables outside the intersection cannot be confounding variables inside the intersection. If two variables have no direct edges, and the nodes part of an indirect edge fall outside the domain-intersection, we have to expect the DSF to find an edge between these two nodes. While this direct edge is wrong, this is actually expected behaviour of most DSFs as the algorithms will find these variables to be correlated (due to the third, now unobserved, vari-\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nable). The only way to overcome these situations, is to use DSFs that naturally handle unobserved confounding.\n\nRelated work. Some work on structure discovery from multiple (non-overlapping) domains has been proposed. For example, Ghassami et al. [78] in the linear setting, Peters et al. [79] for the interventional setting, or Huang et al. [80] in the temporal setting. While the difference between the first ([78]) is clear (only focusing on linear systems, whereas we on a non-parametric setting), the others are not immediately clear. Some intuition into the difference can be achieved by considering that both the interventional and temporal know where the difference in distribution is coming from. So much so, that the known difference is exploited when garnering (causal) structural information. We believe applying our findings on transportability to the settings described earlier can be a promising new avenue of research.\n\nD DEFINITIONS\n\nDefinition 2 (Markov blanket.). A Markov blanket of a random variable Xi in a random set where, when conditioned upon, results in independence\n\nis any subset\n\nX1, . . . , Xd\n\n′\n\n:= X\n{ between\n\nX \\ X\n\n′ (the other variables) and Xi,\n\nX\n\n⊂ X\n\n}\n\nXi We will denote the Markov blanket of Xi as\n\n′.\n\n′\n\n|X\n\n⊥⊥ X \\ X ′(Xi).\n\nX\n\n(6)\n\nIn principle, Def. 2 means that this does not mean that to predict X1.\n\nX \\ X\n\nX\n\n′ contains all the information present in\n\n′ contains no information to infer X1, but variables in\n\nX\n\nto infer X1. Note that ′ are sufficient\n\nX\n\nOne step further, is a Markov boundary [53]: Definition 3 (Markov boundary.). A Markov boundary of a random variable Xi of a random set which is a Markov blanket (Def. 2) itself, but does not X\ncontain any proper subset which itself is a Markov blanket. We will denote the Markov boundary of Xi as\n\nX1, . . . , Xd\n\nis any subset\n\n−(Xi).\n\n⊂ X\n\n:=\n\nX\n\n}\n\n{\n\n−\n\nX\n\nWe can relate the Markov boundary (Def. 3) to probabilistic graphical modelling, as from a simplified factorisation (in eq. (1)), we can compose a Bayesian network. Specifically, each variable Xj −(Xi) depict one of three types of relationships: Xj is a parent of Xi, denoted as Pa(Xi) = Xj; Xj is a child of Xi, denoted as Ch(Xi) = Xj; or Xj is a parent of a child of Xi, denoted as Pa(Ch(Xi)) = Xj. Assuming that P is governed by a Markov random field (rather X\nthan a Bayesian network) simplifies things, as the Markov boundary depicts only directly connected variables.\n\n∈ X\n\nWhile the above may suggest that the Markov boundary only implies a vague graphical structure, will strongly constraint the possible graphical structures respecting doing this for ever variable in any found independence statements. D-separation (Def. 4) is then used to further limit the set of potential DAGs [28, 34]. Relating above definitions to those discussed in Section 2.1. For more information regarding the above, we refer to Koller and Friedman [26]. Definition 4 (d-separation [34].). In a DAG\n\n, a path between nodes\n\nj is blocked by a set\n\ni and\n\nX\n\nj) whenever there is a node\n\nk, such that one of two holds:\n\nX\n\nX\n\nG\n\nX\n\nd\n\nX\n\n⊂ X k\n\n(1)\n\nX\n\n(which excludes\n\nd and\n\n∈ X\n\ni and\n\nX\n\nX\n\nor\n\nor\n\nk+1, k+1, k+1.\n\nk\n\nk\n\n← X\n\n→ X\n\n→ X\n\nk d and\n\nX\n\nX\n\nX\n\nk\n\nk\n\nk\n\n−\n\n−\n\n1 ← X 1 → X 1 ← X X\nk 1 → X\n\n−\n\nk\n\nk+1.\n\n(2) neither\n\nk nor any of its descendants is in\n\nX\n\nX\n\nFurthermore, in a DAG disjoint) subset\n\nG\n\n− , we say that two disjoint subsets\n\n← X\n\nX\n\nd if every path between nodes in\n\nand\n\nand\n\nA B\nis blocked by\n\nare d-seperated by a third (also d. We then write\n\nWhen\n\nd d-seperates\n\nX\n\nand\n\nin\n\nG\n\nB\n\nA\n\nA\n\nB\n\nd.\n\nA ⊥⊥G B|X\n\n, we will denote this as d-sep\n\nX\n\n(\n\n; A\n\nB|X\n\nd).\n\nG\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition 5 (Faithfulness from Peters et al. [34].). Consider a distribution P\n\n(i) P\n\nX\n\nis faithful to\n\nif\n\nG\n\nA ⊥⊥ B|C ⇒ A ⊥⊥G B|C\n\n,\n\nand a DAG\n\nX\n\nG\n\nfor all disjoint sets\n\nand\n\n.\n\nC (ii) a distribution satisfies causal minimality with respect to with respect to\n\n, but not to any proper subgraph of\n\nB\n\n.\n\n, A\n\nG\n\nG\n\nif it is Markovian\n\nG\n\nPart (i) posits an implication that is the opposite of the global Markov condition\n\nfor which we refer to Peters et al. [34, Def. 6.21].\n\nA ⊥⊥G B|C ⇒ A ⊥⊥ B|C\n\n,\n\nPart (ii) is actually implied when part (i) is satisfied, when P , as per Peters et al. [34, prop. 6.35]. To have an idea for when faithfulness is not satisfied, we refer to Zhang and Spirtes [81] and Spirtes et al. [74, Theorem 3.2].\n\nis Markovian w.r.t.\n\nG\n\nX\n\nE INCORPORATING PRIOR KNOWLEDGE ON\n\n(P) USING L-BFGS-B\n\nI\n\nConsider the following, where we wish to discover a structure between 3 variables: X, Y , Z, where Z. According to the rules of d-speration (cfr. Def. 4), we are the ground truth satisfies X |\nalways in a structure where X and Y are only directly connected to Z, i.e. no direct connection between X and Y exists. Let us further assume that the system is linear (as this is what vanilla NOTEARS assumes, but without loss of generality towards recent NOTEARS extensions), then we have the following,\n\n⊥⊥\n\nY\n\nstructural equations\n\nstructure\n\nadjacency matrix\n\nX := εX , Z := βZ,X X + εZ, Y := βY,ZZ + εY ,\n\nX\n\nZ\n\nY\n\nA =\n\n(cid:32)0 0\n0\n\n0 0\n1\n\n(cid:33)\n\n1 0\n0\n\n.\n\nNaturally, using only conditional independence, the direction of the arrows are not identifiable as explained above. However, NOTEARS is unable to narrow it down to the equivalence classes expressed in Def. 4. The reason is simple, NOTEARS’ three optimisation components (the h-measure, an L2 loss, and an L1 regularizer on A, [76]) are satisfied exactly the same with the following system:\n\nstructural equations\n\nstructure\n\nadjacency matrix\n\nX := εX , Z := βZ,X X + εZ, Y := βY,X X + ε′Y ,\n\nX\n\nZ\n\nY\n\nA′ =\n\n(cid:32)0 0\n0\n\n1 0\n0\n\n(cid:33)\n\n1 0\n0\n\n,\n\nwhere βY,X = βY,ZβZ,X , and ε′Y = βY,ZεZ +εY resulting in Y being determined again by a simple linear equation. Both systems allow the same data to be generated, however under the constraint that X\n\nY\n\nZ only the former is possible. |\n\n⊥⊥\n\nWe argue that NOTEARS (and extensions) are unable to differentiate between them. Consider the components optimised by NOTEARS: both solutions propose a DAG (i.e. h(A) = h(A′) = 0); each DAG has an equal amount of arrows, leading to the same L1-loss across A and A′; and each equation is linear so NOTEARS is able to perfectly converge to each solution using its L2 loss. Given that each component scores exactly the same, NOTEARS is unable to differentiate between these two results. Crucially however, in the latter system X is always dependent of Y , resulting in X\nY eliminating v-structures) which is completely opposite to the former Y\n̸⊥⊥ system.\n\nZ (and even X |\n\n̸⊥⊥\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nPrior Markov independencies. We can however force known independence statements into DSFs a priori, using the L-BFGS-B optimizer. For example, consider the following I = Xi Z. If I is known a priori, then we also know there cannot (under any circumstance) exist a direct link between Xi and Xj as this would immediately contradict I which in turn would invalidate a structure proposing such a link.\n\nXj\n\n⊥⊥\n\n|\n\nAs such, we propose to fix these directed edges to 0 ), and exclude them from → A gradient calculation. This will not only constraint each DSL in step 2 above resulting in easier ) to be taken into account. Setting AX,Y = convergence, but it will also enforce any known I\nAY,X = 0 would immediately restrict NOTEARS from converging to this false solution as the solution would require AX,Y to be 1. The same approach is currently used in NOTEARS (and consequentially D-Structs parallel DSFs), by setting bounds of each diagonal element in A to (0, 0).\n\n(P\n\nji(\n\nij(\n\nA\n\n),\n\nG\n\nG\n\nX\n\nSetting some elements to 0 using the L-BFGS-B bounds, we effectively limit the set of possible solutions. In fact, when applied to the above problems, the second solution would sit outside the set of possible solutions, ensuring that NOTEARS cannot converge to it.\n\nF ADDITIONAL DETAILS ON SUBSAMPLING FROM DIFFERENT\n\nDISTRIBUTIONS\n\nIn Section 3.2 we introduced a method to sample subsets from a single-origin dataset such that the subsets correspond to distinct user-defined distributions. To provide some additional detail, we shall first discuss the general case, and then move on to discuss how we implemented this in D-Struct.\n\nF.1 THE GENERAL WAY\n\nA high-level view on our subsampling routine is provided in Fig. 3. From Fig. 3 we learn that we need two ingredients for our subroutine to work:\n\n1. We need a dataset that spans some domain\n\ncalculating the maximum and minimum value of each dimension in a simple dataset in Fig. 3a.\n\nX\n\n. We can retrieve this domain simply by . We have illustrated\n\nX\n\n2. We need a set of K distinct distributions that span\n\non these, besides them being different from one another, and each region in non-zero probability of being sampled. This is illustrated in Fig. 3b.\n\nX\n\n. In principle there is no constraint having a\n\nX\n\nUsing the above two ingredients, we create K empty subsets. For each subset, we then define one distribution, illustrated in Fig. 3b. In Fig. 3 we used a Gaussian for each subset as they span the domain, and are simple to evaluate. Using these distributions, we will fill each subset using data from Fig. 3a. Each data point in our dataset is evaluated K times: using the user-defined distributions in Fig. 3b, we either include the sample in the corresponding subset, or not. When the probability of being sampled is high enough, it is included, when it is not high enough, it is excluded. High enough could be determined by something simple as a threshold, or something less parametric as a Bernoulli experiment. When finished, the subsamples look like Fig. 3c.\n\nAlas, Gaussian distributions become more difficult to handle with increasing dimensionality as data is spread sparser in high dimensions. The provided high-level example may serve well as a (visual) explanation of our subroutine, it does not work well in practice. As such, we used a different implementation for D-Struct, which we explain in Section 3.2, and in more detail below.\n\nF.2 HOW IT’S IMPLEMENTED IN D-STRUCT\n\nRecall that the main issue with the simple Gaussian implementation above is that it does not scale well to high-dimensions. As such, we need a different implementation that scales to highdimensions.\n\nDefining the distributions. We do this using a very simple idea: rather than sampling in covariatespace, we sample the dataset’s indices, which correspond to a sample’s covariates. However, before we do this, we need to make sure that the indices are in some way correlated with the covariates, which is not the case for a standard dataset as they are sampled i.i.d.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nTo provide some correlation between index and covariates, we first sort the covariates and reindex the dataset. This way, a smaller set of covariates now corresponds with a smaller index-value. Note that it is unimportant whether we sort descending or ascending, the only thing that matters is that there is some logical ordering.\n\nHaving an index that is correlated with the covariates allows us to define a distribution over the indices (which are one-dimensional) rather than over the covariates (which are d-dimensional). We chose the beta distribution as our user-specified distribution, where each of the K distributions is given different parameters. The advantage a beta distribution has, is their flexibility to move its density over the entire domain (contrasting Gaussian distributions which are symmetrical). This point is illustrated in Fig. 4.\n\nSampling data. Once we have defined our distributions, we can use them to sample data. As with our high-level idea in Appendix F.1, we will evaluate each data point K times to determine whether or not it should be included in each subset. However, rather than evaluating the chosen distributions using the covariates directly, we now use the index instead. Regardless of the number of dimensions we have, the index remains one-dimensional.\n\nEvaluating a sample in D-Struct is done using a Bernoulli experiment: with the beta distributions we query the probability of being sampled and provide it to a Bernoulli experiment, the outcome determines inclusion or exclusion.\n\nG CIT-BASED METHODS, SCORE-BASED METHODS AND FAITHFULNESS\n\nG.1 CIT-BASED METHODS\n\nCIT-based methods such as the well known PC-algorithm, the SGS algorithm, or the inductive causation (IC) algorithm all require faithfulness as per Def. 5. The reason is such that they render the Markov equivalence class identifiable. As we have explained in Section 3.1, using d-separation we have a one-to-one correspondence to this class of DAGs. Any query of a d-separation statement can therefore be answered by checking the corresponding conditional independence test [15].\n\nMost CIT-based methods have 2 main phases, based on a set of conditional independence statements. Assuming the latter is a correct set (that is, we have correctly inferred all the independence statements present in P ), we first infer a skeleton graph, and then orient the edges. After these two phases, we have either a fully identified DAG, or a Markov equivalence graphs in case there are edges we were not able to orient.\n\nX\n\nPhase 1: inferring a skeleton. Based on lem. 1 (below) introduced in Verma and Pearl [75], the SGS and IC algorithm build a skeleton from a completely unconnected graph.\n\nLemma 1. The following two statements hold:\n\n(i) Two nodes X and Y in a DAG ( separated by any subset\n\nX X, Y\n\n,\n\nS ⊂ X \\ {\n\n(ii) If two nodes X and Y in a DAG ( d-separated by either PaX or PaY .\n\n) are adjacent iff they cannot be d-\n\n) are not adjacent, then they are\n\n,\n\nE\n\nE .\n}\n\nX\n\nClearly, by using above lemma, SGS [74] and IC [28] require faithfulness. Contrasting methods that build from an unconnected graph, is the PC-algorithm which does the reverse: PC starts with a fully connected graph and step-by-step removes edges when they violate (ii) in lem. 1. While a different approach, both require d-separation, i.e. this too requires faithfulness to hold!\n\nPhase 2: orienting the edges. As per Meek [39] there exists a set of graphical rules that is shown complete to correctly orient the edges based only on d-separation. Of course, this requires a complete set of correct independence statements which is arguably a much stricter assumption than faithfulness.\n\nEssentially, we can relax the assumption of a complete set of independencies, but we’ll have to replace it with other assumptions. One such example is assuming a P to be Gaussian (which is also quite strict, but it serves our example). With the latter assumption, we can test for partial correlation [34, Appendices A.1 and A.2], which allows to identify the underlying Markov equivalence class\n\nX\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n[82]. Furthermore, by additionally assuming a condition called strong faithfulness [83, 84], we have uniform consistency [82]. We refer to Peters et al. [34, Ex. 7.9] for an example.\n\nG.2\n\n(DIFFERENTIABLE) SCORE-BASED METHODS\n\nContrasting CIT-based methods, are score-based methods. Score-based methods generalise our differentiable score based methods and non-differentiable methods. Contrasting CIT-based methods, which directly encode the independence statements governing P , a score-based method will on how well it fits the observed data. The rationale behind these score-based methods is evaluate that wrongly encoded independence statements will yield poor model fits [85, 86].\n\ninto\n\nG\n\nG\n\nX\n\nWe can formalise a score-based method as a function S which is to be optimised over candidate DAGs:\n\nˆ G\n\n:= arg max\n\nDAG over\n\nG\n\nD∈X\n\nS(\n\nD\n\n).\n\n,\n\nG\n\nAs such, there are two elements that comprise a score-based method: (i) the function S, and (ii) the way we optimise S. In our case, that is:\n\n(i) S corresponds to eq. (5), which is in large part determined by the underlying DSF through\n\nDSF.\n\nL\n\n(ii) S is optimised using gradient-optimisation, which has proven very efficient in this problem\n\nsetting\n\nImportantly, that rationale behind these methods does not require the faithfulness assumption for them to work. The latter may lead to violations against d-separation in case faithfulness does hold. However, in Appendix E we show how we can combat against this, by also incorporating any known independencies into our graph (which does require the faithfulness assumption to hold for those independence statements) using the L-BFGS-B optimisation algorithm.\n\n28",
    "reference": "# Summary Of The Paper\n\nThe paper proposes transportability in DAG structure learning problem, which can be seen as a multi-task DAG learning problem or a cross validation method from a single task. The changes from NOTEARS methods is the addition from an average graph loss term additional to the differential scoring function. The method is compared with one baseline NOTEARS to show its superior performance in accuracy.\n\n# Strength And Weaknesses\n\nStrength:\nIt seems this is a first work to learn one single DAG from multiple domain.\nSubset construction is interesting, as it may improve the statistical property of the estimation. \n\nWeakness:\nThe technical contribution is rather limited, with a MSE terms on graphs. It would be interesting for authors to discuss the choice of such a regularization against potential other alternatives. \nOnly one baseline is compared. Understandably this may be first work, but other works could and should be adapted as a baseline. For example, the multi-task DAG learning from [62].\n\nComments:\n- subset construction: it is not fully clear how much the performance gain is from d-struct formulation, compared against an ensembled approach on different subset of data (maybe taking an average to obtain the final result with measure to ensure acyclicity).  This could also serve as a baseline.\n- While transportability definition is clear, authors did not discuss much about it with respect to other standard problem regimes. For example, how is transportability problem different from a typical multi-task learning or transfer learning for DAGs?\n- another way to improve technical contribution is to study feature differences or absences in different domains, and how to integrate them into one single DAG.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nclarity: good\n\nquality: good\n\noriginality: problem is new, but technically it is limited.\n\n# Summary Of The Review\n\nthe paper addressed a new problem in DAG learning but with straightforward technical contribution.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDRAFT, SKETCH, AND PROVE: GUIDING FORMAL THEOREM PROVERS WITH INFORMAL PROOFS\n\nAlbert Q. Jiang1,2,†\n\nSean Welleck3,4,†\n\nJin Peng Zhou5,6,†\n\nWenda Li2\n\nJiacheng Liu3\n\nMateja Jamnik2\n\nTimoth ́ee Lacroix1\n\nGuillaume Lample1,‡\n\nYuhuai Wu5,7,‡\n\n1Meta AI 5Google Research 6Cornell University 7Stanford University\n\n2University of Cambridge\n\n3University of Washington 4Allen Institute for AI\n\nABSTRACT\n\nThe formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce wellstructured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.\n\nFigure 1: Draft, Sketch, and Prove. Starting with an informal statement, our framework yields a formal proof through a three-stage process: drafting informal proofs, mapping them into formal sketches, and proving the remaining conjectures. Concretely, an informal statement is a mathematical problem described in a mixture of natural and mathematical languages (e.g., formulae in LATEX). Then, we use a large language model to autoformalize each informal proof into a formal sketch, which is a skeleton of the formal proof with open conjectures left unproven (indicated by the <proof> blocks). The formal sketch mirrors the structure of the informal proof. Finally, the open conjectures/gaps inside each formal sketch are proved by an off-the-shelf prover.\n\n†Equal contributions as leading authors. Correspondence to: qj213@cam.ac.uk. ‡Equal contributions as senior authors.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n1\n\nINTRODUCTION\n\nFormal proof automation is a challenging task that has been the focus of increased attention in recent years (Bansal et al., 2019b; Polu & Sutskever, 2020; Lample et al., 2022; Jiang et al., 2022; Wu et al., 2022). However, deep learning approaches have not been as successful as in other domains, mainly because of the scarcity of formal data. Indeed, formalizing proofs is notoriously difficult and only accessible to a handful of experts, which makes large annotation endeavors unrealistic (Wiedijk, 2008). The largest formal proof corpus is written in Isabelle (Paulson, 1994), and amounts to less than 0.6 GB in size, orders of magnitude smaller than datasets commonly used in vision (Deng et al., 2009) or natural language processing (Brown et al., 2020). To address the scarcity of formal proofs, previous studies have proposed to use synthetic data (Wu et al., 2021b), self-supervision (Polu & Sutskever, 2020; Han et al., 2022), or reinforcement learning (Bansal et al., 2019a; Polu et al., 2022) to synthesize additional formal training data. Although these methods alleviate the data insufficiency to some degree, none are able to capitalize on the bulk of human-written mathematical proofs.\n\nUnlike formal mathematics, informal mathematical data is abundant and widely available. Recently, large language models trained on informal mathematical data showcased impressive quantitative reasoning abilities (Lewkowycz et al., 2022; Welleck et al., 2022). However, they often generate erroneous proofs and it is challenging to detect the faulty reasoning in these proofs automatically. Our work devises a novel approach called Draft, Sketch, and Prove (DSP) to translate informal mathematical proofs into formal ones and thus enjoy both the logical rigor provided by formal systems and the wealth of informal data. We give a schematic diagram of the DSP method in Figure 1 and describe it in Section 3. Recent work (Wu et al., 2022) demonstrates the feasibility of automatically translating informal statements into formal ones with large language models. DSP goes beyond and leverages large language models to generate formal proof sketches (Wiedijk, 2003) from informal proofs. Proof sketches consist of high-level reasoning steps that can be interpreted by formal systems such as interactive theorem provers. They differ from complete formal proofs in that they contain sequences of intermediate conjectures without justification. An example of informal proof with its corresponding formal proof sketch is provided in Figure 2. In the last step of DSP, we elaborate the formal proof sketch into a full formal proof using an automated prover to prove all intermediate conjectures.\n\nWe perform experiments to generate formal proofs of problems from the miniF2F dataset (Zheng et al., 2022) and show that a large portion of theorems can be proved automatically with this method. We investigate two settings where the informal proofs are either written by humans or drafted by a large language model trained on mathematical text. These two settings correspond to situations frequently occurring during the formalization of existing theories, where informal proofs are usually available, but sometimes left as exercises to the reader or missing due to space limits in the margin.\n\nContributions:\n\n• We introduce a novel approach to leverage informal proofs to guide automated provers with\n\nformal proof sketches.\n\n• To evaluate our approach, we build a dataset of manually curated informal statements and informal proofs aligned with formal statements in the miniF2F dataset (Zheng et al., 2022).\n\n• We increase the proportion of problems solved by an automated prover on miniF2F from 20.9% to 38.9% given language-model-generated informal proofs, and up to 39.3% when proofs are written by humans.\n\n• Through three ablation studies, we demonstrate the performance benefit of drafting informal proofs, annotating sketches with informal segments, and using automated provers to close open conjectures for the autoformalization of proofs.\n\n2 BACKGROUND AND RELATED WORK\n\nInteractive theorem proving Modern verification systems for mathematics are centered around interactive theorem provers (ITPs), such as Isabelle (Paulson, 1994), Lean (Moura et al., 2015), Coq (Barras et al., 1997), or Metamath (Megill & Wheeler, 2019). ITPs embed the mathematical definitions and theorems onto a solid logical foundation (e.g., Higher-Order Logic, Dependent Type Theory) implemented by their kernels. Every theorem must be checked by the kernel to be recognized by the ITP. To be proved formally, a theorem is first stated in the ITP’s programming language, and\n\n2\n\nPublished as a conference paper at ICLR 2023\n\niteratively simplified into simpler objectives (or subgoals), until it can be reduced to already proven facts. In this paper, we will refer to proofs verified by a formal theorem prover as formal proofs, and proofs written in “standard” mathematics (e.g. in LATEX) as informal proofs.\n\nMachine learning for formal proof synthesis Several approaches propose to combine machine learning with modern interactive theorem provers (Yang & Deng, 2019; Gauthier et al., 2021), and build upon the recent success of language models (Polu & Sutskever, 2020; Han et al., 2022; Polu et al., 2022; Jiang et al., 2022; Lample et al., 2022). These methods typically rely on sequence-to-sequence models (Sutskever et al., 2014) to generate the next step of a proof given the current proof state and perform search over the generated subgoals using powerful search methods such as MCTS (Silver et al., 2018; Wu et al., 2021a; Laurent & Platzer, 2022). Because search is computationally expensive, these language models are relatively small (with fewer than 1 billion parameters). Our method contrasts with these approaches in that we use a significantly reduced number of calls to the models, but also much larger language models (with up to 175 billion parameters) that showcase outstanding few-shot learning abilities (Brown et al., 2020).\n\nMachine learning for informal reasoning Language models have also been used in the context of purely informal mathematics (Lample & Charton, 2020; Hendrycks et al., 2021; Welleck et al., 2021; Drori et al., 2022; Welleck et al., 2022). Nevertheless, Lewkowycz et al. (2022) note that for quantitative question answering, models are prone to generate false positives: the model guesses the right answer while providing an incorrect proof. These errors are hard to spot without human inspection. Worryingly, the frequency of false positives increases with the difficulty of the problem. Our method builds on these findings and translates informal proofs into formal proofs. Since ITPs are logically grounded, once a formal proof is checked by them, we are guaranteed its correctness.\n\nAutoformalization In a position paper, Szegedy (2020) argued for attaining formal mathematical data from informal sources with neural networks. Wang et al. (2020) performed preliminary experiments where the evaluation was limited to text-level similarities on synthetic datasets. Recently, Wu et al. (2022) found that large language models (Chen et al., 2021; Chowdhery et al., 2022) are capable of few-shot statement autoformalization. Namely, a small number of examples are enough for them to learn to perform informal-to-formal translation of statements. In this paper, we investigate whether these findings can generalize to proof autoformalization, i.e., whether large language models can be used to translate informal proofs into formal ones.\n\n3 METHOD\n\nIn this section, we describe our Draft, Sketch, and Prove (DSP) method for formal proof automation, which leverages informal proofs to guide automated formal theorem provers with proof sketches. We assume that each problem comes with an informal statement and a formal statement describing the problem. Our pipeline consists of three stages (depicted in Figure 1), which we present below.\n\n3.1 DRAFTING INFORMAL PROOFS The initial phase of the DSP method consists in finding informal proofs for a problem according to its description in natural mathematical language (possibly with LATEX). The resulting informal proof is seen as a draft for the subsequent phases. In mathematical textbooks, proofs of theorems are in general provided, but are sometimes missing or incomplete. Therefore, we consider two settings corresponding to the presence or absence of the informal proofs. In the first, we assume that a “ground-truth” informal proof (i.e., one written by a human) is available, which is the typical scenario in the practice of formalizing existing mathematical theories. In the second setting, we make a more general assumption that the ground-truth informal proof is not given, and draft proof candidates with a large language model trained on informal mathematical data. The language model removes the dependence on human proofs and can produce multiple alternative solutions for every problem. Although there is no easy way to automatically verify the correctness of these proofs, the informal proof only needs to be useful for producing a sketch in the next stage.\n\n3.2 MAPPING INFORMAL PROOFS INTO FORMAL SKETCHES A formal proof sketch encodes the structure of a solution and leaves out low-level details (Wiedijk, 2003). Intuitively, it is a partial proof that outlines high-level conjecture statements. A concrete example of a proof sketch is shown in Figure 2. Although informal proofs often leave aside low-level details, (e.g., by stating their triviality), these details cannot be discharged in a formal proof, making\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nInformal Statement: Show that for any real number a, 10a ≤ 28a2 + 1.\n\nInformal Proof: It suffices to show 0 ≤ 28a2 − 10a + 1. First, consider completing the square for 28a2 − 10a and observe that (cid:0)a − 5 28 a+(5/28)2. Multiplying by 28 and simplifying gives 0 ≤ 28a2 − 10a + (25/28). Since 25/28 < 1, the result follows.\n\n28 a+(5/28)2. Since 0 ≤ (cid:0)a − 5\n\n(cid:1)2, we get 0 ≤ a2 − 10\n\n(cid:1)2 = a2 − 10\n\n28\n\n28\n\nFormal Proof Sketch: theorem algebra_binomnegdiscrineq_10alt28asqp1:\n\nfixes a :: real shows \"10 * a ≤ 28 * aˆ2 + 1\"\n\nproof - (* it suffices to show 0 <= 28aˆ2 - 10a + 1 *)\n\nhave c0: \"0 ≤ 28aˆ2 - 10a + 1\" proof - (* observe that (a - (5/28))ˆ2 = aˆ2 - (10/28)a + (5/28)ˆ2 *)\n\nhave c1: \"(a - (5/28))ˆ2 = aˆ2 - 10/28a + (5/28)ˆ2\" < · · · > (* we get 0 <= aˆ2 - (10/28)a + (5/28)ˆ2 *) have c2: \"0 ≤ aˆ2 - 10/28a + (5/28)ˆ2\" using c1 < · · · > (* Multiplying by 28 and simplifying gives 0 <= 28aˆ2 - 10a + (25/28) *) have c3: \"0 ≤ 28aˆ2 - 10a + 28((5/28)ˆ2)\" using c2 < · · · > have c4: \"0 ≤ 28aˆ2 - 10a + 28((5/28)*(5/28))\" using c3 < · · · > have c5: \"0 ≤ 28aˆ2 - 10a + (25/28)\" using c4 < · · · > (* Since 25/28 < 1, the result follows. *) show ?thesis using c5 < · · · >\n\nqed show ?thesis < · · · >\n\nqed\n\nFigure 2: A proof sketch in Isabelle. The problem “Show that for any real number a, 10a ≤ 28a2 + 1” is given with an informal proof and an associated formal proof sketch. The sketch first rewrites the original statement (c0), which is proved through 5 intermediary conjectures (c1..c5). We use a special token (< · · · >) to indicate that the conjecture is “open” and should be tackled by an automated prover later. To facilitate the alignment between the informal and formal languages, we annotate the formal proof sketch examples with informal proof segments (shown in red), which are immediately followed by their formal counterparts.\n\nstraightforward informal-to-formal proof translation difficult. Instead, we propose to map informal proofs to formal proof sketches that share the same high-level structures. The low-level details missing from a proof sketch can later be filled by an automated prover. Since large informal-formal parallel corpora do not exist, standard machine translation methods are unsuitable for this task. Rather, we use the few-shot learning abilities of a large language model. Specifically, we prompt the model with a few example pairs containing informal proofs and their corresponding formal sketches, followed by an informal proof yet to be translated. We then let the model generate the subsequent tokens to obtain the desired formal sketch. We refer to this model as an autoformalizer.\n\n3.3 PROVING OPEN CONJECTURES IN THE SKETCHES As the last part of the process, we execute off-the-shelf automated provers to fill in the missing details in proof sketches, where “automated provers” refers to systems capable of producing formally verifiable proofs. Our framework is agnostic to the specific choice of the automated prover: it can be symbolic provers such as heuristic proof automation tools, neural-network-based provers, or hybrid approaches. If the automated prover successfully closes all the gaps in the proof sketch, it returns the final formal proof which can be checked against the problem’s specification. If the automated prover fails (e.g., it exceeds the allocated time limit), we consider the evaluation to be unsuccessful.\n\n4 EXPERIMENTS\n\n4.1 DATASET AND EVALUATION\n\nWe evaluate our method on the miniF2F dataset (Zheng et al., 2022). The dataset contains the formal statements of 488 problems from high-school mathematical competitions, written in three formal languages: Lean, HOL-Light, and Isabelle. They are split into a valid set and a test set, composed\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nof 244 problems each. In this work, we choose to experiment with Isabelle for three reasons: (1) Isabelle’s proof corpus is one of the largest among interactive theorem provers, conducive to the language models’ mastery of its syntax; (2) Isabelle supports the declarative proof style (detailed discussion in Appendix A), enabling formal proof sketches (Wiedijk, 2003) which are central to our method; (3) although automated proving tools are available in other interactive theorem provers, none are as developed and effective as Sledgehammer (Paulson, 2010) in Isabelle for proving conjectures.\n\nThe miniF2F dataset is comprised of problems from three source categories: (1) 260 problems sampled from the MATH dataset (Hendrycks et al., 2021); (2) 160 problems from actual high-school mathematical competitions (AMC, AIME, and IMO); (3) 68 crafted problems at the same difficulty level as (2). We employ three methods to obtain informal statements and proofs from these sources. For source (1), we access the informal statements and proofs from the MATH dataset; for (2), we retrieve their informal statements and proofs from the AOPS website 1; and for (3), we manually write down their informal statements and proofs. Thus we gather a parallel set of 488 informal statements, informal proofs, and formal statements. This dataset provides the informal statements and proofs for our experiment in the human-as-informal-proof-writer setting and will be released upon publication.\n\nOur task is to generate formal proofs for problems as they are formally stated in miniF2F. We consider a proof valid if and only if it (a) does not contain “cheating” keywords (sorry and oops) that exit a proof without completing it, and (b) Isabelle is able to verify the corresponding formal statement with the proof. We use the Portal-to-ISAbelle API by Jiang et al. (2021) to interact with Isabelle.\n\n4.2 BASELINES\n\nSledgehammer As a baseline, we attempt to prove the formal statement directly with Sledgehammer, a popular proof automation tool in Isabelle. We use the default Sledgehammer configuration in Isabelle2021, including a 120-second timeout and the five automated theorem provers (Z3, CVC4, SPASS, Vampire, E). Appendix B gives a more thorough introduction to Sledgehammer.\n\nSledgehammer + heuristics Occasionally, Sledgehammer may fail without trying simple yet effective tactics. As a second, stronger baseline, we create an automated prover that tries 11 common tactics (auto, simp, blast, fastforce, force, eval, presburger, sos, arith, linarith, auto simp: field simps) for high-school level algebra and number theory problems. If every attempted tactic fails, or times out after 10 seconds, it falls back to Sledgehammer.\n\nLanguage models for proof search Finally, we include baselines which are representative of state-of-the-art neural theorem proving in Isabelle, specifically Thor (Jiang et al., 2022) and Thor with expert iteration on autoformalized data (Wu et al., 2022). The methods GPT-f with expert iteration (Polu et al., 2022), and HyperTree Proof Search (HTPS) (Lample et al., 2022) can solve 36.6% and 41.0% of the problems on miniF2F-test. However, they rely on the Lean theorem prover instead of Isabelle, which greatly influences the performance due to the different tactics and automation, and are not directly comparable to our method.\n\n4.3 EXPERIMENTAL SETUP\n\nThe experimental code is at github.com/albertqjiang/draft sketch prove.\n\nDrafting When informal proofs are generated, we condition a large language model on informal statements to sample 100 informal proofs per problem. Specifically, we use the Codex code-davinci002 model (Chen et al., 2021) through the OpenAI API, and the 8B, 62B, and 540B versions of the Minerva model from Lewkowycz et al. (2022). We use greedy decoding for Codex and nucleus sampling (Holtzman et al., 2019) with temperature T = 0.6 and top p = 0.95 for Minerva models.\n\nSketching For sketching, we manually prepare 20 autoformalization examples of the format (informal statement, informal proof, formal statement, formal sketch), to form a pool of high-quality demonstrations. Of these 20 examples, 10 are of the algebra type and 10 are of the number theory type. All examples are from the validation set of the miniF2F dataset and can be found in the supplementary materials. The sketches contain in-line comments as in Figure 2. If the name of the problem gives away its type (algebra or number theory), we only use examples of the corresponding type. We also ensure that the sampled few-shot examples do not contain the problem being solved. The prompt is 3 uniformly randomly sampled example from the pool concatenated with the current problem’s (informal statement, informal proof, formal statement). We use this prompt to query the\n\n1https://artofproblemsolving.com/community\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Proving success rates on the miniF2F dataset with Isabelle In the table are the success rates of four baselines, the DSP method with human and language model informal proofs, as well as three ablation studies, on the validation and the test sets of miniF2F. The highest success rates on each set are highlighted in bold. The performance difference between ablation studies and DSP with human informal proofs are enclosed in brackets.\n\nSuccess rate\n\nBaselines\n\nminiF2F-valid\n\nminiF2F-test\n\nSledgehammer Sledgehammer + heuristics Thor (Jiang et al., 2022) Thor + expert iteration (Wu et al., 2022)\n\nDraft, Sketch, and Prove\n\nHuman informal proof Codex informal proof 8B Minerva informal proof 62B Minerva informal proof 540B Minerva informal proof\n\n9.9% 18.0% 28.3% 37.3%\n\n42.6% 40.6% 40.6% 43.9% 42.6%\n\n10.4% 20.9% 29.9% 35.2%\n\n39.3% 35.3% 35.3% 37.7% 38.9%\n\nAblations (with human informal statements and proofs)\n\n– In-line comments – Informal proofs – Automated provers\n\n37.7% (−4.9%) 38.9% (−3.7%) 32.8% (−9.8%)\n\n36.5% (−2.8%) 34.0% (−5.3%) 30.3% (−9.0%)\n\nsame Codex model to get the desired proof sketches. We use deterministic greedy decoding and a maximum of 2048 tokens in the generated sequence. For all the experiments, unless stated otherwise, we control the total number of queries made to Codex per problem to be 100. This means 100 queries per human informal solution and one query per language-model-generated solution.\n\nProving To prove the conjectures left open by the formal sketch, we use the Sledgehammer + heuristics automated prover described in Subsection 4.2. We execute the automated prover on every open conjecture in the sketch to synthesize a formal proof that can be verified by Isabelle.\n\n4.4 RESULTS\n\nIn Table 1, we display the proportion of successful formal proofs found on the miniF2F dataset with the interactive theorem prover Isabelle. The results include the four baselines described in Subsection 4.2 and the DSP method with human-written proofs and model-generated proofs. From the table, we can see that the automated prover with 11 additional heuristic tactics significantly increases the performance of Sledgehammer, boosting its success rate from 9.9% to 18.0% on the validation set of miniF2F and from 10.4% to 20.9% on the test set. The two baselines using language models and proof search (Thor and Thor + expert iteration) achieve success rates of 29.9% and 35.2% on the test set of miniF2F, respectively.\n\nWith informal proofs written by humans, the DSP method achieves success rates of 42.6% and 39.3% on the validation and test sets of miniF2F. A total of 200 out of 488 problems can be proved in this way. The Codex model and the Minerva (8B) model give very similar results in solving problems on miniF2F: they both guide the automated prover to solve 40.6% and 35.3% of problems on the validation and the test sets respectively. This is corroborated by Lewkowycz et al. (2022)’s observation that these two models have comparable performances in solving mathematical problems.\n\nWhen we switch to the Minerva (62B) model, the success rates rise up to 43.9% and 37.7% respectively. Compared to human-written informal proofs, its success rates are 1.3% higher on the validation set and 1.6% lower on the test set. In total, the Minerva (62B) model is able to solve 199 problems on miniF2F, one fewer than with human proofs. The DSP method is effective in guiding the automated prover under both settings that we study: using either human informal proofs or language-model-generated informal proofs. DSP almost doubles the prover’s success rate and results in a new state-of-the-art performance on miniF2F with Isabelle. Moreover, the larger Minerva model is almost as helpful as a human in guiding the automated prover in solving problems.\n\n5 ANALYSIS\n\n5.1 ABLATION STUDIES Ablation of in-line comments To facilitate the alignment between the informal proofs and the formal proof sketches, we copy relevant segments of the informal proofs as in-line comments in\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Number of problems solved on miniF2F against the number of autoformalization attempts per problem. Left: The figure displays the experiments carried out with the DSP method and three ablations on it. The curves represent the DSP method (blue), formal proof sketches without the in-line comments (orange), without informal proofs altogether (green), and without the automated provers (red). Right: The figure compares the experimental results with informal proof drafts written by humans (blue), the 540B Minerva model (orange), the 62B Minerva model (green), the 8B Minerva model (red), and the Codex model (purple).\n\nthe sketches. In the manually constructed prompt examples, these comments are prefixed to the corresponding Isabelle code blocks, as shown in Figure 2 (the text in red). We hypothesize that this technique is beneficial for large language models to synthesize formal sketches. To validate this hypothesis, we perform an ablation study by removing the in-line comments in the prompt examples before running the experiment. The results are displayed in Table 1. We find that without in-line comments, the success rates drop by 4.9% and 2.8% on the validation and test sets respectively. We conclude that having in-line comments is helpful for generating formal proof sketches.\n\nAblation of informal proof drafts Drafting informal proofs is the first step of the DSP method. To investigate the necessity of this step, we perform an experiment of formal sketching and proving without informal proofs at all. Because formal proof sketches are written in the declarative proof style, they are fairly similar to the informal proof drafts already. Concretely, we remove the informal proofs and the in-line comments (because they are copied segments of the informal proofs) in the prompt examples. This removes the need for the informal proof writer, whether a human or a neural network. The results of this setup are shown in Table 1. It can be seen that the success rates on the validation and the test sets of miniF2F drop by 3.7% and 5.3% respectively compared to with human-written proofs. They are also inferior to success rates obtained with language-model-generated informal proofs. This demonstrates the importance of drafting informal proofs before sketching and proving.\n\nAblation of automated provers Using an autoformalizer to generate proof sketches which are then completed by an automated prover is central to our method. The effect of utilizing an automated prover to close open conjectures in proof sketches is worth studying, so we conduct an ablation experiment for it. Namely, we replace the proof sketches in the prompt examples with complete formal proofs. The complete formal proofs still follow the declarative proof style, but do not contain any open conjectures. As a result, the large language model will also generate full proofs instead of sketches, and we directly check whether these generated proofs are valid. The results in this setup are presented in Table 1. The results reveal that without an automated prover to close open conjectures, the success rate on miniF2F decreases by 9.8% and 9.0% on the validation and test sets respectively. The drastic performance difference indicates the essential role of automated provers in our approach.\n\nScaling properties of ablation studies To understand the effect of the ablations on the DSP method’s scaling properties, we vary the number of autoformalization attempts per problem and plot the number of successful proofs found on the miniF2F dataset in Figure 3 (left). Three methods are contrasted: the original DSP method with human informal proofs, the DSP method without in-line comments, and the DSP method without sketching. It can be seen from the figure that with the original DSP method, the performance reaches a plateau (no new proofs are found) after 70 autoformalization attempts are made for each problem. For the ablation study with no in-line comments, the plateau is reached much faster, after around 50 autoformalization attempts. This method solves 181 problems in total. The ablation study without sketching can solve 154 problems on miniF2F. In comparison, with human informal proofs, only 7 autoformalization attempts are required to reach this performance.\n\n7\n\n020406080100#Autoformalization Attempts Per Problem50100150200#Successful ProofsMiniF2F Problems Solved (out of 488)DSP with human proofsAblation: no in-line commentsAblation: no informal proofsAblation: no automated provers020406080100#Autoformalization Attempts Per Problem50100150200#Successful ProofsMiniF2F Problems Solved (out of 488)Human informal proof draftsMinerva (540B) proof draftsMinerva (62B) proof draftsMinerva (8B) proof draftsCodex proof draftsPublished as a conference paper at ICLR 2023\n\nStatement: Prove that the fraction 21n+4\n\n14n+3 is irreducible for every natural number n.\n\nInformal Proof (Minerva 62B): We must show that gcd(21n + 4, 14n + 3) = 1. The Euclidean algorithm gives\n\n21n + 4 = 1 · (14n + 3) + 7n + 1\n\n14n + 3 = 2 · (7n + 1) + 1.\n\nSince gcd(7n + 1, 1) = 1, we have gcd(21n + 4, 14n + 3) = 1.\n\nFormal Proof: theorem imo_1959_p1:\n\nfixes n :: nat shows \"gcd (21*n + 4) (14*n + 3) = 1\"\n\nproof -\n\n(* The Euclidean algorithm gives 21n+4=1\\cdot(14n+3)+7n+1 14n+3=2\\cdot(7n+1)+1. *) have c0: \"21*n + 4 = 1*(14*n + 3) + 7*n + 1\" <ATP> by auto </ATP> have c1: \"14*n + 3 = 2*(7*n + 1) + 1\" using c0 <ATP> by auto </ATP> (* Since \\gcd(7n+1,1)=1, we have \\gcd(21n+4,14n+3)=1. *) then have \"gcd (7*n + 1) 1 = 1\"\n\nusing c1 <ATP> by auto </ATP>\n\nthen have \"gcd (21*n + 4) (14*n + 3) = 1\"\n\nusing c1 <ATP> by (smt (z3) BitM_plus_one ab_semigroup_add_class.add_ac(1) add.assoc c0 gcd.commute gcd_add2 gcd_add_mult mult_numeral_1 numeral_One numeral_eq_Suc numerals(1) semiring_norm(3)) </ATP>\n\nthen show ?thesis using c1 <ATP> by blast </ATP>\n\nqed\n\nFigure 4: IMO proof guided by a Minerva informal proof An informal proof of the International Math Olympiad problem imo 1959 p1 generated by Minerva that leads to a successful formal proof. The steps enclosed by the ATP delimiters are generated by an automated prover and all other steps are generated by the DSP autoformalizer.\n\n5.2 LANGUAGE-MODEL-GENERATED PROOFS\n\nOur experiments demonstrated that model-generated informal proofs from Minerva and Codex can help guide a formal theorem prover. In this section, we analyze the properties of these proofs further. Since the Minerva (62B and 540B) models give the best overall performance on miniF2F, we focus on the informal proofs they produce in this section.\n\nMinerva helps solve one IMO problem Interestingly, our approach manages to solve one problem from the International Mathematical Olympiad (imo 1959 1) with a Minerva-generated solution, but not with the human proof. For this problem, we present the successful Minerva-generated informal proof draft and the formal proof in Figure 4. We hypothesize that the reason behind this phenomenon is that human proofs might leave gaps between conjectures that are too difficult for automated provers to solve. On the other hand, the diversity in language model informal proofs makes some of them more amenable to automated provers. In Appendix C, we analyze the human and the Minerva informal proofs for this problem in greater detail. In Appendix D, we present a manual evaluation of Minerva proofs, and 3 more case studies comparing the human and Minerva informal proofs.\n\nIs there a way to detect which Minerva proofs are correct, without human evaluation? For a preliminary investigation, we filter out all the problems that can be solved directly with the automated prover from the 50 and are left with 27 informal proofs. Of these 27, 21 are completely correct, 6 still contain small errors, but none are nonsensical. With this simple filter, we achieve a precision of 77.8% and a recall of 72.4% in identifying correct Minerva informal proofs.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nScaling properties of human and Minerva proofs To understand the influence of different informal proof sources on the scaling properties of DSP, we plot the number of successful proofs found on miniF2F against the number of autoformalization attempts per problem in Figure 3 (right). Note that for each problem, we have 1 informal proof by a human and 100 informal proof drafts by each language model. The one human proof is used 100 times for formal proof sketch generation, while each language model proof draft is used only once. The 62B and the 540B models result in more successful proofs than the smaller (8B) Minerva model and the Codex model, consistently for any number of attempts. The 8B Minerva model and the Codex model behave similarly, both finding 185 proofs in the end. Informal proofs written by humans help solve more problems than those by Minerva models for 1 − 100 autoformalization attempts. However, the difference is small (1 problem) when 100 are made.\n\nNoticing that the number of successful proofs does not plateau for the Minerva-generated proofs, we investigate how further increasing the number of autoformalization attempts changes the number of problems solved for human-written and language-model-generated proofs. For each problem, we use 1 human informal proof and sample 200 sketches for it; we also use the same 100 informal proof drafts by the Minerva (540B) language model and sample 2 sketches for each draft. The total number of sketches per problem is 200 in both settings. We find that with human informal proofs, 203 theorems (106/97 on valid/test) have successful formal proofs, while with language-model-generated informal proofs, 209 (111/98 on valid/test) theorems have successful formal proofs after the same number of attempts. This suggests that the diversity in language-model-generated informal proofs can benefit the automated formalization process more than the “ground-truth” human proofs.\n\n5.3 MEMORIZATION\n\nThis work utilizes two language models that have been trained on a large amount of internet data. Several prior works (Trinh & Le, 2018; Carlini et al., 2022) pointed out that such models can memorize some fraction of the data they encounter during training. For drafting informal proofs, we mainly experimented with Minerva. Lewkowycz et al. (2022, Section 5) discussed the memorization effects within Minerva and concluded that they could not find evidence that its abilities are due to memorization. For the autoformalization of proof sketches, the Codex (code-davinci-002) model was used. Its training data was collected before June 20212, at which time the miniF2F dataset had not been made public. So the model cannot benefit from memorizing the exact problems and proofs. Therefore, it is inappropriate to attribute the abilities of models used in this paper to memorization.\n\n6 CONCLUSION\n\nIn this paper, we introduced Draft, Sketch, and Prove (DSP), a novel approach that takes advantage of informal proofs to synthesize formal proofs. We demonstrated its feasibility and effectiveness by reaching state-of-the-art performance on the miniF2F dataset with the Isabelle theorem prover. Central to our method are formal proof sketches that mirror the high-level reasoning structures of informal proofs. Our ablations showed that the ability to automatically convert informal proofs to proof sketches is critical to the success of DSP.\n\nOur DSP method differs fundamentally from previous applications of machine learning to formal proof synthesis in two aspects. Firstly, while most approaches in the field focus on improving proof search, our method seeks to construct the entire formal proof structure from the informal proof in one decoding operation. The task of the automated prover is then simplified to filling the gaps between intermediate conjectures. Secondly, while existing approaches operate exclusively on formal data, DSP by design benefits from informal proofs.\n\nIn this work, we utilized a purely symbolic automated prover to close the gaps in proof sketches. In the future, we aim to equip DSP with more powerful mechanisms, such as HyperTree Proof Search (Lample et al., 2022), to broaden the scope of provable theorems. Similar to AlphaCode (Li et al., 2022), we found that the number of generations is crucial for performance. The computational cost of the autoformalizer being a bottleneck in our method, we seek to develop approaches able to generate high-quality proof sketches more efficiently.\n\n2https://beta.openai.com/docs/models/codex-series-private-beta\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe thank Rui Yuan and Kunhao Zheng for helping with the informal solutions used in our dataset. We thank Christian Szegedy for his feedback on the early draft.\n\nFUNDING DISCLOSURE\n\nAQJ and WL are supported by the ERC Advanced Grant ALEXANDRIA (Project GA 742178).\n\nREFERENCES\n\nKshitij Bansal, Sarah M. Loos, Markus N. Rabe, and Christian Szegedy. Learning to reason in large theories without imitation. CoRR, abs/1905.10501, 2019a. URL http://arxiv.org/abs/ 1905.10501.\n\nKshitij Bansal, Sarah M. Loos, Markus N. Rabe, Christian Szegedy, and Stewart Wilcox. Holist: An environment for machine learning of higher order logic theorem proving. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 454–463. PMLR, 2019b. URL http://proceedings. mlr.press/v97/bansal19a.html.\n\nBruno Barras, Samuel Boutin, Cristina Cornes, Judica ̈el Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et al. The Coq proof assistant reference manual: Version 6.1. PhD thesis, Inria, 1997.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram`er, and Chiyuan Zhang. Quantifying memorization across neural language models. CoRR, abs/2202.07646, 2022. URL https://arxiv.org/abs/2202.07646.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nIddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32):e2123433119, 2022.\n\nThibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana Kumar, and Michael Norrish. Tactictoe:\n\nlearning to prove with tactics. Journal of Automated Reasoning, 65(2):257–286, 2021.\n\nJesse Michael Han, Jason Rute, Yuhuai Wu, Edward W. Ayers, and Stanislas Polu. Proof artifact co-training for theorem proving with language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\n\ndegeneration. arXiv preprint arXiv:1904.09751, 2019.\n\nAlbert Q. Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. LISA: Language models of Isabelle\n\nproofs. In 6th Conference on Artificial Intelligence and Theorem Proving, 2021.\n\nAlbert Q. Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzyg ́ozdz, Piotr Milos, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers. CoRR, abs/2205.10893, 2022. doi: 10.48550/arXiv.2205.10893. URL https://doi.org/10.48550/arXiv.2205.10893.\n\nGuillaume Lample and Franc ̧ois Charton. Deep learning for symbolic mathematics. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=S1eZYeHFDS.\n\nGuillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aur ́elien Rodriguez, and Timoth ́ee Lacroix. Hypertree proof search for neural theorem proving. CoRR, abs/2205.11491, 2022. doi: 10.48550/arXiv.2205.11491. URL https://doi. org/10.48550/arXiv.2205.11491.\n\nJonathan Laurent and Andr ́e Platzer. Learning to find proofs and theorems by learning to refine search strategies. CoRR, abs/2205.14229, 2022. doi: 10.48550/arXiv.2205.14229. URL https: //doi.org/10.48550/arXiv.2205.14229.\n\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. CoRR, abs/2206.14858, 2022. doi: 10.48550/arXiv.2206.14858. URL https: //doi.org/10.48550/arXiv.2206.14858.\n\nYujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R ́emi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. CoRR, abs/2203.07814, 2022. doi: 10.48550/arXiv.2203.07814. URL https://doi.org/10.48550/arXiv.2203.07814.\n\nNorman D. Megill and David A. Wheeler.\n\nMetamath:\n\nfor Mathematical Proofs. http://us.metamath.org/downloads/metamath.pdf.\n\nLulu Press, Morrisville, North Carolina,\n\nA Computer Language 2019.\n\nLeonardo de Moura, Soonho Kong, Jeremy Avigad, Floris van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In International Conference on Automated Deduction, pp. 378–388. Springer, 2015.\n\nLawrence C. Paulson. Isabelle - A Generic Theorem Prover (with a contribution by T. Nipkow), volume 828 of Lecture Notes in Computer Science. Springer, 1994. ISBN 3-540-58244-4. doi: 10.1007/BFb0030541. URL https://doi.org/10.1007/BFb0030541.\n\nLawrence C. Paulson. Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers. In Renate A. Schmidt, Stephan Schulz, and Boris Konev (eds.), Proceedings of the 2nd Workshop on Practical Aspects of Automated Reasoning, PAAR-2010, Edinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pp. 1–10. EasyChair, 2010. doi: 10.29007/tnfd. URL https://doi.org/10.29007/tnfd.\n\nStanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.\n\nCoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\n\nStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. CoRR, abs/2202.01344, 2022. URL https://arxiv.org/abs/2202.01344.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419): 1140–1144, 2018.\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\n\nAdvances in neural information processing systems, 27, 2014.\n\nDonald Syme. DECLARE: A prototype declarative proof system for higher order logic. Citeseer,\n\n1997.\n\nChristian Szegedy. A promising path towards autoformalization and general artificial intelligence. In Christoph Benzm ̈uller and Bruce R. Miller (eds.), Intelligent Computer Mathematics - 13th International Conference, CICM 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings, volume 12236 of Lecture Notes in Computer Science, pp. 3–20. Springer, 2020. doi: 10.1007/978-3-030-53518-6\\ 1. URL https://doi.org/10.1007/978-3-030-53518-6_1.\n\nTrieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. CoRR, abs/1806.02847,\n\n2018. URL http://arxiv.org/abs/1806.02847.\n\nQingxiang Wang, Chad E. Brown, Cezary Kaliszyk, and Josef Urban. Exploration of neural machine translation in autoformalization of mathematics in mizar. In Jasmin Blanchette and Catalin Hritcu (eds.), Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, CPP 2020, New Orleans, LA, USA, January 20-21, 2020, pp. 85–98. ACM, 2020. doi: 10.1145/3372885.3373827. URL https://doi.org/10.1145/3372885.3373827.\n\nSean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id=Jvxa8adr3iY.\n\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models. CoRR, abs/2205.12910, 2022. doi: 10.48550/arXiv.2205.12910. URL https://doi.org/10.48550/arXiv.2205.12910.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nFreek Wiedijk. Formal proof sketches. In Stefano Berardi, Mario Coppo, and Ferruccio Damiani (eds.), Types for Proofs and Programs, International Workshop, TYPES 2003, Torino, Italy, April 30 - May 4, 2003, Revised Selected Papers, volume 3085 of Lecture Notes in Computer Science, pp. 378–393. Springer, 2003. doi: 10.1007/978-3-540-24849-1\\ 24. URL https: //doi.org/10.1007/978-3-540-24849-1_24.\n\nFreek Wiedijk. Formal proof – getting started. Notices of the American Mathematical Society, 55:\n\n1408–1414, 2008.\n\nMinchao Wu, Michael Norrish, Christian Walder, and Amir Dezfouli. Tacticzero: Learning to prove theorems from scratch with deep reinforcement learning. Advances in Neural Information Processing Systems, 34:9330–9342, 2021a.\n\nYuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Baker Grosse. INT: An inequality benchmark In International Conference on Learning\n\nfor evaluating generalization in theorem proving. Representations, 2021b. URL https://openreview.net/forum?id=O6LPudowNQm.\n\nYuhuai Wu, Albert Q. Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. CoRR, abs/2205.12615, 2022. doi: 10.48550/arXiv.2205.12615. URL https://doi.org/10.48550/arXiv.2205.12615.\n\nKaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In\n\nInternational Conference on Machine Learning (ICML), 2019.\n\nKunhao Zheng, Jesse Michael Han, and Stanislas Polu. miniF2F: a cross-system benchmark In The Tenth International Conference on Learning for formal olympiad-level mathematics. Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=9ZPegFuFTFv.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA CONJECTURES AND THE DECLARATIVE PROOF STYLE\n\nInteractive theorem provers such as Isabelle and Mizar use a declarative proof style (Syme, 1997), in which a proof is interleaved with conjectures and their corresponding proofs. Syme (1997) stated that the list of conjectures in a declarative proof should be analogous to a proof sketch found in a mathematical textbook and sufficiently convincing for the reader. In practice, ITP users often prove a theorem by writing down a list of conjectures (a “formal sketch”), then attempt to find a proof of each conjecture (fill a “gap”) with an automated system.\n\nB SLEDGEHAMMER\n\nSledgehammer (Paulson, 2010) is a powerful system that automates reasoning with the interactive theorem prover Isabelle. It works by flattening the goals encoded in the higher-order logic used by Isabelle/HOL into other logics (e.g., first-order logic) which can then be fed into automated theorem provers such as E 3, CVC4 4, Z3 5, Vampire 6, and SPASS 7. If any of these automated theorem provers succeeds in finding the proof in their own corresponding format, Sledgehammer reconstructs the proof in Isabelle/HOL with certified provers (metis, meson, and smt), which is relatively more interpretable by humans.\n\nAs a practical example of using Sledgehammer, one can declare a conjecture in Isabelle/HOL: have \"4 dvd (a::nat) =⇒ 2 dvd a\" and call Sledgehammer immediately afterwards. If Sledgehammer succeeds, it will return a proof step that proves the conjecture. In this example, the step is by (meson dvd trans even numeral), which uses the meson resolution prover and two facts: that the division relation is transitive and that 4 is an even number. If Sledgehammer does not find the proof or timeouts, it will report failure.\n\nC A PROOF TO AN INTERNATIONAL MATHEMATICAL OLYMPIAD PROBLEM\n\nWith the Minerva-generated solutions, a proof to the problem imo 1959 p1 is discovered. This is the first problem of the first ever International Mathematical Olympiad (IMO). The informal problem statement, Minerva-generated informal solution, and DSP’s formal proof are shown in Figure 4.\n\nIn Figure 4, we can see that the autoformalizer in DSP (a large language model), copies over parts of the informal proof generated by Minerva as in-line comments to precede their corresponding formal proof blocks. The formal proof does not use the first sentence of the informal proof solution as it is already identical to the formal statement. We also notice that the large language model selects relevant premises after writing down the conjectures (the steps starting with using) despite not every premise is strictly needed.\n\nThe formal proof creates 5 conjectures (4 have statements and 1 show statement) which are all subsequently proved by our automated theorem prover. The step to prove the statement have \"gcd (21*n + 4) (14*n + 3) = 1\" involves 2 verified low-level provers smt and z3 and 10 lemmas/facts from outside the scope of the language model. It is highly unlikely that either the large language model or the automated theorem prover can finish this proof on its own.\n\nUnsuccessful human-written proof. problem did not lead to a successful formal proof. The human-written proof is:\n\nIn contrast, the human-written informal proof of this IMO\n\nDenoting the greatest common divisor of a, b as (a, b), we use the Euclidean algorithm:\n\n(21n + 4, 14n + 3) = (7n + 1, 14n + 3) = (7n + 1, 1) = 1\n\n3https://wwwlehre.dhbw-stuttgart.de/ sschulz/E/E.html 4https://cvc4.github.io/index.html 5https://github.com/Z3Prover/z3 6https://vprover.github.io/ 7https://www.spass-prover.org/download/index.html\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nIt follows that 21n+4\n\n14n+3 is irreducible. Q.E.D.\n\nA key difference between the Minerva proof and the human proof is the way that invoking the Euclidean algorithm is described. The Minerva proof explicitly writes out the results of the Euclidean algorithm (e.g. 21n + 4 = 1 · (14n + 3) + 7n + 1), which are translated into the sketch (c1 in Figure 4). The human proof introduces new notation to express the results indirectly in terms of greatest common divisors, which ends up being less suitable for sketching. For example, below is a sketch generated with the human proof, which has a conjecture that is semantically incorrect and hence cannot be closed by the automated prover:\n\ntheorem\n\nfixes n :: nat shows \"gcd (21*n + 4) (14*n + 3) = 1\"\n\nproof -\n\nhave \"(21*n + 4, 14*n + 3) = (7*n + 1, 14*n + 3)\"\n\nATP (* <--- UNSUCCESSFUL *) also have \"... = (7*n + 1, 1)\"\n\nATP\n\nfinally show ?thesis\n\nATP\n\nqed\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nD MORE ANALYSIS ON HUMAN AND MINERVA INFORMAL PROOFS\n\nWe analyze the relationship between the validity of the formal proofs and the correctness of the informal proofs. For our analysis, we randomly sample 50 Minerva proofs of different problems, which are then successfully converted to formal proofs. We then manually evaluate the correctness of these 50 informal proofs. Among them, 29 proofs (58%) are entirely correct, 16 are incorrect with a clearly identifiable incorrect step, and 5 “proofs” are nonsensical and simply rephrase the final conclusions of the problems.\n\nSeeing that a total of 16 + 5 = 21 incorrect informal proofs can lead to successful formal proofs, we study how they guide the automated formal prover despite having flaws themselves. The 21 proofs divide into 2 cases: In the first case, we find 13 problems for which the informal proofs are mostly ignored, and the automated prover can find proofs by itself; In the other 8 problems, although the informal proofs are wrong, the autoformalizer manages to correct them, either by ignoring the erroneous steps or by stating their correct versions in the formal proof sketches. This suggests that the autoformalizer has some understanding of the mathematical statements and is not merely translating them from an informal language to a formal language. It is robust to slight noises in its input.\n\nWe present 3 case studies comparing the human and Minerva informal proofs. Particularly, Figure 7 shows a completely correct example and one example of each pathological case.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nStatement: Given that f + 3z = 11 and 3(f − 1) − 5z = −68, show that f = −10 and z = 7.\n\nInformal Proof (Minerva 62B): We can solve for f in the first equation, obtaining f = 11 − 3z. Then we can substitute this expression for f into the second equation, obtaining 3(11 − 3z − 1) − 5z = −68. Solving for z, we obtain z = 7. Then, we can substitute this value of z into the expression for f , obtaining f = 11 − 3 · 7 = −10.\n\nFormal Proof: theorem algebra_2varlineareq_fp3zeq11_3tfm1m5zeqn68_feqn10_zeq7:\n\nfixes f z::complex assumes h0: \"f + 3*z = 11\"\n\nand h1: \"3*(f - 1) - 5*z = -68\"\n\nshows \"f = -10 \\<and> z = 7\"\n\nproof -\n\n(* We can solve for f in the first equation, obtaining f = 11 - 3z. *) have c0: \"f = 11 - 3*z\"\n\nusing h0 by (auto simp: field_simps) [ATP]\n\n(* Then we can substitute this expression for f into the second equation,\n\nobtaining 3(11 - 3z - 1) - 5z = -68. *)\n\nhave c1: \"3*(11 - 3*z - 1) - 5*z = -68\"\n\nusing h1 c0 by auto [ATP]\n\n(* Solving for z, we obtain z = 7. *) have c2: \"z = 7\"\n\nusing c1 by auto [ATP]\n\n(* Then, we can substitute this value of z into the expression for f,\n\nobtaining f = 11 - 3 \\cdot 7 = -10.*)\n\nhave \"f = 11 - 3*7\"\n\nusing c0 c2 by auto [ATP]\n\nthen have c3: \"f = -10\"\n\nby auto [ATP]\n\nshow ?thesis\n\nusing c2 c3 by auto [ATP]\n\nqed\n\nFigure 5: Algebra example with Minerva informal proof. An informal proof generated by Minerva that led to a successful formal proof. The autoformalizer generated a proof sketch containing all lines of the formal proof except for those with [ATP]. The sketch is structured according to the informal proof, containing five intermediate conjectures based on the informal proof. The autoformalizer generated in-line comments in the proof sketch (shown in red), which correctly identified an alignment between the formal and informal proofs.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nStatement: For any integer a, show that a2 ≡ 0 (mod 4) or a2 ≡ 1 (mod 4).\n\nStatement: For any integer a, show that a2 ≡ 0 (mod 4) or a2 ≡ 1 (mod 4).\n\nInformal Proof (Minerva 62B): Let a = 2k for some integer k. Then a2 = 4k2. Since 4k2 is divisible by 4, we have a2 ≡ 0 (mod 4). Now, let a = 2k + 1 for some integer k. Then a2 = 4k2 + 4k + 1 = 4(k2 + k) + 1. Since k2 + k is an integer, 4(k2 + k) + 1 is not divisible by 4. Thus, a2 ≡ 1 (mod 4).\n\nInformal Proof (Minerva 62B): a (mod 4) ∈ {0, 1, 2, 3}. Using that for any natural number k, a ≡ b (mod 4) implies ak ≡ bk (mod 4), we have a2 (mod 4) ∈ {0, 1, 4, 9}. Since 4 ≡ 0 (mod 4) and 9 ≡ 1 (mod 4), the result follows.\n\nFormal Proof: theorem numbertheory_sqmod4in01d:\n\nFormal Proof: theorem numbertheory_sqmod4in01d:\n\nfixes a :: int shows \"(aˆ2 mod 4 = 0) \\<or> (aˆ2 mod 4 = 1)\"\n\nproof (cases \"even a\") case True\n\n(* Let a=2k for some integer k.\n\nThen aˆ2=4kˆ2. Since 4kˆ2 is divisible by 4, we have aˆ2 \\equiv 0 \\pmod{4}.*)\n\nthen obtain k where \"a=2*k\"\n\nusing evenE by auto [ATP]\n\nthen have \"aˆ2 = 4*kˆ2\"\n\nunfolding power2_eq_square by auto [ATP]\n\nthen have \"aˆ2 mod 4 = 0\"\n\nby auto [ATP] then show ?thesis by auto [ATP]\n\nnext case False\n\n(* Now, let a=2k+1 for some integer k.\n\nThen aˆ2=4kˆ2+4k+1=4(kˆ2+k)+1. Since kˆ2+k is an integer, 4(kˆ2+k)+1 is not divisible by 4. Thus, aˆ2 \\equiv 1 \\pmod{4}.*)\n\nthen obtain k where \"a=2*k+1\"\n\nusing oddE by auto [ATP]\n\nfixes a :: int shows \"(aˆ2 mod 4 = 0) \\<or> (aˆ2 mod 4 = 1)\"\n\nproof - (* a \\pmod 4 \\in {0, 1, 2, 3}. *) have c0: \"a mod 4 \\<in> {0, 1, 2, 3}\"\n\nby auto [ATP]\n\n(* Using that for any natural number k,\n\na \\equiv b \\pmod 4 implies aˆk \\equiv bˆk \\pmod 4,*)\n\nhave \"aˆ2 mod 4 = (a mod 4)\\<ˆsup>\n\n2 mod 4\"\n\nby (smt (z3) numeral_eq_Suc power_mod) [ATP] (* we have\n\naˆ2 \\pmod 4 \\in {0, 1, 4, 9}. *)\n\nalso have \"... \\<in> {0, 1, 4, 9}\"\n\nusing c0 by auto [ATP]\n\n(* Since 4 \\equiv 0 \\pmod 4 and\n\n9 \\equiv 1 \\pmod 4, the result follows. *)\n\nfinally show ?thesis\n\nby auto [ATP]\n\nqed\n\nthen have \"aˆ2 = 4*kˆ2+4*k+1\" unfolding power2_eq_square by (auto simp: field_simps) [ATP]\n\nthen have \"aˆ2 mod 4 = 1\"\n\nby presburger [ATP]\n\nthen show ?thesis by auto [ATP]\n\nqed\n\nFigure 6: Alternative proofs: Minerva (left) and human-written (right). In both proofs, the formal sketch is structured based on the informal proof. The Minerva informal proof and its sketch break the proof into even and odd cases. These cases are not explicitly stated in the informal proof, and the formal sketch makes them explicit (cases “even a”). Each case has three conjectures that are directly based on the informal proof. The sketch excludes parts that are not needed in the formal proof, for instance “Since k2 + k is an integer, 4(k2 + k) + 1 is not divisible by 4”. The human proof uses a different strategy than the Minerva proof, based on the facts that a (mod 4) ∈ {0, 1, 2, 3} and a2 (mod 4) ∈ {0, 1, 4, 9}. The sketch uses these as conjectures, synthesizes an intermediate step not in the informal proof, and excludes the last step of the informal proof.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nStatement: For each plumbing repair job, Mr. Wrench charges N dollars for coming out to the house plus x dollars per hour that he works at the house. He charged $97 for a one-hour repair job and $265 for a five-hour repair job. What is his charge for a two-hour repair job? Show that it is $139.\n\nInformal Proof (Minerva 62B): Let N be the number of dollars charged for coming out to the house and let x be the number of dollars per hour that Mr. Wrench works. For the one-hour repair job, we have that the total cost is N + x = 97. For the five-hour repair job, we have that the total cost is N + 5x = 265. We subtract the first equation from the second equation to obtain 4x = 168 ⇒ x = 42. Plugging this back into the first equation, we obtain that N = 55. Therefore, for a two-hour repair job, the total\n\ncost is 55 + 2x = 55 + 2(42) = $139 .\n\nFormal Proof: theorem mathd_algebra_160:\n\nfixes n x ::real assumes \"n + x = 97\" and \"n + 5 * x = 265\" shows \"n + 2 * x = 139\"\n\nproof -\n\n(* We subtract the first equation from the second equation to obtain 4x = 168 ⇒ x = 42. *) have \"4 * x = 168\" using assms by auto [ATP] then have \"x = 42\" by auto [ATP] (* Plugging this back into the first equation, we obtain that N = 55. *) then have \"n = 55\" using assms by auto [ATP] (* Therefore, for a two-hour repair job, the total cost is 55 + 2x = 55 + 2(42) = $139. *) then show ?thesis by (smt (z3) ⟨x = 42⟩) [ATP]\n\nqed\n\nStatement: Show that for any real number x and any natural number n, if x > −1, then (1 + nx) ≤ (1 + x)n.\n\nInformal Proof (Minerva 62B): This is true for n = 1. Now, suppose that this is true for n. Then we have that\n\n(1 + nx)n = 1 + n2x + n3x2 + · · · + nxn\n\n≤ 1 + nx + n2x2 + · · · + nxn = (1 + x)n\n\nand\n\n(1 + nx)n+1 = (1 + nx)n(1 + nx)\n\n≤ (1 + x)n(1 + x) = (1 + x)n+1.\n\nTherefore, this is true for n + 1.\n\nFormal Proof: theorem induction_1pxpownlt1pnx: fixes x :: real and n :: nat assumes \"-1 < x\" shows \"(1 + n*x) \\<le> (1 + x)ˆn\"\n\nproof (induct n) case 0 then show ?case by auto [ATP] next case (Suc n) then show ?case by (smt (z3) Bernoulli_inequality assms) [ATP] qed\n\nStatement: Determine the remainder of 1529 (mod 6). Show that it is 5.\n\nInformal Proof (Minerva 62B): By definition, 1529 ≡ 5 (mod 6).\n\nFormal Proof: theorem mathd_numbertheory_551 :\n\n\"1529 mod 6 = (5::nat)\"\n\nby auto [ATP]\n\nFigure 7: Three Types of Minerva proofs: correct proof (left), incorrect proof (right top), nonsensical proof (right bottom) In the correct Minerva proof, the formal sketch is structured based on the informal proof and steps are well-aligned. In the incorrect Minerva proof, the step ”This is true for n = 1” is corrected by Codex in the formal sketch to ”case 0” which starts the base case with n = 0 since natural numbers include 0. This is an explicit correction made by Codex and makes a slightly incorrect Minerva proof formalized successfully. Lastly, the meaningless proof contains only a single statement without any calculation or justification. However, Codex also chooses to directly show the statement without any calculation. This suggests that the problem itself could be considered simple by Codex.\n\n19",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a large language models (LLM) to automatically generate formal proofs of mathematical statements. The idea is to use Codex or Minerva model to generate an informal proof (a proof in a natural language) and then use it as a sketch to generate a formal proof. Each claim in the sketch is a some kind of proposition, which is then proved by an off-the-shelf prover, like Sledgehammer. The approach is based on the ability of LLMs to do few-shot learning. A prompt to the model contains a formal statement to be proved, generated informal statement and informal proof, and several examples of tuples <informal statement, informal proof, formal statement, formal proof>. To provide appropriate examples, the authors gathered a dataset of parallel informal-formal proofs for the problems from the MiniF2F dataset and sample 3 random examples to make the prompt.\n\nThe approach was tested on validation and test splits of the MiniF2F dataset. In the experiments, the proposed approach outperforms the baselines. The ablation study demonstrates the benefit of using sketches and aligning them with formal proofs. Also, the results show that larger models have higher proof rates.\n\n# Strength And Weaknesses\n\n+ While using LLMs for auto-formalization seems novel enough, this is the first work where LLM is applied to generate the whole proof. The idea of guiding formal proof generation with informal proof sketches is novel. The experimental section justifies that sketching indeed helps generate more correct proofs. The study in this paper demonstrates that the model makes use of the alignment between formal and informal proofs. This is the main contribution of the paper.\n\nThe paper also contains some analysis of how the prover's performance depends on the correctness of informal proofs. It turns out that the prover can fix some simple errors in informal proof and even ignore the informal proof if it is incorrect. This observation is interesting and suggests that the model can perform some kind of formal mathematical reasoning.\n\nThe paper is generally well written, experimental section demonstrates state-of-the-art results on the MiniF2F dataset, ablation study justifies the usefulness of the guidance by the informal proofs and the alignment between informal and formal proofs. The approach is built on top of existing pre-trained models and seems easy to implement if the models are available. \n\nCons:\n1. The examples play an important role and should be carefully constructed. In the paper, the authors try to use appropriate examples: first of all, all of them are from the MiniF2F dataset, secondly, whenever the problem type is known (from the problem name, e.g. it contains \"number theory\") they use the corresponding examples. So, the approach cannot be easily applied to other datasets.\n2. More analysis of the model should be conducted. Figure 3, \"Human informal proof drafts\" implies that it is also important to have good examples as different proof attempts differ only in provided examples. How close are the examples of successful proofs to the statement compared to failed proofs?\nIt is also interesting to separate the effect of chosen examples from the effect of generated informal proofs.\nFor example, we can use the same 3 examples for all proof attempts and only change the informal proofs.\n3. The method requires access to the LLM which is not widely available and limits the reproducibility of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n1. It is not completely clear how informal proofs are generated. How input prompt is constructed? Does it include examples or does it consists of just the formal statement? Is it fixed across proof attempts or each attempt uses different informal proof? It is mentioned that greedy decoding is used for the Codex model. Does it allow generating different outputs for a fixed input prompt (usually greedy decoding provides deterministic output)?\n2. It is interesting to compare the computational resources required for one proof search. While the approach makes much fewer calls to the LLM generator, we have to run Sledgehammer several times during the proof search, so it is unclear whether the approach reduces the computation time.\n3. Another interesting question here is data leakage. From the paper (Wu et al. 2022. Autoformalization with large language models) we can conclude that it is unlikely that the MiniF2F dataset was in the training dataset of Codex, but there is still a possibility that informal statements and proofs are contained in the training dataset of Codex.\n\n# Summary Of The Review\n\nThe paper introduces a novel idea to guide formal proof generation with generated informal proof using large language models in a few-shot setting. The idea was justified by the experiments and ablation study. However, more analysis of the proposed approach should be conducted to make a clear picture of what part of the approach and to what extent affects the final result.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nPROVABLY EFFICIENT RISK-SENSITIVE REINFORCEMENT LEARNING: ITERATED CVAR AND WORST PATH\n\nSiwei Wang Microsoft Research Beijing, China siweiwang@microsoft.com\n\nYihan Du Institute for Interdisciplinary Information Sciences Tsinghua University Beijing, China duyh18@mails.tsinghua.edu.cn\n\nLongbo Huang ∗ Institute for Interdisciplinary Information Sciences Tsinghua University Beijing, China longbohuang@tsinghua.edu.cn\n\nABSTRACT\n\nIn this paper, we study a novel episodic risk-sensitive Reinforcement Learning (RL) problem, named Iterated CVaR RL, which aims to maximize the tail of the reward-to-go at each step, and focuses on tightly controlling the risk of getting into catastrophic situations at each stage. This formulation is applicable to realworld tasks that demand strong risk avoidance throughout the decision process, such as autonomous driving, clinical treatment planning and robotics. We investigate two performance metrics under Iterated CVaR RL, i.e., Regret Minimization and Best Policy Identification. For both metrics, we design efficient algorithms ICVaR-RM and ICVaR-BPI, respectively, and provide nearly matching upper and lower bounds with respect to the number of episodes K. We also investigate an interesting limiting case of Iterated CVaR RL, called Worst Path RL, where the objective becomes to maximize the minimum possible cumulative reward. For Worst Path RL, we propose an efficient algorithm with constant upper and lower bounds. Finally, our techniques for bounding the change of CVaR due to the value function shift and decomposing the regret via a distorted visitation distribution are novel, and can find applications in other risk-sensitive RL problems.\n\n1\n\nINTRODUCTION\n\nReinforcement Learning (RL) (Kaelbling et al., 1996; Szepesv ́ari, 2010; Sutton & Barto, 2018) is a classic online decision-making formulation, where an agent interacts with an unknown environment with the goal of maximizing the obtained reward. Despite the empirical success and theoretical progress of recent RL algorithms, e.g., (Szepesv ́ari, 2010; Agrawal & Jia, 2017; Azar et al., 2017; Zanette & Brunskill, 2019), they focus mainly on the risk-neutral criterion, i.e., maximizing the expected cumulative reward, and can fail to avoid rare but disastrous situations. As a result, existing algorithms cannot be applied to tackle real-world risk-sensitive tasks, such as autonomous driving (Wen et al., 2020) and clinical treatment planning (Coronato et al., 2020), where policies that ensure low risk of getting into catastrophic situations at all decision stages are strongly preferred.\n\nMotivated by the above facts, we investigate Iterated CVaR RL, a novel episodic RL formulation equipped with an important risk-sensitive criterion, i.e., Iterated Conditional Value-at-Risk (CVaR) (Hardy & Wirch, 2004). Here, CVaR (Artzner et al., 1999) is a popular static (single-stage) risk measure which stands for the expected tail reward. Iterated CVaR is a dynamic (multi-stage) risk measure defined upon CVaR by backward iteration, and focuses on the worst portion of the reward-to-go at each stage. In the Iterated CVaR RL problem, an agent interacts with an unknown\n\n∗Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nepisodic Markov Decision Process (MDP) in order to maximize the worst α-portion of the rewardto-go at each step, where α ∈ (0, 1] is a given risk level. Under this model, we investigate two important performance metrics, i.e., Regret Minimization (RM), where the goal is to minimize the cumulative regret over all episodes, and Best Policy Identification (BPI), where the performance is measured by the number of episodes required for identifying an optimal policy.\n\nCompared to existing CVaR MDP model, e.g., (Boda & Filar, 2006; Ott, 2010; B ̈auerle & Ott, 2011; Chow et al., 2015), which aims to maximize the CVaR (i.e., the worst α-portion) of the total reward, our Iterated CVaR RL concerns the worst α-portion of the reward-to-go at each step, and prevents the agent from getting into catastrophic states more carefully. Intuitively, CVaR MDP takes more cumulative reward into account and prefers actions which have better performance in general, but can have larger probabilities of getting into catastrophic states. Thus, CVaR MDP is suitable for scenarios where bad situations lead to a higher cost instead of fatal damage, e.g., finance. In contrast, our Iterated CVaR RL prefers actions which have smaller probabilities of getting into catastrophic states. Hence, Iterated CVaR RL is suitable for safety-critical applications, where catastrophic states are unacceptable and need to be carefully avoided, e.g., clinical treatment planning (Wang et al., 2019) and unmanned helicopter control (Johnson & Kannan, 2002). For example, consider the case where we fly an unmanned helicopter to complete some task. There is a small probability that, at each time during execution, the helicopter encounters a sensing or control failure and does not take the scheduled action. To guarantee the safety of surrounding workers and the helicopter, we need to make sure that even if the failure occurs, the taken policy ensures that the helicopter does not crash and cause fatal damage (see Appendix C.2, C.3 for more detailed comparisons with existing risk-sensitive MDP models).\n\nIterated CVaR RL faces several unique challenges as follows. (i) The importance (contribution to regret) of a state in Iterated CVaR RL is not proportional to its visitation probability. Specifically, there can be states which are critical (risky) but have a small visitation probability. As a result, the regret for Iterated CVaR RL cannot be decomposed into the estimation error at each step with respect to the visitation distribution, as in standard RL analysis (Jaksch et al., 2010; Azar et al., 2017; Zanette & Brunskill, 2019). (ii) In Iterated CVaR RL, the calculation of estimation error involves bounding the change of CVaR when the true value function shifts to optimistic value function, which is very different from typically bounding the change of expected rewards as in existing RL analysis (Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018). Therefore, Iterated CVaR RL demands brandnew algorithm design and analytical techniques. To tackle the above challenges, we design two efficient algorithms ICVaR-RM and ICVaR-BPI for the RM and BPI metrics, respectively, equipped with delicate CVaR-adapted value iteration and exploration bonuses to allocate more attention on rare but potentially dangerous states. We also develop novel analytical techniques, for bounding the change of CVaR due to the value function shift and decomposing the regret via a distorted visitation distribution. Lower bounds for both metrics are established to demonstrate the optimality of our algorithms with respect to the number of episodes K. Moreover, we present experiments to validate our theoretical results and show the performance superiority of our algorithm (see Appendix A).\n\nWe further study an interesting limiting case of Iterated CVaR RL when α approaches 0, called Worst Path RL, where the goal becomes to maximize the minimum possible cumulative reward (optimize the worst path). This setting corresponds to the scenario where the decision maker is extremely riskadverse and concerns the worst situation (e.g., in clinical treatment planning (Coronato et al., 2020), the worst case can be disastrous). We emphasize that Worst Path RL cannot be directly solved by taking α → 0 in Iterated CVaR RL’s results, as the results there have a dependency on 1 α in both upper and lower bounds. To handle this limiting case, we design a simple yet efficient algorithm MaxWP, and obtain constant upper and lower regret bounds which are independent of K.\n\nThe contributions of this paper are summarized as follows.\n\n• We propose a novel Iterated CVaR RL formulation, where an agent interacts with an unknown environment, with the objective of maximizing the worst α-percent tail of the reward-to-go at each step. This formulation enables one to tightly control risk throughout the decision process, and is most suitable for applications where such safety-at-all-time is critical.\n\n• We investigate two important metrics of Iterated CVaR RL, i.e., Regret Minimization (RM) and Best Policy Identification (BPI), and propose efficient algorithms ICVaR-RM\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nand ICVaR-BPI. We establish nearly matching regret/sample complexity upper and lower bounds with respect to K. Moreover, we develop novel techniques to bound the change of CVaR due to the value function shift and decompose the regret via a distorted visitation distribution, which can be applied to other risk-sensitive decision making problems.\n\n• We further investigate a limiting case of Iterated CVaR RL when α approaches 0, called Worst Path RL, where the objective is to maximize the minimum possible cumulative reward. We develop a simple and efficient algorithm MaxWP, and provide constant regret upper and lower bounds (independent of K).\n\nDue to space limit, we defer all proofs and experiments to Appendix.\n\n2 RELATED WORK\n\nBelow we review the most related works, and defer a full literature review to Appendix B.\n\nCVaR-based MDPs (Known Transition). Boda & Filar (2006); Ott (2010); B ̈auerle & Ott (2011); Chow et al. (2015) study the CVaR MDP where the objective is to minimize the CVaR of the total cost, and show that the optimal policy for CVaR MDP is history-dependent (see Appendix C.2 for a detailed comparison with CVaR MDP). Hardy & Wirch (2004) firstly define the Iterated CVaR measure, and Osogami (2012); Chu & Zhang (2014); B ̈auerle & Glauner (2022) consider iterated coherent risk measures (including Iterated CVaR) in MDPs, and demonstrate the existence of Markovian optimal policies. The above works focus mainly on the planning side, i.e., proposing algorithms and error guarantees for MDPs with known transition, while our work develops RL algorithms (interacting with the environment) and regret/sample complexity results for unknown transition.\n\nRisk-sensitive Reinforcement Learning (Unknown Transition). Tamar et al. (2015); Keramati et al. (2020) study CVaR MDP with unknown transition and provide convergence analysis. Borkar & Jain (2014); Chow & Ghavamzadeh (2014); Chow et al. (2017) investigate RL with CVaR-based constraints. Heger (1994); Coraluppi & Marcus (1997; 1999) consider minimizing the worst-case cost in RL and design heuristic algorithms. Fei et al. (2020; 2021a;b) study risk-sensitive RL with the exponential utility criterion, which takes all successor states into account with an exponential In contrast, our Iterated CVaR RL primarily concerns the worst α-portion reweighting scheme. successor states, and focuses on optimizing the performance under bad situations (see Appendix C.3 for a detailed comparison).\n\n3 PROBLEM FORMULATION\n\nIn this section, we present the problem formulations of Iterated CVaR RL and Worst Path RL.\n\nConditional Value-at-Risk (CVaR). We first introduce two risk measures, i.e., Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR). Let X be a random variable with cumulative distribution function F (x) = Pr[X ≤ x]. Given a risk level α ∈ (0, 1], the VaR at risk level α is the α-quantile of X, i.e., VaRα(X) = min{x|F (x) ≥ α}, and the CVaR at risk level α is defined as (Rockafellar et al., 2000):\n\nCVaRα(X) = sup x∈R\n\n(cid:110)\n\nx −\n\n1 α\n\nE (cid:2)(x − X)+(cid:3) (cid:111)\n\n,\n\nwhere (x)+ := max{x, 0}. If there is no probability atom at VaRα(X), CVaR can also be written as CVaRα(X) = E[X|X ≤ VaRα(X)] (Shapiro et al., 2021). Intuitively, CVaRα(X) is a distorted expectation of X conditioning on its α-portion tail, which depicts the average value when bad situations happen. When α = 1, CVaRα(X) = E[X], and when α → 0, CVaRα(X) tends to min(X) (Chow et al., 2015).\n\nIterated CVaR RL. We consider an episodic Markov Decision Process (MDP) M(S, A, H, p, r). Here S is the state space, A is the action space, and H is the length of horizon in each episode. p is the transition distribution, i.e., p(s′|s, a) gives the probability of transitioning to s′ when starting from state s and taking action a. r : S × A (cid:55)→ [0, 1] is a reward function, and r(s, a) gives a deterministic reward for taking action a in state s. A policy π is defined as a collection of H functions, i.e., π = {πh : S (cid:55)→ A}h∈[H], where [H] := {1, 2, ..., H}.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nh(sk\n\nh, ak\n\nh, ak\n\nh = πk\n\nh+1 according to the transition distribution p(·|sk\n\nThe episodic RL game is as follows. In each episode k, an agent chooses a policy πk, and starts from a fixed initial state s1, i.e., sk 1 := s1, as assumed in many prior RL works (Fiechter, 1994; Kaufmann et al., 2021; M ́enard et al., 2021). At each step h ∈ [H], the agent observes the state sk h\nand takes an action ak h). After that, it receives a reward r(sk h) and transitions to a next state sk h). The episode ends after H steps and the agent enters the next episode. In Iterated CVaR RL, for any risk level α ∈ (0, 1] and a policy π, we use value function V α,π : S (cid:55)→ R and Q-value function Qα,π : S × A (cid:55)→ R to denote the cumulative reward that can be obtained when the agent transitions to the worst α-portion states at each step, starting from s and (s, a) at step h, respectively. For simplicity of notation, when the value of α is clear, we omit the superscript α and use the notations V π h and V π h are recurrently defined in Eq. (i) below. Since S, A and H are finite and the maximization of V π h (s) in Iterated CVaR RL satisfies the optimal substructure property, there exists an optimal policy π∗ which gives the optimal value V ∗ h (s) = maxπ V π h (s) for all s ∈ S and h ∈ [H] (Chu & Zhang, 2014). Therefore, the Bellman equation and the Bellman optimality equation are given in Eqs. (i),(ii) below, respectively (Chu & Zhang, 2014). h+1(s′))\n\nh. Formally, Qπ\n\ns′∼p(·|s,a)(V ∗\n\nh and Qπ\n\nQ∗\n\nh\n\nh\n\ns′∼p(·|s,a)(V π\n\nh+1(s′))\n\nQπ\n\n \n\nh(s, a) = r(s, a)+CVaRα V π H+1(s) = 0, ∀s ∈ S,\n\nh (s) = Qπ\n\nh(s, πh(s))\n\n\n\nV π\n\nh(s, a) = r(s, a)+CVaRα V ∗\n\nh (s) = max\n\nh(s, a)\n\nQ∗\n\na∈A\n\n \n\n\n\nV ∗ h+1(s′)) denotes the CVaR value of random variable V π\n\nH+1(s) = 0, ∀s ∈ S,\n\ns′∼p(·|s,a)(V π\n\nwhere CVaRα h+1(s′) with s′ ∼ p(·|s, a) at risk level α. We also provide the expanded version of value function definitions for Iterated CVaR RL (Eqs. (i), (ii)) in Appendix C.1.\n\n(ii)\n\n(i)\n\nWe consider two performance metrics for Iterated CVaR RL, i.e., Regret Minimization (RM) and Best Policy Identification (BPI). In Iterated CVaR RL-RM, the agent aims to minimize the cumulative regret in K episodes, defined as\n\nR(K) =\n\nK (cid:88)\n\nk=1\n\n(V ∗\n\n1 (s1) − V πk\n\n1 (s1)) .\n\n(1)\n\nIn Iterated CVaR RL-BPI, given a confidence parameter δ ∈ (0, 1] and an accuracy parameter ε > 0, the agent needs to use as few trajectories (episodes) as possible to identify an ε-optimal policy ˆπ, which satisfies V ˆπ 1 (s1) − ε, with probability as least 1 − δ. That is, the performance of BPI is measured by the number of trajectories used, i.e., sample complexity.\n\n1 (s1) ≥ V ∗\n\nWorst Path RL. Furthermore, we investigate an interesting limiting case of Iterated CVaR RL when α approaches 0, called Worst Path RL. In this case, the objective becomes maximizing the minimum possible reward (Heger, 1994). The Bellman (optimality) equations become\n\n \n\n\n\nQπ\n\nh(s, a) = r(s, a) + min\n\n(V π\n\nh+1(s′))\n\ns′∼p(·|s,a)\n\nQ∗\n\nh(s, a) = r(s, a) + min\n\n(V ∗\n\nh+1(s′))\n\ns′∼p(·|s,a)\n\nV ∗\n\nh (s) = max\n\na∈A\n\nQ∗\n\nh(s, a)\n\n(2)\n\n \n\n\n\nV ∗ h+1(s′)) denotes the minimum value of random variable V π\n\nH+1(s) = 0, ∀s ∈ S,\n\nh+1(s′) with s′ ∼\n\nh (s) = Qπ\n\nV π H+1(s) = 0, ∀s ∈ S,\n\nh(s, πh(s))\n\nV π\n\nwhere mins′∼p(·|s,a)(V π p(·|s, a). From Eq. (2), one sees that\n\nQπ\n\nh(s, a) = min\n\n(st,at)∼π\n\n(cid:34) H\n\n(cid:88)\n\nt=h\n\nr(st, at)\n\n(cid:12) (cid:12) (cid:12)sh = s, ah = a, π\n\n(cid:35)\n\n, V π\n\nh (s) = min\n\n(st,at)∼π\n\n(cid:34) H\n\n(cid:88)\n\nt=h\n\nr(st, at)\n\n(cid:12) (cid:12) (cid:12)sh = s, π\n\n(cid:35) .\n\nh(s, a) and V π\n\nThus, Qπ h (s) denote the minimum possible cumulative reward under policy π, starting from (s, a) and s at step h, respectively. The optimal policy π∗ maximizes the minimum possible cumulative reward (i.e., optimizes the worst path) for all starting states and steps. Formally, π∗ gives the optimal value V ∗\n\nh (s) for all s ∈ S and h ∈ [H].\n\nh (s) = maxπ V π\n\nFor Worst Path RL, in this paper we mainly consider the regret minimization setting, where the regret is defined the same as Eq. (1). Note that this case cannot be directly solved by taking α → 0 in Iterated CVaR RL, as the results there have a dependency on 1 α . Thus, changing from CVaR(·) to min(·) in Worst Path RL requires a different algorithm design and analysis.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1: ICVaR-RM Input: δ, α, δ′ := δ 1 for k = 1, 2, . . . , K do\n\n5 , L := log( KHSA\n\nδ′\n\n), ̄V k\n\nH+1(s) = 0 for any k > 0 and s ∈ S\n\n2\n\n3\n\n4\n\n5\n\nfor h = H, H − 1, . . . , 1 do\n\n ̄Qk h(s, a) ← min{r(s, a)+CVaRα h (s) ← maxa∈A ̄Qk ̄V k h(s, a), πk\n\ns′∼ ˆpk(·|s,a)( ̄V k h(s) ← argmaxa∈A\n\nh+1(s′))+ H\n\nα\n\n ̄Qk\n\nh(s, a), ∀s ∈ S;\n\n(cid:113) L\n\nnk(s,a) , H}, ∀(s, a) ∈ S×A;\n\nPlay the episode k with policy πk, and update nk+1(s, a) and ˆpk+1(s′|s, a);\n\nThe best policy identification setting of Worst Path RL, on the other hand, is very challenging. This is because we cannot establish confidence intervals under the min(·) operation, and it is difficult to determine when the estimated optimal policy is accurate enough and when the algorithm should stop. We will further investigate this setting in future work.\n\n4\n\nITERATED CVAR RL WITH REGRET MINIMIZATION\n\nIn this section, we consider regret minimization (Iterated CVaR RL-RM). We propose an algorithm ICVaR-RM with CVaR-adapted exploration bonuses, and demonstrate its sample efficiency.\n\n4.1 ALGORITHM ICVaR-RM AND REGRET UPPER BOUND\n\ns′∼ ˆpk(·|s,a)( ̄V k\n\nWe propose a value iteration-based algorithm ICVaR-RM (Algorithm 1), which adopts Browntype (Brown, 2007) (CVaR-adapted) exploration bonuses and delicately pays more attention to rare but risky states. Specifically, in each episode k, ICVaR-RM computes the empirical CVaR for the values of next states CVaRα nk(s,a) . Here nk(s, a) is the number of times (s, a) was visited up to episode k, and ˆpk(s′|s, a) is the empirical estimate of transition probability p(s′|s, a). Then, ICVaR-RM constructs optimistic Q-value function ̄Qk h(s, a). After calculating the value functions and policy, ICVaR-RM plays episode k with policy πk, observes a trajectory, and updates nk(s, a) and ˆpk+1(s′|s, a). The calculation of CVaR (Line 3) can be implemented efficiently, and costs O(S log S) computation complexity (Shapiro et al., 2021).\n\nh (s), and a greedy policy πk with respect to ̄Qk\n\nh+1(s′)) and Brown-type exploration bonuses H\n\nh(s, a), optimistic value function ̄V k\n\n(cid:113) L\n\nα\n\nWe summarize the regret performance of ICVaR-RM as follows.\n\nTheorem 1 (Regret Upper Bound). With probability at least 1−δ, the regret of algorithm ICVaR-RM is bounded by (cid:32)\n\n√\n\n(cid:40)\n\n(cid:41)\n\nO\n\nmin\n\n(cid:113)\n\n1\n\nminπ,h,s: wπ,h(s)>0 wπ,h(s)\n\n1\n\n,\n\n√\n\nαH−1\n\nHS\n\n·\n\nKHA α\n\nlog\n\n(cid:18) KHSA δ\n\n(cid:19) (cid:33) ,\n\nwhere wπ,h(s) denotes the probability of visiting state s at step h under policy π.\n\n2 and\n\nRemark 1. The regret depends on the minimum between an MDP-intrinsic visitation factor (minπ,h,s: wπ,h(s)>0 wπ,h(s))− 1 αH−1 . When α is small, the first term dominates the bound, which stands for the minimum probability of visiting an available state under any feasible policy. Note that minπ,h,s: wπ,h(s)>0 wπ,h(s) takes the minimum over only the policies under which s is reachable, and thus, this factor will never be zero. Indeed, this factor also exists in the lower bound (see Section 4.2). Thus, it characterizes the essential problem hardness, i.e., when the agent is highly risk-adverse, her regret will be heavily influenced by exploring critical but hard-to-reach states.\n\n1√\n\n1√\n\nαH−1 instead dominates the bound. The intuition behind the factor\n\nWhen α is large, αH−1 is that for any state-action pair, the ratio of the visitation probability conditioning on transitioning to bad αH−1 . This ratio is successor states over the original visitation probability can be upper bounded by critical and will appear in the regret bound (see Lemma 9 for a formal statement).\n\n1\n\n1√\n\nIn the special case when α = 1, our Iterated CVaR RL problem reduces to the classic RL formulation, and our regret bound becomes ̃O(HS KHA), which matches the result in existing classic\n\n√\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n√\n\nHS to the state-of-the-art regret bound for RL work (Jaksch et al., 2010). This bound has a gap of classic RL (Azar et al., 2017; Zanette & Brunskill, 2019). This is because our algorithm is mainly designed for general risk-sensitive cases (which require CVaR-adapted exploration bonuses), and does not use the Bernstein-type exploration bonuses (which only work for classic expectation maximization criterion). Such phenomenon also appears in existing risk-sensitive RL works (Fei et al., 2020; 2021a). Designing an algorithm which achieves an optimal regret simultaneously for both risk-sensitive cases and classic expectation maximization case is still an open problem, which we leave for future work. To validate our theoretical analysis, we also conduct experiments to exhibit the influences of parameters α, δ, H, S, A and K on the regret of ICVaR-RM in practice, and the empirical results well match our theoretical bound (see Appendix A).\n\nChallenges and Novelty in Regret Analysis. The analysis of Iterated CVaR RL faces several challenges. (i) First of all, in Iterated CVaR RL, the contribution of a state to the regret is not proportional to its visitation probability as in standard RL analysis (Jaksch et al., 2010; Azar et al., 2017; Zanette & Brunskill, 2019). Instead, the regret is influenced more by risky but hard-to-reach states. Thus, the regret cannot be decomposed into estimation error with respect to visitation distribution. (ii) Second, unlike existing RL analysis (Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018) which typically calculates the change of expected rewards between optimistic and true value functions, in Iterated CVaR RL, we need to instead analyze the change of CVaR when the true value function shifts to an optimistic value function. To tackle these challenges, we develop a new analytical technique to bound the change of CVaR due to the value function shift via conditional transition probabilities, which can be applied to other CVaR-based RL problems. Furthermore, we establish a novel regret decomposition for Iterated CVaR RL via a distorted (conditional) visitation distribution, and quantify the deviation between this distorted visitation distribution and the original visitation distribution.\n\nBelow we present a proof sketch for Theorem 1 (see Appendix D.1 for a complete proof).\n\nProof sketch of Theorem 1. First, we introduce a key inequality (Eq. (3)) to bound the change of CVaR when the true value function shifts to an optimistic one. To this end, let βα,V (·|s, a) ∈ RS denote the conditional transition probability conditioning on transitioning to the worst α-portion successor states s′, i.e., with the lowest values V (s′). It satisfies that (cid:80) s′∈S βα,V (s′|s, a) · V (s′) = s′∼p(·|s,a)(V (s′)). Then, for any (s, a) and value functions ̄V , V such that ̄V (s′) ≥ V (s′) for CVaRα any s′ ∈ S, we have\n\nCVaRα\n\ns′∼p(·|s,a)( ̄V (s′)) − CVaRα\n\ns′∼p(·|s,a)(V (s′)) ≤ βα,V (·|s, a)⊤ (cid:0) ̄V − V (cid:1) .\n\n(3)\n\nEq. (3) implies that the deviation of CVaR between optimistic and true value functions can be bounded by their value deviation under the conditional transition probability, which resolves the aforementioned challenge (ii), and serves as the basis of our recurrent regret decomposition. Now, since ̄V k\n\nh , we decompose the regret in episode k as\n\nh is an optimistic estimate of V ∗\n\n ̄V k\n\n1 (sk\n\n1)−V πk\n\n1\n\n(cid:115)\n\n(sk 1)\n\n(a) ≤\n\nH α\n\nL 1, ak nk(sk 1)\n\n+ CVaRα\n\ns′∼ ˆpk(·|sk\n\n1 ,ak\n\n1 )( ̄V k\n\n2 (s′)) − CVaRα\n\ns′∼p(·|sk\n\n1 ,ak\n\n1 )( ̄V k\n\n2 (s′))\n\n+ CVaRα\n\ns′∼p(·|sk\n\n1 ,ak\n\n(cid:115)\n\n(b) ≤\n\nH α\n\nL 1, ak nk(sk 1)\n\n+\n\n4H α\n\nH (cid:88)\n\n(cid:88)\n\n(c) ≤\n\nh=1\n\n(s,a)\n\nwCVaR,α,V πk\n\nkh\n\n(s, a) ·\n\nH\n\n1 )( ̄V k\n\n2 (s′)) − CVaRα (cid:115)\n\ns′∼p(·|sk\n\n1 ,ak\n\n1 )(V πk\n\n2\n\n(s′))\n\n+ βα,V πk\n\n2\n\nSL nk(sk 1, ak 1) √\n\n√\n\nSL\n\nL + 4H α(cid:112)nk(s, a)\n\n(·|sk\n\n1, ak\n\n1)⊤( ̄V k\n\n2 − V πk\n\n2\n\n)\n\n(4)\n\nkh\n\nHere wCVaR,α,V πk (s, a) denotes the conditional probability of visiting (s, a) at step h of episode k, conditioning on transitioning to the worst α-portion successor states s′ (i.e., with the lowest αh′+1(s′)) at each step h′ = 1, . . . , h − 1. Intuitively, wCVaR,α,V πk portion values V πk visitation probability under the conditional transition probability βα,V πk (·|·, ·). Inequality (b) uses the concentration of CVaR and Eq. (3). Inequality (c) follows from recurrently applying steps (a)- (b) to unfold ̄V k\n\nh (·) for h = 2, . . . , H, and the fact that wCVaR,α,V πk\n\n(s, a) is the visitation\n\n(s, a) is a distorted\n\nh (·) − V πk\n\nkh\n\nkh\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nprobability under conditional transition probability βα,V πk (·|·, ·). Eq. (4) decomposes the regret into estimation error at all state-action pairs via the distorted (conditional) visitation distribution wCVaR,α,V πk\n\n(s, a), which overcomes the aforementioned challenge (i).\n\nkh\n\nSumming Eq. (4) over all episodes k ∈ [K] and using the Cauchy–Schwarz inequality, we have\n\n√\n\nα\n\n√\n\n√\n\nE[R(K)] ≤\n\n5H\n\n5H\n\n(d)=\n\n5H\n\n(e) ≤\n\nSL\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\n(s,a)\n\nh=1 (cid:118) (cid:117) (cid:117) (cid:116)\n\nKH\n\n√\n\nSL · α\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\nKHSL α\n\n(cid:26)\n\nmin\n\nkh\n\nwCVaR,α,V πk nk(s, a)\n\n(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n·\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)\n\nwCVaR,α,V πk\n\nkh\n\n(s, a)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nwCVaR,α,V πk\n\nkh\n\n(s, a)\n\nk=1\n\nh=1\n\n(s,a)\n\nwkh(s, a)\n\n·\n\nwkh(s, a) nk(s, a)\n\n· 1 {wkh(s, a) ̸= 0}\n\n1\n\nmin π,h,(s,a): wπ,h(s,a)>0\n\nwπ,h(s, a)\n\n,\n\n1 αH−1\n\n(cid:27) K\n\n(cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)\n\nwkh(s, a) nk(s, a)\n\n,\n\nkh\n\nkh\n\n(s,a) wCVaR,α,V πk\n\n(s, a) = 1, and if the visitation probability wkh(s, a) = 0, the con-\n\nHere wkh(s, a) denotes the probability of visiting (s, a) at step h of episode k, and wπ,h(s, a) denotes the probability of visiting (s, a) at step h under policy π. Equality (d) uses the facts that (cid:80) ditional visitation probability wCVaR,α,V πk that wCVaR,α,V πk (s, a)/wkh(s, a) can be bounded by both 1/ minπ,h,(s,a): wπ,h(s,a)>0 wπ,h(s, a) kh and 1/αH−1. the bound 1/ minπ,h,(s,a): wπ,h(s,a)>0 wπ,h(s, a) follows from minπ,h,(s,a): wπ,h(s,a)>0 wπ,h(s, a) ≤ wkh(s, a), and the bound 1/αH−1 comes from the fact that the conditional visitation probability wCVaR,α,V πk (s, a) is at most 1/αH−1 times the visitation probability wkh(s, a). Having established the above, we can use a similar analysis as that in classic RL (Azar et al., 2017; Zanette & Brunskill, 2019) to bound (cid:80)K nk(s,a) , and then, □\nwe can obtain Theorem 1.\n\n(s, a) must be 0 as well.\n\nInequality (e) is due to\n\nSpecifically,\n\nwkh(s,a)\n\n(cid:80)H\n\n(s,a)\n\n(cid:80)\n\nh=1\n\nk=1\n\nkh\n\n4.2 REGRET LOWER BOUND\n\nWe now present a regret lower bound to demonstrate the optimality of algorithm ICVaR-RM.\n\nTheorem 2 (Regret Lower Bound). There exists an instance of Iterated CVaR RL-RM, where minπ,h,s: wπ,h(s)>0 wπ,h(s) > αH−1 and the regret of any algorithm is at least\n\n(cid:32)\n\n(cid:115)\n\nΩ\n\nH\n\nAK α minπ,h,s: wπ,h(s)>0 wπ,h(s)\n\n(cid:33) .\n\nIn addition,\n\nthere\n\nexists an instance of\n\nIterated CVaR RL-RM, where αH−1\n\nminπ,h,s: wπ,h(s)>0 wπ,h(s) and the regret of any algorithm is at least Ω(\n\n(cid:113) AK\n\nαH−1 ).\n\n(5)\n\n>\n\nRemark 2. Theorem 2 demonstrates that when α is small, the factor minπ,h,s: wπ,h(s)>0 wπ,h(s) is inevitable in general. This reveals the intrinsic hardness of Iterated CVaR RL, i.e., when the agent is highly sensitive to bad situations, she must suffer a regret due to exploring risky but hard-to-reach states. This lower bound also validates that ICVaR-RM is near-optimal with respect to K.\n\nLower Bound Analysis. Here we provide the proof idea of the first lower bound (Eq. (5)) in Theorem 2, and defer the full proof to Appendix D.2. We construct an instance with a hard-to-reach bandit state (which has an optimal action and multiple sub-optimal actions), and show that this state is critical for minimizing the regret, but difficult for any algorithm to learn. As shown in Figure 1, we consider an MDP with A actions, n regular states s1, . . . , sn and three absorbing states x1, x2, x3, where n < 1 2 H. The\n\n7\n\nFigure 1: Instance for the lower bound.\n\ns1s2sn−1snx1x2x3μμμμ1−3μ1−μ1−μReward 1Reward 0.8Reward 0.21−α1−α+ηαα−η...μμPublished as a conference paper at ICLR 2023\n\nAlgorithm 2: MaxWP Input: δ, δ′ := δ\n\n1 for k = 1, 2, . . . , K do\n\n2 , L := log( SA\n\nδ′ ), ˆV k\n\nH+1(s) = 0 for any k > 0 and s ∈ S\n\n2\n\n3\n\n4\n\n5\n\nfor h = H, H − 1, . . . , 1 do\n\nˆQk h(s, a) ← r(s, a) + mins′∼ ˆpk(·|s,a)( ˆV k h (s) ← maxa∈A ˆQk ˆV k\n\nh(s, a), πk\n\nh+1(s′)), ∀(s, a) ∈ S × A; ˆQk\n\nh(s, a), ∀s ∈ S;\n\nh(s) ← argmaxa∈A\n\nPlay the episode k with policy πk, and update nk+1(s, a) and ˆpk+1(s′|s, a);\n\nreward function r(s, a) depends only on the states, i.e., s1, . . . , sn generate zero reward, and x1, x2, x3 generate rewards 1, 0.8 and 0.2, respectively. Let μ be a parameter such that 0 < α < μ < 1 3 . Under all actions, state s1 transitions to s2, x1, x2, x3 with probabilities μ, 1 − 3μ, μ and μ, respectively, and state si (2 ≤ i ≤ n − 1) transitions to si+1, x1 with probabilities μ and 1 − μ, respectively. For the bandit state sn, under the optimal action, sn transitions to x2, x3 with probabilities 1 − α + η and α − η, respectively. Under sub-optimal actions, sn transitions to x2, x3 with probabilities 1 − α and α, respectively.\n\nIn this MDP, under the Iterated CVaR criterion, the value function mainly depends on the path s1 → s2 → · · · → sn → x2/x3, and especially on the action choice in the bandit state sn. Thus, to distinguish the optimal action in sn, any algorithm must suffer a regret dependent on the probability of visiting sn, which is exactly the minimum visitation probability over all reachable states minπ,h,s: wπ,h(s)>0 wπ,h(s). Note that in this instance, minπ,h,s: wπ,h(s)>0 wπ,h(s) = μn−1, which does not depend on α and H. This demonstrates that there is an essential dependency on minπ,h,s: wπ,h(s)>0 wπ,h(s) in the lower bound.\n\n5\n\nITERATED CVAR RL WITH BEST POLICY IDENTIFICATION\n\nIn this section, we design an efficient algorithm ICVaR-BPI, and establish sample complexity upper and lower bounds for Iterated CVaR RL with best policy identification (BPI).\n\n5.1 ALGORITHM ICVaR-BPI AND SAMPLE COMPLEXITY UPPER BOUND\n\nAlgorithm ICVaR-BPI introduces a novel distorted (conditional) empirical transition probability to construct estimation error, which effectively assigns more attention to bad situations and fits the main focus of the Iterated CVaR criterion. Due to space limit, we defer the pseudo-code and detailed description of ICVaR-BPI to Appendix E.1. Below we present the sample complexity of ICVaR-BPI.\n\nTheorem 3 (Sample Complexity Upper Bound). The number of trajectories used by algorithm ICVaR-BPI to return an ε-optimal policy with probability at least 1 − δ is bounded by\n\n(cid:18)\n\nO\n\nmin\n\n(cid:110)\n\n1 minπ,h,s: wπ,h(s)>0 wπ,h(s)\n\n,\n\n1 αH−1\n\n(cid:111) H 3S2A ε2α2\n\n(cid:19) ,\n\n· C\n\nwhere C := log2(min{\n\nminπ,h,s: wπ,h(s)>0 wπ,h(s) ,\n\n1\n\n1\n\nαH−1 } HSA\n\nεαδ ).\n\nSimilar to Theorem 1, minπ,h,s: wπ,h(s)>0 wπ,h(s) and αH−1 dominate the bound for a large α and a small α, respectively. When α = 1, the problem reduces to the classic RL formulation with best policy identification, and our sample complexity becomes ̃O( H 3S2A ), which recovers the result in prior classic RL work (Dann et al., 2017). Similar to Theorem 1, this bound has a gap of HS to the state-of-the-art sample complexity for classic RL (M ́enard et al., 2021). This gap is due to the fact that the result in (M ́enard et al., 2021) is obtained using the Bernstein-type exploration bonuses, which are more fine-grained for the classic RL problem but do not work for general risk-sensitive cases, because it cannot be used to quantify the estimation error of CVaR.\n\nε2\n\nTo validate the tightness of Theorem 3, we further provide sample complexity lower bounds (cid:1)) for different instances, which Ω(\n\n(cid:1)) and Ω(\n\nε2α minπ,h,s: wπ,h(s)>0 wπ,h(s) log (cid:0) 1\n\nαH−1ε2 log (cid:0) 1\n\nH 2A\n\nA\n\nδ\n\nδ\n\ndemonstrate that the factor min{1/ minπ,h,s: wπ,h(s)>0 wπ,h(s), 1/αH−1} is indispensable in general (see Appendix E.3 for a formal statement of lower bound).\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n6 WORST PATH RL\n\nIn this section, we investigate an interesting limiting case of Iterated CVaR RL when α → 0, called Worst Path RL, in which case the agent aims to maximize the minimum possible cumulative reward.\n\nWorst Path RL has a unique feature that, the value function (Eq. (2)) concerns only the minimum value of successor states, which are independent of specific transition probabilities. Therefore, once we learn the connectivity among states, we can perform a planning to compute the optimal policy. Yet, this feature does not make the Worst Path RL problem trivial, because it is still challenging to distinguish whether a successor state is hard to reach or does not exist. As a result, a careful scheme is needed to both explore undetected successor states and exploit observations to minimize regret.\n\n6.1 ALGORITHM MaxWP AND REGRET UPPER BOUND\n\nWe design an algorithm MaxWP (Algorithm 2) based on a simple and efficient empirical Q-value function, which makes full use of the unique feature of Worst Path RL, and simultaneously explores undetected successor states and exploits the current best action. Specifically, in episode k, MaxWP constructs empirical Q-value/value functions ˆQk h (s) using the estimated lowest value of next states, and then, takes a greedy policy πk h(s, a) in this episode.\n\nh(s) with respect to ˆQk\n\nh(s, a), ˆV k\n\nThe intuition behind MaxWP is as follows. Since the Q-value function for Worst Path RL uses the min operator, if the Q-value function is not accurately estimated, it can only be over-estimated (not under-estimated). If over-estimation happens, MaxWP will be exploring an over-estimated action and urging its empirical Q-value to get back to its true Q-value. Otherwise, if the Q-value function is already accurate, MaxWP just selects the optimal action. In other words, MaxWP combines the exploration of over-estimated actions (which lead to undetected successor states) and exploitation of current best actions. Below we provide the regret guarantee for algorithm MaxWP.\n\nTheorem 4. With probability at least 1 − δ, the regret of algorithm MaxWP is bounded by\n\n(cid:18) (cid:88)\n\nO\n\n(s,a)∈S×A\n\nH minπ: υπ(s,a)>0 υπ(s, a) · mins′∈supp(p(·|s,a)) p(s′|s, a)\n\nlog\n\n(cid:17)(cid:19)\n\n,\n\n(cid:16) SA δ\n\nwhere υπ(s, a) denotes the probability (s, a) is visited at least once in an episode under policy π.\n\nRemark 3. The factor minπ: υπ(s,a)>0 υπ(s, a) stands for the minimum probability of visiting (s, a) at least once in an episode over all feasible policies, and mins′∈supp(p(·|s,a)) p(s′|s, a) denotes the minimum transition probability over all successor states of (s, a). Note that this result cannot be implied by Theorem 1, because the result for Iterated CVaR RL there depends on 1 α , and simply taking α → 0 leads to a vacuous bound.\n\nTheorem 4 demonstrates that algorithm MaxWP enjoys a constant regret with respect to K. This constant regret is made possible by the unique feature of Worst Path RL that, under the worst path metric, once the agent determines the connectivity among states, she can accurately estimate the value function and find the optimal policy. Furthermore, determining the connectivity among states (with a given confidence) only requires a number of samples independent of K. MaxWP effectively utilizes this problem feature, and efficiently explores the connectivity among states.\n\nTo validate the optimality of our regret upper bound, we also provide a lower bound minπ:υπ (s,a)>0 υπ(s,a)·mins′∈supp(p(·|s,a)) p(s′|s,a) ) for Worst Path RL, which Ω(max(s,a):∃h, a̸=π∗ demonstrates the tightness of the factors minπ: υπ(s,a)>0 υπ(s, a) and mins′∈supp(p(·|s,a)) p(s′|s, a).\n\nh(s)\n\nH\n\n7 CONCLUSION\n\nIn this paper, we investigate a novel Iterated CVaR RL problem with the regret minimization and best policy identification metrics. We design two efficient algorithms ICVaR-RM and ICVaR-BPI, and provide nearly matching regret/sample complexity upper and lower bounds with respect to K. We also study an interesting limiting case called Worst Path RL, and propose a simple and efficient algorithm MaxWP with rigorous regret guarantees. There are several interesting directions for future work, e.g., further closing the gap between upper and lower bounds, and extending our model and results from the tabular setting to the function approximation framework.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nThe work of Yihan Du and Longbo Huang is supported by the Technology and Innovation Major Project of the Ministry of Science and Technology of China under Grant 2020AAA0108400 and 2020AAA0108403, the Tsinghua University Initiative Scientific Research Program, and Tsinghua Precision Medicine Foundation 10001020109. The work of Siwei Wang was supported in part by the National Natural Science Foundation of China Grant 62106122.\n\nREFERENCES\n\nShipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-\n\ncase regret bounds. Advances in Neural Information Processing Systems, 30, 2017.\n\nPhilippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk.\n\nMathematical Finance, 9(3):203–228, 1999.\n\nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The non-stochastic multi-\n\narmed bandit problem. SIAM Journal on Computing, 32(1):48–77, 2002.\n\nMohammad Gheshlaghi Azar, Ian Osband, and R ́emi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pp. 263–272. PMLR, 2017.\n\nNicole B ̈auerle and Alexander Glauner. Markov decision processes with recursive risk measures.\n\nEuropean Journal of Operational Research, 296(3):953–966, 2022.\n\nNicole B ̈auerle and Jonathan Ott. Markov decision processes with average-value-at-risk criteria.\n\nMathematical Methods of Operations Research, 74(3):361–379, 2011.\n\nKang Boda and Jerzy A Filar. Time consistent dynamic risk measures. Mathematical Methods of\n\nOperations Research, 63(1):169–186, 2006.\n\nVivek Borkar and Rahul Jain. Risk-constrained markov decision processes. IEEE Transactions on\n\nAutomatic Control, 59(9):2574–2579, 2014.\n\nVivek S Borkar. A sensitivity formula for risk-sensitive cost and the actor–critic algorithm. Systems\n\n& Control Letters, 44(5):339–346, 2001.\n\nVivek S Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27(2):\n\n294–311, 2002.\n\nDavid B Brown. Large deviations bounds for estimating conditional value-at-risk. Operations\n\nResearch Letters, 35(6):722–730, 2007.\n\nRichard Cheng, G ́abor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3387–3395, 2019.\n\nYinlam Chow and Mohammad Ghavamzadeh. Algorithms for CVaR optimization in MDPs.\n\nIn\n\nAdvances in Neural Information Processing Systems, volume 27, 2014.\n\nYinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decisionmaking: a CVaR optimization approach. In Advances in Neural Information Processing Systems, volume 28, 2015.\n\nYinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research, 18(1): 6070–6120, 2017.\n\nShanyun Chu and Yi Zhang. Markov decision processes with iterated coherent risk measures. In-\n\nternational Journal of Control, 87(11):2286–2293, 2014.\n\nStefano P Coraluppi and Steven I Marcus. Mixed risk-neutral/minimax control of markov decision processes. In Proceedings 31st Conference on Information Sciences and Systems. Citeseer, 1997.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nStefano P Coraluppi and Steven I Marcus. Risk-sensitive and minimax control of discrete-time,\n\nfinite-state markov decision processes. Automatica, 35(2):301–309, 1999.\n\nAntonio Coronato, Muddasar Naeem, Giuseppe De Pietro, and Giovanni Paragliola. Reinforcement learning for intelligent healthcare applications: A survey. Artificial Intelligence in Medicine, 109: 101964, 2020.\n\nChristoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement\n\nlearning. In Advances in Neural Information Processing Systems, pp. 2818–2826, 2015.\n\nChristoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. In Advances in Neural Information Processing Systems, volume 30, 2017.\n\nDotan Di Castro, Aviv Tamar, and Shie Mannor. Policy gradients with variance related risk criteria.\n\narXiv preprint arXiv:1206.6404, 2012.\n\nMehdi Fatemi, Shikhar Sharma, Harm Van Seijen, and Samira Ebrahimi Kahou. Dead-ends and secure exploration in reinforcement learning. In International Conference on Machine Learning, pp. 1873–1881. PMLR, 2019.\n\nMehdi Fatemi, Taylor W Killian, Jayakumar Subramanian, and Marzyeh Ghassemi. Medical deadends and learning to identify high-risk states and treatments. Advances in Neural Information Processing Systems, 34:4856–4870, 2021.\n\nYingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie. Risk-sensitive reinforcement learning: Near-optimal risk-sample tradeoff in regret. In Advances in Neural Information Processing Systems, volume 33, pp. 22384–22395, 2020.\n\nYingjie Fei, Zhuoran Yang, Yudong Chen, and Zhaoran Wang. Exponential bellman equation and improved regret bounds for risk-sensitive reinforcement learning. In Advances in Neural Information Processing Systems, volume 34, 2021a.\n\nYingjie Fei, Zhuoran Yang, and Zhaoran Wang. Risk-sensitive reinforcement learning with function approximation: A debiasing approach. In International Conference on Machine Learning, pp. 3198–3207. PMLR, 2021b.\n\nClaude-Nicolas Fiechter. Efficient reinforcement learning. In Conference on Computational Learn-\n\ning Theory, pp. 88–97, 1994.\n\nJavier Garcıa and Fernando Fern ́andez. A comprehensive survey on safe reinforcement learning.\n\nJournal of Machine Learning Research, 16(1):1437–1480, 2015.\n\nMary R Hardy and Julia L Wirch. The iterated CTE: a dynamic risk measure. North American\n\nActuarial Journal, 8(4):62–75, 2004.\n\nWilliam B Haskell and Rahul Jain. A convex analytic approach to risk-aware markov decision\n\nprocesses. SIAM Journal on Control and Optimization, 53(3):1569–1598, 2015.\n\nMatthias Heger. Consideration of risk in reinforcement learning. In International Conference on\n\nMachine Learning, pp. 105–111. Elsevier, 1994.\n\nThomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement\n\nlearning. Journal of Machine Learning Research, 11(4), 2010.\n\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-\n\ncient? In Advances in Neural Information Processing Systems, volume 31, 2018.\n\nEric Johnson and Suresh Kannan. Adaptive flight control for an autonomous unmanned helicopter.\n\nIn AIAA Guidance, Navigation, and Control Conference and Exhibit, pp. 4439, 2002.\n\nLeslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A\n\nsurvey. Journal of Artificial Intelligence Research, 4:237–285, 1996.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nEmilie Kaufmann, Pierre M ́enard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent, and Michal Valko. Adaptive reward-free exploration. In Algorithmic Learning Theory, pp. 865– 891. PMLR, 2021.\n\nRamtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be conservative: Quickly learning a cvar policy. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 4436–4443, 2020.\n\nPrashanth La and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. Ad-\n\nvances in Neural Information Processing Systems, 26, 2013.\n\nPierre M ́enard, Omar Darwiche Domingues, Anders Jonsson, Emilie Kaufmann, Edouard Leurent, and Michal Valko. Fast active learning for pure exploration in reinforcement learning. In International Conference on Machine Learning, pp. 7599–7608. PMLR, 2021.\n\nTakayuki Osogami. Iterated risk measures for risk-sensitive markov decision processes with dis-\n\ncounted cost. arXiv preprint arXiv:1202.3755, 2012.\n\nJonathan Theodor Ott. A markov decision model for a surveillance application and risk-sensitive\n\nmarkov decision processes. 2010.\n\nR Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal\n\nof Risk, 2:21–42, 2000.\n\nAlexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on stochastic program-\n\nming: modeling and theory. SIAM, 2021.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nCsaba Szepesv ́ari. Algorithms for reinforcement learning. Synthesis Lectures on Artificial Intelli-\n\ngence and Machine Learning, 4(1):1–103, 2010.\n\nAviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the CVaR via sampling.\n\nIn AAAI\n\nConference on Artificial Intelligence, 2015.\n\nPhilip Thomas and Erik Learned-Miller. Concentration inequalities for conditional value-at-risk. In\n\nInternational Conference on Machine Learning, pp. 6225–6233. PMLR, 2019.\n\nChunhao Wang, Xiaofeng Zhu, Julian C Hong, and Dandan Zheng. Artificial intelligence in radiotherapy treatment planning: present and future. Technology in cancer research & treatment, 18: 1533033819873922, 2019.\n\nTsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. Inequalities for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep, 2003.\n\nLu Wen, Jingliang Duan, Shengbo Eben Li, Shaobing Xu, and Huei Peng. Safe reinforcement learning for autonomous vehicles through parallel constrained policy optimization. In IEEE International Conference on Intelligent Transportation Systems, pp. 1–7. IEEE, 2020.\n\nAndrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In International Conference on Machine Learning, pp. 7304–7312. PMLR, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA EXPERIMENTS\n\nIn this section, we provide experimental results to evaluate the empirical performance of our algorithm ICVaR-RM, and compare it to the state-of-the-art algorithms EULER (Zanette & Brunskill, 2019) and RSVI2 (Fei et al., 2021a) for classic RL and risk-sensitive RL, respectively.\n\nIn our experiments, we consider an H-layered MDP with S = 3(H − 1) + 1 states and A actions. There is a single state s0 (initial state) in layer 1. For any 2 ≤ h ≤ H, there are three states s3(h−2)+1, s3(h−2)+2 and s3(h−2)+3 in layer h, which induce rewards 1, 0 and 0.4, respectively. The agent starts from s0 in layer 1, and for each step h ∈ [H], she takes an action from {a1, . . . , aA}, and then transitions to one of three states in the next layer. For any a ∈ {a1, . . . , aA−1}, action a leads to s3(h−1)+1 and s3(h−1)+2 with probabilities 0.5 and 0.5, respectively. Action aA leads to s3(h−1)+2 and s3(h−1)+3 with probabilities 0.001 and 0.999, respectively.\n\nWe set α ∈ {0.05, 0.1, 0.15}, δ ∈ {0.5, 0.005, 0.00005}, H ∈ {2, 5, 10}, S ∈ {7, 13, 25}, A ∈ {3, 5, 12} and K ∈ [0, 10000] (the change of K can be seen from the X-axis in Figure 2). We take α = 0.05, δ = 0.005, H = 5, S = 13, A = 5 and K = 10000 as the basic setting, and change parameters α, δ, H, S, A and K to see how they affect the empirical performance of algorithm ICVaR-RM. For each algorithm, we perform 20 independent runs and report the average regret across runs with 95% confidence intervals.\n\nAs shown in Figure 2, our algorithm ICVaR-RM achieves a significantly lower regret than the other algorithms EULER (Zanette & Brunskill, 2019) and RSVI2 (Fei et al., 2021a), which demonstrates that ICVaR-RM can effectively control the risk under the Iterated CVaR criterion and shows performance superiority over the baselines. Moreover, the influences of parameters α, δ, H, S, A and K on the regret of algorithm ICVaR-RM match our theoretical bounds. Specifically, as α or δ increases, the regret of ICVaR-RM decreases. As H, S or A increases, the regret of ICVaR-RM increases as well. As the number of episodes K increases, the regret of ICVaR-RM increases at a sublinear rate.\n\nB RELATED WORK\n\nBelow we present a complete review of related works.\n\nCVaR-based MDPs (Known Transition). Boda & Filar (2006); Ott (2010); B ̈auerle & Ott (2011); Haskell & Jain (2015); Chow et al. (2015) study the CVaR MDP problem where the objective is to minimize the CVaR of the total cost with known transition, and demonstrate that the optimal policy for CVaR MDP is history-dependent (not Markovian) and is inefficient to exactly compute. Hardy & Wirch (2004) firstly define the Iterated CVaR measure, and prove that it is a coherent dynamic risk measure, and applicable to equity-linked insurance. Osogami (2012); Chu & Zhang (2014); B ̈auerle & Glauner (2022) investigate iterated coherent risk measures (including Iterated CVaR) in MDPs, and prove the existence of Markovian optimal policies for these MDPs. The above works focus mainly on designing planning algorithms and derive planning error guarantees for known transition, while our work develops RL algorithms (interacting with the environment online) and provides regret and sample complexity guarantees for unknown transition.\n\nRisk-Sensitive Reinforcement Learning (Unknown Transition). Heger (1994); Coraluppi & Marcus (1997; 1999) consider minimizing the worst-case cost in RL, and present dynamic programming of value functions and heuristic algorithms without theoretical analysis. Borkar (2001; 2002) study risk-sensitive RL with the exponential utility measure, and design algorithms based on actor–critic learning and Q-learning, respectively. Di Castro et al. (2012); La & Ghavamzadeh (2013) investigate variance-related risk measures, and devise policy gradient and actor-critic-based algorithms with convergence analysis. Tamar et al. (2015) consider maximizing the CVaR of the total reward, and propose a sampling-based estimator for the CVaR gradient and a stochastic gradient decent algorithm to optimize CVaR. Keramati et al. (2020) also investigate optimizing the CVaR of the total reward, and design an algorithm based on an optimistic version of the distributional Bellman operator. Borkar & Jain (2014); Chow & Ghavamzadeh (2014); Chow et al. (2017) study how to minimize the expected total cost with CVaR-based constraints, and develop policy gradient, actor-critic and\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n(a) α = 0.05\n\n(b) α = 0.1\n\n(c) α = 0.15\n\n(d) δ = 0.00005\n\n(e) δ = 0.005\n\n(f) δ = 0.5\n\n(g) H = 2\n\n(h) H = 5\n\n(i) H = 10\n\n(j) S = 7\n\n(k) S = 13\n\n(l) S = 25\n\n(m) A = 3\n\n(n) A = 5\n\n(o) A = 12\n\nFigure 2: Experimental results for Iterated CVaR RL.\n\nstochastic approximation-style algorithms. The above works mainly give convergence analysis, and do not provide finite-time regret and sample complexity guarantees as in our work.\n\nTo our best knowledge, there are only a few risk-sensitive RL works which provide finite-time regret analysis (Fei et al., 2020; 2021a;b). Fei et al. (2020) consider risk-sensitive RL with the exponential utility criterion, and propose algorithms based on logarithmic-exponential transformation and least-squares updates. Fei et al. (2021a) further improve the regret bound in (Fei et al., 2020) by\n\n14\n\nPublished as a conference paper at ICLR 2023\n\ndeveloping an exponential Bellmen equation and a Bellman backup analytical procedure. Fei et al. (2021b) extend the model and results in (Fei et al., 2020; 2021a) from the tabular setting to the function approximation framework. Our work is very different from the above works (Fei et al., 2020; 2021a;b) in formulation, algorithms and results. The above works (Fei et al., 2020; 2021a;b) use the exponential utility criterion to characterize the risk and take all successor states into account in decision making. They design algorithms based on exponential Bellmen equations and doubly decaying exploration bonuses. In contrast, we interpret the risk by the Iterated CVaR criterion, which primarily concerns the worst α-portion successor states. We develop algorithms using CVaR-adapted exploration bonuses.\n\nThe works we discuss above fall in the literature of RL with risk-sensitive criteria. There are also other RL works which focus on state-wise safety. Cheng et al. (2019) utilize control barrier functions (CBFs) to ensure the agent within a set of safe sets and guide the learning by constraining explorable polices. Fatemi et al. (2019; 2021) define the notion of dead-end states (which lead to suboptimal terminal state with probability 1 in finite steps) and aim to avoid getting into dead-end states. The formulations and algorithms in these works greatly differ from ours, and they do not provide finitetime regret and sample complexity analysis as us. We refer interested readers to the survey (Garcıa & Fern ́andez, 2015) for detailed categorization and discussion on safe RL.\n\nC MORE DISCUSSION ON ITERATED CVAR RL\n\nIn this section, we first present the expanded value function definitions for Iterated CVaR RL. Then, we compare Iterated CVaR RL with existing risk-sensitive MDP models, including CVaR MDP (Boda & Filar, 2006; Ott, 2010; B ̈auerle & Ott, 2011; Chow et al., 2015) and the exponential utility-based RL (Fei et al., 2020; 2021a).\n\nC.1 VALUE FUNCTION DEFINITIONS FOR ITERATED CVAR RL\n\nThe value function definition for Iterated CVaR RL, i.e., Eq. (i) in Section 3, can be expanded as\n\nQπ\n\nh(s, a) = r(s, a) + CVaRα\n\nsh+1∼p(·|s,a)\n\n(cid:18)\n\nr(sh+1, πh+1(sh+1))\n\n+ CVaRα\n\nsh+2∼p(·|sh+1,πh+1(sh+1))\n\n(cid:16)\n\n. . . CVaRα\n\nsH ∼p(·|sH−1,πH−1(sH−1))(r(sH , πH (sH )))\n\n(cid:17)(cid:19)\n\nV π\n\nh (s) = r(s, πh(s)) + CVaRα\n\nsh+1∼p(·|s,πh(s))\n\n(cid:18)\n\nr(sh+1, πh+1(sh+1))\n\n+ CVaRα\n\nsh+2∼p(·|sh+1,πh+1(sh+1))\n\n(cid:16)\n\n. . . CVaRα\n\nsH ∼p(·|sH−1,πH−1(sH−1))(r(sH , πH (sH )))\n\n(cid:17)(cid:19)\n\nSimilarly, the optimal value function definition, e.g., Eq. (ii) in Section 3, can be expanded as\n\nQ∗\n\nh(s, a) = max\n\nπ\n\n(cid:40)\n\nr(s, a) + CVaRα\n\nsh+1∼p(·|s,a)\n\n(cid:18)\n\nr(sh+1, πh+1(sh+1))\n\n+ CVaRα\n\nsh+2∼p(·|sh+1,πh+1(sh+1))\n\n(cid:16)\n\n. . .CVaRα\n\nsH ∼p(·|sH−1,πH−1(sH−1))(r(sH , πH (sH )))\n\n(cid:17)(cid:19)(cid:41)\n\n,\n\nV ∗\n\nh (s) = max\n\nπ\n\n(cid:40)\n\nr(s, πh(s)) + CVaRα\n\nsh+1∼p(·|s,πh(s))\n\n(cid:18)\n\nr(sh+1, πh+1(sh+1))\n\n,\n\n.\n\n+ CVaRα\n\nsh+2∼p(·|sh+1,πh+1(sh+1))\n\n(cid:16)\n\n. . .CVaRα\n\nsH ∼p(·|sH−1,πH−1(sH−1))(r(sH , πH (sH )))\n\n(cid:17)(cid:19)(cid:41) .\n\n(6)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Illustrating example for the comparison between CVaR MDP and Iterated CVaR RL.\n\nFrom the above value function definitions, we can see that, Iterated CVaR RL aims to maximize the worst α-portion tail of the reward-to-go at each step, i.e., taking the CVaR operator on the rewardto-go at each step. Intuitively, Iterated CVaR RL wants to optimize the performance even when bad situations happen at each decision stage.\n\nC.2 COMPARISON WITH CVAR MDP\n\nThe objective of CVaR MDP, e.g., (Boda & Filar, 2006; Ott, 2010; B ̈auerle & Ott, 2011; Chow et al., 2015), is to maximize the worst α-portion of the total reward, which is formally defined as\n\nmax π\n\nCVaRα\n\n(sh,ah)∼p,π\n\n(cid:33)\n\nr(sh, ah)\n\n.\n\n(cid:32) H\n\n(cid:88)\n\nh=1\n\nCompared to our Iterated CVaR RL (Eq. (6) and Eq. (ii) in Section 3) which concerns bad situations at each step, CVaR MDP takes more cumulative reward into account and prefers actions which have better performance in general, but can have larger probabilities of getting into catastrophic states. Thus, CVaR MDP is suitable for scenarios where bad situations lead to a higher cost but not fatal damage, e.g., finance. In contrast, Iterated CVaR RL prefers actions which have smaller probabilities of getting into catastrophic states. Hence, Iterated CVaR RL is most suitable for safetycritical applications, where catastrophic states are unacceptable and need to be carefully avoid, e.g., clinical treatment planning.\n\nWe emphasize that Iterated CVaR is not equivalent to simply taking the worst αH -portion of the total reward. In fact, the good (1 − αH )-portion of the total reward also contributes to Iterated CVaR. This is because Iterated CVaR accounts bad situations for all states (both good and bad states) in its iterated computation, instead of just considering bad situations upon bad states.\n\nBelow we provide an example of clinical treatment planning to illustrate the difference between Iterated CVaR and CVaR MDP. Here we interpret the objective as cost minimization for ease of understanding, and set the risk level α = 0.05.\n\n16\n\ns1s4s5s8s9s11s2a1(deterministically)(0.05)(0.95)(0.05)(0.95)(0.05)(0.95)10.40a2(deterministically)CVaR MDP:QCVaR,α(s1,a1)=0.00250.05∙1+0.05−0.00250.05∙0.4=0.43QCVaR,α(s1,a1)=0.00010.05∙1+0.05−0.00010.05∙0.5=0.501Iterated CVaRRL (ours):QICVaR,αs1,a1=0.050.05∙(0.050.05∙1)=1QICVaR,αs1,a2=0.010.05∙(0.010.05∙1+0.05−0.010.05∙0.5)+0.05−0.010.05∙(0.010.05∙0.5+0.05−0.010.05∙0)=0.2s10Cost:0.4s6s7s12s13s15s3(0.01)(0.99)(0.01)(0.99)(0.01)(0.99)10.50s140.5DeathGet much betterCuredDeathGet betterCuredRisk level α=0.05Published as a conference paper at ICLR 2023\n\nConsider a 4-layered binary tree-structured MDP shown in Figure 3. The state sets in layers 1, 2, 3 and 4 are {s1}, {s2, s3}, {s4, . . . , s7} and {s8, . . . , s15}, respectively. There are two actions a1, a2 in each state, and a1, a2 have the same transition distribution in all states except the initial state s1. Thus, a policy is to decide whether to choose a1 or a2 in state s1, which leads to different subsequent costs.\n\nThe agent starts from the initial state s1 in layer 1. If the agent takes action a1, she will transition to state s2 deterministically, and goes into the left sub-tree. On the other hand, if the agent takes action a2 in state s1, she will transition to state s3 deterministically, and enters the right sub-tree.\n\nIf the agent goes into the left sub-tree (state s2) in layer 2, she will transition to s4 and s5 in layer 3 with probabilities 0.05 and 0.95, respectively. Then, if she starts from state s4 in layer 3, she will transition to s8 and s9 in layer 4 with probabilities 0.05 and 0.95, respectively. Otherwise, if she starts from state s5 in layer 3, she will transition to s10 and s11 in layer 4 with probabilities 0.05 and 0.95, respectively.\n\nOn the other hand, if the agent goes into the right sub-tree (state s3) in layer 2, she will transition to s6 and s7 in layer 3 with probabilities 0.01 and 0.99, respectively. Then, if she starts from state s6 in layer 3, she will transition to s12 and s13 in layer 4 with probabilities 0.01 and 0.99, respectively. Otherwise, if she starts from state s7 in layer 3, she will transition to s14 and s15 in layer 4 with probabilities 0.01 and 0.99, respectively.\n\nThe costs are state-dependent, and only the states in layer 4 produce non-zero costs. To be concrete, we use the clinical trial example and the costs represent the patient status. Specifically, in layer 4, s8 and s12 give costs 1, which denote death. s13 and s14 produce costs 0.5, which means the patient is getting better. s9 and s10 induce costs 0.4, which denote that the patient gets much better. s11 and s15 produce costs 0, which stand for that the patient is fully cured.\n\nUnder the CVaR criterion, we have that\n\nQCVaR,α(s1, a1) =\n\n0.0025 0.05\n\n· 1 +\n\n0.05 − 0.0025 0.05\n\n· 0.4 = 0.43,\n\nand\n\nQCVaR,α(s1, a2) =\n\n0.0001 0.05\n\n· 1 +\n\n0.05 − 0.0001 0.05\n\n· 0.5 = 0.501.\n\nThus, CVaR MDP will choose action a1 (and goes into the left sub-tree), since a1 leads to better medium states s9 and s10, which give a lower cost 0.4 than the cost 0.5 produced by the right sub-tree.\n\nOn the other hand, under the Iterated CVaR criterion, we have that\n\nQICVaR,α(s1, a1) =\n\n0.05 0.05\n\n·QICVaR,α(s4, ·) =\n\n0.05 0.05\n\n(cid:18) 0.05 0.05\n\n·\n\n· QICVaR,α(s8, ·)\n\n(cid:19)\n\n=\n\n0.05 0.05\n\n(cid:18) 0.05 ·\n0.05\n\n(cid:19)\n\n· 1\n\n= 1,\n\nand\n\nQICVaR,α(s1, a2) 0.01 0.05 0.01 0.05\n\n· QICVaR,α(s6, ·) + (cid:18) 0.01 0.05\n\n·\n\n=\n\n=\n\n+\n\n0.05 − 0.01 0.05 (cid:18) 0.01 0.05\n\n·\n\n=\n\n0.01 0.05\n\n0.05 − 0.01 0.05\n\n· QICVaR,α(s7, ·)\n\n· QICVaR,α(s12, ·) +\n\n0.05 − 0.01 0.05\n\n· QICVaR,α(s13, ·)\n\n(cid:19)\n\n(cid:18) 0.01 0.05\n\n·\n\n· QICVaR,α(s14, ·) +\n\n0.05 − 0.01 0.05\n\n· QICVaR,α(s15, ·)\n\n(cid:19)\n\n· 1 +\n\n0.05 − 0.01 0.05\n\n(cid:19)\n\n· 0.5\n\n+\n\n0.05 − 0.01 0.05\n\n(cid:18) 0.01 0.05\n\n·\n\n· 0.5 +\n\n0.05 − 0.01 0.05\n\n(cid:19)\n\n· 0\n\n=0.2.\n\nThus, Iterated CVaR RL will instead choose action a2, because a2 has a smaller probability of going into the bad left direction (which leads to the catastrophic state s12).\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nThe above example shows that, Iterated CVaR RL prefers actions with a smaller probability of getting into catastrophic states. In contrast, CVaR MDP favors actions with better average therapeutic effects, but has a larger probability of causing death.\n\nNote that the above example also demonstrates that Iterated CVaR is not equivalent to the worst αH -portion of the total cost. To see this, we have that (here we consider α3 because there are 3 transition steps):\n\nQCVaR,α3\n\n(s1, a2) =\n\n0.0001 0.000125\n\n· 1 +\n\n0.000125 − 0.0001 0.000125\n\n· 0.5 = 0.9,\n\nand\n\nQICVaR,α(s1, a2) 0.01 0.05\n\n(cid:18) 0.01 0.05\n\n·\n\n=\n\n· 1 +\n\n0.05 − 0.01 0.05\n\n(cid:19)\n\n· 0.5\n\n+\n\n0.05 − 0.01 0.05\n\n(cid:18) 0.01 0.05\n\n·\n\n· 0.5 +\n\n0.05 − 0.01 0.05\n\n(cid:19)\n\n· 0\n\n=0.2.\n\nIn addition, one can see that, the good state which gives cost 0 (i.e., s15) also contributes to QICVaR,α(s1, a2), which shows that the good (1 − αH )-portion of the total cost also matters for Iterated CVaR.\n\nC.3 COMPARISON WITH EXPONENTIAL UTILITY-BASED RISK-SENSITIVE RL\n\nThe Bellman optimality equation for risk-sensitive RL with the exponential utility criterion (Fei et al., 2020; 2021a) is defined as\n\nQ∗\n\nh(s, a) = rh(s, a) +\n\n1 β\n\nlog{Es′∼p(·|s,a)[exp(β · V ∗\n\nh+1(s′))]},\n\nwhich takes all successor states s′ into account, i.e., all successor states s′ contribute to the computation of the Q-value. Here β < 0 is a risk-sensitivity parameter.\n\nIn contrast, in Iterated CVaR RL, the Bellman optimality equation is defined as h(s, a) = r(s, a) + CVaRα\n\ns′∼p(·|s,a)(V ∗\n\nh+1(s′)),\n\nQ∗\n\nwhich focuses only on the worst α-portion successor states s′ (i.e., with the lowest α-portion values V ∗ h+1(s′)), i.e., only the worst α-portion successor states s′ contribute to the computation of the Q-value.\n\nBesides the formulation, our algorithm design and results are also very different from those in (Fei et al., 2020; 2021a). The algorithms in (Fei et al., 2020; 2021a) are based on exponential Bellman equations and doubly decaying exploration bonuses, and their results depend on exp(|β|H). In contrast, our algorithms are based on value iteration for Iterated CVaR with CVaR-adapted exploration bonuses, and our results depend on the minimum between an MDP-intrinsic visitation measure 1/ minπ,h,s: wπ,h(s)>0 wπ,h(s) and a risk-level-dependent factor 1/αH−1.\n\nD PROOFS FOR ITERATED CVAR RL WITH REGRET MINIMIZATION\n\nIn this section, we present the proofs of regret upper and lower bounds (Theorems 1 and 2) for Iterated CVaR RL-RM.\n\nD.1 PROOFS OF REGRET UPPER BOUND\n\nD.1.1 CONCENTRATION\n\nFor any k > 0, h ∈ [H] and (s, a) ∈ S × A, let nkh(s, a) denote the number of times that (s, a) was visited at step h before episode k, and let nk(s, a) := (cid:80)H h=1 nkh(s, a) denote the number of times that (s, a) was visited before episode k.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Illustrating example for Lemma 2. For each s′ ∈ {s1, s2, s3, s4}, the height of the bar denotes the value V (s′) (fixed), and the width of the bar denotes the transition probability p(s′|s, a) or ˆpk(s′|s, a). The colored part of the bars denotes the worst α-portion successor states (i.e., with the lowest α-portion values V (s′)). In this example, α = 0.5.\n\nLemma 1 (Concentration for V ∗). It holds that\n\n(cid:34)\n\nPr\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V ∗\n\nh (s′)) − CVaRα\n\ns′∼p(·|s,a)(V ∗\n\nh (s′))\n\n(cid:12) (cid:12) (cid:12) ≤\n\nH α\n\n(cid:115)\n\nlog (cid:0) KHSA nk(s, a)\n\nδ′\n\n(cid:1)\n\n,\n\n∀k ∈ [K], ∀h ∈ [H], ∀(s, a) ∈ S × A\n\n≥ 1 − 2δ′.\n\n(cid:35)\n\nProof of Lemma 1. Using Brown’s inequality (Brown, 2007) (Theorem 2 in (Thomas & LearnedMiller, 2019)) and a union bound over (s, a) ∈ S × A and nk(s, a) ∈ [KH], we can obtain this lemma.\n\nFor any risk level α ∈ (0, 1], function V : S (cid:55)→ R and (s′, s, a) ∈ S × S × A, βα,V (s′|s, a) is the conditional transition probability from (s, a) to s′, conditioning on transitioning to the worst α-portion successor states s′ (i.e., with the lowest α-portion values V (s′)). Let μα,V (s′|s, a) denote how large the transition probability of successor state s′ belongs to the worst α-portion, which satisfies that μα,V (s′|s,a) s′∈S μα,V (s′|s, a) = α. In addition, for any risk level α ∈ (0, 1], function V : S (cid:55)→ R and (s, a) ∈ S × A,\n\n= βα,V (s′|s, a) and (cid:80)\n\nα\n\nCVaRα\n\ns′∼p(·|s,a)(V (s′)) =\n\n(cid:80)\n\ns′∈S μα,V (s′|s, a) · V (s′) α\n\n=\n\n(cid:88)\n\ns′∈S\n\nβα,V (s′|s, a) · V (s′).\n\nLemma 2 (Concentration for any V ). It holds that\n\n(cid:34)\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V (s′)) − CVaRα\n\n(cid:12) s′∼p(·|s,a)(V (s′)) (cid:12) (cid:12) ≤\n\n(cid:115)\n\n2H α\n\n2S log (cid:0) KHSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\n∀V : S (cid:55)→ [0, H], ∀k ∈ [K], ∀(s, a) ∈ S × A\n\n≥ 1 − 2δ′.\n\n(cid:35)\n\nProof of Lemma 2. As shown in Figure 4, we sort all successor states s′ ∈ S by V (s′) in ascending order (from the left to the right). Add a virtual line at the α-quantile, denoted by α-quantile line. Fix the value function V (·), and the transition probability changes from p(·|s, a) to ˆpk(·|s, a).\n\nWithout loss of generality, below we consider the case where as the transition probability changes from p(·|s, a) to ˆpk(·|s, a), the α-quantile line shifts from left to right (the analysis of the contrary\n\n19\n\n(cid:2869)(cid:2870)(cid:2871)ps(cid:2869)s,a=0.3ps(cid:2870)s,a=0.3ps(cid:2871)s,a=0.2μ(cid:3080),(cid:3023)s(cid:2869)s,a=0.3μ(cid:3080),(cid:3023)s(cid:2870)s,a=0.2μ(cid:3080),(cid:3023)s(cid:2871)s,a=0(cid:2869)(cid:2870)(cid:2871)p̂(cid:3038)s(cid:2869)s,a=0.2p̂(cid:3038)s(cid:2870)s,a=0.1p̂(cid:3038)s(cid:2871)s,a=0.1μ̂(cid:3038);(cid:3080),(cid:3023)s(cid:2869)s,a=0.2μ̂(cid:3038);(cid:3080),(cid:3023)s(cid:2870)s,a=0.1μ̂(cid:3038);(cid:3080),(cid:3023)s(cid:2871)s,a=0.1Original -quantilelineShifted -quantileline(cid:3039)(cid:3032)(cid:3033)(cid:3047)(cid:2869)(cid:3040)(cid:3036)(cid:3031)(cid:3031)(cid:3039)(cid:3032)(cid:2871)(cid:3045)(cid:3036)(cid:3034)(cid:3035)(cid:3047)(cid:2872)(cid:2872)ps(cid:2872)s,a=0.2μ(cid:3080),(cid:3023)s(cid:2872)s,a=0p̂(cid:3038)s(cid:2872)s,a=0.6μ̂(cid:3038);(cid:3080),(cid:3023)s(cid:2872)s,a=0.1(cid:3039)(cid:3036)(cid:3041)(cid:3032)(cid:2879)(cid:3039)(cid:2870)(cid:3039)(cid:3036)(cid:3041)(cid:3032)(cid:2879)(cid:3045)(cid:2872)Published as a conference paper at ICLR 2023\n\ncase can also be obtained by interchanging p(·|s, a) and ˆpk(·|s, a)). We use original α-quantile line and shifted α-quantile line to denote the α-quantile line before and after the shift, respectively.\n\nWe divide the successor states s′ ∈ S into five subsets as follows. Let Slef t and Sright denote the sets of states which are always on the left and right sides of the original and shifted α-quantile lines, respectively. Let Smiddle denote the set of states which are in the middle of the original and shifted α-quantile lines. Let sline-l and sline-r denote the states which lie on the original and shifted α-quantile lines, respectively.\n\nFor any s′ ∈ Slef t, we have that μα,V (s′|s, a) = p(s′|s, a) and ˆμk;α,V (s′|s, a) = ˆpk(s′|s, a).\n\nFor any s′ ∈ Sright, we have μα,V (s′|s, a) = ˆμk;α,V (s′|s, a) = 0.\n\nFor any s′ ∈ Smiddle, we have that μα,V (s′|s, a) = 0 and ˆμk;α,V (s′|s, a) = ˆpk(s′|s, a). For state sline-l, we have that μα,V (sline-l|s, a) = p(sline-l|s, a) − ((cid:80) p(sline-l|s, a) − α) and ˆμk;α,V (sline-l|s, a) = ˆpk(sline-l|s, a). For state sline-r, we have that μα,V (sline-r|s, a) = 0 and ˆμk;α,V (sline-r|s, a) = α − (cid:80)\n\np(s′|s, a) +\n\ns′∈Slef t\n\ns′∈Smiddle\n\nˆpk(s′|s, a) − ˆpk(sline-l|s, a).\n\nˆpk(s′|s, a) − (cid:80)\n\ns′∈Slef t\n\nThen, we obtain (cid:88)\n\n(cid:12)ˆμk;α,V (s′|s, a) − μα,V (s′|s, a)(cid:12) (cid:12) (cid:12)\n\ns′∈S\n\n(cid:88)\n\n≤\n\n(cid:12)ˆμk;α,V (s′|s, a) − μα,V (s′|s, a)(cid:12) (cid:12)\n\n(cid:12) +\n\n(cid:88)\n\n(cid:12)ˆμk;α,V (s′|s, a) − μα,V (s′|s, a)(cid:12) (cid:12) (cid:12)\n\ns′∈Slef t\n\n(cid:88)\n\n+\n\ns′∈Smiddle\n\n(cid:12)ˆμk;α,V (s′|s, a) − μα,V (s′|s, a)(cid:12) (cid:12)\n\n(cid:12) + (cid:12)\n\n(cid:12)ˆμk;α,V (sline-l|s, a) − μα,V (sline-l|s, a)(cid:12) (cid:12)\n\ns′∈Sright\n\n+ (cid:12)\n\n(cid:12)ˆμk;α,V (sline-r|s, a) − μα,V (sline-r|s, a)(cid:12) (cid:12) (cid:88)\n\n(cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12)\n\n(cid:12) +\n\n(cid:88)\n\n≤\n\nˆpk(s′|s, a)\n\ns′∈Slef t (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) \n\n+\n\ns′∈Slef t\n\nˆpk(sline-l|s, a) − p(sline-l|s, a) +\n\ns′∈Smiddle \n\n(cid:88)\n\n\n\ns′∈Slef t\n\np(s′|s, a) + p(sline-l|s, a) − α\n\n\n\n+\n\nα −\n\n(cid:88)\n\nˆpk(s′|s, a) −\n\n(cid:88)\n\nˆpk(s′|s, a) − ˆpk(sline-l|s, a)\n\n\n\ns′∈Slef t\n\ns′∈Smiddle\n\n(cid:88)\n\n≤\n\n(cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12)\n\n(cid:12) +\n\n(cid:88)\n\nˆpk(s′|s, a)\n\ns′∈Smiddle (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) +\n\n(cid:88)\n\ns′∈Slef t\n\n+ (cid:12)\n\n(cid:12)ˆpk(sline-l|s, a) − p(sline-l|s, a)(cid:12)\n\np(s′|s, a) + p(sline-l|s, a) − α\n\n\n\n+\n\nα −\n\n(cid:88)\n\nˆpk(s′|s, a) −\n\n(cid:88)\n\ns′∈Slef t\n\ns′∈Smiddle\n\nˆpk(s′|s, a) − ˆpk(sline-l|s, a)\n\n\n\n\n\n(a) ≤\n\n(cid:88)\n\ns′∈Slef t\n\n(cid:12) (cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12)\n\n(cid:12) +\n\n(cid:88)\n\nˆpk(s′|s, a)\n\ns′∈Smiddle \n\n+ (cid:12)\n\n(cid:12)ˆpk(sline-l|s, a) − p(sline-l|s, a)(cid:12)\n\n(cid:12) +\n\n(cid:88)\n\n\n\ns′∈Slef t\n\np(s′|s, a) + p(sline-l|s, a) − α\n\n\n\n\n\n+\n\nα −\n\n(cid:88)\n\nˆpk(s′|s, a) −\n\n(cid:88)\n\ns′∈Slef t\n\ns′∈Smiddle\n\nˆpk(s′|s, a) − ˆpk(sline-l|s, a)\n\n\n\n\n\n20\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\nPublished as a conference paper at ICLR 2023\n\n(cid:88)\n\n=\n\n(cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12)\n\n(cid:12) + (cid:12)\n\n(cid:12)ˆpk(sline-l|s, a) − p(sline-l|s, a)(cid:12) (cid:12)\n\ns′∈Slef t (cid:88)\n\n+\n\ns′∈Slef t (cid:88)\n\np(s′|s, a) −\n\n(cid:88)\n\ns′∈Slef t\n\nˆpk(s′|s, a) + p(sline-l|s, a) − ˆpk(sline-l|s, a)\n\n≤\n\n(cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12)\n\n(cid:12) + (cid:12)\n\n(cid:12)ˆpk(sline-l|s, a) − p(sline-l|s, a)(cid:12) (cid:12)\n\ns′∈Slef t (cid:88)\n\n+\n\n(cid:12)p(s′|s, a) − ˆpk(s′|s, a)(cid:12) (cid:12)\n\n(cid:12) + (cid:12)\n\n(cid:12)p(sline-l|s, a) − ˆpk(sline-l|s, a)(cid:12) (cid:12)\n\ns′∈Slef t (cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12) (cid:12) ,\n\n(cid:88)\n\n≤2\n\n(7)\n\ns′∈S\n\nwhere (a) is due to (cid:80)\n\ns′∈Slef t\n\nThus, we have\n\np(s′|s, a) + p(sline-l|s, a) − α ≥ 0 by the definition of state sline-l.\n\n(cid:12) (cid:12)CVaRα (cid:12) (cid:12) (cid:80) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:80)\n\ns′∈S\n\ns′∼ ˆpk(·|s,a)(V (s′)) − CVaRα\n\ns′∼p(·|s,a)(V (s′)) (cid:80)\n\n(cid:12) (cid:12) (cid:12)\n\ns′∈S ˆμk;α,V (s′|s, a) · V (s′) α\n\ns′∈S μα,V (s′|s, a) · V (s′) α\n(cid:0)ˆμk;α,V (s′|s, a) − μα,V (s′|s, a)(cid:1) · V (s′)(cid:12) (cid:12)\n\n−\n\n(cid:80)\n\ns′∈S\n\nα (cid:12)ˆμk;α,V (s′|s, a) − μα,V (s′|s, a)(cid:12) (cid:12)\n\n(cid:12) · H\n\n2 (cid:80)\n\ns′∈S\n\nα (cid:12)pk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12)\n\n(cid:12) · H\n\nα\n\n=\n\n=\n\n≤\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(8)\n\nUsing Eq. (55) in (Zanette & Brunskill, 2019) (originated from (Weissman et al., 2003)), we have that with probability at least 1 − 2δ′, for any k ∈ [K] and (s, a) ∈ S × A, 2S log (cid:0) KHSA δ′ nk(s, a)\n\n(cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12)\n\n(cid:12) ≤\n\n(cid:88)\n\n(9)\n\n(cid:115)\n\n(cid:1)\n\n.\n\ns′∈S\n\nPlugging Eq. (9) into Eq. (8), we obtain that with probability at least 1 − 2δ′, for any k ∈ [K], (s, a) ∈ S × A and function V : S (cid:55)→ [0, H],\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V (s′)) − CVaRα\n\n(cid:12) s′∼p(·|s,a)(V (s′)) (cid:12) (cid:12) ≤\n\n(cid:115)\n\n2H α\n\n2S log (cid:0) KHSA δ′ nk(s, a)\n\n(cid:1)\n\n.\n\nFor any k > 0, h ∈ [H] and (s, a) ∈ S × A, let wkh(s, a) denote the probability of visiting (s, a) at step h of episode k. Then, it holds that for any k > 0, h ∈ [H] and (s, a) ∈ S ×A, wkh(s, a) ∈ [0, 1] and (cid:80)\n\n(s,a)∈S×A wkh(s, a) = 1.\n\nLemma 3 (Concentration of Visitation). It holds that\n\n(cid:34)\n\nPr\n\nnk(s, a) ≥\n\n1 2\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nwk′h(s, a) − H log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n, ∀k > 0, ∀(s, a) ∈ S × A\n\n≥ 1 − δ′.\n\n(cid:35)\n\nProof of Lemma 3. Applying Lemma F.4 in (Dann et al., 2017), we have that for any fixed h ∈ [H],\n\n(cid:34)\n\nPr\n\nnkh(s, a) ≥\n\n1 2\n\nk−1 (cid:88)\n\nk′=1\n\nwk′h(s, a) − log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n, ∀k > 0, ∀(s, a) ∈ S × A\n\n≥ 1 −\n\n(cid:35)\n\nδ′ H\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nBy a union bound over h ∈ [H], we have\n\n(cid:34)\n\nPr\n\nnk(s, a) ≥\n\n1 2\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nwk′h(s, a) − H log\n\n(cid:19)(cid:35)\n\n(cid:18) HSA δ′\n\n≥ 1 − δ′.\n\nTo sum up, we define several concentration events which will be used in the following proof.\n\n(cid:40)\n\nE1 :=\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V ∗\n\nh (s′)) − CVaRα\n\ns′∼p(·|s,a)(V ∗\n\nh (s′))\n\n(cid:12) (cid:12) (cid:12) ≤\n\nH α\n\n(cid:115)\n\nlog (cid:0) KHSA nk(s, a)\n\nδ′\n\n(cid:1)\n\n,\n\n∀k ∈ [K], ∀h ∈ [H], ∀(s, a) ∈ S × A\n\n(cid:41)\n\n(cid:40)\n\nE2 :=\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V (s′)) − CVaRα\n\n(cid:12) s′∼p(·|s,a)(V (s′)) (cid:12) (cid:12) ≤\n\n(cid:115)\n\n2H α\n\n2S log (cid:0) KHSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\n∀V : S (cid:55)→ [0, H], ∀k ∈ [K], ∀(s, a) ∈ S × A\n\n(cid:41)\n\n(cid:40)\n\nE3 :=\n\nnk(s, a) ≥\n\n1 2\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nwk′h(s, a) − H log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n, ∀k > 0, ∀(s, a) ∈ S × A\n\n(cid:41)\n\nE :=E1 ∩ E2 ∩ E3 Lemma 4. Letting δ′ = δ\n\n5 , it holds that\n\nPr [E] ≥ 1 − δ.\n\nProof of Lemma 4. This lemma can be obtained by combining Lemmas 1-3.\n\nD.1.2 OPTIMISM, VISITATION AND CVAR GAP Recall that L := log (cid:0) KHSA\n\n(cid:1).\n\nδ′\n\nLemma 5 (Optimism). Suppose that event E holds. Then, for any k ∈ [K], h ∈ [H] and s ∈ S, we have\n\n ̄V k\n\nh (s) ≥ V ∗\n\nh (s).\n\nProof of Lemma 5. We prove this lemma by induction. First, for any k ∈ [K], s ∈ S, it holds that ̄V k\n\nH+1(s) = V ∗ Then, for any k ∈ [K], h ∈ [H] and (s, a) ∈ S × A, if ̄Qk holds, and otherwise,\n\nH+1(s) = 0. h(s, a) = H, ̄Qk\n\nh(s, a) ≥ Q∗\n\nh(s, a) trivially\n\n ̄Qk\n\nh(s, a) =r(s, a) + CVaRα\n\ns′∼ ˆpk(·|s,a)( ̄V k\n\nh+1(s′)) +\n\n(a)\n\n≥r(s, a) + CVaRα\n\ns′∼ ˆpk(·|s,a)(V ∗\n\nh+1(s′)) +\n\n(cid:115)\n\n(cid:115)\n\nH α\n\nH α\n\nL nk(s, a)\n\nL nk(s, a)\n\n(b)\n\n≥r(s, a) + CVaRα =Q∗\n\nh(s, a),\n\ns′∼p(·|s,a)(V ∗\n\nh+1(s′))\n\nwhere (a) uses the induction hypothesis and (b) comes from Lemma 1.\n\nThus, we have\n\n ̄V k\n\nh (s) ≥ ̄Qk\n\nh(s, π∗\n\nh(s)) ≥ Q∗\n\nh(s, π∗\n\nh(s)) = V ∗\n\nh (s),\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nwhich concludes the proof.\n\nFollowing (Zanette & Brunskill, 2019), for any episode k > 0, we define the set of state-action pairs which have sufficient visitations in expectation as follows.\n\n(cid:40)\n\nLk :=\n\n(s, a) ∈ S × A :\n\n1 4\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nwk′h(s, a) ≥ H log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:41)\n\n+ H\n\n.\n\n(10)\n\nLemma 6 (Sufficient Visitation). Suppose that event E holds. Then, for any k > 0 and (s, a) ∈ Lk,\n\nnk(s, a) ≥\n\n1 4\n\nk (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nwk′h(s, a).\n\nProof of Lemma 6. This proof is the same as that of Lemma 6 in (Zanette & Brunskill, 2019).\n\nUsing Lemma 3, we have\n\nnk(s, a) ≥\n\n=\n\n(a) ≥\n\n(b) ≥\n\n=\n\n1 2\n\n1 4\n\n1 4\n\n1 4\n\n1 4\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nk (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nwk′h(s, a) − H log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\nwk′h(s, a) +\n\n1 4\n\nk−1 (cid:88)\n\nH (cid:88)\n\nk′=1\n\nh=1\n\nwk′h(s, a) − H log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\nwk′h(s, a) + H\n\nwk′h(s, a) +\n\nH (cid:88)\n\nh=1\n\nwkh(s, a)\n\nwk′h(s, a)\n\nwhere (a) uses the fact that (s, a) ∈ Lk and the definition of Lk, and (b) is due to that for any k > 0, h ∈ [H] and (s, a) ∈ S × A, wkh(s, a) ∈ [0, 1].\n\nLemma 7 (Standard Visitation Ratio). For any K > 0, we have\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nwkh(s, a) nk(s, a)\n\n(cid:115)\n\n≤2\n\nSA log\n\n(cid:18) KHSA δ′\n\n(cid:19)\n\n.\n\nProof of Lemma 7. This proof is the same as that of Lemma 13 in (Zanette & Brunskill, 2019). Recall that for any k > 0, let wk(s, a) := (cid:80)\n\n(s,a)∈S×A wkh(s, a). Then, we have\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nwkh(s, a) nk(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n=\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\n(s,a)∈Lk\n\nwk(s, a) nk(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n=\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\n(s,a)∈S×A\n\nwk(s, a) nk(s, a)\n\n· 1 {(s, a) ∈ Lk}\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(a) ≤2\n\nK (cid:88)\n\n(cid:88)\n\nwk(s, a)\n\nk=1\n\n(s,a)∈S×A\n\n(cid:80)k\n\nk′=1 wk′(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n=2\n\n(cid:88)\n\nK (cid:88)\n\nwk(s, a)\n\n(cid:80)k\n\nk′=1 wk′(s, a)\n\n(s,a)∈S×A\n\nk=1\n\n23\n\n· 1 {(s, a) ∈ Lk}\n\n· 1 {(s, a) ∈ Lk}\n\nPublished as a conference paper at ICLR 2023\n\nwhere (a) is due to Lemma 6.\n\nAccording to the definition of Lk (Eq. (10)), for any (s, a) ∈ S × A, once (s, a) satisfies (s, a) ∈ Lk in some episode k, it will always satisfy (s, a) ∈ Lk′ for all k′ ≥ k. For any (s, a) ∈ S × A, let k0(s, a) denote the first episode k where (s, a) ∈ Lk.\n\nThen, for any k > 0 and (s, a) ∈ S × A, if (s, a) ∈ Lk, we have\n\nk (cid:88)\n\nk′=1\n\nwk′(s, a) =\n\nk0(s,a)−1 (cid:88)\n\nk′=1\n\nwk′(s, a) +\n\nk (cid:88)\n\nk′=k0(s,a)\n\nwk′(s, a)\n\n(a) ≥H +\n\nk (cid:88)\n\nwk′(s, a),\n\nk′=k0(s,a)\n\nwhere (a) uses the fact that (s, a) ∈ Lk0(s,a) and the definition of Lk (Eq. (10)).\n\nThus, we have\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nwkh(s, a) nk(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n≤2\n\n(cid:88)\n\nK (cid:88)\n\nwk(s, a)\n\nH + (cid:80)k\n\nk′=k0(s,a) wk′(s, a)\n\n.\n\n(s,a)∈S×A\n\nk=k0(s,a)\n\nLet a1 := wk0(s,a)(s, a), a2 := wk0(s,a)+1(s, a), . . . , aK−k0(s,a)+1 := wK(s, a). Define function F (x) = (cid:80)⌊x⌋ i=1 ai + a⌈x⌉(x − ⌊x⌋), where 0 ≤ x ≤ K − k0(s, a) + 1. If x is an integer, we have F (x) = (cid:80)x i=1 ai, and otherwise, F (x) interpolates between the function values for integers x. The derivative of F (s) is f (x) = a⌈x⌉.\n\nHence, we have\n\nK (cid:88)\n\nwk(s, a)\n\nk=k0(s,a)\n\nH + (cid:80)k\n\nk′=k0(s,a) wk′(s, a)\n\n=\n\n=\n\n(a) ≤\n\nK−k0(s,a)+1 (cid:88)\n\nk=1\n\nf (k) H + F (k)\n\n(cid:90) K−k0(s,a)+1\n\n0\n\n(cid:90) K−k0(s,a)+1\n\n0\n\nf (⌈x⌉) H + F (⌈x⌉)\n\ndx\n\nf (x) H + F (x)\n\ndx\n\n= log (H + F (K − k0(s, a) + 1)) − log (H + F (0))\n\n(b) ≤ log (KH)\n\n≤ log\n\n(cid:18) KHSA δ′\n\n(cid:19)\n\n,\n\nwhere (a) uses the fact that for any 0 ≤ x ≤ K − k0(s, a) + 1, f (x) = f (⌈x⌉) and F (x) ≤ F (⌈x⌉), and (b) is due to that k0(s, a) ≥ 2 by the definitions of Lk (Eq. (10)) and k0(s, a).\n\nTherefore, we have\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nwkh(s, a) nk(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n≤2\n\n(cid:88)\n\n(s,a)∈S×A\n\nlog\n\n(cid:19)\n\n(cid:18) KHSA δ′\n\n(cid:115)\n\n≤2\n\nSA log\n\n(cid:19)\n\n(cid:18) KHSA δ′\n\nRecall that for any (s′, s, a) ∈ S × S × A, p(s′|s, a) is the transition probability from (s, a) to s′. For any risk level α ∈ (0, 1], function V : S (cid:55)→ R and (s′, s, a) ∈ S × S × A, βα,V (s′|s, a) is the conditional probability of transitioning to s′ from (s, a), conditioning on transitioning to the\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nworst α-portion successor states s′ (i.e., with the lowest α-portion values V (s′)), and it holds that CVaRα\n\ns′∼p(s′|s,a)(V (s′)) = (cid:80)\n\ns′∈S βα,V (s′|s, a) · V (s′).\n\nFor any k > 0, h ∈ [H] and (s, a) ∈ S × A, wkh(s, a) is the probability of visiting (s, a) at step h of episode k (under transition probability p(·|·, ·)), and it holds that wkh(s, a) ∈ [0, 1] and (cid:80) (s,a)∈S×A wkh(s, a) = 1. For any risk level α ∈ (0, 1], k > 0, h ∈ [H] and (s, a) ∈ S × A,\n\nkh\n\nwCV aR,α,V πk (s, a) is the conditional probability of visiting (s, a) at step h of episode k, conditioning on transitioning to the worst α-portion successor states s′ (i.e., with the lowest α-portion values V πk h′+1(s′)) at each step h′ = 1, . . . , h − 1. Here πk is the policy taken in episode k, and\n\nh (·) : S (cid:55)→ R is the value function at step h for policy πk.\n\nV πk (s, a) is the probability of visiting (s, a) at step h of episode k under conditional transition probability βα,V πk h ∈ [H] and (s, a) ∈ S ×A, wCV aR,α,V πk\n\nh′+1(·|·, ·) for each step h′ = 1, . . . , h − 1. It holds that for any risk level α ∈ (0, 1], k > 0,\n\n(s, a) ∈ [0, 1] and (cid:80)\n\n(s,a)∈S×A wCV aR,α,V πk\n\n(s, a) = 1.\n\nIntuitively, wCV aR,α,V πk\n\nkh\n\nkh\n\nkh\n\nLemma 8. For any risk level α ∈ (0, 1], k > 0, h ∈ [H] and (s, a) ∈ S × A, if wkh(s, a) = 0, then wCV aR,α,V πk\n\n(s, a) = 0.\n\nkh\n\nProof of Lemma 8. If wkh(s, a) = 0, then the algorithm has zero probability to visit (s, a) at step h of episode k, which means that (s, a) is unreachable under transition probability p(·|·, ·).\n\nNote that for each step h′ = 1, . . . , h − 1, the conditional transition probability βα,V πk h′+1(s′|s, a) just renormalizes the transition probability and assigns more weights to the worst α-portion successor states s′ (i.e., with the lowest α-portion values V πk h′+1(s′)), but will not make an unreachable successor state reachable. Thus, (s, a) is also unreachable under conditional transition probability βα,V πk Lemma 9. For any functions V1, . . . , VH : S (cid:55)→ R, k > 0, h ∈ [H] and (s, a) ∈ S × A such that wkh(s, a) > 0,\n\nh′+1(·|·, ·) for each step h′ = 1, . . . , h − 1, and therefore wCV aR,α,V πk\n\n(s, a) = 0.\n\nkh\n\nwCVaR,α,V\n\nkh\n\n(s, a)\n\nwkh(s, a)\n\n≤ min\n\n \n\n\n\n1\n\nmin π,h,(s,a): wπ,h(s,a)>0\n\nwπ,h(s, a)\n\n,\n\n1 αh−1\n\n \n\n\n\n,\n\nkh\n\nwhere wCVaR,α,V (s, a) denotes the conditional probability of visiting (s, a) at step h of episode k, conditioning on transitioning to the worst α-portion successor states s′ (i.e., with the lowest αportion values Vh′+1(s′)) at each step h′ = 1, . . . , h − 1.\n\nProof of Lemma 9. Since wCVaR,α,V (s, a) is the conditional probability of visiting (s, a), we have wCVaR,α,V (s, a) ∈ [0, 1]. Since wkh(s, a) is the probability of visiting (s, a) at step h under policy πk and minπ,h,(s,a): wπ,h(s,a)>0 wπ,h(s, a) is the minimum probability of visiting any reachable (s, a) at any step h over all policies π, we have\n\nkh\n\nkh\n\nwkh(s, a) ≥\n\nmin π,h,(s,a): wπ,h(s,a)>0\n\nwπ,h(s, a).\n\nHence, we have\n\nwCVaR,α,V\n\nkh\n\n(s, a)\n\nwkh(s, a)\n\n≤\n\n1\n\nmin π,h,(s,a): wπ,h(s,a)>0\n\nwπ,h(s, a)\n\n.\n\n(11)\n\nLet s1 be the initial state. Since wkh(s, a) and wCVaR,α,V (s, a) are the probabilities of visiting (s, a) kh at step h with policy πk under transition probability p(·|·, ·) and conditional transition probability βα,Vh′+1(·|·, ·) for each step h′ = 1, . . . , h − 1, respectively, we have that\n\nwkh(s, a) =\n\n(cid:88)\n\nh−1 (cid:89)\n\np(sh′+1|sh′, ah′)\n\n(s2,...,sh−1)∈S h−2\n\nh′=1\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nand\n\nwCVaR,α,V\n\nkh\n\n(s, a) =\n\n(cid:88)\n\nh−1 (cid:89)\n\n(s2,...,sh−1)∈S h−2\n\nh′=1\n\nβα,Vh′+1(sh′+1|sh′, ah′),\n\nwhere s1 is the initial state, sh := s, and ah′ := πk(sh′) for h′ = 1, . . . , h − 1. Recall that for any risk level α ∈ (0, 1], function V : S (cid:55)→ R and (s′, s, a) ∈ S×S×A, μα,V (s′|s, a) denotes how large the transition probability of successor state s′ belongs to the worst α-portion successor states (i.e., with the lowest α-portion values V (·)), which satisfies that μα,V (s′|s,a) =\nβα,V (s′|s, a) and 0 ≤ μα,V (s′|s, a) ≤ p(s′|s, a).\n\nα\n\nThus, we have\n\nwCVaR,α,V\n\nkh\n\n(s, a) =\n\n(cid:88)\n\nh−1 (cid:89)\n\n(s2,...,sh−1)∈S h−2\n\nh′=1\n\n(cid:88)\n\nh−1 (cid:89)\n\n≤\n\n(s2,...,sh−1)∈S h−2\n\nh′=1\n\nμα,Vh′+1(sh′+1|sh′, ah′) α\n\np(sh′+1|sh′, ah′) α\n\n=\n\n1 αh−1\n\n(cid:88)\n\nh−1 (cid:89)\n\np(sh′+1|sh′, ah′)\n\n(s2,...,sh−1)∈S h−2\n\nh′=1\n\n=\n\n1\n\nαh−1 · wkh(s, a)\n\nTherefore,\n\nwCVaR,α,V\n\nkh\n\n(s, a)\n\nwkh(s, a)\n\n≤\n\n1\n\nαh−1 .\n\n(12)\n\nCombining Eqs. (11) and (12), we obtain this lemma.\n\nLemma 10 (Insufficient Visitation). It holds that\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a) /∈Lk\n\nwCVaR,α,V πk\n\nkh\n\n(s, a) ≤ min\n\n \n\n\n\n(cid:18)\n\n,\n\n1 αH−1\n\n \n\n\n\n·\n\n1\n\nmin π,h,s: wπ,h(s)>0 (cid:19)\n\n(cid:18) HSA δ′\n\n4SAH log\n\nwπ,h(s)\n\n(cid:19)\n\n+ 5SAH\n\n.\n\nProof of Lemma 10. According to the definition of Lk (Eq. (10)), for any (s, a) ∈ S × A, once (s, a) satisfies (s, a) ∈ Lk in some episode k, it will always satisfy (s, a) ∈ Lk′ for all k′ ≥ k. For any (s, a) ∈ S × A, let ̃k(s, a) denote the last episode k where (s, a) /∈ Lk. Then, we have\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nwkh(s, a) =\n\n(cid:88)\n\nK (cid:88)\n\nH (cid:88)\n\nk=1\n\nh=1\n\n(s,a) /∈Lk\n\n(s,a)∈S×A\n\nk=1\n\nh=1\n\nwkh(s, a) · 1 {(s, a) /∈ Lk}\n\n=\n\n=\n\n(a) <\n\n(cid:88)\n\n ̃k(s,a) (cid:88)\n\nH (cid:88)\n\nwkh(s, a)\n\n(s,a)∈S×A\n\n(cid:88)\n\nh=1\n\nk=1 \n\n\n\n ̃k(s,a)−1 (cid:88)\n\nH (cid:88)\n\nwkh(s, a) +\n\n(s,a)∈S×A\n\nk=1\n\nh=1\n\n\n\nw ̃k(s,a),h(s, a)\n\n\n\nH (cid:88)\n\nh=1\n\n(cid:88)\n\n(cid:18)\n\n4H log\n\n(s,a)∈S×A\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:19)\n\n+ 4H + H\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Illustrating example for Lemma 11. For each s′ ∈ {s1, s2, s3}, the height of the bar denotes the value V (s′) or ̄V (s′), and the width of the bar denotes the transition probability p(s′|s, a) (fixed). The colored part of the bars denotes the worst α-portion successor states (i.e., with the lowest α-portion values V (s′) or ̄V (s′)). In this example, α = 0.5.\n\n≤4SAH log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n+ 5SAH,\n\nwhere (a) is due to that (s, a) /∈ L ̃k(s,a), and for any k > 0, h ∈ [H] and (s, a) ∈ S × A, wkh(s, a) ∈ [0, 1].\n\nFor any policy π, h ∈ [H] and (s, a) ∈ S × A, let wπ,h(s, a) and wπ,h(s) denote the probabilities of visiting (s, a) and s at step h under policy π, respectively. Then, we have\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nwCVaR,α,V πk\n\nkh\n\n(s, a)\n\nk=1\n\nh=1\n\n(s,a) /∈Lk\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\n(a)=\n\nk=1\n\n(s,a) /∈Lk\n\nh=1 \n\n\n(b) ≤ min\n\n\n\n \n\n\n\n(c) ≤ min\n\nwCVaR,α,V πk\n\nkh\n\n(s, a)\n\nwkh(s, a)\n\n1\n\n· wkh(s, a) · 1 {wkh(s, a) ̸= 0}\n\n,\n\n1 αH−1\n\n \n\n\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a) /∈Lk\n\nwkh(s, a)\n\nmin π,h,(s,a): wπ,h(s,a)>0\n\nwπ,h(s, a)\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n(cid:18)\n\n \n\n\n\n4SAH log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:19)\n\n+ 5SAH\n\n,\n\nHere (a) is due to Lemma 8. (b) comes from Lemma 9. (c) uses the fact that for any deterministic policy π, h ∈ [H] and (s, a) ∈ S × A, we have either wπ,h(s, a) = wπ,h(s) or wπ,h(s, a) = 0, and thus minπ,h,(s,a): wπ,h(s,a)>0 wπ,h(s, a) = minπ,h,s: wπ,h(s)>0 wπ,h(s).\n\nRecall that for any risk level α ∈ (0, 1], function V : S (cid:55)→ R and (s′, s, a) ∈ S×S×A, βα,V (s′|s, a) is the conditional probability of transitioning to s′ from (s, a), conditioning on transitioning to the worst α-portion successor states s′ (i.e., with the lowest α-portion values V (s′)), and it holds that CVaRα\n\ns′∼p(s′|s,a)(V (s′)) = (cid:80)\n\ns′∈S βα,V (s′|s, a) · V (s′).\n\nLemma 11 (CVaR Gap due to Value Function Shift). For any (s, a) ∈ S ×A, distribution p(·|s, a) ∈ △S , and functions V, ̄V : S (cid:55)→ [0, H] such that ̄V (s′) ≥ V (s′) for any s′ ∈ S,\n\nCVaRα\n\ns′∼p(·|s,a)( ̄V (s′)) − CVaRα\n\ns′∼p(·|s,a)(V (s′)) ≤ βα,V (·|s, a)⊤ (cid:0) ̄V − V (cid:1) .\n\n27\n\n(cid:2869)(cid:2870)(cid:2871)ps(cid:2869)s,a=0.3ps(cid:2870)s,a=0.3ps(cid:2871)s,a=0.4μ(cid:3080),(cid:3023)s(cid:2869)s,a=0.3μ(cid:3080),(cid:3023)s(cid:2870)s,a=0.2μ(cid:3080),(cid:3023)s(cid:2871)s,a=0(cid:2869)(cid:2870)(cid:2871)ps(cid:2869)s,a=0.3ps(cid:2870)s,a=0.3ps(cid:2871)s,a=0.4μ(cid:3080),(cid:3023)(cid:3365)s(cid:2869)s,a=0.2μ(cid:3080),(cid:3023)(cid:3365)s(cid:2870)s,a=0.3μ(cid:3080),(cid:3023)(cid:3365)s(cid:2871)s,a=0Original -quantilelineShifted -quantileline(cid:3048)(cid:3043)(cid:2869)(cid:3031)(cid:3042)(cid:3050)(cid:3041)(cid:2870)(cid:3048)(cid:3041)(cid:3030)(cid:3035)(cid:2871)Published as a conference paper at ICLR 2023\n\nProof of Lemma 11. Recall that for any risk level α ∈ (0, 1], function V : S (cid:55)→ R and (s′, s, a) ∈ S × S × A, βα,V (s′|s, a) is the conditional transition probability from (s, a) to s′, conditioning on transitioning to the worst α-portion successor states s′ (i.e., with the lowest α-portion values V (s′)), and μα,V (s′|s, a) denotes how large the transition probability of successor state s′ belongs to the worst α-portion, which satisfies that μα,V (s′|s,a) s′∈S μα,V (s′|s, a) = α. Then, for any risk level α ∈ (0, 1], function V : S (cid:55)→ R and (s′, s, a) ∈ S × S × A,\n\n= βα,V (s′|s, a) and (cid:80)\n\nα\n\nCVaRα\n\ns′∼p(·|s,a)(V (s′)) =\n\n(cid:80)\n\ns′∈S μα,V (s′|s, a) · V (s′) α\n\n=\n\n(cid:88)\n\ns′∈S\n\nβα,V (s′|s, a) · V (s′),\n\nAs shown in Figure 5, we sort all successor states s′ ∈ S by their values V (s′) in ascending order (from left to right). Fix the transition probability p(·|s, a) and the value function shifts from V (·) to ̄V (·). Then, below we divide all successor states s′ ∈ S into three subsets, i.e., Sup, Sdown and Sunch, according to how μα,V (s′|s, a) changes to μα, ̄V (s′|s, a) as V (s′) shifts to ̄V (s′).\n\n• For any s′ ∈ Sup, μα, ̄V (s′|s, a) < μα,V (s′|s, a), the rank of s′ goes up, and the position of s′ moves to the right (here “rank” means to rank all successor states s′ ∈ S by their values V (s′) or ̄V (s′) from highest to lowest).\n\n• For any s′ ∈ Sdown, μα, ̄V (s′|s, a) > μα,V (s′|s, a), the rank of s′ goes down, and the\n\nposition of s′ moves to the left.\n\n• For any s′ ∈ Sunch, μα, ̄V (s′|s, a) = μα,V (s′|s, a), the rank and position of s′ keep un-\n\nchanged.\n\nThen, it holds that\n\n(cid:88)\n\n(cid:16)\n\nμα, ̄V (s′|s, a) − μα,V (s′|s, a)\n\n(cid:17)\n\n+\n\n(cid:88)\n\n(cid:16)\n\nμα, ̄V (s′|s, a) − μα,V (s′|s, a)\n\n(cid:17)\n\n= 0.\n\n(13)\n\ns′∈Sup\n\nNext, we have\n\ns′∈Sdown\n\nCVaRα\n\ns′∼p(·|s,a)( ̄V (s′)) − CVaRα\n\ns′∼p(·|s,a)(V (s′))\n\n=\n\n1 α\n\n·\n\n(cid:32)\n\n(cid:88)\n\n(cid:16)\n\nμα, ̄V (s′|s, a) · ̄V (s′) − μα,V (s′|s, a) · V (s′)\n\n(cid:17)\n\ns′∈Sup (cid:16) (cid:88)\n\nμα, ̄V (s′|s, a) · ̄V (s′) − μα,V (s′|s, a) · V (s′)\n\n(cid:17)\n\n+\n\n+\n\n=\n\n1 α\n\n+\n\n+\n\n=\n\n1 α\n\n+\n\ns′∈Sdown\n\n(cid:88)\n\n(cid:16)\n\nμα, ̄V (s′|s, a) · ̄V (s′) − μα,V (s′|s, a) · V (s′)\n\n(cid:17)\n\n(cid:33)\n\ns′∈Sunch (cid:32)\n\n(cid:88)\n\n·\n\n(cid:18)\n\nμα,V (s′|s, a) · (cid:0) ̄V (s′) − V (s′)(cid:1) +\n\n(cid:16)\n\nμα, ̄V (s′|s, a) − μα,V (s′|s, a)\n\n(cid:17)\n\n(cid:19)\n\n· ̄V (s′)\n\ns′∈Sup\n\n(cid:18)\n\n(cid:88)\n\ns′∈Sdown\n\nμα,V (s′|s, a) · (cid:0) ̄V (s′) − V (s′)(cid:1) +\n\n(cid:16)\n\nμα, ̄V (s′|s, a) − μα,V (s′|s, a)\n\n(cid:17)\n\n(cid:19)\n\n· ̄V (s′)\n\n(cid:88)\n\nμα,V (s′|s, a) · (cid:0) ̄V (s′) − V (s′)(cid:1)\n\n(cid:33)\n\ns′∈Sunch (cid:32)\n\n(cid:88)\n\n·\n\nμα,V (s′|s, a) · (cid:0) ̄V (s′) − V (s′)(cid:1) −\n\n(cid:88)\n\n(cid:16)\n\nμα,V (s′|s, a) − μα, ̄V (s′|s, a)\n\n(cid:17)\n\n· ̄V (s′)\n\ns∈S\n\n(cid:88)\n\n(cid:16)\n\ns′∈Sdown\n\nμα, ̄V (s′|s, a) − μα,V (s′|s, a)\n\ns′∈Sup\n\n(cid:33)\n\n· ̄V (s′)\n\n(cid:17)\n\n28\n\nPublished as a conference paper at ICLR 2023\n\n(cid:32)\n\n(a) ≤\n\n1 α\n\n(cid:88)\n\ns∈S\n\nμα,V (s′|s, a) · (cid:0) ̄V (s′) − V (s′)(cid:1) − min\n\ns′∈Sup\n\n+ min s′∈Sup\n\n ̄V (s′) ·\n\n(cid:88)\n\n(cid:16)\n\ns′∈Sdown\n\nμα, ̄V (s′|s, a) − μα,V (s′|s, a)\n\n(b)=\n\n1 α\n\n·\n\n(cid:88)\n\ns∈S\n\nμα,V (s′|s, a) · (cid:0) ̄V (s′) − V (s′)(cid:1)\n\ns′∈Sup (cid:33)\n\n(cid:17)\n\n ̄V (s′) ·\n\n(cid:88)\n\n(cid:16)\n\nμα,V (s′|s, a) − μα, ̄V (s′|s, a)\n\n(cid:17)\n\n=βα,V (·|s, a)⊤ (cid:0) ̄V − V (cid:1) Here (a) is due to that for any s′ ∈ Sup, μα, ̄V (s′|s, a) < μα,V (s′|s, a), and for any s ∈ Sup, s′ ∈ Sdown, ̄V (s) ≥ ̄V (s′). (b) comes from Eq. (13).\n\nD.1.3 PROOF OF THEOREM 1\n\nProof of Theorem 1. Suppose that event E holds. Then, for any k ∈ [K],\n\nV ∗\n\n1 (sk\n\n1) − V πk\n\n1\n\n(a)\n\n≤ ̄V k\n\n1) − V πk 1 (sk (cid:40)\n\n1\n\n(sk 1)\n\n(sk 1)\n\n= min\n\nr(sk\n\n1, ak\n\n1) + CVaRα\n\ns′∼ ˆpk(·|sk\n\n1 ,ak\n\n1 )( ̄V k\n\n2 (s′)) +\n\n(cid:115)\n\nH α\n\nL nk(sk 1, ak 1)\n\n, H\n\n(cid:41)\n\n(cid:16)\n\n−\n\nr(sk\n\n1, ak\n\n1) + CVaRα\n\ns′∼p(·|sk\n\n1 ,ak\n\n1 )(V πk\n\n2\n\n(cid:17)\n\n(s′))\n\n≤r(sk\n\n1, ak\n\n1) + CVaRα\n\ns′∼ ˆpk(·|sk\n\n1 ,ak\n\n1 )( ̄V k\n\n2 (s′)) + min\n\n(cid:115)\n\n(cid:40)\n\nH α\n\nL nk(sk 1, ak 1)\n\n, H\n\n(cid:41)\n\n, H\n\n+ CVaRα\n\ns′∼ ˆpk(·|sk\n\n1 ,ak\n\n1 )( ̄V k\n\n2 (s′)) − CVaRα\n\ns′∼p(·|sk\n\n1 ,ak\n\n1 )( ̄V k\n\n2 (s′))\n\n2 (s′)) − CVaRα\n\n(cid:41)\n\n(cid:40)\n\n, H\n\n+min\n\ns′∼p(·|sk (cid:115)\n\n4H α\n\n1 )(V πk\n\n2\n\n1 ,ak\n\nSL 1, ak nk(sk 1)\n\n(s′))\n\n(cid:41)\n\n, H\n\n+βα,V πk\n\n2\n\n(·|sk\n\n1, ak\n\n1)⊤( ̄V k\n\n2 − V πk\n\n2\n\n)\n\n(cid:41)\n\n, 2H\n\n+\n\n(cid:88)\n\nβα,V πk\n\n2\n\ns2∈S\n\n(cid:41)\n\n, 2H\n\n+\n\n(cid:88)\n\nβα,V πk\n\n2\n\n(s2|sk\n\n1, ak\n\n1) · ( ̄V k\n\n2 (s2) − V πk\n\n2\n\n(s2))\n\n(s2|sk\n\n1, ak\n\n1)·\n\ns2∈S\n\n(cid:41)\n\n, 2H\n\n+\n\n(cid:88)\n\nβα,V πk\n\n3\n\n(s3|s2, a2) · ( ̄V k\n\n3 (s3) − V πk\n\n3\n\n(cid:33)\n\n(s3))\n\nSL\n\n(cid:16)\n\n−\n\nr(sk\n\n1) + CVaRα\n\ns′∼p(·|sk\n\n1 ,ak\n\n1 )(V πk\n\n2\n\n(cid:17)\n\n(s′))\n\n1, ak (cid:115)\n\n(cid:40)\n\n= min\n\n(cid:41)\n\nL nk(sk 1, ak 1) 1 )( ̄V k\n\n1 ,ak\n\nH α\n\nH α\n\nH\n\nH\n\n+ CVaRα (cid:40)\n\ns′∼p(·|sk (cid:115)\n\n(b) ≤ min\n\n(cid:40)\n\n(cid:40)\n\n(c) ≤ min\n\n(d) ≤ min\n\n(cid:40)\n\n(cid:32)\n\nmin\n\n(cid:40)\n\n(e) ≤ min\n\nH\n\n(cid:40)\n\n(cid:32)\n\nmin\n\nL 1, ak nk(sk 1) √\n\n√\n\nSL\n\nL + 4H α(cid:112)nk(sk √\n\nL + 4H α(cid:112)nk(sk √\n\n1, ak 1) √\n\nSL\n\n1, ak 1) √\n\nH\n\nL + 4H α(cid:112)nk(s2, a2) √\n\n√\n\nSL\n\nL + 4H α(cid:112)nk(sk √\n\n1, ak 1) √\n\nH\n\nL + 4H α(cid:112)nk(s2, a2)\n\nSL\n\ns3∈S\n\n(cid:41)\n\n, 2H\n\n+\n\n(cid:88)\n\nβα,V πk\n\n2\n\ns2∈S\n\n(s2|sk\n\n1, ak\n\n1)·\n\n(cid:41)\n\n, 2H\n\n+\n\n(cid:88)\n\nβα,V πk\n\n3\n\n(s3|s2, a2)·\n\ns3∈S\n\n29\n\nPublished as a conference paper at ICLR 2023\n\n(cid:32)\n\n· · ·\n\n(cid:88)\n\nsH ∈S\n\nβα,V πk\n\nH (sH |sH−1, aH−1) ·\n\n(cid:32)\n\n(cid:40)\n\nmin\n\n√\n\n√\n\nH L + 4H SL α(cid:112)nk(sH , aH )\n\n(cid:41)(cid:33)(cid:33) (cid:33)\n\n, 2H\n\nH (cid:88)\n\n(f)=\n\n(cid:88)\n\nwCV aR,α,V πk\n\nkh\n\n(s, a) · min\n\n(cid:40)\n\nH\n\nh=1\n\n(s,a)∈S×A\n\nH (cid:88)\n\n≤\n\n(cid:88)\n\nwCV aR,α,V πk\n\nkh\n\n(s, a) ·\n\nH\n\nh=1\n\n(s,a)∈Lk\n\nSL\n\nL + 4H α(cid:112)nk(s, a)\n\n√\n\n√\n\nSL\n\n(cid:41)\n\n, 2H\n\n√\n\nL + 4H α(cid:112)nk(s, a) √\n\nH (cid:88)\n\n+\n\n(cid:88)\n\nwCV aR,α,V πk\n\nkh\n\n(s, a) · 2H\n\nh=1\n\n(s,a) /∈Lk\n\n(14)\n\ns′∼ ˆpk(·|s,a)( ̄V k\n\nHere ah := πk(sh) for h = 2, . . . , H. (a) is due to Lemma 5. (b) uses Lemma 2 and the fact that for any k > 0, h ∈ [H] and s ∈ S, ̄V k h (s) ∈ [0, H], and thus for any k > 0, h ∈ [H] and (s, a) ∈ S ×A, CVaRα h+1(s′)) ≤ H, and also uses Lemma 11. (c) comes from the property of min{·, ·}. (d) and (e) follow from recurrently applying steps (a)-(c). (f) is due to that wCV aR,α,V πk the conditional transition probability βα,V πk\n\n(s, a) is defined as the probability of visiting (s, a) at step h of episode k under\n\nh′+1(·|·, ·) for each step h′ = 1, . . . , h − 1.\n\nh+1(s′)) − CVaRα\n\ns′∼p(·|s,a)( ̄V k\n\nkh\n\nSince the second term in Eq. (14) can be bounded by Lemma 10, below we analyze the first term.\n\nRecall that for any policy π, h ∈ [H] and (s, a) ∈ S × A, wπ,h(s, a) and wπ,h(s) denote the probabilities of visiting (s, a) and s at step h under policy π, respectively. Summing the first term in Eq. (14) over k ∈ [K], we have\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nwCVaR,α,V πk\n\nkh\n\n(s, a)\n\nH\n\nh=1\n\n(s,a)∈Lk\n\nk=1 √\n\nH\n\n≤\n\nL + 4H α\n\n√\n\nH\n\n(a)=\n\nL + 4H α\n\n√\n\nSL\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\n√\n\nSL\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\n(s,a)∈Lk\n\n√\n\n(H\n\n=\n\n√\n\nL + 4H α\n\nSL)\n\nk=1 √\n\nKH\n\nh=1 (cid:118) (cid:117) (cid:117) (cid:116)\n\n√\n\n√\n\nSL\n\nL + 4H α(cid:112)nk(s, a)\n\nkh\n\nwCVaR,α,V πk nk(s, a)\n\n(s, a)\n\nkh\n\nwCVaR,α,V πk nk(s, a)\n\n(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n·\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nwCVaR,α,V πk\n\nkh\n\n(s, a)\n\n· 1 {wkh(s, a) ̸= 0} ·\n\n√\n\nKH\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nwCVaR,α,V πk\n\nkh\n\n(s, a)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nwkh(s, a)\n\n·\n\nwkh(s, a) nk(s, a)\n\n·1 {wkh(s, a) ̸= 0}\n\n(H\n\n(b) ≤\n\n√\n\nL+4H\n\n√\n\nSL)\n\n√\n\nα\n\nKH\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\nmin\n\n(cid:26)\n\n1\n\nmin π,h,(s,a): wπ,h(s,a)>0\n\nwπ,h(s, a)\n\n,\n\n1 αH−1\n\n(cid:27) K\n\n(cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nwkh(s, a) nk(s, a)\n\n√\n\n(H\n\n(c) ≤\n\n√\n\nL + 4H α\n\n√\n\nSL)\n\nKH\n\n√\n\n· 2\n\nSAL · min\n\n \n\n\n\n1\n\n(cid:114)\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n√\n\n1\n\nαH−1\n\n \n\n\n\n√\n\n10HSL α\n\n≤\n\nKHA\n\n· min\n\n \n\n\n\n(cid:114)\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n√\n\n1\n\nαH−1\n\n \n\n\n\n.\n\nis due to Lemma 8 and the fact\n\nthat\n\nfor any k > 0 and h ∈ [H],\n\nHere (a) (cid:80)\n\n(s,a)∈S×A wCVaR,α,V πk\n\nkh\n\n(s, a) = 1. (b) comes from Lemma 9. (c) uses Lemma 7 and the fact that\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nfor any deterministic policy π, h ∈ [H] and (s, a) ∈ S × A, we have either wπ,h(s, a) = wπ,h(s) or wπ,h(s, a) = 0, and thus minπ,h,(s,a): wπ,h(s,a)>0 wπ,h(s, a) = minπ,h,s: wπ,h(s)>0 wπ,h(s).\n\nThen, summing the first and second terms in Eq. (14) over k ∈ [K] and using Lemma 10, we have\n\nR(K) =\n\nK (cid:88)\n\n(cid:16)\n\nk=1\n\nV ∗\n\n1 (sk\n\n1) − V πk\n\n1\n\n(cid:17)\n\n(sk 1)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\n≤\n\nwCVaR,α,V πk\n\nkh\n\n(s, a)\n\nH\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\n+\n\nwCVaR,α,V πk\n\nkh\n\n(s, a) · 2H\n\n√\n\n√\n\nSL\n\nL + 4H α(cid:112)nk(s, a)\n\nh=1\n\nk=1 \n\n\n(s,a) /∈Lk\n\n1\n\n≤ min\n\n(cid:114)\n\n\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n√\n\n1\n\nαH−1\n\n \n\n\n\n10HS\n\n√\n\nα\n\nKHA\n\nlog\n\n(cid:19)\n\n(cid:18) KHSA δ′\n\n+ min\n\n \n\n\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n \n\n\n\n(cid:18)\n\n8SAH 2 log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:19)\n\n+ 10SAH 2\n\nWhen K is large enough, the first term dominates the bound, and thus we obtain Theorem 1.\n\nD.2 PROOF OF REGRET LOWER BOUND\n\nBelow we prove the regret lower bound (Theorem 2) for Iterated CVaR RL-RM.\n\nProof of Theorem 2. First, we construct an instance where minπ,h,s: wπ,h(s)>0 wπ,h(s) > αH−1,\n\nand prove that on this instance any algorithm must suffer a Ω(\n\nregret.\n\n(cid:113)\n\nH minπ,h,s: wπ,h(s)>0 wπ,h(s)\n\n(cid:113) AK\n\nα )\n\nConsider the instance shown in Figure 6 (the same as Figure 1 in the main text):\n\nThe state space is S = {s1, s2, . . . , sn, x1, x2, x3}, where s1 is the initial state, and n = S − 3 < S < 1\n\n2 H.\n\nThe reward functions are as follows. For any a ∈ A, r(x1, a) = 1, r(x2, a) = 0.8 and r(x3, a) = 0.2. For any i ∈ [n] and a ∈ A, r(si, a) = 0. The transition distributions are as follows. Let μ be a parameter which satisfies that 0 < α < μ < 1 3 . For any a ∈ A, p(s2|s1, a) = μ, p(x1|s1, a) = 1 − 3μ, p(x2|s1, a) = μ and p(x3|s1, a) = μ. For any i ∈ {2, . . . , n − 1} and a ∈ A, p(si+1|si, a) = μ and p(x1|si, a) = 1 − μ. x1, x2 and x3 are absorbing states, i.e., for any a ∈ A, p(x1|x1, a) = 1, p(x2|x2, a) = 1 and p(x3|x3, a) = 1. Let aJ be the optimal action in state sn, which is uniformly drawn from A. For the optimal action aJ , p(x2|sn, aJ ) = 1 − α + η and p(x3|sn, aJ ) = α − η, where η is a parameter which satisfies 0 < η < α and will be chosen later. For any suboptimal action a ∈ A \\ {aJ }, p(x2|sn, a) = 1 − α and p(x3|sn, a) = α. For any aj ∈ A, let Ej[·] and Prj[·] denote the expectation and probability operators under the instance with aJ = aj. Let Eunif [·] and Prunif [·] denote the expectation and probability operators under the uniform instance where all actions a ∈ A in state sn have the same transition distribution, i.e., p(x2|sn, a) = 1 − α and p(x3|sn, a) = α. Fix an algorithm A. Let πk denote the policy taken by algorithm A in episode k. Let Nsn,aj = (cid:9) denote the number of episodes that the policy chooses aj in state sn. Let (cid:80)K Vsn,aj denote the number of episodes that the algorithm A visits (sn, aj). Let w(sn) denote the\n\n1 (cid:8)πk(sn) = aj\n\nk=1\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Instance of lower bounds (Theorems 2 and 5) for the minπ,h,s: wπ,h(s)>0 wπ,h(s) > αH−1 case.\n\nprobability of visiting sn in an episode (the probability of visiting sn is the same for all policies). Then, it holds that E[Vsn,aj ] = w(sn) · E[Nsn,aj ].\n\nRecall that aJ is the optimal action in state sn. According to the definition of the value function for Iterated CVaR RL, we have that\n\nV ∗\n\n1 (s1) =\n\n(α − η) · 0.2(H − n) + η · 0.8(H − n) α\n\n,\n\nand for any policy π,\n\nV π\n\n1 (s1) =\n\n(α − η) · 0.2(H − n) + η · 0.8(H − n) α\n+ 0.2(H − n) · (1 − 1 {π(sn) = aJ }) .\n\n· 1 {π(sn) = aJ }\n\nIf J = j, for any policy π,\n\nη · 0.6(H − n) α\nand summing over all episodes k ∈ [K], we have\n\n1 (s1) − V π\n\n1 (s1) =\n\nV ∗\n\n· (1 − 1 {π(sn) = aj}) ,\n\n(15)\n\nEj [R(K)] =\n\nK (cid:88)\n\nk=1\n\n(cid:16)\n\n1 (s1) − V πk V ∗\n\n1\n\n(cid:17)\n\n(s1)\n\nTherefore, we have\n\n=\n\nη · 0.6(H − n) α\n\n=\n\nη · 0.6(H − n) α\n\n(cid:32)\n\n·\n\nK −\n\nK (cid:88)\n\nk=1\n\n(cid:33)\n\n1 {π(sn) = aj}\n\n· (cid:0)K − Ej[Nsn,aj ](cid:1)\n\nE [R(K)] =\n\n=\n\n1 A\n\n1 A\n\nA (cid:88)\n\nK (cid:88)\n\n(cid:16)\n\nj=1\n\nk=1\n\n1 (s1) − V πk V ∗\n\n1\n\n(cid:17)\n\n(s1)\n\nA (cid:88)\n\nj=1\n\nη α\n\n· 0.6(H − n) (cid:0)K − Ej[Nsn,aj ](cid:1)\n\n=0.6(H − n) ·\n\n\n\n·\n\nK −\n\nη α\n\n1 A\n\nA (cid:88)\n\nj=1\n\n\n\nEj[Nsn,aj ]\n\n\n\n(16)\n\nFor any j ∈ [A], using Pinsker’s inequality and 0 < α < 1\n\nη2\n\n(α−η)(1−α+η) ≤ c1η2\n\n3 , we have that α for some\n\nKL(punif (sn, aj)∥pj(sn, aj)) = KL(Ber(α)∥Ber(α − η)) ≤\n\n32\n\n(cid:2869)(cid:2870)(cid:3041)(cid:2879)(cid:2869)(cid:3041)(cid:2869)(cid:2870)(cid:2871)Reward Reward Reward Action (cid:3046)(cid:3048)(cid:3029):Action ∗:Action (cid:3046)(cid:3048)(cid:3029):Action ∗:...Published as a conference paper at ICLR 2023\n\nconstant c1 and small enough η. Then, using Lemma A.1 in (Auer et al., 2002), we have that for any j ∈ [A],\n\nEj[Nsn,aj ] ≤Eunif [Nsn,aj ] +\n\n≤Eunif [Nsn,aj ] +\n\n(cid:113)\n\n(cid:114)\n\nK 2\n\nK 2\n\nEunif [Vsn,aj ] · KL (punif (sn, aj)||pj(sn, aj))\n\nw(sn) · Eunif [Nsn,aj ] ·\n\nc1η2 α\n\nThen, using (cid:80)A\n\nj=1\n\nEunif [Nsn,aj ] = K and the Cauchy–Schwarz inequality, we have\n\n1 A\n\nA (cid:88)\n\nj=1\n\nEj[Nsn,aj ] ≤\n\n≤\n\n1 A\n\n1 A\n\nA (cid:88)\n\nj=1\n\nA (cid:88)\n\nj=1\n\nEunif [Nsn,aj ] +\n\nEunif [Nsn,aj ] +\n\nKη 2A\n\nKη 2A\n\n(cid:114)\n\n≤\n\nK A\n\n+\n\nKη 2\n\nc1 · w(sn)K αA\n\n(cid:114) c1 α\n\nA (cid:88)\n\nj=1 (cid:118) (cid:117) (cid:117) (cid:116)A\n\n· w(sn) · Eunif [Nsn,aj ]\n\nA (cid:88)\n\nj=1\n\nc1 α\n\n· w(sn) · Eunif [Nsn,aj ]\n\n(17)\n\nBy plugging Eq. (17) into Eq. (16), we have\n\nE [R(K)] ≥ 0.6(H − n) ·\n\n(cid:32)\n\n·\n\nK −\n\nη α\n\nK A\n\n−\n\nKη 2\n\n(cid:114)\n\nc1 · w(sn)K αA\n\n(cid:33)\n\n.\n\nLet η = c2\n\n(cid:113) αA\n\nw(sn)K for a small enough constant c2. We have\n\n(cid:32)\n\n(cid:115)\n\nE [R(K)] =Ω\n\nH\n\n(cid:32)\n\n(cid:115)\n\n=Ω\n\nH\n\n(cid:33)\n\n· K\n\nA α · w(sn)K (cid:33)\n\nAK α · w(sn)\n\nRecall that n < 1 w(sn) = μn−1 > αH−1, and\n\n2 H and 0 < α < μ < 1\n\n3 . Thus, we have that minπ,h,s: wπ,h(s)>0 wπ,h(s) =\n\nE [R(K)] =Ω\n\n\n\n H\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nAK\n\nα ·\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n\n\n  .\n\nNext, we construct another instance where αH−1 > minπ,h,s: wπ,h(s)>0 wπ,h(s), and prove that on\n\nthis instance any algorithm must suffer a Ω(\n\nConsider the instance shown in Figure 7:\n\n(cid:113) AK\n\nαH−1 ) regret.\n\nThe state space is S = {s1, . . . , sn, s′ initial state. Let 0 < α < 1 4 .\n\n2, . . . , s′\n\nn, x1, x2, x3, x4}, where n = H − 1 and s1 is the\n\nThe reward functions are as follows. For any a ∈ A, r(x1, a) = r(x4, a) = 1, r(x2, a) = 0.8 and r(x3, a) = 0.2. For any i ∈ [n] and a ∈ A, r(si, a) = 0. For any i ∈ {2, . . . , n} and a ∈ A, r(s′\n\ni, a) = 0.\n\nThe transition distributions are as follows. For any a ∈ A, p(s2|s1, a) = α, p(s′ 2|s1, a) = γ and p(x1|s1, a) = 1−γ −α. For any i ∈ {2, . . . , n−1} and a ∈ A, p(si+1|si, a) = α and p(x1|si, a) = 1 − α. For any i ∈ {2, . . . , n − 1} and a ∈ A, p(s′ i, a) = γ and p(x1|s′ i, a) = 1 − γ. For any a ∈ A, p(x4|s′ n, a) = γ and p(x1|s′ n, a) = 1 − γ. x1, x2, x3 and x4 are absorbing states, i.e., for any a ∈ A and i ∈ [4], p(xi|xi, a) = 1. Let aJ be the optimal action in state sn, which is uniformly drawn from A. For the optimal action aJ , p(x2|sn, aJ ) = 1 − α + η and p(x3|sn, aJ ) = α − η,\n\ni+1|s′\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Instance of lower bounds (Theorems 2 and 5) for the αH−1 > minπ,h,s: wπ,h(s)>0 wπ,h(s) case.\n\nwhere η is a parameter which satisfies 0 < η < α and will be chosen later. For any suboptimal action a ∈ A \\ {aJ }, p(x2|sn, a) = 1 − α and p(x3|sn, a) = α.\n\nAccording to the definition of the value function for Iterated CVaR RL, we have that\n\nV ∗\n\n1 (s1) =\n\n0.2(α − η) + 0.8η α\n\n,\n\nand for any policy π,\n\nV π\n\n1 (s1) =\n\n0.2(α − η) + 0.8η α\n\n· 1 {π(sn) = aJ } + 0.2 (1 − 1 {π(sn) = aJ }) .\n\nIf J = j, for any policy π,\n\n0.6η α\nand summing over all episodes k ∈ [K], we have\n\n1 (s1) − V π\n\n1 (s1) =\n\nV ∗\n\n(1 − 1 {π(sn) = aj}) ,\n\n(18)\n\nTherefore, we have\n\nEj [R(K)] =\n\nK (cid:88)\n\n(cid:16)\n\n1 (s1) − V πk V ∗\n\n1\n\n(cid:17)\n\n(s1)\n\nk=1\n\n0.6η α\n\n0.6η α\n\n=\n\n=\n\n(cid:32)\n\n·\n\nK −\n\nK (cid:88)\n\nk=1\n\n(cid:33)\n\n1 {π(sn) = aj}\n\n· (cid:0)K − Ej[Nsn,aj ](cid:1)\n\nE [R(K)] =\n\n=\n\n1 A\n\n1 A\n\nA (cid:88)\n\nK (cid:88)\n\n(cid:16)\n\n1 (s1) − V πk V ∗\n\n1\n\n(cid:17)\n\n(s1)\n\nj=1\n\nk=1\n\nA (cid:88)\n\nj=1\n\n0.6η α\n\n(cid:0)K − Ej[Nsn,aj ](cid:1)\n\n\n\nK −\n\n=\n\n0.6η α\n\n1 A\n\nA (cid:88)\n\nj=1\n\n\n\nEj[Nsn,aj ]\n\n\n\n(19)\n\nRecall that 0 < α < 1\n\nKL(Ber(α)∥Ber(α − η)) ≤\n\n4 . For any j ∈ [A], we have that KL(punif (sn, aj)∥pj(sn, aj)) = for some constant c1 and small enough η.\n\n(α−η)(1−α+η) ≤ c1η2\n\nη2\n\nα\n\n34\n\n1−−1−1−Reward 1Reward 0.8Reward 0.2Action : 1−Action ∗: 1−+Action : Action ∗: −...′′′...Reward 11−Published as a conference paper at ICLR 2023\n\nThen, using Lemma A.1 in (Auer et al., 2002), we have that for any j ∈ [A],\n\nEj[Nsn,aj ] ≤Eunif [Nsn,aj ] +\n\n≤Eunif [Nsn,aj ] +\n\n(cid:113)\n\n(cid:114)\n\nK 2\n\nK 2\n\nEunif [Vsn,aj ] · KL (punif (sn, aj)||pj(sn, aj))\n\nw(sn) · Eunif [Nsn,aj ] ·\n\nc1η2 α\n\nThen, using (cid:80)A\n\nj=1\n\nEunif [Nsn,aj ] = K and the Cauchy–Schwarz inequality, we have\n\n1 A\n\nA (cid:88)\n\nj=1\n\nEj[Nsn,aj ] ≤\n\n≤\n\n1 A\n\n1 A\n\nA (cid:88)\n\nj=1\n\nA (cid:88)\n\nj=1\n\nEunif [Nsn,aj ] +\n\nEunif [Nsn,aj ] +\n\nKη 2A\n\nKη 2A\n\n(cid:114)\n\n≤\n\nK A\n\n+\n\nKη 2\n\nc1 · w(sn)K αA\n\n(cid:114) c1 α\n\nA (cid:88)\n\nj=1 (cid:118) (cid:117) (cid:117) (cid:116)A\n\n· w(sn) · Eunif [Nsn,aj ]\n\nA (cid:88)\n\nj=1\n\nc1 α\n\n· w(sn) · Eunif [Nsn,aj ]\n\n(20)\n\nBy plugging Eq. (20) into Eq. (19), we have (cid:32)\n\nE [R(K)] ≥\n\n·\n\nK −\n\n0.6η α\n\n(cid:114)\n\nK A\n\n−\n\nKη 2\n\nc1 · w(sn)K αA\n\n(cid:33)\n\n.\n\nLet η = c2\n\n(cid:113) αA\n\nw(sn)K for a small enough constant c2. We have\n\nE [R(K)] =Ω\n\n=Ω\n\n(cid:33)\n\n· K\n\n(cid:32)(cid:115)\n\n(cid:32)(cid:115)\n\nA α · w(sn)K (cid:33)\n\nAK α · w(sn)\n\nRecall that 0 < γ < α and n = H − 1. Thus, we have minπ,h,s: wπ,h(s)>0 wπ,h(s) = w(x4) = γH−1 < αH−1. In addition, since w(sn) = αn−1 = αH−2, we have\n\n(cid:32)(cid:114)\n\n(cid:32)(cid:114)\n\nE [R(K)] =Ω\n\n=Ω\n\n(cid:33)\n\nAK α · αH−2 (cid:33)\n\nAK αH−1\n\n.\n\nE PROOFS FOR ITERATED CVAR RL WITH BEST POLICY IDENTIFICATION\n\nIn this section, we present the pseudo-code and detailed description of algorithm ICVaR-BPI, and formally state the sample complexity lower bound for Iterated CVaR-BPI (Theorem 5). We also give the proofs of sample complexity upper and lower bounds (Theorems 3 and 5).\n\nE.1 ALGORITHM ICVaR-BPI\n\nAlgorithm ICVaR-BPI (Algorithm 3) constructs optimistic and pessimistic value functions, estimation error, and a hypothesized optimal policy in each episode, and returns the hypothesized optimal policy when the estimation error shrinks within ε. Specifthe empirical CVaR for values of ically, next states CVaRα h+1(s′)) and exploration bonuses\n\nin each episode k, ICVaR-BPI calculates\n\nh+1(s′)), CVaRα\n\ns′∼ ˆpk(·|s,a)( ̄V k\n\ns′∼ ˆpk(·|s,a)(V k\n\n35\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 3: ICVaR-BPI\n\nInput: ε, δ, α, δ′ := δ H+1(s) = ̄V k J k 1 for k = 1, 2, . . . , K do\n\n7 , ̃L(k) := log( 2HSAk3 H+1(s) = V k\n\n) for any k > 0, H+1(s) = 0 for any k > 0 and s ∈ S.\n\nδ′\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\nH α\n\nfor h = H, H − 1, . . . , 1 do\n\nfor s ∈ S do\n\nfor a ∈ A do\n\n ̄Qk\n\nh(s, a) ← min\n\nQk\n\nh\n\n(s, a) ← max\n\nGk\n\nh(s, a) ← min\n\n(cid:110)\n\nr(s, a) + CVaRα (cid:110)\n\nr(s, a) + CVaRα\n\n√\n\n(cid:110) H(1+4 √\n\n√\n\nS)\n\n ̃L(k)\n\nα\n\nnk(s,a)\n\ns′∼ ˆpk(·|s,a)( ̄V k\n\nh+1(s′)) + H\n\nα\n\ns′∼ ˆpk(·|s,a)(V k\n\nh+1(s′)) − 4H\n\nα\n\n+ ˆβk;α,V k\n\nh+1(·|s, a)⊤J k\n\nh+1, H\n\n(cid:113) ̃L(k)\n\n(cid:111) ;\n(cid:111) ;\n\nnk(s,a) , H (cid:113) S ̃L(k)\n\nnk(s,a) , 0 (cid:111) ;\n\nπk\n\nh(s) ← argmaxa∈A V k\n\nh(s) ← Qk\n\n(s, πk\n\n ̄Qk h(s)). J k\n\nh\n\nh(s, a). ̄V k\n\nh (s) ← maxa∈A ̄Qk\n\nh(s, a).\n\nh (s) ← Gk\n\nh(s, πk\n\nh(s));\n\nif J k\n\n1 (s) ≤ ε then return πk(s)\n\nelse\n\nPlay the episode k with policy πk, and update nk+1(s, a) and ˆpk+1(s′|s, a)\n\n(cid:113) ̃L(k)\n\n(cid:113) S ̃L(k)\n\nh\n\nα\n\nnk(s,a) , 4H\n\nnk(s,a) , to establish the optimistic and pessimistic Q-value functions ̄Qk\n\nh(s, a) (s, a), respectively. ICVaR-BPI further maintains a hypothesized optimal policy πk, which\n\nand Qk is greedy with respect to ̄Qk h+1(·|s, a) denote the conditional empirical transition probability in episode k, conditioning on transitioning to the worst α-portion successor states s′ (i.e., h+1(s′)), and it satisfies (cid:80) with the worst α-portion values V k h+1(s′) = CVaRα h(s, a) and h (s) using conditional transition probability ˆβk;α,V k J k h (s) shrinks within accuracy parameter ε, ICVaR-BPI returns the hypothesized optimal policy πk.\n\nh+1(s′|s, a) · V k h+1(s′)) (Line 7). Then, ICVaR-BPI computes estimation error Gk\n\nh+1(·|s, a). Once estimation error J k\n\nh(s, a). Let ˆβk;α,V k\n\ns′∼ ˆpk(·|s,a)(V k\n\nˆβk;α,V k\n\ns′∈S\n\nE.2 PROOFS OF SAMPLE COMPLEXITY UPPER BOUND\n\nE.2.1 CONCENTRATION\n\nIn the best policy identification analysis, we introduce several useful lemmas and concentration events. Different from the regret minimization analysis where the logrithmic factor log( KHSA )\nin the exploration bonuses is an universal constant, here the logrithmic factor log( 2k3HSA increase as the index of the episode k increases. Lemma 12 (Concentration for V ∗ – BPI). It holds that\n\nδ′ ) will\n\nδ′\n\n(cid:34)\n\nPr\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V ∗\n\nh (s′)) − CVaRα\n\ns′∼p(·|s,a)(V ∗\n\nh (s′))\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤\n\nH α\n\nlog (cid:0) 2k3HSA nk(s, a)\n\nδ′\n\n(cid:1)\n\n,\n\n∀k > 0, ∀h ∈ [H], ∀(s, a) ∈ S × A\n\n≥ 1 − 2δ′.\n\n(cid:35)\n\nProof of Lemma 12. Using the same analysis as Lemma 1, we have that for a fixed k,\n\n(cid:34)\n\nPr\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V ∗\n\nh (s′)) − CVaRα\n\ns′∼p(·|s,a)(V ∗\n\nh (s′))\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤\n\nH α\n\nlog (cid:0) 2k3HSA nk(s, a)\n\nδ′\n\n(cid:1)\n\n,\n\n∀h ∈ [H], ∀(s, a) ∈ S × A\n\n≥ 1 − 2 ·\n\n(cid:35)\n\nδ′ 2k2 .\n\n36\n\nPublished as a conference paper at ICLR 2023\n\nBy a union bound over k = 1, 2, . . . , we have\n\n(cid:34)\n\nPr\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V ∗\n\nh (s′)) − CVaRα\n\ns′∼p(·|s,a)(V ∗\n\nh (s′))\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤\n\nH α\n\nlog (cid:0) 2k3HSA nk(s, a)\n\nδ′\n\n(cid:1)\n\n,\n\n∀k > 0, ∀h ∈ [H], ∀(s, a) ∈ S × A\n\n(cid:35)\n\n≥1 − 2 ·\n\n(cid:19)\n\n∞ (cid:88)\n\nk=1\n\n(cid:18) δ′ 2k2\n\n≥1 − 2δ′.\n\nLemma 13 (Concentration for any V – BPI). It holds that\n\n(cid:34)\n\nPr\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V (s′)) − CVaRα\n\ns′∼p(·|s,a)(V (s′))\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤\n\n2H α\n\n2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\n∀V : S (cid:55)→ [0, H], ∀k > 0, ∀(s, a) ∈ S × A\n\n≥ 1 − 2δ′.\n\n(cid:35)\n\nProof of Lemma 13. Using the same analysis as Lemma 2, we have that for a fixed k,\n\n(cid:34)\n\nPr\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V (s′)) − CVaRα\n\ns′∼p(·|s,a)(V (s′))\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤\n\n2H α\n\n2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\n∀V : S (cid:55)→ [0, H], ∀(s, a) ∈ S × A\n\n≥ 1 − 2 ·\n\n(cid:35)\n\nδ′ 2k2 .\n\nBy a union bound over k = 1, 2, . . . , we have\n\n(cid:34)\n\nPr\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V (s′)) − CVaRα\n\ns′∼p(·|s,a)(V (s′))\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤\n\n2H α\n\n2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\n∀V : S (cid:55)→ [0, H], ∀k > 0, ∀(s, a) ∈ S × A\n\n(cid:35)\n\n≥1 − 2 ·\n\n(cid:19)\n\n∞ (cid:88)\n\nk=1\n\n(cid:18) δ′ 2k2\n\n≥1 − 2δ′.\n\nFor any risk level α ∈ (0, 1], function V : S (cid:55)→ R, k > 0 and (s′, s, a) ∈ S × S × A, βα,V (s′|s, a) and ˆβk;α,V (s′|s, a) are the conditional transition probability from (s, a) to s′ and the conditional empirical transition probability from (s, a) to s′ in episode k, conditioning on transitioning to the worst α-portion successor states s′ (i.e., with the lowest α-portion values V (s′)), respectively. μα,V (s′|s, a) and ˆμk;α,V (s′|s, a) denote how large the transition probability of successor state s′ and the empirical transition probability of successor state s′ in episode k belong to the worst α-portion, respectively. It holds that\n\nand\n\nμα,V (s′|s, a) α\n\n= βα,V (s′|s, a),\n\nˆμk;α,V (s′|s, a) α\n\n= ˆβk;α,V (s′|s, a).\n\n37\n\nPublished as a conference paper at ICLR 2023\n\nLemma 14 (Concentration for conditional transition probability). It holds that 2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\nˆβk;α,V (s′|s, a) − βα,V (s′|s, a)\n\n(cid:12) (cid:12) (cid:12) ≤\n\n2 α\n\nPr\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:34)\n\n(cid:1)\n\n,\n\n∀V : S (cid:55)→ R, ∀k > 0, ∀(s, a) ∈ S × A\n\n≥ 1 − 2δ′.\n\n(cid:35)\n\nProof of Lemma 14. Using the analysis of Eq. (7), we have that for any risk level α ∈ (0, 1], function V : S (cid:55)→ R, k > 0 and (s, a) ∈ S × A,\n\n(cid:88)\n\ns′∈S\n\n(cid:12)ˆμk;α,V (s′|s, a) − μα,V (s′|s, a)(cid:12) (cid:12)\n\n(cid:12) ≤ 2\n\n(cid:88)\n\ns′∈S\n\n(cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12) (cid:12) .\n\n(21)\n\nUsing Eq. (55) in (Zanette & Brunskill, 2019) (originated from (Weissman et al., 2003)), we have that for any fixed k, with probability at least 1 − 2 · ( δ′\n\n2k2 ), for any (s, a) ∈ S × A,\n\n(cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12)\n\n(cid:12) ≤\n\n(cid:88)\n\ns′∈S\n\n(cid:115)\n\n2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\nand thus,\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) ˆβk;α,V (s′|s, a) − βα,V (s′|s, a) (cid:12) (cid:12) =\n\n(cid:88)\n\ns′∈S\n\n=\n\n≤\n\n(cid:88)\n\ns′∈S (cid:80)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nˆμk;α,V (s′|s, a) α\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)ˆμk;α,V (s′|s, a) − μα,V (s′|s, a)(cid:12) (cid:12) (cid:12)\n\nμα,V (s′|s, a) α\n\n−\n\ns′∈S\n\n2 (cid:80)\n\ns′∈S\n\nα (cid:12)ˆpk(s′|s, a) − p(s′|s, a)(cid:12) (cid:12) (cid:12)\n\n(cid:115)\n\n≤\n\n2 α\n\nα\n\n2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\n(cid:1)\n\nBy a union bound over k = 1, 2, . . . , we have\n\n(cid:34)\n\nPr\n\n(cid:88)\n\ns′∈S\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) ˆβk;α,V (s′|s, a) − βα,V (s′|s, a) (cid:12) (cid:12) ≤\n\n(cid:115)\n\n2 α\n\n2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\n∀V : S (cid:55)→ R, ∀k > 0, ∀(s, a) ∈ S × A\n\n(cid:35)\n\n≥1 − 2 ·\n\n(cid:19)\n\n∞ (cid:88)\n\nk=1\n\n(cid:18) δ′ 2k2\n\n≥1 − 2δ′.\n\nTo sum up, we define the following concentration events and recall event E3. (cid:115)\n\n(cid:40)\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\nF1 :=\n\ns′∼ ˆpk(·|s,a)(V ∗\n\nh (s′)) − CVaRα\n\ns′∼p(·|s,a)(V ∗\n\nh (s′))\n\n(cid:12) (cid:12) (cid:12) ≤\n\nH α\n\nlog (cid:0) 2k3HSA nk(s, a)\n\nδ′\n\n(cid:1)\n\n,\n\n∀k > 0, ∀h ∈ [H], ∀(s, a) ∈ S × A\n\n(cid:41)\n\n(cid:40)\n\nF2 :=\n\n(cid:12) (cid:12)CVaRα (cid:12)\n\ns′∼ ˆpk(·|s,a)(V (s′)) − CVaRα\n\ns′∼p(·|s,a)(V (s′))\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤\n\n2H α\n\n2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\n38\n\nPublished as a conference paper at ICLR 2023\n\n∀V : S (cid:55)→ [0, H], ∀k > 0, ∀(s, a) ∈ S × A\n\n(cid:41)\n\n(cid:40)\n\nF3 :=\n\n(cid:12) (cid:12) (cid:12)\n\nˆβk;α,V (s′|s, a) − βα,V (s′|s, a)\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤\n\n2 α\n\n2S log (cid:0) 2k3HSA δ′ nk(s, a)\n\n(cid:1)\n\n,\n\n(cid:41)\n\n∀V : S (cid:55)→ R, ∀k > 0, ∀(s, a) ∈ S × A\n\n(cid:40)\n\nE3 :=\n\nnk(s, a) ≥\n\n1 2\n\nk′=1 F :=F1 ∩ F2 ∩ F3 ∩ E3\n\nh=1\n\nk−1 (cid:88)\n\nH (cid:88)\n\nwk′h(s, a) − H log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n, ∀k > 0, ∀(s, a) ∈ S × A\n\n(cid:41)\n\nLemma 15. Letting δ′ = δ\n\n7 , it holds that\n\nPr [F] ≥ 1 − δ.\n\nProof of Lemma 15. This lemma can be obtained by combining Lemmas 12-14 and 3.\n\nE.2.2 OPTIMISM AND ESTIMATION ERROR\n\nFor any k > 0, let ̃L(k) := log\n\n(cid:16) 2HSAk3\n\nδ′\n\n(cid:17)\n\n.\n\nLemma 16 (Optimism and Pessimism). Suppose that event F holds. Then, for any k > 0, h ∈ [H] and s ∈ S,\n\nh (s),\n\n ̄V k\n\nh (s) ≥ V ∗ h(s) ≤ V πk\n\nV k\n\nh (s).\n\nProof of Lemma 16. The proof of ̄V k V πk\n\nh (s) by induction.\n\nh (s) ≥ V ∗\n\nh (s) is similar to Lemma 5. Below we prove V k\n\nh(s) ≤\n\nFirst, for any k > 0, s ∈ S, it holds that V k\n\nH+1(s) = V πk\n\nH+1(s) = 0.\n\nThen, for any k > 0, h ∈ [H] and (s, a) ∈ S × A, if Qk holds, and otherwise,\n\nh\n\n(s, a) = 0, Qk\n\nh\n\n(s, a) ≤ Qπk\n\n(s, a) trivially\n\nQk\n\nh\n\n(s, a) =r(s, a) + CVaRα\n\ns′∼ ˆpk(·|s,a)(V k\n\nh+1(s′)) −\n\n(a)\n\n≤r(s, a) + CVaRα\n\ns′∼ ˆpk(·|s,a)(V πk\n\nh+1(s′)) −\n\n(cid:115)\n\n4H α\n\nS ̃L(k) nk(s, a)\n\n(cid:115)\n\n4H α\n\nS ̃L(k) nk(s, a)\n\n(b)\n\n≤r(s, a) + CVaRα\n\ns′∼ ˆpk(·|s,a)(V πk\n\nh+1(s′))\n\n(cid:16)\n\n−\n\nCVaRα\n\ns′∼ ˆpk(·|s,a)(V πk\n\nh+1(s′)) − CVaRα\n\ns′∼p(·|s,a)(V πk\n\nh+1(s′))\n\n(cid:17)\n\n=r(s, a) + CVaRα\n\ns′∼p(·|s,a)(V πk\n\nh+1(s′))\n\n=Qπk\n\n(s, a),\n\nwhere (a) uses the induction hypothesis, and (b) comes from Lemma 2.\n\nThus, we have\n\nV k\n\nh(s) = Qk\n\nh\n\n(s, πk\n\nh(s)) ≤ Qπk\n\nh (s, πk\n\nh(s)) = V πk\n\nh (s),\n\nwhich concludes the proof.\n\n39\n\nPublished as a conference paper at ICLR 2023\n\nFor any risk level α ∈ (0, 1], k > 0, h ∈ [H] and (s′, s, a) ∈ S × S × A, ˆβk;α,V k h+1 (s′|s, a) is the conditional empirical transition probability from (s, a) to s′ in episode k, conditioning on transitioning to the worst α-portion successor states s′ (i.e., with the lowest α-portion values V k h+1(s′)). It holds that CVaRα\n\nh+1(s′|s, a) · V k\n\nˆβk;α,V k\n\nh+1(s′)) = (cid:80)\n\ns′∼ ˆpk(·|s,a)(V k\n\nh+1(s′).\n\ns′∈S\n\nLemma 17 (Estimation Error). Suppose that event F holds. Then, for any k > 0,\n\n1 (s1) − V πk V ∗\n\n1\n\n(s1) ≤ J k\n\n1 (s1).\n\nProof of Lemma 17. In the following, we prove by induction that for any k > 0, h ∈ [H] and s ∈ S,\n\n ̄V k\n\nh (s) − V k\n\nh(s) ≤ J k\n\nh (s).\n\n(22)\n\nFirst, for any k > 0 and s ∈ S, it holds that ̄V k\n\nH+1(s) − V k\n\nH+1(s) = 0.\n\nH+1(s) = J k h(s, a) = H, ̄Qk\n\nh(s, a)−Qk\n\nh\n\n(s, a) ≤ Gk\n\nh(s, a)\n\nThen, for any k > 0, h ∈ [H] and (s, a) ∈ S ×A, if Gk holds trivially, and otherwise,\n\n ̄Qk\n\nh(s, a) − Qk\n\nh\n\n(s, a) =\n\n(cid:115)\n\nH α\n\n ̃L(k) nk(s, a)\n\n+\n\n4H α\n\n(cid:115)\n\nS ̃L(k) nk(s, a)\n\n+ CVaRα\n\ns′∼ ˆpk(·|s,a)( ̄V k √\n\nS)\n\n(cid:113)\n\n(cid:113)\n\nH\n\n(a) ≤\n\nH\n\n(b) ≤\n\n ̃L(k)(1 + 4 α(cid:112)nk(s, a)\n\n ̃L(k)(1 + 4 α(cid:112)nk(s, a)\n\n√\n\nh+1(s′)) − CVaRα\n\ns′∼ ˆpk(·|s,a)(V k\n\nh+1(s′))\n\n+ ˆβk;α,V k\n\nh+1(·|s, a)⊤ (cid:16) ̄V k\n\nh+1 − V k\n\nh+1\n\n(cid:17)\n\nS)\n\n+ ˆβk;α,V k\n\nh+1(·|s, a)⊤J k\n\nh+1\n\n=Gk\n\nh(s, a),\n\nwhere (a) uses Lemma 11 with empirical transition probability ˆpk(·|s, a), conditional empirical transition probability ˆβk;α,V k h+1, and (b) is due to the induction hypothesis.\n\nh+1 (·|s, a), and values ̄V k\n\nh+1, V k\n\nThus,\n\n ̄V k\n\nh (s) − V k which completes the proof of Eq. (22).\n\nh(s) = ̄Qk\n\nh(s, πk\n\nh(s)) − Qk\n\n(s, πk\n\nh(s)) ≤ Gk\n\nh(s, πk\n\nh(s)) = J k\n\nh (s),\n\nh\n\nHence, for any k > 0,\n\n ̄V k\n\n1 (s1) − V k\n\n1(s1) ≤ J k\n\n1 (s1).\n\nUsing Lemma 16, we have\n\n1 (s) − V πk V ∗\n\n1\n\n(s1) ≤ ̄V k\n\n1 (s1) − V k\n\n1(s1) ≤ J k\n\n1 (s1).\n\nE.2.3 PROOF OF THEOREM 3\n\nProof of Theorem 3. Suppose that event F holds.\n\nFirst, we prove the correctness. Using Lemma 17, when algorithm ICVaR-BPI stops, we have\n\n1 (s1) − V πk V ∗\n\n1\n\n(s1) ≤ J k\n\n1 (s1) ≤ ε.\n\nThus, the output policy πk is ε-optimal.\n\nNext, we prove the sample complexity.\n\n40\n\nPublished as a conference paper at ICLR 2023\n\nUnfolding J k\n\n1 (s1) (a)= min J k\n\n= min\n\n \n\n\n\n1 (s1), we have √\n\nH(1 + 4\n\n(cid:113)\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s1, a1) √\n\n(cid:113)\n\n(cid:40) H(1 + 4\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s1, a1)\n\nˆβk;α,V k\n\n2 (s2|s1, a1) · J k\n\n2 (s2), H\n\n \n\n\n\nβα,V k\n\n2 (s2|s1, a1) · J k\n\n2 (s2)\n\n(cid:88)\n\n+\n\ns2∈S\n\n(cid:88)\n\n+\n\ns2∈S\n\n(cid:16) ˆβk;α,V k\n\n2 (s2|s1, a1) − βα,V k\n\n2 (s2|s1, a1)\n\n(cid:17)\n\n(cid:41)\n\n· J k\n\n2 (s2), H\n\n(cid:88)\n\n+\n\ns2∈S\n\n(b) ≤ min\n\n(cid:40) H(1 + 4\n\n√\n\n(cid:113)\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s1, a1)\n\n(cid:88)\n\n+\n\ns2∈S\n\nβα,V k\n\n2 (s2|s1, a1) · J k\n\n2 (s2)\n\n2 (s2|s1, a1) − βα,V k\n\n2 (s2|s1, a1)\n\n(cid:17)(cid:12) (cid:12) (cid:12) , H\n\n(cid:88)\n\n+\n\nβα,V k\n\n2 (s2|s1, a1) · J k\n\n2 (s2) +\n\n(cid:41)\n\n(cid:115)\n\n4H α\n\nS · ̃L(k) nk(s1, a1)\n\n, H\n\n(cid:41)\n\n(cid:88)\n\n+ H\n\ns2∈S √\n\n(cid:40) H(1 + 4\n\n(cid:16) ˆβk;α,V k\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:113)\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s1, a1) √\n\n(cid:113)\n\nH(1 + 8\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s1, a1) √\n\n(cid:113)\n\nH(1 + 8\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s1, a1) √\n(cid:113)\n\nH(1 + 8\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s2, a2)\n\nH(1 + 8\n\n√\n\n(cid:113)\n\nS)\n\n ̃L(k)\n\ns2∈S \n\n\n, H\n\n\n\n \n\n\n\n, H\n\nα(cid:112)nk(s1, a1) √\n(cid:113)\n\nH(1 + 8\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s2, a2)\n\n \n\n\n\n, H\n\n(c) ≤ min\n\n(d) ≤ min\n\n(e) ≤ min\n\n \n\n\n\n \n\n\n\n\n\nmin\n\n \n\n\n\n(f) ≤ min\n\n \n\n\n\nmin\n\n \n\n\n\n(cid:32)\n\n(cid:32)\n\n(cid:88)\n\n· · ·\n\nsH ∈S\n\nβα,V k\n\nH (sH |sH−1, aH−1) · min\n\n+\n\nβα,V k\n\n2 (s2|s1, a1) · J k\n\n2 (s2)\n\n(cid:88)\n\ns2∈S\n\n(cid:88)\n\n+\n\ns2∈S\n\nβα,V k\n\n2 (s2|s1, a1)·\n\n, H\n\n \n\n\n\n(cid:88)\n\n+\n\ns3∈S\n\nβα,V k\n\n3 (s3|s2, a2) · J k\n\n3 (s3)\n\n\n\n\n\n, H\n\n \n\n\n\n(cid:88)\n\n+\n\ns2∈S\n\nβα,V k\n\n2 (s2|s1, a1)·\n\n(cid:88)\n\n+\n\nβα,V k\n\n3 (s3|s2, a2)·\n\ns3∈S \n\n\n\n\nH(1 + 8\n\n√\n\n(cid:113)\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(sH , aH ) (cid:113)\n\n√\n\n(cid:33)(cid:33)\n\n \n\n\n\n, H\n\nH (cid:88)\n\n(g) =\n\n(cid:88)\n\nh=1\n\n(s,a)∈S×A\n\nwCVaR,α,V k\n\nkh\n\n(s, a) · min\n\n \n\n\n\nH(1 + 8\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(sH , aH )\n\n \n\n\n\n, H\n\nH (cid:88)\n\n≤\n\n(cid:88)\n\nh=1\n\n(s,a)∈Lk\n\nwCVaR,α,V k\n\nkh\n\n(s, a) ·\n\nH(1 + 8\n\n√\n\nS)\n\n(cid:113)\n\n ̃L(k)\n\nα(cid:112)nk(s, a)\n\nH (cid:88)\n\n+\n\n(cid:88)\n\nh=1\n\n(s,a) /∈Lk\n\nwCVaR,α,V k\n\nkh\n\n(s, a) · H\n\nHere (b) is due to that for any k > 0, h ∈ [H] and s ∈ S, J k h (s) ∈ [0, H]. (c) comes from Lemma 14. (e) and (f) follow from recurrently applying steps (a)-(d). (g) uses the fact that wCVaR,α,V k (s, a) is defined as the probability of visiting (s, a) at step h of episode k under the conditional transition probability βα,V k\n\nh′+1(·|·, ·) for each step h′ = 1, . . . , h − 1.\n\nkh\n\n41\n\nPublished as a conference paper at ICLR 2023\n\nLet τ denote the episode in which algorithm ICVaR-BPI stops (ICVaR-BPI will not sample any trajectory in the stopping episode τ ). Then, for any k < τ , we have ε < J k 1 (s1). Summing over k < τ , we have\n\n(τ − 1) · ε <\n\nτ −1 (cid:88)\n\nk=1\n\nJ k\n\n1 (s1)\n\nτ −1 (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\n≤\n\nwCVaR,α,V k\n\nkh\n\n(s, a) ·\n\nH(1 + 8\n\n√\n\n(cid:113)\n\nS)\n\n ̃L(k)\n\nα(cid:112)nk(s, a)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nτ −1 (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\n+\n\nk=1\n\nh=1 √\n\n(s,a) /∈Lk (cid:113)\n\nH(1 + 8\n\nS)\n\n ̃L(τ − 1)\n\n(a) ≤\n\nα\n\nH (cid:88)\n\n(cid:88)\n\nh=1 \n\n\n(s,a)∈Lk\n\n1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nτ −1 (cid:88)\n\nk=1\n\n+ min\n\nwCVaR,α,V k\n\nkh\n\n(s, a) · H\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nτ −1 (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nwCVaR,α,V k\n\nkh\n\n(s, a)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nnk(s, a)\n\n·\n\nwCVaR,α,V k\n\nkh\n\n(s, a)\n\n\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n \n\n\n\n(cid:18)\n\n4SAH 2 log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:19)\n\n+ 5SAH 2\n\nH(1 + 8\n\n(b) ≤\n\n√\n\nS)\n\nα\n\n(cid:113)\n\n ̃L(τ − 1)\n\n· (cid:112)(τ − 1)H ·\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nτ −1 (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nwCVaR,α,V k\n\nkh\n\n(s, a)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nnk(s, a)\n\n+ min\n\n \n\n\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n \n\n\n\n(cid:18)\n\n4SAH 2 log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:19)\n\n+ 5SAH 2\n\n(1 + 8\n\n√\n\nS)H\n\n(c) ≤\n\n(cid:113)\n\nH · ̃L(τ − 1) · (τ − 1)\n\n·\n\nα wCVaR,α,V k\n\nkh\n\n(s, a)\n\nwkh(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nτ −1 (cid:88)\n\nk=1\n\n+ min\n\nH (cid:88)\n\n(cid:88)\n\n(s,a)∈Lk\n\nh=1 \n\n\n1\n\n\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n(1 + 8\n\n√\n\n(cid:113)\n\nS)H\n\nH · ̃L(τ − 1)·(τ − 1)\n\nα\n\n≤\n\n·min\n\n(cid:114)\n\n \n\n\n\n(cid:40)\n\n·\n\nwkh(s, a) nk(s, a)\n\n1 {wkh(s, a) ̸= 0}\n\n,\n\n1 αH−1\n\n(cid:18)\n\n4SAH 2 log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:19)\n\n+ 5SAH 2\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n(cid:41) ·\n\n,\n\n√\n\n1\n\nαH−1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nτ −1 (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\n(s,a)∈Lk\n\nwkh(s, a) nk(s, a)\n\n+ min\n\n \n\n\n\n(cid:18)\n\n4SAH 2 log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:19)\n\n+ 5SAH 2\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n \n\n\n\n·\n\n(1 + 8\n\n√\n\nS)H\n\n(d) ≤\n\n(cid:113)\n\nH · ̃L(τ − 1) · (τ − 1)\n\nα\n\n(cid:40)\n\nmin\n\n(cid:114)\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n(cid:41) ·\n\n,\n\n√\n\n1\n\nαH−1\n\n42\n\nPublished as a conference paper at ICLR 2023\n\n(cid:113)\n\n2\n\nSA ̃L(τ − 1) + min\n\n \n\n\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n \n\n\n\n·\n\n(cid:18)\n\n4SAH 2 log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n(cid:19)\n\n+ 5SAH 2\n\n≤ min\n\n \n\n\n\n+ min\n\n(cid:114)\n\n \n\n\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n√\n\n1\n\nαH−1\n\n \n\n\n\n18SH · ̃L(τ − 1)(cid:112)HA(τ − 1) α\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n \n\n\n\n(cid:18)\n\n4SAH 2 log\n\n(cid:19)\n\n(cid:18) HSA δ′\n\n+ 5SAH 2\n\n(cid:19)\n\n,\n\nwhere (a) is due to Lemma 10, (b) uses the fact that for any risk level α ∈ (0, 1], k > 0 and h ∈ [H], (cid:80)\n\n(s, a) = 1, (c) comes from Lemma 8, and (d) is due to Lemma 7.\n\n(s,a)∈S×A wCV aR,α,V k\n\nkh\n\nThus, when log (cid:0) HSA\n\nδ′\n\n(cid:1) ≥ 1, we have\n\nτ − 1 ≤min\n\n(cid:40)\n\n(cid:114)\n\nwπ,h(s)\n\nmin π,h,s: wπ,h(s)>0 \n\n\n1\n\n\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n1\n\n1\n\n+ min\n\n(cid:40)\n\n≤min\n\n(cid:114)\n\nmin π,h,s: wπ,h(s)>0 \n\n\n1\n\n+ min\n\n\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n(cid:41)\n\n18SH\n\n√\n\n1\n\nαH−1\n\nεα\n\n,\n\n√\n\nHA\n\n√\n\n·\n\nτ − 1·log\n\n(cid:18) 2HSA(τ − 1)3 δ′\n\n(cid:19)\n\n,\n\n1 αH−1\n\n \n\n\n\n4SAH 2 log (cid:0) HSA\n\nδ′ ε\n\n(cid:1) + 5SAH 2\n\n(cid:41)\n\n54SH\n\n√\n\n1\n\nαH−1\n\nεα\n\nHA\n\n√\n\n·\n\nτ − 1·log\n\n(cid:18) 2HSA(τ − 1) δ′\n\n(cid:19)\n\n,\n\n√\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n \n\n\n\n9SAH 2 ε\n\nlog\n\n(cid:19)\n\n(cid:18) HSA δ′\n\nUsing Lemma 24 with A = 1, B = 0, C = min{\n\n(cid:113)\n\n1 minπ,h,s: wπ,h(s)>0 wπ,h(s) , E = 0, β = 2HSA\n\n√\n\n1√\n\nαH−1 } 54SH\n\nεα\n\n,\n\nHA\n\n,\n\nD = min{\n\nminπ,h,s: wπ,h(s)>0 wπ,h(s) ,\n\n1\n\nε recalling that algorithm ICVaR-BPI does not sample any trajectory in the stopping episode τ , we have that the number of used trajectories is bounded by\n\nδ′\n\nand T = τ − 1, and\n\n1\n\nαH−1 } 9SAH 2\n\n(cid:32)\n\n(cid:40)\n\nτ − 1 = O\n\nmin\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n(cid:41)\n\nH 3S2A ε2α2 ·\n\n(cid:32)\n\n(cid:40)\n\nlog2\n\nmin\n\n1\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n,\n\n1 αH−1\n\n(cid:41)\n\n(cid:33)(cid:33) .\n\nHSA εαδ\n\nE.3 SAMPLE COMPLEXITY LOWER BOUND\n\nBelow we present the sample complexity lower bound for Iterated CVaR RL-BPI and provide its proof.\n\nWe say algorithm A is (δ, ε)-correct if A returns an ε-optimal policy ˆπ such that V ˆπ ε with probability 1 − δ.\n\n1 (s1) ≥ V ∗\n\n1 (s1)−\n\n43\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 5 (Sample Complexity Lower Bound). There exists an instance of Iterated CVaR RLBPI, where minπ,h,s: wπ,h(s)>0 wπ,h(s) > αH−1 and the number of trajectories used by any (δ, ε)- correct algorithm is at least\n\n(cid:32)\n\nΩ\n\nH 2A\n\nε2α\n\nmin π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n(cid:19) (cid:33) .\n\n(cid:18) 1 δ\n\nlog\n\nthere also exists an instance of\n\nIterated CVaR RL-BPI, where αH−1 > In addition, minπ,h,s: wπ,h(s)>0 wπ,h(s) and the number of trajectories used by any (δ, ε)-correct algorithm is at least\n\n(cid:32)\n\nΩ\n\nA\n\nαH−1ε2 log\n\n(cid:18) 1 δ\n\n(cid:19) (cid:33) .\n\nTheorem 5 corroborates that when α is small, the factor minπ,h,s: wπ,h(s)>0 wπ,h(s) is unavoidable in general. This reveals the intrinsic hardness of Iterated CVaR RL, i.e., when the agent is highly risk-sensitive, she needs to spend a number of trajectories on exploring critical but hard-to-reach states in order to identify an optimal policy.\n\nProof of Theorem 5. This proof uses a similar analytical procedure as Theorem 2 in (Dann & Brunskill, 2015).\n\nFirst, we consider in the proof of Theorem 2, where minπ,h,s: wπ,h(s)>0 wπ,h(s) > αH−1. Below we prove that on this instance any algorithm must suffer a O(\n\nthe instance in Figure 6 as\n\nlog( 1\n\n1\n\nwπ,h(s) · H 3S2A\n\nε2α2\n\nδ )) regret.\n\nmin π,h,s: wπ,h(s)>0\n\nFix an algorithm A. Define Esn := {ˆπ(sn) = aJ } as the event that the output policy ˆπ of algorithm A chooses the optimal action in state sn.\n\nUsing the similar analysis as in the proof of Theorem 2 (Eq. (15)), we have\n\nV ∗\n\n1 (s1) − V π\n\n1 (s1) =0.6(H − n) ·\n\nη α\n\n· (1 − 1 {Esn }).\n\nFor π to be ε-optimal, we need\n\nε ≥ V ∗\n\n1 (s1) − V π\n\n1 (s1) = 0.6(H − n) ·\n\nη α\n\n· (1 − 1 {Esn}),\n\nwhich is equivalent to\n\n1 {Esn } ≥ 1 −\n\nεα 0.6(H − n) · η\n\n.\n\nLet η = 8e4εα\n\n0.6c0(H−n) for some constant c0 and small enough ε. Then, for π to be ε-optimal, we need\n\n1 {Esn } ≥ 1 −\n\nc0 8e4 .\n\nLet φ := 1 − c0\n\n8e4 . For algorithm A to be (ε, δ)-correct, we need 1 − δ ≤ Pr[V ∗ − V π ≥ ε] ≤ Pr[1 {Esn } ≥ φ]\n\n≤\n\n≤\n\nE[Esn ] φ\n\n1 φ\n\nPr[Esn ],\n\nwhich is equivalent to\n\nPr[ ̄Esn ] = 1 − Pr[Esn ] ≤ 1 − φ + φδ.\n\n44\n\nPublished as a conference paper at ICLR 2023\n\nη2\n\nRecall that 0 < α < 1\n\n3 . For any j ∈ [A], KL(punif (sn, aj)∥pj(sn, aj)) = KL(Ber(α)∥Ber(α − for some constant c1 and small enough η. Let Vsn be the number of\n\n(α−η)(1−α+η) ≤ c1·η2\n\nη)) ≤ times that algorithm A visited state sn. To ensure Pr[ ̄Esn ] ≤ 1 − φ + φδ, we need (cid:19)\n\n(cid:18)\n\nα\n\nE[Vsn ] ≥\n\nA (cid:88)\n\nj=1\n\n1 KL(punif (sn, aj)∥pj(sn, aj))\n\nlog\n\nc2 1 − φ + φδ\n\nαA\n\nc1 · η2 log αA · 0.62c2\n\n≥\n\n=\n\n(cid:18)\n\n(cid:19)\n\nc2 1 − φ + φδ\n\n0(H − n)2\n\nc1 · 64e8ε2α2\n\nlog\n\n(cid:18) c2\n\n(cid:19)\n\nc0\n\n8e4 + δ\n\nfor some constant c2. Let c0 be a small constant such that c0 8e4 < δ. Let w(sn) denote the probability of visiting sn in an episode, and this probability is the same for all policies. Let τ denote the number of trajectories required by A to be (ε, δ)-correct. Then, τ must satisfy\n\nτ ≥\n\n0(H − n)2 A · 0.62c2 c1 · 64e8ε2α · w(sn)\n\nlog\n\n(cid:18) c2\n\n(cid:19)\n\nc0\n\n8e4 + δ\n\n=Ω\n\n(cid:18) H 2A\n\nε2α · w(sn)\n\nlog\n\n(cid:18) 1 δ\n\n(cid:19)(cid:19)\n\n.\n\n2 H and 0 < α < μ < 1\n\nRecall that n < 1 minπ,h,s: wπ,h(s)>0 wπ,h(s) = w(sn) = μn−1 > αH−1, and \n\n3 . Thus, in the constructed instance (Figure 6), we have that\n\nτ =Ω\n\n\n\nε2α ·\n\nH 2A min π,h,s: wπ,h(s)>0\n\nwπ,h(s)\n\n\n\n(cid:19)\n\n .\n\n(cid:18) 1 δ\n\nlog\n\nNext, we consider the instance in Figure 7 as in the proof of Theorem 2, where αH−1 > minπ,h,s: wπ,h(s)>0 wπ,h(s). Below we prove that on this instance any algorithm must suffer a O(\n\nlog( 1\n\n1\n\nαH−1 · H 3S2A\n\nε2α2\n\nδ )) regret.\n\nDefine Esn := {ˆπ(sn) = aJ } as the event that the output policy ˆπ of algorithm A chooses the optimal action in state sn.\n\nUsing the similar analysis as in the proof of Theorem 2 (Eq. (18)), we have\n\nV ∗\n\n1 (s1) − V π\n\n1 (s1) =\n\n0.6η α\n\n· (1 − 1 {Esn }).\n\nFor π to be ε-optimal, we need\n\nε ≥ V ∗\n\n1 (s1) − V π\n\n1 (s1) =\n\n0.6η α\n\n· (1 − 1 {Esn }),\n\nwhich is equivalent to\n\n1 {Esn } ≥ 1 −\n\nεα 0.6η\n\n.\n\nLet η = 8e4εα\n\n0.6c0\n\nfor some constant c0 and small enough ε. Then, for π to be ε-optimal, we need\n\n1 {Esn } ≥ 1 −\n\nc0 8e4 .\n\nLet φ := 1 − c0\n\n8e4 . For algorithm A to be (ε, δ)-correct, we need 1 − δ ≤ Pr[V ∗ − V π ≥ ε] ≤ Pr[1 {Esn } ≥ φ]\n\n45\n\nPublished as a conference paper at ICLR 2023\n\n≤\n\n≤\n\nE[Esn ] φ\n\n1 φ\n\nPr[Esn ],\n\nwhich is equivalent to\n\nPr[ ̄Esn ] = 1 − Pr[Esn ] ≤ 1 − φ + φδ.\n\nη2\n\nRecall that 0 < α < 1\n\n4 . For any j ∈ [A], KL(punif (sn, aj)∥pj(sn, aj)) = KL(Ber(α)∥Ber(α − for some constant c1 and small enough η. Let Vsn be the number of\n\n(α−η)(1−α+η) ≤ c1·η2\n\nη)) ≤ times that algorithm A visited state sn. To ensure Pr[ ̄Esn ] ≤ 1 − φ + φδ, we need (cid:19)\n\n(cid:18)\n\nα\n\nE[Vsn ] ≥\n\nA (cid:88)\n\nj=1\n\n1 KL(punif (sn, aj)∥pj(sn, aj))\n\nlog\n\nc2 1 − φ + φδ\n\n(cid:19)\n\n(cid:18)\n\nαA\n\nc2 1 − φ + φδ (cid:18) c2\n\n≥\n\n=\n\nc1 · η2 log αA · 0.62c2 0\nc1 · 64e8ε2α2 log\n\nc0\n\n8e4 + δ\n\n(cid:19)\n\nfor some constant c2. Let c0 be a small constant such that c0 8e4 < δ. Let w(sn) denote the probability of visiting sn in an episode, and this probability is the same for all policies. Let τ denote the number of trajectories required by A to be (ε, δ)-correct. Then, τ must satisfy\n\nτ ≥\n\nA · 0.62c2 0\nc1 · 64e8ε2α · w(sn) A\nε2α · w(sn)\n\nlog\n\n(cid:18)\n\n=Ω\n\n(cid:19)(cid:19)\n\n.\n\n(cid:18) 1 δ\n\nlog\n\n(cid:18) c2\n\n(cid:19)\n\nc0\n\n8e4 + δ\n\nRecall that n = H − 1. Thus, in the constructed instance (Figure 7), we have that w(sn) = αn−1 = αH−2, and\n\n(cid:33)\n\n(cid:32)\n\n(cid:32)\n\nτ =Ω\n\n=Ω\n\n(cid:1)\n\nA ε2α · αH−2 log (cid:0) 1 δ\n(cid:33)\n\nA ε2αH−1 log (cid:0) 1\n\n.\n\n(cid:1)\n\nδ\n\nF PROOFS FOR WORST PATH RL\n\nIn this section, we provide the proofs of regret upper and lower bounds (Theorems 4 and 6) for Worst Path RL.\n\nF.1 PROOFS OF REGRET UPPER BOUND\n\nF.1.1 CONCENTRATION\n\nRecall that for any k > 0 and (s, a) ∈ S × A, nk(s, a) is the number of times that (s, a) was visited before episode k. For any k > 0 and (s′, s, a) ∈ S × S × A, let nk(s′, s, a) denote the number of times that (s, a) was visited and transitioned to s′ before episode k.\n\nFor any policy π and (s, a) ∈ S × A, let υπ(s, a) and υπ(s) denote the probabilities that (s, a) and s are visited at least once in an episode under policy π, respectively.\n\n46\n\nPublished as a conference paper at ICLR 2023\n\nLemma 18. It holds that\n\n(cid:34)\n\nPr\n\nnk(s, a) ≥\n\n1 2\n\nk−1 (cid:88)\n\nk′=1\n\nυπk′ (s, a) − log\n\n(cid:19)\n\n(cid:18) SA δ′\n\n, ∀k > 0, ∀(s, a) ∈ S × A\n\n≥ 1 − δ′\n\n(cid:35)\n\nProof of Lemma 18. For any k and (s, a) ∈ S × A, conditioning on the filtration generated by episodes 1, . . . , k − 1, whether the algorithm visited (s, a) at least once in episode k is a Bernoulli random variable with success probability υπk (s, a). Then, using Lemma F.4 in (Dann et al., 2017), we can obtain this lemma.\n\nLemma 19. It holds that\n\n(cid:40)\n\nPr\n\nnk(s′, s, a) ≥\n\n1 2\n\n· nk(s, a) · p(s′|s, a) − 2 log\n\n(cid:19)\n\n,\n\n(cid:18) SA δ′ (cid:41)\n\n∀k > 0, ∀(s′, s, a) ∈ S × S × A\n\n≥ 1 − δ′.\n\nProof of Lemma 19. For any k, h ∈ [H] and (s, a) ∈ S × A, conditioning on the event {sk s, ak p(s′|s, a). Then, using Lemma F.4 in (Dann et al., 2017), we can obtain this lemma.\n\nh = h+1 = s′(cid:9) is a Bernoulli random variable with success probability\n\nh = a}, the indicator 1 (cid:8)sk\n\nTo summarize, we define some concentration events which will be used in the following proof. (cid:18) SA δ′\n\n, ∀k > 0, ∀(s, a) ∈ S × A\n\nυπk′ (s, a) − log\n\nnk(s, a) ≥\n\nG1 :=\n\nk−1 (cid:88)\n\n1 2\n\n(cid:40)\n\n(cid:41)\n\n(cid:19)\n\nk′=1\n\n(cid:40)\n\nG2 :=\n\nnk(s′, s, a) ≥\n\n1 2\n\n· nk(s, a) · p(s′|s, a) − 2 log\n\n(cid:19)\n\n(cid:18) SA δ′\n\n, ∀k > 0, ∀(s′, s, a) ∈ S × S × A\n\n(cid:41)\n\nG :=G1 ∩ G2 Lemma 20. Letting δ′ = δ\n\n2 , it holds that\n\nPr [G] ≥ 1 − δ.\n\nProof of Lemma 20. This lemma can be obtained by combining Lemmas 18 and 19.\n\nF.1.2 OVERESTIMATION AND GOOD STAGE\n\nRecall that in Worst Path RL, for any k > 0, h ∈ [H] and (s, a) ∈ S × A, Q∗ In addition, ˆQk mins′∼p(·|s,a)(V ∗ mins′∼ ˆpk(·|s,a)( ˆV k Lemma 21 (Overestimation). For any k > 0, h ∈ [H] and (s, a) ∈ S × A, ˆQk and ˆV k\n\nh (s) := maxa∈A Q∗ h (s) := maxa∈A ˆQk\n\nh+1(s′)) and V ∗ h+1(s′)) and ˆV k\n\nh(s, a). h(s, a).\n\nh (s) ≥ V ∗\n\nh (s).\n\nh(s, a) := r(s, a) + h(s, a) := r(s, a) +\n\nh(s, a) ≥ Q∗\n\nh(s, a)\n\nRemark. Lemma 21 shows that if the Q-value of some state-action pair is not accurately estimated, it can only be overestimated (not underestimated). This feature is due to the min metric in the Worst Path RL formulation (Eq. (2)).\n\nProof of Lemma 21. We prove this lemma by induction.\n\nFor any k > 0 and s ∈ S, ˆV k\n\nH+1(s) = V ∗\n\nH+1(s) = 0.\n\nFor any k > 0, h ∈ [H] and (s, a) ∈ S × A, since r(s, a) is known and ˆV k h+1(s) (due to the induction hypothesis), if ˆpk(·|s, a) has detected all successor states, then ˆQk h(s, a). Otherwise, if ˆpk(·|s, a) has not detected all successor states, due to the property of min, we also have ˆQk\n\nh(s, a). Therefore, we have ˆV k\n\nh (s), which completes the proof.\n\nh(s, a) ≥ Q∗\n\nh+1(s) ≥ V ∗\n\nh(s, a) ≥ Q∗\n\nh (s) ≥ V ∗\n\n47\n\nPublished as a conference paper at ICLR 2023\n\nLemma 22 (Non-increasing Estimated Value). For any k1, k2 > 0 such that k1 ≤ k2, h ∈ [H] and (s, a) ∈ S × A, ˆQk1\n\nh (s, a) and ˆV k1\n\nh (s) ≥ V k2\n\nh (s, a) ≥ ˆQk2\n\nh (s).\n\nProof of Lemma 22. We prove this lemma by induction.\n\nFor any k1, k2 > 0 such that k1 ≤ k2 and s ∈ S, ˆV k1\n\nH+1(s) = V k2\n\nH+1(s) = 0.\n\nh+1(s) ≥ V k2\n\nh (s, a) ≥ ˆQk2\n\nFor any k1, k2 > 0 such that k1 ≤ k2, h ∈ [H] and (s, a) ∈ S × A, since r(s, a) is known and ˆV k1 h+1(s) (due to the induction hypothesis), if ˆpk1 (·|s, a) has detected all successor states, then ˆQk1 h (s, a). Otherwise, if ˆpk1 (·|s, a) has not detected all successor states, due to the min metric and that ˆpk2(·|s, a) will detect more (or the same) successor states than ˆpk1 (·|s, a), we also have ˆQk1 h (s), which completes the proof.\n\nh (s, a). Therefore, we have ˆV k1\n\nh (s, a) ≥ ˆQk2\n\nh (s) ≥ V k2\n\nRemark. Combining Lemmas 21 and 22, we have that as the episode k increases, the estimated value ˆQk\n\nh (s)) will decrease to its true value Q∗\n\nh (s)) or keep the same.\n\nh(s, a) ( ˆV k\n\nh(s, a) (V ∗\n\nLet S∗ := {s ∈ S : vπ∗ (s) > 0} denote the set of states which are reachable for an optimal policy. Lemma 23 (Good Stage). If there exists some episode ̄k > 0 which satisfies that for any h ∈ [H] and s ∈ S∗, ˆV ̄k h(s) suggests an optimal action, then we have that for any k ≥ ̄k, h (s) = V ∗ h(s) suggests an optimal action, and thus for any k ≥ ̄k, h ∈ [H] and s ∈ S∗, ˆV k algorithm MaxWP takes an optimal policy.\n\nh (s) and π ̄k h (s) = V ∗\n\nh (s) and πk\n\nRemark. Lemma 23 reveals that if in some episode ̄k, for any step h and state s ∈ S∗, algorithm MaxWP estimates the V-value accurately and chooses an optimal action, then hereafter, algorithm MaxWP always takes an optimal policy. We say algorithm MaxWP enters a good stage, if starting from some episode ̄k, for any k ≥ ̄k, h ∈ [H] and s ∈ S∗, ˆV k\n\nh(s) suggests an optimal action.\n\nh (s) and πk\n\nh (s) = V ∗\n\nProof of Lemma 23. Suppose that in episode ̄k, we have that for any h ∈ [H] and s ∈ S∗, ˆV ̄k h (s) and π ̄k V ∗ for any h ∈ [H] and s ∈ S∗, for each optimal action a (such that Q∗ h(s, a) < V ∗ Q∗ V ∗\n\nh (s) = h(s) suggests an optimal action. This is equivalent to the statement that in episode ̄k, h(s, a) = h(s, a) <\n\nh(s, a), and for each suboptimal action a (such that Q∗ h (s).\n\nh (s)), ˆQ ̄k h(s, a) ≤ ˆQ ̄k\n\nh(s, a) = V ∗ h (s)), Q∗\n\nUsing Lemmas 21 and 22, we have that for any h ∈ [H] and s ∈ S∗, as k increases, ˆQk either decrease to the true value Q∗ h ∈ [H] and s ∈ S∗, for each optimal action a, ˆQ ̄k a, Q∗\n\nh(s, a) will h(s, a) or keep the same. Therefore, we have that for any k ≥ ̄k, h(s, a), and for each suboptimal action\n\nh(s, a) = Q∗ h (s), which completes the proof.\n\nh(s, a) ≤ ˆQ ̄k\n\nh(s, a) < V ∗\n\nF.1.3 PROOF OF THEOREM 4\n\nProof of Theorem 4. Suppose that event G holds.\n\nLet\n\n ̄T :=\n\n(cid:88)\n\n(s,a)\n\n1\n\nmin π: υπ(s,a)>0\n\nυπ(s, a) ·\n\nmin s′∈supp(p(·|s,a))\n\np(s′|s, a)\n\n(cid:18)\n\n· 8\n\n2 log\n\n(cid:19)\n\n(cid:18) SA δ\n\n(cid:19)\n\n+ 1\n\n.\n\nFor any (s, a) ∈ S × A, let\n\n ̄T (s, a) :=\n\n1\n\nmin π: υπ(s,a)>0\n\nυπ(s, a) ·\n\nmin s′∈supp(p(·|s,a))\n\np(s′|s, a)\n\n(cid:18)\n\n· 8\n\n2 log\n\n(cid:19)\n\n(cid:18) SA δ\n\n(cid:19)\n\n+ 1\n\n.\n\nIt holds that ̄T = (cid:80)\n\n(s,a)∈S×A\n\n ̄T (s, a).\n\n48\n\nPublished as a conference paper at ICLR 2023\n\n ̄T +1 h\n\n ̄T +1 h\n\n ̄T +1 h\n\n(s) > V ∗\n\n(s) = V ∗\n\nh (s) and π\n\nh (s) (the value function can only be overestimated) or π\n\nAccording to Lemma 23, in order to prove Theorem 4, it suffices to prove that in episode ̄T + 1, for any h ∈ [H] and s ∈ S∗, ˆV (s) suggests an optimal action, i.e., algorithm MaxWP has entered the good stage in episode ̄T + 1. We prove this statement by contradiction. Suppose that in episode ̄T + 1, there exists some h ∈ [H] and some s ∈ S∗ which satisfy that ˆV (s) suggests a suboptimal action. If the policy π ̄T +1 taken in episode ̄T + 1 is optimal, then there exists some h ∈ [H], some s ∈ S∗ and some optimal action a which satisfy that vπ ̄T +1(s) > 0 and ˆQ h(s, a). Otherwise, if the policy π ̄T +1 taken in episode ̄T + 1 is suboptimal, then there exists some h ∈ [H], some s ∈ S∗ and some suboptimal action a which satisfy that vπ ̄T +1(s) > 0 and ˆQ h(s, a). Hence, no matter which of the above cases happen, we have that there exists some h ∈ [H] and some (s, a) ∈ S × A which satisfy that vπ ̄T +1(s, a) > 0 and ˆQ Under the min metric in Worst Path RL, the overestimation of a Q-value comes from the following reasons: (i) Algorithm MaxWP has not detected the successor state with the lowest V-value. (ii) The V-values of successor states at the next step are overestimated.\n\n(s, a) > Q∗\n\n(s, a) > Q∗\n\n(s, a) > Q∗\n\nh(s, a).\n\n ̄T +1 h\n\n ̄T +1 h\n\n ̄T +1 h\n\n ̄T +1 h\n\n ̄T +1 h\n\nIf the overestimation of ˆQ (s, a) comes from the overestimation of the V-values at the next step (reason (ii)), then we have that at the next step, there exists some state-action pair whose Q-value is overestimated. Then, we can trace the overestimation from (s, a) at step h to some (s′, a′) at some step h′ ≥ h, which satisfies that ˆQ h′+1(x) for any x ∈ S. In other words, the overestimation of ˆQ (s′, a′) is purely due to that at (s′, a′), algorithm MaxWP has not detected the successor state x with the lowest value V ∗\n\n ̄T +1 h′+1 (x) = V ∗\n\nh′(s′, a′) and ˆV\n\n(s′, a′) > Q∗\n\n ̄T +1 h′\n\n ̄T +1 h′\n\nh′+1(x).\n\nFor any k > 0 and (s, a) ∈ S × A, let T k(s, a) = {k′ < k : υπk′ (s, a) > 0} denote the set of episodes where (s, a) is reachable before episode k. We consider the following two cases according to whether |T ̄T +1(s′, a′)| is large enough to detect all successor states of (s′, a′).\n\nCase (1):\n\nIf |T ̄T +1(s′, a′)| ≥ ̄T (s′, a′), using Lemma 18, we have\n\nk−1 (cid:88)\n\nk′=1\n\nυπk′ (s′, a′) − log\n\n(cid:19)\n\n(cid:18) SA δ′\n\n1 2\n\n1 2\n\nnk(s′, a′) ≥\n\n≥\n\n=\n\n≥\n\n· ̄T (s′, a′) ·\n\n4 (cid:0)2 log (cid:0) SA\n\nδ\n\nδ\n\nmin s′∈supp(p(·|s,a))\n\n2 (cid:0)2 log (cid:0) SA\n\nmin s′∈supp(p(·|s,a))\n\n(cid:1) + 1(cid:1) p(s′|s, a)\n\nυπ(s′, a′) − log\n\n(cid:19)\n\n(cid:18) SA δ\n\nmin π: υπ(s′,a′)>0 (cid:1) + 1(cid:1) p(s′|s, a)\n\n− log\n\n(cid:19)\n\n(cid:18) SA δ\n\nThen, using Lemma 19, we have that for any s ∈ supp(p(·|s′, a′)),\n\nnk(s, s′, a′) ≥\n\n≥\n\n1 2\n\n1 2\n\n· nk(s′, a′) ·\n\nmin s∈supp(p(·|s′,a′))\n\np(s|s′, a′) − 2 log\n\n(cid:18)\n\n· 2\n\n2 log\n\n(cid:19)\n\n(cid:18) SA δ\n\n(cid:19)\n\n+ 1\n\n− 2 log\n\n(cid:19)\n\n(cid:18) SA δ\n\n(cid:19)\n\n(cid:18) SA δ\n\nwhich contradicts that ˆQk\n\nh′(s′, a′) is overestimated.\n\n=1\n\nIf |T ̄T +1(s′, a′)| < ̄T (s′, a′), we say the overestimation in episode ̄T + 1 is due to the Case (2): insufficient visitation on (s′, a′). Then, among episodes 1, . . . , ̄T , we exclude the episodes contained\n\n49\n\nPublished as a conference paper at ICLR 2023\n\nin T ̄T +1(s′, a′), i.e., we ignore the episodes where (s′, a′) is reachable and can be the source of the overestimation. Then, the number of excluded episodes due to (s′, a′) is |T ̄T +1(s′, a′)| < ̄T (s′, a′). According to Lemma 23, since episode ̄T + 1 has not entered the good stage, for any k ≤ ̄T , episode k has also not entered the good stage. Then, for any k ≤ ̄T , there exists some h ∈ [H] and some s ∈ S∗ which satisfy that ˆV k h(s) suggests a suboptimal action. This implies that there exists some h ∈ [H] and some (s, a) ∈ S × A which satisfy that vπk (s, a) > 0 and ˆQk\n\nh (s) > V ∗\n\nh (s) or πk\n\nh(s, a) > Q∗\n\nh(s, a).\n\nh′+1(x) = V ∗\n\nh′(s′, a′) and ˆV k\n\nConsider the last episode k among episodes 1, . . . , ̄T which is not excluded. Using the above argument, let (sk, ak) denote the source of overestimation in episode k which satisfies that ˆQk h′(s′, a′) > Q∗ h′+1(x) for any x ∈ S. Since we have excluded all the episodes where (s′, a′) is reachable among episodes 1, . . . , ̄T and episode k is not excluded, it holds that (sk, ak) ̸= (s′, a′). We repeat the above analysis on T k(sk, ak). If Case (1) happens, i.e., |T k(sk, ak)| ≥ ̄T (sk, ak), then we can derive a contradiction and complete the proof. If Case (2) happens, i.e., |T k(sk, ak)| < ̄T (sk, ak), we exclude episode k and the episodes contained in T k(sk, ak). Then, the number of excluded episodes due to (sk, ak) among episodes 1, . . . , ̄T is at most |T k(sk, ak)| + 1 ≤ | ̄T (sk, ak)|.\n\nWe repeat the above procedure. Once Case (1) happens, we can derive a contradiction and complete the proof. Otherwise, if Case (2) keeps happening, we will exclude the episodes due to the reachability and possible overestimation of (s, a) for all (s, a) ∈ S × A, and the total number of excluded episodes is strictly smaller than (cid:80) ̄T (s, a) = ̄T . Thus, there exists some episode k0 among episodes 1, . . . , ̄T which satisfies that for any (s, a) ∈ S ×A, υπk0 (s, a) = 0, which gives a contradiction.\n\n(s,a)∈S×A\n\nF.2 REGRET LOWER BOUND\n\nIn the following, we establish a regret lower bound for Worst Path RL and give its proof.\n\nTo exclude trivial instance-specific algorithms and formally state our lower bound, we first define an o(K)-consistent algorithm as an algorithm which guarantees an o(K) regret on any instance of Worst Path RL.\n\nTheorem 6. There exists an instance of Worst Path RL, for which the regret of any o(K)-consistent algorithm is at least\n\n(cid:32)\n\nΩ\n\nmax\n\n(s,a): ∃h, a̸=π∗\n\nh(s)\n\nH\n\nmin π: υπ(s,a)>0\n\nυπ(s, a) ·\n\nmin s′∈supp(p(·|s,a))\n\np(s′|s, a)\n\n(cid:33) ,\n\nwhere max(s,a):∃h, a̸=π∗ at some step.\n\nh(s) takes the maximum over all (s, a) such that a is sub-optimal in state s\n\nThe intuition behind this lower bound is as follows. For a critical but hard-to-reach state s, any o(K)-consistent algorithm must explore all actions a in state s, in order to detect their induced successor states s′ and distinguish the optimal action. This process incurs a regret dependent on factors minπ:υπ(s,a)>0 υπ(s, a) and mins′∈supp(p(·|s,a)) p(s′|s, a), and hence the lower bound.\n\nProof of Theorem 6. Consider the instance I as shown in Figure 8:\n\nThe state space is S = The action space contains two actions, {s1, s2, . . . , sn, x1, x2, x3}, where n = S − 3 and s1 is the initial state. Let H ≫ S and 0 < α < 1 4 . The reward functions are as follows. For any a ∈ A, r(x1, a) = 1, r(x2, a) = 0.8 and r(x3, a) = 0.2. For any i ∈ [n] and a ∈ A, r(si, a) = 0.\n\ni.e., A = {a1, a2}.\n\nThe transition distributions are as follows. For any a ∈ A, p(s2|s1, a) = α, p(x1|s1, a) = 1 − 3α, p(x2|s1, a) = α and p(x3|s1, a) = α. For any i ∈ {2, . . . , n − 1} and a ∈ A, p(si+1|si, a) = α and p(x1|si, a) = 1 − α. x1, x2 and x3 are absorbing states, i.e., for any a ∈ A, p(x1|x1, a) = 1, p(x2|x2, a) = 1 and p(x3|x3, a) = 1. The state sn is a bandit state, which has an optimal action and a suboptimal action. Let a∗ denote the optimal action in state sn, which is uniformly drawn from\n\n50\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: The instance for lower bound under the min metric (Theorem 6).\n\n{a1, a2}, and let asub denote the other sub-optimal action in state sn. For the optimal action a∗, p(x2|sn, a∗) = 1. For the sub-optimal action asub, p(x2|sn, asub) = 1 − α and p(x3|sn, asub) = α.\n\nFix an o(K)-consistent algorithm A, which guarantees a sub-linear regret on any instance of Worst Path RL. We have that A needs to observe the transition from (sn, asub) to x3 at least once. Otherwise, without any observation of the transition from (sn, asub) to x3, A can only trivially choose a1 or a2 in state sn, and no matter A chooses a1 or a2, it will suffer a linear regret in the counter instance where the unchosen action is optimal.\n\nThus, any o(K)-consistent algorithm must observe the transition from (sn, asub) to x3 at least once, and needs at least\n\n1 υπsub (sn, asub) · p(x3|sn, asub)\n\nepisodes with sub-optimal policies. Here πsub denotes a policy which chooses asub in state sn, and υπsub (sn, asub) denotes the probability that (sn, asub) is visited at least once in an episode under policy πsub.\n\nOnce the agent takes a sub-optimal policy in an episode, she will suffer regret 0.6(H − n) in this episode.\n\nTherefore, A needs to suffer at least\n\n(cid:18)\n\nΩ\n\n1 υπsub (sn, asub) · p(x3|sn, asub)\n\n(cid:19)\n\n· 0.6(H − n)\n\nregret in expectation.\n\nSince in the constructed instance (Figure 8) 1\n\nmax\n\n(s,a): ∃h, a̸=π∗\n\nh(s)\n\nmin π: υπ(s,a)>0\n\nυπ(s, a) ·\n\nmin s′∈supp(p(·|s,a))\n\np(s′|s, a)\n\n=\n\n1 υ(sn, asub) · p(x3|sn, asub)\n\n,\n\nwe have that A needs to suffer at least \n\n\n\nΩ\n\nregret.\n\nmax\n\n(s,a): ∃h, a̸=π∗\n\nh(s)\n\nH\n\nmin π: υπ(s,a)>0\n\nυπ(s, a) ·\n\nmin s′∈supp(p(·|s,a))\n\np(s′|s, a)\n\n\n\n\n\nG TECHNICAL TOOL\n\nIn this section, we present a useful technical tool.\n\n51\n\n(cid:2869)(cid:2870)(cid:3041)(cid:2879)(cid:2869)(cid:3041)(cid:2869)(cid:2870)(cid:2871)Reward Reward Reward ...Action ∗: 1Action (cid:3046)(cid:3048)(cid:3029): Action (cid:3046)(cid:3048)(cid:3029): Published as a conference paper at ICLR 2023\n\nLemma 24 (Lemma 13 in (M ́enard et al., 2021)). Let A, B, C, D, E and β be positive scalars such that 1 ≤ B ≤ E and β ≥ e. If T ≥ 0 satisfies\n\n(cid:113)\n\nT ≤ C\n\nT (cid:0)A log (βT ) + B log2 (βT )(cid:1) + D (cid:0)A log (βT ) + E log2 (βT )(cid:1) ,\n\nthen we have\n\nwhere\n\nT ≤ C 2 (A + B) C 2\n\n1 +\n\n(cid:16)\n\nD + 2\n\n√\n\n(cid:17)\n\nDC\n\n(A + E) C 2\n\n1 + 1,\n\nC1 =\n\n8 5\n\nlog (cid:0)11β2 (A + E) (C + D)(cid:1) .\n\n52",
    "reference": "# Summary Of The Paper\n\nThis paper studies risk-sensitive reinforcement learning. The objective is to maximize the worst $\\alpha$-percent tail of the reward-to-go at each step. This paper designs algorithms for regret minimization and best policy identification problems using the concentration of conditional value at risk. For the regret minimization problem, regret upper and lower bounds are provided; for the best policy identification problem, the sample complexity upper and lower bounds are provided. This paper also studies the limiting case when $\\alpha$ is close to 0, i.e.the worst path RL.\n\n# Strength And Weaknesses\n\nStrength:\n1. The paper studies the CVaR-based MDPs with the unknown transition. \n2. Both upper and lower bounds results are provided.\n3. The usage of concentration of conditional value at risk is interesting.\n\nWeaknesses:\n1. This paper does not use the Bernstein-type exploration bonuses. There could be potential improvements.\n2. The lower bound results do not involve the size of state space, which seems loose.\n\nI acknowledge the authors' response.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and comprehensive. The extension from the traditional RL framework to CVaR RL is interesting.\n\n# Summary Of The Review\n\nThis paper conducts a novel and comprehensive study for CVaR RL.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTABCAPS: A CAPSULE NEURAL NETWORK FOR TABULAR DATA CLASSIFICATION WITH BOW ROUTING\n\nJintai Chen1, Kuanlun Liao1, Yanwen Fang2, Danny Z. Chen3, Jian Wu∗4\n\n1College of Computer Science and Technology, Zhejiang University 2Department of Statistics & Actuarial Science, The University of Hong Kong 3Department of Computer Science and Engineering, University of Notre Dame 4Department of Public Health, Zhejiang University School of Medicine\n\nABSTRACT\n\nRecords in a table are represented by a collection of heterogeneous scalar features. Previous work often made predictions for records in a paradigm that processed each feature as an operating unit, which requires to well cope with the heterogeneity. In this paper, we propose to encapsulate all feature values of a record into vectorial features and process them collectively rather than have to deal with individual ones, which directly captures the representations at the data level and benefits robust performances. Specifically, we adopt the concept of capsules to organize features into vectorial features, and devise a novel capsule neural network called TABCAPS to process the vectorial features for classification. In TABCAPS, a record is encoded into several vectorial features by some optimizable multivariate Gaussian kernels in the primary capsule layer, where each vectorial feature represents a specific profile of the input record and is transformed into senior capsule layer under the guidance of a new straightforward routing algorithm. The design of routing algorithm is motivated by the Bag-of-Words (BoW) model, which performs capsule feature grouping straightforwardly and efficiently, in lieu of the computationally complex clustering of previous routing algorithms. Comprehensive experiments show that TABCAPS achieves competitive and robust performances in tabular data classification tasks. Codes are available at https://github.com/WhatAShot/TabCaps.\n\n1\n\nINTRODUCTION\n\nTabular data are ubiquitous in real world applications, which records abundantly meaningful information such as medical examination results (Hassan et al., 2020) and company financial statements (Addo et al., 2018). Previous methods often processed a record by treating the scalar feature values as the operating units. For example, decision tree based methods (Breiman et al., 1984; Chen & Guestrin, 2016) used one tabular feature in each decision step, and neural networks (Gorishniy et al., 2021; Chen et al., 2022) elaborately executed feature-wise interactions to capture higher-level semantics.\n\nHowever, it is intractable to design effective feature-wise interaction approaches (Grinsztajn et al., 2022; Ng, 2004) due to the heterogeneity among features. In this paper, we propose a novel paradigm for supervised tabular learning, which encapsulates all feature values of records into vectorial features and directly conducts on the vectorial feature level. Such design utilizes the sufficient representation space of the vectorial feature format to probably learn the comprehensive data level semantics, and avoids executing complex interactions among heterogeneous features.\n\nTo this end, we borrow the concept of capsules (Sabour et al., 2017) to organize vectorial features, and propose a novel capsule neural network (CapsNet) called TABCAPS for tabular data classification. In TABCAPS, several optimizable multivariate Gaussian kernels encode all the features of each record into the primary capsules, in which features in a vector format represent the marginal likelihoods of the record in reference to the corresponding multivariate Gaussian distributions. We set the scale and location parameters of these Gaussian kernels learnable, thus allowing these kernels to model some plausible data patterns for the dataset and each primary capsule feature represents a specific profile\n\n∗ Corresponding author. E-mail: wujian2000@zju.edu.cn.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustrating (a) our proposed TABCAPS (without the decoder) and (b) the process of senior capsule feature synthesis from features in primary capsules (taking features in the j-th senior capsule as example). “P.C” denotes “primary capsule” and “S.C.” denotes “senior capsule”.\n\nof the input record that measures the likelihood to these data patterns. Unlike previous CapsNets that used one senior capsule to predict the belonging probability for one class, we allot multiple senior capsules for a class, motivated by ensemble learning. In previous CapsNets, primary capsule features were transformed into senior capsules by an affinity projection and a routing algorithm (Sabour et al., 2017; Hinton et al., 2018) that groups similar primary capsule features by clustering processes. In TABCAPS, a novel sparse projection method and a novel straightforward routing algorithm are proposed to perform the feature transformation from primary to senior capsules.\n\nOur proposed routing algorithm is much more efficient than previous routing algorithms, since the primary capsules in previous CapsNets captured some unknown semantics from unstructured data (e.g., images) and had to apply feature clustering in an iterative process to attain higher-level semantics. In our TABCAPS, features in primary capsules are the likelihood w.r.t. the Gaussian distributions, which represents stable semantics and allows to perform a straightforward routing algorithm. Motivated by the bag-of-words (BoW) model (Salton & Lesk, 1965) which is efficient in similar information search (Pineda et al., 2011), we propose a straightforward routing (called BoW Routing) for TABCAPS. The proposed BoW Routing computes similarities between primary capsule features and the initialized senior capsule features by counting the co-occurrences of “words” (implemented to be learnable templates), and incorporates those primary capsule features whose similarities are beyond an adaptive threshold to update senior capsule features. Similar to previous CapsNets, we also present a tabular-data-suited decoder, which is tailored for tabular feature reconstruction tasks (e.g., missing value imputation).\n\nContributions. (i) For the first time, we propose to encapsulate all the features of a record in table into vectorial features as operating units, which avoids inefficient interactions between heterogeneous tabular features and directly learns the data-level semantics. (ii) We propose a CapsNet tailored for supervised tabular learning, in which we devise a new type of primary capsule with learnable multivariate Gaussian kernels and conduct a novel straightforward routing to synthesize senior capsule features at low costs. (iii) Experiments on real-world datasets validate that TABCAPS attain robust performances in tabular data classification.\n\n2 RELATED WORK\n\nCapsule Neural Networks. Capsule neural networks (Ribeiro et al., 2022) were first proposed to deal with image data, where a capsule represents an object whole or object part in a vector or matrix format. Firstly, primary capsules capture the basic object parts by using templates (Kosiorek et al., 2019), optical flow analysis (Sabour et al., 2021), or location-based operations (Hinton et al., 2018). Features in primary capsules are linearly projected to transpose the poses of object parts, so as to synthesize senior capsule features (representing object wholes) guided by a routing algorithm. Most routing algorithms performed iteration processes at high computational costs (Sabour et al., 2017; Hinton et al., 2018), since the primary capsules handle different semantics in dealing with different samples. To avoid iterations, some straightforward approaches were proposed (Choi et al., 2019; Ahmed & Torresani, 2019; Chen et al., 2021; Ribeiro et al., 2020). However, these straightforward\n\n2\n\nTabular Feature Extraction...ConcatenationSparse Transformation & BoWRouting...VotingPredicted ProbabilitiesRaw Features(in order)P.C. FeaturesS.C. Features...............Sparse Projection by Entmax1.5(Wj)P.C. FeaturesInitialized S.C. Features v′jP.C. Votes ොuj|∙...Computing AverageP.C. Votes SelectingFeatures Updatingof Final S.C.(a) An illustration of the proposed TABCAPS.(b) The process of the j-thSenior Capsule FeatureSynthesis.Multivariate Gaussian Kernels VotingVotingBoWRouting Alg.s2′s1′s3′Words’ SpaceUpdatingPublished as a conference paper at ICLR 2023\n\napproaches were for image tasks and some of them got rid of the part-to-whole learning assumption. At present, CapsNets have been widely used for image processing (Rajasegaran et al., 2019; Hinton et al., 2018; Chen et al., 2021), point clouds (Zhao et al., 2019b; 2020), recommendation in shopping (Li et al., 2019), and language tasks (Zhao et al., 2019a). However, to our best knowledge, no bespoke CapsNets were proposed for general supervised tabular learning tasks. This paper presents a novel CapsNet for tabular data, in which primary capsules learn some specific kinds of data patterns for the dataset rather than the data-adaptive “object parts”.\n\nSupervised Tabular Learning. Various methods were proposed for tabular data processing, mainly including decision tree algorithms (Breiman et al., 1984; Quinlan, 1979; Chen & Guestrin, 2016; Prokhorenkova et al., 2018; Ke et al., 2017) and neural network (NN) based methods (Arik & Pfister, 2020; Popov et al., 2019; Chen et al., 2022; Abutbul et al., 2021). Different from tree based methods that split tabular feature spaces by some information metrics (Quinlan, 1979; 2014; Breiman et al., 1984) and sought for possible decision paths, NN based methods often introduced attention and grouping mechanisms (Chen et al., 2022; Arik & Pfister, 2020; Guo et al., 2017; Wang et al., 2017; Gorishniy et al., 2021; Yan et al., 2023) for tabular feature interactions to yield high-level features. However, tabular features in a table are heterogeneous and are relation-agnostic, and thus it is hard to conduct all the feature interactions in the right way.\n\n3 METHODOLOGY\n\nWe propose to classify each record in a table by processing a vectorial feature containing all its feature values in order. See the pipeline of our TABCAPS in Fig. 1(a), to better depict an input record, its raw features are first processed by a simple function (e.g., an Abstract Layer (Chen et al., 2022) or a linear projection layer) to yield more comprehensive features, which are then concatenated with raw features and we obtain the basal vectorial feature of size m, x ∈ Rm. Then, the vectorial feature is projected into primary capsules (in a vector format) by various learnable multivariate Gaussian kernels. Each vectorial feature (in a primary capsule) is transformed into feature votes for senior capsules by the proposed sparse projection (see Sec. 3.2.1), and senior capsule features are finally obtained by selecting and merging feature votes, guided by our BoW Routing (see Sec. 3.2.2).\n\n3.1 HOW TO ENCODE RECORDS INTO PRIMARY CAPSULES?\n\nUnlike the known CapsNets for images that utilized various primary capsules to represent different parts of objects, in TABCAPS, different primary capsules represent various profiles for just one record. Formally, a record represented by a vectorial feature x ∈ Rm is transformed into a feature ui ∈ Rm for the i-th primary capsule by a multivariate Gaussian kernel ki, as:\n\nui = ki(x; μi, Σi) =\n\nexp(− 1\n\n2 (x − μi)T Σ−1 (2π)m/2|Σi|\n\ni\n\n(x − μi))\n\n,\n\n(1)\n\nwhere the location parameter vector μi ∈ Rm and the scale parameters Σi = diag(σ1, σ2, . . . , σm) of the multivariate Gaussian kernel ki are set to be learnable, so that TABCAPS allows to use proper multivariate Gaussian kernels in different datasets. Since features in a table are typically regarded as independent, we set Σ as a diagonal matrix. After Eq. (1), a layer normalization is performed on each ui (i = 1, 2, . . . , Np, Np is the count of the primary capsules) for normalization following Tsai et al. (2019). Specially, we also encapsulate the original feature values (denoted by x) to be an additional capsule feature.\n\nThis design can be interpreted that a primary capsule models an data pattern by determining the parameters (μ, Σ) of the multivariate Gaussian kernels, and various primary capsule features of an input record are represented as the likelihoods in reference to these data patterns. It is beneficial to find some dataset-specific data patterns, which allows to implicitly model the tabular feature relations.\n\n3.2 HOW TO SYNTHESIZE SENIOR CAPSULE FEATURES? Motivated by ensemble learning, we use multiple senior capsules to learn the semantics for one target class and obtain the final prediction by voting, which also meets the need of irregular target pattern fitting (Grinsztajn et al., 2022). To attain the senior capsule features, primary capsule features are transformed by a sparse weight matrix into feature votes, and a newly proposed BoW Routing selects feature votes to synthesize senior capsule features. Note that the elements in vectorial features are linearly transformed (reversible), and the non-linear mapping (the routing) is performed by taking vectorial features as operating units.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3.2.1 SPARSE PROJECTION The affinity projection in previous work (Sabour et al., 2017) uses a learnable weight Wj|i ∈ Rm×n to linearly map the i-th primary capsule feature ui ∈ Rm to yield a feature vote ˆuj|i ∈ Rn for the j-th senior capsule, by ˆuj|i = uiWj|i. We modify the weight matrix by adding a sparse operation entmax (Peters et al., 2019), and the transformation is computed by:\n\nˆuj|i = uiW ′\n\nj|i , W ′\n\nj|i = entmaxα(Wj|i),\n\n(2)\n\nwhere α = 1.5 as default. In this way, the weight matrix W ′ j|i shall be sparse and only parts of elements of uj|i is used to synthesize the feature votes and the target-irrelevant elements are excluded. Different from sparse attention operations, the entmax is applied to the weight matrix. Thus, the capsule feature component selection is consistent for different records.\n\n3.2.2 BOW ROUTING\n\nGiven the synthesized feature votes ˆuj|i for the j-th senior capsule (where i = 1, 2, . . . , Np, j = 1, 2, . . . , Ns, Np and Ns are the amounts of primary capsules and senior capsules respectively), we use a routing algorithm to synthesize senior capsule features by selecting and merging ˆuj|i. Note that the vanilla routing algorithm was a clustering-like approach (Sabour et al., 2017), because a primary capsule may capture any kinds of object parts from images and thus the “cluster centroids” are unknown before the routing step. In our model, each primary capsule learns concrete semantics that represent record profiles in reference to a specific multivariate Gaussian distribution, making it possible to predict the initialized senior capsule features (“cluster centroids”) directly using a straightforward function w.r.t. semantics (features) captured by primary capsules. Here we simply use the average of the feature votes ˆuj|i to initialize senior capsule features v′\n\nˆuj|i/Np.\n\nj = (cid:80)Np\n\nj by v′\n\ni\n\nTo select some feature votes for senior capsule feature synthesis, we measure the similarities between the initialized senior capsule feature v′ j and the primary capsule feature vote ˆuj|i. An intuitive solution is to compute the similarity directly. However, directly computing similarity might hinder the performance, which overlooks the special data patterns and importance differences of different vectorial feature elements. Motivated by the success of the bag-of-words (BoW) model in similarity measure for information retrieval, we introduce the BoW model to the routing algorithm. A classical BoW model first clusters the given features, and collects a word dictionary whose words are the centroids of feature clusters. Then, a feature can be quantized as the “occurrence” of various words, and feature similarities can be computed by counting the co-occurring words. In this way, the similarity is computed in the semantic space guided by the “words” in discrete steps. Our differentiable BoW Routing is depicted as follow.\n\nOccurrence status computing. The dictionary words in the traditional BoW model are computed according to the extracted features. Motivated by Chen et al. (2021), we directly implement the word dictionary with a learnable parameter set of size M , {sm | m = 1, 2, . . . , M }, with each “word” sm ∈ Rn (of the identical size as the feature votes ˆuj|i). To compute the “occurrences” of the “words”, an occurrence status (a scalar) between ˆuj|i and the initialized senior capsule feature v′ j for the “word” sm is defined by:\n\npj|i;m = sigmoid(ˆuT\n\nj|i · s′\n\nm) ∈ (0, 1), pj;m = sigmoid(v′\n\nj\n\nT · s′\n\nm) ∈ (0, 1),\n\n(3)\n\nwhere pj|i;m and pj;m represent the degrees (i.e., the occurrence statuses) in which uT reveal the semantics of sm, we let pj|i = [pj|i;1, pj|i;2, . . . , pj|i;M ] and pj = [pj;1, pj;2, . . . , pj;M ] represent semantic codes of ˆuj|i and v′ j. In computing Eq. (3), the “word” sm is normalized into s′ m\nby the l2 norm to avoid the impact of the vector lengths. In traditional BoW models, an occurrence status is represented as binary (0 or 1), but by our routing algorithm, we generalize the discrete binary status to a continuous status form (in (0, 1)) using the “sigmoid” function.\n\nj|i and v′\n\nj\n\nT\n\nFeature vote merging. To compute the similarity between the occurrence statuses pj|i and pj, traditional BoW models may use the Jaccard distance. Since the occurrence statuses are continuous values in our routing algorithm, to which the Jaccard distance cannot be simply applied, we compute the pairwise distances dj|i between pj|i and pj using the squared Euclidean distance function D:\n\ndj|i = D(pj|i, pj) =\n\n(cid:88)M\n\nm=1\n\n(pj|i;m − pj;m)2.\n\n(4)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nNote that the squared Euclidean distance has been proved to be an effective approximation of the Jaccard distance (Reina et al., 2014). Based on the pairwise distances dj|i of feature votes and initialized senior capsule features, we can directly obtain the neighboring feature votes ˆuj|i to update the features in the j-th senior capsule by performing k-Nearest Neighbor (k-NN) search. However, we find that k-NN can lead to instability in training, since the distributions of feature votes are unknown and k-NN might select some feature votes with extreme values.\n\nTo gain training stability, we propose to use an adaptive threshold to determine a range around the initialized senior capsule feature v′ j (see Fig. 1(b)). We set this threshold as a learnable positive value γ2 > 0, and the selection of feature votes is determined by: qj|i = ε(γ2 − dj|i), where ε(·) is the Heaviside function, and if and only if qj|i = 1, it indicates that the feature vote from the i-th primary capsule, ˆuj|i, is selected. Then, we compute the weights for the feature votes and obtain the final senior capsule feature vj by a weighted sum, as:\n\n(5)\n\nwj|i = softmax(qj|i · e−dj|i), vj =\n\n(cid:88)Np i\n\nwj|i ˆuj|i,\n\n(6)\n\nwhere the “softmax” operation guarantees all wj|i > 0 (giving small weights to unselected votes), avoiding possible failure if no feature vote is selected to yield vj. In this way, our proposed BoW Routing performs a straightforward process to obtain the senior capsule features.\n\nRemarks. Distinct from previous straightforward routing algorithms (Chen et al., 2021; Ribeiro et al., 2020) for images, the efficiency of our BoW Routing partially results from the innovative definition of the primary capsules that learns concrete semantics, allowing to predict initialized senior capsule features directly, in lieu of learning unknown object parts and requiring clustering-like routing algorithms to group them. Our BoW Routing algorithm, with its strong ability to extract semantic patterns and flexible measurement of similarity, allows us to utilize a straightforward average method for initializing the senior capsule features.\n\n3.3 SENIOR CAPSULE VOTING FOR CLASSIFICATION\n\nSimilar to the known CapsNets, we let the vector lengths of the senior capsule features represent the existence probabilities of one semantics, and this work models each target class by multiple senior capsules motivated by ensemble learning. Suppose there are K classes. We divide all the senior capsules into K groups, such that the features in each group vote for one particular class. The classification outcomes are defined by:\n\nc = softmax([l1, . . . , lk, . . . , lK]) , ˆy = argmax(c), (7) where the vector c represents the predicted probabilities for all the K classes, and ˆy denotes the predicted label. A value lk in Eq.(7) denotes the mean of the senior capsule feature vector lengths for the k-th class (i.e., in the group Gk), which is defined by: lk = (cid:80) ||vj||2/||Gk||. Note that we dropout 20% senior capsules in a group before voting in training. We use the margin loss in training our TABCAPS following Sabour et al. (2017), as: K\n(cid:88)\n\nj∈Gk\n\nTk max(0, t+ − lk)2 + λ(1 − Tk) max(0, lk − t−)2,\n\nL =\n\n(8)\n\nk=1\n\nwhere Tk = 1 if the k-th class is the target, and otherwise Tk = 0. We set t+ = 0.9, t− = 0.1, and λ = 0.5 following Sabour et al. (2017).\n\n3.4 A DECODER FOR TABULAR FEATURE RECONSTRUCTION\n\nWe build a decoder based on the Abstract Layers (Chen et al., 2022) for tabular feature reconstruction. We stack two Abstract Layers in sequence with a shortcut performing feature concatenation, as shown in Fig. 2. Finally, a fully connected layer is on top of the decoder to produce the reconstructed tabular features. Since there are multiple senior capsules voting for a target class, the decoder input is the concatenation of features in the group of senior capsules that votes for the correct class. We do not use the decoder as regularization like previous CapsNets (Sabour et al., 2017) and TABCAPS without the decoder can perform well on classification tasks (see Fig. 3). The decoder is used when we attempt to reconstruct raw features for some tasks (e.g., missing value imputation), which is trained under the specification of the mean square error (MSE) that is scaled down by 5 × 10−6 and jointly works with the loss L (in Eq. (8)).\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUPS Datasets. We conduct experiments on 8 realworld tabular datasets, including Heart Failure Prediction (Heart) (Chicco & Jurman, 2020), Click (Cup, 2012), Diabetes (Ukani, 2020), EEG Eye State (EEG) (Dua & Graff, 2015), Gas Concentrations (Gas) (Vergara et al., 2012), HillValley (Hill) (Lee & Franz, 2008), Higgs-small (Higgs) (Baldi et al., 2014) and Epsilon (Challenge, 2008). All of these datasets have few or no categorical features, eliminating the impacts of categorical feature embedding approaches in model comparison. In pre-processing, data are standardized by the z-score normalization. We follow the train-test split and the data preprocessing in (Popov et al., 2019) for the Click dataset. For the other datasets, we randomly divide 80% of the data into the training sets, and the rest 20% are for test. We further sample 15% training data for validation. Experiments were run for 5 times with regard to random seeds.\n\nFigure 2: The decoder for reconstructing raw tabular features. The decoder input is the senior capsule features in the group that votes for the correct class. “Rec.” = “Reconstructed” and “S.C.” = “Senior Capsule”.\n\nImplementation details and comparison baselines. Our method is implemented with PyTorch on Python 3.7, and is run on GeForce RTX 2080 Ti. In training, we set the batch size as 2,048. We use the QHAdam optimizer (Ma & Yarats, 2019) with weight decay rate 10−5, v = (0.7, 0.99), and β = (0.95, 0.998). We compare TABCAPS with several baseline models, including TabNet (Arik & Pfister, 2020), NODE (Popov et al., 2019), FT-Transformer (Gorishniy et al., 2021), DANet24 (Chen et al., 2022), Net-DNF (Abutbul et al., 2021), XGboost (Chen & Guestrin, 2016), CatBoost (Prokhorenkova et al., 2018), the Vector CapsNet (Sabour et al., 2017), and fully connected neural networks (FCNNs) (Nair & Hinton, 2010) with lasso and mixup (Zhang et al., 2018) (α = 1.0) respectively. The Vector CapsNet (Sabour et al., 2017) was designed for images; we replace its convolution layer by a fully connected layer for tabular data. Following Popov et al. (2019); Arik & Pfister (2020), the performances of all the methods are hyperparameter-tuned on the validation sets (e.g., by grid search). The hyperparameter search spaces and search algorithms are given in Appendix A.1. We employ the “log-loss” metric to measure the model performances, which is considered as a more sensitive metric than accuracy, and is widely used (e.g., in Kaggle competitions). All the performances of the compared methods are obtained by running official codes.\n\n4.2 CLASSIFICATION PERFORMANCE COMPARISON\n\nThe classification performances are reported in Table 1. One can see that our TABCAPS obtains the best or second best performances on 7 out of 8 datasets, and is especially superior on some datasets by clear margins. In addition, we also report the computational complexity and efficiency on the Diabetes dataset, using the metrics of model size (# param.) and fps (the amount of records that a model processes per second). Since XGboost and CatBoost are traditional machine learning models, we do not report their complexity. One can see that TABCAPS is much more efficient and greatly lighter (over 10×) than the other state-of-the-art methods (e.g., FT-Transformer, Net-DNF, and DANet-24), and significantly outperforms the models with similar model complexity (e.g., regularized FCNNs and Vector CapsNet). Also, the results compared with Vector CapsNet shows the superiority of TABCAPS, obtaining better performances with lower complexity on tabular data.\n\n4.3 EXTREME GENERALIZATION TEST To further examine the generalization capability of TABCAPS, we attempt to test it with train-test split that the training data and test data are with very dissimilar feature values. We conduct t-SNE projections (with default hyper-parameters of the “sklearn” package 1) on all the records of datasets,\n\n1https://scikit-learn.org/stable/modules/generated/sklearn.manifold.\n\nTSNE.html\n\n6\n\n...Voting.........VotingVotingConcatenationAbstract LayerConcatenationAbstract LayerFully ConnectionS.C. FeaturesA Rec. Tabular Feature/Predicted Class ProbabilityCorrectClassPublished as a conference paper at ICLR 2023\n\nTable 1: Classification Performances. The best and second best performances of deep learning approaches are respectively marked in bold and underlined. Note that the reported log-loss values (the lower the better) are with a 100× factor. The model size (# param.) and inference speed (fps) are on the Diabetes dataset. The performances are reported as “mean±std”. Click\n\nEpsilon # param.\n\nDiabetes\n\nMethod\n\nHeart\n\nHiggs\n\nEEG\n\nGas\n\nHill\n\nfps\n\nXGboost Catboost\n\n62.253±0.02 14.338±0.03 14.117±0.02 2.087±0.06 32.371±0.04 69.049±1e-3 53.158±0.01 26.748±1e-3 64.273±0.08 14.777±0.07 18.423±0.12 2.064±0.05 30.043±0.14 69.174±0.06 53.273±0.05 27.228±2e-3\n\n– –\n\n– –\n\n3.4M 73.1 62.303±0.03 17.964±0.04 45.340±0.04 4.647±0.04 44.967±0.01 87.804±0.07 54.668±0.03 26.743±0.02 TabNet 67.633±0.02 13.767±0.02 17.386±0.01 1.229±0.04 55.371±0.03 15.787±0.03 53.417±0.02 27.122±0.03 8.5M 175.2 Net-DNF 63.206±0.05 45.951±0.03 47.654±0.04 38.774±0.04 46.541±0.04 69.220±0.04 61.864±0.08 27.838±0.54 13.4M 145.2 NODE 70.487±0.02 12.382±0.03 7.446±0.06 FT-Transformer 9.3M 284.7 5.5M 54.9 DANet-24 73.708±0.02 13.338±0.02 9.301±0.04 FCNN w/ mixup 63.863±0.07 12.715±0.05 9.572±0.07 0.7M 594.3 0.7M 568.8 FCNN w/ lasso 0.4M 318.5 Vector CapsNet TABCAPS (Ours) 62.054±0.04 12.043±0.03 8.130±0.05 0.2M 501.1\n\n2.258±0.04 27.547±0.05 20.084±0.03 53.310±0.02 25.958±0.85 2.171±0.02 49.643±0.04 24.763±0.03 53.033±0.01 26.431±0.01 2.083±0.06 36.742±0.02 56.005±0.05 56.787±0.04 27.467±0.03 87.005±0.17 41.071±0.75 31.852±0.05 4.141±0.06 44.881±0.06 69.302±0.01 132.102±0.07 32.282±0.02 64.135±0.05 52.635±0.03 53.587±0.06 161.547±0.03 58.516±0.04 51.591±0.02 62.654±0.02 54.252±0.02 2.013±0.03 34.047±0.02 14.301±0.04 53.776±0.03 25.821±0.02\n\nTable 2: Extreme generalization performances. The best and second best performances of deep learning approaches are respectively marked in bold and underlined. Note that the reported log-loss values (the lower the better) are with a 100× factor. The Epsilon dataset is not included due to its extremely high computation complexity in conducting t-SNE projection.\n\nMethod\n\nClick\n\nDiabetes\n\nEEG\n\nGas\n\nHeart\n\nHill\n\nHiggs\n\nTraining-Test Split\n\nXGboost Catboost\n\n66.070±0.03 63.925±0.04\n\n65.886±0.09 70.654±0.02 31.504±0.04 35.650±0.01 69.657±0.09 54.557±0.04 68.819±0.06 68.799±0.04 18.864±0.04 35.207±0.08 69.162±0.03 54.632±0.07\n\nTabNet Net-DNF NODE FT-Transformer DANet-24 FCNN mixup FCNN lasso Vector CapsNet TABCAPS (Ours)\n\n115.907±0.11 225.22±0.08 79.666±0.07 158.618±0.03 44.967±0.06 89.114±0.08 55.763±0.11 58.792±0.05 68.261±0.04 15.124±0.03 55.371±0.07 48.301±0.04 55.738±0.06 67.625±0.02 67.021±0.04 68.357±0.04 57.698±0.06 46.541±0.03 69.771±0.10 61.870±0.03 63.839±0.04 59.283±0.04 68.278±0.07 6.416±0.06 26.132±0.05 66.972±0.05 53.970±0.10 78.431±0.11 59.736±0.06 69.021±0.03 10.395±0.01 49.643±0.02 37.976±0.04 54.182±0.01 74.401±0.02 60.262±0.04 68.850±0.08 25.102±0.03 35.674±0.17 67.126±1e-3 55.847±0.01 66.052±0.05 106.123±3e-3 67.082±0.04 93.170±0.04 61.310±0.02 76.854±0.03 75.853±0.02 106.580±0.06 66.009±0.02 67.845±0.04 163.193±0.04 60.848±0.04 64.743±0.09 62.791±0.02 64.724±0.05 58.409±0.02 67.471±0.01 8.750±0.06 34.503±0.05 17.887±0.04 54.707±0.07 63.355±0.04\n\nand make the train-test split based on the t-SNE embedding (see the 2nd row in Table 2). We use the records inside the black rectangles for training (containing ∼70% data) and the rest for test. Notably, the rectangles are biasedly placed and the records for training and test do not overlap in the t-SNE embedding, so that they are clearly in different distributions. The performances reported in Table 2 show that our TABCAPS obtains best or second best performances among deep learning approaches on most of datasets. Such results imply that our method can learn robust features and generalize well. Besides, our TABCAPS exhibits considerably better generalization performances than the Vector CapsNet with very clear margins (e.g., TABCAPS obtains 8.750 × 10−2 on Gas dataset, while the Vector CapsNet obtains 163.193 × 10−2).\n\n4.4 PERFORMANCE STABILITY DURING TRAINING\n\nMany previous works required the early stop (Yao et al., 2007) to avoid over-fitting because they managed to exploit the potential of each tabular feature, and thus might be impacted by the uninformative features. However, our TABCAPS does not seem to require such training trick since it learns the data-level semantics directly. To show this, we compare the real-time validation performances (on log-loss values) during traning, among several regularization strategies (e.g., FCNN with lasso, mixup (Zhang et al., 2018)), commonly used neural networks (e.g., TabNet with sparse regularization, NODE and Net-DNF with ensemble learning strategies), and traditional boosting methods (XGboost and CatBoost), on the Click dataset. As shown in Fig. 3, most of these methods obtain their lowest log-loss values on the validation set after few training steps, and then their models gradually collapse as the training step increases. Notably, during the training process, all the corresponding loss values on training data keeps on decreasing or slightly fluctuating. One can see that the lasso and mixup\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Ablation study results. For the missing value imputation (MVI) test, we report the performances of a XGBoost model trained on training sets with filled values. The log-loss values are 100×. Method\n\nPerformances\n\nRouting & projection\n\nClassification\n\nfps\n\nBoW Routing & sparse projection\n\n(1) 12.043±0.03 (2) dynamic routing & sparse projection 14.543±0.03 (3) BoW Routing & simple projection 13.166±0.03\n\n1666 1342 1683\n\nRouting\n\nDecoder\n\nMVI (log-loss (100×))\n\n(3) BoW Routing (4) BoW Routing (5) Auto-Encoder with a classifier\n\nOurs fully connection\n\nCapsule Layers\n\n(6) (7) (8)\n\n(9) (10)\n\n(11) (12)\n\n2 3\n4\n\nOthers\n\nw/ Tabular Feature Extraction w/o Tabular Feature Extraction\n\nS.C. ensemble single S.C.\n\n22.531±0.01 26.338±0.07 25.765±6e-3\n\nClassification\n\n12.043±0.03 6.254±0.04 11.138±0.03\n\nClassification\n\n12.043±0.03 12.684±0.02\n\n12.043±0.03 12.515±0.05\n\nFigure 3: Real-time validation performances (logloss, the lower the better) on Click dataset. A “step” (on the x-axis) indicates an “epoch” for neural networks and is a “boosting step” for XGboost and CatBoost.\n\nregularization can alleviate this issues to some extent, but the validation log-loss values still increase slowly. Different from these methods, our TABCAPS with and without the decoder performs evidently stable, stabilizing the log-loss values at a low level (around 0.62). This finding reveals an advantage of using vectorial features as operating units and suggests that our method can facilitate the users to obtain a well-performed model without intensely monitoring the performances on the validation sets or agonizing over the use of training tricks (e.g., regularization methods or early stop).\n\n4.5 EFFECTS OF BOW ROUTING\n\nHere we further inspect how our proposed BoW Routing determines the transformation paths for the vectorial features from primary capsules to senior capsules. We compute the averaged transformation weights (wj|i in Eq.(6)) over the whole test set and visualize them as in Fig. 4. One can see that a senior capsule features are yielded by merging limited vectorial feature votes from primary capsules. Besides, we found the weights are sparse and the values are similar to different samples (with the standard deviation values varying from 0.001 to 0.003), which suggests that primary capsules model some semantically meaningful data patterns and our BoW Routing succeeds in finding out clear decision paths for classification.\n\n4.6 ABLATION STUDIES\n\nWe conduct ablation studies to examine the effects of the routing algorithms and the decoder (in a missing value imputation test) on the Diabetes dataset. We compare our BoW Routing with the dynamic routing (Sabour et al., 2017) on our architecture. See rows (1), (2), and (3) in Table 3. It is obvious that TABCAPS with our proposed BoW Routing outperforms that with the dynamic routing (Sabour et al., 2017) on the Diabetes data classification task by around 2.5 × 10−2 on log-loss. We also compute the fps of the BoW Routing and dynamic routing, and find that BoW Routing is nearly 25% faster than the dynamic routing, suggesting the superiority of our straightforwardness design. Compared with the simple projection (without entmax), our sparse projection is witnessed to be beneficial and is compatible with the BoW Routing, which shows that the use of entmax benefits in finding informative features and excludes the useless ones.\n\nTo inspect the capability of our proposed decoder, we conduct a test on missing value imputation, and employ an XGboost classifier to evaluate the effectiveness. We randomly mask out 20% of the feature values of the Diabetes training data, and simply fill the missing values with the means of feature values within the same classes. We train three models on the training set: (i) our TABCAPS with the proposed decoder (using the loss L in Eq. (8) and MSE), (ii) our TABCAPS with the fullyconnection-based decoder of the Vector CapsNet (Sabour et al., 2017) (using L in Eq. (8) and MSE), and (iii) an auto-encoder with an auxiliary classifier that takes the hidden states as input (using MSE and cross-entropy loss). After training, we take the reconstructed feature values provided by these 3 decoders to refill the missing values respectively, and obtain 3 different training sets. To\n\n8\n\n50100150200250300#. Training Steps0.60.81.01.21.41.61.8Real-Time Validation Log-loss ValuesTabCaps. w/o decoderTabCaps. w/ decoderTabNetNODENet-DNFFCNNFCNN w/ mixupFCNN w/ lassoXGBoostCatBoostPublished as a conference paper at ICLR 2023\n\nFigure 4: The averaged transformation weights determined by BoW Routing. The x-axis represents indices of primary capsules while the y-axis for senior capsules. The counts of primary capsules and senior capsules on different datasets are determined by the hyperparameter tuning.\n\nfairly compare the qualities of the reconstructed values, we train 3 XGboost models with identical hyper-parameters on these 3 training sets, and report their performances on the original test set. A better performance indicates that the reconstructed values are more trusty.\n\nAs reported in Table 3, XGboost that is trained on the training set provided by our decoder is markedly superior to the other two methods, while the fully-connection-based decoder (Sabour et al., 2017) performs the worst. Notably, we train another XGboost on the training set in which the masked values remain missing, and find that this XGboost version still obtains 26.174 on the test set. Thus, one can see that our proposed decoder is able to yield significant performance gain in filling the missing values, while the fully-connection-based decoder predicts noisy values.\n\nThe effects of capsule layers were tested. See rows (6), (7), and (8). The 3-layer and 4-layer TABCAPS obtain better performances on the Diabetes dataset compared with the default 2-layer model. On the Heart dataset, similar conclusion is attained that TABCAPS with 2, 3, 4 capsule layers obtained log-loss values of 0.34047, 0.33992, and 0.33673, respectively. In general, we consider TABCAPS with different capsule layers perform competitive, and a 2-capsule-layer model is sufficient to attain a good performance. We also inspected the effects of the “tabular feature extraction” module at the bottom of TABCAPS (see Fig. 1), and the effects of the ensemble of the senior capsules. Comparing row (9) with (10), and row (11) with (12), it is evident that both of the tabular feature extraction and the senior capsule ensemble are beneficial, but are not the core reasons of the superior performances of TABCAPS. We consider it is the novel architecture makes TABCAPS perform well.\n\n5 CONCLUSIONS AND FUTURE WORK\n\nThis paper proposed a novel and simple paradigm for supervised tabular learning, which encapsulated all features of a record into vectorial features as operating units and thus avoids interactions among heterogeneous tabular features. Consequently, our proposed new CapsNet (called TABCAPS), with a new primary capsule synthesis approach, a novel sparse projection, and a novel BoW Routing, coped with vectorial features and obtained superior performances on tabular data classification tasks at low computational costs. Specifically, the proposed BoW Routing performs straightforwardly without iterative clustering processes of previous CapsNets. This work is a beneficial attempt in the research areas of capsule neural networks, routing algorithms, and supervised tabular learning. Following previous works, our proposed method only considered classification tasks, and it would be interesting to explore how to use TABCAPS for tabular data regression. Moreover, it is also interesting to design other approach for directly learning data-level semantics without feature-wise interactions.\n\nAcknowledgements. This research was partially supported by the National Key R&D Program of China under grant No. 2018AAA0102102 and National Natural Science Foundation of China under grants No. 62132017.\n\nReproducibility Statement. To ensure the reproducibility, we include the settings of hyperparameter tuning for various methods in Appendix A.1. The classification performances measured by “accuracy” and “ROC-AUC” are reported in Appendix A.2 for reference. Besides, an additional discussion between our proposed “multivariate Gaussian embedding” and “linear feature embedding” approaches are given in Appendix A.3.\n\n9\n\nGasClickHeartDiabetesEEGHillPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nAmi Abutbul, Gal Elidan, et al. Net-DNF: A neural architecture for tabular data. In ICLR, 2021.\n\nPeter Martey Addo, Dominique Guegan, et al. Credit risk analysis using machine and deep learning\n\nmodels. Risks, 2018.\n\nKarim Ahmed and Lorenzo Torresani. STAR-Caps: Capsule networks with straight-through attentive\n\nrouting. In NeurIPS, 2019.\n\nSercan O Arik and Tomas Pfister. TabNet: Attentive interpretable tabular learning. In AAAI, 2020.\n\nPierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy\n\nphysics with deep learning. Nature Communications, 2014.\n\nLeo Breiman, Jerome H Friedman, et al. Classification and Regression Trees. CRC press, 1984.\n\nPASCAL Challenge. Epsilon Dataset, 2008. https://www.csie.ntu.edu.tw/ ̃cjlin/\n\nlibsvmtools/datasets/binary.html#epsilon.\n\nJintai Chen, Hongyun Yu, et al. A receptor skeleton for capsule neural networks. In ICML, 2021.\n\nJintai Chen, Kuanlun Liao, et al. DANets: Deep abstract networks for tabular data classification and\n\nregression. In AAAI, 2022.\n\nTianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In KDD, 2016.\n\nDavide Chicco and Giuseppe Jurman. Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making, 2020.\n\nJaewoong Choi, Hyun Seo, et al. Attention routing between capsules. In CVPR Workshops, 2019.\n\nKDD Cup. Predict the click-through rate of ads given the query and user information, 2012.\n\nhttps://www.kaggle.com/c/kddcup2012-track2/.\n\nDheeru Dua and Casey Graff. EEG eye state dataset, 2015. https://www.openml.org/d/\n\n1471.\n\nYury Gorishniy, Ivan Rubachev, et al. Revisiting deep learning models for tabular data. NeurIPS,\n\n2021.\n\nLeo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform\n\ndeep learning on typical tabular data? In ICLR, 2022.\n\nHuifeng Guo, Ruiming Tang, et al. DeepFM: A factorization-machine based neural network for CTR\n\nprediction. In IJCAI, 2017.\n\nMd Rafiul Hassan, Sadiq Al-Insaif, et al. A machine learning approach for prediction of pregnancy\n\noutcome following IVF treatment. Neural Computing and Applications, 2020.\n\nGeoffrey E Hinton, Sara Sabour, et al. Matrix capsules with EM routing. In ICLR, 2018.\n\nGuolin Ke, Qi Meng, et al. LightGBM: A highly efficient gradient boosting decision tree. In NeurIPS,\n\n2017.\n\nAR Kosiorek, S Sabour, et al. Stacked capsule autoencoders. In NeurIPS, 2019.\n\nGraham Lee and Oppacher Franz. Hill-valley data set, 2008. http://archive.ics.uci.\n\nedu/ml/datasets/hill-valley.\n\nChao Li, Zhiyuan Liu, Mengmeng Wu, et al. Multi-interest network with dynamic routing for\n\nrecommendation at Tmall. In CIKM, 2019.\n\nJerry Ma and Denis Yarats. Quasi-hyperbolic momentum and Adam for deep learning. In ICLR,\n\n2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted Boltzmann machines. In\n\nICML, 2010.\n\nAndrew Y Ng. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML, 2004.\n\nBen Peters, Vlad Niculae, et al. Sparse sequence-to-sequence models. In ACL, 2019.\n\nGibran Fuentes Pineda, Hisashi Koga, et al. Scalable object discovery: A hash-based approach to clustering co-occurring visual words. IEICE Transactions on Information and Systems, 2011.\n\nSergei Popov, Stanislav Morozov, et al. Neural oblivious decision ensembles for deep learning on\n\ntabular data. In ICLR, 2019.\n\nLiudmila Prokhorenkova, Gleb Gusev, et al. CatBoost: Unbiased boosting with categorical features.\n\nIn NeurIPS, 2018.\n\nJ Ross Quinlan. Discovering rules by induction from large collections of examples. Expert Systems\n\nin the Micro Electronics Age, 1979.\n\nJ Ross Quinlan. C4.5: Programs for Machine Learning. Elsevier, 2014.\n\nJathushan Rajasegaran, Vinoj Jayasundara, et al. DeepCaps: Going deeper with capsule networks. In\n\nCVPR, 2019.\n\nDG Reina, SL Toral, et al. Improving discovery phase of reactive ad hoc routing protocols using\n\nJaccard distance. The Journal of Supercomputing, 2014.\n\nFabio De Sousa Ribeiro, Georgios Leontidis, and Stefanos D Kollias. Introducing routing uncertainty\n\nin capsule networks. In NeurIPS, 2020.\n\nFabio De Sousa Ribeiro, Kevin Duarte, Miles Everett, Georgios Leontidis, and Mubarak Shah.\n\nLearning with capsules: A survey. arXiv preprint arXiv:2206.02664, 2022.\n\nSara Sabour, Nicholas Frosst, et al. Dynamic routing between capsules. In NeurIPS, 2017.\n\nSara Sabour, Andrea Tagliasacchi, et al. Unsupervised part representation by flow capsules. In ICML,\n\n2021.\n\nGerard Salton and Michael E Lesk. The SMART automatic document retrieval systems–An illustra-\n\ntion. Communications of the ACM, 1965.\n\nYao-Hung Hubert Tsai, Nitish Srivastava, et al. Capsules with inverted dot-product attention routing.\n\nIn ICLR, 2019.\n\nVikas Ukani.\n\nDiabetes data set, 2020.\n\nhttps://www.kaggle.com/vikasukani/\n\ndiabetes-data-set.\n\nAlexander Vergara, Shankar Vembu, et al. Chemical gas sensor drift compensation using classifier\n\nensembles. Sensors and Actuators B: Chemical, 2012.\n\nRuoxi Wang, Bin Fu, et al. Deep & cross network for ad click predictions. In ADKDD, 2017.\n\nJiahuan Yan, Jintai Chen, Yixuan Wu, Danny Z Chen, and Jian Wu. T2G-Former: Organizing tabular\n\nfeatures into relation graphs promotes heterogeneous feature interaction. In AAAI, 2023.\n\nYuan Yao, Lorenzo Rosasco, et al. On early stopping in gradient descent learning. Constructive\n\nApproximation, 2007.\n\nHongyi Zhang, Moustapha Cisse, et al. Mixup: Beyond empirical risk minimization. In ICLR, 2018.\n\nWei Zhao, Haiyun Peng, et al. Towards scalable and reliable capsule networks for challenging NLP\n\napplications. In ACL, 2019a.\n\nYongheng Zhao, Tolga Birdal, et al. 3D point capsule networks. In CVPR, 2019b.\n\nYongheng Zhao, Tolga Birdal, et al. Quaternion equivariant capsule networks for 3D point clouds. In\n\nECCV, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 HYPERPARAMETER-TUNING SETTINGS\n\nAs mentioned in the main paper, some hyperparameters of methods were turned for datasets by grid search or the HyperOpt library2 with 50 steps of the Tree-structured Parzen Estimator (TPE), similar to the configurations in (Popov et al., 2019). The details are listed in the subsections below. As for XGBoost, CatBoost, FCNN, TabNet, DANets, Vector CapsNet, and our TABCAPS, we used the HyperOpt hyperparameter search library; as for NODE and Net-DNF, we used grid search (following the settings in their papers). For FT-Transformer (Gorishniy et al., 2021), we used their official open source.\n\nThe hyperparameter search spaces and search algorithms of XGBoost (Chen & Guestrin, 2016), CatBoost (Prokhorenkova et al., 2018), NODE (Popov et al., 2019), and FCNN (Nair & Hinton, 2010) were set identical as in (Popov et al., 2019). We set the maximal number of the trees in XGBoost and CatBoost up to 4096 (instead of 2048 used in NODE (Popov et al., 2019)), since we found that 2048 is not enough for XGBoost and CatBoost to obtain good performances. We also set the maximal layer number in FCNN up to 9. Besides, the hyperparameter search settings of Net-DNF (Abutbul et al., 2021) following its original paper. The architectures of FCNN with lasso regularization or mixup were constructed following the FCNN in (Popov et al., 2019). For both of DANet-24 (Chen et al., 2022) and TabNet (Arik & Pfister, 2020), we set the hyperparameter search spaces to contain all the possible values provided by the main papers.\n\nA.1.1 XGBOOST\n\nFor easy re-implementation, the hyperparameter variable names followed the sklearn package3 settings.\n\n• eta (learning rate): Log-Uniform distribution [e−7, 1];\n\n• max depth: Discrete uniform distribution [2, 10];\n\n• subsample: Uniform distribution [0.5, 1];\n\n• colsample bytree: Uniform distribution [0.5, 1];\n\n• colsample bylevel: Uniform distribution [0.5, 1];\n\n• min child weight: Uniform distribution [e−16, e5];\n\n• alpha: Uniform choice {0, Log-Uniform distribution [e−16, e2]};\n\n• lambda: Uniform choice {0, Log-Uniform distribution [e−16, e2]};\n\n• gamma: Uniform choice {0, Log-Uniform distribution [e−16, e2]}.\n\nA.1.2 CATBOOST\n\nFor easy re-implementation, the hyperparameter variable names followed the sklearn package settings.\n\n• learning rate: Log-Uniform distribution [e−5, 1];\n\n• random strength: Discrete uniform distribution [1, 20];\n\n• one hot max size: Discrete uniform distribution [0, 25];\n\n• l2 leaf reg: Log-Uniform distribution [1, 10];\n\n• bagging temperature: Uniform distribution [0, 1];\n\n• leaf estimation iterations: Discrete uniform distribution [1, 10].\n\n2https://github.com/hyperopt/hyperopt 3https://scikit-learn.org/\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA.1.3 NODE\n\n• number of layers: {2, 4, 8};\n\n• total number of trees: {1024, 2048};\n\n• tree depth: {6, 8};\n\n• output dimension of trees: {2, 3};\n\n• learning rate: 10−3.\n\nA.1.4 NET-DNF\n\nFollowing the original setting in (Abutbul et al., 2021), the learning rate is initialized as 0.05 and is reduced by monitoring the training loss (check its official codes for details).\n\n• number of formulas: {64, 128, 256, 512, 1024, 2048, 3072};\n\n• feature selection beta: {1.6, 1.3, 1.0, 0.7, 0.4, 0.1}.\n\nA.1.5 TABNET\n\n• λsparse: {0.0, 10−6, 10−4, 10−3, 10−2, 0.1};\n\n• γ: {1.0, 1.2, 1.5, 2.0};\n\n• batch size B: {256, 512, 1024, 2048, 4096, 8192, 16384, 32768};\n\n• Nd and Na: {8, 16, 24, 32, 64, 128};\n\n• Nsteps: {3, 4, 5, 6, 7, 8, 9, 10};\n\n• BV : {256, 512, 1024, 2048, 4096};\n\n• mB: {0.6, 0.7, 0.8, 0.9, 0.95, 0.98};\n\n• learning rate: {0.002, 0.01, 0.02, 0.025}.\n\nA.1.6 FCNN\n\n• number of layers: Discrete uniform distribution [2, 9];\n\n• output dimensions of full-connections: Discrete uniform distribution {128, 256, 512, 1024};\n\n• learning rate: Uniform distribution [10−4, 10−2];\n\n• dropout rate: Uniform distribution [0, 0.5];\n\n• lasso coefficient (optional): Discrete uniform distribution uniform [10−7, 10−4];\n\n• learning rate: Uniform distribution [10−4, 0.01].\n\nA.1.7 FT-TRANSFORMER\n\n• the number of layers: Discrete uniform distribution [1, 4];\n\n• feature embedding size: Discrete uniform distribution [64, 512];\n\n• residual dropout: Uniform distribution [0, 0.2];\n\n• attention dropout: Uniform distribution [0, 0.5];\n\n• FFN dropout: Uniform distribution [0, 0.5];\n\n• FFN factor: Uniform distribution [ 2\n\n3 , 8\n\n3 ];\n\n• learning rate: Log-Uniform distribution [10−5, 10−3];\n\n• weight decay: Log-Uniform distribution [10−6, 10−3];\n\n• the number of iterations: 100.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA.1.8 DANET-24\n\nThe learning rate of DANet-24 is initially set 0.008 and is decayed by 5% in every 20 epochs.\n\n• k0: {5, 8}; • d0: {32, 48}; • d1: {64, 96}.\n\nA.1.9 VECTOR CAPSNET\n\n• learning rate: Uniform distribution [0.02, 0.1];\n\n• primary capsule size: Discrete uniform distribution [64, 128];\n\n• the number of primary capsules: Discrete uniform distribution [4, 32];\n\n• senior capsule size: Discrete uniform distribution [4, 32].\n\nA.1.10 TABCAPS\n\n• learning rate: Uniform distribution [0.02, 0.1];\n\n• the number of senior capsules for a class: Discrete uniform distribution [1, 5];\n\n• primary capsule size: Discrete uniform distribution [64, 128];\n\n• the number of primary capsules: Discrete uniform distribution [4, 32];\n\n• senior capsule size: Discrete uniform distribution [4, 32];\n\n• the number of learnable “words”: Discrete uniform distribution [16, 64].\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA.2 CLASSIFICATION PERFORMANCES ON “ACCURACY” AND “ROC-AUC”\n\nHere we report the model performances measured by the “accuracy” in Table 4 and the binary classification performances measured by “ROC-AUC” in Table 5. These classification performances are obtained in 5 runs with regard to random seeds. One can see that our TABCAPS obtains better or competitive performances in comparison with previous works.\n\nTable 4: Classification Performances measured by “accuracy”.\n\nMethod\n\nClick Diabetes EEG Gas Heart Hill\n\nHig Epsilon\n\nXGboost Catboost\n\n0.6666 0.9575 0.9342 0.9957 0.8833 0.5761 0.7262 0.8884 0.6599 0.9250 0.9322 0.9935 0.8501 0.4856 0.7256 0.8878\n\n0.6684 0.9725 0.8815 0.9949 0.8166 0.9012 0.7190 0.8890 TabNet Net-DNF 0.6694 0.9775 0.9379 0.9942 0.8333 0.7819 0.7268 0.8922 0.6688 0.7903 0.7871 0.8606 0.8667 0.6214 0.7260 0.8952 NODE FT-Transformer 0.6606 0.9725 0.9826 0.9953 0.9166 0.8971 0.7290 0.8976 0.6698 0.9687 0.9776 0.9949 0.8333 0.9095 0.7284 0.8962 DANet-24 0.6674 0.9375 0.9746 0.9956 0.7333 0.9136 0.7075 0.8934 FCNN mixup 0.6670 0.9403 0.9639 0.9863 0.6677 0.8949 0.6677 0.8958 FCNN lasso Vector CapsNet 0.6678 0.9025 0.9726 0.9939 0.6833 0.6419 0.7102 0.8905 TABCAPS (Ours) 0.6685 0.9725 0.9776 0.9968 0.9023 0.9012 0.7300 0.8975\n\nTable 5: Binary classification Performances measured by “ROC-AUC”. The Gas dataset is not included because it is not for binary classification.\n\nMethod\n\nClick Diabetes EEG Heart Hill\n\nHig Epsilon\n\nXGboost Catboost\n\n0.7216 0.9859 0.9884 0.9447 0.5939 0.8054 0.9542 0.7199 0.9878 0.9859 0.9330 0.4965 0.8048 0.9524\n\n0.7122 0.9853 0.9535 0.9095 0.9632 0.7911 0.9562 TabNet 0.7169 0.9932 0.9838 0.9412 0.8707 0.8018 0.9532 Net-DNF 0.7146 0.8411 0.8173 0.9506 0.6626 0.7444 0.9497 NODE 0.7180 0.9883 0.9978 0.9764 0.9833 0.8078 0.9586 FT-Transformer 0.7189 0.9825 0.9981 0.8931 0.9710 0.8042 0.9582 DANet-24 0.7178 0.9653 0.9962 0.8978 0.9531 0.7844 0.9598 FCNN mixup 0.7128 0.9702 0.9958 0.8343 0.5809 0.7247 0.9323 FCNN lasso Vector CapsNet 0.7168 0.9902 0.9965 0.9036 0.5724 0.7754 0.9545 TABCAPS (Ours) 0.7199 0.9907 0.9970 0.9436 0.9911 0.8081 0.9606\n\nA.3 PERFORMANCES WITH OTHER EMBEDDING APPROACHES\n\nIn this paper, we devise multivariate Gaussian kernels to synthesize vectorial features. Here we compare our proposed multivariate Gaussian embedding approach with the commonly used linear feature embedding approach (like the embedding approach in FT-Transformer). On the Diabetes dataset, TABCAPS with the linear embedding layer achieves 0.1328 on log-loss (↓) while TABCAPS with multivariate Gaussian kernels achieves 0.1204. The comparison results suggest that our multivariate Gaussian kernels is beneficial, which might be due to the non-linearity of the multivariate Gaussian embedding approach.\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper proposed TabCaps, to process the multiple features into one vectorized unit, avoiding the interaction among tabular features.  In TabCaps, a tabular instance is respectively encoded into several vectorial features by some optimizable multivariate Gaussian kernels in the primary capsule layer, where each vectorial feature represents a specific \"profile\" of the input instance and is transformed into senior capsule layer under the guidance of a novel straightforward routing algorithm. Rather than clustering, an efficient bag-of-words is used in routing.\n\n# Strength And Weaknesses\n\nStrengths\n\n- The paper is well written. \n- The experiment setting is comprehensive.\n- The idea is interesting, and the experiment results demonstrate looks good.\n\nWeaknesses  & Questions\n\n1. Current method seems cannot apply to regression tasks.\n2. The experiment result seems not very significant, especially the accuracy comparison from Table 4.\n3. Can you explicitly show #classes of different tasks in the experiments? And I would like to see the AUC metric for the binary classification, rather than log-loss. Besides, how do you use XGBoost for multi-class (K > 2) tasks?\n4. Can we replace the \"Multivariate Gaussian Kernels\" with FFN? If it can, how about the performance? if not, why?\n5. In the widely used boosting tree based methods, there are many interactions among tabular features. It surprised me that non-feature-interaction is better in neural networks. Can authors explain more about that?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper is well-written and easy to follow.\n\nQuality: The proposed method technologically sounds good, and the experiment design is solid.\n\nNovelty: I think it is the first work to use Capsule Neural Network in tabular data.\n\nReproducibility: The details in the appendix help to reproduce, but it would be better if the authors can release the source code.\n\n# Summary Of The Review\n\nOverall, I lean to accept the paper. Although the experiment result is not very significant, I think this is a good paper, since it may inspire future research in neural networks for tabular data.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nON THE ROBUSTNESS OF RANDOMIZED ENSEMBLES TO ADVERSARIAL PERTURBATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRandomized ensemble classifiers (RECs), where one classifier is randomly selected during inference, have emerged as an attractive alternative to traditional ensembling methods for realizing adversarially robust classifiers with limited compute requirements. However, recent works have shown that existing methods for constructing RECs are more vulnerable than initially claimed, casting major doubts on their efficacy and prompting fundamental questions such as: “When are RECs useful?”, “What are their limits?”, and “How do we train them?”. In this work, we first demystify RECs as we derive fundamental results regarding their theoretical limits, necessary and sufficient conditions for them to be useful, and more. Leveraging this new understanding, we propose a new boosting algorithm (BARRE) for training robust RECs, and empirically demonstrate its effectiveness at defending against strong l∞ norm-bounded adversaries across various network architectures and datasets. Our code is submitted as part of the supplementary material, and will be publicly released on GitHub.\n\n1\n\nINTRODUCTION\n\nDefending deep networks against adversarial perturbations (Szegedy et al., 2013; Biggio et al., 2013; Goodfellow et al., 2014) remains a difficult task. Several proposed defenses (Papernot et al., 2016; Pang et al., 2019; Yang et al., 2019; Sen et al., 2019; Pinot et al., 2020) have been subsequently “broken” by stronger adversaries (Carlini & Wagner, 2017; Athalye et al., 2018; Tram`er et al., 2020; Dbouk & Shanbhag, 2022), whereas strong defenses (Cisse et al., 2017; Tram`er et al., 2018; Cohen et al., 2019), such as adversarial training (AT) (Goodfellow et al., 2014; Zhang et al., 2019; Madry et al., 2018), achieve unsatisfactory levels of robustness1.\n\nA popular belief in the adversarial community is that single model defenses, e.g., AT, lack the capacity to defend against all possible perturbations, and that constructing an ensemble of diverse, often smaller, models should be more cost-effective (Pang et al., 2019; Kariyappa & Qureshi, 2019; Pinot et al., 2020; Yang et al., 2020b; 2021; Abernethy et al., 2021; Zhang et al., 2022). Indeed, recent deterministic robust ensemble methods, such as MRBoost (Zhang et al., 2022), have been successful at achieving higher robustness compared to AT baselines using the same network architecture, at the expense of 4× more compute (see Fig. 1). In fact, Fig 1 indicates that one can simply adversarially training larger deep nets that can match the robustness and compute requirements of MRBoost models, rendering state-of-the-art boosting techniques obsolete for designing classifiers that are both robust and efficient.\n\nIn contrast, randomized ensembles, where one classifier is randomly selected during inference, offer a unique way of ensembling that can operate with limited compute resources. However, the recent work of Dbouk & Shanbhag (2022) has cast major concerns regarding their efficacy, as they successfully compromised the state-of-the-art randomized defense of Pinot et al. (2020) by large margins using their proposed ARC adversary. Furthermore, there is an apparent lack of proper theory on the robustness of randomized ensembles, as fundamental questions such as: “when does randomization help?” or “how to find the optimal sampling probability?” remain unanswered.\n\nContributions. In this work, we first provide a theoretical framework for analyzing the adversarial robustness of randomized ensmeble classifiers (RECs). Our theoretical results enable us to better\n\n1when compared to the high clean accuracy achieved in a non-adversarial setting\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The efficacy of employing randomized ensembles (⋆) for achieving robust and efficient inference compared to AT (•) and deterministic ensembling MRBoost (♦) on CIFAR-10. Robustness is measured using the standard l∞ norm-bounded adversary with radius ε = 8/255.\n\nunderstand randomized ensembles, revealing interesting and useful answers regarding their limits, necessary and sufficient conditions for them to be useful, and efficient methods for finding the optimal sampling probability. Next, guided by our threoretical results, we propose BARRE, a new boosting algorithm for training robust randomized ensemble classifiers achieving state-of-the-art robustness. We validate the effectiveness of BARRE via comprehensive experiments across multiple network architectures and datasets, thereby demonstrating that RECs can achieve similar robustness to AT and MRBoost, at a fraction of the computational cost (see Fig. 1).\n\n2 BACKGROUND AND RELATED WORK\n\nAdversarial Robustness. Deep neural networks are known to be vulnerable to adversarial perturbations (Szegedy et al., 2013; Biggio et al., 2013). In an attempt to robustify deep nets, several defense methods have been proposed (Katz et al., 2017; Madry et al., 2018; Cisse et al., 2017; Zhang et al., 2019; Yang et al., 2020b; Zhang et al., 2022; Tjeng et al., 2018; Xiao et al., 2018; Raghunathan et al., 2018; Yang et al., 2020a). While some heuristic-based empirical defenses have later been broken by better adversaries (Carlini & Wagner, 2017; Athalye et al., 2018; Tram`er et al., 2020), strong defenses, such as adversarial training (AT) (Goodfellow et al., 2014; Madry et al., 2018; Zhang et al., 2019), remain unbroken but achieve unsatisfactory levels of robustness.\n\nEnsemble Defenses. Building on the massive success of classic ensemble methods in machine learning (Breiman, 1996; Freund & Schapire, 1997; Dietterich, 2000), robust ensemble methods (Kariyappa & Qureshi, 2019; Pang et al., 2019; Sen et al., 2019; Yang et al., 2020b; 2021; Abernethy et al., 2021; Zhang et al., 2022) have emerged as a natural solution to compensate for the unsatisfactory performance of existing single-model defenses, such as AT. Earlier works (Kariyappa & Qureshi, 2019; Pang et al., 2019; Sen et al., 2019) relied on heuristic-based techniques for inducing diversity within the ensembles, and have been subsequently shown to be weak (Tram`er et al., 2020; Athalye et al., 2018). Recent methods, such as RobBoost (Abernethy et al., 2021) and MRBoost (Zhang et al., 2022), formulate the design of robust ensembles from a margin boosting perspective, achieving state-of-the-art robustness for deterministic ensemble methods. This achievement comes at a massive (4 − 5×) increase in compute requirements, as each inference requires executing all members of the ensemble, deeming them unsuitable for safety-critical edge applications (Guo et al., 2020; Sehwag et al., 2020; Dbouk & Shanbhag, 2021). Randomized ensembles (Pinot et al., 2020), where one classifier is chosen randomly during inference, offer a more compute-efficient alternative. However, their ability to defend against strong adversaries remains unclear (Dbouk & Shanbhag, 2022; Zhang et al., 2022). In this work, we show that randomized ensemble classifiers can be effective at defending against adversarial perturbations, and propose a boosting algorithm for training such ensembles, thereby achieving high levels of robustness with limited compute requirements.\n\n2\n\nmethods:networks:goal4.5% robust accuracy4less FLOPsUnder review as a conference paper at ICLR 2023\n\nRandomized Defenses. A randomized defense, where the defender adopts a random strategy for classification, is intuitive: if the defender does not know what is the exact policy used for a certain input, then one expects that the adversary will struggle on average to fool such a defense. Theoretically, Bayesian Neural Nets (BNNs) (Neal, 2012) have been shown to be robust (in the large data limit) to gradient-based attacks (Carbone et al., 2020), whereas Pinot et al. (2020) has shown that a randomized ensemble classifier (REC) with higher robustness exists for every deterministic classifier. However, realizing strong and practical randomized defenses remains elusive as BNNs are too computationally prohibitive and existing methods (Xie et al., 2018; Dhillon et al., 2018; Yang et al., 2019) often end up being compromised by adaptive attacks (Athalye et al., 2018; Tram`er et al., 2020). Even BAT, the proposed method of Pinot et al. (2020) for robust RECs, was recently broken by Zhang et al. (2022); Dbouk & Shanbhag (2022). In contrast, our work first demystifies randomized ensembles as we derive fundamental results regarding the limit of RECs, necessary and sufficient conditions for them to be useful, and efficient methods for finding the optimal sampling probability. Empirically, our proposed boosting algorithm (BARRE) can successfully train robust RECs, achieving state-of-the-art robustness for RECs.\n\n3 PRELIMINARIES & PROBLEM SETUP\n\nNotation. Let F = {f1, ..., fM } be a collection of M arbitrary C-ary classifiers fi : Rd → [C]. A soft classifier, denoted by ̃f : Rd → RC, can be used to construct a hard classifier f (x) = arg maxc∈[C][ ̃f (x)]c, where [v]c = vc. We use the notation f (·|θ) to represent parametric classifiers where f is a fixed mapping and θ ∈ Θ represents the learnable parameters. Let ∆M = {v ∈ [0, 1]M : (cid:80) vi = 1} be the probability simplex of dimension M − 1. Given a probability vector α ∈ ∆M , we construct a randomized ensemble classifier (REC) fα such that fα(x) = fi(x) with probability αi. In contrast, traditional ensembling methods construct a deterministic ensemble classifier (DEC) using the soft classifiers as follows2: ̄f (x) = arg maxc∈[C][(cid:80)M ̃fi(x)]c. Denote z = (x, y) ∈ Rd ×[C] as a feature-label pair that follows some unknown distribution D. Let S ⊂ Rd be a closed and bounded set representing the attacker’s perturbation set. A typical choice of S in the adversarial community is the lp ball of radius ε: Bp(ε) = {δ ∈ Rd : ∥δ∥p ≤ ε}. For a classifier fi ∈ F and data-point z = (x, y), define Si(z) = {δ ∈ S : fi(x + δ) ̸= y} to be the set of valid adversarial perturbations to fi at z. Definition 1. For any (potentially random) classifier f , define the adversarial risk η: (cid:20)\n\ni=1\n\n(cid:21)\n\nη(f ) = Ez∼D\n\nEf [1 {f (x + δ) ̸= y}]\n\nmax δ∈S\n\n(1)\n\nThe adversarial risk measures the robustness of f on average in the presence of an adversary (attacker) restricted to the set S. For the special case of S = {0}, the adversarial risk reduces to the standard risk of f :\n\nη0(f ) = Ez∼D [Ef [1 {f (x) ̸= y}]] = P {f (x) ̸= y} The more commonly reported robust accuracy of f , i.e., accuracy against adversarially perturbed inputs, can be directly computed from η(f ). The same can be said for the clean accuracy and η0(f ).\n\n(2)\n\nWhen working with an REC fα, the adversarial risk can be expressed as:\n\nη(fα) ≡ η(α) = Ez∼D\n\n(cid:34)\n\nmax δ∈S\n\nM (cid:88)\n\ni=1\n\nαi1 {fi(x + δ) ̸= y}\n\n(3)\n\n(cid:35)\n\nwhere we use the notation η(α) whenever the collection F is fixed. Let {ei}M standard basis vectors of RM , then we employ the notation η(fi) = η(fei) ≡ η(ei) = ηi.\n\ni=1 ⊂ {0, 1}M be the\n\n4 THE ADVERSARIAL RISK OF A RANDOMIZED ENSEMBLE CLASSIFIER\n\nIn this section, we develop our main theoretical findings regarding the adversarial robustness of any randomized ensemble classifier. Detailed proofs of all statements and theorems can be found in Appendix B.\n\n2the normalizing constant 1\n\nM does not affect the classifier output\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Illustration of the equivalence in (6) using an example of three classifiers in R2. The shaded areas represent regions in the attacker-restricted input space where each classifier makes an error. All classifiers correctly classify x. The set U uniquely captures the interaction between z and f1, f2, & f3 inside S.\n\n4.1 PROPERTIES OF η\n\nWe start with the following statement: i=1, perturbation set S ⊂ Rd, and data distribution D, the adverProposition 1. For any F = {fi}M sarial risk η is a piece-wise linear convex function ∀α ∈ ∆M . Specifically, ∃K ∈ N configurations Uk ⊆ {0, 1}M ∀k ∈ [K] and p.m.f. p ∈ ∆K such that:\n\nη(α) =\n\nK (cid:88)\n\n(cid:18)\n\nk=1\n\npk · max u∈Uk\n\n(cid:19)\n\n(cid:8)u⊤α(cid:9)\n\n(4)\n\nBefore we explain the intuition behind Proposition 1, we first make the following observations:\n\nGenerality. Proposition 1 makes no assumptions about the classifiers F, i.e., it applies even to the enigmatic deep nets. While the majority of theoretical results in the literature have been restricted to lp-bounded adversaries, Proposition 1 holds for any closed and bounded perturbation set S. This is crucial, as real-world attacks are often not restricted to lp balls around the input (Liu et al., 2018; Duan et al., 2020). This generality is further inherited by all of our results, as they build on Proposition 1.\n\nAnalytic Form. Proposition 1 allows us to re-write the adversarial risk in (3) using the analytic form in (4), which is much simpler to analyze and work with. In fact, the analytic form in (4) enables us to derive our main theoretical results in Sections 4.2 & 4.3, which include tight fundamental bounds on η.\n\nOptimal Sampling. The convexity of η implies that any local minimum α∗ is also a global minimum. The probability simplex is a closed convex set, thus a global minimum, which need not be unique, is always achievable. Since η is piece-wise linear, then there always exists a finite set of candidate solutions for α∗. For M ≤ 3, we efficiently enumerate all candidates in Section 4.2, eliminating the need for any sophisticated search method. For larger M however, enumeration becomes intractable. In Section 4.4, we construct an optimal algorithm for finding α∗ by leveraging the classic sub-gradient method (Shor, 2012) for optimizing sub-differentiable functions. Intuition. Consider a data-point z ∈ Rd × [C], then for any δ ∈ S and α ∈ ∆M we have the per sample risk:\n\nr (z, δ, α) =\n\nM (cid:88)\n\ni=1\n\nαi1 {fi(x + δ) ̸= y} = u⊤α\n\n(5)\n\nwhere u ∈ {0, 1}M such that ui = 1 if and only if δ is adversarial to fi at z. Since u is independent of α, we thus obtain a many-to-one mapping from δ ∈ S to u ∈ {0, 1}M . Therefore, for any α and z, we can always decompose the perturbation set S, i.e., S = G1 ∪ ... ∪ Gn, into n ≤ 2M subsets, such that: ∀δ ∈ Gj : r (z, δ, α) = α⊤uj for some binary vector uj independent of α. Let U = {uj}n\n\nj=1 be the collection of these vectors, then we can write:\n\nmax δ∈S\n\nr (z, δ, α) = max\n\nδ∈G1∪...∪Gn\n\nr (z, δ, α) = max j∈[n]\n\nmax δ∈Gj\n\nr (z, δ, α)\n\n= max u∈U\n\n(cid:26)\n\n(cid:27)\n\n(cid:8)u⊤α(cid:9)\n\n(6)\n\n4\n\nperturbation set decomposition captured by configuration shaded regions are misclassified Under review as a conference paper at ICLR 2023\n\nFigure 3: Enumeration of all K = 5 unique configurations with two classifiers and a data-point around a set S. Note that since αi ≥ 0 ∀i, the 0 vector is redundant in Uk for k ∈ [4], which explains why K = 5 and not more.\n\nThe main idea behind the equivalence in (6) is that we can represent any configuration of classifiers, data-point and perturbation set using a unique set of binary vectors U. For example, Fig. 2 pictorially depicts this equivalence using a case of M = 3 classifiers in R2 with S = B2(ε). This equivalence is the key behind Proposition 1, since the point-wise max term in (6) is piece-wise linear and convex ∀α ∈ ∆M . Finally, Proposition 1 holds due to the pigeon-hole principle and the linearity of expectation.\n\n4.2 SPECIAL CASE OF TWO CLASSIFIERS\n\nWith two classifiers only, we can leverage the analytic form of η in (4) and enumerate all possible classifiers/data-point configurations around S by enumerating all configurations Uk ⊆ {0, 1}2. Specifically, Fig. 3 visualizes all K = 5 such unique configurations, which allows us to write ∀α ∈ ∆2:\n\nη(α) = p1 · max{α1, α2} + p2 · 1 + p3 · α1 + p4 · α2 + p5 · 0 (7) where p ∈ ∆5 is the p.m.f. of “binning” any data-point z into any of the five configurations, under the data distribution z ∼ D. Using (7), we obtain the following result:\n\nTheorem 1. For any two classifiers f1 and f2 with individual adversarial risks η1 and η2, respectively, subject to a perturbation set S ⊂ Rd and data distribution D, if:\n\nP {z ∈ R1} > |η1 − η2|\n\n(8)\n\nwhere:\n\nR1 = {z ∈ Rd × [C] : S1(z) ̸= ∅, S2(z) ̸= ∅, S1(z) ∩ S2(z) = ∅} (9) then the optimal sampling probability α∗ = [1/2 1/2]⊤ uniquely minimizes η(α) resulting in 2 (η1 + η2 − P {z ∈ R1}). Otherwise, α∗ ∈ {e1, e2} minimizes η(α), where eis are η(α∗) = 1 the standard basis vectors of R2.\n\nTheorem 1 provides us with a complete description of how randomized ensembles operate when M = 2. We discuss its implications below:\n\nInterpretation. Theorem 1 states that randomization is guaranteed to help when the condition in (8) is satisfied, i.e., when the probability of data-points z (P {z ∈ R1}) for which it is possible to find adversarial perturbations that can fool f1 or f2 but not both (see configuration 1 in Fig. 3), is greater than the absolute difference (|η1 − η2|) of the individual classifiers’ adversarial risks. Consequently, if the adversarial risks of the classifiers are heavily skewed, i.e., |η1 −η2| is large, then randomization is less likely to help, since condition (8) becomes harder to satisfy. This, in fact, is the case for BAT defense (Pinot et al., 2020) since it generates two classifiers with η1 < 1 and η2 = 1. Theorem 1 indicates that adversarial defenses should strive to achieve η1 ≈ η2 for randomization to be effective. In practice, it is very difficult to make P {z ∈ R1} very large compared to η1 and η2 due to transferability of adversarial perturbations.\n\nOptimality Condition. In fact, the condition in (8) is actually a necessary and sufficient condition for η(α∗) < min{η1, η2}. That is, a randomized ensemble of f1 and f2 is guaranteed to achieve smaller adversarial risk than either f1 of f2 if and only if (8) holds. This also implies that it is\n\n5\n\nconfiguration 1configuration 2configuration 3configuration 4configuration 5Under review as a conference paper at ICLR 2023\n\nimpossible to have a nontrivial3 unique global minimizer other than α∗ = [1/2 1/2]⊤, which provides further theoretical justification for why the BAT defense in (Pinot et al., 2020) does not work, where α∗ = [0.9 0.1]⊤ was claimed to be a unique optimum (obtained via sweeping α).\n\nSimplified Search. Theorem 1 eliminates the need for sweeping α to find the optimal sampling probability α∗ when working with M = 2 classifiers as done in (Pinot et al., 2020; Dbouk & and check if it is smaller than min{η1, η2} Shanbhag, 2022). We only need to evaluate η to choose our optimal sampling probability. In Appendix C.1, we extend this result for M = 3. Interestingly, Vorobeychik & Li (2014) derive a similar result for M = 2 for a different problem of an adversary attempting to reverse engineer the defender’s classifier via queries.\n\n[1/2 1/2]⊤(cid:17)\n\n(cid:16)\n\nTheoretical Limit. From Theorem 1, we can directly obtain a tight bound on the adversarial risk: Corollary 1. For any two classifiers f1 and f2 with individual adversarial risks η1 and η2, respectively, perturbation set S, and data distribution D: (cid:26) 1 2\n\nmax{η1, η2}, min{η1, η2}\n\nη(α) = η(α∗) ≥ min\n\nmin α∈∆2\n\n(10)\n\n(cid:27)\n\n.\n\nIn other words, it is impossible for a REC with M = 2 classifiers to achieve a risk smaller than the RHS in (10). In the next section, we derive a more general version of this bound for arbitrary M .\n\n4.3 TIGHT FUNDAMENTAL BOUNDS\n\nA fundamental question remains to be answered: given an ensemble F of M classifiers with adversarial risks η1, ..., ηM , what is the tightest bound we can provide for the adversarial risk η(α) of a randomized ensemble constructed from F? The following theorem answers this question:\n\nTheorem 2. For a perturbation set S, data distribution D, and collection of M classifiers F with individual adversarial risks ηi (i ∈ [M ]) such that 0 < η1 ≤ ... ≤ ηM ≤ 1, we have ∀α ∈ ∆M : (cid:111)\n\n(11)\n\nmin k∈[M ]\n\n(cid:110) ηk k\n\n≤ η(α) ≤ ηM\n\nBoth bounds are tight in the sense that if all that is known about the setup F, D, and S is {ηi}M i=1, then there exist no tighter bounds. Furthermore, the upper bound is always met if α = eM , and the lower bound (if achievable) can be met if α = (cid:2) 1 k }.\n\n, where m = arg mink∈[M ]{ ηk\n\nm 0 ... 0(cid:3)⊤\n\nm ... 1\n\nUpper bound: The upper bound in (11) holds due to the convexity of η (Proposition 1) and the fact ∆M = H (cid:0){ei}M\n\n(cid:1), where H(X ) is the convex hull of the set of points X .\n\ni=1\n\nImplications of upper bound: Intuitively, we expect that a randomized ensemble cannot be worse than the worst performing member (in this case fM ). A direct implication of this is that if all the members have similar robustness ηi ≈ ηj ∀i, j, then randomized ensembling is guaranteed to either improve or achieve the same robustness. In contrast, deterministic ensemble methods that average logits (Zhang et al., 2022; Abernethy et al., 2021; Kariyappa & Qureshi, 2019) do not even satisfy this upper bound (see Appendix C.2). In other words, there are no worst-case performance guarantees with deterministic ensembling, even if all the classifiers are robust.\n\nLower bound: The main idea behind the proof of the lower bound in (11) is to show that ∀α ∈ ∆M :\n\nη(α) ≥\n\nM (cid:88)\n\n(cid:18)\n\ni=1\n\n(ηi − ηi−1) · max\n\nj∈{i,...,M }\n\n{αj}\n\n= h(α) ≥ min α∈∆M\n\n(cid:19)\n\nh(α) = h(α∗) =\n\nηm m\n\n(12)\n\n. where η0 = 0, m = arg mink∈[M ]{ηk/k}, and h can be interpreted as the adversarial risk of an REC constructed from an optimal set of classifiers F ′ with the same individual risks as F. We make the following observations:\n\nImplications of lower bound: The lower bound in (11) provides us with a fundamental limit on the adversarial risk of RECs viz., it is impossible for any REC constructed from M classifiers with\n\n3that is different than e1 or e2\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nsorted risks {ηi}M i=1 to achieve an adversarial risk smaller than mink∈[M ]{ηk/k} = ηm/m. This limit is not always achievable and generalizes the one in (10) which holds for M = 2. Theorem 2 states that if the limit is achievable then the corresponding optimal sampling probability α∗ = (cid:2) 1 m 0 ... 0(cid:3)⊤ m ... 1 . Note that this does not imply that the optimal sampling probability is always equiprobable sampling ∀F!\n\nAdditionally, the lower bound in (11) provides guidelines for robustifying individual classifiers in order for randomized ensembling to enhance the overall adversarial risk. Given classifiers f1, ..., fm obtained via any sequential ensemble training algorithm, a good rule of thumb for the classifier obtained via the training iteration m + 1 is to have:\n\n(cid:18)\n\nηm ≤ ηm+1 ≤\n\n1 +\n\n(cid:19)\n\n1 m\n\nηm\n\n(13)\n\nNote that only for m = 1 does (13) become a necessary condition: If η2 > 2η1, then f1 will always achieve better risk than an REC of f1 and f2. If a training method generates classifiers f1, ..., fM with risks: η1 < 1 and ηi = 1 ∀i ∈ {2, ..., M }, i.e., only the first classifier is somewhat robust and the remaining M − 1 classifiers are compromised (such as BAT), the lower bound in (11) reduces to:\n\nη(α) ≥ min\n\nη1,\n\n(cid:26)\n\n(cid:27)\n\n1 M\n\n(14)\n\nimplying the necessary condition M ≥ ⌈η−1 1 ⌉ for RECs constructed from F to achieve better risk than f1. Note: the fact that this condition is violated by (Pinot et al., 2020) hints to the existence of strong attacks that can break it (Zhang et al., 2022; Dbouk & Shanbhag, 2022).\n\n4.4 OPTIMAL SAMPLING\n\nIn this section, we leverage Proposition 1 to extend the results in Section 4.2 to provide a theoretically optimal and efficient solution for computing the optimal sampling probability (OSP) algorithm (Algorithm 1) for M > 3.\n\nIn practice, we do not know the true data distribution D. Instead, we are provided a training set z1, ..., zn, assumed to be sampled i.i.d. from D. Given the training set, and a fixed collection of classifiers F, we wish to find the optimal sampling probability:\n\nα∗ = arg min α∈∆M\n\nˆη(α) = arg min\n\nα∈∆M\n\n1 n\n\nn (cid:88)\n\nj=1\n\n(cid:32)\n\narg max δ∈S\n\nM (cid:88)\n\ni=1\n\nαi1 {fi(xj + δ) ̸= yj}\n\n(15)\n\n(cid:33)\n\nNote that the empirical adversarial risk ˆη is also piece-wise linear and convex in α, and hence all our theoretical results apply naturally. In order to numerically solve (15), we first require access to an adversarial attack oracle (attack) for RECs that solves ∀S, F, z, and α:\n\nattack (F, S, α, z) = arg max\n\nδ∈S\n\nM (cid:88)\n\ni=1\n\nαi1 {fi(x + δ) ̸= y}\n\n(16)\n\nUsing the oracle attack, Algorithm 1 updates its solution iteratively given the adversarial error-rate of each classifier over the training set. The projection operator Π∆M in Line (15) of Algorithm 1 ensures that the solution is a valid p.m.f.. Wang & Carreira-Perpin ́an (2013) provide a simple and exact method for computing Π∆M . Finally, we state the following result on the optimality of OSP: Theorem 3. The OSP algorithm output αT satisfies:\n\n0 ≤ ˆη(αT ) − ˆη(α∗) ≤\n\n∥α(1) − α∗∥2\n\n2 + M a2 (cid:80)T t=1 t−1\n\n2a (cid:80)T\n\nt=1 t−2\n\n−−−−→ T →∞\n\n0\n\n(17)\n\nfor all initial conditions α(1) ∈ ∆M , a > 0, where α∗ is a global minimum.\n\nTheorem 3 follows from a direct application of the classic convergence result of the projected subgradient method for constrained convex minimization (Shor (2012)). The optimality of OSP relies on the existence of an attack oracle for (16) which may not always exist. However, attack algorithms\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nsuch as ARC (Dbouk & Shanbhag (2022)) were found to yield good results in the common setting of differentiable classifiers and lp-restricted adversaries.\n\nAlgorithm 1 The Optimal Sampling Probability (OSP) Algorithm for Randomized Ensembles\n\nAlgorithm 2 The Boosting Algorithm for Robust Randomized Ensembles (BARRE)\n\ni=1, perturbation set S, atj=1, initial step-\n\n1: Input: classifiers F = {fi}M\n\nperforms well\n\nM\n\n(cid:3)⊤\n\ng ← 0, at ← a for j ∈ {1, ..., n} do\n\ntack algorithm attack, training set {zj}n size a > 0, and number of iterations T ≥ 1. 2: Output: optimal sampling probability α∗. 3: initialize α(1) ∈ ∆M , ηbest ← 1 4: /∗ we find that α(1) = (cid:2) 1 M ... 1 5: for t ∈ {1, ..., T } do 6: 7: 8: 9: 10: 11:\n\nend for g ← 1 n g η(t) ← g⊤α(t) if η(t) ≤ ηbest then tbest ← t, ηbest ← η(t) /∗ projection-update step\n\nδj ← attack (cid:0)F, S, α(t), zj ∀i ∈ [M ]: gi ← gi + 1 {fi(xj + δj) ̸= yj}\n\n(cid:1)\n\nt\n\n▷ sub-gradient of η(α(t)) ▷ η(α(t))\n\n12: 13: 14: 15: α(t+1) ← Π∆M 16: end for 17: return α(tbest)\n\n(cid:0)α(t) − atg(cid:1)\n\n1: Input: Number of classifiers M , perturbation set S, training set j=1, learning rate ρ, mini-batch size B, number of epochs E,\n\n{zj}n OSP frequency Eo, OSP number of iterations To.\n\n2: Output: Robust randomized ensemble classifier (F, α) 3: initialize θ0 ∈ Θ, F ← ∅ 4: for m ∈ {1, ..., M } do\n\n5: 6: 7: 8: 9:\n\nθm ← θm−1, F ← F ∪ {f (·|θm)}, α ← (cid:2) 1 for e ∈ {1, ..., E} do\n\nm ... 1\n\nm\n\n(cid:3)⊤\n\nfor mini-batch {zb}B\n\nb do\n\ncompute ∀b ∈ [B]: δb ← attack (F, S, α, zb) update θm via SGD:\n\nθm ← θm −\n\nρ B\n\nB (cid:88)\n\nb=1\n\n∇θml\n\n(cid:16) ̃f (xb + δb|θm), yb\n\n(cid:17)\n\nend for /∗ update α every Eo epochs if e mod Eo = 0 then α ← OSP(F, S, {zj}n\n\nj=1, To)\n\n10: 11: 12: 13: 14: end for 15: return F, α\n\nend for\n\n5 A ROBUST BOOSTING ALGORITHM FOR RANDOMIZED ENSEMBLES\n\nInspired by BAT (Pinot et al., 2020) and MRBoost (Zhang et al., 2022), we leverage our results in Section 4 and propose BARRE: a unified Boosting Algorithm for Robust Randomized Ensembles described in Algorithm 2. Given a dataset {zj}n j=1 and an REC attack algorithm attack, BARRE iteratively trains a set of parametric classifiers f (·|θ1), ..., f (·|θM ) such that the adversarial risk of the corresponding REC is minimized. The first iteration of BARRE reduces to standard AT (Madry et al., 2018). Doing so typically guarantees that the first classifier achieves the lowest adversarial risk and η(α∗) ≤ η1, i.e., Theorem 3 ensures the REC is no worse than single model AT.\n\nIn each iteration m ≥ 2, BARRE initializes the m-th classifier f (·|θm) with θm = θm−1. The training procedure alternates between updating the parameters θm via SGD using adversarial samples of the current REC and solving for the optimal sampling probability α∗ ∈ ∆m via OSP. Including f (·|θm) in the attack (Line (8)) is crucial, as it ensures that the robustness of f (·|θm) is not completely compromised, thereby improving the bounds in Theorem 2. Note that for iterations m ≤ 3, we replace the OSP procedure in Line (12) with a simplified search over a finite set of candidate solutions (see Section 4.2 and Appendix C.1).\n\n5.1 EXPERIMENTAL RESULTS\n\nIn this section, we validate the effectiveness of BARRE in constructing robust RECs.\n\nSetup. Per standard practice, we focus on defending against l∞ norm-bounded adversaries. We report results for three network architectures with different complexities: ResNet-20 (He et al., 2016), MobileNetV1 (Howard et al., 2017), and ResNet-18, across CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al., 2009). Computational complexity is measured via the number of floating-point operations (FLOPs) required per inference. To ensure a fair comparison across different baselines, we use the same hyper-parameter settings detailed in Appendix D.1.\n\nAttack Algorithm. For all our robust evaluations, we will adopt the state-of-the-art ARC algorithm (Dbouk & Shanbhag, 2022) which can be used for both RECs and single models. Specifically, we shall use a slightly modified version that achieves better results in the equiprobable sampling setting (see Appendix D.3). For training with BARRE, we adopt adaptive PGD (Zhang et al., 2022) for better generalization performance (see Appendix D.4).\n\nBenefit of Randomization. Table 1 demonstrates that BARRE can successfully construct RECs that achieve competitive robustness (within ∼ 0.5%) compared to MRBoost-trained deterministic\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison between BARRE and MRBoost across different network architectures and ensemble sizes on CIFAR-10. Robust accuracy is measured against an l∞ norm-bounded adversary using ARC with ε = 8/255.\n\nNetwork\n\nMethod\n\nResNet-20\n\nMobileNetV1\n\nResNet-18\n\nMRBoost BARRE\n\nMRBoost BARRE\n\nMRBoost BARRE\n\nM = 1 Arob\n\nAnat\n\nFLOPs\n\n73.18\n\n41.99\n\n81 M\n\n79.01\n\n46.22\n\n312 M\n\n80.96\n\n48.7\n\n1.1 B\n\nAnat\n\n75.22 74.63\n\n80.19 79.58\n\n83.90 82.66\n\nM = 2 Arob\n\n44.68 44.38\n\n48.58 48.32\n\n50.72 50.51\n\nFLOPs\n\nAnat\n\n162 M 76.13 81 M 75.55\n\n624 M 79.79 312 M 79.53\n\n2.2 B 85.07 1.1 B 83.40\n\nM = 3 Arob\n\n46.09 45.41\n\n49.39 48.75\n\n52.15 51.57\n\nFLOPs\n\nAnat\n\n243 M 76.96 81 M 75.95\n\n936 M 80.14 312 M 79.54\n\n3.3 B 85.07 1.1 B 83.54\n\nM = 4 Arob\n\n46.34 46.44\n\n49.36 49.38\n\n52.15 52.13\n\nFLOPs\n\n324 M 81 M\n\n1.2 B 312 M\n\n4.4 B 1.1 B\n\nensembles, across three different network architectures on CIFAR-10. The benefit of randomization can be seen for M ≥ 2, as we obtain massive 2 − 4× savings in compute requirements. Note that both methods have the same4 memory footprint. These observations are further corroborated by CIFAR-100 experiments in Appendix D.5.\n\nTable 2: Comparison between BARRE and other methods at constructing robust randomized ensemble classifiers across various network architectures and datasets. Robust accuracy is measured against an l∞ norm-bounded adversary using ARC with ε = 8/255.\n\nNetwork\n\nFLOPs Method\n\nSize M\n\nCIFAR-10\n\nCIFAR-100\n\nAnat [%] Arob [%] Anat [%] Arob [%]\n\nResNet-20\n\n81 M\n\nMobileNetV1\n\n312 M\n\nAT\n\nM = 1\n\n73.18\n\nM = 5 IAT MRBoost M = 5 M = 5 BARRE\n\n73.90 75.89 76.28\n\nAT\n\nM = 1\n\n79.01\n\nM = 5 IAT MRBoost† M = 5 MRBoost M = 5 M = 5 BARRE\n\n78.89 76.70 78.65 79.55\n\nAT\n\nM = 1\n\n80.96\n\n41.99\n\n45.77 46.66 47.35\n\n46.22\n\n49.57 48.05 48.91 49.91\n\n48.72\n\nResNet-18\n\n51.43 51.82 51.82 52.13 † result obtained assuming equiprobable sampling instead of using OSP\n\nM = 4 IAT MRBoost† M = 4 MRBoost M = 4 M = 4 BARRE\n\n80.99 83.13 83.13 83.54\n\n1.1 B\n\n38.34\n\n38.57 41.69 41.86\n\n51.87\n\n51.41 50.14 52.96 52.95\n\n53.85\n\n54.30 51.06 52.04 54.63\n\n17.69\n\n19.65 21.04 21.11\n\n23.45\n\n25.74 24.76 25.95 27.53\n\n24.15\n\n26.73 24.04 25.65 26.93\n\nBARRE vs. Other Methods. Due to the lack of dedicated randomized ensemble defenses, we establish baselines by constructing RECs from both MRBoost and independently adversarially trained (IAT) models. We use OSP (Algorithm 1) to find the optimal sampling probability for each REC. All RECs share the same first classifier f1, which is adversarially trained. Doing so ensures a fair comparison, and guarantees all the methods cannot be worse than AT. Table 2 provides strong evidence that BARRE outperforms both IAT and MRBoost for both CIFAR-10 and CIFAR-100 datasets. Interestingly, we find that MRBoost ensembles can be quite ill-suited for RECs. This can be seen for MobileNetV1, where the optimal sampling probability obtained was α∗ = [0.25 0.25 0.25 0.25 0]⊤, i.e., the REC completely disregards the last classifier. In contrast, BARRE-trained RECs utilize all members of the ensemble.\n\n6 CONCLUSION\n\nWe have demonstrated both theoretically and empirically that robust randomized ensemble classifiers (RECs) are realizable. Theoretically, we derive the robustness limits of RECs, necessary and sufficient conditions for them to be useful, and efficient methods for finding the optimal sampling probability. Empirically, we propose BARRE, a new boosting algorithm for constructing robust RECs and demonstrate its effectiveness at defending against strong l∞ norm-bounded adversaries.\n\n4ignoring the negligible memory overhead of storing α\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJacob Abernethy, Pranjal Awasthi, and Satyen Kale. A multiclass boosting framework for achieving\n\nfast and provable adversarial robustness. arXiv preprint arXiv:2103.01276, 2021.\n\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning, pp. 274–283. PMLR, 2018.\n\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ˇSrndi ́c, Pavel Laskov, GiorIn Joint gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. European conference on machine learning and knowledge discovery in databases, pp. 387–402. Springer, 2013.\n\nStephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-\n\nversity press, 2004.\n\nLeo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\n\nGinevra Carbone, Matthew Wicker, Luca Laurenti, Andrea Patane, Luca Bortolussi, and Guido Sanguinetti. Robustness of bayesian neural networks to gradient-based attacks. Advances in Neural Information Processing Systems, 33:15602–15613, 2020.\n\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017\n\nieee symposium on security and privacy (sp), pp. 39–57. IEEE, 2017.\n\nMoustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pp. 854–863. PMLR, 2017.\n\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019.\n\nHassan Dbouk and Naresh Shanbhag. Generalized depthwise-separable convolutions for adversarially robust and efficient neural networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nHassan Dbouk and Naresh Shanbhag. Adversarial vulnerability of randomized ensembles. In Inter-\n\nnational Conference on Machine Learning, pp. 4890–4917. PMLR, 2022.\n\nGuneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy D Bernstein, Jean Kossaifi, Aran Khanna, and Animashree Anandkumar. Stochastic activation pruning for robust adversarial defense. In International Conference on Learning Representations, 2018.\n\nThomas G Dietterich. Ensemble methods in machine learning. In International workshop on multi-\n\nple classifier systems, pp. 1–15. Springer, 2000.\n\nRanjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A Kai Qin, and Yun Yang. Adversarial camouflage: Hiding physical-world attacks with natural styles. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1000–1008, 2020.\n\nYoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an\n\napplication to boosting. Journal of computer and system sciences, 55(1):119–139, 1997.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. arXiv preprint arXiv:1412.6572, 2014.\n\nMinghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, and Dahua Lin. When NAS meets robustness: In Proceedings of the IEEE/CVF\n\nIn search of robust architectures against adversarial attacks. Conference on Computer Vision and Pattern Recognition, pp. 631–640, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\nSanjay Kariyappa and Moinuddin K Qureshi. Improving adversarial robustness of ensembles with\n\ndiversity training. arXiv preprint arXiv:1901.09981, 2019.\n\nGuy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In International conference on computer aided verification, pp. 97–117. Springer, 2017.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\nTechnical report, Citeseer, 2009.\n\nXin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, and Yiran Chen. Dpatch: An adversarial\n\npatch attack on object detectors. arXiv preprint arXiv:1806.02299, 2018.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.\n\nRadford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business\n\nMedia, 2012.\n\nTianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via promoting ensemble diversity. In International Conference on Machine Learning, pp. 4970–4979. PMLR, 2019.\n\nNicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE symposium on security and privacy (SP), pp. 582–597. IEEE, 2016.\n\nRafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, and Jamal Atif. Randomization matters how to defend against strong adversarial attacks. In International Conference on Machine Learning, pp. 7717–7727. PMLR, 2020.\n\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-\n\nples. In International Conference on Learning Representations, 2018.\n\nLeslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning.\n\nIn\n\nInternational Conference on Machine Learning, pp. 8093–8104. PMLR, 2020.\n\nVikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana. HYDRA: Pruning adversarially robust neural networks. Advances in Neural Information Processing Systems (NeurIPS), 7, 2020.\n\nSanchari Sen, Balaraman Ravindran, and Anand Raghunathan. EMPIR: Ensembles of mixed precision deep networks for increased robustness against adversarial attacks. In International Conference on Learning Representations, 2019.\n\nNaum Zuselevich Shor. Minimization methods for non-differentiable functions, volume 3. Springer\n\nScience & Business Media, 2012.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nVincent Tjeng, Kai Y Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed\n\ninteger programming. In International Conference on Learning Representations, 2018.\n\nFlorian Tram`er, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020.\n\nFlorian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick In International ConferMcDaniel. Ensemble adversarial training: Attacks and defenses. ence on Learning Representations, 2018. URL https://openreview.net/forum?id= rkZvSe-RZ.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYevgeniy Vorobeychik and Bo Li. Optimal randomized classification in adversarial settings.\n\nIn\n\nAAMAS, pp. 485–492, 2014.\n\nWeiran Wang and Miguel A Carreira-Perpin ́an. Projection onto the probability simplex: An efficient\n\nalgorithm with a simple proof, and an application. arXiv preprint arXiv:1309.1541, 2013.\n\nKai Y Xiao, Vincent Tjeng, Nur Muhammad Mahi Shafiullah, and Aleksander Madry. Training for faster adversarial robustness verification via inducing ReLU stability. In International Conference on Learning Representations, 2018.\n\nCihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. In International Conference on Learning Representations, 2018.\n\nGreg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing of all shapes and sizes. In International Conference on Machine Learning, pp. 10693– 10705. PMLR, 2020a.\n\nHuanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner, Andrew Touchet, Wesley Wilkes, Heath Berry, and Hai Li. DVERGE: Diversifying vulnerabilities for enhanced robust generation of ensembles. Advances in Neural Information Processing Systems, 33, 2020b.\n\nYuzhe Yang, Guo Zhang, Dina Katabi, and Zhi Xu. Me-net: Towards effective adversarial robustness with matrix estimation. In International Conference on Machine Learning, pp. 7025–7034. PMLR, 2019.\n\nZhuolin Yang, Linyi Li, Xiaojun Xu, Shiliang Zuo, Qian Chen, Pan Zhou, Benjamin Rubinstein, Ce Zhang, and Bo Li. TRS: Transferability reduced ensemble via promoting gradient diversity and model smoothness. Advances in Neural Information Processing Systems, 34, 2021.\n\nDinghuai Zhang, Hongyang Zhang, Aaron Courville, Yoshua Bengio, Pradeep Ravikumar, and Arun Sai Suggala. Building robust ensembles via margin boosting. In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp. 26669–26692. PMLR, 17–23 Jul 2022.\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, pp. 7472–7482. PMLR, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA THEORETICAL RATIONALE FOR BARRE\n\nIn this section, we expand on Section 5.1 and provide a more detailed rationale behind the steps in BARRE.\n\nGiven a dataset {zj}n j=1 and an REC attack algorithm attack, BARRE iteratively trains a set of parametric classifiers f (·|θ1), ..., f (·|θM ) in a boosting fashion. Note that the optimality of OSP (Theorem 3) implies that a BARRE-trained REC is guaranteed to achieve better or the same performance than the BEST performing member of the ensemble. This explains the choice of boosting for BARRE, since the first iteration reduces to standard AT, the most effective method to date for generating robust classifiers. Thus, BARRE trained RECs are guaranteed to perform better than AT.\n\nIn each iteration m ≥ 2, BARRE initializes the m-th classifier f (·|θm) with θm = θm−1. The training procedure alternates between updating the parameters θm via SGD using adversarial samples of the current REC and solving for the optimal sampling probability α∗ ∈ ∆m via OSP. Including f (·|θm) in the attack (Line (8)) is crucial, as it ensures that the robustness of f (·|θm) is not completely compromised, thereby improving the bounds in Theorem 2.\n\nFurthermore, the rationale behind the sequence of steps in BARRE can be better understood using Theorem 1 (for the case of M = 2). Theorem 1 states that the optimal REC adversarial risk would 2 (η1 + η2 − P {z ∈ R1}) (assuming (8) is met), therefore it is equally important to be η(α∗) = 1 minimize both η’s and maximize P {z ∈ R1}. BARRE does so by initially adversarially training a robust classifier f1 (minimizing η1), then training f2 (initialized from f1 to minimizes η2) on the adversarial examples of the REC of f1 and f2. Doing so increases P {z ∈ R1} while maintaining η2 as low as possible.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB OMITTED PROOFS AND DERIVATIONS\n\nB.1 PROOF OF PROPOSITION 1\n\nWe provide the proof of Proposition 1 (restated below): i=1, perturbation set S ⊂ Rd, and data distribution Proposition (Restated). For any F = {fi}M D, the adversarial risk η is a piece-wise linear convex function ∀α ∈ ∆M . Specifically, ∃K ∈ N configurations Uk ⊆ {0, 1}M ∀k ∈ [K] and p.m.f. p ∈ ∆K such that:\n\nη(α) =\n\nK (cid:88)\n\n(cid:18)\n\nk=1\n\npk · max u∈Uk\n\n(cid:19)\n\n(cid:8)u⊤α(cid:9)\n\n(18)\n\nProof. Consider having one data-point z ∈ Rd × [C], then for any δ ∈ S and α ∈ ∆M we have:\n\nr (z, δ, α) =\n\nM (cid:88)\n\ni=1\n\nαi1 {fi(x + δ) ̸= y} = u⊤α\n\n(19)\n\nwhere u ∈ {0, 1}M such that ui = 1 if and only if δ is adversarial to fi at z. Since u is independent of α, we thus obtain a many-to-one mapping from δ ∈ S to u ∈ {0, 1}M . Therefore, for any α and z, we can always decompose the perturbation set S, i.e., S = G1 ∪ ... ∪ Gn, into n ≤ 2M subsets, such that: ∀δ ∈ Gj : r (z, δ, α) = α⊤uj for some binary vector uj independent of α. Let U = {uj}n\n\nj=1 be the collection of these vectors, then we can write:\n\nmax δ∈S\n\nr (z, δ, α) = max\n\nδ∈G1∪...∪Gn\n\nr (z, δ, α) = max j∈[n]\n\nmax δ∈Gj\n\nr (z, δ, α)\n\n= max u∈U\n\n(cid:26)\n\n(cid:27)\n\n(cid:8)u⊤α(cid:9)\n\n(20)\n\nj=1 define a unique classifier and data-point configuration that is independent of\n\nThe vectors {uj}n the sampling probability. The function maxδ r is thus convex and piece-wise linear in α. Partitioning the data-point space R ⊆ Rd × [C] into K subsets R = R1 ∪ ... ∪ RK such that all the data-points z ∈ Rk share the same set “configuration” Uk, we obtain:\n\nη(α) = Ez∼D\n\n(cid:34)\n\nmax δ∈S\n\nM (cid:88)\n\ni=1\n\nαi1 {fi(x + δ) ̸= y}\n\n(cid:35)\n\npz(z) · max δ∈S\n\nr (z, δ, α) dz\n\n=\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\nz∈R (cid:90)\n\nK (cid:88)\n\nk=1\n\nz∈Rk\n\nK (cid:88)\n\n(cid:90)\n\nz∈Rk\n\nmax u∈Uk\n\n(cid:18)\n\n(cid:18)\n\nk=1\n\nK (cid:88)\n\nk=1\n\nK (cid:88)\n\nk=1\n\nz∈Rk\n\n(cid:19)\n\n(cid:8)u⊤α(cid:9)\n\npk · max u∈Uk\n\npz(z) · max δ∈S\n\nr (z, δ, α) dz\n\n(cid:18)\n\npz(z) ·\n\nmax u∈Uk\n\n(cid:19)\n\n(cid:8)u⊤α(cid:9)\n\n(cid:90)\n\n(cid:8)u⊤α(cid:9) ·\n\npz(z) dz\n\n(21)\n\ndz\n\n(cid:19)\n\nwhere the total size of the partition K is finite (exponential in the size M ) and p ∈ ∆K such that pk = P {z ∈ Rk}. Finally, η is convex and piece-wise linear in α since the summation of convex and piece-wise linear functions is also convex and piece-wise linear.\n\nB.2 PROOF OF THEOREM 1\n\nFirst, we state and prove this useful lemma:\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 1. Let h : R → R be a convex piece-wise linear, hence sub-differentiable, function of the form:\n\nh(x) = max{a1x + b1, a2x + b2} + a3x + b3 (22) such that a1 < a2. We wish to minimize h over x ∈ [c, d] where c ≤ y ≤ d, and y is the intersection point b2−b1 a1−a2\n\n.\n\nThen, the optimal value x∗ that minimizes h(x) in (22), is given by\n\nx∗ =\n\n \n\n\n\ny, c, d,\n\nif a3 ∈ (−a2, −a1) if a3 ≥ −a1 if a3 ≤ −a2\n\nNote: only in the first case is the solution unique.\n\nProof. From constrained convex optimization (Boyd et al. (2004); Shor (2012)), we know that x∗ is the minimizer of h over [c, d] if there exists a sub-gradient g ∈ ∂h(x∗) such that:\n\ng · (x − x∗) ≥ 0 ∀x ∈ [c, d]\n\n(23)\n\nFor x ̸= y, h is differentiable with ∇h = a3 + a1 (if x < y) or ∇h = a3 + a2 (if x > y), and for x = y the sub-differential is given by ∂h(y) = {a3 + βa1 + (1 − β)a2 : β ∈ [0, 1]}.\n\nIf a3 ∈ (−a2, −a1), then ∃β ∈ [0, 1] such that a3 + βa1 + (1 − β)a2 = 0, and thus 0 ∈ ∂h(y), which is a sufficient condition for global minimization, thus x∗ = y. Furthermore, x∗ = y is unique, since ∀x ̸= y, we will have ∇h = a1 + a3 < 0 (if x < y) or ∇h = a2 + a3 > 0 (if x > y) which in both cases implies ∀z ̸= y ∃x ∈ [c, d] such that ∇h(z)(x − z) < 0.\n\nIf a3 /∈ (−a2, −a1), then either a3 ≥ −a1 or a3 ≤ −a2. If a3 ≥ −a1, then a1 + a3 = ∇h(c) ≥ 0, which implies that: (a1 + a3)(x − c) ≥ 0 ∀x ∈ [c, d], hence x∗ = c. Otherwise if a3 ≤ −a2, then a2 + a3 = ∇h(d) ≤ 0, which implies that: (a2 + a3)(x − d) ≥ 0 ∀x ∈ [c, d], hence x∗ = d.\n\nWe now provide the proof of Theorem 1 (restated below): Theorem (Restated). For any two classifiers f1 and f2 with individual adversarial risks η1 and η2, respectively, subject to a perturbation set S ⊂ Rd and data distribution D, if:\n\nP {z ∈ R1} > |η1 − η2|\n\n(24)\n\nwhere:\n\nR1 = {z ∈ Rd × [C] : S1(z) ̸= ∅, S2(z) ̸= ∅, S1(z) ∩ S2(z) = ∅} (25) then the optimum sampling probability α∗ = (1/2, 1/2)⊤ uniquely minimizes η(α) resulting in 2 (η1 + η2 − P {z ∈ R1}). Otherwise, α∗ ∈ {e1, e2} minimizes η(α), where eis are η(α∗) = 1 the standard basis vectors of R2.\n\nProof. We know that, for M = 2, the adversarial risk η can be re-written ∀α ∈ ∆2:\n\nη(α) = p1 · max{α1, α2} + p2 · 1 + p3 · α1 + p4 · α2 + p5 · 0\n\n(26)\n\nwhere pk = P {z ∈ Rk}, and the regions {Rk}K\n\nk=1 partition the input space Rd × [C] as follows:\n\nR1 = {z ∈ Rd × [C] : S1(z) ̸= ∅, S2(z) ̸= ∅, S1(z) ∩ S2(z) = ∅} R2 = {z ∈ Rd × [C] : S1(z) ∩ S2(z) ̸= ∅} R3 = {z ∈ Rd × [C] : S1(z) ̸= ∅, S2(z) = ∅} R4 = {z ∈ Rd × [C] : S1(z) = ∅, S2(z) ̸= ∅} R5 = {z ∈ Rd × [C] : S1(z) = S2(z) = ∅}\n\nUsing α1 = 1 − α2 = α, we have ∀α ∈ [0, 1]: (α, 1 − α)⊤(cid:17)\n\n(cid:16)\n\nη\n\n= h(α) = p1 · max{α, 1 − α} + (p3 − p4) · α + p2 + p4\n\nwhere we wish to find α∗ ∈ [0, 1] that minimizes h(α). Applying Lemma 1 with:\n\na1 = −p1, b1 = p1, a2 = p1, b2 = 0, a3 = p3 − p4, b3 = p2 + p4\n\nand utilizing η1 = η(e1) = p1 + p2 + p3 and η2 = η(e2) = p1 + p2 + p4, yields the main result.\n\n15\n\n(27)\n\n(28)\n\n(29)\n\nUnder review as a conference paper at ICLR 2023\n\nB.3 PROOF OF COROLLARY 1\n\nWe provide the proof of Corollary 1 (restated below):\n\nCorollary. For any two classifiers f1 and f2 with individual adversarial risks η1 and η2, respectively, perturbation set S, and data distribution D: (cid:26) 1 2\n\nmax{η1, η2}, min{η1, η2}\n\nη(α) = η(α∗) ≥ min\n\nmin α∈∆2\n\n(30)\n\n(cid:27)\n\n.\n\nProof. From Theorem 1, we have that: (cid:26) 1 2\n\nη(α∗) = min\n\n(η1 + η2 − P {z ∈ R1}) , min{η1, η2}\n\n(31)\n\n(cid:27)\n\nUsing the tight upper bound on P {z ∈ R1} ≤ min{η1, η2}, we obtain the main result.\n\nB.4 PROOF OF THEOREM 2\n\nB.4.1 USEFUL LEMMAS\n\nWe first state and prove a few useful lemmas that are vital for proving Theorem 2. While some lemmas are trivial and have been proven elsewhere, we nonetheless state their proofs for completeness. Lemma 2. Let h : Rn → R be a convex function, and H(X ) ⊂ Rn be the convex hull of X = {x1, ..., xd} where {xi}d\n\ni=1 ∈ Rn, then there exists xm ∈ X such that:\n\nmax u∈H(X )\n\nh(u) = h(xm)\n\nProof. Let u be any arbitrary vector in H(X ), that is ∃α ∈ ∆d:\n\nu =\n\nd (cid:88)\n\ni=1\n\nαixi\n\n(32)\n\n(33)\n\nLet m ∈ [d] such that h(xm) ≥ h(xi) ∀i ∈ [d]. From the convexity of h, we upper bound h(u) as follows:\n\nh(u) = h\n\n(cid:33)\n\nαixi\n\n≤\n\n(cid:32) d\n\n(cid:88)\n\ni=1\n\nd (cid:88)\n\ni=1\n\nαih(xi) ≤\n\nd (cid:88)\n\ni=1\n\nαih(xm) = h(xm)\n\nd (cid:88)\n\ni=1\n\nαi = h(xm)\n\n(34)\n\nThus, (32) holds for any u ∈ H(X ).\n\nLemma 3 (Redistribution Lemma). ∀p, q such that 0 ≤ p ≤ q ≤ 1, ∀α ∈ ∆M , and ∀I, J ⊆ [M ] such that I /∈ J /∈ I we have:\n\np · max i∈I\n\n{αi} + q · max j∈J\n\n{αj} ≥ p · max i∈I∪J\n\n{αi} + (q − p) · max j∈J\n\n{αj} + p · max k∈I∩J\n\n{αk}\n\n(35)\n\nProof.\n\np · max i∈I\n\n{αi} + q · max j∈J\n\n{αj} = p · αi∗ + q · αj∗\n\n= p · (αi∗ + αj∗ ) + (q − p) · αj∗\n\n(cid:18)\n\n(a)= p ·\n\nmax i∈I∪J\n\n{αi} + min{αi∗ , αj∗ }\n\n+ (q − p) · αj∗\n\n(36)\n\n(cid:19)\n\n(b) ≥ p · max i∈I∪J\n\n= p · max i∈I∪J\n\n{αi} + (q − p) · αj∗ + p · max k∈I∩J {αj} + p · max k∈I∩J\n\n{αi} + (q − p) · max j∈J\n\n{αk}\n\n{αk}\n\nwhere (a) holds because the maximum over I ∪ J is either αi∗ or αj∗ , and (b) holds since the smallest of the two maximizers cannot be smaller than the maximizer of the smaller set I ∩ J .\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 4. Let {fi}M i=1 be an arbitrary collection of C-ary classifiers with individual adversarial risks ηi such that 0 < η1 ≤ ... ≤ ηM ≤ 1. For any data distribution D and perturbation set S we have ∀α ∈ ∆M :\n\nη(α) ≥\n\nM (cid:88)\n\n(cid:18)\n\ni=1\n\n(ηi − ηi−1) · max\n\nj∈{i,...,M }\n\n(cid:19)\n\n{αj}\n\n(37)\n\nwhere η0\n\n. = 0.\n\nProof. From Proposition 1 we know that ∃K ∈ N, p ∈ ∆K, and Uk ⊆ {0, 1}M ∀k ∈ [K] such that:\n\nη(α) =\n\nK (cid:88)\n\n(cid:18)\n\nk=1\n\npk · max u∈Uk\n\n(cid:8)u⊤α(cid:9)\n\n(cid:19)\n\n(38)\n\nLet Lk ⊆ [M ] represent the set of classifier indices i1, ..., in that are active in the configuration Uk, that is:\n\nm ∈ Lk ⇐⇒ ∃v ∈ Uk such that vm = 1\n\n(39)\n\nWe then lower bound η as follows:\n\nη(α) =\n\nK (cid:88)\n\nk=1\n\n(cid:18)\n\npk · max u∈Uk\n\n(cid:8)u⊤α(cid:9)\n\n(cid:19)\n\n≥\n\nK (cid:88)\n\n(cid:18)\n\nk=1\n\n(cid:19)\n\npk · max i∈Lk\n\n{αi}\n\n= η′(α)\n\n(40)\n\nThe bound trivially holds, since the sum of positive numbers is always larger than any summand. It is noteworthy to point out that the RHS quantity η′(α) can be interpreted as the adversarial risk of an auxiliary set of classifiers F ′ with same individual risks {ηi} such that for any z ∈ Rd × [C], the classifiers have no common adversarial perturbations, i.e.:\n\nM (cid:92)\n\ni=1\n\nS ′\n\ni(z) = ∅\n\nand:\n\ni = η′(ei) = η′\n\n(cid:88)\n\nk:i∈Lk\n\npk = η(ei) = ηi\n\n(41)\n\n(42)\n\nAssume that the conditions of Lemma 3 are met by two terms in η′, i.e., ∃k1, k2 ∈ [K] such that Lk1 /∈ Lk2 /∈ Lk1 and pk1 ≤ pk2, then we can apply the bound in Lemma 3 and obtain:\n\nη′(α) −\n\n(cid:88)\n\n(cid:18)\n\n(cid:19)\n\npk · max i∈Lk\n\n{αi}\n\n= pk1 · max i∈Lk1\n\n{αi} + pk2 · max i∈Lk2\n\n{αi}\n\nk∈[K]\\{k1,k2} ≥ pk1 · max\n\ni∈Lk1 ∪Lk2\n\n= η′′(α) −\n\n{αi} + (pk2 − pk1 ) · max j∈Lk2 (cid:19)\n\n(cid:18)\n\n(cid:88)\n\npk · max i∈Lk\n\n{αi}\n\nk∈[K]\\{k1,k2}\n\n{αj} + pk1 · max\n\nk∈Lk1 ∩Lk2\n\n{αk}\n\n(43)\n\nwhere η′′(α) is the modified ensemble adversarial risk. The application of Lemma 3 can be understood as a way to “re-distribute” the classifiers’ adversarial vulnerabilities while preserving the adversarial risk identities ∀i ∈ [M ]:\n\nηi = η′(ei) =\n\n(cid:88)\n\nk:i∈Lk\n\npk = η′′(ei)\n\n(44)\n\nThe main idea of this proof is to keep applying Lemma 3 to the modified ensemble adversarial risks (if possible) to obtain a better lower bound. The process stops when the conditions are no longer met, and we obtain an adversarial risk h(α):\n\nη′(α) ≥ η′′(α) ≥ .. ≥ h(α) =\n\nL (cid:88)\n\nk=1\n\n(cid:18)\n\n(cid:19)\n\nqk · max j∈Jk\n\n{αj}\n\n(45)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nWithout loss of generality, we will assume that {Jk} are distinct and qk ̸= 0. Furthermore, since the conditions of Lemma 3 cannot be met by any two sets in {Jk}, we must have (up to a re-ordering of the indices):\n\nWe now make the following observations:\n\nJL ⊂ JL−1 ⊂ ... ⊂ J1 ⊆ [M ]\n\n1. Due to (46), we have that L ≤ M and for all i ∈ [M ], ∃mi ∈ [L] such that:\n\nηi =\n\n(cid:88)\n\nqk =\n\nk:i∈Jk\n\nmi(cid:88)\n\nk=1\n\nqk\n\n(46)\n\n(47)\n\n2. Since {ηi} are sorted, we get that mi+1 = mi + 1 if ηi < ηi+1 or mi+1 = mi otherwise\n\n3. J1 = [M ] since η1 ̸= 0\n\n4. For any two consecutive sets Jk and Jk+1, we can always find n ≥ 1 indices from [M ] such that Jk = Jk+1 ∪ {i1, ..., in}. The indices i1, ..., in are consecutive, share the same mi (i.e., ηil is the same for all l ∈ [n]), and also satisfy:\n\nmin l∈[n]\n\n{il} = max j∈Jk+1\n\n{j} + 1\n\n(48)\n\nWe first prove the lemma for the special case of distinct risks, i.e. ηi < ηi+1 ∀i.\n\nSpecial Case. The risks are distinct, then we must have L = M , with every two consecutive sets Jk and Jk+1 differing by one index. Therefore we have Jk = Jk+1 ∪ {k} and JM +1 = ∅. Furthermore, we will get ηi − ηi−1 = qi ∀i ∈ [M ] with η0 = 0. Thus we can write:\n\nh(α) =\n\nM (cid:88)\n\n(cid:18)\n\nk=1\n\n(cid:19)\n\nqk · max j∈Jk\n\n{αj}\n\n=\n\nM (cid:88)\n\n(cid:18)\n\ni=1\n\n(ηi − ηi−1) · max\n\nj∈{i,...,M }\n\n(cid:19)\n\n{αj}\n\n(49)\n\nGeneral Case. For the general case we will have L ≤ M distinct risks ηi1 < ... < ηiL and M − L repeated risks, where i1 = 1. Thus we have qk = ηik − ηik−1 ∀k ∈ [L], and ηi0 = η0 = 0 by definition. Using observations 3 and 4, we have that Jk = {uk, ..., M } for some index uk ∈ [M ], with u1 = 1. Thus we have uk+1 − uk − 1 ≥ 0 to be the number of of consecutive repeated risks equal to ηik . Let {J ′ k} be the M − L index sets missing from {i ∈ [M ] : {i, ..., M }}, then we have:\n\nL (cid:88)\n\n(cid:18)\n\nh(α) =\n\n(cid:18)\n\n(cid:18)\n\n(cid:18)\n\nk=1\n\nL (cid:88)\n\nk=1\n\nL (cid:88)\n\nk=1\n\nM (cid:88)\n\ni=1\n\n=\n\n=\n\n(a)=\n\n(cid:19)\n\nqk · max j∈Jk\n\n{αj}\n\n(cid:0)ηik+1 − ηik\n\n(cid:1) · max\n\nj∈{uk,...,M }\n\n(cid:19)\n\n{αj}\n\n(cid:0)ηik+1 − ηik\n\n(cid:1) · max\n\nj∈{uk,...,M }\n\n(cid:19)\n\n{αj}\n\n+\n\nM −L (cid:88)\n\n(cid:18)\n\nk=1\n\n(cid:19)\n\n0 · max j∈J ′ k\n\n{αj}\n\n(50)\n\n(ηi − ηi−1) · max\n\nj∈{i,...,M }\n\n(cid:19)\n\n{αj}\n\nwhere (a) holds due to the fact ηi − ηi−1 = 0 for all the merged M − L terms.\n\nLemma 5. Given a sequence {γi}M m 0 ... 0(cid:3)⊤ α∗ = (cid:2) 1\n\nm ... 1\n\ni=0 such that 0 = γ0 < γ1 ≤ ... ≤ γM ≤ 1, the vector\n\n∈ ∆M is a solution to the following minimization problem:\n\nmin α∈∆M\n\nh(α) = min α∈∆M\n\nM (cid:88)\n\n(cid:18)\n\ni=1\n\nwhere γm\n\nm ≤ γi\n\ni , ∀i ∈ [M ].\n\n(γi − γi−1) · max\n\n{αj}\n\nj∈{i,...,M }\n\n(cid:19)\n\n=\n\nγm m\n\n(51)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nProof. We know that h is a piece-wise linear convex function over a closed and convex set, which implies the existence of a global minimizer.\n\nDefine the mapping g : ∆M → [0, 1]M such that ∀i ∈ [M ]:\n\ngi(α) = max\n\nj∈{i,...,M }\n\n{αj} −\n\nmax j∈{i+1,...,M }\n\n{αj}\n\n(52)\n\nWe can re-write the function h via a simple re-arrangement to obtain:\n\nh(α) =\n\nM (cid:88)\n\ni=1\n\n(cid:18)\n\n(cid:19)\n\nγi ·\n\nmax j∈{i,...,M }\n\n{αj} −\n\nmax j∈{i+1,...,M }\n\n{αj}\n\n=\n\nM (cid:88)\n\ni=1\n\nγi · gi(α) = γ⊤g(α)\n\n(53)\n\nDefine the decomposition over the probability simplex: ∆M = ∆1 ∀n ∈ [M !], ∃i1, i2, ..., iM such that ∀α ∈ ∆n\n\nM ∪ ∆2\n\nM ∪ ... ∪ ∆M !\n\nM , where\n\nM we have: αi1 ≥ αi2 ≥ αi3 ≥ ... ≥ αiM\n\n(54)\n\nIn other words, ∆n M is the set of all probability vectors that share the same sorting indices. Since we have M ! ways to arrange M numbers, the size of the decomposition will be M !. We now make the following observations:\n\nM is a convex set. quick proof : Let α, β ∈ ∆n\n\n1. ∀n, ∆n M , then ∃i1, i2, ..., iM such that αi1 ≥ ... ≥ αiM and βi1 ≥ ... ≥ βiM . ∀λ ∈ [0, 1] we have q = λα + (1 − λ)β ∈ ∆M , since (cid:80) qi = (cid:80) λαi + (1 − λ)βi = 1 and qi ≥ 0. We also have ∀l ∈ [M − 1]:\n\nqil = λαil + (1 − λ)βil ≥ λαil+1 + (1 − λ)βil+1 = qil+1\n\n(55)\n\n1 , ..., pn\n\nM } ⊂ ∆n\n\nM such that ∆n\n\nM = H(P n), where H(X ) is the convex hull of the 2. ∀n, ∃P n = {pn set of points X . quick proof : Let i1, ..., iM be the sorted indices associated with an arbitrary subset ∆n M . Construct the M probability vectors as follows: ∀k ∈ [M ] pn k if j ∈ {i1, ..., ik} else pn k,j = 0. It is easy to verify that pn .\nk,iM Since ∆n M ⊆ H(P n), which can be established if we show that ∀α ∈ ∆n k λkpn k . We shall prove it by construction, specifically define:\n\n≥ ... ≥ pn M . What is left is to show that ∆n\n\nM is convex (Claim 1), we thus have that H(P n) ⊆ ∆n\n\nM , ∃λ ∈ ∆M such that α = (cid:80)\n\nk,j = k/k = 1, and pn\n\nM , since (cid:80)\n\nk ∈ ∆n\n\nk,j = 1\n\nj pn\n\nk,i1\n\nλk = k · (αik − αik+1) ≥ 0\n\n(56)\n\nThis induces a valid convex coefficient vector λ, since (cid:80) It is also easy to verify that αil = (cid:80)\n\nk λk = (cid:80) for all indices il ∈ [M ], since:\n\nk(αik −αik+1)·k = (cid:80)\n\nk λkpn\n\nk,il\n\nk αik = 1.\n\nM (cid:88)\n\nk=1\n\nλkpn\n\nk,il\n\n=\n\nM M\n\n· (αiM − 0) +\n\nM − 1 M − 1\n\n· (αiM −1 − αiM ) + ... +\n\nl l\n\n· (αil − αil+1) = αil\n\n(57)\n\nby construction of λ and P n.\n\n3. ∀n, the function g is linear over α ∈ ∆n M . quick proof : Define the maximum index s(α, i) = arg maxj∈{i,...,M }{αj}. By definition, α ∈ ∆n M implies that s(i) = s(α, i) is independent of α. Therefore ∀i ∈ [M ] we have gi(α) = αs(i) − αs(i+1) with the slight abuse of notation αM +1 = 0. Therefore ∃Gn ∈ {−1, 0, 1}M ×M such that g(α) = Gnα for all α ∈ ∆n\n\nM .\n\nCombining observations 1,2&3, we can re-write the original optimization problem as follows:\n\nmin α∈∆M\n\nh(α) =\n\nmin M ∪...∪∆M !\n\nM\n\nα∈∆1\n\nγ⊤g(α)\n\n(cid:27)\n\nγ⊤g(α)\n\nmin α∈∆n M\n\n(cid:26)\n\n(cid:26)\n\n(cid:26)\n\n= min\n\nn∈[M !]\n\n= min\n\nn∈[M !]\n\n(a)= min\n\nn∈[M !]\n\n=\n\nmin n∈[M !],k∈[M ]\n\nγ⊤g(pn k )\n\n19\n\n(cid:27)\n\nmin α∈H(P n)\n\nγ⊤Gnα\n\n(cid:27)\n\nγ⊤Gnp\n\nmin p∈P n\n\n(58)\n\nUnder review as a conference paper at ICLR 2023\n\nwhere (a) holds because the minimum of a linear function over the convex hull of a set of points X is obtained at one of the points in X .\n\nThus, to solve the original optimization problem, we only need to evaluate M ! linear functions with M vectors each, and pick the one that achieves the smallest value. Finally, we will now show that the search space can be significantly reduced from M ! × M to M possible solutions.\n\nLet ∆n {pn with jn\n\nM be an arbitrary subset of ∆M whose associated sorted indices are in k }k are the associated extreme points. We first note that, ∀k ∈ [M ], g(pn\n\n1 , in 2 , ..., in k ) = (cid:2)0 ... 0 1\n\nM , and P n = k 0 ... 0(cid:3)⊤\n\nk = max{in\n\n1 , ..., in\n\nk } is the non-zero index. Therefore, we have that ∀n, k:\n\nh(pn\n\nk ) = γ⊤g(pn\n\nk ) =\n\nk\n\nγjn k\n\n(59)\n\nEquation (59) reveals that, amongst all vectors pn by the subset n whose associated jn to be sorted. Furthermore, the smallest value that jn amongst k arbitrary indices from [M ]. Therefore, let ∆m ik = k, i.e. α ∈ ∆m max{1, ..., k} = k which implies that ∀n ∈ [M !] and ∀k ∈ [M ]:\n\nk with fixed k, the smallest error is always achieved k index is the smallest, since the robust errors are always assumed k can achieve is k, since it is the largest index M be the subset whose sorting indices are k =\n\nM implies α1 ≥ ... ≥ αM . For this subset, we will always have jm\n\nwhere pm\n\nk = (cid:2) 1\n\nk ... 1\n\nk 0 ... 0(cid:3)⊤\n\nh(pn\n\nk ) =\n\nk\n\nγjn k\n\n≥\n\nγk k\n\n= h(pm k )\n\n. Combining (58)&(60) we obtain:\n\nmin α∈∆M\n\nh(α) = min k∈[M ]\n\nγ⊤g(pm\n\nk ) = min\n\nk∈[M ]\n\nγk k\n\n=\n\nγk∗ k∗\n\n(60)\n\n(61)\n\nwhich can be achieved using α∗ = (cid:2) 1\n\nk∗ ... 1\n\nk∗ 0 ... 0(cid:3)⊤\n\n.\n\nB.4.2 MAIN PROOF\n\nWe now restate and prove Theorem 2:\n\nTheorem (Restated). For a perturbation set S, data distribution D, and collection of M classifiers F with individual adversarial risks ηi (i ∈ [M ]) such that 0 < η1 ≤ ... ≤ ηM ≤ 1, we have ∀α ∈ ∆M :\n\nmin k∈[M ]\n\n(cid:111)\n\n(cid:110) ηk k\n\n≤ η(α) ≤ ηM\n\n(62)\n\nBoth bounds are tight in the sense that if all that is known about the setup F, D, and S is {ηi}M i=1, then there exist no tighter bounds. Furthermore, the upper bound is always met if α = eM , and the lower bound (if achievable) can be met if α = (cid:2) 1 k }.\n\n, where m = arg mink∈[M ]{ ηk\n\nm 0 ... 0(cid:3)⊤\n\nm ... 1\n\nProof. We first prove the upper bound and then the lower bound.\n\nUpper bound: From Proposition 1, we have that η is convex in α ∈ ∆M . Using ∆M = H ({e1, ..., eM }) and applying Lemma 2, we get ∀α ∈ ∆M :\n\nη(α) ≤ max α∈∆M\n\nη(α) = max i∈[M ]\n\nη(ei) = ηM\n\n(63)\n\nThis establishes the upper bound in (62). The bound is tight, since η(eM ) = ηM is achievable.\n\nLower bound: From Lemmas 4&5, we establish ∀α ∈ ∆M , the following result:\n\nη(α) ≥\n\nM (cid:88)\n\n(cid:18)\n\ni=1\n\n(ηi − ηi−1) · max\n\nj∈{i,...,M }\n\n{αj}\n\n= h(α) ≥ min α∈∆M\n\n(cid:19)\n\nh(α) = h(α∗) =\n\nηm m\n\n(64)\n\nwhere m = arg mink∈[M ]{ ηk (62).\n\nk } and α∗ = (cid:2) 1\n\nm ... 1\n\nm 0 ... 0(cid:3)⊤\n\n. This establishes the lower bound in\n\nThe bound is tight, since for fixed 0 < η1 ≤ ... ≤ ηM ≤ 1, we can construct F, S, and D such that η(α) = h(α) and ∀i ∈ [M ] : η(ei) = h(ei) = ηi, as shown next.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nLet S ⊂ Rd be any closed and bounded set containing at least M distinct vectors {δj}M j=1 ⊆ S. Let D be any valid distribution over R = Rd × [C] such that ∀i ∈ [M ]: P {z ∈ Ti} = ηi, P {z ∈ TM +1} = 1, and ∅ = T0 ⊂ T1 ⊆ T2 ⊆ ... ⊆ TM ⊆ TM +1 ⊂ R. Finally, we construct classifiers fi (∀i ∈ [M ]) to satisfy the following assignment ∀z ∈ TM +1:\n\nfi(x + δ) = y ∀δ ∈ S \\ {δi} & fi(x + δi) =\n\n(cid:26)y\n\ny′ ̸= y\n\nif (x, y) /∈ Ti otherwise\n\n(65)\n\ni.e., the i-th classifier decision fi(x + δ) is incorrect only if δ = δi and z ∈ Ti.\n\nGiven the above construction, we establish\n\nαi1 {fi(x + δ) ̸= y}\n\n(cid:35)\n\nη(α) = Ez∼D\n\n(cid:90)\n\n(a)=\n\n(cid:34)\n\nmax δ∈S\n\nM (cid:88)\n\ni=1\n\npz(z) ·\n\n(cid:32)\n\nmax δ∈S\n\nM (cid:88)\n\nαi1 {fi(x + δ) ̸= y}\n\ndz\n\n(cid:33)\n\ni=1 (cid:32)\n\npz(z) ·\n\nmax δ∈S\n\nαi1 {fi(x + δ) ̸= y}\n\ndz\n\n(cid:33)\n\nM (cid:88)\n\ni=1\n\nz∈TM +1\n\nM (cid:88)\n\n(cid:90)\n\nk=1\n\nz∈Tk\\Tk−1\n\nM (cid:88)\n\n(cid:90)\n\nz∈Tk\\Tk−1\n\n(b)=\n\n(c)=\n\n=\n\n(d)=\n\nk=1\n\nM (cid:88)\n\nk=1\n\nM (cid:88)\n\nk=1\n\n(cid:18)\n\n(cid:19)\n\npz(z) ·\n\nmax j∈{k,..,M }\n\n{αj}\n\ndz\n\n(cid:35)\n\nmax j∈{k,..,M }\n\n(cid:34)(cid:18)\n\n(cid:20)\n\n(cid:19) (cid:90)\n\n{αj}\n\npz(z) dz\n\nTk\\Tk−1\n\n(cid:21)\n\n(ηi − ηi−1) · max\n\nj∈{k,..,M }\n\n{αj}\n\n= h(α)\n\n(66)\n\nwhere: (a) holds because P {z ∈ TM +1} = 1; (b) holds because we can partition TM +1 into M + 1 sets: T1 ∪ (T2 \\ T1) ∪ . . . ∪ (TM +1 \\ TM ), and because the max term is 0 ∀z ∈ TM +1 \\ TM ; (c) holds by construction of F and S, and (d) holds since ηi = P {z ∈ Ti} and Ti ⊆ Ti+1.\n\nB.5 PROOF OF THEOREM 3\n\nFirst, we state the classic result on the convergence of the projected sub-gradient method for convex minimization (Shor (2012)): Lemma 6 (Projected Sub-gradient Method). Let h : Rd → R be a a convex and sub-differentiable function. Let C ⊂ Rd be a convex set. For iterations t = 1, .., T , define the projected sub-gradient method:\n\nx(t+1) = ΠC\n\n(cid:16)\n\nx(t) − atg(t)(cid:17)\n\nh(t+1)\n\nbest = min\n\n(cid:110)\n\nh(t)\n\nbest, h(x(t+1))\n\n(cid:111)\n\n(67)\n\n(68)\n\nwhere at = a/t for some positive a > 0, x(1) ∈ C is an arbitrary initial guess, h(1) best = h(x(1)), and g(t) ∈ ∂h(x(t)) is a sub-gradient of h at x(t). Let tbest designate the best iteration index thus far. Then, if h has norm-bounded sub-gradients: ∥g∥2 ≤ G for all g ∈ ∂h(x) and x ∈ C, we have:\n\nh(t)\n\nbest − h∗ ≤\n\nwhere:\n\n∥x(1) − x∗∥2 2 (cid:80)t\n\n2 + G2 (cid:80)t k=1 ak\n\nk=1 a2\n\nt\n\nh∗ = h(x∗) = min\n\nx∈C\n\nh(x)\n\n−−−→ t→∞\n\n0\n\n(69)\n\n(70)\n\nWe then prove Theorem 3 (restated below) via a direct application of Lemma 6:\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem (Restated). The OSP algorithm output αT satisfies:\n\n0 ≤ ˆη(αT ) − ˆη(α∗) ≤\n\n∥α(1) − α∗∥2\n\n2 + M a2 (cid:80)T t=1 t−1\n\n2a (cid:80)T\n\nt=1 t−2\n\n−−−−→ T →∞\n\n0\n\n(71)\n\nfor any initial condition α(1) ∈ ∆M , a > 0, where α∗ is a global minimum.\n\nProof. The ensemble empirical adversarial risk ˆη is convex and sub-differentiable (Proposition 1), being minimized over a convex set ∆M . At each iteration t in OSP, the vector g obtained at line (12) M , the vector g is also a sub-gradient of ˆη at α(t), therefore Lemma 6 is norm-bounded with G = applies.\n\n√\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nC ADDITIONAL THEORETICAL RESULTS\n\nC.1 SPECIAL CASE OF THREE CLASSIFIERS\n\nIn this section, we derive a simplified search strategy for finding the optimal sampling probability for the special case of M = 3, akin to Section 4.2. Similar to (7), we can enumerate all possible classifiers/data-point configurations around S, which allows us to write ∀α ∈ ∆3:\n\nη(α) = p1 · max{α1, α2, α3}\n\n+ p2 · max{α1 + α2, α3} + p3 · max{α2 + α3, α1} + p4 · max{α1 + α3, α2} + p5 · max{α1, α2} + p6 · max{α2, α3} + p7 · max{α1, α3} + p8 · α1 + p9 · α2 + p10 · α3 + p11 · 1 + p12 · 0\n\n(72)\n\nwhere p ∈ ∆12. Using (72), we obtain the following result:\n\nTheorem 4. Define A ⊂ ∆3 to be the set of the following vectors:\n\nA =\n\n(cid:35)\n\n(cid:35)\n\n(cid:40)(cid:34)1 0\n0\n\n(cid:35)\n\n(cid:34)0 1\n0\n\n(cid:34)0 0\n1\n\n,\n\n,\n\n(cid:35)\n\n(cid:34)1/2 1/2 0\n\n,\n\n(cid:35)\n\n(cid:34) 0 1/2 1/2\n\n(cid:35)\n\n(cid:34)1/2 0\n1/2\n\n,\n\n,\n\n,\n\n(cid:35)\n\n(cid:34)1/2 1/4 1/4\n\n(cid:34)1/4 1/2 1/4\n\n,\n\n(cid:35)\n\n(cid:35)\n\n(cid:34)1/4 1/4 1/2\n\n,\n\n,\n\n(cid:35)(cid:41)\n\n(cid:34)1/3 1/3 1/3\n\n(73)\n\nThen for any three classifiers f1, f2, and f3, perturbation set S ⊂ Rd, and data distribution D, we have:\n\nmin α∈∆3\n\nη(α) = min α∈A\n\nη(α)\n\n(74)\n\nThe set A is optimal, in the sense that there exist no smaller set A′ such that (74) holds.\n\nTheorem 4 simplifies the search for the optimal sampling probability significantly, as it is sufficient to evaluate η at exactly 10 different candidate solutions, captured by A, and pick the best performing one. Theorem 4 also guarantees that the search procedure is efficient, since every candidate solution in A needs to be evaluated.\n\n3, ..., ∆6\n\nProof. We shall use the same technique used in the proof of Lemma 5. We can decompose ∆3 into 6 such subsets ∆1 3, such that each subset contains vectors that share the same sorting indices. These subsets are convex, and they can be represented as the convex hull of three vectors. Due to the symmetry of the problem, we shall focus on one subset ∆1 3 = 3, we have: α1 ≥ α2 ≥ α3. Notice 3, all the terms in (72) become linear in α, except for the term max{α2 +α3, α1}.\n\n(1, 0, 0)⊤, (1/2, 1/2, 0)⊤, (1/3, 1/3, 1/3)⊤(cid:111)(cid:17)\n\nH that for any α ∈ ∆1 Therefore, we can further decompose ∆1\n\n3 into two convex subsets ∆1,1\n\nwhere ∀α ∈ ∆1\n\n3 , such that:\n\nand ∆1,2\n\n(cid:16)(cid:110)\n\n3\n\n∆1,1\n\n3 = {α ∈ ∆1\n\n3 : α1 ≥ α2 + α3} ∆1,2\n\n3 = {α ∈ ∆1\n\n3 : α1 ≤ α2 + α3}\n\nand η is linear over both subsets (but not their union).\n\nClaim: we have:\n\n∆1,1\n\n3 = H\n\n∆1,2\n\n3 = H\n\n(cid:16)(cid:110)\n\n(cid:16)(cid:110)\n\n(1, 0, 0)⊤, (1/2, 1/2, 0)⊤, (1/2, 1/4, 1/4)⊤(cid:111)(cid:17) (1/3, 1/3, 1/3)⊤, (1/2, 1/2, 0)⊤, (1/2, 1/4, 1/4)⊤(cid:111)(cid:17)\n\nSince both ∆1,1\n\n3\n\nand ∆1,2\n\n3\n\nare convex, it is enough to show that:\n\n∆1,1\n\n3 ⊆ H\n\n∆1,2\n\n3 ⊆ H\n\n(cid:16)(cid:110)\n\n(cid:16)(cid:110)\n\n(1, 0, 0)⊤, (1/2, 1/2, 0)⊤, (1/2, 1/4, 1/4)⊤(cid:111)(cid:17) (1/3, 1/3, 1/3)⊤, (1/2, 1/2, 0)⊤, (1/2, 1/4, 1/4)⊤(cid:111)(cid:17)\n\n(75)\n\n(76)\n\n(77)\n\nfor (76) to hold. For all α ∈ ∆1,1\n\n3 , define:\n\nλ1 = α1 − α2 − α3 ≥ 0,\n\nλ2 = 2 · (α2 − α3) ≥ 0, & λ3 = 4α3 ≥ 0\n\n(78)\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nThen we always have:\n\nα = λ1 ·\n\n(cid:35)\n\n(cid:34)1 0\n0\n\n+ λ2 ·\n\n(cid:35)\n\n(cid:34)1/2 1/2 0\n\n+ λ1 ·\n\n(cid:35)\n\n(cid:34)1/2 1/4 1/4\n\n(79)\n\nwhere it is easy to verify that λ1 + λ2 + λ3 = 1. The same can be shown for any α ∈ ∆1,2 the following:\n\n3 , using\n\nλ1 = 2 · (α2 − α3) ≥ 0,\n\nλ2 = 4 · (α1 − α2) ≥ 0, & λ3 = 3 · (α2 + α3 − α1) ≥ 0\n\n(80)\n\nwhich establishes the claim in (76).\n\nUsing (76) and the linearity of η on each subset, we can write: (cid:41)\n\n(cid:40)\n\nη(α) = min\n\nmin α∈∆1 3\n\nmin α∈∆1,1\n\n3\n\nη(α), min\n\nα∈∆1,2\n\n3\n\nη(α)\n\n(cid:40)\n\n= min\n\nη\n\n(cid:35)(cid:33)\n\n(cid:32)(cid:34)1 0\n0\n\n, η\n\n(cid:35)(cid:33)\n\n(cid:32)(cid:34)1/2 1/2 0\n\n, η\n\n(cid:35)(cid:33)\n\n(cid:32)(cid:34)1/2 1/4 1/4\n\n, η\n\n(cid:32)(cid:34)1/3 1/3 1/3\n\n(cid:35)(cid:33)(cid:41)\n\n(81)\n\nFinally, repeating this procedure for the remainder 5 sets ∆2 3 establishes (74). To show that the set A is minimal, we provide 10 constructions of η using the p vector in (72) such that the ith vector α ∈ A is a unique (amongst A) global optimum of η characterized by the ith p vector (listed below):\n\n3, ..., ∆6\n\n(cid:20)\n\np1 =\n\n0 0 0 0 0 0 0 0\n\n(cid:20)\n\np2 =\n\n0 0 0 0 0 0 0\n\n(cid:20)\n\np3 =\n\n0 0 0 0 0 0 0\n\n1 2\n\n1 2\n\n1 2\n\n0\n\n1 2\n\n(cid:20)\n\np4 =\n\n0 0 0 0\n\n1 2\n\n0 0 0 0\n\n1 2\n\n1 2\n\n0 0\n\n0 0\n\n0 0 0\n\n1 2\n\n0 0\n\n(cid:20)\n\np5 =\n\n0 0 0 0 0\n\n1 2\n\n(cid:20)\n\np6 =\n\n0 0 0 0 0 0\n\n(cid:20)\n\np7 =\n\n0 0\n\n1 2\n\n0 0\n\n1 2\n\n1 2\n\n0\n\n0\n\n1 2\n\n0 0 0 0\n\n1 2\n\n0 0 0\n\n0 0 0 0 0 0\n\n(cid:20)\n\np8 =\n\n0 0 0\n\n1 2\n\n0 0\n\n1 2\n\n0 0 0 0 0\n\n(cid:20)\n\np9 =\n\n0\n\n1 2\n\n0 0\n\n1 2\n\n0 0 0 0 0 0 0\n\n(cid:21)⊤\n\n(cid:21)⊤\n\n(cid:21)⊤\n\n(cid:21)⊤\n\n(cid:21)⊤\n\n(cid:21)⊤\n\n(cid:21)⊤\n\n(cid:21)⊤\n\n(cid:21)⊤\n\np10 = [1 0 0 0 0 0 0 0 0 0 0 0]⊤\n\n(82)\n\nC.2 WORST CASE PERFORMANCE OF DETERMINISTIC ENSEMBLES\n\nIn Section 4.3, we showed via Theorem 2 that the adversarial risk of any randomized ensemble classifier is upper bounded by the worst performing classifier in the ensemble F. In this section, we will show that the same cannot be said regarding deterministic ensemble classifiers. That is, there exist an ensemble F, data distribution D, and perturbation set S such that:\n\nη( ̄f ) > max\n\ni∈[M ]\n\nη(fi)\n\n24\n\n(83)\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ̄f is the deterministic ensemble classifier constructed via the rule:\n\n ̄f (x) = arg max\n\nc∈[C]\n\n(cid:35)\n\n ̃fi(x)\n\n(cid:34) M (cid:88)\n\ni=1\n\nc\n\nConsider the following setup:\n\n1. two binary classifiers in R2:\n\nfi(x) =\n\n(cid:26)1 2\n\ni x ≥ 0\n\nif w⊤ otherwise\n\nwhich can be obtained from the “soft” classifiers: (cid:20) w⊤ i x −w⊤ i x\n\n ̃fi(x) =\n\n(cid:21)\n\n(84)\n\n(85)\n\n(86)\n\nusing fi(x) = arg maxc∈{1,2}[ ̃fi(x)]c, where w1 = [1 1]⊤ and w2 = [1 − 1]⊤.\n\n2. a Ber(p) data distribution D over two data-points in R2 × [2]:\n\nz1 = (x1, y1) =\n\n(cid:21)\n\n(cid:18)(cid:20)−1 2\n\n(cid:19)\n\n, 1\n\nand\n\nz2 = (x2, y2) =\n\n(cid:21) (cid:18)(cid:20)−1 −2\n\n(cid:19)\n\n, 1\n\n(87)\n\n3. the l2 norm-bounded perturbation set S = {δ : ∥δ∥ ≤ ε} for some 0 < ε < 1/√\n\n2.\n\nWe first note that for binary linear classifiers and l2-norm bounded adversaries, we have that:\n\n• the shortest distance between a point x and the decision boundary of linear classifier f with\n\nweight w and bias b is:\n\nζ =\n\n|w⊤x + b| ∥w∥\n\n• if f (x) ̸= y, then the optimal adversarial perturbation is given by:\n\nδ = −sign (cid:0)w⊤x + b(cid:1) εw\n\n∥w∥\n\nWe can now evaluate the adversarial risks of each classifier: (cid:18)\n\n(cid:18)\n\n(cid:19)\n\nη1 = p ·\n\n(cid:16)\n\n= p ·\n\nmax ∥δ∥≤ε (cid:110)\n\n1\n\n1 −\n\n1 (cid:8)w⊤\n\n1 (x1 + δ) < 0(cid:9)\n\n+ (1 − p) ·\n\nmax ∥δ∥≤ε\n\n√\n\n(cid:111)(cid:17)\n\n2ε < 0\n\n+ (1 − p) · (1 {−3 < 0}) = 1 − p\n\n1 (cid:8)w⊤\n\n1 (x2 + δ) < 0(cid:9)\n\n(cid:19)\n\nwhere we use ε < 1/√ 2. Due to symmetry, we also get η2 = p. The average ensemble classifier ̄f constructed from f1 and f2 is defined via the rule:\n\n ̄f (x) =\n\n(cid:26)1 2\n\nif x1 ≥ 0 otherwise\n\nwhose adversarial risk can be computed as follows:\n\n(cid:18)\n\n ̄η = p ·\n\nmax ∥δ∥≤ε\n\n1 {x1,1 + δ1) < 0}\n\n(cid:19)\n\n+ (1 − p) ·\n\n(cid:18)\n\nmax ∥δ∥≤ε\n\n1 {x2,1 + δ1) < 0 < 0}\n\n(cid:19)\n\n= p · (1 {−1 < 0}) + (1 − p) · (1 {−1 < 0}) = p + 1 − p = 1\n\n(88)\n\n(89)\n\n(90)\n\n(91)\n\n(92)\n\nwhich is strictly greater than max{p, 1−p} ∀p ∈ (0, 1). Therefore, we have constructed an example where deterministic ensembling is always worse than using any of the individual classifiers, which proves that deterministic ensemble classifiers do not satisfy the upper bound.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nD ADDITIONAL EXPERIMENTS AND COMPARISONS\n\nD.1 EXPERIMENTAL SETUP\n\nIn this section, we describe the complete experimental setup used for all our experiments.\n\nTraining. All models are trained for 100 epochs via SGD with a batch size of 256 and 0.1 initial learning rate, decayed by 0.1 first at the 50th epoch and twice at the 75th epoch. We employ the recently proposed margin-maximizing cross-entropy (MCE) loss from Zhang et al. (2022) with 0.9 momentum and a weight decay factor of 5 × 10−4. We use 10 attack iterations during training with ε = 8/255 and a step size β = 2/255. For IAT, each classifier is indepdenelty trained from a different random initialization, using a standard PGD adversary. For MRBoost, we use their public implementation from GitHub to reproduce all their results. For BARRE, we use an adaptive PGD (APGD) adversary (discussed in detail in Section D.4) as our training attack algorithm. We apply OSP for To = 10 iterations every Eo = 10 epochs.\n\nTo avoid catastrophic overfitting (Rice et al., 2020), we always save the best performing checkpoint during training. Since all the ensemble methods considered reduce to adversarial training for the first iteration, we use a shared adversarially trained first classifier. Doing so ensures a fair comparison between different ensemble methods. For both CIFAR-10, and CIFAR-100 datasets, we adopt standard data augmentation (random crops and flips). Per standard practice, we apply input normalization as part of the model, so that the adversary operates on physical images x ∈ [0, 1]d.\n\nEvaluation. For all our robust evaluations, we will adopt the state-of-the-art ARC algorithm (Dbouk & Shanbhag, 2022) which can be used for both RECs and single models. Specifically, we use 20 iterations of ARC, with an attack strength ε = 8/255 and approximation parameter G = 2. Following the recommendations of Dbouk & Shanbhag (2022), we use a step size of 2/255 when evaluating single models (M = 1) and a step size of 8/255 when evaluating RECs (M ≥ 2).\n\nD.2\n\nINDIVIDUAL MODEL ROBUSTNESS\n\nIn Tables 3&4, we provide the clean and robust accuracies of all the individual classifiers constructed via the different ensemble methods on CIFAR-10 and CIFAR-100, respectively. Robust accuracy is measured using ARC.\n\nAs expected, only ensembles produced via IAT consist of classifiers achieving near-identical robust and natural accuracies. In contrast, ensembles produced via MRBosst or BARRE witness a degradation in individual classifier robust accuracy as the ensemble size grows. However, since MRBoost was not initially designed for randomized ensemble classifiers, this degradation in robust accuracy can be rather severe as seen for MobileNetV1 in both Tables 3&4. This explains why, for such ensembles, the optimal sampling probability obtained for the constructed REC completely disregards the last classifier as highlighted in Section 5.1.\n\nTable 3: Natural and robust accuracies of the individual classifiers of all ensembles methods trained on CIFAR-10 (from Table 2). Robust accuracy is measured against an l∞ norm-bounded adversary using ARC with ε = 8/255.\n\nNetwork\n\nMethod\n\nResNet-20\n\nMobileNetV1\n\nResNet-18\n\nIAT MRBoost BARRE\n\nIAT MRBoost BARRE\n\nIAT MRBoost BARRE\n\nf1\n\nAnat\n\nArob\n\n73.18\n\n41.99\n\n79.01\n\n46.22\n\n80.96\n\n48.72\n\nAnat\n\n73.42 76.00 76.08\n\n79.17 80.11 80.15\n\n80.64 84.01 84.35\n\nf2\n\nf3\n\nf4\n\nf5\n\nAnat\n\n74.44 76.59 77.40\n\n79.05 77.54 79.43\n\n81.24 83.67 84.89\n\nArob\n\n42.25 39.60 39.87\n\n46.60 42.03 42.67\n\n48.83 45.72 45.86\n\nAnat\n\n74.27 77.25 77.12\n\n78.44 77.94 79.56\n\n81.13 83.88 83.88\n\nArob\n\n42.06 38.38 39.07\n\n46.11 39.36 39.65\n\n48.70 43.38 43.09\n\nAnat\n\n74.17 76.43 77.60\n\n78.76 68.89 79.60\n\n− −\n−\n\nArob\n\n42.14 36.62 37.01\n\n46.74 33.40 38.28\n\n− −\n−\n\nArob\n\n41.94 41.42 41.18\n\n46.21 44.52 44.56\n\n48.23 47.56 46.48\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Natural and robust accuracies of the individual classifiers of all ensembles methods trained on CIFAR-100 (from Table 2). Robust accuracy is measured against an l∞ norm-bounded adversary using ARC with ε = 8/255.\n\nNETWORK\n\nMETHOD\n\nRESNET-20\n\nMOBILENETV1\n\nRESNET-18\n\nIAT MRBOOST BARRE\n\nIAT MRBOOST BARRE\n\nIAT MRBOOST BARRE\n\nf1\n\nAnat\n\nArob\n\n38.34\n\n17.69\n\n51.87\n\n23.45\n\n53.85\n\n24.15\n\nf2\n\nf3\n\nf4\n\nf5\n\nAnat\n\n38.64 41.69 41.57\n\n51.46 53.96 52.75\n\n53.85 54.78 55.21\n\nArob\n\n17.68 17.29 18.22\n\n23.01 22.63 22.90\n\n24.17 22.28 22.26\n\nAnat\n\n38.40 42.69 42.96\n\n50.61 53.45 53.61\n\n54.80 47.49 55.69\n\nArob\n\n17.89 17.67 17.24\n\n23.00 20.48 21.21\n\n24.30 16.28 21.05\n\nAnat\n\n39.13 42.92 42.69\n\n51.21 52.55 54.31\n\n54.71 48.13 53.73\n\nArob\n\n17.63 17.44 17.14\n\n23.40 19.90 18.67\n\n24.50 15.98 17.99\n\nAnat\n\n38.36 42.83 43.72\n\n51.89 38.88 51.99\n\n− −\n−\n\nArob\n\n18.13 16.11 16.30\n\n23.56 11.34 18.02\n\n− −\n−\n\nD.3 ATTACKS FOR RANDOMIZED ENSEMBLES\n\nGiven a data-point z = (x, y) and a potentially random classifier f , the goal of an adversary is to find an adversarial perturbation that maximizes the single-point expected adversarial risk:\n\nδ∗ = arg max δ:∥δ∥p≤ε\n\nr(z, δ) = arg max δ:∥δ∥p≤ε\n\nEf [1 {f (x + δ) ̸= y}] = arg max\n\nP {f (x + δ) ̸= y}\n\n(93)\n\nδ:∥δ∥p≤ε\n\nwhere we adopt the lp norm-bounded adversary for the remainder of this section.\n\nProjected gradient descent (PGD) (Madry et al., 2018) is perhaps the most popular attack algorithm for solving (93) for the case of differentiable deterministic classifiers. Specifically, given a surrogate loss function l, such as the cross-entropy loss, PGD finds an adversarial δ iteratively via the following:\n\nδ(k) = Πp\n\nε\n\n(cid:16)\n\nδ(k−1) + ημp\n\n(cid:16)\n\n∇xl\n\n(cid:16) ̃f\n\n(cid:16)\n\nx + δ(k−1)(cid:17)\n\n, y\n\n(cid:17)(cid:17)(cid:17)\n\n(94)\n\nwhere μp is the lp steepest direction projection operator, and Πp ball of radius ε.\n\nε is the projection operator on the lp\n\nIn order to adapt PGD for evaluating randomized ensemble classifiers, Pinot et al. (2020) first proposed adaptive PGD (APGD-L) using the expectation-over-transformation (EOT) method (Athalye\n\nTable 5: Comparing the success of different attack algorithms at fooling various RECs using l∞ norm-bounded attacks with ε = 8/255 on CIFAR-10. All the RECs are constructed with equiprobable sampling.\n\nNetwork\n\nMethod\n\nAPGD-L APGD-S ARC ARC-R\n\nResNet-20\n\nMobileNetV1\n\nResNet-18\n\nIAT MRBoost BARRE\n\nIAT MRBoost BARRE\n\nIAT MRBoost BARRE\n\n49.34 49.61 49.75\n\n52.91 51.02 51.94\n\n54.49 54.23 54.07\n\n46.73 47.74 48.05\n\n50.68 49.37 51.16\n\n52.42 53.19 53.62\n\n45.77 46.66 47.35\n\n49.57 48.05 49.91\n\n51.43 51.82 52.13\n\n49.31 49.65 49.79\n\n52.94 51.19 52.16\n\n54.50 54.51 54.52\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The robust (left) and natural (right) accuracies of an REC of two ResNet-20’s trained on CIFAR-10 using BARRE vs. the training epochs of the second classifier f2, where the first classifier f1 is pre-adversarially trained. Robust and natural accuracies are reported on the test set, using l∞ norm-bounded adversaries with ARC and ε = 8/255.\n\net al., 2018), which uses (94) with the expected loss function as follows:\n\nδ(k) = Πp\n\nε\n\n(cid:16)\n\nδ(k−1) + ημp\n\n(cid:16)\n\n∇xE\n\n(cid:104)\n\n(cid:16) ̃f\n\n(cid:16)\n\nx + δ(k−1)(cid:17)\n\nl\n\n, y\n\n(cid:17)(cid:105)(cid:17)(cid:17)\n\n(95)\n\nNote that the discrete nature of randomized ensembles allows for an exact computation of the expectation in (95).\n\nRecently, Zhang et al. (2022) proposed a stronger version of adaptive PGD, where the expectation is taken at the softmax level (APGD-S). Using APGD-S, Zhang et al. (2022) were able to compromise the BAT defense. Independently, Dbouk & Shanbhag (2022) studied the effectiveness of EOTbased adaptive attacks for evaluating the robustness of RECs, and concluded that such methods are fundamentally ill-suited for the task. Instead, they proposed the ARC attack (Algorithm 2 of (Dbouk & Shanbhag, 2022)), which relied on iteratively updating the perturbation based on estimating the direction towards the decision boundary of each classifier and using an adaptive step size method.\n\nIn this section, we propose a small modification to ARC (ARC-R) that proves to be quite more effective in the equiprobable setting. Specifically, instead of looping over the classifiers in a deterministic fashion based on the order of the sampling probability vector, we propose using a randomized order loop. This ensures that ARC is never biased towards certain classifiers. In fact, Table 5 demonstrates that ARC-R is better than APGD-L (Pinot et al., 2020), APGD-S Zhang et al. (2022), and ARC (Dbouk & Shanbhag, 2022) at evaluating the robustness of RECs on CIFAR-10, constructed with equiprobable sampling across various network architectures and ensemble training methods. Hence, we shall adopt this version of ARC for all our experiments.\n\nD.4 ARC VS. ADAPTIVE PGD FOR BARRE\n\nAs highlighted in Section 5.1, we find that ARC, despite being the strongest adversary, leads to poor performance when adopted as the training attack in BARRE. In this section, we investigate this phenomenon, as we study the performance of BARRE using two different attacks during training, APGD (Zhang et al., 2022) and ARC (Dbouk & Shanbhag, 2022). Specifically, we train two RECs on CIFAR-10 using the ResNet-20 architecture. Both RECs share the same first classifier f1, which is adversarially trained using standard PGD. The second classifier f2 is trained via either APGD or ARC.\n\nFigure 4 plots the evolution of both robust and clean accuracies of the two RECs across the 100 training epochs of f2, measured on the test set. Note that in both RECs, the robust accuracy is evaluated via the stronger ARC adversary. When evaluated on clean images, we find that BARRE with\n\n28\n\n020406080100Training Epochs343638404244Robust Accuracy [%]020406080100Training Epochs68707274767880Natural Accuracy [%]Train with ARCTrain with APGDUnder review as a conference paper at ICLR 2023\n\nARC leads to significantly more accurate RECs when compared to BARRE with APGD. However, this comes at the expense of robust accuracy, as the REC obtained via BARRE with ARC is much more vulnerable than the APGD counterpart. We hypothesize that the adversarial samples generated via ARC during training do not generalize well to the test set. This explains why we observe that the REC obtained via BARRE with ARC achieves much higher robust accuracies on the training set. Thus, for better generalization performance, we shall adopt adaptive PGD during training in all our experiments.\n\nD.5 ADDITIONAL RESULTS\n\nIn this section, we complete the CIFAR-10 results reported in Table 1 for showcasing the benefit of randomization. Specifically, Table 6 provides further evidence that BARRE can train RECs of competitive robustness compared to MRBoost-trained deterministic ensembles, while requiring significantly less compute.\n\nTable 6: Comparison between BARRE and MRBoost across different network architectures and ensemble sizes on CIFAR-100. Robust accuracy is measured against an l∞ norm-bounded adversary using ARC with ε = 8/255.\n\nNetwork\n\nMethod\n\nResNet-20\n\nMobileNetV1\n\nResNet-18\n\nMRBoost BARRE\n\nMRBoost BARRE\n\nMRBoost BARRE\n\nM = 1 Arob\n\nAnat\n\nFLOPs\n\n38.34\n\n17.69\n\n81 M\n\n51.87\n\n23.45\n\n312 M\n\n53.85\n\n24.15\n\n1.1 B\n\nAnat\n\n41.08 39.95\n\n54.41 52.31\n\n55.83 54.53\n\nM = 2 Arob\n\n19.38 19.13\n\n25.73 24.96\n\n25.99 25.37\n\nFLOPs\n\nAnat\n\n162 M 42.60 81 M 40.96\n\n624 M 54.91 312 M 52.74\n\n2.2 B 55.39 1.1 B 54.92\n\nM = 3 Arob\n\n20.48 19.85\n\n26.63 25.75\n\n26.09 25.76\n\nFLOPs\n\nAnat\n\n243 M 43.62 81 M 41.40\n\n936 M 55.03 312 M 53.14\n\n3.3 B 55.80 1.1 B 54.63\n\nM = 4 Arob\n\n21.36 21.41\n\n26.97 27.12\n\n26.50 26.90\n\nFLOPs\n\n324 M 81 M\n\n1.2 B 312 M\n\n4.4 B 1.1 B\n\n29",
    "reference": "# Summary Of The Paper\n\nThe paper considers the problem of adversarial robustness. For this problem the paper investigates the usefulness of randomized ensemble classifiers (REC) where one classifier is randomly selected from the ensemble during inference.  The main motivation behind considering RECs over deterministic ensembles is that the former has much smaller inference time than the latter. \n\nThe paper makes two main contributions. The first contribution, which is on the theory front, involves careful characterization of the adversarial risk of RECs. In particular, the authors obtain reasonably tight upper and lower bounds for the adversarial risk of RECs that depend on the adversarial risks of the component classifiers in the ensemble. Based on these bounds, the authors provide some useful insights on how to design RECs with good adversarial risk guarantees. The second contribution is to provide a boosting style algorithm (BARRE) for constructing robust RECs that can tolerate adversarial attacks. The algorithm is mostly inspired by a recently proposed robust boosting technique called MRBoost.\n\n# Strength And Weaknesses\n\nStrengths:\n  *  Randomized ensembling is an interesting defense strategy that is under-explored in the literature. While there are some works that provide algorithms for building robust RECs, they are easily broken by adaptive defenses. So there is a need for better randomized ensembling strategies. The paper takes a step towards solving this problem. The theoretical results provided in the paper on bounding the adversarial risk of RECs, are novel and interesting. However, it is not immediately clear how the insights gained from these theoretical results can be used to develop a good algorithm (more on this below).\n  *  The paper is easy to read. The clarity and presentation is good. The proofs are elementary and easy to follow.\n\nWeaknesses:\n  *  Motivation: \n      - The authors motivate the paper by saying that randomized ensembles can be computationally more efficient than deterministic ensembles. But there is a drawback of randomized ensembles that hasn't been brought up in the paper. In many application domains (e.g., healthcare), it is important to have deterministic predictions (otherwise it is hard to trust and understand these complex models). By moving to randomized ensembles, we lose this property. Can the authors provide some concrete use cases for randomized ensembles?\n      -  Low compute resource devices also tend to have low memory. But RECs have high memory requirement ( same as deterministic ensembles). Given this, and the fact that deterministic ensembling techniques (like MRBoost) have better performance than BARRE, it is not entirely clear what the practical applications of RECs could be.  \n\n  * Empirical results:  \n     - the empirical results look weak. For example, in table 1, MRBoost has better performance than BARRE both in terms of robust and standard accuracy. This seems to be in contrast with the message one gets by reading the theory section in the paper. For example in page 6 (in paragraph titled \"implications of upper bound\"), it is mentioned that deterministic ensembles have much worse performance in the worst-case. Why is there this mismatch between theory and experiments? \n         - under what circumstances are deterministic ensembles better than RECs and vice-versa? \n\n * BARRE: \n    - the algorithm looks almost identical to MRBoost, except for line 12 in Algorithm 2. In BARRE, the weights for each component classifier in the ensemble are recomputed after every boosting iteration. Whereas in MRBoost, all the component classifiers are given equal weights. Given this, why can't we simply add step 12 to MRBoost and get a randomized classifier out of it? How would the resulting algorithm compare with BARRE?\n    - What is the computational overhead of BARRE over MRBoost?  It looks like computing weights (line 12) can be expensive, especially for large M.\n    - In the introduction the authors claim that BARRE is based on the theoretical results in the paper (page 2). But I don't see any connection between the two. In particular, I don't see how the theoretical results in sections 3.2, 3.3 are used to derive this algorithm.  \n\n *  Theoretical Results: \n    -  A number of insights on how to obtain robust RECs have been provided in the discussion after theoretical results. Can these insights be used to derive a better algorithm than BARRE?\n\n * Minor comments:\n    -  a more detailed explanation on why K=5 in section 3.2 would be helpful to the readers\n    -\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee comments above\n\n# Summary Of The Review\n\nThe clarity and presentation in the paper are good. The theoretical results are interesting and novel. But their usefulness is a little bit unclear. The empirical results look weak. Moreover, the proposed algorithm looks identical to MRBoost, except for a minor step which involves setting the weights of the component classifiers in the ensemble. Given this, I'm a little bit inclined towards rejecting the paper. But I'm happy to upgrade my score if the authors address my concerns.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPROMETHEUS: ENDOWING LOW SAMPLE AND COMMUNICATION COMPLEXITIES TO CONSTRAINED DECENTRALIZED STOCHASTIC BILEVEL LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn recent years, constrained decentralized stochastic bilevel optimization has become increasingly important due to its versatility in modeling a wide range of multi-agent learning problems, such as multi-agent reinforcement learning and multi-agent meta-learning with safety constraints. However, one under-explored and fundamental challenge in constrained decentralized stochastic bilevel optimization is how to achieve low sample and communication complexities, which, if not addressed appropriately, could affect the long-term prospect of many emerging multi-agent learning paradigms that use decentralized bilevel optimization as a bedrock. In this paper, we investigate a class of constrained decentralized bilevel optimization problems, where multiple agents collectively solve a nonconvexstrongly-convex bilevel problem with constraints in the upper-level variables. Such problems arise naturally in many multi-agent reinforcement learning and meta learning problems. In this paper, we propose an algorithm called Prometheus (proximal tracked stochastic recursive estimator) that achieves the first O((cid:15)−1) results in both sample and communication complexities for constrained decentralized bilevel optimization, where (cid:15) > 0 is a desired stationarity error. Collectively, the results in this work contribute to a theoretical foundation for low sample- and communicationcomplexity constrained decentralized bilevel learning.\n\n1\n\nINTRODUCTION\n\nIn recent years, the problem of constrained decentralized bilevel optimization has attracted increasing attention due to its foundational role in many emerging multi-agent learning paradigms with safety or regularization constraints. Such applications include, but are not limited to, safety-constrained multiagent reinforcement learning for autonomous driving (Bennajeh et al., 2019), sparsity-regularized multi-agent meta-learning (Poon & Peyré, 2021), and rank-constrained decentralized matrix completion for recommender systems (Pochmann & Von Zuben, 2022), etc. As its name suggests, a defining feature of constrained decentralized bilevel optimization is “decentralized,” which implies that the problem needs to be solved over a network without any coordination from a centralized server. As a result, all agents must rely on communications to reach a consensus on an optimal solution. Due to the potentially unreliable network connections and the limited computation capability at each agent, such network-consensus approaches for constrained decentralized bilevel optimization typically call for low sample and communication complexities. To date, however, none of the existing works on sample- and communication-efficient decentralized bilevel optimization in the literature considered domain constraints (e.g., Gao et al. (2022); Yang et al. (2022); Lu et al. (2022); Chen et al. (2022b) and Section 2 for detailed discussions). In light of the growing importance of constrained decentralized bilevel optimization, our goal in this paper is to fill this gap by developing sample- and communication-efficient consensus-based algorithms that can effectively handle domains constraints.\n\nSpecifically, this paper focuses on a class of constrained decentralized multi-task bilevel optimization problems, where we aim to solve a decentralized nonconvex-strongly-convex bilevel optimization problem with i) multiple lower-level problems and ii) consensus and domain constrains on the upper level. Such problems naturally arise in security-constrained bi-level model for integrated natural gas and electricity system (Li et al., 2017), multi-agent actor-critic reinforcement learning (Zhang et al., 2020) and constraint meta-learning (Liu et al., 2019). In the optimization literature, a natural\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\napproach for handling domain constraints is the proximal operator. However, as will be shown later, proximal algorithm design and theoretical analysis for constrained decentralized bilevel optimization problems is much more complicated than those of unconstrained counterparts and the results are very limited. In fact, in the literature, the proximal operator for constrained bilevel optimization has been under-explored even in the single-agent setting, not to mention the more complex multi-agent settings. The most related works in terms of handling domain constraints can be found in (Hong et al., 2020; Chen et al., 2022a; Ghadimi & Wang, 2018), which rely on direct projected (stochastic) gradient descent to solve the constrained bilevel problem. In contrast, our work considers general domain constraints that require evaluation of proximal operators in each iteration. Also, these works only considered the single-agent setting, and hence their techniques are not implementable over networks. Actually, up until this work, it is unclear how to design proximal algorithms to handle domain constraints for decentralized bilevel optimization. Moreover, it is worth noting that existing methods for hyper-gradient approximation in both single- and multi-agent bilevel optimization are either based on first-order Taylor-type approximation approaches (Khanduri et al., 2021; Ghadimi & Wang, 2018; Hong et al., 2020), implicit differentiation (Ghadimi & Wang, 2018; Gould et al., 2016; Ji et al., 2021), or iterative differentiation (Franceschi et al., 2017; Maclaurin et al., 2015; Ji et al., 2021), all of which suffer from high communication and sample complexities that are problematic in decentralized settings over networks.\n\nThe main contribution of this paper is that we propose a series of new proximal-type algorithmic techniques to overcome the challenges mentioned above and achieve low sample and communication complexities for constrained decentralized bilevel optimization problem. The main technical contributions of this work are summarized below:\n\n• We propose a decentralized optimization approach called Prometheus (proximal tracked stochastic recursive estimator), which is a cleverly designed hybrid algorithm that integrates proximal operations, recursive variance reduction, lower-level gradient tracking, and upper-level consensus techniques. We show that, to achieve an (cid:15)-stationary point, Prometheus enjoys a convergence rate of O(1/T ), where T is the maximum number of iterations. This implies O((cid:15)−1) communication complexity and O(\n\nnK(cid:15)−1 + n) sample complexity per agent.\n\n√\n\n• We propose a new hyper-gradient estimator for the upper-level function, which leads to a far more accurate stochastic estimation than the conventional stochastic estimator used in (Khanduri et al., 2021; Ghadimi & Wang, 2018; Hong et al., 2020; Liu et al., 2022). We show that our new hyper-gradient stochastic estimator has a smaller variance and outperforms existing estimators both theoretically and experimentally. We note that our proposed estimator could be of independent interest for other bilevel optimization problems.\n\n• We reveal an interesting insight that the variance reduction in Prometheus is not only sufficient but also necessary in the following sense: a “non-variance-reduced” special version of Prometheus could only achieve a much slower O(1/ T ) convergence to a constant error-ball rather than an (cid:15)-stationary point with arbitrarily small (cid:15)-tolerance. This insight advances our understanding and state of the art of algorithm design for constrained decentralized bilevel optimization.\n\n√\n\nThe rest of the paper is organized as follows. In Section 2, we review related literature. In Section 3, we provide the preliminaries of the decentralized bilevel optimization problem. In Section 4, we provide details on our proposed Prometheus algorithm, including the convergence rate, communication complexity, and sample complexity results. Section 5 provides numerical results to verify our theoretical findings and Section 6 concludes this paper.\n\n2 RELATED WORK\n\nIn this section, we first provide a quick overview of the state-of-the-art on single-agent constrained bilevel optimization as well as decentralized bilevel optimization.\n\n1) Constrained Bilevel Optimization in the Single-Agent Setting: As mentioned in Section 1, various techniques have been proposed to solve single-agent bilevel optimization, such as utilizing full-gradient-based techniques (e.g., AID-based methods (Rajeswaran et al., 2019; Franceschi et al., 2018; Ji et al., 2021), ITD-based methods (Pedregosa, 2016; Maclaurin et al., 2015; Ji et al., 2021)), stochastic gradient-based techniques (Ghadimi & Wang, 2018; Khanduri et al., 2021; Guo & Yang, 2021), STORM-based techniques (Cutkosky & Orabona, 2019), and VR-based techniques (Yang\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\net al., 2021). However, none of these existing works have considered domain constraints. To our knowledge, the only works that considered domain constraints in the single-agent setting can be found in (Hong et al., 2020; Chen et al., 2022a; Ghadimi & Wang, 2018). In (Ghadimi & Wang, 2018), the authors proposed a double-loop algorithm called BSA, where in the inner loop the lower level problem is solved to sufficient accuracy, while in the outer loop projected (stochastic) gradient descent is utilized to update the model parameters. The double-loop structure of BSA led to slow convergence. In (Hong et al., 2020), a two-timescale single loop stochastic approximation (TTSA) algorithm based on projected (stochastic) gradient descent was proposed to solve the constrained bilevel optimization problems. However, TTSA has to choose step-sizes of different orders for the upper and lower level problems to ensure convergence, which leads to suboptimal complexity results. Later in (Chen et al., 2022a), an algorithm called STABLE algorithm is proposed to utilize a momentum-based gradient estimator and combines the Moreau-envelop-based analysis to achieve an O((cid:15)−2) sample-complexity. As mentioned in Section 1, however, the methods in (Ghadimi & Wang, 2018; Hong et al., 2020; Chen et al., 2022a) consider only simple constraints. Moreover, the aforementioned methods are not applicable in the decentralized setting.\n\n2) Decentralized Bilevel Optimization: Decentralized bilevel optimization has also received increasing attention in recent years. For example, Yang et al. (2022), Lu et al. (2022) and Chen et al. (2022b) respectively proposed stochastic gradient (SG)-type decentralized algorithms for bilevel optimization and achieve an O((cid:15)−2) sample-communication complexity. The VRDBO method in (Gao et al., 2022) employed the momentum-based techniques for decentralized bilevel optimization to achieve better O((cid:15)−1.5) complexity results. However, VRDBO updates upper- and lower-level variables in an alternating fashion. As will be shown later, our Prometheus algorithm updates upper-level and lower-level variables simultaneously, which renders a much lower implementation nK(cid:15)−1 + n) sample complexities, complexity than VRDBO. Besides, Prometheus achieves O( which is a near-optimal sample complexity and outperforms existing decentralized bilevel algorithms. It is worth noting that, the in aforementioned works, consensus requirements exist on both lowerand upper-level subproblems. To certain extent, such a formulation can be viewed as multiple agents collaboratively solving the same bilevel optimization problem. In contrast, our work only has a consensus requirement in the upper-level subproblem, which implies multiple different lower-level tasks. This is more practically-relevant and a more appropriate formulation for multi-agent reinforcement learning, multi-agent meta-learning, etc. We note that the most related work on decentralized bilevel optimization is (Liu et al., 2022), which also considered multiple lower-level tasks. However, the INTERACT method in (Liu et al., 2022) is unconstrained and cannot handle non-smooth objectives considered in our work. In contrast, we propose a special proximal operator ̃xi,t to address this challenge. Last but not least, we note that none of the aforementioned works on decentralized bilevel optimization took domain constraints into consideration. For clearer comparisons, we summarize and compare the complexity results of all algorithms mentioned above in Table 1.\n\n√\n\n3 PROBLEM FORMULATION AND MOTIVATING APPLICATIONS\n\n1) Network Consensus Formulation for Decentralized Bilevel Optimization: Consider an undirected connected network G = (N , L) that represents a peer-to-peer network, where N and L are the sets of agents (nodes) and edges, respectively, with |N | = m. Each agent i has local computation capability and can share information with its neighboring agents denoted as Ni (cid:44) {i(cid:48) ∈ N : (i, i(cid:48)) ∈ L}. Each agent i has access to a local dataset of size n. All agents in the network collaboratively solve the following constrained decentralized bilevel optimization problem:\n\nmin xi∈X\n\n1 m\n\nm (cid:88)\n\ni=1\n\n[(cid:96)(xi) + h(xi)] (cid:44) 1\n\nmn\n\nm (cid:88)\n\nn (cid:88)\n\n[f (cid:0)xi, y∗\n\ni (xi; ̄ξij)(cid:1) + h(xi)]\n\ni=1\n\nj=1\n\ns.t. y∗\n\ni (xi) = arg min yi∈Rp2\n\ng(xi, yi) (cid:44) 1\n\nn\n\nn (cid:88)\n\nj=1\n\ng(xi, yi; ζij), ∀i; xi = xi(cid:48),\n\nif (i, i(cid:48)) ∈ L,\n\n(1)\n\n(cid:80)n\n\nwhere X ⊆ Rp1 is a convex constraint set, and xi ∈ X and yi ∈ Rp2 are parameters to be trained for the upper-level and lower-level subproblems at agent i, respectively. Here, (cid:96)(xi) (cid:44) f (xi, y∗ i (xi)) = (cid:1) is the local objective function, and h(xi) is a convex proximal function 1\nn (possibly non-differentiable) for regularization. The equality constraints xi = xi(cid:48) ensure that the local copies at connected agents i and i(cid:48) are equal to each other, hence the name “consensus form.”\n\nj=1 f (cid:0)xi, y∗\n\ni (xi); ̄ξij\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparisons among algorithms for bilevel optimization problems. Sample complexities (both upper and lower) as defined in the sense of achieving an (cid:15)-stationary point defined in (2), n is the size of dataset at each agent.\n\nAlgorithms\n\nConstriants\n\nSamp. Complex.\n\nComm. Complex.\n\nDecentralized\n\nBSA (Ghadimi & Wang, 2018)\n\nSUSTAIN (Khanduri et al., 2021)\n\nRSVRB (Guo & Yang, 2021)\n\nVRBO (Yang et al., 2021)\n\nAID-BiO /ITD-BiO Ji et al. (2021)\n\nTTSA (Hong et al., 2020)\n\nSTABLE (Chen et al., 2022a)\n\nDSBO (Yang et al., 2022)\n\nSPDB (Lu et al., 2022)\n\nDSBO (Chen et al., 2022b)\n\nVRDBO (Gao et al., 2022)\n\nINTERACT (Liu et al., 2022)\n\nINTERACT-VR Liu et al. (2022)\n\nPrometheus [Ours.]\n\n(cid:51)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:51)\n\n(cid:51)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:51)\n\nO((cid:15)−2)\n\n ̃O((cid:15)−1.5)\n\nO((cid:15)−1.5)\n\nO((cid:15)−1.5)\n\nO(n(cid:15)−1)\n\nO((cid:15)−5/2)\n\nO((cid:15)−2)\n\nO((cid:15)−2)\n\nO((cid:15)−2)\n\nO((cid:15)−2)\n\nO((cid:15)−1.5)\n\nO(n(cid:15)−1)\n\n√\n\nO(\n\n√\n\nO(\n\nnK(cid:15)−1 + n)\n\nnK(cid:15)−1 + n)\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\nO((cid:15)−2)\n\nO((cid:15)−2)\n\nO((cid:15)−2)\n\nO((cid:15)−1.5)\n\nO((cid:15)−1)\n\nO((cid:15)−1)\n\nO((cid:15)−1)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:55)\n\n(cid:51)\n\n(cid:51)\n\n(cid:51)\n\n(cid:51)\n\n(cid:51)\n\n(cid:51)\n\n(cid:51)\n\nNext, we define the notion of (cid:15)-stationarity point for Problem (1) for convergence performance characterization. We say that {xi, yi, ∀i ∈ [m]} is an (cid:15)-stationarity point if it satisfies:\n\nE(cid:107) ̃x − 1 ⊗ ̄x(cid:107)2 (cid:125) (cid:123)(cid:122) (cid:124) Saddle point error\n\n+ E(cid:107)x − 1 ⊗ ̄x(cid:107)2 (cid:125) (cid:123)(cid:122) (cid:124) Consensus error\n\n+ E(cid:107)y − y∗(cid:107)2 (cid:125)\n\n(cid:123)(cid:122) lower problem error\n\n(cid:124)\n\n≤ (cid:15),\n\n(2)\n\n(cid:80)m\n\nm\n\n1 , ...y(cid:62)\n\ni=1 xi, y (cid:44) [y(cid:62)\n\n(cid:62)](cid:62), and ̃x is a proximal point m](cid:62), and y∗ (cid:44) [y∗ where ̄x (cid:44) 1 that will be defined later in Section 4. The first term in (2) quantifies the convergence of the ̄x to a proximal point of stationarity of the global objective. The second term in (2) measures the consensus error among local copies of the upper variable, while the last term in (2) quantifies the (aggregated) error in the lower problem’s iterates across all agents. Thus, (cid:15) → 0 implies that the algorithm achieves three goals simultaneously: i) consensus of upper variables, ii) stationary point of Problem (1), and iii) solution to the lower problem. As mentioned in Section 1, two of the most important performance metrics in decentralized optimization are the sample and communication complexities.\n\n(cid:62), ...y∗\n\nm\n\n1\n\n2) Motivating Applications: Problem (1) arises naturally from many interesting real-world applications. Here, we present two motivating applications to showcase its practical relevance: • Multi-agent meta-learning (Rajeswaran et al., 2019): Meta-learning (or learning to learn) is to find model that can adapt to multiple related tasks. A popular meta-learning framework is the model-agnostic meta learning (MAML), which minimizes an upper objective of empirical risk on all tasks. Consider a multi-agent meta-learning task with m lower level problems and m agents collectively solve this meta-learning problem over a network. This problem can be formulated as:\n\nmin x∈X\n\nm (cid:88)\n\ni=1\n\nf (x, y∗\n\ni (x)) , s.t. y∗\n\ni (x) ∈ argmin yi∈Rp2\n\ng (x, yi) , i = 1, . . . , m.\n\n(3)\n\nHere, agent i has a local dataset with n samples, x ∈ X is the constrained (e.g., due to safety) model parameters shared by all agents, and yi are task-specific parameters solved by each agent. • Decentralized min-max optimization (Huang et al., 2022): Another application of the constrained decentralized bilevel optimization in (1) is the decentralized nonconvex strongly-concave min-max optimization problem, which is typically seen in, e.g., multi-agent reinforcement learning (Zhang et al., 2021), fair multi-agent machine learning (Baharlouei et al., 2019), and data poisoning attack (Liu et al., 2020b). A decentralized min-max optimization problem is a special case of a decentralized bilevel optimization problem because:\n\nmin x∈X\n\nmax yi∈Rp2 i=1,...,m\n\nm (cid:88)\n\ni=1\n\nf (x, yi) ⇐⇒ min x∈X\n\nm (cid:88)\n\ni=1\n\nf (x, yi (x∗)) , s.t. y∗\n\ni (x) = argmin yi∈Rp2\n\n−f (x, yi) , ∀i.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n4 SOLUTION APPROACH\n\nIn this section, we first present the Prometheus algorithm for solving the constrained decentralized bilevel optimization problems in Problem (1) in Sections 4.1–4.2. Then, we provide its theoretical convergence guarantees in Section 4.3. Lastly, we will reveal a key insight on the necessity of using the proposed variance reduction techniques in Section 4.4. Due to space limitation, we relegate the proofs and the notation Table. 2 to supplementary material.\n\n4.1 PRELIMINARIES To present the Prometheus algorithm, we first introduce several basic components as preparation.\n\n1) Network-Consensus Matrix: Our Prometheus algorithm is based on the network-consensus mixing approach: in each iteration, every agent exchanges and aggregates neighboring information through a consensus weight matrix M ∈ Rm×m. We define λ as the second largest eigenvalue of the matrix M. Let [M]ii(cid:48) represent the element in the i-th row and the i(cid:48)-th column in M. The choice of M should satisfy the following properties: (a) doubly stochastic: (cid:80)m j=1[M]ii(cid:48) = 1; (b) symmetric: [M]ii(cid:48) = [M]i(cid:48)i, ∀i, i(cid:48) ∈ N ; and (c) network-defined sparsity: [M]ii(cid:48) > 0 if (i, i(cid:48)) ∈ L; otherwise [M]ii(cid:48) = 0, ∀i, i(cid:48) ∈ N .\n\ni=1[M]ii(cid:48) = (cid:80)m\n\n2) Stochastic Estimators: In Prometheus, we need to estimate the stochastic gradient of the bilevel problem using the implicit function theorem. We note that in the literature of bilevel optimization with stochastic gradient, a commonly adopted stochastic gradient estimator is of the form (Khanduri et al., 2021; Ghadimi & Wang, 2018; Hong et al., 2020; Liu et al., 2022):\n\n ̄∇f (xi,t, yi,t; ̄ξij) = ∇xf (xi,t, yi,t; ξ0\n\ni ) −\n\n1 Lg\n\n∇2\n\nxyg(xi,t, yi,t; ζ 0\n\ni ) ˆHi,k∇yf (xi,t, yi,t; ξ0\n\ni ),\n\n(4)\n\n∇2\n\nyyg(xi,t,yi,t;ζp i ) Lg\n\np=1 (I−\n\nwhere ˆHi,k (cid:44) K (cid:81)k(K) ). Here, K ∈ N is a predefined parameter and k(K) ∼ U{0, . . . , K − 1} is an integer-valued random variable uniformly chosen from {0, . . . , K − 1}. It can be shown that ˆHi,k is a biased estimator for the Hessian inverse (cid:2)∇2 =\n(cid:80)∞ yyg (x, y; ζ))i. However, this estimator has the limitation that it only incorporates the first term in the Taylor approximation, thus resulting in a large variance and could eventually increase the communication complexity of decentralized bilevel optimizaiton.\n\nyyg (x, y; ζ)(cid:3)−1\n\ni=1(I − ∇2\n\n;\n\n(5)\n\nTo address this issue, in this paper, we propose a new stochastic gradient estimator as follows: (cid:33)\n\n(cid:32)\n\nHi,0 = I; Hi,k=I+\n\nI−\n\nHi,k−1 = I+\n\n∇2\n\nyyg (cid:0)xi,t, yi,t; ζ k Lg\n\ni\n\n(cid:1)\n\nk(K) (cid:88)\n\nj(cid:48) (cid:89)\n\n(cid:18)\n\nj(cid:48)=1\n\np=1\n\nI−\n\n∇2\n\nyyg (xi,t, yi,t; ζ p i ) Lg\n\n(cid:19)\n\n ̄∇f (xi,t, yi,t; ̄ξij)=∇xf (xi,t, yi,t; ξ0\n\ni ) −\n\n∇2\n\nxyg (cid:0)xi,t, yi,t; ζ 0\n\ni\n\n(cid:1) Hi,k∇yf (xi,t, yi,t; ξ0\n\ni ).\n\n1 Lg\n\nyyg(x, y; ξ) by 1/Lg as it ensures that 1/Lg × ∇2\n\nCompared to the conventional estimator, the key difference in our new estimator lies in the matrix Hi,k. The new Hessian inverse estimator is inspired by ideas in stochastic second-order optimization (Agarwal et al., 2016). Similar technique to estimate the Hessian inverse can also be found in Koh & Liang (2017). However, our Hessian inverse estimator differ from Koh & Liang (2017) in the following key aspects: (i) In our Hessian inverse estimator, we multiply the hessian term ∇2 yyg(x, y; ξ) will have eigenvalue less than 1. Otherwise, the power series of the Hessian Inverse will not converge. In comparison, Koh & Liang (2017) does not have 1/Lg term because the authors assume w.l.o.g. that the Hessian ∇2 yyg(x, y) (cid:22) 1, which implies that the authors implicitly assume Lg = 1. (ii) Koh & Liang (2017) is only designed for solving a conventional single-level minimization problem with loss function L(·). In comparision, our proposed stochastic estimator can be used in bilevel learning especially for solving non-smooth regularizers in the upper-level problems. Note that our Hi,k is in a recursive form that is able to capture the entire Taylor series at once without increasing the sample complexity. Thanks to this recursive form, Hi,k utilizes O(k2) samples, as opposed to only O(k) samples in the conventional ˆHi,k-Hessian inverse estimator, thus leading to a much smaller variance and eventually much lower communication complexity. It is worth noting that although our Hi,k estimator leverages more training samples, the computation cost is the same as that of ˆHi,k due to the recursive structure in (5). In Sections 4.3 and 5, we will theoretically and numerically demonstrate the smaller variance of our new estimator over the conventional one.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 THE Prometheus ALGORITHM.\n\nOur Prometheus algorithm is an advanced triple-hybrid of proximal, gradient tracking, and variance reduction techniques. The procedure of Prometheus can be organized into three key steps:\n\n• Step 1 (Local Proximal Operations): In each iteration t, each agent i performs the following\n\nproximal operations to cope with the domain constraint set X for the upper-level variables:\n\n(cid:101)xi,t = ̃xi(xi,t) = arg minx∈X [(cid:104)ui,t, x − xi,t(cid:105) + τ\n\n2 (cid:107)x − xi,t(cid:107)2 + h(x)],\n\n(6)\n\nwhere τ > 0 is a proximal control parameter and ui,t is an auxiliary vector. The proximal update rule is motivated by the SONATA method (Scutari & Sun, 2019) used in a decentralized minimization.\n\n• Step 2 (Consensus Update in Upper-Level Variables): Next, each agent i updates the upper and\n\nlower model parameters xi, yi as follows:\n\nxi,t+1 =\n\n(cid:88)\n\ni(cid:48)∈Ni\n\n[M]ii(cid:48)xi(cid:48),t + α( ̃xi(xi,t) − xi,t),\n\nyi,t+1 = yi,t − βvi,t,\n\n(7)\n\nwhere α and β are constant step-sizes for updating x- and y-variables, respectively. Note that updating xi,t+1 in Eq. (7) is a local weighted average at agent i and plus a local update in the spirit of Frank-Wolfe given a proximal point. The right-hand side of Eq. (7) performs a local stochastic gradient descent update for the y-variable at each agent i. Remark 1. The used auxiliary proximal operator ̃xi,t and the resultant local update α( ̃xi(xi,t) − xi,t) in the consensus step play an important role in helping us alleviate the non-smooth objective challenge. It will be difficult to achieve convergence guarantees in decentralized learning if we use (cid:107) ̃x − (xi,t − αui,t)(cid:107)2 instead. See proof details in Lemma xi,t+1 = PX (xi,t − αui,t) = argmin\n\n5 and 7 in our Appendix.\n\n(cid:101)x∈X\n\n• Step 3 (Local Variance-Reduced Stochastic Gradient Estimate): In the local gradient estimator step, each agent i estimates its local gradients using the following stochastic gradient estimators:\n\npi(xi,t, yi,t) =\n\ndi(xi,t, yi,t) =\n\n \n\n \n\n\n\n\n ̄∇f (xi,t, yi,t) = 1 n\npi(xi,t−1, yi,t−1) (cid:80) + 1\n\nj∈Si,t\n\n|Si,t|\n\n ̄∇g(xi,t, yi,t) = 1 n\ndi(xi,t−1, yi,t−1) (cid:80) + 1\n\nj∈Si,t\n\n|Si,t|\n\n(cid:80)n\n\nj=1\n\n ̄∇f (xi,t, yi,t; ̄ξij),\n\nif mod(t, q) = 0,\n\n(cid:0) ̄∇f (cid:0)xi,t, yi,t, ̄ξij\n\n(cid:1) − ̄∇f (cid:0)xi,t−1, yi,t−1, ̄ξij\n\n(cid:1)(cid:1) ,\n\n(cid:80)n\n\ni=1\n\n ̄∇g(xi,t, yi,t; ζij),\n\nif mod(t, q) = 0,\n\n(∇g (xi,t, yi,t, ζij) −∇g (xi,t−1, yi,t−1, ζij)) .\n\n(8a)\n\n(8b)\n\nHere, Si,t is the sample mini-batch in the t-th iteration, and q is a pre-determined inner loop iteration number. The local stochastic gradient estimation is a recursive estimator that shares some structural similarity with those in SARAH (Nguyen et al., 2017), SPIDER (Fang et al., 2018), and PAGE (Li et al., 2021) used for traditional minimization problems.\n\n• Step 4 (Gradient Tracking in Upper-Level Parameters): Each agent i updates ui,t and vi,t by\n\naveraging over its neighboring tracked gradients:\n\nui,t =\n\n(cid:88)\n\ni(cid:48)∈Ni\n\n[M]ii(cid:48)ui(cid:48),t−1 + pi(xi,t, yi,t) − pi(xi,t−1, yi,t−1);\n\nvi,t = di(xi,t, yi,t).\n\n(9)\n\nTo summarize, we illustrate the Prometheus algorithm in Algorithm 1.\n\n4.3 CONVERGENCE PERFORMANCE ANALYSIS OF THE Prometheus ALGORITHM\n\nNow, we focus on the convergence performance analysis for the proposed Prometheus algorithm. Before presenting the main convergence results, we first state several technical assumptions: Assumption 1. For all ζ ∈ supp (πg) where supp(π) is the support of π, x ∈ X , X ⊆ Rp1 , y ∈ Rp2, the lower-level function g has the following properties : i) g(x, y; ζ) is μg-strongly convex with μg > 0, ∇yg(x, y; ζ) is Lg-Lipschitz continuous with Lg > 0; ii) (cid:13) ≤ Cgxy for some Cgxy > 0, ∇2 yyg(x, y; ζ) are Lipschitz continuous with constants Lgxy > 0 and Lgyy > 0, respectively.\n\nxyg(x, y; ζ) and ∇2\n\nxyg(x, y; ζ)(cid:13) 2\n(cid:13)\n\n(cid:13)∇2\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 The Prometheus Algorithm at Each Agent i.\n\nSet parameter pair (xi,0, yi,0) = (x0, y0). Calculate local gradients: ui,0 = ̄∇f (xi,0, yi,0); vi,0 = ∇yg(xi,0, yi,0); for t = 1, · · · , T do\n\nUpdate local parameters (xi,t+1, yi,t+1) as in Eq. (6)-(7); if Prometheus : then\n\nCompute local estimators (pi(xi,t+1, yi,t+1), di(xi,t+1, yi,t+1)) as in Eq. (8);\n\nend if if Prometheus-SG: then\n\nCompute local estimators (pi(xi,t+1, yi,t+1), di(xi,t+1, yi,t+1)) as in Eq. (10);\n\nend if Track global gradients (ui,t+1, vi,t+1) as in Eq. (9);\n\nend for\n\nAssumption 2. For all ξ ∈ supp (πf ) where supp(π) is the support of π, x ∈ X , X ⊆ Rp1, the upperlevel function f has the following properties : ∇xf (x, y; ξ), ∇yf (x, y; ξ) (w.r.t. y) are Lipschitz smooth continuous with constant Lfx ≥ 0, Lfy ≥ 0. (cid:107)∇yf (x, y; ξ)(cid:107) ≤ Cfy , for some Cfy ≥ 0. Assumption 3. E ̄ξ[(cid:107) ̄∇f (x, y; ̄ξ) − E ̄ξ[ ̄∇f (x, y; ̄ξ)](cid:107)2] ≤ σ2 level function satisfies: Eζ[(cid:107)∇yg(x, y; ζ) − ∇yg(x, y)(cid:107)2] ≤ σ2 g.\n\nfunction satisfies: the upper-level f ; and ii) The stochastic gradient estimate of the lower-\n\ni) The stochastic gradient estimate of\n\nWe note that Assumptions.1, 2 and 3(b) are standard in the literatures of bilevel optimization (see, e.g., Ghadimi & Wang (2018); Khanduri et al. (2021). In addition, Assumption 3(a) has been verified in (Khanduri et al., 2021).\n\nTo establish the convergence result of Prometheus, we first prove the Lipschitz-smoothness of the new gradient estimator proposed in (5), which is stated as follows:\n\n(cid:13) ̄∇f (cid:0)x1, y; ̄ξ(cid:1) − ̄∇f (cid:0) x2, y; ̄ξ(cid:1)(cid:13) 2\n(cid:13)\n\nLemma 1. (Lipschitz-smoothness of the new stochastic gradient estimator in (5)). If the stochastic functions f (x, y; ξ) and g(x, y; ζ) satisfy Assumptions 1–3, then we have (i) for a fixed y ∈ Rp2 , (cid:13) f (cid:107) x1 − x2(cid:107)2 , ∀ x1, x2 ∈ Rp1 ; and (ii) for a fixed x ∈ Rp1 , (cid:13) ≤ L2 In the above (cid:1) + + 6C 2 expressions, Lf > 0 is defined as: L2\n\nf (cid:107)y1 − y2(cid:107)2 , ∀y1, y2 ∈ Rp2 .\n\n≤ L2 (cid:13) ̄∇f (cid:0)x, y1; ̄ξ(cid:1) − ̄∇f (cid:0)x, y2; ̄ξ(cid:1)(cid:13) 2\n(cid:13) f := 2L2 L2\n\nK 2μgLg−μ2 g\n\nK 2μgLg−μ2 g\n\n(cid:1) + 6C 2\n\nL2 fy\n\n(cid:80)K\n\nL2\n\ngxy\n\ngxy\n\nfx\n\nfy\n\n(cid:0)\n\n(cid:0)\n\nj=1 j2(cid:0)1 − μg\n\nLg\n\n(cid:1)2(j−1) 1 L2 g\n\n6C 2\n\ngxy\n\nC 2 fy\n\ngyy .\n\nK L2 g\n\nRemark 2. We note that the Lipschitz-smoothness constant Lf of Lemma 1 is smaller than that of the conventional estimator in (4), which we denote as Lconv here, i.e., Lf ≤ Lconv. This also shows superiority of our new estimator. Due to space limitation, we state the definition of Lconv in Lemma 4 in the appendix.\n\nNext, we need the following Lipschitz-continuity properties of the approximate gradient ̄∇f (x, y), the lower level solution y∗, and the true gradient ∇(cid:96)(x), which have been proved in the literature:\n\n(Ghadimi & Wang, 2018) Under Assumptions 1–2, we have (cid:107) ̄∇f (x, y) − Lemma 2. ∇(cid:96)(x)(cid:107) ≤ L (cid:107)y∗(x)−y(cid:107), (cid:107)y∗ (x1)−y∗ (x2)(cid:107) ≤ Ly (cid:107)x1 −x2(cid:107) , (cid:107)∇(cid:96) (x1)−∇(cid:96) (x2)(cid:107) ≤ L(cid:96) (cid:107)x1 −x2(cid:107) for all x, x1, x2 ∈ Rp1 , y ∈ Rp2, where the Lipschitz constants are defined as: L (cid:44) Lfx +\n\n, and Ly (cid:44) Cgxy\n\n), L(cid:96) (cid:44) L +\n\n+ Cfy (\n\n+\n\n.\n\nLfy Cgxy μg\n\nLCgxy μg\n\nLgxy μg\n\nLgyy Cgxy μ2 g\n\nμg\n\nLemma 2 establishes the smoothness of the implicit function in (1), which only relies on the Assumptions 1 and 2 to hold. Lastly, following the same token as in (Hong et al., 2020), we show a critical fact on the exponentially fast decay of the bias of our stochastic estimator in (5), which is stated below.\n\nLemma 3 (Exponentially Decaying Bias). Under Assumptions 1–3, the stochastic gradient estimate of the upper level objective in (5) satisfies (cid:107)∇f (x, y) − E[ ̄∇f (x, y; ̄ξ)](cid:107) ≤\n\n)K.\n\nCgxy Cfy μg\n\n(1 − μg Lg\n\nThe assumptions and Lemmas 1-3 above lead to the main convergence result of Prometheus which is stated next.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 1. Under Assumptions1-3, if the step-sizes α ≤ min\n\n(cid:110) (1−λ)m\n\n√\n\n2\n\nβ(L(cid:96)+τ )\n\nτ\n\n6+3τ , (1−λ)m\n\n√\n\n8\n\nβL2 f\n\nτ\n\n6+3τ , τ\n\n3L(cid:96)\n\n,\n\ngβ1.5 yL2 ,\n\n(1−λ)μ2 23040L2 (cid:110) √\n\nβ ≤\n\n40Ly 3Lf\n\n√\n\n8\n\nβτ\n\n12m(1−λ) ,\n\n20L2 27(1−λ)β1.5L2\n\nyτ\n\nf m , τ (1−λ)\n\n24mL2\n\nf β , τ\n\n√\n\nβ(1−λ)\n\n12m , μg(1−λ)\n\n240L2 y\n\nβ2.5 9L2 f\n\nm τ\n\n6+3τ , (1−λ)β 2m\n\n3\n\n(cid:111) ,\n\nτ 6+3τ\n\n, 1−λ 16Lf\n\n, ( μg(1−λ)2 yL2\n\n1440L2\n\nf\n\n)2, 2μg 81L2 f\n\n(cid:111)\n\n, then the outputs of Prometheus satisfy:\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\n(cid:2)E(cid:107)xt − 1 ⊗ ̄xt(cid:107)2 + E(cid:107) ̃xt − 1 ⊗ ̄xt(cid:107)2 + E(cid:107)yt − y∗\n\nt (cid:107)2(cid:3) = O\n\n(cid:19)\n\n.\n\n(cid:18) 1 T\n\nRemark 3. It is worth noting that, compared to existing works on decentralized bilevel optimization, the major challenge in proving the convergence results in Theorem 1 stems from the proximal operator needed to solve the upper-level subproblem, which prevents the use of conventional descent lemma for convergence analysis (see Eq. (34) in the appendix). Also, compared to single-agent constrained bilevel optimization, one cannot provide theoretical convergence guarantee by using the direct projection method (cid:101)xi,t = arg minx∈X (cid:107)x − (xi,t − τ ui,t)(cid:107)2 as in (Hong et al., 2020; Chen et al., 2022a) due to the gradient tracking procedure in the decentralized learning. Instead, we use a different proximal update rule as shown in (6). We will numerically show in Section 5 that Prometheus with the direct proximal operator can only converge to a neighborhood of a stationary point. Further, Theorem 1 implies the following sample and communication complexity results:\n\nCorollary 2 (Sample and Communication Complexities of Prometheus). Under the conditions of Theorem 1, to achieve an (cid:15)-stationary solution, Prometheus requires that: i) the total number of communication rounds is O((cid:15)−1), and ii) the total number of samples is O(\n\nnK(cid:15)−1 + n).\n\n√\n\n4.4 DISCUSSION: THE BENEFIT OF VARIANCE REDUCTION IN Prometheus\n\nSince the variance reduction in (8) in Step 3 of Prometheus requires full gradient evaluation, it is tempting to ask what is the benefit of using the variance reduction technique. In other words, could we relinquish variance reduction (VR) in Step 3 to avoid full gradient evaluation? To answer this question, consider changing Step 3 to the following basic stochastic gradient estimator without VR:\n\npi(xi,t, yi,t) = ̄∇f (xi,t, yi,t, ̄ξi0);\n\ndi(xi,t, yi,t) = ∇g(xi,t, yi,t; ζi0).\n\n(10)\n\nInterestingly, the following convergence result states that there always exists a non-vanishing constant independent of m, n, and α if (10) is used in Step 3 of Prometheus (i.e., a constant only dependent on problem instance and cannot be made arbitrarily small algorithmically).\n\n√\n\nProposition 3. Under Assumptions1–3, with step-sizes α ≤ min{ 1−λ 8βLf\n\nτ\n\nβ\n\n6m(1−λ) , τ (1−λ) yL2 , O(T − 1 f β , have the following result if (10) replaces Step 3 in Prometheus,\n\n2 ), (1−λ)m\n\nβτ }, β ≤ min{ 1−λ 8Lf\n\n48mL2\n\n4\n\n(1−λ)μ2 23040L2\n\ngβ1.5\n\n,\n\n√\n\n,\n\n, τ 3L(cid:96) (1−λ)4μ2 yL2 4802L2\n\ng\n\nf\n\n(1−λ)m √\n\n2\n\nβ(L(cid:96)+τ )\n\nτ\n\n6+3τ ,\n\n, O(T − 1\n\n3 )}, we\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\n(cid:0)E(cid:107)xt − 1 ⊗ ̄xt(cid:107)2 + E(cid:107) ̃xt − 1 ⊗ ̄xt(cid:107)2(cid:1) = O\n\n(cid:19)\n\n(cid:18) 1 √\nT\n\n+ C (cid:48)\n\nσ,\n\n(11)\n\nσ\n\nτ 2\n\n((\n\nCgxy Cfy μg\n\n(1− μg Lg\n\n(cid:44) 9(6+3τ )\n\n)K)2+σ2\n\nσ is defined as C (cid:48)\n\nwhere the constant C (cid:48)\n\nατ σ2 g. Remark 4. A key insight of Proposition 3 is in order. The SG-type update in (10) is similar to the SGtype update in unconstrained bilevel optimization in the single-agent setting (Ji et al., 2021). However, T ) convergence unlike the SG-type method in (Ji et al., 2021) that can approach zero at an O(1/ rate, the SG-type method can only approach a constant error C (cid:48) T ) convergence rate in σ at an O(1/ the constrained decentralized setting. The non-vanishing constant error C (cid:48) σ is caused by the variance σ2 g of the stochastic gradient . This make the benefit of using the variance reduction techniques to eliminate the {σf , σg}-variance in order to approach zero asymptotically.\n\nf and σ2\n\n40(8+4α2)L2 y\n\nf )+ 27(1−λ)\n\nβ1.5\n\n√\n\n√\n\n5 NUMERICAL RESULTS\n\nIn this section, we will first conduct experiments to demonstrate the small variance of our new stochastic gradient estimator. Then, we will compare Prometheus’ convergence with several baselines.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Five-agent network.\n\nFigure 3: Ten-agent network.\n\n1) New estimator vs. conventional estimator: Note that the major difference between the new and conventional estimators lies in how they estimate the Hessian inverse of the matrix A. Thus, it suffices to compare the Hessian inverse approximations. The conventional estimator to estimate conv = K (cid:81)k(K) the A−1 can be denoted as ̃A−1 p=1 (I−As), while the new es- (cid:81)j(cid:48) timator can be denoted as ̃A−1 = (cid:80)k(K) p=1(I − As). To see the benefits of our estimator and due to the high complexity of computing matrix inverse, here we consider a small example A = [[0.25, 0.0], [0.0, 0.25]], so that A−1 true = [[4, 0], [0, 4]]. Let As be a random matrix obtained from A plus Gaussian noise. We use ̃A−1 conv and ̃A−1 to estimate A−1, respectively. We run 10000 independent trials with K = 10 and the results are shown in Fig. 1. We can see from Fig. 1 that the new Hessian inverse estimator has a much smaller variance than the conventional one. Additional experiments on varing K and different matrix A are relegated to our Appendix.\n\nFigure 1: Hessian inverse estimator comparison.\n\nj(cid:48)=1\n\n2) Convergence Performance: We verify our theoretical results of Prometheus by conducting experiments on a meta-learning problem tested on MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky et al., 2009) datasets. Due to space limitation, we provide additional experiments on hyper-parameter optimization in the appendix. Due to the lack of existing algorithms for solving constrained decentralized bilevel optimization problem, we compare the convergence performance of Prometheus against several stripped-down version of Prometheus: • Prometheus with Stochastic Gradient (Prometheus-SG): Prometheus-SG is the SG-type algorithm discussed in Section 4.4: pi(xi,t, yi,t) = ̄∇f (xi,t, yi,t, ̄ξi0); di(xi,t, yi,t) = ∇g(xi,t, yi,t; ζi0). • Prometheus with Direct Proximal Method (Prometheus-dir): Instead of performing (cid:101)xi,t = 2 (cid:107)x − xi,t(cid:107)2 + h(xi)] in Prometheus, Prometheus-dir directly\n\narg minx∈X [(cid:104)ui,t, x − xi,t(cid:105) + τ adds the constraints on x: (cid:101)xi,t = arg minx∈X (cid:107)x − (xi,t − τ ui,t)(cid:107)2.\n\n• Proximal Decentralized Stochastic Gradient Descent (Prox-DSGD): This algorithm is motivated by the DSGD algorithm, which can be viewed as Prometheus without using gradient tracking. Specifically, we updates local gradient as ui,t = ̄∇f (xi,t, yi,t; ̄ξi0); vi,t = ∇g(xi,t, yi,t; ζi0). We also note that the Prox-DSGD algorithm can be seen as a generalization of DSBO (Yang et al., 2022), SPDB (Lu et al., 2022), DSBO (Chen et al., 2022b) with the proximal operator. Prometheus - dir can also be seen as an extension of the algorithm INTERACT (Liu et al., 2020a) to handle the constrained decentralized bilevel optimization problem. We compare Prometheus with these baselines using a two-hidden-layer neural network with 20 hidden units. The consensus matrix is chosen as M = I− 2L 3λmax(L) , where L is the Laplacian matrix of G and λmax(L) denotes the largest eigenvalue of L. Due to space limitation, we relegate the detailed parameter choices of all algorithms to the appendix. In Fig. 2, we compare the performance of Prometheus, Prometheus-SG, Prometheus-dir, and Prox-DSGD on the MNIST and CIFAR-10 datasets with with a five-agent network. The network topology can be seen in Fig. 4 in Appendix D. We note that Prometheus converges much faster than than all other algorithms in terms of the total number of communication rounds. In Fig. 3, we also observe similar results when the number of tasks (and agents) is increased to 10. Our experimental results thus verify our theoretical analysis that Prometheus has the lowest communication complexity.\n\n6 CONCLUSION\n\n√\n\nIn this paper, we studied the constrained decentralized nonconvex-strongly-convex bilevel optimization problems. First, we proposed an algorithm called Prometheus with a new stochastic estimator. We then showed that, to achieve an (cid:15)-stationary point, Prometheus achieves a sample complexity of n(cid:15)−1 + n) and a communication complexity of O((cid:15)−1). Our numerical studies also showed O(K the advantages of our proposed Prometheus and verified the theoretical results. Collectively, the results in this work contribute to the state of the art of low sample- and communication-complexity constrained decentralized bilevel learning.\n\n9\n\n02004006008001000Communication round100MNISTPrometheus-SGPrometheusProx-DSGDPrometheus-dir 02004006008001000Communication round100CIFAR-10Prometheus-SGPrometheusPrometheus-dir Prox-DSGD02004006008001000Communication round100MNISTPrometheus-SGPrometheusPrometheus-dir Prox-DSGD02004006008001000Communication round100CIFAR-10Prometheus-SGPrometheusPrometheus-dir Prox-DSGDUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nNaman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization in linear time.\n\nstat, 1050:15, 2016.\n\nSina Baharlouei, Maher Nouiehed, Ahmad Beirami, and Meisam Razaviyayn. Rbackslashenyi fair\n\ninference. arXiv preprint arXiv:1906.12005, 2019.\n\nAnouer Bennajeh, Slim Bechikh, Lamjed Ben Said, and Samir Aknine. Bi-level decision-making modeling for an autonomous driver agent: application in the car-following driving behavior. Applied Artificial Intelligence, 33(13):1157–1178, 2019.\n\nChih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM\n\ntransactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.\n\nTianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin. A single-timescale method for stochastic bilevel optimization. In International Conference on Artificial Intelligence and Statistics, pp. 2466–2488. PMLR, 2022a.\n\nXuxing Chen, Minhui Huang, and Shiqian Ma. Decentralized bilevel optimization. arXiv preprint\n\narXiv:2206.05670, 2022b.\n\nAshok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.\n\nIn Advances in Neural Information Processing Systems, pp. 15210–15219, 2019.\n\nCong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems, pp. 689–699, 2018.\n\nLuca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In International Conference on Machine Learning, pp. 1165–1173. PMLR, 2017.\n\nLuca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, pp. 1568–1577. PMLR, 2018.\n\nHongchang Gao, Bin Gu, and My T Thai. Stochastic bilevel distributed optimization over a network.\n\narXiv preprint arXiv:2206.15025, 2022.\n\nSaeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint\n\narXiv:1802.02246, 2018.\n\nStephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. arXiv preprint arXiv:1607.05447, 2016.\n\nRiccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity of hypergradient computation. In International Conference on Machine Learning, pp. 3748–3758. PMLR, 2020.\n\nZhishuai Guo and Tianbao Yang. Randomized stochastic variance-reduced methods for stochastic\n\nbilevel optimization. arXiv e-prints, pp. arXiv–2105, 2021.\n\nMingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint arXiv:2007.05170, 2020.\n\nFeihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Accelerated zeroth-order and first-order momentum methods from mini to minimax optimization. Journal of Machine Learning Research, 23(36):1–70, 2022.\n\nKaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced\n\ndesign. In International Conference on Machine Learning, pp. 4882–4892. PMLR, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nPrashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. Advances in Neural Information Processing Systems, 34, 2021.\n\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In\n\nInternational conference on machine learning, pp. 1885–1894. PMLR, 2017.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nYann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. Available: http://yann.\n\nlecun. com/exdb/mnist, 1998.\n\nGuoqing Li, Rufeng Zhang, Tao Jiang, Houhe Chen, Linquan Bai, and Xiaojing Li. Securityconstrained bi-level economic dispatch model for integrated natural gas and electricity systems considering wind power and power-to-gas process. Applied energy, 194:696–704, 2017.\n\nZhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtarik. Page: A simple and optimal In International Conference on\n\nprobabilistic gradient estimator for nonconvex optimization. Machine Learning, pp. 6286–6295. PMLR, 2021.\n\nMingrui Liu, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jarret Ross, Tianbao Yang, and Payel Das. A decentralized parallel algorithm for training generative adversarial nets. In Proceedings of Advances in Neural Information Processing Systems, volume 33, 2020a.\n\nSijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah Al-Dujaili, Mingyi Hong, and Una-May O’Reilly. Min-max optimization without gradients: Convergence and applications to black-box evasion and poisoning attacks. In International Conference on Machine Learning, pp. 6282–6293. PMLR, 2020b.\n\nZechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and Jian Sun. Metapruning: Meta learning for automatic neural network channel pruning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 3296–3305, 2019.\n\nZhuqing Liu, Xin Zhang, Prashant Khanduri, Songtao Lu, and Jia Liu. Interact: Achieving low sample and communication complexities in decentralized bilevel learning over networks. arXiv preprint arXiv:2207.13283, 2022.\n\nSongtao Lu, Xiaodong Cui, Mark S Squillante, Brian Kingsbury, and Lior Horesh. Decentralized bilevel optimization for personalized client learning. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5543–5547. IEEE, 2022.\n\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International conference on machine learning, pp. 2113–2122. PMLR, 2015.\n\nLam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In International Conference on Machine Learning, pp. 2613–2621. PMLR, 2017.\n\nFabian Pedregosa. Hyperparameter optimization with approximate gradient.\n\nIn International\n\nconference on machine learning, pp. 737–746. PMLR, 2016.\n\nVítor O Pochmann and Fernando J Von Zuben. Multi-objective bilevel recommender system for food\n\ndiets. In 2022 IEEE Congress on Evolutionary Computation (CEC), pp. 1–8. IEEE, 2022.\n\nClarice Poon and Gabriel Peyré. Smooth bilevel programming for sparse regularization. Advances in\n\nNeural Information Processing Systems, 34:1543–1555, 2021.\n\nAravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit\n\ngradients. Advances in neural information processing systems, 32, 2019.\n\nGesualdo Scutari and Ying Sun. Distributed nonconvex constrained optimization over time-varying\n\ndigraphs. Mathematical Programming, 176(1):497–544, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJunjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization.\n\nAdvances in Neural Information Processing Systems, 34, 2021.\n\nShuoguang Yang, Xuezhou Zhang, and Mengdi Wang. Decentralized gossip-based stochastic bilevel\n\noptimization over communication networks. arXiv preprint arXiv:2206.10870, 2022.\n\nHaifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. Bi-level actor-critic for multi-agent coordination. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7325–7332, 2020.\n\nXin Zhang, Zhuqing Liu, Jia Liu, Zhengyuan Zhu, and Songtao Lu. Taming communication and sample complexities in decentralized policy evaluation for cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.\n\n12",
    "reference": "# Summary Of The Paper\n\nThis paper introduces a new algorithm, Prometheus, for the composite stochastic bilevel optimization problem in the decentralized setting. In particular, the paper considers the setting where we have a distributed optimization problem on $m$ agents, each of which has a loss function of the form $l(x) + h(x)$ where $l(x)$ has bilevel, finite-sum structure and $h$ is a constraint function with an easy-to-compute proximal operator. The agents are connected to each other via a decentralized network (i.e. there is no central node to exchange iterates with, and therefore the agents have to solve a consensus problem as well during the optimization process). The bilevel optimization problem is nonconvex-strongly-convex (the outer function is nonconvex and smooth, the inner function is strongly convex), this problem has been solved before in the decentralized setting, but only with simple projection constraints (e.g. as in [1]). The algorithm introduced here, Prometheus, solves this problem in the setting where $h$ can be an arbitrary constraint but whose proximal operator is easily computed. Prometheus achieves the same communication complexity and local sample complexity as INTERACT-VR from [1], but can do so under more difficult constraints.\n\n[1] Zhuqing Liu, Xin Zhang, Prashant Khanduri, Songtao Lu, and Jia Liu. INTERACT: Achieving Low Sample and Communication Complexities in Decentralized Bilevel Learning over Networks. MobiHoc 2022\n\n# Strength And Weaknesses\n\n- (Strength) The introduced algorithm, Prometheus, achieves a sample complexity of $\\mathcal{O} (\\sqrt{n} K \\epsilon^{-1} + n)$. This is currently the best-known sample complexity for finite-sum stochastic bilevel optimization problems. The same can be said for the communication complexity $\\mathcal{O} (\\epsilon^{-1})$.\n\n- (Strength) The proposed algorithm can handle complex constraints through the use of the proximal operator, enabling more applications than previous algorithms.\n\n- (Strength) The authors introduce a new recursive estimator for the stochastic gradient of the bilevel problem (see eq. 5), and this new estimator seems to show some benefits in practice over the conventional estimator used by previous works.\n\n- (Weakness) The authors do not give many examples of the constraints their algorithm can solve, and their relevance to practice. The motivating examples all do not really make use of the new proximal formulation. \n\n- (Weakness) The benefits of the new recursive estimator are only illustrated in one experiment, and it is difficult to see the benefit since Figure 1 only tests out this estimator on a single matrix with fixed norm. Can you plot how the estimator performs for varying norm of ||A^-1||? How about with increasing variance? There is too little information to figure out how it performs empirically.\n\n- (Weakness) It is not clear why the ordinary inverse Hessian estimator does not work for this problem, given that it works just fine for INTERACT-VR and achieves the same rate for the unconstrained setting.\n\n- (Weakness) The discussion about the necessity of VR is a little misleading. The authors introduce a variant of Prometheus without variance reduction, and then give an upper bound for this variant that shows it does worse than the variance-reduced version. However, this is not a correct way to show the necessity of some technique-- at best, it provides some evidence for this and nothing more. The correct way would be to give a lower bound, but no such lower bound is given here.\n\n- (Weakness) The authors do not really explain why the proximal setting is significantly more difficult than the projected setting. In ordinary, non-bilevel, stochastic optimization, the extension from the projected case to the proximal case is very simple and straightforward. What is the source of difficulty here?\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- (Quality and Clarity) The paper is reasonably well-written, though I believe a table of notation would be immensely helpful. There is so much notation in the paper and it is very easy to get lost.\n\n- (Reproducibility) While the main contribution of the paper is theoretical, the code used to run experiments is not provided. Therefore, the paper's experiments are not reproducible. This reduces the value of the paper for future work that would seek to build on it.\n\n- (Novelty) While the paper relies on SARAH/SPIDER-style estimators for variance reduction, it introduces a new estimator for the stochastic gradient of the bilevel problem that is interesting in its own right. The algorithm Prometheus as a whole isn't very novel outside of this, and as the authors state can be seen as an extension of INTERACT-VR [1] to the proximal case.\n\n[1] Zhuqing Liu, Xin Zhang, Prashant Khanduri, Songtao Lu, and Jia Liu. INTERACT: Achieving Low Sample and Communication Complexities in Decentralized Bilevel Learning over Networks. MobiHoc 2022\n\n# Summary Of The Review\n\nI think this is a paper which is quite borderline, but for which the weaknesses outweigh the strengths. My biggest concern with this paper is that I am not sure where the difficulty is in the new proximal setting. I cannot really see it in the proof, and I don't think it is highlighted enough. Moreover, the experimental evaluation is not enough to see the benefits of some of the proposed techniques (like the new stochastic gradient estimator). The discussion on the necessity of variance reduction is somewhat flawed (for the reason outlined above), and as such I'd prefer to see that section rewritten as well.\n\n-----------------------\n\nPost-rebuttal summary: \nI thank the reviewers for their long, detailed responses. The examples given are satisfying to me, and I think there is some technical difficulty that the authors have overcome here. However, after discussion with other reviewers, I am not convinced the technical novelty here may not be enough, especially given that the Hessian estimator used is not novel. As such, I can not recommend acceptance.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nUNSUPERVISED LEARNING OF STRUCTURED REPRESENTATIONS VIA CLOSED-LOOP TRANSCRIPTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThis paper proposes an unsupervised method for learning a unified representation that serves both discriminative and generative purposes. While most existing unsupervised learning approaches focus on a representation for only one of these two goals, we show that a unified representation can enjoy the mutual benefits of having both. Such a representation is attainable by generalizing the recently proposed closed-loop transcription framework, known as CTRL, to the unsupervised setting. This entails solving a constrained maximin game over a rate reduction objective that expands features of all samples while compressing features of augmentations of each sample. Through this process, we see discriminative low-dimensional structures emerge in the resulting representations. Under comparable experimental conditions and network complexities, we demonstrate that these structured representations enable classification performance close to state-of-the-art unsupervised discriminative representations, and conditionally generated image quality significantly higher than that of state-of-the-art unsupervised generative models.\n\nINTRODUCTION\n\n1 In the past decade, we have witnessed an explosive development in the practice of machine learning, particularly with deep learning methods. A key driver of success in practical applications has been marvelous engineering endeavors, often focused on fitting increasingly large deep networks to input data paired with task-specific sets of labels. Brute-force approaches of this nature, however, exert tremendous demands on hand-labeled data for supervision and computational resources for training and inference. As a result, an increasing amount of attention has been directed toward using selfsupervised or unsupervised techniques to learn representations that can not only learn without human annotation effort, but also be shared across downstream tasks.\n\nDiscriminative versus Generative. Tasks in unsupervised learning are typically separated into two categories. Discriminative ones frame high-dimensional observations as inputs, from which lowdimensional class or latent information can be extracted, while generative ones frame observations as generated outputs, which should often be sampled given some semantically meaningful conditioning.\n\nUnsupervised learning approaches targeted at discriminative tasks are mainly based on a key idea: to pull different views from the same instance closer while enforcing a non-collapsed representation by either contrastive learning techniques (Chen et al., 2020b; He et al., 2020; Grill et al., 2020a), covariance regularization methods (Bardes et al., 2021; Zbontar et al., 2021), or using architecture design (Chen & He, 2020; Grill et al., 2020b). Their success is typically measured by the accuracy of a simple classifier (say a shallow network) trained on the representations that they produce, which have progressively improved over the years. Representations learned from these approaches, however, do not emphasize much about the intrinsic structure of the data distribution, and have not demonstrated success for generative purposes.\n\nIn parallel, generative methods like GANs (Goodfellow et al., 2014) and VAEs (Kingma & Welling, 2013) have also been explored for unsupervised learning. Although generative methods have made striking progress in the quality of the sampled or autoencoded data, when compared to the aforementioned discriminative methods, representations learned with these approaches demonstrate inferior performance in classification.\n\nToward A Unified Representation? The disparity between discriminative and generative approaches in unsupervised learning, contrasted against the fundamental goal of learning representations that in are useful across many tasks, leads to a natural question that we investigate in this paper: the unsupervised setting, is it possible to learn a unified representation that is effective for both\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ndiscriminative and generative purposes? Further, do they mutually benefit each other? Concretely, we aim to learn a structured representation with the following two properties:\n\n1. The learned representation should be discriminative, such that simple classifiers applied to\n\nlearned features yield high classification accuracy.\n\n2. The learned representation should be generative, with enough diversity to recover raw inputs,\n\nand structure that can be exploited for sampling and generating new images.\n\nThe fact that human visual memory serves both discriminative tasks (for example, detection and recognition) and generative or predictive tasks (for example, via replay) (Keller & Mrsic-Flogel, 2018; Josselyn & Tonegawa, 2020; Ven et al., 2020) indicates that this goal is achievable. Beyond being possible, these properties are also highly practical – successfully completing generative tasks like unsupervised conditional image generation (Hwang et al., 2021), for example, inherently requires that learned features for different classes be both structured for sampling and discriminative for conditioning. On the other hand, the generative property can serve as a natural regularization to avoid representation collapse.\n\nClosed-Loop Transcription via a Constrained Maximin Game. The class of linear discriminative representations (LDRs) has recently been proposed for learning diverse and discriminative features for multi-class (visual) data, via optimization of the rate reduction objective (Chan et al., 2022). In the supervised setting, these representations have been shown to be be both discriminative and generative if learned in a closed-loop transcription framework via a maximin game over the rate reduction utility between an encoder and a decoder (Dai et al., 2022). Beyond the standard joint learning setting, where all classes are sampled uniformly throughout training, the closed-loop framework has also been successfully adapted to the incremental setting (Tong et al., 2022), where the optimal multi-class LDR is learned one class at a time. In the incremental (supervised) learning setting, one solves a constrained maximin problem over the rate reduction utility which keeps learned memory of old tasks intact (as constraints) while learning new tasks. It has been shown that this new framework can effectively alleviate the catastrophic forgetting suffered by most supervised learning methods.\n\nContributions. In this work, we show that the closed-loop transcription framework proposed for learning LDRs in the supervised setting (Chan et al., 2022) can be adapted to a purely unsupervised setting. In the unsupervised setting, we only have to view each sample and its augmentations as a “new class” while using the rate reduction objective to ensure that learned features are both invariant to augmentation and self-consistent in generation; this leads to a constrained maximin game that is similar to the one explored for incremental learning (Tong et al., 2022). Our overall approach is illustrated in Figure 1.\n\nAs we experimentally demonstrate in Section 4, our formulation benefits from the mutual benefits of both discriminative and generative properties. It largely bridges the gap between two formerly distinct set of methods: by standard metrics and under comparable experimental conditions, it enables classification performance close to discriminative methods and unsupervised conditional generative quality significantly higher than state-of-the-art techniques. Coupled with evidence from prior work, this suggests that the closed-loop transcription through the (constrained) maximin game between the encoder and decoder has the potential to offer a unifying framework for both discriminative and generative representation learning, across supervised, incremental, and unsupervised settings.\n\nMethod\n\nLinear Probe\n\nImage Generation\n\nUCIG\n\nSimCLR (Chen et al., 2020b) MOCO-V2 (He et al., 2020) ContraD (Jeong & Shin, 2021) PATCH-VAE (Parmar et al., 2021) CTRL-Binary (Dai et al., 2022) SLOGAN (Hwang et al., 2021) U-CTRL (ours)\n\n✔ ✔\n✔ ✔\n✔ ✗\n✔\n\n✗ ✗\n✔ ✔\n✔ ✔\n✔\n\n✗ ✗\n✗ ✗\n✗ ✔\n✔\n\nTable 1: Comparison of the downstream task capabilities of different unsupervised learning methods. UCIG refers to Unsupervised Conditional Image Generation (Hwang et al., 2021).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Overall framework of closed-loop transcription for unsupervised learning. Two additional constraints are imposed on the Binary-CTRL method proposed in prior work (Dai et al., 2022): 1) self-consistency for sample-wise features zi and ˆzi, say zi ≈ ˆzi; and 2) invariance/similarity among a = τ (xi) is an a, say zi ≈ zi features of augmented samples zi and zi augmentation of sample xi via some transformation τ (·).\n\na = f (τ (xi), θ), where xi\n\n2 RELATED WORK\n\nOur work is mostly related to three categories of unsupervised learning methods: (1) self-supervised learning via discriminative models, (2) self-supervised learning via generative models, and (3) unsupervised conditional image generation. Table 1 compares the capabilities of models learned by various representative unsupervised learning methods.\n\nSelf-Supervised Learning for Discriminative Models. On the discriminative side, works like SimCLR (Chen et al., 2020b), MoCo (He et al., 2020), and BYOL (Grill et al., 2020a) have recently shown overwhelming effectiveness in learning discriminative representations of data. MoCo (He et al., 2020) and SimCLR (Chen et al., 2020b) seek to learn features by pulling together features of augmented versions of the same sample while pushing apart features of all other samples, while BYOL (Grill et al., 2020a) trains a student network to predict the representation of a teacher network in a contrastive setting. BarlowTwins (Zbontar et al., 2021) and TCR (Li et al., 2022) learn by regularizing the covariance matrix of the embedding. However, features learned by this class of methods are typically highly compressed, and not designed to be used for generative purposes.\n\nSelf-Supervised Learning with Generative Models. On the generative side, the original GAN (Goodfellow et al., 2014) can be viewed as a natural self-supervised learning task. With an additional linear probe, works like DCGAN (Radford et al., 2015) have shown that features in the discriminator can be used for discriminative tasks. To further enhance the features, extensions like BiGAN (Donahue et al., 2016) and ALI (Dumoulin et al., 2016) introduce a third network into the GAN framework, aimed at learning an inverse mapping for the generator, which when coupled with labeled images can be used to study and supervise semantics in learned representations. Other works like SSGAN (Chen et al., 2019), SSGAN-LA (Hou et al., 2021), and ContraD (Jeong & Shin, 2021) propose to put augmentation tasks into GAN training to facilitate representation learning. Outside of GANs, variational autoencoders (VAEs) have been adapted to generate more semantically meaningful representations by trading off latent channel capacity and independence constraints with reconstruction accuracy (Higgins et al., 2016), an idea that has also been incorporated into recognition improvements using patch-level bottlenecks (Gupta et al., 2020), which encourage a VAE to focus on useful patterns in images. By incorporating data-augmentation, VAE is also shown to achieve fair discriminative performance (Falcon et al., 2021). Recently, works like MAE (He et al., 2021) and CAE (Chen et al., 2022) have learned representations by solving masked reconstruction tasks using vision transformers. Autogressive approaches like iGPT (Chen et al., 2020a) have also demonstrated decent self-supervised learning performance, which improves further with the incorporation of contrastive learning (Kim et al., 2021). However, unless supervised, features learned by those previously mentioned methods either do not have strong discriminative performance, or cannot be directly exploited to condition the generative task.\n\nUnsupervised Conditional Image Generation (UCIG). For generative models, we often want to be able to generate images conditioned on a certain class or style, even in a completely unsupervised setting. This requires that the learned representations have structures that correspond to the desired conditioning. InfoGAN (Chen et al., 2016) proposes to learn interpretable representations by maximizing the mutual information between the observation and a subset of the latent code. ClusterGAN (Mukherjee et al., 2019) assumes a discrete Gaussian prior where discrete variables are defined as a\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\none-hot vector and continuous variables are sampled from Gaussian distribution. Self-Conditioned GAN (Liu et al., 2020) uses clustering of discriminative features as labels to train. SLOGAN (Hwang et al., 2021) proposes a new conditional contrastive loss (U2C) to learn latent distribution of the data. Note that compared to our work, ClusterGAN and SLOGAN introduce an additional encoder that leads to increased computational complexity. On the VAE side, works like VaDE (Jiang et al., 2016) cluster based on the learned feature of a supervised ResNet. Variational Cluster (Prasad et al., 2020) simultaneously learns a prior that captures the latent distribution of the images and a posterior to help discriminate between data points in an end-to-end unsupervised setting. In this work, we will see how clusters can be estimated in a principled way in a more unified framework, by optimizing the same type of objective function that we use for learning features.\n\n3 METHOD\n\n3.1 PRELIMINARIES: RATE REDUCTION AND CLOSED-LOOP TRANSCRIPTION\n\nAssumptions on Data. Our work, as well as prior work in closed-loop transcription (Dai et al., 2022; Tong et al., 2022), considers a set of N images X = [x1, x2, ..., xN ] ⊂ RD sampled from k classes. Borrowing notation from (Yu et al., 2020), the membership of the N samples in the k classes is denoted using k diagonal matrices: Π = {Πj ∈ RN ×N }k j=1, where the diagonal entry Πj(i, i) of .\n= {Π | (cid:80) Πj = I, Πj ≥ 0.} be the Πj is the probability of sample i belonging to subset j. Let Ω set of all such matrices. WLOG, we may assume that classes are separable, with images for each belonging to a low-dimensional submanifold in the space RD.\n\nUnsupervised Discriminative Autoencoding. The goal of transcription is to learn a unified representation, with the structure required to both classify and generate images from these k classes. Concretely, this is achieved by learning two continuous mappings: (1) an encoder parametrized by θ: f (x,θ) f (·, θ) : x (cid:55)→ z ∈ Rd with d ≪ D such that all samples are mapped to their features as X −−−−→ Z with Z = [z1, z2, ..., zN ] ⊂ Rd, and (2) an inverse map g(·, η) : z (cid:55)→ ˆx ∈ RD such that x and\n\nˆx = g(f (x)) is close. In other words, X\n\nf (x,θ) −−−−→ Z\n\ng(z,η)\n\n−−−−→ ˆX forms an autoencoding.\n\nIn this work, we specifically learn this mapping in an entirely unsupervised fashion, without knowing the ground-truth class labels Π at all. As stated in the introduction, a both discriminative and generative representation is difficult to achieve by standard generative methods like VAEs and GANs. This is one of the motivations for the closed-loop transcription framework (CTRL) proposed by (Dai et al., 2022), which we will generalize to the unsupervised setting.\n\nMaximizing Rate Reduction. The CTRL framework (Dai et al., 2022) was proposed for the supervised setting, where it aims to map each class onto an independent linear subspace. As shown in (Yu et al., 2020), such a linear discriminative representation (LDR) can be achieved by maximizing a coding rate reduction objective, known as the MCR2 principle:\n\n∆R(cid:0)Z|Π)\n\n. =\n\n1 2\n(cid:124)\n\n(cid:18)\n\nlog det\n\nI +\n\nd\n\nN ε2 ZZ⊤\n\n(cid:123)(cid:122) R(Z)\n\ntr(Πj) 2N\n\n(cid:19)\n\n−\n\nk (cid:88)\n\n(cid:125)\n\nj=1 (cid:124)\n\n(cid:18)\n\nlog det\n\nI +\n\n(cid:123)(cid:122) Rc\n\nd\n\ntr(Πj)ε2 ZΠjZ⊤\n\n(cid:19)\n\n. (1)\n\n(cid:125)\n\nwhere each Πj encodes the membership of the N samples described before. As discussed in (Chan et al., 2022), the first term R(Z) measures the total rate (volume) of all features whereas the second term Rc measures the average rate (volume) of the k components. Our work adapts this formula to design meaningful objectives in the unsupervised setting.\n\n−−−−→ ˆX, a fundamental Closed-Loop Transcription. To learn the autoencoding X question is how we measure the difference between X and the regenerated ˆX = g(f (X)). It is typically very difficult to put a proper distance measure in the image space (Wang et al., 2004). To bypass this difficulty, the closed-loop transcription framework (Dai et al., 2022) proposes to measure the difference between X and ˆX through the difference between their features Z and ˆZ mapped through the same encoder:\n\nf (x,θ) −−−−→ Z\n\ng(z,η)\n\n−−−−−−→ ˆX (2) The difference can be measured by the rate reduction between Z and ˆZ, a special case of (1) with k = 2 classes:\n\n−−−−−−→ ˆZ.\n\n−−−−−−→ Z\n\nX\n\nf (x,θ)\n\ng(z,η)\n\nf (x,θ)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n∆R(cid:0)Z, ˆZ(cid:1) .\n\n= R(cid:0)Z ∪ ˆZ(cid:1) −\n\n(cid:0)R(cid:0)Z) + R(cid:0) ˆZ)(cid:1).\n\n(3)\n\n1 2\n\nSuch a ∆R is a principled distance between subspace-like Gaussian ensembles, with the property that ∆R(Z, ˆZ) = 0 iff Cov(Z) = Cov( ˆZ) (Ma et al., 2007).\n\nAs shown in (Dai et al., 2022), applying this measure in the closed-loop CTRL formulation can already learn a decent autoencoding, even without class information. This is known as the CTRL-Binary program:\n\nmax θ\n\nmin η\n\n∆R(Z, ˆZ)\n\n(4)\n\nHowever, note that (4) is practically limited because it only aligns the dataset X and the regenerated ˆX at the distribution level. There is no guarantee that for each sample x would be close to the decoded ˆx = g(f (x)). For example, (Dai et al., 2022) shows that a car sample can be decoded into a horse; the so obtained (autoencoding) representations are not sample-wise self-consistent! 3.2 SAMPLE-WISE CONSTRAINTS FOR UNSUPERVISED TRANSCRIPTION\n\nTo improve discriminative and generative properties of representations learned in the unsupervised setting, we propose two additional mechanisms for the above CTRL-Binary maximin game (4). For simplicity and uniformity, here these will be formulated as equality constraints over rate reduction measures, but in practice they can be enforced softly during optimization.\n\nSample-wise Self-Consistency via Closed-Loop Transcription. First, to address the issue that CTRL-Binary does not learn a sample-wise consistent autoencoding, we need to promote ˆx to be close to x for each sample. In the CTRL framework, this can be achieved by enforcing that their corresponding features z = f (x) and ˆz = f ( ˆx) are the same or close. To promote sample-wise self-consistency, where ˆx = g(f (x)) is close to x , we want the distance between z and ˆz to be zero or small, for all N samples. This can be formulated using rate reduction; note that this again avoids measuring differences in the image space: (cid:88)\n\n∆R(zi, ˆzi) = 0.\n\n(5)\n\ni∈N\n\nSelf-Supervision via Compressing Augmented Samples. Since we do not know any class label information between samples in the unsupervised settings, the best we can do is to view every sample and its augmentations (say via translation, rotation, occlusion etc) as one “class” — a basic idea behind almost all self-supervised learning methods. In the rate reduction framework, it is natural to compress the features of each sample and its augmentations. In this work, we adopt the standard transformations in SimCLR (Chen et al., 2020b) and denote such a transformation as τ . We denote each augmented sample xa = τ (x), and its corresponding feature as za = f (xa, θ). For discriminative purposes, we hope the classifier is invariant to such transformations. Hence it is natural to enforce that the features za of all augmentations are the same as that z of the original sample x. This is equivalent to requiring the distance between z and za, measured in terms of rate reduction again, to be zero (or small) for all N samples:\n\n∆R(zi, zi\n\na) = 0.\n\n(cid:88)\n\ni∈N\n\n(6)\n\n3.3 UNSUPERVISED REPRESENTATION LEARNING VIA CLOSED-LOOP TRANSCRIPTION So far, we know the CTRL-Binary objective ∆R(Z, ˆZ) in (4) helps align the distributions while sample-wise self-consistency (5) and sample-wise augmentation (6) help align and compress features associated with each sample. Besides consistency, we also want learned representations are maximally discriminative for different samples (here viewed as different “classes”). Notice that the rate distortion term R(Z) measures the coding rate (hence volume) of all features. It has been observed in (Li et al., 2022) that by maximizing this term, learned features expand and hence become more discriminative.\n\nUnsupervised CTRL. Putting these elements together, we propose to learn a representation via the following constrained maximin program, which we refer to as unsupervised CTRL (U-CTRL):\n\nmax θ\n\nmin η\n\nsubject to\n\nR(Z) + ∆R(Z, ˆZ)\n\n(7)\n\n∆R(zi, ˆzi) = 0, and\n\n(cid:88)\n\ni∈N\n\n∆R(zi, zi\n\na) = 0.\n\n(cid:88)\n\ni∈N\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nIn practice, the above program can be optimized by alternating maximization and minimization between the encoder f (·, θ) and the decoder g(·, η). We adopt the following optimization strategy that works well in practice, which is used for all subsequent experiments on real image datasets: (cid:88)\n\n(cid:88)\n\nR(Z) + ∆R(Z, ˆZ) − λ1\n\n∆R(zi, zi\n\na) − λ2\n\n∆R(zi, ˆzi);\n\n(8)\n\nmax θ\n\nR(Z) + ∆R(Z, ˆZ) + λ1\n\nmin η\n\ni∈N (cid:88)\n\ni∈N\n\n∆R(zi, zi\n\na) + λ2\n\ni∈N (cid:88)\n\ni∈N\n\n∆R(zi, ˆzi),\n\n(9)\n\nwhere the constraints (cid:80) i∈N ∆R(zi, ˆzi) = 0 and (cid:80) (and relaxed) to Lagrangian terms with corresponding coefficients λ1 and λ2.1\n\ni∈N ∆R(zi, zi\n\na) = 0 in (7) have been converted\n\nUnsupervised Conditional Image Generation via Rate Reduction. The above representation is learned without class information. In order to facilitate discriminative or generative tasks, it must be highly structured. As we will see via experiments, specific and unique structure indeed emerges naturally in the representations learned using U-CTRL: globally, features of images in the same class tend to be clustered well together and separated from other classes (Figure 2); locally, features around individual samples exhibit approximately piecewise linear low-dimensional structures (Figure 5).\n\nThe highly-structured feature distribution also suggests that the learned representation can be very useful for generative purposes. For example, we can organize the sample features into meaningful clusters, and model them with low-dimensional (Gaussian) distributions or subspaces. By sampling from these compact models, we can conditionally regenerate meaningful samples from computed clusters. This is known as unsupervised conditional image generation (Hwang et al., 2021).\n\nTo cluster features, we exploit the fact that the rate reduction framework (1) is inspired by unsupervised clustering via compression (Ma et al., 2007), which provides a principled way to find the membership Π. Concretely, we maximize the same rate reduction objective (1) over Π, but fix the learned representation Z instead. We simply view the membership Π as a nonlinear function of the features Z, say hπ(·, ξ) : Z (cid:55)→ Π with parameters ξ. In practice, we model this function with a simple neural network, such as an MLP head right after the output feature z. To estimate a “pseudo” membership ˆΠ of the samples, we solve the following optimization problem over Π:\n\nˆΠ = arg max\n\nξ\n\n∆R(Z|Π(ξ)).\n\n(10)\n\nExperiments in Section 4.2 demonstrate that conditional image generation from clusters produced in this manner result in high-quality images that are highly similar in style.\n\n4 EXPERIMENTS\n\nWe now evaluate the performance of the proposed U-CTRL framework and compare it with representative unsupervised generative and discriminative methods. The first set of experiments (Section 4.1 show that despite being a generative method in nature, U-CTRL can learn discriminative representations competitive with state-of-the-art discriminative methods. The second set (Section 4.2) show that the learned generative representation can significantly boost the performance of unsupervised conditional image generation. Finally, the third set (Section 4.3) study the advantages that generative represeentations have over discriminative ones.\n\nWe conduct experiments on the following datasets: CIFAR-10 (Krizhevsky et al., 2014), CIFAR-100 (Krizhevsky et al., 2009), and Tiny ImageNet (Deng et al., 2009). Standard augmentations for self-supervised learning are used across all datasets (Chen et al., 2020b).\n\nWe design all experiments to ensure that comparisons against U-CTRL are fair. For all methods that we compare against, we ensure that experiments are conducted with similar model sizes. If code for similar size structure can not be found, we uniformly use ResNet-18 to reproduce results for baselines, which is larger than the network used by our method. Details about network architectures and the experimental setting are given in Appendix A. All methods have runned 400 epochs or equivalent iterations (because generative models often count in iteration).\n\n1Notice that computing the rate reduction terms ∆R for all samples or a batch of samples requires computing the expensive log det of large matrices. In practice, from the geometric meaning of ∆R for two vectors, ∆R can be approximated with an l2 norm or the cosine distance between two vectors.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nGAN based methods\n\nSSGAN-LA(Hou et al., 2021) DAGAN+(Antoniou et al., 2017) ContraD(Jeong & Shin, 2021)\n\nVAE based methods\n\nPATCH-VAE (Parmar et al., 2021) β-VAE (Higgins et al., 2016)\n\nCTRL based methods\n\nCTRL-Binary(Dai et al., 2022) U-CTRL (ours)\n\nCIFAR-10 Accuracy\n\nCIFAR-100 Accuracy\n\nTiny-ImageNet Accuracy\n\n0.803 0.772 0.852\n\n0.471 0.531\n\n0.599 0.874\n\n0.543 0.519 0.514\n\n0.325 0.315\n\n- 0.552\n\n0.344 0.224 -\n\n- -\n\n- 0.360\n\nTable 2: Comparison of classification accuracy on CIFAR-10, CIFAR-100, and Tiny-ImageNet with other generative self-supervised learning methods. U-CTRL is clearly better.\n\nMethod\n\nSIMCLR MoCoV2 BYOL U-CTRL (ours)\n\nCIFAR-10 Accuracy\n\nCIFAR-100 Accuracy\n\nTiny-ImageNet Accuracy\n\n0.869 0.872 0.883 0.874\n\n0.545 0.589 0.581 0.552\n\n0.359 0.365 0.371 0.360\n\nTable 3: Comparison of classification accuracy on CIFAR-10, CIFAR-100, and Tiny-ImageNet with purely discriminative self-supervised learning methods. U-CTRL is close to these non-generative methods. 4.1 DISCRIMINATIVE QUALITY OF LEARNED REPRESENTATIONS\n\nTo evaluate the discriminative quality of the learned representations, we follow the standard practice of evaluating the accuracy of a simple linear classifier trained on the learned representation. Table 2 compares our method against SOTA generative self-supervised learning methods, and Table 3 compares our method against SOTA discriminative self-supervised methods. Experimental and training details are given in Appendix A.\n\nQuantitative Comparisons of Classification Performance. From Table 2, we observe that on all chosen datasets, our method achieves substantial improvements compared to existing generative self-supervised learning methods. This includes more complex datasets like CIFAR-100 and TinyImageNet, where we surpass the current SOTA models. From Table 3, our method achieves similar performance compared to SOTA discriminative self-supervised models. These results echo our goal of seeking a more unifed generative and discriminative representations: despite resembling a generative method architecturally, our method still produces highly discriminative representations. In addition, these results lead us to ask a fundamental question: when is incorporating both discriminative and generative properties greater than seperately handling these two parts? We provide preliminary answers in Section 4.3.\n\nQualitative Visualization of Learned Representations. To explain the classification performance of our method, we visualize the incoherence between features learned for the training datasets. Figure 2 shows cosine similarity heatmaps between the learned features, organized by ground-truth class labels. A block-diagonal pattern emerges automatically from U-CTRL training for all three datasets, similar to those observed in features learned in a supervised setting (Dai et al., 2022). In this case, however, these blocks emerge and correspond with classes labels despite the absence of any supervision at all.\n\n4.2\n\nIMPROVED UNSUPERVISED CONDITIONAL GENERATION QUALITY\n\nTo evaluate the quality of unsupervised conditional image generation, we measure performance on two axes: cluster quality and image quality. We estimate clusters by optimizing (10), and show results and comparisons with both recent and classical methods in Table 4. Training details of our method for the additional MLP head can be found in the Appendix A.\n\nCluster Quality. We measure normalized mutual information (NMI) and clustering accuracy for cluster quality on CIFAR-10 clustered into 10 classes and CIFAR-100(20), which is clustered into 20 super-classes. From Table 4, we observe that on CIFAR-10, U-CTRL results in an NMI that is almost\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) CIFAR-10\n\n(b) CIFAR-100\n\n(c) Tiny ImageNet\n\nFigure 2: Emergence of block-diagonal structures of |Z ⊤Z| in the feature space for CIFAR-10 (left), 10 random classes from CIFAR-100 (middle), and 10 random classes from Tiny ImageNet (right).\n\n(a) CIFAR-10 X\n\n(b) CIFAR-10 ˆX\n\nFigure 3: Sample-wise self-consistency: visualization of images X and reconstructed ˆX on CIFAR-10 dataset.\n\ndouble that of the existing SOTA on both GAN-based and VAE-based methods, with significantly improved clustering accuracy. Unlike many baselines, we also demonstrate that our method scales to the more challenging CIFAR-100(20) dataset, where it also significantly outperforms alternatives. Our improved clustering quality suggests potential for improving unsupervised conditional image generation, which relies on first finding statistically (and hence visually) meaningful clusters.\n\nImage Quality. We use Frechet Inception Distance (FID) (Heusel et al., 2017) and Inception Score (IS) (Salimans et al., 2016) to measure image quality. From Table 4, it is evident that U-CTRL maintains competitive image quality compared to other methods, measured both by FID and IS. We also compare original images against reconstructed ones in Figure 3, where we see that the original X is very similar to the reconstructed ˆX; U-CTRL indeed achieves very good sample-wise self-consistency.\n\nUnsupervised Conditional Image Generation. In Figure 4, we visualize images generated from the ten unsupervised clusters from (10). Each block represents one cluster and each row represents one principal component for each cluster. Despite learning and training without labels, the model not only organizes samples into correct clusters, but is also able to preserve statistical diversities within each cluster/class. We can easily recover the diversity within each cluster by computing different principal components and then sample and generate accordingly! More detailed illustrations with more samples is provided in Appendix B.\n\n4.3 BENEFITS OF U-CTRL’S STRUCTURED REPRESENTATION\n\nAs shown in the previous section, on datasets like CIFAR-10, CIFAR-100, and Tiny-ImageNet, our framework is able to achieve representation quality close to with the best discriminative selfsupervised learning methods. A clear advantage of this is computational efficiency; only a single representation needs to be trained for a much broader set of tasks. This subsection aims to provide additional insights on how a unified model can be more beneficial for a broader range of tasks.\n\nDomain Transfer. Regenerating images is demanding on the encoder, which is required to produce a more informative representation than contrastive training would. We hypothesize that the encoder trained with generative task may retain more information about the image and allow the representation to generalize better. To verify this, we compare the accuracy on CIFAR-100 using models learned from CIFAR-10 in Table 5. When compared to purely discriminative self-supervised learning models, we observe that U-CTRL is 4 percent better than other methods on classification accuracy.\n\nVisualization of Emerged Structures. The representations learned by U-CTRL are significantly different from those learned from previous either discriminative and generative methods. To illustrate this, we use t-SNE (Van der Maaten & Hinton, 2008) to visualize the learned representation in 2D. Figure 5 compares the t-SNE of representations learned for CIFAR-10 by U-CTRL and MoCoV2,\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Unsupervised conditional image generation from each cluster of CIFAR-10, using U-CTRL. Images from different rows mean generation from different principal components of each cluster.\n\nMethod\n\nGAN based methods\n\nCIFAR-10\n\nCIFAR-100(20)\n\nNMI Accuracy FID↓ IS↑ NMI Accuracy FID↓ IS↑\n\nSelf-Conditioned GAN (Liu et al., 2020) 0.333 0.340 SLOGAN (Hwang et al., 2021)\n\n0.117 -\n\n18.0 20.6\n\nVAE based methods\n\nGMVAE(Dilokthanakul et al., 2016) Variational Clustering\n\n- -\n\n0.247 0.445\n\n- -\n\n7.7 0.214\n\n-\n\n- -\n\n-\n\n- -\n\n0.092 -\n\n24.1 -\n\n5.2 -\n\n- -\n\n- -\n\n- -\n\nCTRL based methods\n\nU-CTRL (ours)\n\n0.658\n\n0.799\n\n17.4\n\n8.1 0.374\n\n0.433\n\n20.1\n\n7.7\n\nTable 4: Comparison of the quality of UCIG on CIFAR-10 and CIFAR-100(20). Many of the methods compared do not provide code that scales up to CIFAR-100(20), in which case we leave the corresponding table cell blank.\n\nrespectively. It is clear that the representation learned by U-CTRL are highly structured and organized: classes are more evident, and features within each class form clear piecewise linear structures. We present more t-SNE comparisons in Appendix E.\n\nMethod\n\nSIMCLR MoCoV2 BYOL U-CTRL\n\nAccuracy\n\n0.422\n\n0.436\n\n0.437\n\n0.481\n\nTable 5: Comparing the transfer ability with purely discriminative self-supervised learning methods. All methods are trained unsupervised on CIFAR-10 and tested on CIFAR-100.\n\n(a) U-CTRL\n\n(b) MoCoV2\n\nFigure 5: t-SNE visualizations of learned features of CIFAR-10 with different models.\n\n5 CONCLUSION AND DISCUSSION In this work, we proposed an unsupervised formulation of the closed-loop transcription framework (Dai et al., 2022). We experimentally demonstrate that it is possible to learn a unified representation for both discriminative and generative purposes, resulting in highly structured representations. Further, we show that these two purposes mutually benefit each other in various tasks, e.g., conditional image generation and domain tranfers. Compared to the more specialized representations learned in prior works, our results suggest that such a unified representation has the potential in supporting and benefiting a wider range of new tasks. In future work, we believe the learned representations can be further improved by jointly optimizing the feature representation and feature clusters, as suggested in the original rate reduction paper (Chan et al., 2022). Features with high likelihood of belonging to the same cluster can be further linearized and compressed. Due to its unifying nature and the simplicity of the underlying concepts, this new framework may be extended beyond image data, such as sequential or dynamical observations.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nAll authors agree and will adhere to the conference’s Code of Ethics. We do not anticipate any potential ethics issues regarding the research conducted in this work.\n\nREPRODUCIBILITY STATEMENT\n\nSettings and implementation details of network architectures, optimization methods, and some common hyper-parameters are described in the Appendix A. We will also make our source code available upon request by the reviewers or the area chairs.\n\nREFERENCES\n\nAntreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial\n\nnetworks. arXiv preprint arXiv:1711.04340, 2017.\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization\n\nfor self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\n\nKwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. ReduNet: A white-box deep network from the principle of maximizing rate reduction. Journal of Machine Learning Research, 23(114):1–103, 2022. URL http://jmlr.org/papers/v23/ 21-0631.html.\n\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pp. 1691– 1703. PMLR, 2020a.\n\nTing Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via auxiliary rotation loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12154–12163, 2019.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020b.\n\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29, 2016.\n\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2020.\n\nXili Dai, Shengbang Tong, Mingyang Li, Ziyang Wu, Michael Psenka, Kwan Ho Ryan Chan, Pengyuan Zhai, Yaodong Yu, Xiaojun Yuan, Heung-Yeung Shum, et al. Ctrl: Closed-loop transcription to an ldr via minimaxing rate reduction. Entropy, 24(4):456, 2022.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nNat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.\n\nJeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\n\narXiv:1605.09782, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nVincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.\n\nWilliam Falcon, Ananya Harsh Jha, Teddy Koker, and Kyunghyun Cho. Aavae: Augmentation-\n\naugmented variational autoencoders. arXiv preprint arXiv:2107.12329, 2021.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271–21284, 2020a.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020b.\n\nKamal Gupta, Saurabh Singh, and Abhinav Shrivastava. Patchvae: Learning local latent codes for recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4746–4755, 2020.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked\n\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. arXiv preprint arXiv:1804.03599, 2016.\n\nLiang Hou, Huawei Shen, Qi Cao, and Xueqi Cheng. Self-supervised gans with label augmentation.\n\nAdvances in Neural Information Processing Systems, 34, 2021.\n\nUiwon Hwang, Heeseung Kim, Dahuin Jung, Hyemi Jang, Hyungyu Lee, and Sungroh Yoon. Stein latent optimization for generative adversarial networks. arXiv preprint arXiv:2106.05319, 2021.\n\nJongheon Jeong and Jinwoo Shin. Training gans with stronger augmentations via contrastive\n\ndiscriminator. arXiv preprint arXiv:2103.09742, 2021.\n\nZhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep embedding: An unsupervised and generative approach to clustering. arXiv preprint arXiv:1611.05148, 2016.\n\nSheena A. Josselyn and Susumu Tonegawa. Memory engrams: Recalling the past and imagining the\n\nfuture. Science, 367, 2020.\n\nGeorg B Keller and Thomas D Mrsic-Flogel. Predictive processing: A canonical cortical computation.\n\nNeuron, 100(2):424–435, October 2018.\n\nSaehoon Kim, Sungwoong Kim, and Juho Lee. Hybrid generative-contrastive representation learning.\n\narXiv preprint arXiv:2106.06162, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n\nThe CIFAR-10 dataset.\n\nonline:\n\nhttp://www.cs.toronto.edu/kriz/cifar.html, 55, 2014.\n\nAlex Krizhevsky et al. Learning multiple layers of features from tiny images. arXiv preprint\n\narXiv:1312.6114, 2009.\n\nZengyi Li, Yubei Chen, Yann LeCun, and Friedrich T Sommer. Neural manifold clustering and\n\nembedding. arXiv preprint arXiv:2201.10000, 2022.\n\nSteven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, and Antonio Torralba. Diverse image generation via self-conditioned gans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14286–14295, 2020.\n\nYi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data via\n\nlossy data coding and compression. PAMI, 2007.\n\nSudipto Mukherjee, Himanshu Asnani, Eugene Lin, and Sreeram Kannan. Clustergan: Latent space clustering in generative adversarial networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 4610–4617, 2019.\n\nGaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n\nautoencoder. Recognition, pp. 823–832, 2021.\n\nVignesh Prasad, Dipanjan Das, and Brojeshwar Bhowmick. Variational clustering: Leveraging variational autoencoders for image clustering. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–10. IEEE, 2020.\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\n\nShengbang Tong, Xili Dai, Ziyang Wu, Mingyang Li, Brent Yi, and Yi Ma. Incremental learning of\n\nstructured memory via closed-loop transcription. arXiv:2202.05411, 2022.\n\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\n\nlearning research, 9(11), 2008.\n\nGido M Ven, Hava T Siegelmann, Andreas S Tolias, et al. Brain-inspired replay for continual learning\n\nwith artificial neural networks. Nature Communications, 11(1):1–14, 2020.\n\nZhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004. doi: 10.1109/TIP.2003.819861.\n\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discriminative representations via the principle of maximal coding rate reduction. Advances in Neural Information Processing Systems, 33:9422–9434, 2020.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pp. 12310– 12320. PMLR, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA TRAINING DETAILS\n\nA.1 NETWORK ARCHITECTURES\n\nTable 6, 7 and Figure 6 give details on the network architecture for the decoder and the encoder networks used for experiments. The black rectangle marked with \"conv, s=2\" means a convlutional layer with stride 2. The orange rectangle marked with \"dconv, s=2\" means a deconvolutional layer with stride 2. The \"x k\" besides red frame means we regard these layers in red frame as a block and stack it k times. All α values in Leaky-ReLU (i.e. lReLU) of the encoder are set to 0.2. We set (nz = 128, nc = 3, k = 3) for CIFAR-10, (nz = 256, nc = 3, k = 4) for CIFAR-100, and (nz = 256, nc = 3, k = 4) for Tiny-ImageNet. As a comparison, ResNet-18 contains around 11 million parameters, whereas our encoder only contains between 4 and 6 million parameters depending on the choice of k.\n\nTable 8 gives details of the network architecture for the linear classifier and Table 9 gives details of the network architecture for the additional MLP head used for unsupervised conditional image generation training.\n\nz ∈ R1×1×nz ResBlockUp. 256 ResBlockUp. 128 ResBlockUp. 64 4 × 4, stride=2, pad=1 deconv. 1 Tanh\n\nTable 6: Network architecture of the decoder g(·, η).\n\nImage x ∈ R32×32×nc ResBlockDown 64 ResBlockDown 128 ResBlockDown 256 4 × 4, stride=1, pad=0 conv nz\n\nTable 7: Network architecture of the encoder f (·, θ).\n\n(a) ResBlock Up architecture\n\n(b) ResBlock Down architecture\n\nFigure 6: Architecture of two ResBlock.\n\nA.2 OPTIMIZATION\n\nFor all experiments, we use Adam (Kingma & Ba, 2014) as our optimizer, with hyperparameters β1 = 0.5, β2 = 0.999. The learning rate is set to be 0.0001. We choose ε2 = 0.2. For all experiments, we adopt augmentation from SimCLR (Chen et al., 2020b).\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nz ∈ R1×1×nz Linear(nz, number of class)\n\nTable 8: Network architecture of the linear classifier.\n\nz ∈ R1×1×nz Linear(nz, nz) ReLU Linear(nz, number of clusters)\n\nTable 9: Network architecture of the MLP head for unsupervised conditional image generation\n\nFor CIFAR-10, CIFAR-100, and Tiny ImageNet, we train our framework with a batch size of 1024 over 20,000 iterations. All experiments are conducted with at most 4 RTX 3090 GPUs. Methods that are compared against in Table 3 are trained with the batch size of 256, because Chen et al. (2020b) observe that purely discriminative methods tend to perform better with smaller batch sizes. Table 2 methods have used their optimal parameters in their github code.\n\nFor training of the MLP head for unsupervised conditional image generation(10), we again use Adam (Kingma & Ba, 2014) as our optimizer with hyperparameters β1 = 0.5, β2 = 0.999. We choose the learning rate to be 0.0001 and ε2 as 0.2, with batch size 1024 over 5000 iterations.\n\nFor training of the linear classifier, we use Adam (Kingma & Ba, 2014) as our optimizer with hyperparameters β1 = 0.5, β2 = 0.999. We choose learning rate to be 0.0001, with batch size 1024 over 5000 iterations.\n\nB ADDITIONAL UNSUPERVISED CLUSTERING AND GENERATION RESULTS\n\nB.1 CLUSTER RECONSTRUCTION\n\nIn this subsection, we visualize the reconstruction of ten clusters that are predicted and generated by U-CTRL on the CIFAR-10 training set. Each block in Figure 7 contains both a random sample of reconstructed data in a cluster and the total number of samples within it. Note that CIFAR-10 contains 50,000 training samples, split across 10 classes. As we see in Figure 7, the number of samples in each cluster are very close to 5,000, with the largest deviator (cluster 9) containing 3,942 samples. Without any cues, one can easily identify correspond each unsupervised cluster with a CIFAR-10 class. For a class like ‘bird’, we observe that the model is able to group images of standing birds, flying birds, and bird heads, despite their visual differences.\n\nB.2 UNSUPERVISED CONDITIONAL IMAGE GENERATION\n\nBuilding on U-CTRL’s ability to cluster CIFAR-10 samples, we demonstrate the model’s ability to perform unsupervised conditional image generation in Figure 8. In contrast to reconstruction, where images are regenerated from features corresponding to real samples, we generate images based on the feature sampling technique proposed in (Dai et al., 2022). From these results, we observe that the U-CTRL framework maintains in-cluster diversity, and that the diversity can be recovered and visualized via simple principal component analysis.\n\nC ABLATION STUDIES\n\nC.1 THE IMPORTANCE OF EACH TERM IN U-CTRL FORMULATION\n\nIn this section, we study the significance of the sample-wise constraints and extra rate distortion term in the formulation 7. Table 10 presents the following objectives that we study:\n\n• Objective I is the constrained U-CTRL maximin 7.\n\n• Objective II is the constrained maximin without the augmentation compression constraint 6.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Cluster 1\n\n(b) Cluster 2\n\n(c) Cluster 3\n\n(d) Cluster 4\n\n(e) Cluster 5\n\n(f) Cluster 6\n\n(g) Cluster 7\n\n(h) Cluster 8\n\n(i) Cluster 9\n\n(j) Cluster 10\n\nFigure 7: More result on the reconstruction of clusters in CIFAR-10\n\n• Objective III is the constrained maximin without the sample-wise self-consistency constraint 5.\n\n• Objective IV is the constrained maximin without the extra rate distortion term.\n\n• Objective V is the U-CTRL without the augmentation compression constraint and sample-wise\n\nself-consistency constraint.\n\n• Objective VI is the CTRL-Binary maximin formulation 4.\n\nTable 11 shows the result of a linear probe for representations trained using each objective on CIFAR10. From the table, it is evident that both constraints and the rate distortion term are pivotal to the success of our framework.\n\nmaxθ minη R(Z) + ∆R(Z, ˆZ) s.t. (cid:80) Objective I: maxθ minη R(Z) + ∆R(Z, ˆZ) s.t. (cid:80) Objective II: Objective III: maxθ minη R(Z) + ∆R(Z, ˆZ) s.t. (cid:80) Objective IV: maxθ minη ∆R(Z, ˆZ) s.t. (cid:80) Objective V: Objective VI: maxθ minη ∆R(Z, ˆZ)\n\nmaxθ minη R(Z) + ∆R(Z, ˆZ)\n\ni∈N ∆R(zi, ˆzi) = 0, and (cid:80) i∈N ∆R(zi, ˆzi) = 0 i∈N ∆R(zi, zi a) = 0\n\ni∈N ∆R(zi, zi\n\na) = 0\n\ni∈N ∆R(zi, ˆzi) = 0, and (cid:80)\n\ni∈N ∆R(zi, zi\n\na) = 0\n\nTable 10: Five different objective functions for U-CTRL.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Cluster 1\n\n(b) Cluster 2\n\n(c) Cluster 3\n\n(d) Cluster 4\n\n(e) Cluster 5\n\n(f) Cluster 6\n\n(g) Cluster 7\n\n(h) Cluster 8\n\n(i) Cluster 9\n\nFigure 8: Unsupervised conditional image generation on CIFAR-10. Each block represents a cluster, within which each row represents one principal component direction in the cluster, and samples along each row represent different noises applied in that principal direction.\n\n(j) Cluster 10\n\nC.2 THE IMPORTANCE OF MCR2 IN U-CTRL FORMULATION\n\nIn this section, we verify the significance of MCR2 term ∆R(Z, ˆZ) in our method. We do ablation study on CIFAR-10 with the same network and training condition. If we take away MCR2 from our\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nObjective I Objective II Objective III Objective IV Objective V Objective VI\n\nAccuracy\n\n0.874\n\n0.578\n\n0.644\n\n0.522\n\n0.633\n\n0.599\n\nformulation, it changes (11). For simplicity, we call it U-CTRL-noMCR2\n\nTable 11: Ablation study on the significance of different terms in U-CTRL.\n\nmax θ\n\nmin η\n\nR(Z)\n\nsubject to\n\n(cid:88)\n\ni∈N\n\n∆R(zi, ˆzi) = 0, and\n\n∆R(zi, zi\n\na) = 0.\n\n(cid:88)\n\ni∈N\n\n(11)\n\nTable 12 shows that U-CTRL without the MCR2 not only learns worse representation but also generalizes worse to out of distribution data. Figure 9 visualizes the reconstructed ˆX by U-CTRLnoMCR2. It is clear from the image figure that without the MCR2, the decoder fails to reconstruct high-quality images.\n\nAccuracy on CIFAR-10 Transfer Accuracy on CIFAR-100 0.874 U-CTRL U-CTRL-noMCR2 0.836\n\n0.481 0.418\n\nTable 12: Ablation study on the significance of MCR2 in U-CTRL.\n\n(a) CIFAR-10 X\n\n(b) CIFAR-10 ˆX\n\nFigure 9: Visualization of images trained by U-CTRL-noMCR2: X and reconstructed ˆX on CIFAR-10 dataset.\n\nIt follows our discussion in the introduction that discriminative tasks and generative tasks together learn feature that benifits each other.\n\nD RANDOM SEED SENSITIVITY\n\nIn this section, we verify the stability of our method against different random seeds. We report in Table 13 the accuracy of U-CTRL on CIFAR-10 with different seeds. We observe that the choice of seed has very little impact on performance.\n\nRandom Seed\n\n1\n\n5\n\n10\n\n15\n\n100\n\nAccuracy\n\n0.874\n\n0.876\n\n0.870\n\n0.874\n\n0.871\n\nTable 13: Ablation study on varying random seeds.\n\nE MORE COMPARISON ON T-SNE\n\nDue to limited space in the main body, we present a comparison of t-SNE between u-CTRL and other discriminative-based methods in this section. As shown in Figure 10, u-CTRL enjoys more structured representation comparing to purely discriminative methods.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) U-CTRL\n\n(b) MoCoV2\n\n(c) BYOL\n\n(d) SimCLR\n\nFigure 10: t-SNE visualizations of learned features of CIFAR-10 with different models.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis work proposes an unsupervised extension of the closed-loop transcription, named as U-CTRL, to learn the better representation for discriminative and generative learning tasks. The authors show that the learned representation is more structured, i.e. having the cluster shapes in semantically-meaningful way, improving the performance of linear evaluation and conditional generative tasks.\n\n# Strength And Weaknesses\n\n**Strengths**:\n\n* This manuscript is basically very well-written, and clearly summarizes the necessary previous works to understand the main argument. It's easy to follow most of the parts in the manuscript. \n* It seems that the approach based on closed-loop transcription and rate-distortion theory for learning better representation is quite principal. \n* This manuscript includes all the details in the experiments, including detailed setup, hyperparameters, and network configuration. It seems that all the results are reproducible. \n\n**Weaknesses**: \n\nMy biggest concern on this work is the low performance of discriminative learning shown in Table 2. Compared to the similar baselines, Table 2 is not enough to argue that the learned representation by U-CTRL is highly discriminative. I extracted the linear evaluation experiments in GCRL and W-MSE, as shown in below:\n\n|Method| CIFAR-10 | CIFAR-100 | Tiny ImageNet|\n|----------|----------|----------|----------|\n|U-CTRL| 87.4 | 55.2 | 36.0 | \n|GCRL | 83.9-95.1 | 58.7 - 76.0 | - | \n|W-MSE | 91.6 - 92.0 | 66.1 - 67.6 | 48.2 - 49.2 |\n\nBesides the comparison to GCRL and W-MSE, I’ve found that the performance of BYOL in Table 2 is much worse than the reported numbers in Table 1 in W-MSE paper. Could you explain a little bit more why the performance gap is observed? \n\nI’m also still not convinced that the learned representation based on U-CTRL is more structured than SSL methods, especially the methods based on contrastive losses. Therefore, it could be necessary to measure the cluster quality of representation learned by contrastive losses, since the contrastive loss enforces the semantically-meaningful clusters in the representation space. In this work, U-CTRL was only compared to the learned representation based on generative learning, which is not sufficient to argue that the representation from U-CTRL is more structured than the ones from existing approaches. Figure 5 seems to support the argument, but it could be better to include the results of SimCLR and BYOL. \n\nThis work also argues that the structured representation improves the quality of generated samples a lot. However, as shown in unCLIP (a.k.a. DALL-E 2), the representation based on a simple contrastive loss definitely helps to train a decoder. It means that the learned representation based on contrastive losses is already structured enough to train a generative model.\n\nReferences:\n* [W-MSE] Whitening for Self-Supervised Representation Learning, ICML’21. \n* [GCRL] Hybrid Generative-Contrastive Representation LEarning, arXiv’21.\n* [unCLIP] Hierarchical Text-Conditional Image Generation with CLIP Latents, arXiv’22.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIn terms of clarity, I really enjoyed reading the manuscript. Most parts of this manuscript is clearly well-written. And, in terms of reproducibility, many details are well described, so I believe that all the results could be reproducible. \n\nIn terms of quality and novelty, as described in the section above, I’ve raised some issues on the experiments and motivation.\n\n# Summary Of The Review\n\nMy initial evaluation is borderline, slightly leaning towards rejection. My major concern on this work is the insufficient empirical justification. In specific, I’m not sure that the learned representation based on U-CTRL is discriminative enough compared to the similar recent works. In addition, I failed to figure out that the representation learned by U-CTRL is more structured than the ones by contrastive losses.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSYNERGIES BETWEEN DISENTANGLEMENT AND SPARSITY: A MULTI-TASK LEARNING PERSPECTIVE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAlthough disentangled representations are often said to be beneficial for downstream tasks, current empirical and theoretical understanding is limited. In this work, we provide evidence that disentangled representations coupled with sparse base-predictors improve generalization. In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations. Motivated by this theoretical result, we propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem. Finally, we explore a meta-learning version of this algorithm based on group Lasso multiclass SVM base-predictors, for which we derive a tractable dual formulation. It obtains competitive results on standard few-shot classification benchmarks, while each task is using only a fraction of the learned representations.\n\n1\n\nINTRODUCTION\n\nThe recent literature on self-supervised learning has provided evidence that learning a representation on large corpuses of data can yield strong performances on a wide variety of downstream tasks (Devlin et al., 2018; Chen et al., 2020), especially in few-shot learning scenarios where the training data for these tasks is limited (Brown et al., 2020b; Dosovitskiy et al., 2021; Radford et al., 2021). Beyond transferring across multiple tasks, these learned representations also lead to improved robustness against distribution shifts (Wortsman et al., 2022) as well as stunning text-conditioned image generation (Ramesh et al., 2022). However, preliminary assessments of the latter has highlighted shortcomings related to compositionality (Marcus et al., 2022), suggesting new algorithmic innovations are needed to make further progress.\n\nAnother line of work has argued for the integration of ideas from causality to make progress towards more robust and transferable machine learning systems (Pearl, 2019; Schölkopf, 2019; Goyal & Bengio, 2022). Causal representation learning has emerged recently as a field aiming to define and learn representations suited for causal reasoning (Schölkopf et al., 2021). This set of ideas is strongly related to learning disentangled representations (Bengio et al., 2013). Informally, a representation is considered disentangled when its components are in one-to-one correspondence with natural and interpretable factors of variations, such as object positions, colors or shape. Although a plethora of works have investigated theoretically under which conditions disentanglement is possible (Hyvärinen & Morioka, 2016; 2017; Hyvärinen et al., 2019; Khemakhem et al., 2020a; Locatello et al., 2020a; Klindt et al., 2021; Von Kügelgen et al., 2021; Gresele et al., 2021; Lachapelle et al., 2022; Lippe et al., 2022b; Ahuja et al., 2022c), fewer works have tackled how a disentangled representation could be beneficial for downstream tasks. Those who did mainly provide empirical rather than theoretical evidence for or against its usefulness (Locatello et al., 2019; van Steenkiste et al., 2019; Miladinovi ́c et al., 2019; Dittadi et al., 2021; Montero et al., 2021).\n\nIn this work, we explore synergies between disentanglement and sparse base-predictors in the context of multi-task learning. At the heart of our contributions is the assumption that only a small subset of all factors of variations are useful for each downstream task, and this subset might change from one task to another. We will refer to such tasks as sparse tasks, and their corresponding sets of useful factors as their supports. This assumption was initially suggested by Bengio et al. (2013, Section 3.5): “the feature set being trained may be destined to be used in multiple tasks that may have distinct [and unknown] subsets of relevant features. Considerations such as these lead us to\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe conclusion that the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical”. This strategy is very much in line with the current self-supervised learning trend (Radford et al., 2021), except for its focus on disentanglement.\n\nOur main contributions are the following: (i) We formalize this “sparse task assumption” and argue theoretically and empirically how, in this context, disentangled representations coupled with sparsity-regularized base-predictors can obtain better generalization than their entangled counterparts (Section 2.1). (ii) We introduce a novel identifiability result (Theorem 1) which shows how one can leverage multiple sparse tasks to learn a shared disentangled representation by regularizing the task-specific predictors to be maximally sparse (Section 2.2.1). Crucially, Assumption 7 formalizes how diverse the task supports have to be in order to guarantee disentanglement. (iii) Motivated by this result, we propose a tractable bi-level optimization (Problem (4)) to learn the shared representation while regularizing the task-specific base-predictors to be sparse (Section 2.2.2). We validate our theory by showing our approach can indeed disentangle latent factors on tasks constructed from the 3D Shapes dataset (Burgess & Kim, 2018). (iv) Finally, we draw a connection between this bi-level optimization problem and some formulations from the meta-learning literature (Section 2.3). Inspired by our identifiability result, we enhance an existing method (Lee et al., 2019), where the base-learners are now group-sparse SVMs. We show that this new meta-learning algorithm achieves competitive performance on the miniImageNet benchmark (Vinyals et al., 2016), while only using a fraction of the learned representation.\n\n2 SYNERGIES BETWEEN DISENTANGLEMENT AND SPARSITY\n\nIn this section, we formally introduce the notion of entangled and disentangled representations. First, we assume the existence of some ground-truth encoder function fθ : Rd → Rm that maps observations x ∈ X ⊆ Rd, e.g., images, to its corresponding interpretable and usually lower dimensional representation fθ(x) ∈ Rm, m ≤ d. The exact form of this ground-truth encoder depends on the task at hand, but also on what the machine learning practitioner considers as interpretable. The learned encoder function is denoted by f ˆθ : Rd → Rm, and should not be conflated with the groundtruth representation fθ. For example, f ˆθ can be parametrized by a neural network. Throughout, we are going to use the following definition of disentanglement. Definition 1 (Disentangled Representation, Khemakhem et al. 2020a; Lachapelle et al. 2022). A learned encoder function f ˆθ : Rd → Rm is said to be disentangled w.r.t. the ground-truth representation fθ when there exists an invertible diagonal matrix D and a permutation matrix P such that, for all x ∈ X , f ˆθ(x) = DP fθ(x). Otherwise the encoder f ˆθ is said to be entangled.\n\nIntuitively, a representation is disentangled when there is a one-to-one correspondence between its components and the components of the ground-truth representation, up to rescaling. Note that there exist less stringent notions of disentanglement which allow for component-wise nonlinear invertible transformations of the factors (Hyvärinen & Morioka, 2017; Hyvärinen et al., 2019).\n\nNotation. Capital bold letters denote matrices and lower case bold letters denote vectors. The set of integers from 1 to n is denoted by [n]. We write ∥·∥ for the Euclidean norm on vectors and the Frobenius norm on matrices. For a matrix A ∈ Rk×m, ∥A∥2,1 = (cid:80)m j=1 ∥A:j∥, and ∥A∥2,0 = (cid:80)m j=1 1∥A:j ∦=0, where 1 is the indicator function. The ground-truth parameter of the encoder function is θ, while that of the learned representation is ˆθ. We follow this convention for all the parameters throughout. Table 1 in Appendix A summarizes all the notation.\n\n2.1 DISENTANGLEMENT AND SPARSE BASE-PREDICTORS FOR IMPROVED GENERALIZATION\n\nIn this section, we compare the generalization performance of entangled and disentangled representations on sparse downstream tasks. We show that the maximum likelihood estimator (defined in Problem (1)) computed on linearly equivalent representations (entangled or disentangled) yield the same model (Proposition 1). However, disentangled representations have better generalization properties when combined with a sparse base-predictor (Proposition 2 and Figure 1).\n\nFirst, the learned representation f ˆθ is assumed to be linearly equivalent to the ground-truth representation fθ, i.e. there exists an invertible matrix L such that, for all x ∈ X , f ˆθ(x) = Lfθ(x).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nNote that despite being assumed linearly equivalent, the learned representation f ˆθ might not be disentangled (Definition 1); in that case, we say the representation is linearly entangled. When we refer to a disentangled representation, we write L := DP . Roeder et al. (2021) have shown that many common methods learn representations identifiable up to linear equivalence, such as deep neural networks for classification, contrastive learning (Oord et al., 2018; Radford et al., 2021) and autoregressive language models (Mikolov et al., 2010; Brown et al., 2020a).\n\nConsider the following maximum likelihood estimator (MLE):1\n\nˆW ( ˆθ)\n\nn\n\n:= arg max\n\n(cid:88)\n\n ̃W\n\n(x,y)∈D\n\nlog p(y; η = ̃W f ˆθ(x)) ,\n\n(1)\n\nwhere y denotes the label, D := {(x(i), y(i))}n i=1 is the dataset, p(y; η) is a distribution over labels parameterized by η ∈ Rk, and ˆW ∈ Rk×m is the task-specific predictor2. The following result shows that the maximum likelhood estimator defined in Problem (1) is invariant to invertible linear transformations of the features. Note that it is an almost direct consequence of the invariance of MLE to reparametrization (Casella & Berger, 2001, Thm. 7.2.10). See Appendix A for a proof. Proposition 1 (MLE Invariance to Invertible Linear Transformations of the Features). Let ˆW ( ˆθ) and ˆW (θ) be the solutions to Problem (1) with the representations f ˆθ and fθ, respectively (which we assume are unique). If there exists an invertible matrix L such that, ∀x ∈ X , f ˆθ(x) = Lfθ(x); then we have, ∀x ∈ X , ˆW ( ˆθ)\n\nn f ˆθ(x) = ˆW (θ)\n\nn fθ(x).\n\nn\n\nn\n\nProposition 1 shows that the model p(y; ˆW ( ˆθ) n f ˆθ(x)) learned by Problem (1) is independent of L, i.e., the model is the same for disentangled and linearly entangled representations. We thus expect both disentangled and linearly entangled representations to perform identically on downstream tasks.\n\nIn what follows, we assume the data is generated according to the following process.\n\nAssumption 1 (Data generation process). The input-label pairs are i.i.d. samples from the distribution p(x, y) := p(y | x)p(x) with p(y | x) := p(y; W fθ(x)), where W ∈ Rk×m is the ground-truth coefficient matrix.\n\nTo formalize the hypothesis that only a subset of the features fθ(x) are actually useful to predict the target y, we assume that the ground-truth coefficient matrix W is column sparse, i.e., ∥ ˆW ∥2,0 = l < m. Under this assumption, it is natural to constrain the MLE as such:\n\nˆW ( ˆθ,l)\n\nn\n\n:= arg max\n\n(cid:88)\n\n ̃W\n\n(x,y)∈D\n\nlog p(y; ̃W f ˆθ(x))\n\ns.t.\n\n∥ ̃W ∥2,0 ≤ l .\n\n(2)\n\nThe following proposition will help us understand how this additional constraint interacts with representations that are disentangled or linearly entangled. See Appendix A for a proof. Proposition 2 (Population MLE for Linearly Entangled Representations). Let ˆW ( ˆθ) ∞ be the solution of the population-based MLE, arg max ̃W Ep(x,y) log p(y; ̃W f ˆθ(x)) (assumed to be unique). Suppose f ˆθ is linearly equivalent to fθ, and Assumption 1 holds, then, ˆW ( ˆθ)\n\n∞ = W L−1.\n\nFrom Proposition 2, one can see that if the representation f ˆθ is disentangled, then ∥ ˆW ( ˆθ) ∞ ∥2,0 = ∥W (DP )−1∥2,0 = ∥W ∥2,0 = l. Thus, in that case, the sparsity constraint in Problem (2) does not exclude the population MLE estimator from its hypothesis class, and yields a decrease in the generalization gap (Bickel et al., 2009; Lounici et al., 2011a; Mohri et al., 2018) without biasing the estimator. Contrarily, when f ˆθ is linearly entangled, the population MLE might have more nonzero columns than the ground-truth, and thus would be excluded from the hypothesis space of Problem (2), which, in turn, would bias the estimator.\n\nEmpirical validation. We now present a simple simulated experiment to validate the above claim that disentangled representations coupled with sparsity regularization can have better generalization. Figure 1 compares the generalization performance of the convex relaxation of Problem (2)\n\n1We assume the solution is unique. 2p(y; η) could be a Gaussian density (regression) or a categorical distribution (classification).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Test performance for the entangled and disentangled representation using Lasso and Ridge regression. All the results are averaged over 10 seeds, with standard error shown in error bars.\n\n(Lasso regression, Tibshirani 1996) and Ridge regression (Hoerl & Kennard, 1970) on both disentangled and linearly entangled representations. Lasso regression coupled with the disentangled representation obtains better generalization than the other alternatives when l/m = 5% and when the number of samples is very small. We can also see that, disentanglement, sparsity regularization and sufficient sparsity in the ground-truth data generating process are necessary to see a significant improvement, in line with our discussion. Lastly, the performance of all methods converge to the same value when the number of samples grows. See Appendix D.1 for more details and discussion on the results.\n\n2.2 DISENTANGLEMENT VIA SPARSE MULTITASK LEARNING\n\nIn Section 2.1, we argued that disentangled representations can improve generalization when combined with sparse base-predictors, but we did not provide an approach to learn them. We first provide a new identification result (Theorem 1, Section 2.2.1), which states that in the context of sparse multitask learning, sparse base-predictors yield disentangled representations. Then, in Section 2.2.2, we provide a practical way to learn disentangled representations motivated by our identifiability result.\n\nThroughout this section, we assume the learner is given a set of T datasets {D1, . . . , DT } where each dataset Dt := {(x(t,i), y(t,i))}n i=1 consists of n couples of input x ∈ Rd and label y ∈ Y. The set of labels Y might contain either class indices or real values, depending on whether we are concerned with classification or regression tasks.\n\n2.2.1\n\nIDENTIFIABILITY ANALYSIS\n\nWe now present the main theoretical result of our work which shows how learning a shared representation across tasks while penalizing the task-specific base-predictor to be sparse can induce disentanglement. Our theory relies on the following ground-truth data generating process: Assumption 2 (Ground-truth data generating process). For each task t, the dataset Dt is made of i.i.d. samples from the distribution p(x, y | W (t)) := p(y | x, W (t))p(x | W (t)) with p(y | x, W (t)) := p(y; W (t)fθ(x)), where W (t) ∈ Rk×m is the task-specific ground-truth coefficient matrix. Moreover, the matrices W (t) are i.i.d. samples from some probability measure PW with support W. Also, for all W ∈ W, the support of p(x | W ) is X ⊆ Rd (fixed across tasks).\n\nThe above assumption states that (i) the ground-truth coefficient matrices W (t) are task-specific while the representation fθ is shared across all the tasks, (ii) the task-specific W (t) are sampled i.i.d. from some distribution PW , and (iii) the support of x is shared across tasks. Assumption 3 (Identifiability of η). The parameter η is identifiable from p(y; η), i.e. ∀y; p(y; η) = p(y; ̃η) =⇒ η = ̃η.\n\nThis property holds, e.g., when p(y; η) is a Gaussian in the usual μ, σ2 parameterization. Generally, it also holds for minimal parameterizations of exponential families (Wainwright & Jordan, 2008).\n\nThe following assumption requires the ground-truth representation fθ(x) to vary enough such that its image cannot be trapped inside a proper subspace.\n\n4\n\n50100150#samples0.00.51.0R2‘/m=5%50100150#samples‘/m=20%50100150#samples‘/m=80%Entangled-LassoDisentangled-LassoEntangled-RidgeDisentangled-RidgeUnder review as a conference paper at ICLR 2023\n\nFigure 2: Illustration of Assumption 6 showing three examples of distribution PW |S. The red distribution satisfies the assumption, but the blue and orange distributions do not. The red lines are level sets of a Gaussian distribution with full rank covariance. The blue line represents the support of a Gaussian distribution with a low rank covariance. The orange dots represents a distribution with finite support. The green vector a shows that the condition is violated for both the blue and the orange distribution, since, in both cases, W1,Sa = 0 (orthogonal) with probability greater than zero.\n\nFigure 3: The leftmost figure represents S, the support of some p(S). The other figures form a verification that Assumption 7 holds for S.\n\nAssumption 4 (Sufficient representation variability). There exists x(1), . . . , x(m) ∈ X such that the matrix F := [fθ(x(1)), . . . , fθ(x(m))] is invertible.\n\nThe following assumption requires that the support of the distribution PW is sufficiently rich. Assumption 5 (Sufficient task variability). There exists W (1), . . . , W (m) ∈ W and row indices i1, . . . , im ∈ [k] such that the rows W (1)\n\nim,: are linearly independent.\n\ni1,:, . . . , W (m)\n\nUnder Assumptions 2 to 5 the representation fθ is identifiable up to linear equivalence (see Theorem 2 in Appendix B). Similar results where shown by Roeder et al. (2021); Ahuja et al. (2022c). The next assumptions will guarantee disentanglement.\n\nIn order to formalize the intuitive idea that most tasks do not require all features, we will denote by S(t) the support of the matrix W (t), i.e. S(t) := {j ∈ [m] | W (t) ̸= 0}. In other words, S(t) is the set of features which are useful to predict y in the t-th task; note that it is unknown to the learner. For our analysis, we decompose PW as PW = (cid:80) S∈P([m]) p(S)PW |S , where P([m]) is the collection of all subsets of [m], p(S) is the probability that the support of W is S and PW |S is the conditional distribution of W given that its support is S. Let S be the support of the distribution p(S), i.e. S := {S ∈ P([m]) | p(S) > 0}. The set S will have an important role in Assumption 7 & Theorem 1.\n\n:j\n\nThe following assumption requires that PW |S does not concentrate on certain proper subspaces. Assumption 6 (Intra-support sufficient task variability). For all S ∈ S and all a ∈ R|S|\\0, PW |S{W ∈ Rk×m | W:Sa = 0} = 0.\n\nWe illustrate the above assumption in the simpler case where k = 1. For instance, Assumption 6 holds when the distribution of W1,S | S has a density w.r.t. the Lebesgue measure on R|S|, which is true for example when W1,S | S ∼ N (0, Σ) and the covariance matrix Σ is full rank (red distribution in Figure 2). However, if Σ is not full rank, the probability distribution of W1,S | S concentrates its mass on a proper linear subspace V ⊊ R|S|, which violates Assumption 6 (blue distribution in Figure 2). Another important counter-example is when PW |S concentrates some of its mass on a point W (0), i.e. PW |S{W (0)} > 0 (orange distribution in Figure 2). Interestingly, there are distributions over W1,S | S that do not have a density w.r.t. the Lebesgue measure, but still satisfy Assumption 6. This is the case, e.g., when W1,S | S puts uniform mass over a (|S| − 1)- dimensional sphere embedded in R|S| and centered at zero. See Appendix B.2 for a justification.\n\nThe following assumption requires that the support S of p(S) is “rich enough”. Assumption 7 (Sufficient support variability). For all j ∈ [m], (cid:83)\n\nS∈S|j̸∈S S = [m] \\ {j}.\n\nIntuitively, Assumption 7 requires that, for every feature j, one can find a set of tasks such that their supports cover all features except j itself. Figure 3 shows an example of S satisfying Assumption 7. Removing the latter would only yield partial disentanglement (Lachapelle & Lacoste-Julien, 2022).\n\nWe are now ready to show the main theoretical result of this work, which provides a bi-level optimization problem for which the optimal representations are guaranteed to be disentangled. It as-\n\n5\n\n12341234123412341234Under review as a conference paper at ICLR 2023\n\nsumes infinitely many tasks are observed, with task-specific ground-truth matrices W sampled from PW . We denote by ˆW (W ) the task-specific estimator of W . See Appendix B.1 for a proof. Note that we suggest a tractable relaxation in Section 2.2.2. Theorem 1 (Sparse multi-task learning for disentanglement). Let ˆθ be a minimizer of\n\nmin ˆθ\n\ns.t.\n\nEPW Ep(x,y|W ) − log p(y; ˆW (W )f ˆθ(x))\n\n∀ W ∈ W, ˆW (W ) ∈\n\narg min ̃W s.t. || ̃W ||2,0≤||W ||2,0\n\nEp(x,y|W ) − log p(y; ̃W f ˆθ(x)) .\n\n(3)\n\nThen, under Assumptions 2 to 7, f ˆθ is disentangled w.r.t. fθ (Definition 1).\n\nIntuitively, this optimization problem effectively selects a representation f ˆθ that (i) allows a perfect fit of the data distribution, and (ii) allows the task-specific estimators ˆW (W ) to be as sparse as the ground-truth W . With the same disentanglement guarantees, Theorem 4 in Appendix B presents a variation of Problem (3) which enforces the weaker constraint EPW ∥ ˆW (W )∥2,0 ≤ EPW ∥W ∥2,0, instead of ∥ ˆW (W )∥2,0 ≤ ∥W ∥2,0 for each task W individually.\n\n2.2.2 TRACTABLE BILEVEL OPTIMIZATION PROBLEMS FOR SPARSE MULTITASK LEARNING\n\nProblem (3) was shown to yield a disentangled representation (Theorem 1), but is intractable due to the L2,0-seminorm. Thus we use the L2,1 convex relaxation of the L2,0-seminorm, which is also known to promote group sparsity (Obozinski et al., 2006; Argyriou et al., 2008; Lounici et al., 2009):\n\nmin ˆθ\n\n−\n\n1 T n\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\n(x,y)∈Dt\n\nlog p(y; ˆW (t)f ˆθ(x))\n\ns.t.\n\n∀ t ∈ [T ], ˆW (t) ∈ arg min\n\n−\n\n ̃W\n\n1 n\n\n(cid:88)\n\n(x,y)∈Dt\n\nlog p(y; ̃W f ˆθ(x)) + λt|| ̃W ||2,1 .\n\n(4)\n\nFollowing Bengio (2000); Pedregosa (2016), one can compute the (hyper)gradient of the outer function using implicit differentiation, even if the inner optimization problem is non-smooth (Bertrand et al., 2020; Bolte et al., 2021; Malézieux et al., 2022; Bolte et al., 2022). Once the hypergradient is computed, one can optimize Problem (4) using usual first-order methods (Wright & Nocedal, 1999). Note that the quantity ˆW (t)f ˆθ(x) is invariant to simultaneous rescaling of ˆW (t) by a scalar and of f ˆθ(x) by its inverse. Thus, without constraints on f ˆθ(x), ∥ ˆW (t)∥2,1 can be made arbitrarily small. This is a usual problem in sparse dictionary learning (Kreutz-Delgado et al., 2003; Mairal et al., 2008; 2009; 2011), where unit-norm constraints are usually imposed on the column of the dictionary. Here, since f ˆθ is parametrized by a neural network, we suggest to apply batch or layer normalization (Ioffe & Szegedy, 2015; Ba et al., 2016) to control its norm. Since the number of relevant features might be task-dependent, Problem (4) has one regularization hyperparameter λt per task. To limit the number of hyperparameters in practice, we select λt := λ for all t ∈ [T ].\n\n2.3 LINK WITH META-LEARNING\n\nt\n\nIn the setting known as meta-learning (Finn et al., 2017), for a large number of tasks T , we are given training datasets Dtrain , which usually contains a small number of samples n. Unlike in the multi-task setting though (i.e., unlike in Section 2.2), we are also given separated test datasets Dtest to evaluate how well the learned model generalizes to new test samples. In meta-learning, t\nthe goal is to learn a training procedure which will generalize well on out-of-distribution tasks. The bi-level formulation Problem (4) is closely related to metric-based meta-learning (Snell et al., 2017; Bertinetto et al., 2019), where a shared representation f ˆθ is learned across all tasks. The representation is jointly learned with simple task-specific classifiers, which are usually optimizationbased classifiers, such as support-vector machines. Formally, metric-based meta-learning can be formulated as follows\n\nmin ˆθ\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\n(x,y)∈Dtest\n\nt\n\nLout\n\n(cid:0) ˆW (t) ˆθ\n\n; f ˆθ(x), y(cid:1)\n\ns.t. ˆW (t)\n\nˆθ\n\n∈ arg min\n\n ̃W\n\n(cid:88)\n\nLin\n\n(cid:0) ̃W ; f ˆθ(x), y(cid:1).\n\n(x,y)∈Dtrain\n\nt\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nInspired by Lee et al. (2019), where the base-classifiers were multiclass support-vector machines (SVMs, Crammer & Singer 2001), we propose to use group Lasso penalized multiclass SVMs, in order to introduce sparsity in the base-learners, with Y ∈ Rn×k the one-hot encoding of y ∈ Rn: (cid:0)(Wyi: − Wl:) · f ˆθ(x) − Y:l\n\nLin(W ; f ˆθ(x)), y) := max\n\nn ∥W ∥2,1 + λ2\n\n2n ∥W ∥2.\n\n(cid:1) + λ1\n\n(5)\n\nl∈[k]\n\nIn few-shot learning settings, the number of features m is usually much larger than the number of samples n (in Lee et al. 2019, m = 1.6·104 and n ≤ 25). In such scenarios, SVMs-like problems are usually solved through their dual (Boyd et al., 2004, Chap. 5) problems, for computational (Hsieh et al., 2008) and theoretical (Shalev-Shwartz & Zhang, 2012) benefits.\n\nProposition 3. (Dual Group Lasso Soft-Margin Multiclass SVM.) The dual of the inner problem with Lin as defined in (5) writes\n\nmin Λ∈Rn×k\n\n1 λ2\n\nm (cid:88)\n\nj=1\n\n∥BST (cid:0)(Y − Λ)⊤F:j, λ1\n\n(cid:1)∥2 + ⟨Y , Λ⟩ +\n\nn (cid:88)\n\ni=1\n\n1 k (cid:80)\n\nl=1\n\nΛil=1\n\nn (cid:88)\n\nk (cid:88)\n\n+\n\ni=1\n\nl=1\n\n1Λil≥0 ,\n\n(6)\n\nwith BST the block soft-thresholding operator: BST : (a, τ ) (cid:55)→ (1 − τ /∥a∥)+ a, F ∈ Rn×m the concatenation of {f ˆθ(x)}(x,y)∈Dtrain. In addition, the primal-dual link writes, for all j ∈ [m], W:j = BST (cid:0)(Y − Λ)⊤F:j, λ1\n\n(cid:1) /λ2.\n\nProof of Proposition 3 can be found in Appendix C.1. The objective of Problem (6) is composed of a smooth term and block separable non-smooth term, hence it can be solved efficiently using proximal block coordinate descent (Tseng, 2001). As stated in Section 2.2, argmin differentiation of the solution of Problem (6) can be done using implicit differentiation (Bertrand et al., 2022). Although Theorem 1 is not directly applicable to the meta-learning formulation proposed in this section, we conjecture that similar techniques could be reused to prove an identifiability result in this setting.\n\n3 RELATED WORK\n\nDisentanglement. Since the work of Bengio et al. (2013), many methods have been proposed to learn disentangled representations based on various heuristics (Higgins et al., 2017; Chen et al., 2018; Kim & Mnih, 2018; Kumar et al., 2018; Bouchacourt et al., 2018). Following the work of Locatello et al. (2019), which highlighted the lack of identifiability in modern deep generative models, many works have proposed more or less weak forms of supervision motivated by identifiability analyses (Locatello et al., 2020a; Klindt et al., 2021; Von Kügelgen et al., 2021; Ahuja et al., 2022a;c; Zheng et al., 2022). A similar line of work have adopted the causal representation learning perspective (Lachapelle et al., 2022; Lachapelle & Lacoste-Julien, 2022; Lippe et al., 2022b;a; Ahuja et al., 2022b; Yao et al., 2022; Brehmer et al., 2022). The problem of identifiability was well known among the independent component analysis (ICA) community (Hyvärinen et al., 2001; Hyvärinen & Pajunen, 1999) which came up with solutions for general nonlinear mixing functions by leveraging auxiliary information (Hyvärinen & Morioka, 2016; 2017; Hyvärinen et al., 2019; Khemakhem et al., 2020a;b). Another approach is to consider restricted hypothesis classes of mixing functions (Taleb & Jutten, 1999; Gresele et al., 2021). Contrarily to most of the above works, we do not assume that the inputs x are generated by transforming a latent random variable z through a bijective decoder g. Instead, we assume the existence of a not necessarily bijective ground-truth feature extractor fθ(x) from which the labels can be predicted using only a subset of its components in every tasks (Assumption 2). Many of these works make assumptions about the distribution of latent factors, e.g., (conditional) independence, exponential family or other parametric assumptions. In contrast, we make comparatively weaker assumptions on the support of the ground-truth features (Assumption 4), which are allowed to present dependencies (Section 4). Locatello et al. (2020b) proposed a semi-supervised learning approach to disentangle in cases where a few samples are labelled with the factors of variations themselves. This is different from our approach as the labels that we consider can be sampled from some p(y; W f ˆθ(x)), which is more general. Ahuja et al. (2022c) consider a setting similar to ours, but they rely on the independence and non-gaussianity of the latent factors for disentanglement using linear ICA.\n\nMulti-task, transfer & invariant learning. The statistical advantages of multi-task representation learning is well understood (Lounici et al., 2011a;b; Maurer et al., 2016). However, apart from Zhang\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Disentanglement performance (MCC) for inner-Lasso, inner-Ridge and inner-Ridge combined with ICA as a function of the regularization parameter (left and middle). Varying level of correlation between latents (top) and of noise on the latents (bottom). The right columns shows performance of the best hyperparameter for different values of correlation and noise.\n\net al. (2022), theoretical benefits of disentanglement for transfer learning are not clearly established. Some works have investigated this question empirically and obtained both positive (van Steenkiste et al., 2019; Miladinovi ́c et al., 2019; Dittadi et al., 2021) and negative results (Locatello et al., 2019; Montero et al., 2021). Invariant risk minimization (Arjovsky et al., 2020; Ahuja et al., 2020; Krueger et al., 2021; Lu et al., 2021) aims at learning a representation that elicits a task-invariant base-predictor. This differs from our approach which learns base-predictors that are task-specific .\n\nDictionary learning and sparse coding. We contrast our approach, which jointly learns a dense representation and sparse base-predictors (Problem (4)), with the line of work which consists in learning sparse representations (Chen et al., 1998; Gribonval & Lesage, 2006). For instance, sparse dictionary learning (Mairal et al., 2009; 2011; Maurer et al., 2013) is an unsupervised technique which aims at learning sparse representations that refer to atoms of a learned dictionary. Contrarily to our method which computes the representation of a single input x by evaluating a function approximator f ˆθ, in sparse dictionary learning, the representation of a single input is computed by minimizing a reconstruction loss. In the case of supervised dictionary learning (Mairal et al., 2008), an additional (potentially expressive) classifier is learned. This large literature has lead to a wide variety of estimators: for instance, Mairal et al. (2008, Eq. 4), which minimizes the sum of the classification error and the approximation error of the code, or Mairal et al. (2011); Malézieux et al. (2022), which introduce bi-level formulations which shares similarities with our formulations.\n\n4 EXPERIMENTS\n\nSemi-real experiments on 3D Shapes. We now illustrate Theorem 1 by applying Problem (4) to tasks generated using the 3D Shapes dataset (Burgess & Kim, 2018).\n\nData generation. For all tasks t, the labelled dataset Dt = {(x(t,i)), y(t,i))}n i=1 is generated by first sampling the ground-truth latent variables z(t,i) := fθ(x(t,i)) i.i.d. according to some distribution p(z), while the corresponding input is obtained doing x(t,i) := f −1 θ (z(t,i)) (fθ is invertible in 3D Shapes). Then, a sparse weight vector w(t) is sampled randomly to compute the labels of each example as y(t,i) := w(t) · x(t,i) + ε(t,i), where ε(t,i) is independent Gaussian noise. Figure 4 explores various choice of p(z), i.e. by varying the level of correlation between the latent variables and by varying the level of noise on the ground-truth latents. See Appendix D.2 for more details about the data generating process.\n\nAlgorithms. In this setting where p(y; η) is a Gaussian with fixed variance, the inner problem of Problem (4) amounts to Lasso regression, we thus refer to this approach as inner-Lasso. We also evaluate a simple variation of Problem (4) in which the L1 norm is replaced by an L2 norm, and refer to it as inner-Ridge. In addition we evaluate the representation obtained by performing linear\n\n8\n\n0.00.010.030.10.31.0λ/λmax0.51.0MCCCorrelation=0.00.00.010.030.10.31.0λ/λmaxCorrelation=0.90.60.81.0CorrelationMultiplecorrelationvalues0.00.010.030.10.31.0λ/λmax0.51.0MCCNoisescale=00.00.010.030.10.31.0λ/λmaxNoisescale=100.250.50.751NoisescaleMultiplenoisevaluesUnder review as a conference paper at ICLR 2023\n\nFigure 5: Effect of sparsity on the percentage of tasks using specific features, with our meta-learning objective, on miniImageNet (left). The accuracy of the meta-learning algorithm and the average level of sparsity in the base-learners, as λ varies (right).\n\nICA (Comon, 1992) on the representation learned by inner-Ridge: the case λ = 0 corresponds to the approach of Ahuja et al. (2022c).\n\nDiscussion. Figure 4 reports disentanglement performance of the three methods, as measured by the mean correlation coefficient, or MCC (Hyvärinen & Morioka, 2016; Khemakhem et al., 2020a) (Appendix D.2). In all settings, inner-Lasso obtains high MCC for some values of λ, being on par or surpassing the baselines. As the theory suggests, it is robust to high levels of correlations between the latents, as opposed to inner-Ridge with ICA which is very much affected by strong correlations (since ICA assumes independence). We can also see how additional noise on the latent variables hurts inner-Ridge with ICA while leaving inner-Lasso unaffected. Figure 6 in Appendix D.2 shows that all methods find a representation which is linearly equivalent to the ground-truth representation, except for very large values of λ. Refer to Appendix D.2 for more details. Appendix D.2.4 presents experiments showing to what extent inner-Lasso is robust to violations of Assumption 7. Appendix D.2.5 presents a visual evaluation of disentanglement. Appendix D.2.6 shows results for the same experiments with the DCI metric (Eastwood & Williams, 2018).\n\nFew-shot learning experiments. Despite the lack of ground-truth latent factors in standard few-shot learning benchmarks, we also evaluate our meta-learning objective introduced in Section 2.3, using the dual formulation of the group Lasso penalized SVM as our base-learner, on the miniImageNet dataset (Vinyals et al., 2016). The objective of this experiment is to show that the sparse formulation of the meta-learning objective is capable of reaching similar levels of performance, while using a fraction of the features. Details about the experimental settings are provided in Appendix D.3.\n\nDiscussion. In Figure 5 (left), we report how frequently the learned features are used by the baselearner on meta-training tasks; the gradual decrease in usage suggests that the features are reused in different contexts, across different tasks. We also observe (Figure 5, right) that adding sparsity to the base learner may also improve performance on meta-training tasks, while only using a fraction of all the features available in the learned representation, supporting our observations in Section 2.1 on the effect of sparsity on generalization on natural images (see Appendix D.3 for further discussion about how this still tests generalization). We also observe that some level of sparsity improves the performance on novel meta-test tasks, albeit to a smaller extent.\n\n5 CONCLUSION\n\nIn this work, we investigated the synergies between sparsity, disentanglement and generalization. We showed that when the downstream task can be solved using only a fraction of the factors of variations, disentangled representations combined with sparse base-predictors can improve generalization (Section 2.1). Our novel identifiability result (Theorem 1) sheds light on how, in a multi-task setting, sparsity regularization on the task-specific predictors can induce disentanglement. This led to a practical bi-level optimization problem that was shown to yield disentangled representations on regression tasks based on the 3D Shapes dataset. Finally, we explored a meta-learning formulation extending this approach, and showed how sparse base-learners can help with generalization, while only using a small fraction of the features.\n\n9\n\n0250050007500100001250015000Features0.00.20.40.60.81.0Usage percentage0.00.20.40.60.81.0Average sparsity level0.60.70.80.9AccuracyMeta-trainMeta-test/max00.0050.010.020.030.050.070.1Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nK. Ahuja, K. Shanmugam, K. R. Varshney, and A. Dhurandhar. Invariant risk minimization games.\n\nIn Proceedings of the 37th International Conference on Machine Learning, 2020.\n\nK. Ahuja, J. Hartford, and Y. Bengio. Properties from mechanisms: an equivariance perspective on identifiable representation learning. In International Conference on Learning Representations, 2022a.\n\nK. Ahuja, J. Hartford, and Y. Bengio. Weakly supervised representation learning with sparse pertur-\n\nbations, 2022b.\n\nK. Ahuja, D. Mahajan, V. Syrgkanis, and I. Mitliagkas. Towards efficient representation identifica-\n\ntion in supervised learning. In First Conference on Causal Learning and Reasoning, 2022c.\n\nA. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine learning, 73\n\n(3):243–272, 2008.\n\nM. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization, 2020.\n\nJ. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n\n2016.\n\nY. Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889–1900,\n\n2000.\n\nY. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.\n\nIEEE transactions on pattern analysis and machine intelligence, 2013.\n\nL. Bertinetto, J. F. Henriques, P. HS Torr, and Andrea Vedaldi. Meta-learning with differentiable\n\nclosed-form solvers. 2019.\n\nQ. Bertrand, Q. Klopfenstein, M. Blondel, S. Vaiter, A. Gramfort, and J. Salmon. Implicit differentiation of lasso-type models for hyperparameter optimization. In International Conference on Machine Learning, pp. 810–821. PMLR, 2020.\n\nQ. Bertrand, Q. Klopfenstein, M. Massias, M. Blondel, S. Vaiter, A. Gramfort, and J. Salmon. Implicit differentiation for fast hyperparameter selection in non-smooth convex learning. JMLR, 2022.\n\nP. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of lasso and Dantzig selector. The\n\nAnnals of statistics, 37(4):1705–1732, 2009.\n\nJ. Bolte, T. Le, E., Pauwels, and T. Silveti-Falls. Nonsmooth implicit differentiation for machinelearning and optimization. Advances in neural information processing systems, 34:13537–13549, 2021.\n\nJ. Bolte, E. Pauwels, and S. Vaiter. Automatic differentiation of nonsmooth iterative algorithms.\n\nNeurIPS, 2022.\n\nD. Bouchacourt, R. Tomioka, and S. Nowozin. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. Proceedings of the AAAI Conference on Artificial Intelligence, 2018.\n\nS. P. Boyd, , and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\nJ. Brehmer, P. De Haan, P. Lippe, and T. Cohen. Weakly supervised causal representation learning.\n\nIn Advances in Neural Information Processing Systems, 2022.\n\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are fewshot learners. In Advances in Neural Information Processing Systems, 2020a.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020b.\n\nChris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,\n\n2018.\n\nG. Casella and R. Berger. Statistical Inference. Duxbury Resource Center, 2001.\n\nR. T. Q. Chen, X. Li, R. G., and D. Duvenaud. Isolating sources of disentanglement in vaes. In\n\nAdvances in Neural Information Processing Systems, 2018.\n\nS. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM\n\nJournal on Scientific Computing, 1998.\n\nT. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning In International Conference on Machine Learning, pp. 1597–1607.\n\nof visual representations. PMLR, 2020.\n\nP. Comon. Independent component analysis. Higher-Order Statistics, 1992.\n\nK. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector\n\nmachines. Journal of machine learning research, 2(Dec):265–292, 2001.\n\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional trans-\n\nformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nA. Dittadi, F. Träuble, F. Locatello, M. Wuthrich, V. Agrawal, O. Winther, S. Bauer, and B. Schölkopf. On the transfer of disentangled representations in realistic settings. In International Conference on Learning Representations, 2021.\n\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2021.\n\nCian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disen-\n\ntangled representations. In International Conference on Learning Representations, 2018.\n\nC. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep net-\n\nworks. In International conference on machine learning, pp. 1126–1135. PMLR, 2017.\n\nA. Goyal and Y. Bengio. Inductive biases for deep learning of higher-level cognition. Proc. R. Soc.\n\nA 478: 20210068, 2022.\n\nL. Gresele, J. Von Kügelgen, V. Stimper, B. Schölkopf, and M. Besserve. Independent mechanism\n\nanalysis, a new concept? In Advances in Neural Information Processing Systems, 2021.\n\nR. Gribonval and S. Lesage. A survey of sparse component analysis for blind source separation: principles, perspectives, and new challenges. In ESANN’06 proceedings - 14th European Symposium on Artificial Neural Networks, 2006.\n\nI. Higgins, L. Matthey, A. Pal, C. P. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. In ICLR,\n\nbeta-vae: Learning basic visual concepts with a constrained variational framework. 2017.\n\nA. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems.\n\nTechnometrics, 12(1):55–67, 1970.\n\nT. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149–5169, 2021.\n\nC.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear svm. In Proceedings of the 25th international conference on Machine learning, pp. 408–415, 2008.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA. Hyvärinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning and\n\nnonlinear ica. In Advances in Neural Information Processing Systems, 2016.\n\nA. Hyvärinen and H. Morioka. Nonlinear ICA of Temporally Dependent Stationary Sources. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017.\n\nA. Hyvärinen and P. Pajunen. Nonlinear independent component analysis: Existence and uniqueness\n\nresults. Neural Networks, 1999.\n\nA. Hyvärinen, J. Karhunen, and E. Oja. Independent Component Analysis. Wiley, 2001.\n\nA. Hyvärinen, H. Sasaki, and R. E. Turner. Nonlinear ica using auxiliary variables and generalized\n\ncontrastive learning. In AISTATS. PMLR, 2019.\n\nS. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, 2015.\n\nI. Khemakhem, D. Kingma, R. Monti, and A. Hyvärinen. Variational autoencoders and nonlinear In Proceedings of the Twenty Third International Conference on\n\nica: A unifying framework. Artificial Intelligence and Statistics, 2020a.\n\nI. Khemakhem, R. Monti, D. Kingma, and A. Hyvärinen. Ice-beem: Identifiable conditional energybased deep models based on nonlinear ica. In Advances in Neural Information Processing Systems, 2020b.\n\nH. Kim and A. Mnih. Disentangling by factorising. In Proceedings of the 35th International Con-\n\nference on Machine Learning, 2018.\n\nD. A. Klindt, L. Schott, Y Sharma, I Ustyuzhaninov, W. Brendel, M. Bethge, and D. M. Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. In 9th International Conference on Learning Representations, 2021.\n\nK. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T.-W. Lee, and T. J. Sejnowski. Dictionary\n\nlearning algorithms for sparse representation. Neural computation, 15(2):349–396, 2003.\n\nD. Krueger, E. Caballero, J.-H. Jacobsen, A. Zhang, J. Binas, R. Le Priol, D. Zhang, and\n\nA. Courville. Out-of-distribution generalization via risk extrapolation ({re}x), 2021.\n\nA. Kumar, P. Sattigeri, and A. Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. In International Conference on Learning Representations, 2018.\n\nS. Lachapelle and S. Lacoste-Julien. Partial disentanglement via mechanism sparsity. In UAI 2022\n\nWorkshop on Causal Representation Learning, 2022.\n\nS. Lachapelle, P. Rodriguez Lopez, Y. Sharma, K. E. Everett, R. Le Priol, A. Lacoste, and S. LacosteJulien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In First Conference on Causal Learning and Reasoning, 2022.\n\nK. Lee, S.Maji, A. Ravichandran, and S. Soatto. Meta-learning with differentiable convex optimization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10657–10665, 2019.\n\nP. Lippe, S. Magliacane, S. Löwe, Y. M Asano, T. Cohen, and E. Gavves. iCITRIS: Causal representation learning for instantaneous temporal effects. In UAI 2022 Workshop on Causal Representation Learning, 2022a.\n\nP. Lippe, S. Magliacane, S. Löwe, Y. M. Asano, T. Cohen, and E. Gavves. CITRIS: Causal identifi-\n\nability from temporal intervened sequences, 2022b.\n\nF. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Schölkopf, and O. Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In Proceedings of the 36th International Conference on Machine Learning, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nF. Locatello, B. Poole, G. Raetsch, B. Schölkopf, O. Bachem, and M. Tschannen. Weakly-supervised disentanglement without compromises. In Proceedings of the 37th International Conference on Machine Learning, 2020a.\n\nF. Locatello, M. Tschannen, S. Bauer, G. Rätsch, B. Schölkopf, and O. Bachem. Disentangling factors of variations using few labels. In International Conference on Learning Representations, 2020b. URL https://openreview.net/forum?id=SygagpEKwB.\n\nK. Lounici, M. Pontil, A. B. Tsybakov, and S. Van De Geer. Taking advantage of sparsity in multi-\n\ntask learning. arXiv preprint arXiv:0903.1468, 2009.\n\nK. Lounici, M. Pontil, and A. B. Tsybakov. Oracle inequalities and optimal inference under group\n\nsparsity. The Annals of statistics, 2011a.\n\nK. Lounici, M. Pontil, S. Van De Geer, and A. B Tsybakov. Oracle inequalities and optimal inference\n\nunder group sparsity. The annals of statistics, 2011b.\n\nChaochao Lu, Yuhuai Wu, Jo ́se Miguel Hernández-Lobato, and Bernhard Schölkopf. Nonlinear\n\ninvariant risk minimization: A causal approach, 2021.\n\nJ. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. Bach. Supervised dictionary learning. Advances\n\nin neural information processing systems, 21, 2008.\n\nJ. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning, pp. 689–696, 2009.\n\nJ. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE transactions on pattern\n\nanalysis and machine intelligence, 34(4):791–804, 2011.\n\nB. Malézieux, T. Moreau, and M. Kowalski. Dictionary and prior learning with unrolled algorithms\n\nfor unsupervised inverse problems. ICLR, 2022.\n\nG. Marcus, E. Davis, and S. Aaronson. A very preliminary analysis of dall-e 2. arXiv preprint\n\narXiv:2204.13807, 2022.\n\nA. Maurer, M. Pontil, and B. Romera-Paredes. Sparse coding for multitask and transfer learning.\n\nICML’13, 2013.\n\nA. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning. J.\n\nMach. Learn. Res., 2016.\n\nT. Mikolov, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. Recurrent neural network based\n\nlanguage model. ISCA, 2010.\n\nÐ. Miladinovi ́c, M. W. Gondal, B. Schölkopf, J. M. Buhmann, and S. Bauer. Disentangled state\n\nspace representations. arXiv preprint arXiv:1906.03255, 2019.\n\nM. Mohri, A. Rostamizadeh, and A. Talwalkar. MIT Press, 2018.\n\nM. L. Montero, C. JH Ludwig, R. P. Costa, G. Malhotra, and J. Bowers. The role of disentanglement\n\nin generalisation. In International Conference on Learning Representations, 2021.\n\nG. Obozinski, B. Taskar, and M. Jordan. Multi-task feature selection. Statistics Department, UC\n\nBerkeley, Tech. Rep, 2(2.2):2, 2006.\n\nA. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. Advances\n\nin Neural Information Processing Systems, 2018.\n\nJ. Pearl. The seven tools of causal inference, with reflections on machine learning. Commun. ACM,\n\n2019.\n\nF. Pedregosa. Hyperparameter optimization with approximate gradient. In International conference\n\non machine learning, pp. 737–746. PMLR, 2016.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image\n\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nP. Richtárik and M. Takáˇc. Iteration complexity of randomized block-coordinate descent methods\n\nfor minimizing a composite function. Mathematical Programming, 144(1):1–38, 2014.\n\nG. Roeder, L. Metz, and D. P. Kingma. On linear identifiability of learned representations.\n\nIn\n\nProceedings of the 38th International Conference on Machine Learning, 2021.\n\nB. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. Toward causal representation learning. Proceedings of the IEEE - Advances in Machine Learning and Deep Neural Networks, 2021.\n\nB. Schölkopf. Causality for machine learning, 2019.\n\nS. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss\n\nminimization. The Journal of Machine Learning Research, 2012.\n\nJ. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. Advances in Neural\n\nInformation Processing Systems, 30, 2017.\n\nA. Taleb and C. Jutten. Source separation in post-nonlinear mixtures. IEEE Transactions on Signal\n\nProcessing, 1999.\n\nR. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\n\nSociety: Series B (Methodological), 58(1):267–288, 1996.\n\nP. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization.\n\nJournal of optimization theory and applications, 109(3):475–494, 2001.\n\nS. van Steenkiste, F. Locatello, J. Schmidhuber, and O. Bachem. Are disentangled representations helpful for abstract visual reasoning? In Advances in Neural Information Processing Systems, 2019.\n\nO. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning.\n\nAdvances in neural information processing systems, 29, 2016.\n\nJ. Von Kügelgen, Y. Sharma, L. Gresele, W. Brendel, B. Schölkopf, M. Besserve, and F. Locatello. Self-supervised learning with data augmentations provably isolates content from style. In ThirtyFifth Conference on Neural Information Processing Systems, 2021.\n\nM. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational infer-\n\nence. Found. Trends Mach. Learn., 2008.\n\nM. Wortsman, G. Ilharco, J. W. Kim, M. Li, S. Kornblith, R. Roelofs, R. G. Lopes, H. Hajishirzi, In ProA. Farhadi, H. Namkoong, and L. Schmidt. Robust fine-tuning of zero-shot models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7959–7971, June 2022.\n\nS. Wright and J. Nocedal. Numerical optimization. Springer Science, 35(67-68):7, 1999.\n\nW. Yao, Y. Sun, A. Ho, C. Sun, and K. Zhang. Learning temporally causal latent processes from\n\ngeneral temporal data. In International Conference on Learning Representations, 2022.\n\nH. Zhang, Y.-F. Zhang, W. Liu, A. Weller, B. Schölkopf, and E. Xing. Towards principled disentanglement for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n\nY. Zheng, I. Ng, and K. Zhang. On the identifiability of nonlinear ICA with unconditional priors. In ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality, 2022.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nCONTENTS\n\n1 Introduction\n\n2 Synergies between Disentanglement and Sparsity\n\n2.1 Disentanglement and sparse base-predictors for improved generalization . . . . . .\n\n2.2 Disentanglement via sparse multitask learning . . . . . . . . . . . . . . . . . . . .\n\n2.2.1\n\nIdentifiability analysis\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2.2.2 Tractable bilevel optimization problems for sparse multitask learning . . .\n\n2.3 Link with meta-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n3 Related Work\n\n4 Experiments\n\n5 Conclusion\n\nA Proofs of Section 2.1\n\nB Identifiability Theory\n\nB.1 Proof of Theorem 1 .\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nB.2 A distribution without density satisfying Assumption 6 . . . . . . . . . . . . . . .\n\nC Optimization details\n\nC.1 Group Lasso SVM Dual\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD Experimental details\n\nD.1 Disentangled representation coupled with sparsity regularization improves general- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nization .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nD.2 Semi-Real Experiments on 3D Shapes . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.2.1 Dataset generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.2.2 Metrics .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.2.3 Architecture, inner solver & hyperparameters . . . . . . . . . . . . . . . .\n\nD.2.4 Experiments violating assumptions\n\n. . . . . . . . . . . . . . . . . . . . .\n\nD.2.5 Visual evaluation .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.2.6 Additional metrics for disentanglement\n\n. . . . . . . . . . . . . . . . . . .\n\nD.3 Meta-learning experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n15\n\n1\n\n2\n\n2\n\n4\n\n4\n\n6\n\n6\n\n7\n\n8\n\n9\n\n16\n\n17\n\n20\n\n21\n\n21\n\n21\n\n23\n\n23\n\n24\n\n24\n\n25\n\n25\n\n25\n\n26\n\n27\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Table of Notation.\n\nNorms & pseudonorms\n\n∥·∥ ∥A∥2,1 ∥A∥2,0\n\nx ∈ Rd X ⊂ Rd y ∈ R Y ⊂ R\n\nEuclidean norm on vectors and Frobenius norm on matrices\n\n:= (cid:80)m := (cid:80)m\n\nj=1 ∥A:j∥ j=1 1∥A:j ∦=0, where 1 is the indicator function.\n\nData\n\nObservations Support of observations Target Support of targets\n\nLearned/ground-truth model\n\nW ∈ Rk×m ˆW ∈ Rk×m θ\nˆθ fθ : Rd → Rm f ˆθ : Rd → Rm η ∈ Rk PW S\nPW |S p(S) S\n\nGround-truth coefficients Learned coefficients Ground-truth parameters of the representation Learned parameters of the representation Ground-truth representation Learned representation Parameter of the distribution p(y; η) Distribution over ground-truth coefficient matrices W\n\n:= {j ∈ [m] | W:j ̸= 0} (support of W )\n\nConditional distribution of W given S. Ground-truth distribution over possible supports S Support of the distribution p(S)\n\nOptimization\n\nW Λ\n\nPrimal variable Dual variable\n\nh∗ : a (cid:55)→ supb∈Rd ⟨a, b⟩ − h(b), Fenchel conjugate of the function h : Rd → R\n\nf □g : a (cid:55)→ minb f (a − b) + g(b) , inf-convolution of the functions f and g\n\nBST : (a, τ )\n\n(cid:55)→ (1 − τ /∥a∥)+ a, block soft-thresholding operator\n\nA PROOFS OF SECTION 2.1\n\nProposition 1 (MLE Invariance to Invertible Linear Transformations of the Features). Let ˆW ( ˆθ) and ˆW (θ) be the solutions to Problem (1) with the representations f ˆθ and fθ, respectively (which we assume are unique). If there exists an invertible matrix L such that, ∀x ∈ X , f ˆθ(x) = Lfθ(x); then we have, ∀x ∈ X , ˆW ( ˆθ)\n\nn f ˆθ(x) = ˆW (θ)\n\nn fθ(x).\n\nn\n\nn\n\nProof. By definition of ˆW ( ˆθ), we have that, for all ˆW ∈ Rk×m,\n\n(cid:88)\n\nlog p(y; ˆW ( ˆθ)f ˆθ(x)) ≥\n\n(x,y)∈D (cid:88)\n\nlog p(y; ˆW ( ˆθ)Lfθ(x)) ≥\n\n(x,y)∈D\n\n(cid:88)\n\n(x,y)∈D (cid:88)\n\n(x,y)∈D\n\nlog p(y; ˆW f ˆθ(x))\n\nlog p(y; ˆW Lfθ(x)) .\n\n(7)\n\n(8)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nBecause Rk×mL = Rk×m, we have that, for all ˆW ∈ Rk×m,\n\n(cid:88)\n\nlog p(y; ˆW ( ˆθ)Lfθ(x)) ≥\n\n(cid:88)\n\nlog p(y; ˆW fθ(x)) ,\n\n(x,y)∈D\n\n(x,y)∈D\n\nwhich is to say that ˆW (θ) = ˆW ( ˆθ)L, or put differently, ˆW ( ˆθ) = ˆW (θ)L−1. It implies\n\nˆW ( ˆθ)f ˆθ(x) = ˆW (θ)L−1Lfθ(x) = ˆW (θ)fθ(x) ,\n\n(9)\n\n(10)\n\nwhich is what we wanted to show. Proposition 2 (Population MLE for Linearly Entangled Representations). Let ˆW ( ˆθ) ∞ be the solution of the population-based MLE, arg max ̃W Ep(x,y) log p(y; ̃W f ˆθ(x)) (assumed to be unique). Suppose f ˆθ is linearly equivalent to fθ, and Assumption 1 holds, then, ˆW ( ˆθ)\n\n∞ = W L−1.\n\nProof. By definition of ˆW ( ˆθ)\n\n∞ , we have that, for all ̃W ∈ Rk×m,\n\nEp(x,y) log p(y; ˆW ( ˆθ)\n\nEp(x,y) log p(y; ˆW ( ˆθ)\n\n∞ f ˆθ(x)) ≥ Ep(x,y) log p(y; ̃W f ˆθ(x)) ∞ Lfθ(x)) ≥ Ep(x,y) log p(y; ̃W Lfθ(x)) .\n\nIn particular, the inequality holds for ̃W := W L−1, which yields\n\nEp(x,y) log p(y; ˆW ( ˆθ)\n\n∞ Lfθ(x)) ≥ Ep(x,y) log p(y; W fθ(x))\n\n0 ≥ Ep(x,y)\n\n(cid:104)\n\nlog p(y; W fθ(x)) − log p(y; ˆW ( ˆθ)\n\n(cid:105) ∞ Lfθ(x))\n\n0 ≥ Ep(x)KL(p(y; W fθ(x)) || p(y; ˆW ( ˆθ)\n\n∞ Lfθ(x))) .\n\nSince the KL is always non-negative, we have that,\n\nEp(x)KL(p(y; W fθ(x)) || p(y; ˆW ( ˆθ)\n\n∞ Lfθ(x))) = 0 ,\n\nwhich in turn implies\n\nEp(x,y) log p(y; ˆW ( ˆθ) Ep(x,y) log p(y; ˆW ( ˆθ)\n\nEp(x,y) log p(y; ˆW ( ˆθ)\n\n∞ Lfθ(x)) = Ep(x,y) log p(y; W fθ(x)) ∞ Lfθ(x)) = Ep(x,y) log p(y; W L−1Lfθ(x)) ∞ f ˆθ(x)) = Ep(x,y) log p(y; W L−1f ˆθ(x))\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\nSince the solution to the population MLE from Proposition 2 is assumed to be unique, this equality holds if and only if ˆW ( ˆθ)\n\n∞ = W L−1.\n\nB IDENTIFIABILITY THEORY\n\nThe following lemma will be important for proving Theorem 3. from Lachapelle et al. (2022). Lemma 1 (Sparsity pattern of an invertible matrix contains a permutation). Let L ∈ Rm×m be an invertible matrix. Then, there exists a permutation σ such that Li,σ(i) ̸= 0 for all i.\n\nThe argument\n\nis taken\n\nProof. Since the matrix L is invertible, its determinant is non-zero, i.e.\n\ndet(L) :=\n\n(cid:88)\n\nσ∈Sm\n\nsign(σ)\n\nm (cid:89)\n\ni=1\n\nLi,σ(i) ̸= 0 ,\n\n(21)\n\nwhere Sm is the set of m-permutations. This equation implies that at least one term of the sum is non-zero, meaning there exists σ ∈ Sm such that for all i ∈ [m], Li,σ(i) ̸= 0.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFor all W ∈ W, we are going to denote by ˆW (W ) some estimator of W . The following result provides conditions under which if ˆW (W ) allows a perfect fit of the ground-truth distribution p(y | x, W ), then the representation fθ and the parameter W are identified up to an invertible linear transformation. Many works have showed similar results in various context (Hyvärinen & Morioka, 2016; Khemakhem et al., 2020a; Roeder et al., 2021; Ahuja et al., 2022c). We reuse some of their proof techniques.\n\nTheorem 2 (Linear identifiability). Let ˆW (·) : W → Rk×m. Suppose Assumptions 2 to 5 hold and that, for all W ∈ W, x ∈ X and y ∈ Y, the following holds\n\np(y; ˆW (W )f ˆθ(x)) = p(y; W fθ(x)) .\n\n(22)\n\nThen, there exists an invertible matrix L ∈ Rm×m such that, for all x ∈ X , fθ(x) = Lf ˆθ(x) and such that, for all W ∈ W, ˆW (W ) = W L\n\nProof. By Assumption 3, Equation (23) implies that W fθ(x) = ˆW (W )f ˆθ(x). Assumption 5\n\nensures that we can construct an invertible matrix U :=\n\n\n\n\n\n \n\n\ni1,:\n\nW (1) ... W (dz) idz ,:\n\n. Construct analogously ˆU :=\n\n \n\n\n\n\n\n\n \n\n\ni1,:\n\nˆW (W (1)) ... ˆW (W (dz )) idz ,:\n\n. This allows us to write U fθ(x) = ˆU f ˆθ(x). Left-multiplying by U −1 on both sides\n\n \n\n\nyields fθ(x) = Lf ˆθ(x), where L := U −1 ˆU . Using the invertible matrix F from Assumption 4, we can thus write F = L ˆF where we defined ˆF := [f ˆθ(x(1)), · · · , f ˆθ(x(dz))]. Since F is invertible, so are L and ˆF . By substituting F = L ˆF in W F = ˆW (W ) ˆF , we obtain W L ˆF = ˆW (W ) ˆF . By right-multiplying both sides by ˆF −1, we obtain W L = ˆW (W ).\n\nThe following theorem is where most of the theoretical contribution of this work lies. Note that Theorem 1, from the main text, is a straightforward application of this result.\n\nTheorem 3. (Disentanglement via task sparsity) Let ˆW (·) : W → Rk×m. Suppose Assumptions 3 to 7 hold and that, for all W ∈ W, x ∈ X and y ∈ Y, the following holds\n\np(y; ˆW (W )f ˆθ(x)) = p(y; W fθ(x)) .\n\n(23)\n\nMorevover, assume that E∥ ˆW (W )∥2,0 ≤ E∥W ∥2,0, where both expectations are taken w.r.t. PW and ∥W ∥2,0 := (cid:80)m 1(W:j ̸= 0) with 1(·) the indicator function. Then, f ˆθ is disentangled w.r.t. fθ (Definition 1).\n\nj=1\n\nProof. First of all, by Assumptions 3 to 5, we can apply Theorem 2 to conclude that fθ(x) = Lf ˆθ(x) and W L = ˆW (W ) for some invertible matrix L. We can thus write E∥W L∥2,0 ≤ E∥W ∥2,0.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nWe can write\n\nE∥W ∥2,0 = Ep(S)E[\n\nm (cid:88)\n\n1(W:j ̸= 0) | S]\n\nj=1\n\n= Ep(S)\n\n= Ep(S)\n\n= Ep(S)\n\nm (cid:88)\n\nj=1\n\nm (cid:88)\n\nj=1\n\nm (cid:88)\n\nj=1\n\nE[1(W:j ̸= 0) | S]\n\nPW |S[W:j ̸= 0]\n\n1(j ∈ S) ,\n\nwhere the last step follows from the definition of S.\n\nWe now perform similar steps for E∥W L∥2,0:\n\nE∥W L∥2,0 = Ep(S)E[\n\nm (cid:88)\n\nj=1\n\n1(W L:j ̸= 0) | S]\n\n= Ep(S)\n\n= Ep(S)\n\n= Ep(S)\n\nm (cid:88)\n\nj=1\n\nm (cid:88)\n\nj=1\n\nm (cid:88)\n\nj=1\n\nE[1(W L:j ̸= 0) | S]\n\nPW |S[W L:j ̸= 0]\n\nPW |S[W:SLS,j ̸= 0] .\n\nNotice that\n\nPW |S[W:SLS,j ̸= 0] = 1 − PW |S[W:SLS,j = 0]\n\n(24)\n\n(25)\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\nLet Nj be the support of L:j, i.e. Nj := {i ∈ [m] | Li,j ̸= 0}. When S ∩ Nj = ∅, LS,j = 0 and thus PW |S[W:SLS,j = 0] = 1. When S ∩ Nj ̸= ∅, LS,j ̸= 0, by Assumption 6 we have that PW |S[W:SLS,j = 0] = 0. Thus\n\nwhich allows us to write\n\nWe thus have that\n\nPW |S[W:SLS,j ̸= 0] = 1 − 1(S ∩ Nj = ∅)\n\n= 1(S ∩ Nj ̸= ∅) ,\n\nm (cid:88)\n\nj=1\n\n1(S ∩ Nj ̸= ∅) .\n\nE∥W L∥2,0 = Ep(S)\n\nE∥W L∥2,0 ≤ E∥W ∥2,0 m\n(cid:88)\n\n1(S ∩ Nj ̸= ∅) ≤ Ep(S)\n\n1(j ∈ S) .\n\nj=1\n\nEp(S)\n\nm (cid:88)\n\nj=1\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\nSince L is invertible, by Lemma 1, there exists a permutation σ : [m] → [m] such that, for all j ∈ [m], Lj,σ(j) ̸= 0. In other words, for all j ∈ [m], j ∈ Nσ(j). Of course we can permute the terms of the l.h.s. of eq. (37), which yields\n\nEp(S)\n\nm (cid:88)\n\nj=1\n\n1(S ∩ Nσ(j) ̸= ∅) ≤ Ep(S)\n\nm (cid:88)\n\nj=1\n\n1(j ∈ S)\n\nEp(S)\n\nm (cid:88)\n\nj=1\n\n(cid:0)1(S ∩ Nσ(j) ̸= ∅) − 1(j ∈ S)(cid:1) ≤ 0 .\n\n(38)\n\n(39)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nWe notice that each term 1(S ∩ Nσ(j) ̸= ∅) − 1(j ∈ S) ≥ 0 since whenever j ∈ S, we also have that j ∈ S ∩ Nσ(j) (recall j ∈ Nσ(j)). Thus, the l.h.s. of eq. (39) is a sum of non-negative terms which is itself non-positive. This means that every term in the sum is zero: ∀S ∈ S, ∀j ∈ [m], 1(S ∩ Nσ(j) ̸= ∅) = 1(j ∈ S) .\n\n(40)\n\nImportantly,\n\nand since S ∩ Nσ(j) = ∅ ⇐⇒ Nσ(j) ⊆ Sc we have that\n\n∀j ∈ [m], ∀S ∈ S, j ̸∈ S =⇒ S ∩ Nσ(j) = ∅ ,\n\n∀j ∈ [m], ∀S ∈ S, j ̸∈ S =⇒ Nσ(j) ⊆ Sc Sc .\n\n∀j ∈ [m], Nσ(j) ⊆\n\n(cid:92)\n\n(41)\n\n(42)\n\n(43)\n\nBy Assumption 7, we have that (cid:83) S∈S|j̸∈S S = [m] \\ {j}. By taking the complement on both sides and using De Morgan’s law, we get (cid:84) S∈S|j̸∈S Sc = {j}, which implies that Nσ(j) = {j} by Equation (43). Thus, L = DP where D is an invertible diagonal matrix and P is a permutation matrix.\n\nS∈S|j̸∈S\n\nB.1 PROOF OF THEOREM 1\n\nBefore presenting Theorem 1 from the main text, we first present a variation of it where we constrain E∥ ˆW (W )∥2,0 to be smaller than E∥W ∥2,0. We note that this is weaker than imposing ∥ ˆW (W )∥2,0 ≤ ∥W ∥2,0 for all W ∈ W, as is the case in Problem (3) of Theorem 1. Theorem 4 (Sparse multitask learning for disentanglement). Let ˆθ be a minimizer of\n\nmin ˆθ\n\ns.t.\n\nEPW Ep(x,y|W ) − log p(y; ˆW (W )f ˆθ(x))\n\n∀ W ∈ W, ˆW (W ) ∈ arg min\n\n ̃W\n\nEp(x,y|W ) − log p(y; ̃W f ˆθ(x))\n\n(44)\n\nEPW ∥ ˆW (W )∥2,0 ≤ EPW ∥W ∥2,0 .\n\nThen, under Assumptions 2 to 7, f ˆθ is disentangled w.r.t. fθ (Definition 1).\n\nProof. First, notice that\n\n(45)\n\n0 ≤ EPW Ep(x|W )KL(p(y; W fθ(x)) || p(y; ˆW (W )f ˆθ(x))) EPW Ep(x,y|W ) − log p(y; W fθ(x)) ≤ EPW Ep(x,y|W ) − log p(y; ˆW (W )f ˆθ(x)) .\n\n(46) For a fixed value of x and W , it is well known that KL(p(y; W fθ(x)) || p(y; ˆW (W )f ˆθ(x))) = 0 if and only if, for all y ∈ Y, p(y; W fθ(x)) = p(y; ˆW (W )f ˆθ(x)). By Assumption 3, this is equivalent to W fθ(x) = ˆW (W )f ˆθ(x). Thus, for the equality to hold in eq. (45), we need W fθ(x) = ˆW (W )f ˆθ(x) everywhere. Of course, the global minimum can be achieved by respecting EPW ∥ ˆW (W )∥2,0 ≤ EPW ∥W ∥2,0, simply by setting ˆθ := θ and ˆW (W ) := W . The above implies that if ˆθ is some minimizer of Problem (44), we must have that W fθ(x) = ˆW (W )f ˆθ(x) everywhere and EPW || ˆW (W )||0 ≤ EPW ||W ||0. Thus, Theorem 3 implies the desired conclusion.\n\nBased on Theorem 4, we can slightly adjust the argument to prove Theorem 1 from the main text. Theorem 1 (Sparse multi-task learning for disentanglement). Let ˆθ be a minimizer of\n\nmin ˆθ\n\ns.t.\n\nEPW Ep(x,y|W ) − log p(y; ˆW (W )f ˆθ(x))\n\n∀ W ∈ W, ˆW (W ) ∈\n\narg min ̃W s.t. || ̃W ||2,0≤||W ||2,0\n\nEp(x,y|W ) − log p(y; ̃W f ˆθ(x)) .\n\n(3)\n\nThen, under Assumptions 2 to 7, f ˆθ is disentangled w.r.t. fθ (Definition 1).\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nProof. The first part of the argument in the proof of Theorem 4 applies here as well, meaning: for the equality to hold in eq. (45), we need W fθ(x) = ˆW (W )f ˆθ(x) everywhere. This global minimum can be achieved by respecting ∥ ˆW (W )∥2,0 ≤ ∥W ∥2,0 for all W ∈ W simply by setting ˆθ := θ and ˆW (W ) := W . This means that if ˆθ is some minimizer of Problem (3), we must have that W fθ(x) = ˆW (W )f ˆθ(x) holds everywhere and that, for all W ∈ W, ∥ ˆW (W )∥2,0 ≤ ∥W ∥2,0. Of course, this means EPW || ˆW (W )||0 ≤ EPW ||W ||0, which allows us to apply Theorem 3 to obtain the desired conclusion.\n\nB.2 A DISTRIBUTION WITHOUT DENSITY SATISFYING ASSUMPTION 6\n\nInterestingly, there are distributions over W1,S | S that do not have a density w.r.t. the Lebesgue measure, but still satisfy Assumption 6. This is the case, e.g., when W1,S | S puts uniform mass over a (|S| − 1)-dimensional sphere embedded in R|S| and centered at zero. In that case, for all a ∈ R|S|\\{0}, the intersection of span{a}⊥, which is (|S| − 1)-dimensional, with the (|S| − 1)- dimensional sphere is (|S| − 2)-dimensional and thus has probability zero of occurring. One can certainly construct more exotic examples of measures satisfying Assumption 6 that concentrate mass on lower dimensional manifold.\n\nC OPTIMIZATION DETAILS\n\nC.1 GROUP LASSO SVM DUAL\n\nNotation. The Fenchel conjugate of a function h : Rd → R is written h∗ and is defined for any y ∈ Rd, by h∗(y) = supx∈Rd ⟨x, y⟩ − h(x). Definition 2. (Primal Group Lasso Soft-Margin Multiclass SVM.) The primal problem of the group Lasso soft-margin multiclass SVM is defined as\n\nmin W ∈Rk×m\n\nLin(W ; F , Y ) :=\n\nn (cid:88)\n\ni=1\n\nmax l∈[k]\n\n(1 + (Wyi: − Wl:)Fi: − Yil) + λ1∥W ∥2,1 + λ2\n\n2 ∥W ∥2\n\n(47)\n\nProposition 3. (Dual Group Lasso Soft-Margin Multiclass SVM.) The dual of the inner problem with Lin as defined in (5) writes\n\nmin Λ∈Rn×k\n\n1 λ2\n\nm (cid:88)\n\nj=1\n\n∥BST (cid:0)(Y − Λ)⊤F:j, λ1\n\n(cid:1)∥2 + ⟨Y , Λ⟩ +\n\nn (cid:88)\n\ni=1\n\n1 k (cid:80)\n\nl=1\n\nΛil=1\n\nn (cid:88)\n\nk (cid:88)\n\n+\n\ni=1\n\nl=1\n\n1Λil≥0 ,\n\n(6)\n\nwith BST the block soft-thresholding operator: BST : (a, τ ) (cid:55)→ (1 − τ /∥a∥)+ a, F ∈ Rn×m the concatenation of {f ˆθ(x)}(x,y)∈Dtrain. In addition, the primal-dual link writes, for all j ∈ [m], W:j = BST (cid:0)(Y − Λ)⊤F:j, λ1\n\n(cid:1) /λ2.\n\nThe primal objective 47 can be hard to minimize with modern solvers. Moreover in few-shot learning applications, the number of features m is usually much larger than the number of samples n (in Lee et al. 2019, m = 1.6 · 104 and n ≤ 25), hence we solve the dual of Problem (47).\n\nProof of Proposition 3. Let g : u (cid:55)→ λ1∥u∥ + λ2 following lemmas.\n\n2 ∥u∥2. Proof of Proposition 3 is composed of the\n\nLemma 2.\n\ni) The dual of Problem (47) is m\n(cid:88)\n\nmin Λ∈Rn×k\n\nj=1\n\ng∗((Y − Λ)⊤F:j) + ⟨Y , Λ⟩\n\ns.t. ∀i ∈ [n],\n\nk (cid:88)\n\nl=1\n\nΛil = 1 ,\n\n∀i ∈ [n], l ∈ [k], Λil ≥ 0 ,\n\nwhere g∗ is the Fenchel conjugate of the function g.\n\n21\n\n(48)\n\nUnder review as a conference paper at ICLR 2023\n\nii) The Fenchel conjugate of the function g writes\n\n∀v ∈ RK, g∗(v) =\n\n1 λ2\n\n∥BST(v, λ1)∥2 .\n\n(49)\n\nLemmas 4 i) and 4 ii) yields Proposition 3.\n\nProof of Lemma 4 i). The Lagrangian of Problem (47) writes:\n\nL(W , ξ, Λ) =\n\nm (cid:88)\n\nj=1\n\ng(W:j) +\n\n(cid:88)\n\nξi +\n\nn (cid:88)\n\nk (cid:88)\n\ni\n\ni=1\n\nl=1\n\n(1 − ξi − Wyi: · Fi: + Wl: · Fi: − Yil)Λil . (50)\n\n∂ξL(W , ξ, Λ) = 0 yields ∀i ∈ [n], (cid:80)k\n\nl=1 Λil = 1. Then the Lagrangian rewrites\n\nmin W\n\nmin ξ\n\nL(W , ξ, Λ) = min W ,ξ\n\nm (cid:88)\n\nj=1\n\ng(W:j) +\n\n=\n\nm (cid:88)\n\nj=1\n\nmin W:j\n\ng(W:j) −\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1 (cid:124)\n\n(cid:124)\n\nThen the dual problem writes:\n\nξi +\n\nn (cid:88)\n\nk (cid:88)\n\ni=1\n\nl=1\n\n(−ξi − Wyi: · Fi: + Wl: · Fi: − Yil)Λil\n\nk (cid:88)\n\n(Fi:Yil − Fi:Λil)Wl:\n\nl=1\n\n(cid:123)(cid:122) =⟨(Y −Λ)⊤F:j ,W:j ⟩ (cid:123)(cid:122) =−g∗((Y −Λ)⊤F:j )\n\n(cid:125)\n\n(cid:125)\n\nn (cid:88)\n\nk (cid:88)\n\n−\n\ni=1\n\nl=1\n\nYilΛil .\n\nmin Λ∈Rn×k\n\nm (cid:88)\n\nj=1\n\ng∗ (cid:0)(Y − Λ)⊤F:j\n\n(cid:1) + ⟨Y , Λ⟩\n\ns. t. ∀i ∈ [n]\n\nk (cid:88)\n\nl=1\n\nΛil = 1 , ∀i ∈ [n], l ∈ [k], Λil ≥ 0 .\n\n(51)\n\n(52)\n\nProof of Lemma 4 ii). Let h : u (cid:55)→ ∥u∥2 + κ following steps.\n\n2 ∥u∥2. The proof of Lemma 4 i) is done using the\n\nLemma 3.\n\ni) h∗(v) = 1\n\n2κ ∥v∥2\n\n2 − (cid:0) κ\n\n2 ∥·∥2\n\n2\n\n□∥·∥2\n\n(cid:1) (v/κ).\n\nii) (cid:0) κ\n\n2 ∥·∥2\n\n2\n\n□∥·∥2\n\n(cid:1) (v) = κ\n\n2 ∥v∥2\n\n2 − 1\n\n2κ ∥BST(κv, 1)∥2.\n\nProof of Lemma 4 i). With κ = λ2/λ1, the Fenchel transform of h : w (cid:55)→ ∥w∥2 + κ∥w∥2.\n\nh(u) = ∥u∥2 + κ h∗(v) = sup\n\n2 ∥u∥2\n\n2\n\n(cid:0)v⊤w − ∥w∥2 − κ\n\n2 ∥w∥2\n\n2\n\n(cid:1)\n\n2κ ∥v∥2\n\n2 + sup\n\n(cid:0)− κ\n\n2 ∥w − v/κ∥2\n\n2 − ∥w∥2\n\n(cid:1)\n\nw = 1\n\nw\n\n2 − inf\n\nw\n\n= 1\n\n= 1\n\n2κ ∥v∥2 2κ ∥v∥2\n\n(cid:0) κ\n\n2 ∥w − v/κ∥2\n\n2 + ∥w∥2\n\n(cid:1)\n\n2 − ( κ\n\n2 ∥·∥2\n\n2\n\n□∥·∥2)(v/κ) .\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nProof of Lemma 4 ii).\n\n2 ∥·∥2 ( κ\n\n2\n\n□∥·∥2)(v) = ( κ = ( 1\n\n2\n\n2 ∥·∥2 2κ ∥·∥2\n\n= sup\n\n∥w∥2≤1\n\n□∥·∥2)∗∗(v) 2 + ιB2)∗(v) (cid:0)v⊤w − 1\n\n2κ ∥w∥2\n\n2\n\n(cid:1)\n\n= κ\n\n2 ∥v∥2 + sup\n\n∥w∥2≤1\n\n− 1\n\n2κ ∥κv − w∥2\n\n2\n\n= κ\n\n2 ∥v∥2 − 1\n\n2κ ∥BST(κv, 1)∥2\n\n2 .\n\n=\n\ng∗(u) = λ1h∗(u/λ1) λ1 2κ λ2 1\n2λ2 1\nλ2\n\n=\n\n=\n\n∥BST(u/λ1, 1)∥2\n\n∥BST(u/λ1, 1)∥2\n\n∥BST(u, λ1)∥2 .\n\nD EXPERIMENTAL DETAILS\n\nD.1 DISENTANGLED REPRESENTATION COUPLED WITH SPARSITY REGULARIZATION\n\nIMPROVES GENERALIZATION\n\nWe consider the following data generating process: We sample the ground-truth features fθ(x) from a Gaussian distribution N (0, Σ) where Σ ∈ Rm×m and Σi,j = 0.9|i−j|. Moreover, the labels are given by y = w · fθ(x) + ε where w ∈ Rm, ε ∼ N (0, 0.04) and m = 100. The ground-truth weight vector w is sampled once from N (0, Im×m) and mask some of its components to zero: we vary the fraction of meaningful features (l/m) from very sparse (l/m = 5%) to less sparse (l/m = 80%) settings. For each case, we study the sample complexity by varying the number of training samples from 25 to 150, but evaluating the generalization performance on a larger test dataset (1000 samples). To generate the entangled representations, we multiply the true latent variables fθ(x) by a randomly sampled orthogonal matrix L, i.e., f ˆθ(x) := Lfθ(x). For the disentangled representation, we simply consider the true latents, i.e. f ˆθ(x) := fθ(x). Note that in principle we could have considered an invertible matrix L that is not orthogonal for the linearly entangled representation and a component-wise rescaling for the disentangled representation. The advantage of not doing so and opting for our approach is that the conditioning number of the covariance matrix of f ˆθ(x) is the same for both the entangled and the disentangled, hence offering a fairer comparison.\n\nFor both the case of entangled and disentangled representation, we solve the regression problem with Lasso and Ridge regression, where the associated hyperparameters (regularization strength) were inferred using 5-fold cross validation on the input training dataset. Using both lasso and ridge regression would help us to show the effect of encouraging sparsity.\n\nIn Figure 1 for the sparsest case (l/m = 5%), we observe that that Disentangled-Lasso approach has the best performance when we have less training samples, while the Entangled-Lasso approach performs the worst. As we increase the number of training samples, the performance of EntangledLasso approaches that of Disentangled-Lasso, however, learning under the Disentangled-Lasso approach is sample efficient. Disentangled-Lasso obtains R2 greater than 0.5 with only 25 training samples, while other approaches obtain R2 close to zero. Also, Disentagled-Lasso converges to the\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\noptimal R2 using only 50 training samples, while Entangled-Lasso does the same with 150 samples samples.\n\nNote that the improvement due to disentanglement does not happen for the case of ridge regression as expected and there is no of a difference between the methods Disentangled-Ridge and EntangledRidge because the L2 norm is invariant to orthogonal transformation. Also, having sparsity in the underlying task is important. Disentangled-Lasso shows the max improvement for the case of l/m = 5%, with the gains reducing as we decrease the sparsity in the underlying task (l/m = 80%).\n\nD.2 SEMI-REAL EXPERIMENTS ON 3D SHAPES\n\nFigure 6: Prediction performance (R Score) for inner-Lasso, inner-Ridge and inner-Ridge combined with ICA as a function of the regularization parameter (left and middle). Varying level of correlation between latents (top) and noise on the latents (bottom). The right columns shows performance of the best hyperparameter for different values of correlation and noise levels.\n\nD.2.1 DATASET GENERATION\n\nDetails on 3D Shapes. The 3D Shapes dataset (Burgess & Kim, 2018) contains synthetic images of colored shapes resting in a simple 3D scene. These images vary across 6 factors: Floor hue (10 values linearly spaced in [0, 1]); Wall hue (10 values linearly spaced in [0, 1]); Object hue (10 values linearly spaced in [0, 1]); Scale (8 values linearly spaced in [0, 1]); Shape (4 values in [0, 1, 2, 3]); and Orientation (15 values linearly spaced in [-30, 30]). These are the factors we aim to disentangle. We standardize them to have mean 0 and variance 1. We denote by Z ⊂ R6, the set of all possible latent factor combinations. In our framework, this corresponds to the support of the ground-truth features fθ(x). We note that the points in Z are arranged in a grid-like fashion in R6.\n\nTask generation. For all tasks t, the labelled dataset Dt = {(x(t,i)), y(t,i))}n i=1 is generated by first sampling the ground-truth latent variables z(t,i) := fθ(x(t,i)) i.i.d. according to some distribution p(z) over Z, while the corresponding input is obtained doing x(t,i) := f −1 θ (z(t,i)) (fθ is invertible in 3D Shapes). Then, a sparse weight vector w(t) is sampled randomly by doing w(t) := ̄w(t) ⊙ s(t), were ⊙ is the Hadamard (component-wise) product, ̄w(t) ∼ N (0, I) and s ∈ {0, 1}6 is a binary vector with independent components sampled from a Bernoulli distribution with (p = 0.5). Then, the labels are computedfor each example as y(t,i) := w(t) · x(t,i) + ε(t,i), where ε(t,i) is independent Gaussian noise. In every tasks, the dataset has size n = 50. New tasks are generated continuously as we train. Figures 4 and 6 explores various choices of p(z), i.e. by varying the level of correlation between the latent variables and by varying the level of noise on the ground-truth latents.\n\n24\n\n0.00.010.030.10.31.0λ/λmax01RCorrelation=0.00.00.010.030.10.31.0λ/λmaxCorrelation=0.90.00.51.0CorrelationMultiplecorrelationvalues0.00.010.030.10.31.0λ/λmax0.40.60.81RNoisescale=00.00.010.030.10.31.0λ/λmaxNoisescale=100.250.50.751NoisescaleMultiplenoisevaluesUnder review as a conference paper at ICLR 2023\n\nNoise on latents. To make the dataset slightly more realistic, we get rid of the artificial grid-like structure of the latents by adding noise to it. This procedure transforms Z into a new support Zα, where α is the noise level. Formally, Zα := (cid:83) z∈Z {z + uz} where the uz are i.i.d samples from the uniform over the hypercube\n\n(cid:20)\n\n−α\n\n∆z1 2\n\n, α\n\n∆z1 2\n\n(cid:21)\n\n(cid:20)\n\n×\n\n−α\n\n(cid:21)\n\n∆z2 2\n\n, α\n\n∆z2 2\n\n(cid:20)\n\n× . . . ×\n\n−α\n\n∆z6 2\n\n, α\n\n∆z6 2\n\n(cid:21)\n\n,\n\nwhere ∆zi denotes the gap between contiguous values of the factor zi. When α = 0, no noise is added and the support Z is unchanged, i.e., Z1 = Z. As long as α ∈ [0, 1], contiguous points in Z cannot be interchanged in Zα. We also clarify that the ground-truth mapping fθ is modified to fθ,α consequently: for all x ∈ X , fθ,α(x) := fθ(x) + uz. We emphasize that the uz are sampled only once such that fθ,α(x) is actually a deterministic mapping.\n\nVarying correlations. To verify that our approach is robust to correlations in the latents, we construct p(z) as follows: We consider a Gaussian density centered at 0 with covariance Σi,j := ρ + 1(i = j) ̇(1 − ρ). Then, we evaluate this density on the points of Zα and renormalize to have a well-defined probability distribution over Zα. We denote by pα,ρ(z) the distribution obtain by this construction.\n\nIn the top rows of Figures 4 and 6, the latents are sampled from pα=1,ρ(z) and ρ varies between 0 and 0.99. In the bottom rows of Figures 4 and 6, the latents are sampled from pα,ρ=0.9(z) and α varies from 0 to 1.\n\nD.2.2 METRICS\n\n(cid:80)m\n\nWe evaluate disentanglement via the mean correlation coefficient (Hyvärinen & Morioka, 2016; Khemakhem et al., 2020a) which is computed as follows: The Pearson correlation maThen, MCC = trix C between the ground-truth features and learned ones is computed. maxπ∈permutations j=1 |Cj,π(j)|. We also evaluate linear equivalence by performing linear regression to predict the ground-truth factors from the learned ones, and report the mean of the Pearson correlations between the ground-truth latents and the learned ones. This metric is known as the coefficient of multiple correlation, R, and turns out to be the square-root of the more widely known coefficient of determination, R2. The advantage of using R over R2 is that we always have MCC ≤ R.\n\n1 m\n\nD.2.3 ARCHITECTURE, INNER SOLVER & HYPERPARAMETERS\n\nWe use the four-layer convolutional neural network typically used in the disentanglement literature (Locatello et al., 2019). As mentioned in Section 2.2.2, the norm of the representation f ˆθ(x) must be controlled to make sure the regularization remains effective. To do so, we apply batch normalization (Ioffe & Szegedy, 2015) at the very last layer of the neural network and do not learn its scale and shift parameters. Empirically, we do see the expected behavior that, without any normalization, the norm of f ˆθ(x) explodes as we train, leading to instabilities and low sparsity. In these experiments, the distribution p(y; η) used for learning is a Gaussian with fixed variance. In that case, the inner problem of Section 2.2.2 reduces to Lasso regression. Computing the hypergradient w.r.t. θ requires solving this inner problem. To do so, we use Proximal Coordinate Descent (Tseng, 2001; Richtárik & Takáˇc, 2014).\n\nIn Figures 4 and 6, we explore various levels of regularization λ. In our implementation of innerLasso, λmax := 1 n ∥F ⊤y∥∞ where F ∈ Rn×m is the design matrix of the features of the samples of a task, while in the inner-Ridge implementation, λmax := 1\n\nn ∥F ∥2.\n\nD.2.4 EXPERIMENTS VIOLATING ASSUMPTIONS\n\nIn this section, we explore variations of the experiments of Section 4, but this time the assumptions of Theorem 1 are violated.\n\nFigure 7 shows different degrees of violation of Assumption 7. We consider the cases where S := {{1, 2}, {3, 4}, {5, 6}} (block size = 2), S := {{1, 2, 3}, {4, 5, 6}} (block size = 3) and S := {{1, 2, 3, 4, 5, 6}} (block size = 6). Note that the latter case corresponds to having no sparsity\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nat all in the ground-truth model, i.e. all tasks requires all features. The reader can verify that these three cases indeed violate Assumption 7. In all cases, the distribution p(S) puts uniform mass over its support S. Similarly to the experiments from the main text, w := ̄w ⊙ s, where ̄w ∼ N (0, I) and s ∼ p(S) (s is the binary representation of the set S). Overall, we can see that inner-Lasso does not perform as well when Assumption 7 is violated. For example, when there is no sparsity at all (block size = 6), inner-Lasso performs poorly and is even surpassed by inner-Ridge. Nevertheless, for mild violations (block size = 2), disentanglement (as measured by MCC) remains reasonably high. We further notice that all methods obtain very good R score in all settings. This is expected in light of Theorem 2, which guarantees identifiability up to linear transformation without requiring Assumption 7.\n\nFigure 7: Disentanglement (MCC, top) and prediction (R Score, bottom) performances for innerLasso, inner-Ridge and inner-Ridge combined with ICA as a function of the regularization parameter. The metrics are plotted for multiple value of block size for the support. Block size = 6 corresponds to no sparsity in the ground truth coefficients.\n\nFigure 8 presents experiments that are identitical to those of Figure 4 in the main text, except for how w is generated. Here, the components of w are sampled independently according to wi ∼ Laplace(μ = 0, b = 1). We note that, under this process, the probability that wi = 0 is zero. This means all features are useful and Assumption 7 is violated. That being said, due to the fat tail behavior of the Laplacian distribution, many components of w will be close to zero (relatively to its variance). Thus, this can be thought of as a weaker form of sparsity where many features are relatively unimportant. Figure 8 shows that inner-Lasso can still disentangle very well. In fact, the performance is very similar to the experiments that presented actual sparsity (Figure 4).\n\nD.2.5 VISUAL EVALUATION\n\nFigures 9 to 12 show how various learned representations respond to changing a single factor of variation in the image (Higgins et al., 2017, Figure 7.A.B). We see what was expected: the higher the MCC, the more disentangled the learned features appear, thus validating MCC as a good metric for disentanglement. See captions for details.\n\n26\n\n0.00.010.030.10.31.0λ/λmax0.20.40.60.81.0MCCBlocksize=20.00.010.030.10.31.0λ/λmaxBlocksize=30.00.010.030.10.31.0λ/λmaxBlocksize=60.00.010.030.10.31.0λ/λmax0.00.51.0RBlocksize=20.00.010.030.10.31.0λ/λmaxBlocksize=30.00.010.030.10.31.0λ/λmaxBlocksize=6Under review as a conference paper at ICLR 2023\n\nFigure 8: Same experiment as Figure 4, but the task coefficient vectors w are sampled from a Laplacian distribution (instead of what was described in Appendix D.2.1). Performance is barely affected, showing some amount of robustness to violations of Assumption 7.\n\nD.2.6 ADDITIONAL METRICS FOR DISENTANGLEMENT\n\nWe implemented metrics from the DCI framework (Eastwood & Williams, 2018) to evaluate disentanglement. 1) DCI-Disentanglement: How many ground truth latent components are related to a particular component of the learned latent representation; 2) DCI-Completeness: How many learned latent components are related to a particular component of the ground truth latent representation. Note that for the definition of disentanglement used in the present work Definition 1, we want both DCI-disentanglement and DCI-completeness to be high.\n\nThe DCI framework requires a matrix of relative importance. In our implementation, this matrix is the coefficient matrix resulting from performing linear regression with inputs as the learned latent representation f ˆθ(x) and targets as the ground truth latent representation fθ(x), and denote the solution as the matrix W . Further, denote by I = |W | as the importance matrix, as Ii,j denotes the relevance of inferred latent f ˆθ(x)j for predicting the true latent fθ(x)i.\n\n27\n\n0.0.01.030.10.31.0/max0.40.60.81.0MCCCorrelation = 0.00.0.01.030.10.31.0/maxCorrelation = 0.950.60.81.0CorrelationMultiple correlation values0.0.01.030.10.31.0/max0.00.51.0RCorrelation = 0.00.0.01.030.10.31.0/maxCorrelation = 0.950.60.81.0CorrelationMultiple correlation values0.0.01.030.10.31.0/max0.40.60.81.0MCCNoise scale = 00.0.01.030.10.31.0/maxNoise scale = 100.250.50.751Noise scaleMultiple noise values0.0.01.030.10.31.0/max0.40.60.81RNoise scale = 00.0.01.030.10.31.0/maxNoise scale = 100.250.50.751Noise scaleMultiple noise valuesUnder review as a conference paper at ICLR 2023\n\nNow, for computing DCI-disentanglement, we normalize each row of the importance matrix I[i, :] by its sum so that it represents a probability distribution. Then disentanglement is given by 1 m × (cid:80)m i 1 − H(I[i, :]), where H denotes the entropy of a distribution. Note that for the desired case of each ground truth latent component being explained by a single inferred latent component, we would have H(I[i, :]) = 0 as we have a one-hot vector for the probability distribution. Similarly, for the case of each ground truth latent component being explained uniformly by all the inferred latents, H(I[i, :]) would be maximized and hence the DCI score would be minimized. To compute the DCI-completeness, we first normalize each column of the importance matrix I[:, j] by its sum so that it represents a probability distribution and then compute 1\n\ni 1 − H(I[:, j]).\n\nm × (cid:80)m\n\nFigure 13 shows the results for the 3D Shapes experiments (Section 4) with the DCI metric to evaluate disentanglement. Notice that we find the same trend as we had with the MCC metric 4, that inner-Lasso is more robust to correlation between the latent variables, and inner-Ridge + ICA performance drops down significantly with increasing correlation.\n\nD.3 META-LEARNING EXPERIMENTS\n\nExperimental settings. We evaluate the performance of our meta-learning algorithm based on a group-sparse SVM base-learner on the miniImageNet (Vinyals et al., 2016) dataset. Following the standard nomenclature in few-shot classification (Hospedales et al., 2021) with k-shot N -way, where N is the number of classes in each classification task, and k is the number of samples per class in the training dataset Dtrain , we consider 2 settings: 1-shot 5-way, and 5-shot 5-way. Note that the results presented in Figure 5 only show the performance on 5-shot classification. We use the same residual network architecture as in (Lee et al., 2019), with 12 layers and a representation of size p = 1.6 × 104.\n\nt\n\nEven though we consider a similar base-learner as MetaOptNet (Lee et al., 2019) (namely, a SVM), our control experiment with λ = 0 cannot be directly compared to the performance of the model reported in that prior work. The reason is that in order to control for any other sources of “effective regularization” (e.g., data augmentation, label smoothing), we do not include the modifications made in MetaOptNet to improve performance. Moreover, we used a different solver (proximal blockcoordinate descent, as opposed to a QP solver) to solve the inner problem Problem (6).\n\nGeneralization on meta-training tasks. In Section 2.3, we argued that evaluating the performance of the learned representations on meta-training tasks (i.e., tasks similar to the ones seen during metatraining) still shows the generalization capacity to new tasks. Indeed, those new tasks on which we evaluate performance were created using the same classes as the tasks used during meta-training, but using a combination of classes that may have not been seen in any tasks used for optimizing Problem (5). However, evaluation in meta-learning is typically done on meta-test tasks, i.e. tasks based on concepts that were never seen by any task during meta-training. This evaluation requires a stronger notion of generalization, closer to out-of-distribution generalization.\n\nBase-learner\n\n5-way 1-shot\n\n5-way 5-shot\n\nSVM (λ = 0) Group-sparse SVM (λ = 0.01)\n\n53.29 ± 0.60% 69.26 ± 0.51% 54.22 ± 0.61% 70.01 ± 0.51%\n\nMetaOptNet (Lee et al., 2019)\n\n64.09 ± 0.62% 80.00 ± 0.45%\n\nTable 2: Performance of our meta-learning algorithm on the miniImageNet benchmark. The performance is reported as the mean accuracy and 95% confidence interval on 1000 meta-test tasks. We also report the performance of MetOptNet (Lee et al., 2019) as reference, even though the performance is not directly comparable to our SVM baseline (see text for details).\n\nNonetheless, we observe in Table 2 that the performance of the meta-learning method improves as the base-learners are group-sparse.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Varying one factor at a time in the image and showing how the learned representation varies in response. This representation was learned by inner-Lasso (best hyperparameter) on a dataset with 0 correlation between latents and a noise scale of 1. The corresponding MCC is 0.99. We can see that varying a single factor in the image always result in changing a single factor in the learned representation.\n\n29\n\n21012z1z2z3z4z5z621012z1z2z3z4z5z621012z1z2z3z4z5z62101z1z2z3z4z5z621012z1z2z3z4z5z62101z1z2z3z4z5z6Under review as a conference paper at ICLR 2023\n\nFigure 10: Varying one factor at a time in the image and showing how the learned representation varies in response. This representation was learned without regularization of any kind (i.e. with inner-Ridge without regularization coefficient equal to zero) on a dataset with 0 correlation between and a noise scale of 1. The corresponding MCC is 0.63. We can see that varying a single factor in the image result in changing multiple factors in the learned representation, i.e. the representation is not disentangled.\n\n30\n\n210123z1z2z3z4z5z6202z1z2z3z4z5z621012z1z2z3z4z5z6202z1z2z3z4z5z621012z1z2z3z4z5z621012z1z2z3z4z5z6Under review as a conference paper at ICLR 2023\n\nFigure 11: Varying one factor at a time in the image and showing how the learned representation varies in response. This representation was learned with inner-Lasso (best hyperparameter) on a dataset with correlation 0.9 between latents and a noise scale of 1. The corresponding MCC is 0.96. Qualitatively, the representation appears to be well disentangled, but not as well as in Figure 9 (reflected by a drop in MCC of 0.03).\n\n31\n\n21012z1z2z3z4z5z621012z1z2z3z4z5z621012z1z2z3z4z5z62101z1z2z3z4z5z621012z1z2z3z4z5z621012z1z2z3z4z5z6Under review as a conference paper at ICLR 2023\n\nFigure 12: Varying one factor at a time in the image and showing how the learned representation varies in response. This representation was learned with inner-Ridge (best hyperparameter) on a dataset with correlation 0.9 between latents and a noise scale of 1. The corresponding MCC is 0.79. For most latent factors, we cannot identify a dominant feature, except maybe for background and object colors. The representation appears more disentangled than Figure 10, but less disentangled than Figure 11, as reflected by their corresponding MCC values.\n\n32\n\n42024z1z2z3z4z5z6024z1z2z3z4z5z62024z1z2z3z4z5z6024z1z2z3z4z5z62024z1z2z3z4z5z62024z1z2z3z4z5z6Under review as a conference paper at ICLR 2023\n\nFigure 13: Disentanglement performance (DCI) for inner-Lasso, inner-Ridge and inner-Ridge combined with ICA as a function of the regularization parameter (left and middle). The right columns shows performance of the best hyperparameter for different values of correlation and noise. The top row shows the results for the disentanglement metric of DCI and the bottom row shows the results for the completeness metric of DCI.\n\n33\n\n0.00.010.030.10.31.0λ/λmax0.00.5DCI-DisentanglementCorrelation=0.00.00.010.030.10.31.0λ/λmaxCorrelation=0.90.00.51.0CorrelationMultiplecorrelationvalues0.00.010.030.10.31.0λ/λmax0.00.5DCI-CompletenessCorrelation=0.00.00.010.030.10.31.0λ/λmaxCorrelation=0.90.00.51.0CorrelationMultiplecorrelationvalues",
    "reference": "# Summary Of The Paper\n\nThis paper studies the disentangled representation learning. In particular, authors provide the evidence on the improved generalization when the disentangled representations  coupled with sparse base-predictors are learned. The paper then presents a theoretical result on identifiability condition wonder with maximally sparse base-predictors that lead to the desired disentangled representations. Relying on the presented theory, representation that maximizes sparsity is sought, in a hope of eventual disentanglement, in the context of the multi-task learning. Such sparsity maximization however not being trivial, authors propose a sparsity-promoting bi-level optimization paradigm for learning. Later, a connection between bi-lavel optimization and the meta-learning problems is established.\n\n# Strength And Weaknesses\n\n++The paper is well written and motivated. The addressed problem is a problem of sufficient interest. \n\n++The theoretical consideration of the paper is praiseworthy. Specifically, identifiability result  of Theorem 1 is particularly interesting, intuitive, and convincing. \n\n++The paper also proposes bi-level optimization and meta-learning based approaches, which are empirically shown to be effective in Figure 1,  with fewer samples for the Disentangled-Lasso method.\n\n++Supplementary material with additional theoretical and experimental analysis is helpful.\n\n--The reported experiments are rather toy-like settings. It is not clear when the proposed method is expected to be useful. For example, results reported in Table.1 (supplementary material) shows that the proposed method does not compete well against MetaOptNet (although not being directly comparable).\n\n--The meta-learning formulation of Section 3.2 is not easy to follow. The section lacks intuitive explanation and the jump form equation (7) to Proposition (3) is not well explained.\n\n--The paper relies a lot on the supplementary material. This in itself is not a big problem, given the nature of the addressed problem. However, I believe that the paper can be made more intuitive with the help of graphic illustrations and by bringing the B.2 Discussion of Assumption in the main paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease, refer to the strengths above.\n\n# Summary Of The Review\n\nOverall, the paper makes a good case for the addressed problems and provides convincing and theoretical insights. The paper can benefit from more intuitive explanations (considering a broader audience) and the experiments on more realistic datasets.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nNo concerning ethical issues as far as can be seen."
  },
  {
    "input": "THINKSUM: PROBABILISTIC REASONING OVER SETS USING LARGE LANGUAGE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLarge language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the largest LLMs fail in scenarios that require reasoning over multiple objects or facts or making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, THINKSUM, that reasons over sets of objects or facts in a structured manner. In the first stage (THINK – ‘fast’ retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (SUM – ‘slow’ probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the advantages of THINKSUM on the BIG-bench suite of evaluation tasks, achieving improvements over the state of the art using GPT-family models on ten difficult tasks, often with far smaller model variants. We compare and contrast THINKSUM with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. We argue that because the probabilistic inference in THINKSUM is performed outside of calls to the LLM, THINKSUM is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs.\n\n1\n\nINTRODUCTION\n\nLarge language models (LLMs) (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) can recall a broad range of basic facts, recognize and mimic various forms in language, and efficiently extrapolate analogies in structure and meaning. These abilities allow LLMs to excel in zero-shot and few-shot tasks that are formulated as generation or selection of a likely completion of a prompt. This formulation requires LLMs to perform associative fast thinking, in which each token of text in the sequence making up the answer is generated or scored in one pass through the model and, other than that, no intermediate information is created or retained. Fast thinking is made possible by the compression in the LLM weights of information that is repeated in a variety of ways in large training datasets.\n\nHowever, it is increasingly evident that when reasoning, or slow thinking, is required, failure modes of LLMs are revealed. In our usage, reasoning is sequential manipulation of concepts that can be expressed in language. Tasks that require iterative retrieval of rarely stated knowledge, uncertainties over multiple objects or facts, or multiple steps of deduction are difficult even for the most advanced LLMs. In a recently designed suite of evaluations, BIG-bench (Srivastava et al., 2022), some of the tasks where the gap between machine and human performance is large involve inference sequences with nested counterfactuals (LOGICAL DEDUCTION), concepts introduced though definitions (CONCEPTUAL COMBINATIONS), etc. (see Fig. A.1). These are tasks where a human solver’s intuitive feeling of ‘(in)coherence’ is not sufficient to produce the right answer: the solution is obtained by a sequence of thoughts that can be explained in words and may even require writing down intermediate results if working memory is insufficient.\n\nWe show on several examples in BIG-bench that such problems can be addressed by a twocomponent mechanism, which we name THINKSUM:\n\n• THINK (fast thinking / association / knowledge retrieval step): creating an association of spans of text with sets of strings. This process may involve generation from a language model, as is the case in Fig. 1, where the novel word ‘binne’ is associated with the set of strings {‘cat’, ‘mink’, . . . }\n\n1\n\nFigure 1: An example adapted from the CONCEPTUAL COMBINATIONS (INVENTED WORDS) task, in which models must select the most likely completion of a phrase that includes nonce words whose definitions are given. Direct prompting evaluates completion likelihoods normalized over the four answer choices (‘people’, ‘animals’, ‘birds’, ‘researchers’). Chain-of-thought-like or auxiliary knowledge approaches would query a LLM or knowledge base for additional context. Our THINKSUM approach to this task queries a LLM (GPT-2 XL) to produce sets of examples defining the nonce words, then marginalizes over substitutions of these examples into the target phrase.\n\nby prompting GPT-3 with the definition and asking for examples. However, it may also consist of scoring alone, in order to form a matrix of probabilities over which probabilistic inference is performed.\n\n• SUM (slow thinking / SUMmarization / reasoning step): probabilistic inference that aggregates generated strings or probabilities to produce the final answer. The summarization typically involves, and often entirely consists of, summing of probabilities of strings (computed in the THINK step), as in Fig. 1, where the final word is assumed to be sampled from a mixture of possible substitutions of ‘binne’ and ‘bam’ words into the input.\n\nTHINKSUM is named by analogy with other algorithms with two basic operations that ‘expand’ and ‘aggregate’, like MapReduce in distributed computing and sum-product in graphical models.\n\nWe discuss different ways to THINK and to SUM in section §2, but we start with one example, illustrated in Fig. 1, motivated by the CONCEPTUAL COMBINATIONS (INVENTED WORDS) task in BIG-bench (Srivastava et al., 2022). In this task, the LLM is provided with two invented words and their definitions in the input. The LLM is then asked to infer the most plausible sentence that uses a combination of the invented words. As the words are invented, they are not common or consistently used in the training set, and the LLM needs to understand and combine the definitions of the invented words to reason about the meaning of the combination. The LLM is queried to produce example instances of the invented words with the help of the definitions. These example instances can be substituted into the query in place of the invented words. In this way, by mapping individual spans of the text of interest to sets we arrive at a mixture model (in this example, a mixture with 25 components, for 5 possible replacements of each of the words), which can be used in the same manner the original LLM is used, either to score text or to generate it token by token. In this case, when we score all candidate completions using this mixture model and normalize over the four choices, the correct answer – that ‘binne bams’ are for animals and not people – becomes most likely.\n\nAn important difference between THINKSUM and existing chain-of-thought-like prompt engineering methods (Wei et al., 2022; Kojima et al., 2022), is that the reasoning step is not reduced to a generation problem for the LLM, but is performed as a probabilistic inference external to the LLM. This reduces its vulnerability to features of the prompt, such as accidental distraction of the LLM by spurious patterns. Instead, we engineer the slow thinking process to make parallel calls to the LLM to query for intermediate information, then possibly perform programmatic recombination of strings (THINK). The final reasoning step – in which likelihoods obtained from the LLM for the recombinations derived from earlier steps of the reasoning process are combined to make the final prediction\n\n2\n\nAbinneisanyfurryfour-leggedcreature,andabamisasimpledwelling.DIRECTPROMPTINGCHAINOFTHOUGHT/AUXILIARYKNOWLEDGETHINKSUMAbinnebamisaplaceforpeople(55%)animals(44%)birds(0.87%)researchers(0.022%)Abinneisanyfurryfour-leggedcreature,andabamisasimpledwelling.Examplesofbinnes:cat,mink,ferret,guineapig,rabbit.Examplesofbams:hut,cabin,cottage,shelter,shack.Abinnebamisaplaceforpeople(51%)animals(48%)birds(0.76%)researchers(0.011%)Abinneisanyfurryfour-leggedcreature,andabamisasimpledwelling.binne={cat,mink,ferret,guineapig,rabbit}bam={hut,cabin,cottage,shelter,shack}Abinnebamisaplaceforanimals(65%)people(34%)birds(1.5%)researchers(0.056%)⌉⌋THINK(auxiliaryLMcallstodefinesets)AcatcottageisaplaceforArabbitcabinisaplaceforAminkshelterisaplacefor···X⌉⌋SUM(aggregateLMlikelihoods)– is left to classical probabilistic reasoning (SUM). In a sense, SUM replaces the self-attention mechanism over linear text, which is used as the sole ‘reasoning’ mechanism in chain-of-thought-like approaches that expect the intermediate ‘thoughts’ to take the form of generated tokens intervening between the input and output. Fig. 1 shows the potential brittleness of such ‘reasoning’, especially in smaller models, which have stronger recency bias (Malkin et al., 2022): if we simply list generated examples as additional context in the prompt, the recency bias causes the LLM to still give higher probability to ‘people’ than to ‘animals’, simply because ‘bam’ (simple dwelling) examples are given after the ‘binne’ examples.\n\nImposing an alternative reasoning system over an associative “knee-jerk reaction” system has an analogy with models of human cognitive processes (Tversky & Kahneman, 1974; Kahneman, 2011) that separate System 1 (fast thinking) and System 2 (slow thinking). System 2 acts as a ‘controller’ that can prime System 1 to appropriately bias its fast thinking, and, in the context of reasoning with deep learning models, has been interpreted as operating with sparse concepts that can be described in language (Bengio, 2017; Goyal & Bengio, 2020). Through repeated usage, the slow-thinking functions of System 2 can become efficiently compressed into System 1 intuitions, in the same manner that iterative ‘reasoning’ functions of which smaller LLMs are not capable become zeroshot generation capacities for large LLMs.\n\nAs is the case with humans, there is always the next frontier of problems where a trained model with remarkable ‘intuition’ needs to be slowed down. The main claim of this paper is that more is possible with LLMs of existing scale when they are used in concert with a wise System 2 controller that allows for probabilistic inference.\n\n2 FAST AND SLOW THINKING WITH LLMS\n\nThe example in Fig. 1 falls into a general THINKSUM paradigm which extends the reasoning capabilities of a given model by explicitly associating certain text spans with sets of other strings, which may serve as alternatives or elaborations/explanations and can be either defined by the user or inferred by the LLM itself. These associations then provide multiple texts to be evaluated, again by the LLM itself. The collection of resulting probabilities provides an opportunity to summarize the text using standard probabilistic inference techniques, which usually include a summation.\n\n2.1 HOW TO THINK\n\nHere we list examples of the “fast thinking” that precedes the summarization stage.\n\nElementary string manipulations. Standard ways to turn a question into a prompt that can be given to a LLM for generation or scoring involve choices (e.g., of the prompt format) that can be seen as being made by a controlling agent. The standard approach to multiple-choice questions is to write them as Cloze tasks. However, there are nontrivial operations used in inference procedures that sometimes work better, such as:\n\n• Order inversion: Exchanging the order of the question and answers, as in Min et al. (2022). • Premise erasure: Deleting a part of the question. Removing a premise with which the answer is expected to have high mutual information is a step in inference procedures that aim to correct for bias towards answers with high unconditional likelihood (Zhao et al., 2021; Holtzman et al., 2021; Malkin et al., 2022).\n\nSubstitution and normalization. An example is shown in Fig. 1. Elements from a set may be substituted in place of ‘slot’ words in a prompt, such as ‘cat’ substituted for ‘binne’ in the prompt “A binne bam is a place for”. This operation can be combined with syntax-normalization steps that are reliably achieved by standard NLP tools, such as ensuring subject-verb agreement.\n\nExample and list generation. A LLM can be prompted to generate or score lists of words or phrases. We suggest and experiment with three instances of this:\n\n• Example generation: In Fig. 1, the LLM is prompted to turn a definition or characterizing property, such as ‘simple dwelling’, into a list of examples. This can be achieved with a prompt such as “A bam is a simple dwelling. 1.”. The generated completion can be parsed into a set to be used later in the inference procedure.\n\nExamples:\n\n• List extension: A similar approach can also be used to hallucinate additional possible answers to\n\nquestions, as we will show in some of the experiments.\n\n• List of words: Similar prompts provide an even simpler THINK method that we use for scoring A,\n\n– but not generation – in several tasks. Just prompting a LLM with “List of words:\n\n3\n\nB”, where A and B are words or phrases, and computing the likelihood of B conditioned on “List of words:\n\nA,” is a good measure of semantic relatedness of A and B.\n\nFact generation. This way of THINKing associates an input word with a set of phrases, in a similar manner to generating examples from a definition. It can be achieved with prompts such as “List facts about cats. 1.” The generated facts are good targets for substitutions of other concepts (‘dogs’, ‘galaxies’) in place of the concept (‘cats’) about which facts are generated. A variation on this asks the LLM to generate differences between two concepts, as shown in Fig. 2 (right).\n\nTranslation. The LLM can be prompted to convert between different forms of representing the same concept as a sequence of tokens. We use two basic examples of this in experiments:\n\n• Translation between languages by prompting the LLM in formats such as “French: J’adore les chats noirs. English:”. A very similar approach can be used to convert non-alphabetic symbols, such as emoji, into words with a similar meaning.\n\n• Converting text to formal (symbolic) structures, like turning a word problem into a collection of\n\nmathematical equations.\n\n2.2 HOW TO SUM\n\nElementary inference. As above, we begin by listing existing standard ways of turning LLM outputs into answers, which we see as trivial cases of aggregation (SUM).\n\n• Posterior computation by normalizing probabilities over a set. • Majority/minority vote (argmin/argmax): a component of most answer selection procedures. • Thresholding: Used when an answer depends on the value of a probability (or a difference or ratio of probabilities). This can be used when a discrete answer needs to be produced from a real-valued output likelihoods.\n\n• Ratio of likelihoods: Likelihoods from different variants of the same prompt can be combined by considering their ratio or more general log-linear or other mixture. For example, this can be done to correct the likelihood of an answer conditioned on a question by its unconditional likelihood, in combination with the Premise erasure operation described above.\n\nMixture (average) aggregation. A collection of prompts can be treated as the components of a mixture model over completions. An example is shown in Fig. 1, where substitutions of a set of words yield 25 different prompts. Likelihoods of the completion over these 25 prompts are averaged.\n\nProduct aggregation. We use products of likelihoods in two different ways:\n\n• In a similar way as mixtures, but when the more natural probabilistic model has all elements of a set (of prompts) generating the answer, such as when a description or definition must be satisfied by all concepts in a set.\n\n• In a task where we are to determine whether a statement S or its negation S′ is true, we can compute the likelihood of both S and S′ being true (as posterior over the tokens ‘True’ and ‘False’ in an appropriate prompt), then compare p(True|S) p(False|S′) (S is true and S′ is false) with p(False|S) p(True|S′) (S is false and S′ is true).\n\nFitting latent variable models. See §C for an example of fitting a discrete latent variable (clustering) model over the likelihoods produced by a LLM.\n\n3 EXPERIMENTS\n\nWe compare THINKSUM with the published results for n-shot GPT-3 175B (davinci) in BIG-bench, where n ∈ {0, 1, 2, 3}. Our main results are shown in Table 1. Below, we describe THINKSUM for each task. Detailed descriptions are in §B, and examples of each task appear in Table D.1.\n\n3.1 SEMANTIC RELATEDNESS\n\n3.1.1\n\nINTRODUCTORY EXAMPLES: PHRASE RELATEDNESS AND CODENAMES\n\nPhrase relatedness. Each question in the multiple-choice PHRASE RELATEDNESS task requires to determine which of a given set of words or phrases {wi } is related to a query phrase q. We query the LLM for the likelihood of q following a List of words prompt to form a vector of likelihoods: pi = pLLM (q | “List of words: wi, ”). The answer selected is the one with highest likelihood, arg maxi pi (a trivial SUM operation). We note that this is also an instance of Order inversion: the query is scored following a prompt in which each of the candidate answers is substituted.\n\n4\n\nTable 1: Standard metric (BLEU for CODENAMES, accuracy for other tasks) for GPT-3 175B (davinci) and THINKSUM with 175B (davinci), InstructGPT and GPT-2 XL on BIG-bench tasks.\n\nGPT-3 (davinci) n-shot\n\nTHINKSUM\n\nTask\n\nAvg. H n = 0\n\nPHRASE RELATEDNESS (§3.1.1) CODENAMES (§3.1.1) ODD ONE OUT (§3.1.2) NOVEL CONCEPTS (§3.2.1) INVENTED WORDS (§3.2.2) SPORTS UNDERSTANDING (§3.3.1) KNOWN UNKNOWNS (§3.3.2) MISCONCEPTIONS RUSSIAN (§3.4.1) EMOJI MOVIE (§3.4.1) FIVE OBJECTS (§3.4.2)\n\n0.74 0.18 0.80 0.67 N/A 0.71 0.80 0.65 0.93 N/A\n\n0.37 0.01 0.27 0.47 0.29 0.50 0.61 0.33 0.12 0.23\n\n1\n\n0.42 0.11 0.20 0.47 0.14 0.50 0.52 0.33 0.18 0.29\n\n2\n\n0.52 0.16 0.23 0.56 0.14 0.50 0.48 0.41 0.12 0.28\n\n3\n\nGPT-3\n\nInstructGPT GPT-2 XL\n\n0.59 0.19 0.23 0.56 0.21 0.50 0.50 0.35 0.19 0.32\n\n0.85 0.37 0.80 0.72 0.64 0.71 0.54 0.70 0.80 –\n\n0.87 0.41 0.84 0.75 0.71 0.74 0.76 0.61 0.75 0.77\n\n0.79 0.36 0.71 0.50 0.29 0.54 –\n– –\n–\n\nFigure 2: ODD ONE OUT. Left: Performance of GPT-3 (n-shot, n = 0, 1, 2, 3), auxiliary knowledge, and THINKSUM with various model sizes. Middle: Auxiliary knowledge vs. THINKSUM with varying number of differences. Right: Prompt used to generate knowledge statements.\n\nCodenames. Each question in CODENAMES requires selecting the k words from a set {wi } that are most closely related to a query word q. We form a vector pi in the same way as for PHRASE RELATEDNESS, then select the top k entries in pi to produce the output.1\n\n3.1.2 ODD ONE OUT\n\nWe examine possible THINK and SUM approaches in greater depth on the ODD ONE OUT task, in which the word in a set W = {wi } that is least semantically related to the others must be chosen. List of words. We first consider an approach similar to that used in §3.1.1. We form a matrix Pi j using a List of words THINK prompt for each pair of indices i, j:\n\nPi j = pLLM (w j | “List of words: wi, ”). This matrix is aggregated by averaging over j (in log domain) and selecting the i with lowest average, i.e., least likelihood of being generated by a product mixture of all words in the set: i = arg mini (cid:206) j Pi j . This is a case of the Product aggregation operation.\n\nBecause this approach is the most successful with all model sizes we experimented with, its performance is reported in Table 1. Remarkably, near-average-human accuracy is maintained for all model sizes from GPT-2 Small to the largest GPT-3 model (Fig. 2 (left)).\n\nFact generation. As an alternative approach, we use a Fact generation prompt. An effective way to mine facts for semantic relatedness tasks is to consider two items in the same context in order to get relevant facts regarding how items are related to each other (prompt in Fig. 2 (right)). The demonstration used in the prompt ensures that the LLM generates statements in an expected format, which can be parsed and used for probability computation later. Using this prompt, we obtain a collection of statements S = {si } about items w j . We treat each generated si as a template into\n\n1Because the task is evaluated by BLEU score against the reference answers listed in alphabetical order, we perform the additional step of converting the top indices to the answer in the right format. Alphabetization of short lists is trivial in code, but can also very reliably be done by prompting GPT-3.\n\n5\n\nSMLXLAdaBabbageCurieda Vinci020406080100accuracy (%)text-davinci-002GPT-2GPT-30-shot1-shot2-shot3-shotAux. knowledgeThinkSumAverage humanwhich different words w can be substituted and denote by si ⟨w⟩ the Substitution of word w into template si. We then form a |S| × |W | matrix Pi j , defined by Pi j = pLLM (si ⟨w j ⟩). Then, we can perform Minority voting: we take argmin over j and pick as the answer the most frequently occurring value, i.e., the item that is most often the least likely to fit a generated statement.\n\nComparison with auxiliary knowledge approaches. We compare our method with a knowledgebased prompting method, herein referred to as auxiliary knowledge. In auxiliary knowledge, we prepend generated facts in the prompt before the question. Details of the prompt for auxiliary knowledge are provided in §D.2. In Figure 2 (middle), we show that the accuracy of Fact generation-based THINKSUM rises as the number of generated facts is increased, while the auxiliary knowledge technique peaks and then degrades as the prompt lengthens.\n\nFig. 2 (left) shows how performance varies with the size of the LLM used for GPT-3, auxiliary knowledge and THINKSUM on ODD ONE OUT. Even with GPT-2 Small, THINKSUM dramatically improves over much larger largest zero- or few-shot models with or without auxiliary knowledge. The latest iteration of the largest GPT-3 model, text-davinci-002, is the only model variant that, with the help of auxiliary knowledge, achieves competitive performance with THINKSUM. This result provides experimental evidence for our claim that while new models may create qualitative jumps, THINKSUM can push the performance limits of smaller model variants through slow thinking. Additional experiments on auxiliary knowledge are provided in §C.\n\n3.2 SUBSTITUTION AND AGGREGATION\n\n3.2.1 NOVEL CONCEPTS\n\nIn the multiple-choice NOVEL CONCEPTS task, a set of words or phrases W = {wi } and a set of statements S = {s j } with third-person plural pronoun subjects (‘They all...’) are given, and the statement which is true for all items in W must be determined. We treat each statement s j as a template, into which words w can be substituted by replacing ‘They all’ with w. Denoting by s j ⟨w⟩ the substitution of w into s j , we form a |W | × |S| matrix Pi j by scoring the Substitution of each word into each statement and considering the Ratio of likelihoods with the template without substitution: Pi j = .We then perform Product aggregation to select the statement which is most likely to be generated by all words in the set. To be precise, the selected statement is arg max j (cid:206)i Pi j .\n\npLLM (s j ⟨wi ⟩) pLLM (s j )\n\nINVENTED WORDS\n\n3.2.2 In INVENTED WORDS, two nonce words x1, x2 are defined and the correct statement must be chosen out of a set of statements S = {s j } that begin with (possibly inflected forms of) “x1 x2” (Fig. 1). We use an Example generation prompt to obtain a set of example words fitting the definitions of x1 and x2. We thus obtain sets S1 and S2 of words that can be substituted for x1 and x2, respectively. We treat each statement s j as a template into which words w1 ∈ S1 and w2 ∈ S2 can be substituted by replacing xi with wi and normalizing the syntax to ensure subject-verb agreement. Denoting by s j ⟨w1, w2⟩ such a substitution, we form a vector of probabilities p j by scoring the Substitution of each possible pair of words into each statement and performing Mixture aggregation and considering the Ratio of likelihoods with the template without substitution:\n\np j =\n\n1 |S1 | |S2 |\n\n(cid:205)w1 ∈S1,w2 ∈S2\n\npLLM (s j ⟨w1, w2⟩)\n\npLLM (s j )\n\n.\n\nThe statement s j with highest likelihood under this normalized mixture, arg max j p j , is selected.\n\n3.3 UNCERTAINTY AND HALLUCINATION DETECTION\n\nLLMs are prone to generating hallucinations that contain incorrect statements. The likelihoods of these statements are often dominated by short plausible patterns, which also makes it difficult for LLMs to evaluate their own uncertainty about a fact. Thus, detection (Liu et al., 2021; Zhou et al., 2021) and reduction of such hallucinations is crucial for widespread use of LLMs in real applications. (Dziri et al., 2021; Shuster et al., 2021).\n\n3.3.1 SPORTS UNDERSTANDING\n\nQuestions in SPORTS UNDERSTANDING ask to determine whether it is ‘plausible’ or ‘implausible’ that a professional sports player x (e.g., ‘Draymond Green’, a basketball player) performed an action\n\n6\n\na associated with a sport (e.g., ‘threw a touchdown’, an action in American football). It is implied that the combination of x and a is plausible if the sport with which player x is associated coincides with the sport in which action a is performed. We consider an approach that does not rely on identifying the latent variable (sport) as an intermediate step and is thus more generalizable to other domains.\n\nWe use an Example generation THINK prompt to produce a set S of players who perform action a, then do Posterior computation by normalizing the likelihood assigned by the LLM to each player in S, as well as x, performing action a:\n\n∀y ∈ S ∪ {x}\n\np(y|a) =\n\npLLM (“y a”) (cid:205)y′ ∈S∪{x } pLLM (“y′ a”)\n\nThe statement is considered to be implausible if the posterior on x is sufficiently low (Thresholding) – see Fig. 3.\n\n3.3.2 KNOWN UNKNOWNS\n\nQuestions in the KNOWN UNKNOWNS task ask to determine whether the answer to a question is a certain precise concept or ‘unknown’. Given a question q (e.g., “What was the temperature in Cuzco on the day of the Emperor Vespasian’s birth”) and the candidate precise answer a (e.g., 25◦C), we use a List extension prompt to generate a set S of other possible answers to q. We then do a Posterior computation over S and the original answer a, similar to that used for SPORTS UNDERSTANDING:\n\n∀y ∈ S ∪ {a}\n\np(y|q) =\n\npLLM (“q? y”) (cid:205)y′ ∈S∪{a } pLLM (“q?\n\n.\n\ny′”)\n\nThe answer a is chosen if the posterior on a is sufficiently high (Thresholding), and otherwise ‘unknown’ is chosen.\n\n3.4 TRANSLATION\n\nFigure 3: Example posterior probabilities generated from textdavinci-002 for SPORTS UNDERSTANDING with the description “threw a touchdown”. The basketball player given in the question Draymond Green has a much lower posterior probability than the from generated football players, which we conclude the sentence “Draymond Green threw a touchdown.” is implausible.\n\n3.4.1 TRANSLATING BETWEEN LANGUAGES AND WRITING SYSTEMS\n\nRussian misconceptions. chosen out of a pair of Russian sentences: a statement s and its negation t.\n\nIn the MISCONCEPTIONS RUSSIAN task, the true statement must be\n\nWe first describe an approach that does not use translation and already performs better than random guessing – and better than baseline methods that simply select the more likely of the two statements – using the largest GPT-3 model, which has sufficient knowledge of Russian. We compute the posterior over the two hypotheses “s is true, t is false” and “s is false, t is true”:\n\npLLM (“T” | “T or F? s. pLLM (“F” | “T or F? s.\n\nAnswer: ”) pLLM (“F” | “T or F? t. Answer: ”) pLLM (“T” | “T or F? t.\n\nAnswer: ”), Answer: ”).\n\nwhere T denotes True and F False in the actual prompt. This is a kind of Product aggregation. If the posterior on the first option is higher, s is chosen as the true statement; otherwise, t is chosen. This approach can be combined with a Translation prompt that produces translations of s and t into English, then uses these translations in place of s and t in the above computations. The approach can be further extended by sampling a set of translations and performing Mixture aggregation over the translations. Our reported result uses 10 generated translation for each statement, but it is only 2% higher than the result using one generated translation.\n\nEmoji movie. The multiple-choice EMOJI MOVIE task requires selecting the name of a movie from a list {mi } that is best described by a sequence of emoji symbols s = (s1 . . . sn). An Order inversion prompt performs best on this task using the Davinci variant of GPT-3: choosing the answer\n\narg max i\n\npLLM (s | “Emoji describing the movie mi”).\n\nWe also attempt to use a Translation prompt to obtain a single-word English description w j of each emoji s j in s, then score using\n\narg max i\n\npLLM (w1 . . . wn | “Words describing the movie mi”).\n\n7\n\nFigure 4: Details for LOGICAL DEDUCTION. (a) Example question from the task, (b) demonstration for the THINK prompt, (c) example LLM output.\n\nThis approach performs slightly better than Order inversion alone using InstructGPT. However, it does not work with the base GPT-3 models, which do not as reliably translate emoji to English.\n\n3.4.2 LOGICAL DEDUCTION\n\nIn the LOGICAL DEDUCTION task, different types of items and clues regarding their placement are provided, as shown in Fig. 4(a). The goal is to select the correct statement from a set of statements about their placements. The task creators emphasize that this requires parsing information about multiple objects and their relationships, understanding rules regarding ordered objects in various scenarios, and iteratively applying of these rules. The LLM calls in the THINK stage of THINKSUM can perform mappings required to parse information and understand rules, and the SUM stage can integrate mappings of objects to the placements under these rules. Here, we can use a Translation prompt to map the given problem into a set of mathematical (in)equalities (Fig. 4(c)).\n\nA Translation prompt as in Fig. 4(b) containing generic ordering statements and object names that are not used in the task is sufficient to perform the translation from natural language to mathematical equations, as shown in Fig. 4. To solve the problem, we replace the problem statement with its translation T consisting of (in)equalities, and map each of the objects to the set of strings corresponding to numbers from 1 to N, where N is the number of objects. This is accomplished by appending a given problem to the fixed prompt in Fig. 4(b), which acts as a demonstration for one-shot in-context learning. The ordering problems involve a variety of types of objects (cars, birds, etc.) and types of orderings (by size, price, contest ranking, etc.). For a particular problem in Fig. 4(a), we show the returned text from InstructGPT in Fig. 4(c), showing that the demonstration used in the THINK prompt generalizes from objects ordered by size to books ordered by position.\n\nOnce a translation of the problem into a set of inequalities is obtained, the SUM stage considers all possible mappings of items to indices to determine the mapping compatible with the discovered set of (in)equalities. This can be done by an external algorithm or with the LLM itself, as an LLM may be capable of understanding that, for example, “2>3” is a less likely string than “2>1” (see Fig. B.1).\n\nThe probability of a target statement like “yellow book=2” can thus be obtained by:\n\np(“yellow book=2” | T) ∝\n\n∑︁\n\nb∈ {1,..., N } N\n\npLLM ({Tt ⟨b⟩ : Tt ∈ T } ∪ {“yellow book=2”⟨b⟩})\n\n(1)\n\nwhere b denotes the vector of placements for the N items, T = {Tt } N t=1 is obtained from the Translation prompt as a set of strings, and s⟨b⟩ denotes the substitution of the corresponding entry in b in place of the object name in the string s. The term inside the sum is a case of Product aggregation: the LLM likelihoods of all strings in the set are multiplied together.\n\nIn summary, our solution to this task involves composition of two THINK operations – a Translation into a set of equations and then Substitution of numbers in place of item names – and two\n\n8\n\nSUM operations – a Product aggregation followed by a Mixture aggregation. (Other options are discussed in §C.) Results and discussion. For the 500 LOGICAL DEDUCTION problems with N = 5 objects, THINKSUM yields an accuracy rate of 77% (see Table 1), besting the average human performance. When the necessary summations become large, it becomes very unlikely that pure prompt engineering can be competitive, as even humans need paper and pencil to create and attend to many alternative solutions, and would likely translate the premises into a simpler notation using a single letter (representing a variable to which a numeric value can be assigned) to represent each object, rather than directly attending to facts in the problem statement.\n\nWe also tested an auxiliary knowledge method akin to chain-of-thought reasoning, where the information obtained with the prompt in Fig. 4 is appended to the LLM input. In particular, the problem, together with its translation into inequalities, is used as a prompt to each of the answer options, and then the option with the highest likelihood is chosen for the answer. This approach does improve over straightforward zero-shot GPT-3 scoring, but only raises the accuracy to 50% (see Table B.1).\n\n4 RELATED WORK\n\nImprovements to LLM inference. After the discovery of the in-context learning abilities of LLMs, there has been an explosion of interest in improving inference with LLMs in the zero-shot and few-shot setting Brown et al. (2020); Chowdhery et al. (2022); Rae et al. (2021). One approach to improving the reasoning abilities of LLMs involves appending, or learning to generate, auxiliary knowledge within the prompt (Shwartz et al., 2020; Zelikman et al., 2022; Nye et al., 2021a). Recently, more general auxiliary knowledge or chain-of-thought prompting methods have been proposed (Wei et al., 2022; Wang et al., 2022b; Zhou et al., 2022; Creswell et al., 2022; Wang et al., 2022a; Liu et al., 2022b). Later, Kojima et al. (2022) showed zero-shot chain-of-thought prompting can improve performance on a variety of reasoning tasks. This method does not require any handcrafted few-shot examples, which is a shared property with THINKSUM. (Nye et al., 2021b) observed that a dual-system approach where an associative “System 1” and a logical “System 2” can increase coherence of LLMs in tasks such as robust story generation and grounded instruction following. The two-step paradigm in THINKSUM is similar, where “System 1” is the (querying of the LLM for) fast thinking, and “System 2” is the probabilistic inference step.\n\nBrittleness of chain-of-thought prompting. Despite the recent success of chain-of-thought approaches, recent studies have raised concerns regarding the limitations of chain-of-thought approaches. Webson & Pavlick (2022) observed that instructive prompts perform similarly with misleading or intentionally irrelevant prompts. Additionally, Ye & Durrett (2022) showed improvements due to few-shot chain-of-thought are not observed in question answering, or natural language inference. More critically, few-shot prompts are highly sensitive to the order in which the samples are provided, the prompt format, and the selection of in-context examples, (Lu et al., 2022; Zhao et al., 2021). Thus, it is crucial to design techniques that are robust to such changes in the prompt.\n\nProbabilistic inference. More recent approaches have proposed probabilistic inference approaches for tackling true/false question answering and commonsense question answering (Jung et al., 2022; Liu et al., 2022a). Xie et al. (2021) presents a Bayesian inference perspective on incontext learning, and Dohan et al. (2022) formalizes and unifies existing prompting techniques in a probabilistic framework. Our work generalizes such approaches to perform arbitrary probabilistic inference outside of the LLM.\n\n5 CONCLUSION\n\nIn this paper we presented THINKSUM, a two-step probabilistic inference paradigm that reasons over sets in a structured manner. The fast thinking stage of THINKSUM allows elementary string manipulations as well as natural language prompting, which may enable numerous approaches to solve a natural language task. Even with far smaller model variants, THINKSUM achieves state-ofthe-art results on ten difficult tasks in BIG-bench using GPT-family models. The two-step paradigm allows operating over sets instead of manipulating the prompt itself, preventing sensitivity to prompt format during the probabilistic inference in THINKSUM, which is performed outside of calls to the LLM. As a result, THINKSUM is more robust to prompt design, yields more interpretable predictions, and can be combined with many probabilistic inference approaches to tackle a diverse set of tasks.\n\n9\n\nREFERENCES\n\nYoshua Bengio. The consciousness prior. arXiv preprint arXiv:1709.08568, 2017.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS), 2020.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large lan-\n\nguage models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.\n\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the\n\nEM algorithm. Journal of the Royal Statistical Society B, 39(1):1–38, 1977.\n\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.\n\nNouha Dziri, Andrea Madotto, Osmar Za ̈ıane, and Avishek Joey Bose. Neural path hunter: Reducing hallucination in dialogue systems via path grounding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 2197–2214, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.168. URL https://aclanthology.org/2021. emnlp-main.168.\n\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of human cognition. arXiv\n\npreprint arXiv:2011.15091, 2020.\n\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form comIn Proceedings of the 2021 petition: Why the highest probability answer isn’t always right. Conference on Empirical Methods in Natural Language Processing, pp. 7038–7051, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.564. URL https://aclanthology.org/2021. emnlp-main.564.\n\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint arXiv:2205.11822, 2022.\n\nDaniel Kahneman. Thinking, fast and slow. Macmillan, 2011.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\n\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3154–3169, Dublin, Ireland, May 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.225. URL https://aclanthology.org/2022. acl-long.225.\n\nTianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and Bill Dolan. A token-level reference-free hallucination detection benchmark for free-form text generation. arXiv preprint arXiv:2104.08704, 2021.\n\n10\n\nZihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai Prabhumoye, Wei Ping, Mohammad Shoeybi, and Bryan Catanzaro. Multi-stage prompting for knowledgeable dialogue generation. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 1317–1337, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.104. URL https://aclanthology.org/2022.findings-acl.104.\n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086–8098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022. acl-long.556.\n\nNikolay Malkin, Zhen Wang, and Nebojsa Jojic. Coherence boosting: When your pretrained language model is not paying enough attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8214–8236, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.565. URL https://aclanthology.org/2022.acl-long.565.\n\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5316–5330, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 365. URL https://aclanthology.org/2022.acl-long.365.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021a.\n\nMaxwell Nye, Michael Tessler, Josh Tenenbaum, and Brenden M Lake. Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Neural Information Processing Systems (NeurIPS), 2021b.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Neural Information Processing Systems (NeurIPS), 2019.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training Gopher. arXiv preprint arXiv:2112.11446, 2021.\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentaIn Findings of the Association for Computational tion reduces hallucination in conversation. Linguistics: EMNLP 2021, pp. 3784–3803, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.320. URL https://aclanthology.org/2021.findings-emnlp.320.\n\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4615–4629, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.373. URL https://aclanthology.org/2020.emnlp-main.373.\n\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n\n11\n\nAmos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Science, 185(4157):1124–1131, 1974.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-\n\naugmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022a.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.\n\nAlbert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of In Proceedings of the 2022 Conference of the North American Chapter of the their prompts? Association for Computational Linguistics: Human Language Technologies, pp. 2300–2344, Seattle, United States, July 2022. Association for Computational Linguistics. URL https: //aclanthology.org/2022.naacl-main.167.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context\n\nlearning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\n\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot in-context learning. arXiv\n\npreprint arXiv:2205.03401, 2022.\n\nEric Zelikman, Yuhuai Wu, and Noah D Goodman. STaR: Bootstrapping reasoning with reasoning.\n\narXiv preprint arXiv:2203.14465, 2022.\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. International Conference on Machine Learning (ICML), 2021.\n\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzm ́an, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 1393– 1404, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-acl.120. URL https://aclanthology.org/2021.findings-acl.120.\n\nDenny Zhou, Nathanael Sch ̈arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n\nA BIG-BENCH LITE\n\nFigure A.1 shows the performance margin between an average human and zero-shot GPT-3 on tasks in BIG-bench Lite, a select subset of tasks chosen by the authors of the benchmark to showcase the most important aspects of LLMs that need improvement. The vertical black bar separates the dataset into tasks where GPT-3 is already within the margin of just 10% compared to the average human accuracy, and the harder tasks (on the left). We show in the main text that some of these harder tasks, in particular EMOJI MOVIE, CONCEPTUAL COMBINATIONS,KNOWN UNKNOWNS, NOVEL CONCEPTS, MISCONCEPTIONS RUSSIAN and LOGICAL DEDUCTION, the margins are shrunk considerably, often exceeding average human performance. Other tasks in BIG-bench lite such as LOGIC GRID PUZZLE and SYMBOL INTERPRETATION share a similar structure to the addressed by THINKSUM, and thus could be investigated as part of future work. Another example where THINKSUM can be applied is the CODE LINE DESCRIPTION task, where we observe in our preliminary experiments that a simple order inversion can significantly outperform average human accuracy.\n\n12\n\nFigure A.1: Margin between 0-shot GPT-3 and average human performance for BIG-bench Lite tasks. Using THINKSUM, we address many of the tasks that have greater than 10% performance margin with average human, and significantly reduce and often overturn the margin.\n\nB ADDITIONAL TASK DESCRIPTIONS\n\nB.1 SEMANTIC RELATEDNESS\n\n\\\n\nPhrase relatedness. In this task, an input phrase, and four target phrases are presented to the language model. The language model is asked to identify the most related choice from the listed target options. An example from the task is “For each word or phrase, identify the most related choice from the listed options. n\nOption: native city\n\n\\ n Option: home run”\n\nn Option: town center\n\nn Input: home town\n\nn Option: location\n\n\\\n\n\\\n\n\\\n\nCodenames. In this task, the language model is asked to identify words associated with a given word. An example from the task is “Try to identify the 2 words best associated with the word WHITE from the following list: n book, anchor, rainbow, shoulder, tunnel, sack, drum, pacific, page, mark, gear, glacier. Give your answer in alphabetical order.”\n\n\\\n\nOdd one out. This task is aimed at evaluating the capability of LLMs in semantic relatedness. This task presents the model with four to six words, where all words except one word are semantically or grammatically related to each other. The goal for the language model is to identify the odd word. An example question from the task is “Pick the odd word out: glass, head, arm, leg, hand, foot”.\n\nB.2 HALLUCINATION DETECTION\n\nKnown unknowns. In this task, the aim is to measure the ability of LLMs to identify whether the answer to a question is known, or unknown. If the answer to the question cannot be known, an imaginary answer is generated, and the model is asked to choose between the imaginary answer and the word “Unknown”. If the question has a correct answer, the correct answer and “Unknown” are provided as options. An example question from this task is “How often did Abraham Lincoln cut his toenails?”, and the options are “Every Saturday night”,“Unknown”. Similar to the sports understanding task, we can use an LLM to generate other plausible answers with the following prompt: How often did Abraham Lincoln cut his toenails? A possible answer is: n 1. Every Saturday night. List 4 other possible answers in the same format as the first: n 2. Then, the answer given in the question is predicted to be “known” if its posterior is higher by the second most likely option by some margin. In our experiments, we chose this value to be 1/Ne where Ne is the number of examples, including the original option.\n\n\\\n\n\\\n\nSports understanding. In this task, artificial sentences are constructed using the name of professional sports players and actions from particular sports. The model is then asked to identify whether\n\n13\n\nTable B.1: THINKSUM vs. auxiliary knowledge.\n\nODD ONE OUT\n\nPHRASE RELATEDNESS\n\nLOGICAL DEDUCTION (N = 5)\n\nTHINKSUM Auxiliary knowledge\n\n0.84 0.71\n\n0.87 0.75\n\n0.77 0.50\n\nFigure B.1: Probabilities of different (in)equalities according to GPT-3 text-davinci-002 (logit).\n\nthe sentence is plausible, where a sentence is considered plausible if the sport of the player matches the sport of the action described in the sentence. An example from the task is “Statement: Draymond Green threw a touchdown. Plausible/implausible?”\n\nFor THINKSUM to be able to parse outputs, GPT-3 generations need to be in a pre-determined format. While larger models can obey a format without demonstrations, for smaller models it is helpful to demonstrate the format with an example. Thus, we use the following prompt: “List 4 examples of players who scored a rabona goal. n\n4. Angel Di Maria\n\nn 1. Cristiano Ronaldo n List 4 examples of players who threw a touchdown.\n\nn 3. Mario Balotelli\n\nn 2. Erik Lamela\n\n\\ n1.”.\n\n\\\n\n\\\n\n\\\n\n\\\n\n\\\n\nB.3 CONCEPT UNDERSTANDING\n\nIn the following tasks, the shared goal is to test the ability of LLMs on concepts over entities that have likely not been observed during training.\n\nConceptual combinations: Invented words. In this task, the LLM is provided with two invented words, and their definitions in the input. The LLM is then asked to infer the most plausible meaning resulting from the combination of the invented words. As the words are invented, they are not present in the training set, and the LLM needs to understand and combine the definitions of the invented words to reason about the meaning of the combination. An example is: “The word ’binne’ means any animal that is furry and has four legs, and the word ’bam’ means a simple sort of dwelling. Question: Which of the following sentences best characterizes binne bams?”. Similar to SPORTS UNDERSTANDING, we can use the following prompt to force the LLM to obey a fixed format: “List synonyms of binne, separate synonyms by comma:”\n\nNovel concepts. In this task, the LLM is presented with two to four disparate entities that typically would not co-occur frequently, but share an underlying conceptual or linguistic concept. The aim is to test the ability of the LLM to reason about entities that are unlikely to have been observed in the same context during training. In a multiple-choice setting, the LLM is given concepts relating to the entities, and is asked to generate the intended concepts against carefully chosen tempting distractors. The choices are not presented in the prompt. An example question from the task is as follows: “What do the following have in common? 1) bumble bees 2) 01010101 3) race cars”, and the answer options are They all make noise, “They all are yellow, They all are binary, They all go fast, They all have stripes”.\n\nB.4 TRANSLATION\n\nEmoji Movie. In this task, the LLM is presented with emojis that describe the plot of a movie, and the model is asked to guess the correct movie from the given options. The task evaluates the LLMs understanding of emojis, and its knowledge of popular movies. An example from the task is\n\n14\n\n123456789123456789i<j123456789123456789i=j123456789123456789i>jTable C.1: Different alternatives of probabilistic reasoning with THINKSUM for solving ODD ONE OUT.\n\nModel\n\nList of words Latent variable model Minority voting\n\nTHINKSUM davinci-002 THINKSUM davinci-001\n\n0.84 0.74\n\n0.67 0.77\n\n0.70 0.70\n\n“What movie does this emoji describe?\n\nlying-face-emoji\n\nn, pinocchio\n\nn inception\n\nn lilo & stitch\n\n{\n\n} \\\n\nn heat\n\n\\\n\n\\\n\nn the big sleep,\n\n\\ \\\nMisconceptions Russian. In this task, the LLM is presented with sentences in Russian that contain misconceptions, and their true counterparts. The task evaluates if the LLM believes common misconceptions. One example of a target pair from the task, translated to English is as follows: “Vaccines do not cause autism.\n\nn Vaccines cause autism.”\n\n\\\n\nC ADDITIONAL EXPERIMENTS\n\nTHINKSUM vs. auxiliary knowledge. Table B.1 shows the comparison between auxiliary knowledge and THINKSUM. All methods used text-davinci-002. For PHRASE RELATEDNESS, auxiliary knowledge was generated using the “list differences” prompt shown in Figure 2 (right). For both auxiliary knowledge and THINKSUM, 6 number of differences were used, as that was the best for auxiliary knowledge, shown in Figure 2 (middle). THINKSUM ODD ONE OUT and PHRASE RELATEDNESS are solved with the list of words prompt. For LOGICAL DEDUCTION, the THINK prompt shown in Fig4 was included before the question in the prompt. In all cases, THINKSUM outperforms auxiliary knowledge.\n\nLatent variable models for ODD ONE OUT. While the detection of the odd item can be performed with simple operations, it is also possible to assume that there is a latent structure consisting of two or more clusters such that the facts and items belonging to a cluster can be freely interchanged keeping probability of such combination high. While there are simpler alternative solutions to this task, the latent variable model enables selecting the facts that characterize the majority class, explaining why the minority item is ruled as the odd one out. Thus, expanding on the problem and applying a latent variable model can help interpret the decisions of the system. More formally, since I includes items that are semantically related, and the odd, we can model i ∈ I and f ∈ F to be generated from a latent class c ∈ {0, 1}. Then, the conditional distribution can be modeled as:\n\nP(i, f |c) = P(i|c)P( f |c) P(i, f ) =\n\nP(i, f |c)\n\n∑︁\n\nc\n\nThe semantic components, groupings P(i|c) and P( f |c) can be computed from the matrix using expectation-maximization (EM; Dempster et al., 1977). Then, the score for an item i belonging to a cluster and all other items m ∈ S, {m ≠ i} belonging to another cluster can be found as Si = (cid:205)c,c′≠c P(i|c)P(c) (cid:206)m≠i P(m|c′)P(c′). We show the effectiveness of the latent variable models in Table C.1, where we analyze different methods for solving ODD ONE OUT using the InstructGPT variants text-davinci-001 and textdavinci-002. For the “latent variable model” and “minority voting” methods, we use number of differences Nd = 5. The latent variable model is run for 200 iterations. All probabilistic reasoning methods perform well, outperforming previous baselines reported in Table 1. Each of these approaches can be applicable in other tasks of similar structure to ODD ONE OUT.\n\nUsing GPT-3 or external algorithms to evaluate inequalities in LOGICAL DEDUCTION. Fig. B.1 shows the matrix of posterior probabilities evaluated using InstructGPT (text-davinci-002) for strings of form “x=y”, “x<y”, “x>y” for x, y ∈ {1, .., 9}. The probabilities are computed using prompts of the form “True or false: x<y? The answer is:” where x and y are substituted with numbers from {1, .., 9}, and normalizing the probability of the first token over the two options “true” and “false”. These are the probabilities that statements Tt would evaluate in (1).\n\nGiven the capability of InstructGPT to both translate given logical deduction problems into (in)equalities (Fig. 4) and to evaluate each of them after replacement of items with placement\n\n15\n\nnumbers (Fig. B.1), we conclude that it is perfectly within its capabilities to parse and understand the problem, and the SUM stage is there simply to go over all possible mappings, the way a human might. But, just as a human might use shortcuts in the search, the SUM stage of THINKSUM could be implemented in more or less efficient ways. For example, instead of summing over all possible assignments of the five items, we can avoid the ones that are not permutations of {1, 2, 3, 4, 5}. Furthermore, instead of using pLLM from Fig. B.1, we can simply evaluate each inequality externally, giving a high constant probability for each statement in T and the target statement where a configuration of item placements makes the statement correct and low constant probability whenever the statement is incorrect. Whichever evaluation mechanism we use, the summing can be aborted whenever an incorrect statement is detected (typically there are four (in)equalities that describe the problem, and one equality statement that describe the option to be evaluated; we want to find a configuration of item placements such that all of these are correct, or have high probability under pLLM.\n\nThe prompt in Fig. 4(b) instructs the LLM to assign positive or negative ordinal numbers depending on the language used (the smallest object gets placement 1, while the second largest one gets −2, meaning ‘second from the end’). A negative order number r is then turned in our code into N + r + 1 before evaluating statements. Sometimes, LLM does not follow this instruction, but simply labels the largest, right-most, most expensive, etc. as N, instead of −1, which is equivalent. One possible failure mode of this kind of THINKSUM is that the LLM may translate inequality statements inconsistently with equality statements (e.g., by treating the leftmost item as “1”, and being consistent with this choice for other equality constraints, but translating inequality constraints consistent with the reverse order, with ‘left of’ meaning >). This can be dealt with by adding an option to replace placement numbers r in equality statements by N − r + 1. This doubles the number of evaluations to be done (as each T now has two versions), but allows for an auto-correction in SUM.\n\nC.1 COMPARISONS WITH CHAIN-OF-THOUGHT APPROACHES\n\nFollowing Wei et al. (2022), we use “chain-of-thought” to mean LLM scoring approaches that use insertion of generated tokens between the prompt and the target answer. The model is taught, using few-shot demonstrations, how to generate these intermediate tokens. Above we have compared THINKSUM with approaches that add extracted (from an auxiliary LM call), not generated (within the LM’s linear workspace) token sequences after the prompt, for the ODD ONE OUT, PHRASE RELATEDNESS, and LOGICAL DEDUCTION tasks (see Table B.1).\n\nWith suitable examples, it may be possible for a chain-of-thought approach to replace the THINK phase, by learning from demonstrations to generate the appropriate knowledge, and parts of the SUM phase, although inference over parallel evaluations of the LLM is no longer possible. Our auxiliary knowledge baselines make precisely that generous assumption and focus the comparisons on the need for parallel calls and reasoning over possibilities using probabilistic inference (instead of leaving it to the LLM to make the right conclusions from the list of extracted alternatives).\n\nAlthough we expect that appending facts in a standard format to the prompt would help the model more than teaching the model to generate these facts, we experimented with chain-of-thought approaches on several tasks. Table C.3 shows example demonstrations and prompt formats used for each task, and Table C.2 shows the results using two variants of the largest GPT-3 model.\n\nTable C.2: Comparison of THINKSUM with chain-of-thought prompting approaches.\n\nGPT-3 (davinci)\n\nGPT-3 (davinci-002)\n\nTask\n\nDirect CoT\n\nTHINKSUM CoT\n\nTHINKSUM\n\nODD ONE OUT PHRASE RELATEDNESS LOGICAL DEDUCTION KNOWN UNKNOWNS INVENTED WORDS\n\n0.27 0.59 0.32 0.61 0.29\n\n0.33 0.55 0.25 0.70 0.50\n\n0.80 0.85 –\n0.54 0.64\n\n0.64 0.79 0.39 0.74 0.64\n\n0.84 0.87 0.77 0.76 0.71\n\nAs expected, THINKSUM outperforms chain-of-thought prompting on all tasks with all variants except KNOWN UNKNOWNS with the davinci variant, where direct prompting already performs well. (We did not evaluate THINKSUM with davinci on LOGICAL DEDUCTION because prompts like the one in Figure 4 did not reliably produce outputs in the correct format; notice that chain-of-thought is barely better than random guessing (20%).)\n\n16\n\nFigure D.1: Auxiliary knowledge prompting applied to ODD ONE OUT. Facts are generated using the “list differences” prompt described in Figure 2 (right) and post-processed according to §D.2.\n\nWhen interpreting these results, it is important to note that only one prompt format was evaluated for both chain of thought and THINKSUM, and the format of prompts and demonstrations can have a strong and often unpredictable effect on the LLM. We observed that chain-of-thought approaches are highly sensitive to minor changes in the prompt format or the construction of in-context examples, consistent with the known biases of in-context learning when a potentially lengthy prompt is evaluated (Lu et al., 2022; Zhao et al., 2021). On the other hand, using structured, shorter components is more reliable, as demonstrated by the efficacy of the THINK prompts used in THINKSUM.\n\nD ADDITIONAL EXPERIMENTAL DETAILS\n\nOur experiments are performed using four different sizes of GPT-2 (Small, Medium, Large, and XL) (Radford et al., 2019), GPT-3 with four different model sizes (ada,babbage,curie,davinci) (Brown et al., 2020), and InstructGPT (Ouyang et al., 2022). All GPT-3 experiments are run between August 2022 and September 2022 by using the OpenAI API. Our GPT-2 experiments were run in PyTorch (Paszke et al., 2019) and the Hugging Face Transformers library with a Tesla K80 GPU.\n\nD.1 HYPERPARAMETERS\n\nMaximum generation length. For tasks that require example and list generation, such as CONCEPTUAL COMBINATIONS, KNOWN UNKNOWNS, and SPORTS UNDERSTANDING, we use max tokens = 100. For fact generation in ODD ONE OUT with auxiliary knowledge and THINKSUM, we use max tokens = 1000. Temperature. All GPT-2 experiments used temperature = 0.5. For SPORTS UNDERSTANDING and translation tasks, we used temperature = 0.5 to promote diversity of generated plausible options. All other experiments used temperature = 0. Number of examples (Ne). For CONCEPTUAL COMBINATIONS we used Ne = 2, and for KNOWN UNKNOWNS and SPORTS UNDERSTANDING we used Ne = 4. Threshold. A threshold of 0.01 was used for SPORTS UNDERSTANDING.\n\nD.2 KNOWLEDGE GENERATION DETAILS\n\nPost-processing. In our knowledge generation experiments for both THINKSUM and the auxiliary knowledge approach, we post-process the generated knowledge statements, to ensure formatting does not harm the predictions of each method. We first remove the extra spaces and the numbers and punctuation generated by the LLM before each fact while enumerating the items of the list. Later, we only keep sentences that contain only one of the objects of interest from the task, to make sure each sentence contains a knowledge statement into which any of the objects can be substituted. Finally, sentences with less than 3 words are removed as these are not likely to contain informative statements.\n\nAuxiliary knowledge. For auxiliary knowledge experiments, we prepend the generated and postprocessed knowledge statements before the question in the task. An example is illustrated in Figure D.1.\n\n17\n\nTable C.3: Few-shot demonstrations used for chain of thought (Table C.2).\n\nWords: blue, pink, magenta, banana All words are colors except banana. The odd one out is banana.\n\nODD ONE OUT\n\nPHRASE RELATEDNESS\n\nKNOWN UNKNOWNS\n\nLOGICAL DEDUCTION\n\nINVENTED WORDS\n\nWords: pencil, eraser, baby, rule, notebook All words are office supplies except baby. The odd one out is baby.\n\nFor each word or phrase, identify the most related choice from the listed options. Input: Ice Cream Option: Antarctica Option: Titanic Option: Dessert Option: Sour Cream Ice cream is a type of dessert. Therefore, ice cream and dessert are the most related. Answer: Dessert What was the population of San Francisco in 2018? Option: 879,676 Option: Unknown The question asks the population of San Francisco in 2018, for which data can be collected. Population data for cities on a yearly basis is available, and thus the answer is known, and it is 879,676. Answer: 879,676 What was the population of San Francisco yesterday? Option: 891,402 Option: Unknown The question asks the population of San Francisco yesterday. As it is not possible to know the exact population of a city on a daily basis, the answer for this question is unknown. Answer: Unknown On a table, there are five plates: a black plate, a white plate, a green plate, a blue plate, and a red plate. The white plate is bigger than the green plate. The red plate is the biggest. The black plate is bigger than the blue plate. The black plate is smaller than the green plate. Which plate is the smallest? Option: The red plate is the smallest. Option: The black plate is the smallest. Option: The white plate is the smallest. Option: The green plate is the smallest. Option: The blue plate is the smallest. The black plate is bigger than the blue plate. The black plate is smaller than the green plate, as a result the green plate is bigger than the blue plate as well. The white plate is bigger than the green plate, which is bigger than the blue plate. As a result, the green plate is bigger than the blue plate. The red plate is the biggest, so it is bigger than the blue plate. Since all other plates are bigger than the blue plate, the blue plate is smallest. Answer: The blue plate is the smallest.\n\nThe word ’borger’ are animals who bite specific things for fun, and the word ’folpt’ is a type of a chewy toy. Question: Which of the following sentences best characterizes borger folpts? Option: Borger folpts are leashes for animals. Option: Borger folpts are toys for infants. Option: Borger folpts are hard to swallow. Option: Borger folpts are pet toys. Borgers are animals, and folpts are chewy toys. Therefore, borger folpts are chewy toys that animals, or pets, can play with. Therefore, the answer is borger folpts are pet toys. Answer: Borger folpts are pet toys.\n\n18\n\nTable D.1: List of examples for the studied BIG-bench tasks.\n\nTask: PHRASE RELATEDNESS Input: For each word or phrase, identify the most related choice from the listed options. Input: home town Option: town center Option: location Option: native city Option: home run\n\nTask: ODD ONE OUT Input: Pick the odd word out: glass, head, arm, leg, hand, foot\n\nTask: CONCEPTUAL COMBINATIONS: INVENTED WORDS Input: The word ‘binne’ means any animal that is furry and has four legs, and the word ‘bam’ means a simple sort of dwelling. Question: Which of the following sentences best characterizes binne bams? Option: Binne bams are for pets. Option: Binne bams are typically furnished with sofas and televisions. Option: Binne bams are luxurious apartments. Option: Binne bams are places where people live\n\nTask: NOVEL CONCEPTS Input: What do the following have in common? 1) bumble bees 2) 01010101 3) race cars Option: They all make noise. Option: They all are yellow. Option: They all are binary. Option: They all go fast. Option: They all have stripes.\n\nTask: LOGICAL DEDUCTION: FIVE OBJECTS Input: On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book. The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right. Option: The {gray, red, purple, blue, black} book is second from the right.\n\nTask: CODENAMES Input: Try to identify the 2 words best associated with the word WHITE from the following list: book, anchor, rainbow, shoulder, tunnel, sack, drum, pacific, page, mark, gear, glacier. Give your answer in alphabetical order.\n\nTask: KNOWN UNKNOWNS Input: How often did Abraham Lincoln cut his toenails? Option: Every Saturday night Option: Unknown\n\nTask: SPORTS UNDERSTANDING Input: Statement: Draymond Green threw a touchdown. Plausible/implausible?\n\nTask: EMOJI MOVIE Input: What movie does this emoji describe? {lying-face-emoji} Option: pinocchio Option: heat Option: the big sleep Option: inception Option: lilo & stitch\n\nTask: MISCONCEPTIONS RUSSIAN Input: Vaccines cause autism. / Vaccines do not cause autism. [in Russian]\n\n19",
    "reference": "# Summary Of The Paper\n\n1. This paper proposed a two-stage method that reasons over sets of objects or statements.\n2. The paper performs empirical evaluation on the BIG-bench benchmark and shows improvements over fewshot GPT3\n\n# Strength And Weaknesses\n\nStrengths:\n1) Experimental results suggest the proposed method has an improvement over few-shot GPT.\n\nWeaknesses:\n1) The paper should reduce the usage of color in the main text\n2) The paper should reduce the amount of branding and focus on the actual technique.  I don't see how this has much to do with thinking fast and slow\n3) Figure 1 is confusing.  The think step involves manipulating the inputs to get new sequences instead of just generating a set of relevant words?  Then, the second step is the actual summing over the sequences generated in the first step.\n4) There is a lack of technical details.  In particular, it is not exactly clear how the sum step is performed.  If this follows a probabilistic framework, please write down the steps formally.\n5) This paper first contrasts to chain of thought prompting, but it doesn't compare to that in experiments.\n6) Table 1 is confusing.  What's the difference between GPT-3 175B (davinci) and InstructGPT?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity, Quality and Reproducibility need to be improved.\nWithout clear writing, it is hard to evaluate novelty.\n\n# Summary Of The Review\n\nThe writing is not clear, making it hard to evaluate the contribution of this paper.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nGROUP-WISE VERIFIABLE DISTRIBUTED COMPUTING FOR MACHINE LEARNING UNDER ADVERSARIAL ATTACKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDistributed computing has been a promising solution in machine learning to accelerate the training procedure on large-scale dataset by utilizing multiple workers in parallel. However, there remain two major issues that still need to i) adversarial attacks from malicious workers, and ii) the effect be addressed: In this paper, we tackle both problems of slow workers known as stragglers. simultaneously by proposing Group-wise Verifiable Coded Computing (GVCC), which leverages coding techniques and group-wise verification to provide robustness to adversarial attacks and resiliency to straggler effects in distributed computing. The key idea of GVCC is to verify a group of computation results from workers at a time, while providing resilience to stragglers through encoding tasks assigned to workers with Group-wise Verifiable Codes. Experimental results show that GVCC outperforms the existing methods in terms of overall processing time and verification time for executing matrix multiplication, which is a key computational component in machine learning and deep learning.\n\n1\n\nINTRODUCTION\n\nRecently, machine learning and big data analysis have achieved huge success in various areas such as computer vision, natural language processing, and reinforcement learning, etc. Since they usually demand a massive amount of computation on a large dataset, there has been increasing interest in distributed systems, where one node is used as a master and the others are used as workers.\n\nOne possible option is distributed computing (Dalcín et al., 2005; 2011), where the workers compute partial computation task received from the master. In distributed computing, the master divides and distributes tasks (which require far small memory than the original task) to workers, and they compute the assigned tasks and send results back to a master. Distributed computing can be utilized to compute matrix multiplication in machine learning, the most important and frequent computation block. In a distributed setting, however, there exist two foremost considerations to embed distributed computing in machine learning applications, i) stragglers and ii) adversarial workers.\n\nStragglers are workers that return their computation results much slower than others. It has been reported that stragglers can be a serious bottleneck to performing large-scale computation tasks (Dean & Barroso, 2013; Huang et al., 2017; Tandon et al., 2017). To handle straggler effects, coded computing was first suggested in Lee et al. (2018). In coded computing, a master encodes a computation task with a coding technique while retaining redundancy in task allocation. Due to the redundancy arisen from the coding technique, a master does not need all results of tasks to achieve the final output and can ignore stragglers. This approach has been applied to various computation tasks, especially on matrix multiplication (Dutta et al., 2016; Yu et al., 2017; Park et al., 2018; Reisizadeh et al., 2019; Dutta et al., 2019; Yu et al., 2020).\n\nMoreover, some of the workers could be adversarial workers, which send perturbed results to the master to contaminate or degrade the performance of neural networks. Many studies (Biggio et al., 2012; Blanchard et al., 2017; El Mhamdi et al., 2018; Sohn et al., 2020; Bagdasaryan et al., 2020; Wang et al., 2020) demonstrate that adversarial workers slow down the overall training process and\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Training curve of neural networks adversarial under attacks.\n\n(b) Distributed computing for a matrix multiplication C = AB under adversarial workers and stragglers.\n\nFigure 1: Training curve of neural networks under adversarial workers and system model for distributed computing\n\nseverely degrade the performance of a neural network. In this paper, we mainly consider attacks from adversarial workers (i.e. Byzantine workers) in distributed computing, where they try to contaminate the final computation product by returning wrong computation results for the assigned task.\n\nTo demonstrate the effect of adversarial workers in distributed computing, we trained a neural network under adversarial attack. Fig 1(a) shows the training curve of neural networks on CIFAR100. 32 workers were used for training and adversarial workers send random values with the same magnitude as the original results during training. As we can see in the figure, the training accuracy of a neural network degraded severely when there exist adversarial workers. The training accuracy of a neural network converged to 0.99 after 560 epochs when there is no adversarial worker. However, with only 3.125% of adversarial workers, the training accuracy of the neural network was only 0.89 after 1000 epochs.1\n\nTo be robust against adversarial attacks, a master can check whether the computation results from workers are correct or not to figure out adversarial workers. Based on this idea, there have been some recent studies that suggest an encoding scheme for assigning computation tasks to workers to tolerate wrong computation results when obtaining the final computation product, and to identify adversarial workers from the returned computation results (Yu et al., 2019; Soleymani et al., 2021; Hong et al., 2022). Another line of research, which we call verifiable computing, has focused on identifying adversarial workers with a fast verification procedure. In verifiable computing, a master verifies the correctness of computation results by using a verification key (Freivalds, 1977; Tang et al., 2022).\n\nIn this paper, we focus on designing a fast and robust distributed computing system for matrix multiplication. We use both a coding approach and verifiable computing to combat adversarial attacks from malicious workers and straggler effects. Our system model is depicted in Fig 1(b), where a master encodes the computation tasks, distributes them to the workers, and decodes the final product after receiving and verifying the computation results from the workers.\n\nTo be specific, we suggest a novel encoding scheme that enables verifying a large number of computation results at a time, and decoding the final computation product from a part of computation results from the workers to mitigate straggler effects as well. We experiment with various settings and demonstrate that our proposed scheme can identify adversarial workers and obtain the final computation product much faster than existing methods. The main contributions of this paper are as follows.\n\n• We propose Group-wise Verifiable Coded Computing (GVCC), handling both straggler effects and adversarial attacks in distributed computing for matrix multiplication tasks. To be more specific, we suggest group-wise verifiable codes (GVC) for encoding the computation tasks of workers and provide a suitable group-wise verification algorithm.\n\n1We provide more explanation and detailed experiment settings in Appendix A.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nWe first provide a process of group-wise verification trial (GVT), then propose a two-stage verification algorithm based on GVT. We also provide modified verification algorithm that can be utilized under straggler effects.\n\n• We demonstrate the performance of GVCC via experiments in terms of overall processing time and verification time for executing matrix multiplication tasks in distributed computing under adversarial attacks and straggler effects. According to experimental results, GVCC speeds up overall processing time and verification time up to ×1.11 and ×3.34 compare to that of the existing verifiable coded computing scheme in Tang et al. (2022), and speeds up up to ×2.46 and ×174 than group testing based adversarial attack identification scheme for coded matrix multiplication in Hong et al. (2022), respectively.\n\nNotations. For a, b ∈ Z, [a : b] denotes {a, a + 1, . . . , b}.\n\n2 PROBLEM SETTING\n\nWe consider a distributed computing scenario for matrix multiplication task with one master and E workers {Wi}E i=1. A master wants to perform a matrix multiplication C = AB on input matrices for a sufficiently large finite field Fq. The master generates encoded A ∈ Fa×b matrices using A and B, then distributes them to workers to allocate tasks. To be specific, the master encodes ̃Ai, ̃Bi and sends them to Wi for i ∈ [1 : E]. After receiving ̃Ai and ̃Bi, each worker Wi computes the allocated task\n\nand B ∈ Fb×c\n\nq\n\nq\n\nWe assume that some of the workers could be adversarial workers or stragglers. We list our assumptions on adversarial workers and stragglers as follows:\n\n• There exist l adversarial workers WL, where L denotes a set of adversarial workers for\n\nL ⊂ [1 : E] and |L| = l.\n\n• Stragglers occur randomly depending on the network condition and currently available\n\ncomputing resource at each worker.\n\n• Adversarial workers have knowledge about input matrices A and B, and all the protocol of distributed computing including encoding, task allocation, and decoding, only except the random key r that is used for verification.2\n\n• A master cannot be aware of the number of adversarial workers in advance.\n\n• Adversarial workers cannot collude with each other, which implies that they cannot access\n\nthe information about assigned tasks of other workers.\n\nTo improve the performance of distributed computing systems under adversarial attacks and stragglers, we aim to reduce verification time and overall processing time of distributed matrix multiplication.\n\n3 BACKGROUND: VERIFIABLE COMPUTING AND CODED COMPUTING\n\nIn this section, we first introduce two methods that are leveraged in GVCC: i) verifiable computing that enables identifying adversarial workers and ii) coded computing that mitigates the effects of stragglers.\n\n3.1 VERIFIABLE COMPUTING FOR ADVERSARIAL ATTACK\n\nThe authors of Freivalds (1977) proposed a verification method (Freivalds’ method) that uses a random key to verify the computation results. Freivalds’ method for matrix multiplication can be summarized in three steps as follows. Computation assignment: A master assigns a matrix multiplication task C = AB for A ∈ Fa×b and B ∈ Fb×c to an available worker. The worker executes the assigned task, and returns its result C′ to the master.\n\nq\n\nq\n\n2Protecting the random key from the adversarial workers can be realized by randomly generating r at the\n\nmaster, after receiving results from the workers.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nVerification key generation: The master generates a random key r ∈ F1×a verification key s1 = rA.\n\nq\n\nand computes a\n\nCorrectness check: The master computes s2 = s1B = rAB = rC and s2 received result. By comparing s2 and s′\n\n2, the master can verify whether C′ is correct or not.\n\n′ = rC′ from the\n\nIt has been known that the master can identify adversarial workers using this verification process q in the finite field Fq (Sahraei & Avestimehr, 2019). with probability 1 − 1 In this verification procedure, the master computes ab + bc + ca scalar products while the original task C = AB requires abc scalar products.\n\n3.2 CODED COMPUTING FOR STRAGGLERS\n\nIn distributed computing, several methods have been proposed to mitigate straggler effects. One traditional method is to replicate a computational task and allocate it to multiple workers (Wang et al., 2014). However, this replication-based method is inefficient as it requires K additional workers to tolerate a single straggler, when the computational task is divided into K smaller tasks. To handle straggler effects more efficiently, coded computing was suggested in Lee et al. (2018). By using a coding theoretic approach for allocation of tasks to workers, the master can retrieve the final product after receiving enough number of workers. We now define an important metric that has been used in many coded computing researches.\n\nRecovery threshold R: The minimum number of computation results of tasks from non-adversarial workers that the master requires to obtain the final product for the worst-case scenario.3\n\nIn coded computing, the master can decode the final output from R out of E results and it is able to tolerate up to E − R stragglers. Therefore, coded computing requires S additional workers for handling S stragglers, while the replication-based method requires KS additional workers for S stragglers.\n\n4 GROUP-WISE VERIFIABLE CODED COMPUTING\n\nIn this section, we provide GVC and propose an algorithm for group-wise verification. We also suggest a modified algorithm for GVCC under straggler effects.\n\n4.1 ENCODING OF GROUP-WISE VERIFIABLE CODES (GVC)\n\nWe now explain the encoding procedure of GVC that facilitates group-wise verification at the master. m ×b In GVC, the master divides input matrices A and B into sub-matrices of equal size Aw ∈ F q\nfor w ∈ [1 : m], and Bz ∈ Fb× c are given by\n\nfor z ∈ [1 : n]. Then input matrices and their product C = AB\n\nq\n\nn\n\na\n\nA =\n\n\n\n \n\nA1 ... Am\n\n\n\n  , B = [ B1\n\n· · · Bn ] , C =\n\n\n\n \n\nA1B1 ... AmB1\n\n· · · A1Bn . . . · · · AmBn\n\n...\n\n\n\n  .\n\n(1)\n\nTo generate the encoded matrices, the master uses encoding functions pA and pB constructed by using sub-blocks of A and B as coefficients, which are given by\n\npA(x) = Σm\n\ni=1Aixi−1, pB(x) = Σn\n\nj=1Bjf j−1(x),\n\n(2)\n\nwhere x represents the variable of encoding functions pA and pB. f (x) is the basic building component for constructing pB and we now introduce the conditions on f (x) that enable groupwise verification of GVC.4\n\n3It should be noted that previous works have included stragglers or adversarial workers in the recovery\n\nthreshold and in this case, the recovery threshold of GVC can be expressed as R′ = R + S + l.\n\n4The condition for f (x) was also used in squeezed polynomial codes (Hong et al., 2020), in order to reduce\n\ncommunication load in distributed computing.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nCondition I : The polynomial function f (x) used in the encoding function pB(x) should satisfy the following conditions.\n\ni) f (x) is an mth order polynomial function.\n\nii) f (x) has at least E f (αt,2) = · · · = f (αt,m) = wt for t ∈ (cid:2)1 : E\n\nm sets of m evaluation points α1,1, α1,2, ..., α E (cid:3).\n\nm\n\nm ,m that satisfy f (αt,1) =\n\ncUsing the encoding functions and evaluation points satisfying Condition I, the master generates the encoded matrices ̃At,u and ̃Bt,u for t ∈ (cid:2)1 : E\n\n(cid:3) and u ∈ [1 : m] as\n\nm\n\n ̃At,u = pA(αt,u) = Σm ̃Bt,u = pB(αt,u) = Σn\n\ni=1Aiαi−1 t,u , j=1Bjf j−1(αt,u) = Σn\n\nj=1Bjwj−1\n\nt = ̃Bt,∗,\n\n(3)\n\nwhere wt = f (αt,u) for u ∈ [1 : m], which are the values of encoding functions pA and pB at the evaluation points x = αt,u.\n\nRemark 1. (Quantization and embedding into the finite field) In this paper, we assume that input matrices are defined on a finite field Fq, while the dataset for machine learning and polynomial function f (x) used for encoding are based on the real field R. To express real field data and function in the finite field, we quantize matrices A, B, and f (x) using v bits, and embed them into a finite field Fq of integers at modulo a prime q. Quantization of real numbers and embedding into the finite field have been used in several previous works (Yu et al., 2017; Ji et al., 2021; Tang et al., 2022). To analyze the effect of quantization in training of the deep neural network, we provide an experiment in Appendix B.\n\nRemark 2. (Polynomials for GVC) GVC can be constructed using any polynomial satisfying Condition I. For instance, Chebyshev polynomial, which has been used in several coded computing studies (Fahim & Cadambe, 2021; Hong et al., 2021), can be used as f (x) by using quantization as stated in Remark 1. We also provide the proof in Appendix C.\n\nRemark 3. (Reduced Computational overhead for encoding) In GVC, since it is guaranteed ̃Bt,u = ̃Bt,∗, ∀u ∈ [1 : m] for t ∈ (cid:2)1 : E m )E matrices to assign E tasks, instead of encoding 2E matrices as in the conventional methods. Thus, the computational overhead for encoding can be lowered by GVC.\n\n(cid:3), the master encodes (1 + 1\n\nm\n\n4.2\n\nTASK ASSIGNMENT AND COMPUTING AT WORKERS\n\nTo assign the tasks to workers, the master randomly rearranges workers into E which can be denoted as Wt,u for t ∈ (cid:2)1 : E\n\nm groups of same size, (cid:3) and u ∈ [1 : m], and distributes ̃At,u, ̃Bt,u to Wt,u.\n\nm\n\nEach worker Wt,u computes the assigned task ̃Ct,u = ̃At,u ̃Bt,u, which corresponds to the value of the evaluation function pC(x) at the evaluation point x = αt,u, where pC(x) is given by\n\npC(x) = pA(x) × pB(x) = Σm\n\ni=1Σn\n\nj=1AiBjxi−1f j−1(x).\n\n(4)\n\nAfter each worker finishes the assigned task, it returns its computation result to the master.\n\n4.3 GROUP-WISE VERIFICATION FOR GVC\n\nAfter receiving the results from workers, the master begins a verification process. We first explain a group-wise verification process and suggest two-stage verification algorithm based on this process.\n\n4.3.1 A GROUP-WISE VERIFICATION TRIAL (GVT)\n\nLet us denote workers with finished computation among {Wt,u}m m ]. By GVC, a master can verify results from group Tt in a single trial. In conventional verifiable computing approach, a master verifies the computation result ̃C i for i ∈ [1 : E] individually. However, if ̃At,u ̃Bt,u = ΣTt ̃Ct,u = ΣTt ̃At,u ̃Bt,∗ = the tasks are encoded by GVC, it is ensured that ΣTt u=1 where |Tt| = k (k ≤ m). Therefore, ΣTt\n\n ̃At,u × ̃Bt,∗ for a group of workers Tt among {Wt,u}m\n\nu=1 by Tt for t ∈ [1 : E\n\n′\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nthe master can verify k computation results ̃At,u ̃Bt,u in a group by verifying ΣTt On the basis of this fact, we suggest a process for group-wise verification trial (GVT).\n\n ̃At,u× ̃Bt,∗ instead.\n\nVerification key generation: For each trial of group-wise verification, a master generates a random key ̄rt ∈ F1× a\n\nm and computes a verification key ̄s1,t = ̄rtΣTt\n\n ̃At,u.\n\n ̃At,u × ̃Bt,∗ = ̄rtΣTt( ̃At,u ̃Bt,u) = ̄rtΣTt\n\n ̃Bt,u = ̄s1,t × ̃Bt,∗ = Group-wise Correctness check: The master computes ̄s2,t = 1 ̃C′ ̄rtΣTt t,u from the received computation results. If ̄s2,t ̸= ̄s′ 2,t, then the master marks the output of verification trial as Positive, which implies there exist more than one adversarial worker in the tested group Tt, and marks as Negative otherwise.5\n\nk ̄s1,tΣTt 2,t = ̄rtΣTt\n\n ̃Ct,u and ̄s′\n\n4.3.2 TWO-STAGE VERIFICATION ALGORITHM\n\nWe now provide a two-stage verification algorithm of GVC, given by Algorithm 1.6\n\nAlgorithm 1: Group-wise Verifiable Coded Computing\n\n1 Two-stage group-wise verification: After receiving all computation results from workers, the\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\nmaster begins group-wise verification.\n\nfor t ∈ [1 : E\n\nm ] do\n\nThe master verifies m workers using GVT in Section 4.3.1, where Tt = {Wt,u}m if Group-wise verification result for Tt is negative then\n\nu=1.\n\nPut all workers in Tt to a set of non-adversarial workers T n.\n\n// Stage 1\n\nelse\n\nPut all workers in Tt to a set of possible adversarial workers T p.\n\nif |T n| ≥ R then\n\nbreak;\n\nif |T n| < R then\n\nThe master verifies each worker in T p using GVT for k = 1, and put non-adversarial\n\nworkers whose verification result is negative to T n until |T n| = R.\n\n// Stage 2\n\nTwo-stage group-wise verification: After receiving all computation results of tasks from workers, the master starts group-wise verification based on GVT. However, one thing we should take into account is that GVC can verify up to m results at once. Thus, we propose a two-stage group-wise verification algorithm adjusted to the parameter m.\n\n• Stage 1: The master groups {Wt,u}m\n\nu=1 as Tt for t ∈ [1 : E\n\nm ]. The master performs a\n\ngroup-wise verification on Tt using GVT.\n\n• Stage 2: The master verifies each computation result in Tt if group-wise verification result\n\nfor Tt is positive, until it achieves R non-adversarial workers to obtain C.\n\n4.4 DECODING AT THE MASTER\n\nAfter verifying R non-adversarial computation results, the master starts decoding. As we can see in equation 1 and equation 4, AiBj for i ∈ [1 : m], j ∈ [1 : n] can be retrieved from pC(x) to achieve C. Since ̃Ct,u = pC(αt,u) and pC(x) is a polynomial of degree mn−1, it can be interpolated from mn distinct value of ̃Ct,u. Hence, the master needs any fastest mn non-adversarial computation results to decode the final output C. Thus, the recovery threshold of GVC is R = mn.\n\n5It should be noted that group-wise verification trial has the same accuracy with the individual verification\n\nin Freivalds (1977). We demonstrate this by experiment under various adversarial attacks in Appendix D.\n\n6We do not consider straggler effects here for simplicity, but provide a modified verification algorithm of\n\nGVC that considers straggler effects in Section 4.5.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nDecoding can be done by the inversion of the coefficient matrix in\n\n\n\n \n\n ̃C1 ... ̃Cmn\n\n\n\n\n\n\n\n  =\n\n \n\n \n\n0\n\nβ1 ... βmn\n\n0\n\n. . .\n\nm−1\n\nβ1\n\n. . . βmn\n\nm−1\n\nf (β1) . . . f (βmn)\n\n. . .\n\nβ1\n\nm−1f (β1)n−1 ...\n\n. . . βmn\n\nm−1f (βmn)n−1\n\n\n\n  ⊗ I a\n\nm × a\n\nm\n\n\n\n\n\n  ×\n\n \n\n\n\n  ,\n\nA1B1 ... AmBn\n\n(5)\n\nwhere ̃Ck and βk for k ∈ [1 : mn] denote the fastest mn computation results and the evaluation points of them, respectively and ⊗ denotes Kronecker product. It can be also achieved by interpolation (Kedlaya & Umans, 2011) of pC(x) and extracting AiBj from pC(x) by repeated division of f (x). We provide more detailed decoding process in Appendix E.\n\nRemark 4. (Optimal Recovery threshold) It has been known in Yu et al. (2017) that the optimal recovery threshold for coded matrix multiplication is mn when A and B are divided into m and n partitions in a row-wise and a column-wise manner, respectively. Thus GVC also achieves the optimal recovery threshold.\n\nRemark 5. (Upper bound of tolerating adversarial attacks) Since GVCC requires mn computation results to obtain the final product, GVCC can tolerate up to lupper = E − S − mn adversarial workers, where S denotes the number of stragglers.\n\n4.5 MODIFIED VERIFICATION ALGORITHM OF GVCC UNDER STRAGGLER EFFECTS\n\nWe now provide a modified algorithm that can be used under straggler effects.\n\nTwo-stage group-wise verification in modified GVCC:\n\n• Stage 1: The master groups {Wt,u}m\n\nm ] as in GVCC. The master establishes a certain threshold z. The master starts a group-wise verification for the computation results of all groups received up to that time when the master receives z computation results from workers.\n\nu=1 as Tt for t ∈ [1 : E\n\n• Stage 2: The master first individually verifies the computation results that arrived late, and verifies each computation result in Tt if group-wise verification result for Tt is positive, until it finds R non-adversarial workers to obtain C.\n\nBy setting a proper threshold z, the modified verification algorithm can efficiently deal with straggler effects. However, it may lose some of the benefits of group-wise verification as a trade-off because the master verifies a smaller group of workers in a single trial. The impact on overall processing time will be analyzed in Section 6 via experiment.\n\n5 RELATED WORKS\n\nIn distributed computing, there have been several studies on handling adversarial attacks and straggler effects simultaneously. First, to have tolerance over adversarial attacks and provide resiliency to straggler effects, Lagrange coded computing (LCC) has been suggested in Yu et al. In LCC, a master requires 2K additional non-adversarial workers to be robust against (2019). K adversarial workers. In addition, a list-decoding approach has been applied to distributed matrix multiplication in Subramaniam et al. (2019), and it reduces the additional cost of tolerating adversarial attacks by a factor of two asymptotically compared to LCC. However, the computational complexity to perform list-decoding increases quadratically with the number of workers.\n\nRecently, verifiable computing has been introduced to coded computing schemes. The authors of Tang et al. (2022) have suggested AVCC to counter straggler and adversarial workers. In AVCC, a master only requires K additional computation results from workers if there exist K adversarial workers, because it can verify each computation result using a verification key. However, since the master can verify only a single computation result at a time, the verification process can be a bottleneck for overall processing if the master utilizes numerous workers in the system.\n\nIn addition, there have been several studies that are based on a group-wise identification to find adversarial workers, while mitigating straggler effects using coded matrix multiplication. First,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\ngroup testing algorithm has been applied to coded computing in Solanki et al. (2019). However, since it has a constraint on the minimum testable size for group testing, the number of test trials can be significantly large. To mitigate this limitation, hierarchical group testing (HGT) has been proposed in Hong et al. (2022). However, HGT imposes high computational complexity on a master since it requires matrix inversion or Reed-Solomon decoding to identify adversarial workers, whereas verifiable computing requires matrix-vector multiplications to verify them.\n\n6 EXPERIMENTAL RESULTS\n\n(a) Number of adversarial workers l\n\n(b) Straggler probability\n\n(c) Matrix shape\n\n(d) Number of workers\n\nFigure 2: Overall processing time of AVCC, HGT, and GVCC (proposed).\n\nIn this section, we provide the performance of GVCC and modified GVCC in terms of overall processing time, and compare with the two existing schemes, AVCC (Tang et al., 2022) and HGT (Hong et al., 2022). We proceed distributed matrix multiplication in a cluster of Amazon EC2 cloud, and use one t2.2xlarge node as a master and use t2.micro nodes as workers. Implementation is done by MPI4py (Dalcín et al., 2005) and matrices are randomly generated by NumPy. We set the verification time limit as 100 seconds and if the verification process takes more than 100 seconds, we terminate the experiment and mark it as a failure. We run four experiments and evaluate the overall processing time. Each experiment is repeated 20 times and the result is depicted in Fig. 2. We also list stage-wise processing time (encoding (Enc), task assignment (SA), computation & return (CR), verification (Ver), decoding (Dec), and overall processing times) of experiments in Appendix F.1.\n\nq\n\nq\n\n, B ∈ F6300×3600\n\nIn Fig. 2(a), we compare overall processing time with different number of adversarial workers l, when A ∈ F3500×6300 , m = 5, n = 9, and E = 60. GVCC speeds up overall processing time compared to AVCC for all cases, and also shows faster overall processing than HGT when l ≥ 3. As we have claimed in Remark 3, the computational overhead for encoding the tasks can lowered by GVC, which results in reduced encoding and overall processing time compared to AVCC. Furthermore, for the verification time of HGT increases significantly with the number of workers, the verification and overall processing time of GVCC is much lower than HGT when l ≥ 3. To be specific, GVCC speeds up verification time up to ×3.34 than AVCC, and ×174 than HGT, and HGT even fails to proceed verification process in the given time limit (100s) when l = 5.\n\nIn Fig. systems, where A ∈ F3500×6300\n\n2(b), we show the overall processing time with stragglers in distributed computing , m = 5, n = 9, E = 60, and l = 3. To\n\n, B ∈ F6300×3600\n\nq\n\nq\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nsimulate straggler effects in distributed computing, we randomly pick stragglers with probabilities p = 0, 0.02, 0.05, 0.1, and 0.2. Stragglers are forced to run background thread which slows down the execution of assigned tasks. In this experiment, we use the modified algorithm in Section 4.5 for GVCC (z = R = 63) and compare with AVCC and HGT. Since verification is done asynchronously for the received computation results in GVCC, verification time is added to computation & return time. As we can see in the Fig 2(b), GVCC outperforms AVCC and HGT for all cases. Thus, it is claimed that GVCC can efficiently handle straggler effects while leveraging a group-wise verification approach. We also provide additional experiments with a modified algorithm on another parameter setting in Appendix F.2.\n\nIn Fig. 2(c), we compare overall processing time with different shape of input matrices with three parameter settings: i) Square (A ∈ F3600×3600 , B ∈ q\nF1500×4500 ), iii) Fat (A ∈ F2700×7000 ), where m = 5, n = 9, l = 3, and E = 60. GVC speeds up overall processing time compared to AVCC and HGT, regardless of matrix shapes.\n\n), ii) Tall (A ∈ F4500×1500\n\n, B ∈ F3600×3600\n\n, B ∈ F7000×2700\n\nq\n\nq\n\nq\n\nq\n\nq\n\nThe performance of GVCC with the different numbers of workers is depicted in Fig. 2(d). We run experiments to multiply two input matrices A ∈ F2100×4000 , with three parameter setting (E, m, n): i) (60, 5, 9), ii) (84, 6, 11), iii) (112, 7, 13), where l = 3. GVCC achieves the lowest overall processing time in every setting. To be specific, GVCC speeds up verification time up to ×3.5 than AVCC and ×57.29 than HGT. Furthermore, GVCC speeds up overall processing time up to ×1.20 than AVCC and ×1.10 than HGT.\n\nand B ∈ F4000×3861\n\nq\n\nq\n\nVerification time (ms)\n\nHGT AVCC GVC\n\nGroup-wise verifiable size m\n\n5 2750 140 48\n\n6 1920 170 48\n\n7 2440 210 46\n\nMatrix Size N 1800 9554 45 17\n\n3600 42163 163 62\n\n900 2348 16 5.6\n\nTable 1: Verification time of GVC, AVCC, and HGT with different matrix size and m.\n\nFurthermore, it should be noted that GVC shows more benefit than AVCC as N or m gets bigger. In specific, our methods have a verification complexity of O( R m N 2) while AVCC has O(RN 2). To show the performance of GVC with different m and N , we indicate the verification time of GVC and the other schemes in table 1. We used parameter setting ((E, m, n): i) (60, 5, 9), ii) (84, 6, 11), iii) (112, 7, 13), where l = 3 (which are same setting with experiment in Fig 2(d)). We indicate the verification time of HGT, AVCC, and GVC in Table 1. We can see that the verification time of GVC remains the same value, while other methods show an increase in the verification time. This is because other methods require the verification of more results as the recovery threshold increases, but GVC has the same number of groups that needs to be verified. Moreover, to show the benefits of GVC, we conduct additional experiments. We experimented with the same parameter setting in Fig 2(a) when l = 3 and only changed the size of the matrix (m, n = 5, 9, E = 60, l = 3 and a = b = c = N ). We have changed the size of input matrix N from 900 to 3600 to show the benefit of GVC as N increases, and indicate the verification time in Table 1. As we can see in the table, verification gap between GVCC and other scheme becomes larger as N increases. In specific, HGT shows far larger verification time than GVC. Moreover, gap between AVCC and GVC increases ×2.66 when N becomes ×2, and ×9.36 when N becomes ×4.\n\n7 CONCLUSION\n\nIn this paper, we propose GVCC, a robust and fast distributed computing scheme for matrix multiplication that can deal with two important problems: adversarial attacks and straggler effects. By combining the verifiable computing with coded computing, GVCC allows a master to verify a group of computation results at a time, reducing verification time to find adversarial workers and provide resiliency to straggler effects simultaneously. Consequently, GVCC is shown to have significant reduction in verification and overall processing time, and experimental results demonstrate the effectiveness of GVCC in distributed computing systems.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nEugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to In Proceedings of the Twenty Third International Conference on\n\nbackdoor federated learning. Artificial Intelligence and Statistics, volume 108, pp. 2938–2948. PMLR, 26–28 Aug 2020.\n\nBattista Biggio, Blaine Nelson, and Pavel Laskov.\n\nPoisoning attacks against support vector\n\nmachines. In ICML, 2012.\n\nPeva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, volume 30, 2017.\n\nLisandro Dalcín, Rodrigo Paz, and Mario Storti. Mpi for python. Journal of Parallel and Distributed\n\nComputing, 65(9):1108–1115, 2005.\n\nLisandro Dalcín, Rodrigo Paz, Pablo Kler, and Alejandro Cosimo. Parallel distributed computing\n\nusing python. Advances in Water Resources, 34(9):1124–1139, 2011. ISSN 0309-1708.\n\nJeffrey Dean and Luiz André Barroso. The tail at scale. Commun. ACM, 56(2):74–80, 2013.\n\nSanghamitra Dutta, Viveck Cadambe, and Pulkit Grover. Short-dot: Computing large linear In Advances in Neural Information\n\ntransforms distributedly using coded short dot products. Processing Systems, volume 29, 2016.\n\nSanghamitra Dutta, Mohammad Fahim, Farzin Haddadpour, Haewon Jeong, Viveck Cadambe, IEEE\n\nand Pulkit Grover. On the optimal recovery threshold of coded matrix multiplication. Transactions on Information Theory, 66(1):278–301, 2019.\n\nEl Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault. The hidden vulnerability of In Proceedings of the 35th International Conference on\n\ndistributed learning in Byzantium. Machine Learning, volume 80, pp. 3521–3530. PMLR, 10–15 Jul 2018.\n\nMohammad Fahim and Viveck R. Cadambe. Numerically stable polynomially coded computing.\n\nIEEE Transactions on Information Theory, 67(5):2758–2785, 2021.\n\nRusins Freivalds. Probabilistic machines can use less running time. In IFIP congress, volume 839,\n\n1977.\n\nSangwoo Hong, Heecheol Yang, and Jungwoo Lee. Squeezed polynomial codes: CommunicationIEEE\n\nefficient coded computation in straggler-exploiting distributed matrix multiplication. Access, 8:190516–190528, 2020.\n\nSangwoo Hong, Heecheol Yang, Youngseok Yoon, Taehyun Cho, and Jungwoo Lee. Chebyshev polynomial codes: Task entanglement-based coding for distributed matrix multiplication. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pp. 4319– 4327. PMLR, 18–24 Jul 2021.\n\nSangwoo Hong, Heecheol Yang, and Jungwoo Lee. Hierarchical group testing for byzantine IEEE Journal on Selected Areas in\n\nattack identification in distributed matrix multiplication. Communications, 40(3):1013–1029, 2022.\n\nPeng Huang, Chuanxiong Guo, Lidong Zhou, Jacob R. Lorch, Yingnong Dang, Murali Chintalapati, and Randolph Yao. Gray failure: The achilles’ heel of cloud-scale systems. In Proceedings of the 16th Workshop on Hot Topics in Operating Systems, HotOS 2017, Whistler, BC, Canada, May 8-10, 2017, pp. 150–155. ACM, 2017.\n\nRuowan Ji, Asit Kumar Pradhan, Anoosheh Heidarzadeh, and Krishna R. Narayanan. Squeezed random khatri-rao product codes. In 2021 IEEE Information Theory Workshop (ITW), pp. 1–6, 2021.\n\nKiran S Kedlaya and Christopher Umans. Fast polynomial factorization and modular composition.\n\nSIAM Journal on Computing, 40(6):1767–1802, 2011.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris S. Papailiopoulos, and Kannan Ramchandran. Speeding up distributed machine learning using codes. IEEE Trans. Inf. Theory, 64(3):1514–1529, 2018.\n\nHyegyeong Park, Kangwook Lee, Jy-Yong Sohn, Changho Suh, and Jaekyun Moon. Hierarchical coding for distributed computing. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 1630–1634, 2018. doi: 10.1109/ISIT.2018.8437669.\n\nAmirhossein Reisizadeh, Saurav Prakash, Ramtin Pedarsani, and Amir Salman Avestimehr. Coded computation over heterogeneous clusters. IEEE Transactions on Information Theory, 65(7):4227– 4242, 2019.\n\nSaeid Sahraei and A. Salman Avestimehr. Interpol: Information theoretically verifiable polynomial evaluation. In 2019 IEEE International Symposium on Information Theory (ISIT), pp. 1112–1116, 2019.\n\nJy-yong Sohn, Dong-Jun Han, Beongjun Choi, and Jaekyun Moon. Election coding for distributed In Advances in Neural Information\n\nlearning: Protecting signsgd against byzantine attacks. Processing Systems, volume 33, pp. 14615–14625, 2020.\n\nAtnav Solanki, Martina Cardone, and Soheil Mohajer. Non-colluding attacks identification in\n\ndistributed computing. In 2019 IEEE Information Theory Workshop (ITW), pp. 1–5, 2019.\n\nMahdi Soleymani, Ramy E. Ali, Hessam Mahdavifar, and A. Salman Avestimehr. List-decodable coded computing: Breaking the adversarial toleration barrier. IEEE Journal on Selected Areas in Information Theory, 2(3):867–878, 2021.\n\nAdarsh M. Subramaniam, Anoosheh Heidarzadeh, and Krishna R. Narayanan. Collaborative In 2019 IEEE Information Theory\n\ndecoding of polynomial codes for distributed computation. Workshop (ITW), pp. 1–5, 2019.\n\nRashish Tandon, Qi Lei, Alexandros G. Dimakis, and Nikos Karampatziakis. Gradient coding: Avoiding stragglers in distributed learning. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 3368–3376. PMLR, 06–11 Aug 2017.\n\nTingting Tang, Ramy E. Ali, Hanieh Hashemi, Tynan Gangwani, Salman Avestimehr, and Murali Annavaram. Adaptive verifiable coded computing: Towards fast, secure and private distributed machine learning. In 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pp. 628–638, 2022.\n\nDa Wang, Gauri Joshi, and Gregory Wornell. Efficient task replication for fast response times in parallel computation. In The 2014 ACM International Conference on Measurement and Modeling of Computer Systems, pp. 599–600, 2014.\n\nHongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jyyong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. In Advances in Neural Information Processing Systems, volume 33, pp. 16070–16084, 2020.\n\nQian Yu, Mohammad Ali Maddah-Ali, and A Salman Avestimehr. Polynomial codes: an optimal design for high-dimensional coded matrix multiplication. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 4406–4416, 2017.\n\nQian Yu, Songze Li, Netanel Raviv, Seyed Mohammadreza Mousavi Kalan, Mahdi Soltanolkotabi, and Salman A. Avestimehr. Lagrange coded computing: Optimal design for resiliency, security, In Proceedings of the Twenty-Second International Conference on Artificial and privacy. Intelligence and Statistics, volume 89, pp. 1215–1225. PMLR, 16–18 Apr 2019.\n\nQian Yu, Mohammad Ali Maddah-Ali, and A Salman Avestimehr. Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding. IEEE Transactions on Information Theory, 66(3):1920–1933, 2020.\n\n11",
    "reference": "# Summary Of The Paper\n\nThe paper develops techniques for distributed matrix multiplication with adversarially corrupt nodes and stragglers.  A error correction coding scheme is developed that simultaneously detects corrupt computations via random-key-based verification, and can decode in the presence of stragglers.\n\n# Strength And Weaknesses\n\nStrengths:\n+ The simultaneous handling of polynomial-based verification and stragglers in a distributed computing context seems new.  \n\nWeaknesses;\n- A natural scheme is to verify each node separately to check if the result is adversarial. The benefit of the proposed scheme over this approach, as I understand it, is that it is computationally more efficient - but the gain appears to be small for large matrices. Assuming that all matrices are $N \\times N$,  verification is of order $O(N^2)$. The proposed scheme performs one verification for every $m$ nodes, instead of one for every node, but I am not convinced that this is practically very important when $N \\gg m$. Note that gain in computational complexity comes at the cost of a significant assumption - that the adversarial nodes do not collude. This is quite a weak assumption in the context of security literature. \n\n- Although matrix multiplication is important, the paper (main body) does not directly test their results in the context of a machine learning application. The supplemental section does seem to have an application, but it is too sparse in the details for reproduciblity (e.g., how are the codes applied? What is the developed accuracy? What are the wallclock times of various schemes?).\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The technical parts are explained quite clearly. The introduction and the paper is written in quite a winding way, and does not state the main contribution directly, I would recommend writing a more focused introduction. \n- The weaknesses section of the view above also provides examples of lack of clarity in terms of experiments. \n- The statements pertaining to Chebyshev polynomials and their applicability to condition seem flawed. This is because Chebyshev polynomials are defined over reals (and this assumption is also made in the appendix in the paper), but Proposition 1 requires a polynomial over finite fields.\n\n# Summary Of The Review\n\nThe paper has several major weaknesses including (1) a weak model for adversary, (2) lack of precision in writing some parts, and (2) lack of satisfactory experimental evaluation details for the ML applications mentioned. The positive side (lower computation overhead) seems too thin for large matrices to compensate for the above drawbacks.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nFACE RECONSTRUCTION FROM FACIAL TEMPLATES BY LEARNING LATENT SPACE OF A GENERATOR NETWORK\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFace recognition systems are increasingly deployed in different applications. In these systems, a feature vector (also called facial embeddings or templates) is typically extracted from each face image and is stored in the system’s database during the enrollment stage, which is later used for comparison during the recognition stage. In this paper, we focus on the template inversion attack against face recognition systems and propose a new method to reconstruct face images from facial templates. Within a generative adversarial network (GAN)-based framework, we learn a mapping from facial templates to the intermediate latent space of a pre-trained face generation network, from which we can generate high-resolution realistic reconstructed face images. We show that our proposed method can be applied in whitebox and blackbox attacks against face recognition systems. Furthermore, we evaluate the transferability of our attack when the adversary uses the reconstructed face image to impersonate the underlying subject in an attack against another face recognition system. Considering the adversary’s knowledge and the target face recognition system, we define five different attacks and evaluate the vulnerability of state-of-the-art face recognition systems. Our experiments show that our proposed method achieves high success attack rates in whitebox and blackbox scenarios. Furthermore, the reconstructed face images are transferable and can be used to enter target face recognition systems with a different feature extractor model.\n\nl a\nn\n\ni\n\ng\n\ni r\n\nO\n\nd e\nt c\nu r\nt s\nn o\nc e\nR\n\n0.815\n\n0.629\n\n0.618\n\n0.728\n\n0.602\n\n0.742\n\nFigure 1: Sample face images from the FFHQ dataset and their corresponding reconstructed images using our template inversion method from ArcFace templates. The values below each image show the cosine similarity between the corresponding templates of original and reconstructed face images.\n\n1\n\nINTRODUCTION\n\nFace recognition (FR) systems tend toward ubiquity, and their applications, which range from cell phone unlock to national identity system, border control, etc., are growing rapidly. Typically, in such systems, a feature vector (called embedding or template) is extracted from each face image using a deep neural network, and is stored in the system’s database during the enrollment stage. During the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Block diagram of our proposed template inversion attack\n\nrecognition stage, either verification or identification, the extracted feature vector is compared with the ones in the system’s database to measure the similarity of identities. Among potential attacks against FR systems (Galbally et al., 2014; Marcel et al., 2014; Biggio et al., 2015; Hadid et al., 2015; Mai et al., 2018), the template inversion (TI) attack significantly jeopardizes the users’ privacy. In a TI attack, the adversary gains access to templates stored in the FR system’s database and aims to reconstruct the underlying face image. Then, the adversary not only achieves privacy-sensitive information (such as gender, ethnicity, etc.) of enrolled users, but also can use reconstructed face images to impersonate.\n\nIn this paper, we focus on the TI attack against FR systems and propose a novel method to reconstruct face images from facial templates (Fig. 1 shows sample reconstructed face images using our proposed method). Within a generative adversarial network (GAN)-based framework, we learn a mapping from face templates to the intermediate latent space of StyleGAN3 (Karras et al., 2021), as a pre-trained face generation network. Then, using the synthesis part of StyleGAN3, we can generate high-resolution realistic face image. Our proposed method can be applied for whitebox and blackbox attacks against FR systems. In the whitebox scenario, the adversary knows the internal functioning of the feature extraction model and its parameters. However, in the blackbox scenario, the adversary does not know the internal functioning of the feature extraction model and can only use it to extract features from any arbitrary image. Instead, we assume that the adversary has a whitebox of another FR model, which can be used for training the face reconstruction network. We also evaluate the transferability of our attack by considering the case where the adversary uses the reconstructed face image to impersonate the underlying subject in an attack against another FR system (which has a different feature extraction model). Considering the adversary’s knowledge and the target FR system, we define five different attacks, and evaluate the vulnerability of state-of-the-art (SOTA) FR systems. Fig. 2 illustrates the general black diagram of our proposed template inversion attack.\n\nTo elaborate on the contributions of our paper, we list them hereunder:\n\n• We propose a novel method to generate high-resolution realistic face images from facial templates. Within a GAN-based framework, we learn the mapping from facial templates to the latent space of a pre-trained face generation network.\n\n• We propose our method for whitebox and blackbox scenarios. While our method is based on the whitebox knowledge of the FR model, we extend our attack blackbox scenario, using another FR model that the adversary has access to.\n\n• We define five different attacks against FR systems (based on the adversary’s knowledge\n\nand the target system), and evaluate the vulnerability of SOTA FR models.\n\nThe remainder of the paper is organized as follows: Section 2 introduces the problem formulation and our proposed face reconstruction method. Section 3 covers the related works in the literature and compares them with our proposed method. Section 4 presents our experiential results. Finally, the paper is concluded in Section 5.\n\n2 PROBLEM DEFINITION AND PROPOSED METHOD\n\nIn this paper, we consider a TI attack against a FR system based on the following threat model:\n\n2\n\nMappingNetwork(Ours)StyleGAN’sSynthesisNetworkNoise( )Template( )Latent Space( )Reconstructed Face Image ( )Face Reconstruction Network (trained using )Target FR system ( )FR system from which the template is leaked ( )DatabaseDatabaseFeature Extraction( )Leaked TemplateFeature Extraction( )CompareRecognized ?Injecting the Reconstructed Face ImageUnder review as a conference paper at ICLR 2023\n\n• Adversary’s goal: The adversary aims to reconstruct a face image from a template, and use the reconstructed face image to enter the same or a different face recognition system, which we call the target FR system.\n\n• Adversary’s knowledge: The adversary knows a face template of a user enrolled in the FR system’s database. The adversary also has either whitebox or blackbox knowledge of the feature extractor model in the same FR system.\n\n• Adversary’s capability: The adversary can present the reconstructed face image to the target FR system (e.g., using a printed photograph). However, for simplicity, we consider that adversary can inject the reconstructed face image as a query to the target FR system.\n\n• Adversary’s strategy: The adversary can train a face reconstruction model to invert facial templates and reconstruct underlying face images. Then, the adversary can use the reconstructed face images to inject as a query to the target FR system, to enter that system.\n\nLet F (.) denotes a facial feature extraction model, which gets the face image I ∈ I and extracts facial template x = F (I) ∈ X . According to the threat model, the adversary has access to the target facial template xdatabase = Fdatabase(I) and aims to generate a reconstructed face image ˆI. Then, the adversary can use the reconstructed face image ˆI to impersonate the corresponding subject and attack a target FR system with Ftarget(.), which might be different from Fdatabase(.).\n\nTo train a face reconstruction model, we can use a dataset of face images {Ii}N i=1 with N face images (no label is required), and generate a training dataset {(xi, Ii)}N i=1, where xi = Fdatabase(Ii). Then, a face reconstruction model G(.) can be trained to reconstruct face image ˆI = G(x) given each facial template x ∈ X . To train such a face reconstruction model, we consider a multi-term face reconstruction loss function as follows:\n\nLrec = Lpixel + LID,\n\n(1)\n\nwhere Lpixel and LID indicate pixel loss and ID loss, respectively, and are defined as: Lpixel = Ex∼X [(cid:107)I − G(x)(cid:107)2 LID = Ex∼X [(cid:107)Floss(I) − Floss(G(x))(cid:107)2 The pixel loss is used to minimize the pixel-level reconstruction error of the generated face image. The ID loss is also used to minimize the distance between facial templates extracted by Floss(.) from original and reconstructed face images. In Eq. 3, Floss(.) denotes a feature extraction model that the adversary is assumed to have complete knowledge of its parameters and internal functioning. Based on the adversary’s knowledge of Fdatabase(.) (i.e., whitebox or blackbox scenarios), Floss(.) might be the same or different from Fdatabase(.).\n\n2].\n\n2],\n\n(3)\n\n(2)\n\nFor the face reconstruction model, we consider StyleGAN3 (Karras et al., 2021), as a pre-trained face generation network. The StyleGAN3 model is trained on a dataset of face images using a GAN-based framework that can generate highresolution and realistic face images. The structure of StyleGAN3 is composed of two networks, mapping and synthesis networks. The mapping network MStyleGAN(.) gets a random noise z ∈ Z and generates an intermediate latent code w = MStyleGAN(z) ∈ W. Then, the latent code w is given to the synthesis network SStyleGAN(.) to generate a face image. In our training process, we fix the synthetic network SStyleGAN(.) and train a new mapping Mrec(.) to generate ˆw corresponding to the given facial template x ∈ X . Then, the generated latent code ˆw is given to the synthesis network SStyleGAN(.)\n\nFigure 3: Block diagram of our face reconstruction network.\n\n3\n\nMappingNetwork(Ours)StyleGAN’sSynthesisNetworkNoise( )Template( )Latent Space( )Critic NetworkReconstructed Face Image ( )StyleGAN’sMappingNetworkNoise( )realfakeLatent Space( )Under review as a conference paper at ICLR 2023\n\nto generate the reconstructed face image ˆI = SStyleGAN( ˆw). We can train our new mapping Mrec(.) using our reconstruction loss function as in Eq. 1. However, to obtain a realistic face image from the generated ˆw through the pre-trained synthetic network SStyleGAN(.), the generated ˆw needs to be in the distribution W; otherwise, the output may not look like a real human face. Hence, to generate ˆw vectors such that they have the same distribution as StyhleGAN’s intermediate latent, w ∈ W, we use a GAN-based framework to learn the distribution W. To this end, we use the Wasserstein GAN (WGAN) Arjovsky et al. (2017) algorithm to train a critic network C(.) which critics the generated ˆw vectors compared to the real StyleGAN’s w ∈ W vectors, and simultaneously we optimize our mapping network to generate ˆw vectors with the same distribution as W. Hence, we can consider our mapping network Mrec(.) as a conditional generator in our WGAN framework, which generates ˆw = Mrec([n, x]) given a facial template x ∈ X and a random noise vector n ∈ N . Then, we can train our mapping network and critic network using the following loss functions:\n\nLWGAN C\nLWGAN Mrec\n\n= Ew∼W [C(w)] − E ˆw∼Mrec([n,x])[C( ˆw)] = E ˆw∼Mrec([n,x])[C( ˆw)]\n\n(4)\n\n(5)\n\nIn a nutshell, we train a new mapping network Mrec(.) using our reconstruction loss function in Eq. 1, and also optimize Mrec(.) within our WGAN framework using Eq. 5. Simultaneously, we also train the critic network C(.) within our WGAN using Eq. 4 to learn the distribution of StyleGAN’s intermediate latent space W and help our mapping network Mrec(.) to generate vectors with the same distribution as W. Fig. 3 depicts the block diagram of the proposed method. We should note that our mapping network Mrec(.) has 2 fully connected layers with Leaky ReLU activation function.\n\nIn our problem formulation, we consider three different feature extraction models, including Fdatabase(.), Floss(.), and Ftarget(.). Hence, based on the adversary’s knowledge and the target system, we can consider five different attacks:\n\n• Attack 1: The adversary has whitebox knowledge of the system from which the template\n\nis leaked and want to attack the same system (i.e., Fdatabase = Floss = Ftarget).\n\n• Attack 2: The adversary has whitebox knowledge of the feature extractor of the system from which the template is leaked, but aims to attack to a different FR system (i.e., Fdatabase = Floss (cid:54)= Ftarget).\n\n• Attack 3: The adversary wants to attack the same system from which the template is leaked, but has only blackbox access to the feature extractor of the system. Instead, we assume that the adversary has the whitebox knowledge of another FR model to use for training (i.e., Fdatabase = Ftarget (cid:54)= Floss).\n\n• Attack 4: The adversary aims to attack a different FR system than the one from which the template is leaked. In addition, the adversary has whitebox knowledge of the feature extractor of the target system (i.e., Fdatabase (cid:54)= Floss = Ftarget).\n\n• Attack 5: The adversary aims to attack a different FR system from which the template is leaked and has only blackbox knowledge of both the target system and the one from which the template is leaked. However, the adversary instead has the whitebox knowledge of another FR model to use for training (i.e., Fdatabase (cid:54)= Floss (cid:54)= Ftarget).\n\nIn the attack 1 and attack 2, the adversary has the whitebox knowledge of the system from which the template is leaked (i.e., Fdatabase(.)) and uses the same model as Floss(.) for training the reconstruction network. However, in attacks 3-5, the adversary has the blackbox knowledge of the system from which the template is leaked, and therefore uses another FR model as Floss(.). Comparing the knowledge of the adversary in these attacks, we expect that attack 1 be the easiest attack for the adversary and attack 5 be the most difficult one.\n\n3 RELATED WORKS\n\nTable 1 compares our proposed method with related works in the literature. Generally, the methods for TI attack against FR systems, can be categorized based on different aspects, including the resolution of generated face images (high/low resolution), the type of attack (whitebox/blackbox attack), and the basis of the method (optimization/learning-based).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison with related works.\n\nReference\n\nResolution White/Black-box Basis\n\nAvailable code\n\nZhmoginov & Sandler (2016)\n\nCole et al. (2017) Mai et al. (2018) Duong et al. (2020) Truong et al. (2022) Dong et al. (2021) Vendrow & Vendrow (2021) Dong et al. (2022) Ours\n\nlow\n\nlow low low low high high high high\n\nwhitebox\n\nboth∗ blackbox both∗∗ both∗∗ blackbox blackbox blackbox both∗∗∗\n\n1) optimization 2) learning learning learning learning learning learning optimization optimization learning\n\n(cid:55)\n\n(cid:55) (cid:51) (cid:55) (cid:55) (cid:51) (cid:51) (cid:55) (cid:51)\n\n∗ ∗∗ ∗∗∗\n\nThe method is based on the whitebox attack, and is extended to blackbox by removing a loss term that required the FR model.\n\nThe method is based on the whitebox attack, and the blackbox attack is performed by knowledge distillation of the FR model.\n\nThe method is based on the whitebox attack, and is extended to blackbox using a different FR model.\n\nZhmoginov & Sandler (2016) proposed an optimization-based method and a learning-based method to generate low-resolution face images in the whitebox attack against FR systems. In their optimization-based attack, they used a gradient-descent-based approach to find an image that minimizes the distance of the face template as well as some regularization terms to generate a smooth image, including the total variation and Laplacian pyramid gradient normalization (Burt & Adelson, 1987) of the reconstructed face image. In their learning-based attack, they trained a convolutional neural network (CNN) with the same loss terms to generate face images from given facial templates.\n\nCole et al. (2017) proposed a learning-based attack to generate low-resolution images using a multilayer perceptron (MLP) to estimate landmark coordinates and a CNN to generate face textures, and then reconstructed face images using a differentiable warping based on estimated landmarks and face texture. They trained their networks in an end-to-end fashion, and minimized the errors for landmark estimation and texture generation as well as the distance of face template as their loss function. To extend their method from the whitebox attack to the blackbox attack, they proposed not to minimize the distance of face templates in their loss function.\n\nMai et al. (2018) proposed a learning-based attack to generate low-resolution images in the blackbox attack against FR systems. They proposed new convolutional blocks, called neighborly deconvolution blocks A/B (shortly, NbBlock-A and NbBlock-B), and used these blocks to reconstruct face images. They trained their proposed networks using two loss functions, including pixel loss (i.e., (cid:96)2 norm of reconstruction pixel error) and perceptual loss (i.e., (cid:96)2 norm of distance for intermediate features of VGG-19 (Simonyan & Zisserman, 2014) given original and reconstructed face images).\n\nDuong et al. (2020) and Truong et al. (2022) used a same bijection learning framework and trained a GAN with a generator with structure of PO-GAN (Karras et al., 2017) and TransGAN (Jiang et al., 2021), respectively. While their method is based on the whitebox attack, they proposed to use knowledge distillation to extend to the blackbox attack. To this end, they trained a student network that mimics the target FR model. However, they did not provide any details (nor source code) about student network training, such as the structure of the student network, etc.\n\nDong et al. (2021) used a pre-trained StyleGAN to generate high-resolution face images in the blackbox attack against FR systems. They generated synthetic face images using pre-trained StyleGAN and extracted their embedding. Then, they trained a fully connected network using mean squared error to map extracted embeddings to the corresponding noise in the input of StyleGAN. Instead of a learning-based approach, Vendrow & Vendrow (2021) used a grid search optimization using the simulated annealing (Van Laarhoven & Aarts, 1987) approach to find the noise in the input of StyleGAN, which generates an image that has the same embedding. As their iterative method has a large computation cost, they evaluated their method on 20 images only. Along the same lines, Dong et al. (2022) also tried to solve a similar optimization to (Vendrow & Vendrow, 2021) with a different approach. They used the genetic algorithm to find the noise in the input of StyleGAN that can generate an image with the same embedding.\n\nCompared to most works in the literature that generate low-resolution face images, our proposed method generates high-resolution realistic face images. While low-resolution reconstructed images\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ncan be used for evaluating the vulnerability of FR systems under some assumptions, high-resolution images can lead to different types of presentation attacks against FR systems. We also propose our method for both whitebox and blackbox scenarios and evaluate the transferability of our attack. Similar to (Cole et al., 2017; Duong et al., 2020; Truong et al., 2022), our method is based on the whitebox knowledge of FR model, however our approach for extending our method to the blackbox attack using another FR model is novel. Last but not least, we define five different attacks against FR systems and evaluate the vulnerability of SOTA FR models to our attacks.\n\n4 EXPERIMENTS\n\nIn this section, we present our experiments and discuss our results. First, in Section 4.1 we describe our experimental setup. Then, we present our experimental results in Section 4.2 and discuss our findings.\n\n4.1 EXPERIMENTAL SETUP\n\nmodel\n\nMOBIO\n\nTable 2: Recognition performance of face recognition models used in our experiments in terms of true match rate (TMR) at the thresholds correspond to false match rates (FMRs) of 10−2 and 10−3 evaluated on the MOBIO and LFW datasets. The values are in percentage.\n\nTo evaluate the performance of our method, we consider two SOTA FR models, including ArcFace (Deng et al., 2019), ElasticFace (Boutros et al., 2022), as the models from which templates are leaked (i.e., Fdatabase). For transferability evaluation, we also use three different FR models with SOTA backbones from FaceX-Zoo (Wang et al., 2021), including HRNet (Wang et al., 2020), AttentionNet (Wang et al., 2017), and Swin (Liu et al., 2021), for the target FR system (i.e., Ftarget). The recognition performance of these models are reported in Table 2. All these models are trained on MS-Celeb1M dataset (Guo et al., 2016). We assume that the adversary does not have access to the FR training dataset, and therefore we use another dataset for training our face reconstruction models. To this end, we use the Flickr-Faces-HQ (FFHQ) dataset (Karras et al., 2019), which consists of 70,000 high-resolution (i.e., 1024 × 1024) face images (without identity labels) crawled from the internet. We use 90% random portion of this dataset for training, and the remaining 10% for validation.\n\nArcFace ElasticFace HRNet AttentionNet Swin\n\nFMR=10−2 FMR=10−3 FMR=10−2 FMR=10−3\n\n99.98 100.00 98.23 97.73 98.98\n\n100.00 100.00 98.98 99.71 99.75\n\n96.40 94.70 78.43 72.77 87.83\n\n97.60 96.87 89.30 84.27 91.70\n\nLFW\n\nTo evaluate different attacks against FR systems, we consider two other face image datasets with identity labels, including the MOBIO (McCool et al., 2013) and Labeled Faces in the Wild (LFW) (Huang et al., 2007) datasets. The MOBIO dataset consists of bi-modal (face and voice) data captured using mobile devices from 150 people in 12 sessions (6-11 samples in each session). The LFW dataset includes 13,233 face images of 5,749 people collected from the internet, where 1,680 people have two or more images.\n\nFor each of the attacks described in Section 2, we build one or two separate FR systems with one or two SOTA FR models based on the attack type. If the target system is the same as the system from which the template is leaked, we have only one FR system. Otherwise, if the target system is different the system from which the template is leaked, we have two FR systems with two different feature extractors. In each case, we use one of our evaluation datasets (i.e., MOBIO and LFW) to build both FR systems (so that the subject with the leaked template be enrolled in the target system too). In each evaluation, we assume that the target FR system is configured at the threshold corresponding to a false match rate (FMR) of 10−3, and we evaluate the adversary’s success attack rate (SAR) in entering that system.\n\nWe should note that the templates extracted by the aforementioned FR models have 512 dimensions. The input noise z ∈ Z to the mapping network of StyleGAN’s pre-trained network is from the standard normal distribution and has 512 dimensions. The input noise n ∈ N to our mapping network Mrec(.) is with dimension of 8 and also from the standard normal distribution. We also use Adam (Kingma & Ba, 2015) optimizer to train our mapping network.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Evaluation of attacks with whitebox knowledge of the system from which the template is leaked (i.e., Floss = Fdatabase) against SOTA FR models in terms of adversary’s success attack rate (SAR) using our proposed method on the MOBIO and LFW datasets. The values are in percentage and correspond to the threshold where the target system has FMR = 10−3. Cells are color coded according the type of attack as defined in Section 2 for attack 1 ( light gray ) and attack 2 ( dark gray ).\n\nFdatabase\n\nArcFace ElasticFace HRNet AttentionNet Swin ArcFace ElasticFace HRNet AttentionNet Swin\n\nMOBIO\n\nLFW\n\nArcFace ElasticFace\n\n92.38 78.10\n\n81.90 87.62\n\n71.43 64.29\n\n70.48 64.76\n\n74.29 69.05\n\n86.82 78.25\n\n74.20 82.52\n\n36.57 41.80\n\n36.40 40.25\n\n58.86 61.09\n\nTable 4: Evaluation of attacks (with blackbox knowledge of the system from which the template is leaked i.e., Fdatabase) against SOTA FR models in terms of adversary’s success attack rate (SAR) using different methods on the MOBIO and LFW datasets. The values are in percentage and correspond to the threshold where the target system has FMR = 10−3. M1: NbNetB-M (Mai et al., 2018), M2: NbNetB-P (Mai et al., 2018), M3: (Dong et al., 2021), and M4: (Vendrow & Vendrow, 2021). Cells are color coded according the type of attack as defined in Section 2 for attack 3 ( lightest gray ),\n\nattack 4 ( middle dark gray ), and attack 5 ( darkest gray ).\n\nFdatabase\n\nFloss\n\nFtarget\n\nArcFace\n\nElasticFace\n\nElasticFace\n\nArcFace\n\nArcFace ElasticFace HRNet AttentionNet Swin\n\nArcFace ElasticFace HRNet AttentionNet Swin\n\nMOBIO\n\nM1 M2 M3 M4\n\n1.90 1.43 0.95 0\n1.43\n\n2.38 3.81 0.48 1.90 0.95\n\n15.24 11.43 6.19 6.67 13.33\n\n18.57 43.81 20.00 18.10 26.19\n\n2.38 4.29 2.86 3.33 3.81\n\n2.86 4.76 1.43 3.33 2.86\n\n28.10 15.24 10.00 4.29 10.95\n\n16.19 43.33 10.48 9.05 15.24\n\nOurs\n\n81.90 73.81 57.14 54.29 67.14\n\n87.14 89.05 73.81 71.90 75.24\n\nLFW M1 M2 M3 M4 Ours\n\n10.68 8.36 1.30 1.33 4.27\n\n15.33 21.44 3.46 2.89 9.22\n\n40.25 34.39 7.78 7.17 23.85\n\n48.67 58.16 18.36 16.31 38.79\n\n12.91 6.35 1.75 2.29 5.97\n\n11.81 11.59 2.74 2.91 8.26\n\n58.88 29.10 9.20 9.17 21.75\n\n37.45 52.88 11.82 10.95 24.62\n\n77.16 68.06 28.45 28.87 48.28\n\n83.20 83.43 49.02 46.63 66.89\n\n4.2 ANALYZE\n\nIn this section, we consider SOTA FR models and evaluate the performance of our face reconstruction method in five different attacks described in Section 2. We also explore the effect of our WGAN traning as well as effect of loss terms as our ablation study.\n\nWhitebox knowledge of Fdatabase For attacks 1-2, the adversary is assumed to have whitebox knowledge of the system from which the template is leaked (i.e., Fdatabase) and use the same feature extraction model for training (i.e., Floss), thus in such cases Floss = Fdatabase. We considered ArcFace and ElasticFace models and reconstructed face images from the templates extracted by these models in attacks against different FR systems. Table 3 reports the vulnerability of different target systems to our attacks1 1-2 in terms of adversary’s SAR at the system’s FMR of 10−3. Similar results for the system’s FMR of 10−2 are reported in Table 6 of Appendix. According to these tables, our method achieves considerable SAR against ArcFace and ElasticFace target systems in attack 1. In attack 2, we observe that there is a degradation in SAR with respect to attack 1. However, the reconstructed face images can still be used to enter another target system. Meanwhile, the FR model with a higher recognition accuracy is generally more vulnerable to attack 2. For instance, when ArcFace is considered as Fdatabase, we observe that ElasticFace and Swin have the highest SAR as target systems, while there is the same order for their recognition performance in Table 2.\n\nBlackbox knowledge of Fdatabase For attacks 3-5, the adversary is assumed to have blackbox knowledge of the system from which the template is leaked (i.e., Fdatabase) and use another feature extraction model for training (i.e., Floss), therefore in such cases Floss (cid:54)= Fdatabase. Table 4 compares\n\n1We should highlight that since there is no whitebox method in the literature with available source code (as\n\nmentioned in Table 1), we could not compare our proposed method with other whitebox methods.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nthe performance of our method with blackbox methods2 in the literature (Mai et al., 2018; Dong et al., 2021; Vendrow & Vendrow, 2021) for attacks 3-5 in terms of adversary’s SAR at system’s FMR of 10−3. Similar results for the system’s FMR of 10−2 are available in Table 7 of Appendix.\n\nAs these tables show, our proposed method achieves the highest SAR compared to (Mai et al., 2018; Dong et al., 2021; Vendrow & Vendrow, 2021) against FR systems on the MOBIO and LFW datasets. In particular, in attack 5 which is the hardest attack, where Fdatabase, Floss, and Ftarget are different, the results show that the target FR system is still vulnerable to our attack. The results of our method for attack 5 also show transferability of our attack to different FR systems. Similar to attack 2, we can also observe that in attack 5, the FR model with a higher recognition accuracy is generally more vulnerable to our attack. Fig. 4 also shows sample face images from the LFW dataset and the reconstructed images using our proposed method from ArcFace templates in different attacks. We should highlight that as show in Fig. 4, the reconstructed face images in attack 1 and attack 2 are the same, but they are used to enter different target FR system. The same holds for the reconstructed face images in attacks 3-5.\n\nAblation Study To evaluate the effect of WGAN in training our mapping network and the effect of each term in our loss function (i.e., Eq. 1), we consider the ArcFace model in the whitebox scenario and train different face reconstruction networks with different loss functions. Then, we attack a system with the ArcFace model as a feature extractor (i.e., attack 1) and compare the SARs as reported in Table 5. According to these results, the proposed adversarial training has a significant effect on our face reconstruction method. In other words, the WGAN framework helps our mapping network to learn the distribution of StyleGAN’s intermediate latent space to generate face-like images. When we use the WGAN training and based on the results in Table 5, the ID loss has a high impact on the performance of the template inversion model. While the pixel loss by itself does not achieve a good performance, it improves the performance of ID loss in our reconstruction loss function in Eq. 1. This table confirms that the proposed WGAN training and our reconstruction loss function lead to a more successful attack.\n\nl a\nn\n\ni\n\ng\n\ni r\n\nO\n\n2 -\n1\n\ns k\nc a\nt t\n\nA\n\n5 -\n3\n\ns k\nc a\nt t\n\nA\n\n0.723\n\n0.682\n\n0.704\n\n0.702\n\n0.650\n\n0.662\n\nFigure 4: Sample face images from the LFW dataset (first raw) and their corresponding reconstructed images using our template inversion method from ArcFace templates in different attacks, attacks 1-2 (second raw) and attacks 3-5 (second raw, using ElasticFace for Floss). The values below each image show the cosine similarity between the corresponding ArcFace templates of original and reconstructed face images.\n\nl a\nn\n\ni\n\ng\n\ni r\n\nO\n\nd e\nt c\nu r\nt s\nn o\nc e\nR\n\n0.098\n\n0.126\n\n0.163\n\nFigure 5: Sample failure cases images from the LFW dataset and their corresponding reconstructed images using our template inversion method from ArcFace templates in the attack 3 (using ElasticFace for Floss). The values below each image show the cosine similarity between the corresponding templates of original and reconstructed face images.\n\nLimitations Despite the significant performance of our method in terms of success attack rate in all types of attacks reported in Table 3 and Table 4, the reconstructed face images fail to enter the system in some cases. Fig. 5 illustrates sam-\n\n2The other blackbox methods in the literature do not have available source code and we could not reproduce\n\ntheir results.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Evaluating the effect of each loss term in our loss function in attack 1 against ArcFace in terms of SAR in the system with FMRs of 10−2 and 10−3 evaluated on the MOBIO and LFW datasets. The values are in percentage.\n\nWGAN training (Eqs. 4 and 5)\n\nReconstruction Loss Function\n\nMOBIO FMR=10−2 FMR=10−3\n\nLFW\n\nFMR=10−2\n\nFMR=10−3\n\n(cid:51)\n\n(cid:55)\n\nLrec = Lpixel + LID Lrec = LID Lrec = Lpixel Lrec = Lpixel + LID Lrec = LID Lrec = Lpixel\n\n100.00 98.10 0\n0 0\n0\n\n92.38 82.38 0\n0 0\n0\n\n93.64 90.56 0.65 0.32 0.14 0.44\n\n86.82 80.74 0.07 0.02 0.02 0.09\n\nple failure cases in the attack 3 against ArcFace (using ElasticFace for Floss) on the LFW dataset. From the failure cases, we can conclude that there is a bias in the face reconstruction for specific demographies, like elderly or dark skin people. Indeed, such kind of bias in the reconstructed face images is caused by inherent biases in datasets used to train FR model, the StyleGAN model, and our mapping network in our face reconstruction model3.\n\n5 CONCLUSION\n\nIn this paper, we proposed a new method to reconstruct high-resolution realistic face images from facial templates in a FR system. We used a pre-trained StyleGAN3 network and learned a mapping from facial templates to intermediate latent space of StyleGAN within a GAN-based framework. We proposed our method for whitebox and blackbox scenarios. In the whitebox scenario, the adversary can use the feature extraction model for training the face reconstruction network; however, in the blackbox scenario, we assume that the adversary has access to another feature extraction model. In addition, we consider the threat model where the adversary might impersonate in the same or another (i.e., transferable attack) FR system. Based on the adversary’s knowledge of the feature extraction model and the target FR system, we defined five different attacks and evaluated the vulnerability of SOTA FR systems to our proposed method. Our experiments showed that the reconstructed face images by our proposed method not only can achieve a high SAR in whitebox and blackbox scenarios, but also are transferable and can be used to enter target FR systems with a different FR model.\n\nETHICS STATEMENT\n\nMotivations The proposed face reconstruction method is presented with the motivation of showing vulnerability of face recognition systems to template inversion attacks. We hope this work encourages researcher of the community to investigate the next generation of safe and robust face recognition systems and to develop new algorithms to protect existing systems.\n\nConsiderations While the proposed method might pose a social threat against unprotected systems, we do not condone using our work with the intent of attacking a real face recognition system or other malicious purposes. The authors also acknowledge a potential lack of diversity in the reconstructed face images, stemming from inherent biases of datasets used in our experiments.\n\nMitigation of such attacks This paper demonstrates an important privacy and security threat to the state-of-the-art unprotected face recognition systems. Along the same lines, data protection frameworks, such as the European Union General Data Protection Regulation (EU-GDPR) (European Council, 2016), put legal obligations to protect biometric data as sensitive information. To this end and to prevent such attacks to face recognition systems, several biometric template protection algorithms are proposed in the literature (Nandakumar & Jain, 2015; Sandhya & Prasad, 2017; Kaur et al., 2022; Kumar et al., 2020).\n\n3The biases for different demographies in verification task for ArcFace model are studied in (de Freitas Pereira & Marcel, 2021). Similarly, biases in StyleGAN generated images and also the FFHQ dataset (i.e., our training dataset) are investigated in (Karakas et al., 2022; Tan et al., 2020; Balakrishnan et al., 2020).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nIn our experiments, we use PyTorch package and trained our models on a system equipped with an NVIDIA GeForce RTXTM 3090. We use the pre-trained model of StyleGAN34 to generate 1024 × 1024 high-resolution images. The source code of our experiments is publicly available to help reproduce our results5.\n\nREFERENCES\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks.\n\nIn International conference on machine learning, pp. 214–223. PMLR, 2017.\n\nGuha Balakrishnan, Yuanjun Xiong, Wei Xia, and Pietro Perona. Towards causal benchmarking of bias in face analysis algorithms. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 547–563. Springer, 2020.\n\nBattista Biggio, Paolo Russu, Luca Didaci, Fabio Roli, et al. Adversarial biometric recognition: A review on biometric system security from the adversarial machine-learning perspective. IEEE Signal Processing Magazine, 32(5):31–41, 2015.\n\nFadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper. Elasticface: Elastic margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1578–1587, 2022.\n\nPeter J Burt and Edward H Adelson. The laplacian pyramid as a compact image code. In Readings\n\nin computer vision, pp. 671–679. Elsevier, 1987.\n\nForrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, and William T Freeman. Synthesizing normalized faces from facial identity features. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3703–3712, 2017.\n\nTiago de Freitas Pereira and S ́ebastien Marcel. Fairness in biometrics: a figure of merit to assess biometric verification systems. IEEE Transactions on Biometrics, Behavior, and Identity Science, 4(1):19–29, 2021.\n\nJiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nXingbo Dong, Zhe Jin, Zhenhua Guo, and Andrew Beng Jin Teoh. Towards generating high definition face images from deep templates. In 2021 International Conference of the Biometrics Special Interest Group (BIOSIG), pp. 1–11. IEEE, 2021.\n\nXingbo Dong, Zhihui Miao, Lan Ma, Jiajun Shen, Zhe Jin, Zhenhua Guo, and Andrew Beng Jin Teoh. Reconstruct face from features using gan generator as a distribution constraint. arXiv preprint arXiv:2206.04295, 2022.\n\nChi Nhan Duong, Thanh-Dat Truong, Khoa Luu, Kha Gia Quach, Hung Bui, and Kaushik Roy. Vec2face: Unveil human faces from their blackbox features in face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6132–6141, 2020.\n\nEuropean Council. Regulation of the european parliament and of the council on the protection of individuals with regard to the processing of personal data and on the free movement of such data (general data protection regulation), April 2016.\n\nJavier Galbally, S ́ebastien Marcel, and Julian Fierrez. Biometric antispoofing methods: A survey in\n\nface recognition. IEEE Access, 2:1530–1552, 2014.\n\n4Available at https://github.com/NVlabs/stylegan3 5The source code will be available upon acceptance of the paper.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In European conference on computer vision, pp. 87–102. Springer, 2016.\n\nAbdenour Hadid, Nicholas Evans, Sebastien Marcel, and Julian Fierrez. Biometrics systems unIEEE Signal Processing\n\nder spoofing attack: an evaluation methodology and lessons learned. Magazine, 32(5):20–30, 2015.\n\nGary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.\n\nYifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make one strong gan, and that can scale up. Advances in Neural Information Processing Systems, 34: 14745–14758, 2021.\n\nCemre Efe Karakas, Alara Dirik, Eyl ̈ul Yalc ̧ınkaya, and Pinar Yanardag. Fairstyle: Debiasing stylegan2 with style channel manipulations. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 570–586. Springer, 2022.\n\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-\n\nproved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019.\n\nTero Karras, Miika Aittala, Samuli Laine, Erik H ̈ark ̈onen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nPrabhjot Kaur, Nitin Kumar, and Maheep Singh. Biometric cryptosystems: a comprehensive survey.\n\nMultimedia Tools and Applications, pp. 1–56, 2022.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), San Diego, California., USA, May 2015.\n\nNitin Kumar et al. Cancelable biometrics: a comprehensive survey. Artificial Intelligence Review,\n\n53(5):3403–3446, 2020.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.\n\nGuangcan Mai, Kai Cao, Pong C Yuen, and Anil K Jain. On the reconstruction of face images from deep face templates. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(5): 1188–1202, 2018.\n\nS ́ebastien Marcel, Mark S Nixon, and Stan Z Li. Handbook of biometric anti-spoofing, volume 1.\n\nSpringer, 2014.\n\nChris McCool, Roy Wallace, Mitchell McLaren, Laurent El Shafey, and S ́ebastien Marcel. Session IET Biometrics, 2(3):117–129, September 2013.\n\nvariability modelling for face authentication. ISSN 2047-4938. doi: 10.1049/iet-bmt.2012.0059.\n\nKarthik Nandakumar and Anil K Jain. Biometric template protection: Bridging the performance\n\ngap between theory and practice. IEEE Signal Processing Magazine, 32(5):88–100, 2015.\n\nMulagala Sandhya and Munaga VNK Prasad. Biometric template protection: A systematic literature review of approaches and modalities. In Biometric Security and Privacy, pp. 323–370. Springer, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nShuhan Tan, Yujun Shen, and Bolei Zhou. Improving the fairness of deep generative models without\n\nretraining. arXiv preprint arXiv:2012.04842, 2020.\n\nThanh-Dat Truong, Chi Nhan Duong, Ngan Le, Marios Savvides, and Khoa Luu. Vec2face-v2: Unveil human faces from their blackbox features via attention-based network in face recognition, 2022. URL https://arxiv.org/abs/2209.04920.\n\nPeter JM Van Laarhoven and Emile HL Aarts. Simulated annealing. In Simulated annealing: Theory\n\nand applications, pp. 7–15. Springer, 1987.\n\nEdward Vendrow and Joshua Vendrow. Realistic face reconstruction from deep embeddings.\n\nIn\n\nNeurIPS 2021 Workshop Privacy in Machine Learning, 2021.\n\nFei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3156–3164, 2017.\n\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 2020.\n\nJun Wang, Yinglu Liu, Yibo Hu, Hailin Shi, and Tao Mei. Facex-zoo: A pytorch toolbox for face recognition. In Proceedings of the 29th ACM international conference on Multimedia, 2021.\n\nAndrey Zhmoginov and Mark Sandler. Inverting face embeddings with convolutional neural net-\n\nworks. arXiv preprint arXiv:1606.04189, 2016.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nTable 6: Evaluation of attacks with whitebox knowledge of the system from which the template is leaked (i.e., Floss = Fdatabase) against SOTA FR models in terms of adversary’s success attack rate (SAR) using our proposed method on the MOBIO and LFW datasets. The values are in percentage and correspond to the threshold where the target system has FMR= 10−2. Cells are color coded according the type of attack as defined in Section 2 for attack 1 ( light gray ) and attack 2 ( dark gray ).\n\nFdatabase\n\nArcFace ElasticFace HRNet AttentionNet Swin ArcFace ElasticFace HRNet AttentionNet Swin\n\nMOBIO\n\nLFW\n\nArcFace ElasticFace\n\n100.00 90.95\n\n93.81 93.33\n\n80.00 78.57\n\n81.90 83.81\n\n85.24 84.29\n\n93.64 87.88\n\n90.89 92.80\n\n68.08 71.82\n\n62.75 64.24\n\n76.24 75.70\n\nTable 7: Evaluation of attacks (with blackbox knowledge of the system from which the template is leaked i.e., Fdatabase) against SOTA FR models in terms of adversary’s success attack rate (SAR) using different methods on the MOBIO and LFW datasets. The values are in percentage and correspond to the threshold where the target system has FMR= 10−2. M1: NbNetB-M (Mai et al., 2018), M2: NbNetB-P (Mai et al., 2018), M3: (Dong et al., 2021), and M4: (Vendrow & Vendrow, 2021). Cells are color coded according the type of attack as defined in Section 2 for attack 3 ( lightest gray ), attack 4 ( middle dark gray ), and attack 5 ( darkest gray ).\n\nFdatabase\n\nFloss\n\nFtarget\n\nMOBIO M1 M2 M3 M4 Ours\n\nLFW M1 M2 M3 M4 Ours\n\nArcFace\n\nElasticFace\n\nElasticFace\n\nArcFace\n\nArcFace ElasticFace HRNet AttentionNet Swin\n\nArcFace ElasticFace HRNet AttentionNet Swin\n\n26.67 11.90 10.48 11.43 10.48\n\n17.14 30.00 8.10 12.86 10.00\n\n49.05 49.52 24.76 38.10 45.24\n\n49.05 70.95 47.14 47.14 54.76\n\n20.48 16.19 10.00 18.10 10.95\n\n20.95 25.7 15.24 23.43 13.81\n\n67.14 34.29 26.19 24.29 29.52\n\n47.14 75.24 31.43 40.95 37.14\n\n89.52 86.67 79.05 80.48 82.86\n\n95.24 94.76 83.81 87.14 89.05\n\n26.66 32.42 18.69 10.84 14.79\n\n33.08 52.99 29.27 18.53 24.50\n\n61.66 66.61 43.21 31.88 45.80\n\n67.89 81.74 60.34 46.36 60.19\n\n28.31 23.05 17.37 13.31 16.98\n\n26.35 33.53 23.22 17.78 21.40\n\n76.98 57.84 33.55 26.73 38.03\n\n57.48 79.62 39.06 31.53 41.13\n\n87.85 87.43 60.93 53.86 67.80\n\n91.23 93.34 76.68 69.45 80.15\n\n13",
    "reference": "# Summary Of The Paper\n\nThe authors of this paper propose  new method to reconstruct high-resolution realistic face images from\nfacial templates in a FR system. They focus on the template inversion attack against face recognition systems.\n\n# Strength And Weaknesses\n\nStrength\n- The motivation is very clear.\n-  It is well written and the experimentation seems correct.\n- Five different attacks and evaluated the vulnerability of SOTA FR systems to the proposed method are defined.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The novelty and scope appear limited and incremental. The main contribution in the paper is in using results in StyleGAN in  FR systems.\n- The authors promise to make publicly available the experimentation code.\n\n# Summary Of The Review\n\nThe paper is clearly written and easy to read and understand. But the novelty and scope appear limited.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nThe authors should explain how to mitigate the potential issues deriving from the proposed attack method."
  },
  {
    "input": "POPGYM: BENCHMARKING PARTIALLY OBSERVABLE REINFORCEMENT LEARNING\n\nSteven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, Amanda Prorok\n\nABSTRACT\n\nReal world applications of Reinforcement Learning (RL) are often partially observable, thus requiring memory. Despite this, partial observability is still largely ignored by contemporary RL benchmarks and libraries. We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 15 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines – the most in a single RL library. Existing partially observable benchmarks tend to fixate on 3D visual navigation, which is computationally expensive and only one type of POMDP. In contrast, POPGym environments are diverse, produce smaller observations, use less memory, and often converge within two hours of training on a consumer-grade GPU. We implement our high-level memory API and memory baselines on top of the popular RLlib framework, providing plug-andplay compatibility with various training algorithms, exploration strategies, and distributed training paradigms. Using POPGym, we execute the largest comparison across RL memory models to date. POPGym is available at https: //github.com/proroklab/popgym.\n\n1\n\nINTRODUCTION\n\nDatasets like MNIST (Lecun et al., 1998) have driven advances in Machine Learning (ML) as much as new architectural designs (Levine et al., 2020). Comprehensive datasets are paramount in assessing the progress of learning algorithms and highlighting shortcomings of current methodologies. This is evident in the context of RL, where the absence of fast and comprehensive benchmarks resulted in a reproducability crisis (Henderson et al., 2018). Large collections of diverse environments, like the Arcade Learning Environment, OpenAI Gym, ProcGen, and DMLab provide a reliable measure of progress in deep RL. These fundamental benchmarks play a role in RL equivalent to that of MNIST in supervised learning (SL).\n\nThe vast majority of today’s RL benchmarks are designed around Markov Decision Processes In MDPs, agents observe a Markov state, which contains all necessary information to (MDPs). solve the task at hand. When the observations are Markov states, the Markov property is satisfied, and traditional RL approaches guarantee convergence to an optimal policy (Sutton & Barto, 2018, Chapter 3). But in many RL applications, observations are ambiguous, incomplete, or noisy – any of which makes the MDP partially observable (POMDP) (Kaelbling et al., 1998), breaking the Markov property and all convergence guarantees. Furthermore, Ghosh et al. (2021) find that policies trained under the ideal MDP framework cannot generalize to real-world conditions when deployed, with epistemic uncertainty turning real-world MDPs into POMDPs. By introducing memory (referred to as sequence to sequence models in SL), we can summarize the observations1 therefore restoring policy convergence guarantees for POMDPs (Sutton & Barto, 2018, Chapter 17.3).\n\nDespite the importance of memory in RL, most of today’s comprehensive benchmarks are fully observable or near-fully observable. Existing partially observable benchmarks are often navigationbased – representing only spatial POMDPs, and ignoring applications like policymaking, disease diagnosis, teaching, and ecology (Cassandra, 1998). The state of memory-based models in RL libraries is even more dire – we are not aware of any RL libraries that implement more than three\n\n1Strictly speaking, the agent actions are also required to guarantee convergence. We consider the previous\n\naction as part of the current observation.\n\n1\n\n(a) Stateless Cartpole Stateless Pendulum\n\nand\n\n(b) Battleship, Concentration, Higher Lower and Mine Sweeper\n\n(c) Labyrinth Escape and Explore\n\nFigure 1: Renders from select POPGym environments.\n\nor four distinct memory baselines. In nearly all cases, these memory models are limited to frame stacking and LSTM.\n\nTo date, there are no popular RL libraries that provide a diverse selection of memory models. Of the few existing POMDP benchmarks, even fewer are comprehensive and diverse. As a consequence, there are no large-scale studies comparing memory models in RL. We propose to fill these three gaps with our proposed POPGym.\n\n1.1 CONTRIBUTIONS\n\nPOPGym is a collection of 15 partially observable gym environments (Figure 1) and 13 memory baselines. All environments come with at least three difficulty settings and randomly generate levels to prevent overfitting. The POPGym environments use low-dimensional observations, making them fast and memory efficient. Many of our baseline models converge in under two hours of training on a single consumer-grade GPU ( Table 1, Figure 2). The POPGym memory baselines utilize a simple API built on top of the popular RLlib library (Liang et al., 2018), seamlessly integrating memory models with an assortment of RL algorithms, sampling, exploration strategies, logging frameworks, and distributed training methodologies. Utilizing POPGym and its memory baselines, we execute a large-scale evaluation, analyzing the capabilities of memory models on a wide range of tasks. To summarize, we contribute:\n\n1. A comprehensive collection of diverse POMDP tasks.\n\n2. The largest collection of memory baseline implementations in an RL library.\n\n3. A large-scale, principled comparison across memory models.\n\n2 RELATED WORK\n\nThere are many existing RL benchmarks, which we categorize as fully (or near-fully) observable and partially observable. In near-fully observable environments, large portions of the the Markov state are visible in an observation, though some information may be missing. We limit our literature review to comprehensive benchmarks (those that contain a wide set of tasks), as environment diversity is essential for the accurate evaluation of RL agents (Cobbe et al., 2020).\n\n2.1 FULLY AND NEAR-FULLY OBSERVABLE BENCHMARKS\n\nThe Arcade Learning Environment (ALE) (Bellemare et al., 2013) wraps Atari 2600 ROMs in a Python interface. Most of the Atari games, such as Space Invaders or Missile Command are fully observable (Cobbe et al., 2020). Some games like asteroids require velocity observations, but models can recover full observability by stacking four consecutive observations (Mnih et al., 2015), an approach that does not scale for longer timespans. Even seemingly partially-observable multi-room games like Montezuma’s Revenge are made near-fully observable by displaying the player’s score and inventory (Burda et al., 2022).\n\nOpenAI Gym (Brockman et al., 2016) came after ALE, implementing classic fully observable RL benchmarks like CartPole and MountainCar. Their Gym API found use in many other environments, including our proposed benchmark.\n\n2\n\nCobbe et al. (2020) find that randomly generated environments are critical to training general agents, showing policies will overfit to specific levels otherwise. They propose ProcGen: 16 procedurally generated environments with pixel-space observations. Most environments are fully or near-fully observable, although a few environments provide a partially observable mode, effectively turning them into 2D area coverage (navigation) tasks. ProcGen motivates POPGym’s use of random level generation.\n\n2.2 PARTIALLY OBSERVABLE BENCHMARKS\n\nWhen enumerating partially observable benchmarks, we find many are based on 3D first-person navigation. DeepMind Lab (Beattie et al., 2016) (DMLab) is a 3D first-person view navigation simulator based on the Quake 3 physics engine. It implements various tasks such as collecting fruits, maze exploration, and laser tag. VizDoom (Kempka et al., 2016) is another 3D navigation simulator based on the PC game Doom. It gives the agent weapons and adds computer-controlled characters that can shoot at the player. Miniworld (Chevalier-Boisvert, 2018) is a third 3D first-person view navigation simulator that is easier to install than DMLab or VizDoom. MiniGrid (Chevalier-Boisvert et al., 2018) and GridVerse (Baisero & Katt, 2021) are 2D navigation simulators with a first-person view. Unlike the previously mentioned 3D simulators, agents converge on gridworld environments much faster due to the smaller observation space. This makes it a popular benchmark for memory models.\n\nThere are few POMDP libraries that provide tasks beyond navigation. Behaviour suite (BSuite) evaluates agents on a variety of axes, one of which is memory (Osband et al., 2020), but they only provide two POMDPs. Similar to our benchmark, (Zheng & Tellex, 2020) provide classic POMDPs with low-dimensional observation spaces. But their tasks are solvable without neural networks and are not difficult enough for modern deep RL. Ni et al. (2022) provide 21 environments, most of which are a special case of POMDP known as latent MDPs (Kwon et al., 2021), where a specific MDP is chosen from a set of possible MDPs at the beginning of an episode. (Morad et al., 2022) provides three POMDPs, which is insufficient for a benchmark.\n\nWe briefly mention the Starcraft (Samvelyan et al., 2019) and VMAS (Bettini et al., 2022) benchmarks because multi-agent environments are intrinsically partially observable, but we focus specifically on single-agent POMDPs.\n\n2.3 SHORTCOMINGS OF CURRENT BENCHMARKS\n\nPopular fully observable benchmarks use pixel-based observation spaces, adding a layer of complexity that takes an order of magnitude longer to train when compared against state-based observation counterparts (Seita, 2020). In fully observable environments, visually pleasing results are worth a few extra hours training. This dogma persists into partial observability, where environments often take 10x longer to converge than their fully observable counterparts. Popular benchmarks using 3D graphics take hundreds of billions of timesteps (Parisotto et al., 2020) and multiple weeks (Morad et al., 2021) on a GPU to train a single policy to convergence. Until sample efficiency in partially observable RL improves, we must forgo pixel-based observations or continue to struggle with reproducibility.\n\nMany partially observable tasks with pixel-based observation spaces are based on some form of navigation (Ramani, 2019). Although navigation can be a partially observable task, wall following behavior in perfect mazes guarantees complete area coverage without the need for memory. When mazes are imperfect (i.e. contain cycles), deterministic wall following can get stuck in infinite loops. However, RL policies often have some amount of stochasticity that can break out of these loops. Kadian et al. (2020) and Morad et al. (2021) inadvertently show that memory-free navigation agents learn wall following strategies2 that are surprisingly effective in imperfect real-world mazes. We confirm these findings with our experiments, showing that memory-free agents are competitive with memory-endowed agents in certain navigation benchmarks.\n\nAll other (imperfect) mazes can be fully explored by storing no more than two past locations (observations) in memory (Blum & Kozen, 1978). Navigation-based tasks like area coverage, moving to a coordinate, or searching for items can be reduced to the maze exploration task. We do not claim\n\n2https://en.wikipedia.org/wiki/Maze-solving_algorithm#Wall_follower\n\n3\n\nthat navigation tasks are easy, but rather that it is important to have a variety of tasks to ensure we evaluate all facets of memory, such as memory capacity, that navigation tasks might miss.\n\n2.4 EXISTING MEMORY BASELINES\n\nThe state of memory models in RL is even more bleak than the benchmarks. Most libraries provide frame stacking and a single type of RNN. OpenAI Baselines (Dhariwal et al., 2017), StableBaselines3 (Raffin et al., 2021), and CleanRL (Huang et al., 2021) provide implementations of PPO with frame stacking and an LSTM. Ray RLlib (Liang et al., 2018) currently implements frame stacking, LSTM, and a transformer for some algorithms. Ni et al. (2022) implement LSTM, GRUs, and two model-based memory models. Yang & Nguyen (2021) provides recurrent versions of the DDPG, TD3, and SAC RL algorithms, which utilize GRUs and LSTM. Zheng & Tellex (2020) implement multiple classical POMDP solvers, but these do not use neural networks, preventing their application to more complex tasks. There is currently no go-to library for users who want to compare or apply non-standard memory models.\n\n2.5 A BRIEF SUMMARY ON MEMORY\n\nWhen designing a library of memory models, it is important to select competitive models. Ni et al. (2022) show that sequence to sequence models from SL are competitive or better than RL-specific memory methods while being more straightforward to implement, so we focus specifically on sequence to sequence models (called memory throughout the paper). Although a strict categorization of memory is elusive, most methods are based on RNNs, attention, or convolution.\n\nRNNs (Elman, 1990) take an input and hidden state, feed them through a network, and produce a corresponding output and hidden state. RNNs depend on the previous state and must be executed sequentially, resulting in slow training but fast inference when compared with other methods.\n\nAttention-based methods (Vaswani et al., 2017) have supplanted RNNs in many applications of SL, but traditional transformers have quadratically-scaling memory requirements, preventing them from running on long episodes in RL. Recent linear attention formulations (Schlag et al., 2021; Katharopoulos et al., 2020) claim to produce transformer-level performance in linear time and space, potentially enabling widespread use of attention in RL.\n\nLike attention, convolutional methods are computationally efficient (Bai et al., 2018), lending themselves well to RL. They are less common than recurrent or attention-based methods in SL, and there is little literature on their use in RL.\n\n3 POPGYM ENVIRONMENTS\n\nAll of our environments bound the cumulative episodic reward in [−1, 1]. In some cases (e.g. repeating previous observations) an optimal policy would receive a cumulative reward of one in expectation. In other environments (e.g. playing battleship with randomly placed ships), an optimal policy has an expected episodic cumulative reward of less than one.\n\nWe tag our proposed environments as diagnostic, control, noisy, game, and navigation. Each tag is designed to represent a different class of POMDP, and each environment has at least three distinct difficulty settings, creating the most diverse POMDP benchmark thus far. Our proposed environments are all overcomplete POMDPs, meaning our environments have more unique latent Markov states than unique observations (Sharan et al., 2017; Jin et al., 2020).\n\nDiagnostic environments probe model capabilities with respect to the duration of memories, forgetting, and compression and recall. They are designed to quickly summarize the strengths and weaknesses of a specific model.\n\nControl environments are control RL environments made partially observable by removing part of the observation. Solving these tasks only requires short-term memory.\n\nNoisy environments require the memory model to estimate the true underlying state by computing an expectation over many observations. These are especially useful for real-world robotics tasks.\n\n4\n\nGame environments provide a more natural and thorough evaluation of memory through card and board games. They stress test memory capacity, duration, and higher-level reasoning.\n\nNavigation environments are common in other benchmarks, and we include a few to ensure our benchmark is comprehensive. More than anything, our navigation environments examine how memory fares over very long sequences.\n\n3.1 ENVIRONMENT DESCRIPTIONS\n\n1. Repeat First (Diagnostic): At the first timestep, the agent receives one of four values and a remember indicator. Then it randomly receives one of the four values at each successive timestep without the remember indicator. The agent receives a reward for outputting (remembering) the first value.\n\n2. Repeat Previous (Diagnostic): Like Repeat First, observations contain one of four values. The agent is rewarded for outputting the observation from some constant k timesteps ago, i.e. observation ot−k at time t.\n\n3. Autoencode (Diagnostic): During the WATCH phase, a deck of cards is shuffled and played in sequence to the agent with the watch indicator set. The watch indicator is unset at the last card in the sequence, where the agent must then output the sequence of cards in order. This tests whether the agent can encode a series of observations into a latent state, then decode the latent state one observation at a time.\n\n4. Stateless Cartpole (Control): The cartpole environment from Barto et al. (1983), but with the angular and linear positions removed from the observation. The agent must integrate to compute positions from velocity.\n\n5. Stateless Pendulum (Control): The swing-up pendulum (Doya, 1995), with the angular po-\n\nsition information removed.\n\n6. Noisy Stateless Cartpole (Control, Noisy): Stateless Cartpole (Env. 4) with Gaussian noise.\n\n7. Noisy Stateless Pendulum (Control, Noisy): Stateless Pendulum (Env. 5) with Gaussian\n\nnoise.\n\n8. Multiarmed Bandit (Noisy, Diagnostic): The multiarmed bandit problem (Slivkins & others, 2019; Lattimore & Szepesv ́ari, 2020) posed as an episodic task. Every episode, bandits are randomly initialized. Over the episode, the player must trade off exploration and exploitation, remembering which bandits pay best. Each bandit has some probability of paying out a positive reward, otherwise paying out a negative reward. Note that unlike the traditional multiarmed bandit task where the bandits are fixed once initialized, these bandits reset every episode, forcing the agent to learn a policy that can adapt between episodes.\n\n9. Higher Lower (Game, Noisy): Based on the card game higher-lower, the agent must guess if the next card is of higher or lower rank than the previous card. The next card is then flipped faceup and becomes the previous card. Using memory, the agent can utilize card counting strategies to predict the expected value of the next card, improving the return.\n\n10. Count Recall (Game, Diagnostic, Noisy): Each turn, the agent receives a next value and query value. The agent must answer the query with the number of occurrences of a specific value. In other words, the agent must store running counts of each unique observed value, and report a specific count back, based on the query value. This tests whether the agent can learn a compressed structured memory representation, such that it can continuously update portions of memory over a long sequence.\n\n11. Concentration (Game): A deck of cards is shuffled and spread out face down. The player flips two cards at a time face up, receiving a reward if the flipped cards match. The agent must remember the value and position of previously flipped cards to improve the rate of successful matches.\n\n12. Battleship (Game): A partially observable version of Battleship, where the agent has no access to the board and must derive its own internal representation. Observations contain either HIT or MISS and the position of the last salvo fired. The player receives a positive reward for striking a ship, zero reward for hitting water, and negative reward for firing on a specific tile more than once.\n\n5\n\nTable 1: Frames per second (FPS) of our environments, computed on the Google Colab free tier and a Macbook Air (2020) laptop.\n\nEnvironment Repeat First Repeat Previous Autoencode Stateless Cartpole Stateless Pendulum Noisy Stateless Cartpole Noisy Stateless Pendulum\n\nColab FPS Laptop FPS 155,201 136,392 251,997 218,446 26,358 66,891 20,090\n\n23,895 50,349 121,756 73,622 8,168 6,269 6,808\n\nEnvironment Multiarmed Bandit Battleship Concentration Higher Lower Count Recall Minesweeper Labyrinth Escape Labyrinth Explore\n\nColab FPS Laptop FPS 469,325 235,402 157,217 76,903 53,779 32,003 41,122 30,611\n\n48,751 117,158 47,515 24,312 16,799 8,434 1,399 1,374\n\n13. Mine Sweeper (Game): The computer game Mine Sweeper, but like our Battleship implementation, the agent does not have access to the board. Each observation contains the position and number of adjacent mines to the last square “clicked” by the agent. Clicking on a mined square will end the game and produce a negative reward. The agent must remember where it has already searched and must integrate information from nearby tiles to narrow down the location of mines. Once the agent has selected all non-mined squares, the game ends.\n\n14. Labyrinth Explore (Navigation): The player is placed in a discrete, 2D procedurallygenerated maze, receiving a reward for each previously unreached tile it reaches. The player can only observe adjacent tiles. The agent also receives a small negative reward at each timestep, encouraging the agent to reach all squares quickly and end the episode.\n\n15. Labyrinth Escape (Navigation): The player must escape the procedurally-generated maze, using the same observation space as Labyrinth Explore. This is a sparse reward setting, where the player receives a positive reward only after solving the maze.\n\n4 POPGYM BASELINES\n\nOur memory model API relies on an abstract memory model class, only requiring users to implement memory forward and initial state methods. Our memory API builds on top of RLlib, exposing various algorithms, exploration methods, logging, distributed training, and more.\n\nWe collect well-known memory models from SL domains and wrap them in this API, enabling their use on RL tasks. We rewrite models where the existing implementation is slow, unreadable, not amenable to our API, or not written in Pytorch. Some of these sequence models have yet to be applied in the context of reinforcement learning.\n\n1. MLP: An MLP that cannot remember anything. This serves to form a lower bound for memory performance, as well and ensuring memory models are actively using memory, rather than just leveraging their higher parameter counts.\n\n2. Positional MLP (PosMLP): An MLP that can access the current episodic timestep. The current timestep is fed into the positional encoding from Vaswani et al. (2017), which is summed with the incoming features. PosMLP enables agents to learn time-dependent policies (those which evolve over the course of an episode) without explicitly using memory.\n\n3. Elman Networks: The original RNN, from Elman (1990). Elman networks sum the recurrent state and input, passing the resulting vector through a linear layer and activation function to produce the next hidden state. Elman networks are not used much in SL nowadays due to vanishing and exploding gradients.\n\n4. Long Short-Term Memory (LSTM): Hochreiter & Schmidhuber (1997) designed LSTM to address the vanishing and exploding gradient problems present in earlier RNNs like the Elman Network. LSTM utilizes a constant error carousel to handle longer dependencies and gating to ensure recurrent state stability during training. It has two recurrent states termed the hidden and cell states.\n\n5. Gated Recurrent Unit (GRU): The GRU is a simplification of LSTM, using only a single recurrent state. The GRU appears to have similar performance to LSTM in many applications while using fewer parameters (Chung et al., 2014).\n\n6\n\nFigure 2: Performance characteristics for POPGym memory baselines on random inputs. We use a recurrent state size of 256, a batch size of 64, and a episode length of 1024. We compute CPU statistics on a 3GHz Xeon Gold and GPU statistics on a 2080Ti, reporting the mean and 95% confidence interval over 10 trials. Train times correspond to a full batch while inference times are per-element (i.e. the latency to compute a single action). Note that GPU Train Time has logarithmic scale.\n\n6. Independently Recurrent Networks (IndRNN): Stacking LSTM and GRU cells tends to provide few benefits when compared with traditional deep neural networks. IndRNNs update the recurrent state using elementwise connections rather than a dense layer, enabling much deeper RNNs and handling longer dependencies than the LSTM and GRU (Li et al., 2018). In our experiments, we utilize a 2-layer IndRNN.\n\n7. Differentiable Neural Computers (DNC): Graves et al. (2016) introduce a new type of recurrent model using external memory. The DNC utilizes an RNN as a memory controller, reading and writing to external storage in a differentiable manner.\n\n8. Fast Autoregressive Transformers (FART): Unlike the traditional attention matrix whose size scales with the number of inputs, FART computes a fixed-size attention matrix at each timestep, taking the cumulative elementwise sum over successive timesteps (Katharopoulos et al., 2020). FART maintains two recurrent states, one for the running attention matrix and one for a normalization term, which helps mitigate large values and exploding gradients as the attention increases grows over time. The original paper omits a positional encoding, but we find it necessary for our benchmark.\n\n9. Fast Weight Programmers (FWP): The theory behind FART and FWP is different, but the implementation is relatively similar. FWP also maintains a running cumulative sum. Unlike FART, FWP normalizes the key and query vectors to sum to one, requiring only a single recurrent state and keeping the attention matrix of reasonable scale (Schlag et al., 2021). Unlike the original paper, we add a positional encoding to FWP.\n\n10. Frame Stacking (Fr.Stack): Mnih et al. (2015) implemented frame stacking to solve Atari games. Frame stacking is the concatenation of k observations along the feature dimension. Frame stacking is not strictly convolutional, but is implemented similarly to other convolutional methods. Frame stacking is known to work very well in RL, but the number of parameters scales with the receptive field, preventing it from learning long-term dependencies.\n\n11. Temporal Convolutional Networks (TCN): TCNs slide 1D convolutional filters over the temporal dimension. On long sequences, they are faster and use less memory than RNNs. TCNs avoid the vanishing gradient problem present in RNNs because the gradient feeds through each sequence element individually, rather than propagating through the entire sequence (Bai et al., 2018).\n\n12. Legendre Memory Units (LMU): LMUs are a mixture of convolution and RNNs. They apply Legendre polynomials across a sliding temporal window, feeding the results into an RNN hidden state (Voelker et al., 2019). LMUs can handle temporal dependencies spanning up to 100K timesteps.\n\n13. Diagonal State Space Models (S4D): S4D treats memory as a controls problem. It learns a linear time invariant (LTI) state space model for the recurrent state. S4D applies a Vandermonde matrix to the sequence of inputs, which we can represent using either convolution or a recurrence. Computing the result convolutionally makes it very fast. In SL, S4D was able to solve the challenging 16,000 timestep Path-X task, demonstrating significant capacity for long-term dependencies (Gu et al., 2022).\n\n7\n\n50010001500Train Memory(MB)MLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModel250500Num. Params(k)10100100010000GPU Train Time(ms)0100200CPU Inf. Time(ms)02GPU Inf. Time(ms)Model\n\nMMER MMER\n\n0.067 MLP 0.064 PosMLP 0.112 FWP 0.138 FART -0.180 S4D 0.233 TCN 0.190 Fr.Stack LMU 0.229 IndRNN 0.259 0.249 Elman 0.349 GRU 0.255 LSTM 0.065 DNC\n\nw/o Nav\n\n-0.010 0.053 0.200 0.202 -0.119 0.219 0.177 0.246 0.302 0.224 0.326 0.294 0.016\n\nFigure 3: (Left) A summary comparison of baselines aggregated over all environments. We normalize the MMER such that 0 denotes the worst trial and 1 denotes the best trial for a specific environment. We report the interquartile range (box), median (horizontal line), and mean (dot) normalized MMER over all trials. (Right) Single value scores for each model, produced by meaning the MMER over all POPGym environments. We also provide scores with navigation (Labyrinth) environments excluded; the reasoning is provided in the discussion section.\n\n5 EXPERIMENTS\n\nOur memory framework hooks into RLlib, providing integration with IMPALA, DQN, and countless other algorithms. Due to computational constraints, we only execute our study on Proximal Policy Optimization (PPO) (Schulman et al., 2017). We tend to use conservative hyperparameters to aid in reproducability – this entails large batch sizes, low learning rates, and many minibatch passes over every epoch. We run three trials of each model over three difficulties for each environment, resulting in over 1700 trials. We utilize the max-mean episodic reward (MMER) in many plots. We compute MMER by finding the mean episodic reward for each epoch, then taking the maximum over all epochs, resulting in a single MMER value for each trial. We present the full experimental parameters in Appendix A and detailed results for each environment and model in Appendix B. We provide a summary over models and tasks in Figure 3. Figure 2 reports model throughput and Table 1 provides environment throughput.\n\n6 DISCUSSION\n\nIn the following paragraphs, we pose some questions and findings made from the results of our large-scale study.\n\nSupervised learning is a bad proxy for RL. Supervised learning experiments show that IndRNN, LMU, FART, S4D, DNC, and TCN surpass LSTM and GRUs by a wide margin (Li et al., 2018; Voelker et al., 2019; Katharopoulos et al., 2020; Gu et al., 2022; Graves et al., 2016; Bai et al., 2018). S4D is unstable and often crashed due to exploding weights, suggesting it is not suitable for RL out of the box and that further tuning may be required. Although linear attention methods like FWP and FART show significant improvements across a plethora of supervised learning tasks, they were some of the worst contenders in RL. Classical RNNs outperformed modern memory methods, even though RNNs have been thoroughly supplanted in SL (Figure 3). The underlying cause of the disconnect between RL and SL performance is unclear and warrants further investigation.\n\nUse GRUs for performance and Elman nets for efficiency. Within traditional RNNs, there seems little reason to use LSTM, as GRUs are more efficient and perform better. Elman networks are largely ignored in modern SL and RL due to vanishing or exploding gradients, but these issues did not impact our training. We find Elman networks perform on-par with LSTM while exhibiting some of the best parameter and memory efficiency out of any model (Figure 2). Future work could investigate why Elman networks work so well in RL given their limitations, and distill these properties into memory models suited specifically for RL.\n\nAre maze navigation tasks sufficient for benchmarking memory? Existing POMDP benchmarks focus primarily navigation tasks. In our experiments, we show that the MLP received the\n\n8\n\nMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModel0.00.20.40.60.81.0Normalized MMERFigure 4: Selected results used in the discussion section. We standardize the MMER from [−1, 1] to [0, 1] for readability. The colored bars denote the mean and the black lines denote the 95% bootstrapped confidence interval. Full results across all environments are in Appendix B\n\nhighest score on almost all navigation tasks, beating all memory models (Figure 4). This is in line with our hypothesis from subsection 2.3, and raises doubts concerning previous models evaluated solely on navigation tasks. Does a novel memory method outperform baselines because of a better memory architectures, or just because it has more trainable parameters? Future work can bypass this scrutiny by including a diverse set of tasks beyond navigation, and by modifying simple navigation tasks to better leverage memory (e.g. positive reward for correctly answering “how many rooms are there in the house?”).\n\nPositional MLPs are an important baseline. Masked control tasks turn MDPs into POMDPs by hiding the velocity or position portions of classic control problems, and are probably the second most popular type of POMDP in literature after navigation. The positional MLP performed notably better than the MLP, nearly solving the Stateless Cartpole masked control task on easy (Figure 4). This is entirely unexpected, as providing the current timestep to an MLP is insufficient to compute the position and underlying Markov state. Outside of masked control, the positional MLP regularly outperformed the MLP (Figure 3). Stateless policies that evolve over time could be an interesting topic for future work, and should be a standard baseline in future memory comparisons.\n\nIs PPO enough? Memory models do not noticeably outperform the MLP in many game environments, such as Autoencode or Battleship, indicating that the memory is minimally effective in these tasks (Figure 4). All thirteen models converge to the nearly same reward, suggesting this could be due to issues with PPO rather than the memory models themselves. Future work could focus on designing new algorithms to solve these tasks. In parallel, research institutions with ample compute could ablate POPGym across other algorithms, such as Recurrent Replay Distributed DQN (Kapturowski et al., 2019).\n\n7 CONCLUSION\n\nWe presented the POPGym benchmark, a collection of POMDPs and memory baselines designed to standardize RL in partially observable environments. We discovered a notable disconnect between memory performance in supervised and reinforcement learning, with older RNNs surpassing linear transformers and modern memory models. According to our experiments, the GRU is the best general-purpose memory model, with Elman networks providing the best tradeoff between performance and efficiency. We revealed shortcomings in prior benchmarks focused on control and navigation POMDPs, emphasizing the importance of numerous and diverse POMDPs for evaluating memory. There is still a great deal of work to be done towards solving POMDPs, and we hope POPGym provides some measure of progress along the way.\n\n9\n\nMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelStatelessCartPoleEasyBattleshipMediumAutoencodeMedium0.00.20.40.60.81.0Standardized MMERMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelLabyrinthExploreEasy0.00.20.40.60.81.0Standardized MMERLabyrinthExploreMedium0.00.20.40.60.81.0Standardized MMERLabyrinthExploreHard8 ACKNOWLEDGEMENTS\n\nSteven Morad and Stephan Liwicki gratefully acknowledge the support of Toshiba Europe Ltd. Ryan Kortvelesy was supported by Nokia Bell Labs through their donation for the Centre of Mobile, Wearable Systems and Augmented Intelligence to the University of Cambridge.\n\n10\n\nREFERENCES\n\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional\n\nand recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\n\nAndrea Baisero and Sammie Katt.\n\ngym-gridverse: Gridworld domains for fully and partially observable reinforcement learning, 2021. URL https://github.com/abaisero/ gym-gridverse. Publication Title: GitHub repository.\n\nAndrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834–846, 1983. Publisher: IEEE.\n\nCharles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K ̈uttler, Andrew Lefrancq, Simon Green, V ́ıctor Vald ́es, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. Technical Report arXiv:1612.03801, arXiv, December 2016. URL http://arxiv.org/abs/1612.03801. arXiv:1612.03801 [cs] type: article.\n\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research, 47:253– 279, June 2013. ISSN 1076-9757. doi: 10.1613/jair.3912. URL https://www.jair.org/ index.php/jair/article/view/10819.\n\nMatteo Bettini, Ryan Kortvelesy, Jan Blumenkamp, and Amanda Prorok. VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning. The 16th International Symposium on Distributed Autonomous Robotic Systems, 2022. Publisher: Springer.\n\nManuel Blum and Dexter Kozen. On the power of the compass (or, why mazes are easier to search than graphs). In 19th Annual Symposium on Foundations of Computer Science (sfcs 1978), pp. 132–142, October 1978. doi: 10.1109/SFCS.1978.30. ISSN: 0272-5428.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba. OpenAI Gym, 2016. eprint: arXiv:1606.01540.\n\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. February 2022. URL https://openreview.net/forum?id=H1lJJnR5Ym.\n\nAnthony R Cassandra. A survey of POMDP applications.\n\nIn Working notes of AAAI 1998 fall symposium on planning with partially observable Markov decision processes, volume 1724, 1998.\n\nMaxime Chevalier-Boisvert. MiniWorld: Minimalistic 3D Environment for RL & Robotics Research, 2018. URL https://github.com/maximecb/gym-miniworld. Publication Title: GitHub repository.\n\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic Gridworld Environment for OpenAI Gym, 2018. URL https://github.com/maximecb/gym-minigrid. Publication Title: GitHub repository.\n\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n\nKarl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging Procedural Generation to Benchmark Reinforcement Learning. In Proceedings of the 37th International Conference on Machine Learning, pp. 2048–2056. PMLR, November 2020. URL https://proceedings. mlr.press/v119/cobbe20a.html. ISSN: 2640-3498.\n\nPrafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI Baselines, 2017. URL https://github.com/openai/baselines. Publication Title: GitHub repository.\n\n11\n\nKenji Doya. Temporal Difference Learning in Continuous Time and Space. In D. Touretzky, M. C. Mozer, and M. Hasselmo (eds.), Advances in Neural Information Processing Systems, volume 8. MIT Press, 1995. URL https://proceedings.neurips.cc/paper/1995/file/ 1e1d184167ca7676cf665225e236a3d2-Paper.pdf.\n\nJeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990. Publisher:\n\nWiley Online Library.\n\nDibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why In Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability. Advances in Neural Information Processing Systems, volume 34, pp. 25502–25515. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ d5ff135377d39f1de7372c95c74dd962-Abstract.html.\n\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwi ́nska, Sergio G ́omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, and others. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471–476, 2016. Publisher: Nature Publishing Group.\n\nAlbert Gu, Karan Goel, and Christopher Re.\n\nEfficiently Modeling Long Sequences with Structured State Spaces. March 2022. URL https://openreview.net/forum?id= uYLFoz1vlAC.\n\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. In Proceedings of the Thirty-Second AAAI ConferDeep reinforcement learning that matters. ence on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI’18/IAAI’18/EAAI’18, pp. 3207–3214, New Orleans, Louisiana, USA, February 2018. AAAI Press. ISBN 978-1-57735-800-8.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long Short-Term Memory. Neural Comput., 9(8):1735– ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https: 1780, November 1997. //doi.org/10.1162/neco.1997.9.8.1735. Place: Cambridge, MA, USA Publisher: MIT Press.\n\nShengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, and Jeff Braga. CleanRL: Highquality Single-file Implementations of Deep Reinforcement Learning Algorithms. 2021. eprint: 2111.08819.\n\nChi\n\nJin, Sham Kakade, Akshay Krishnamurthy,\n\nReinforcement Learning of Undercomplete POMDPs. formation Processing Systems, Inc., d783823cc6284b929c2cd8df2167d212-Abstract.html.\n\nSample-Efficient In18530–18539. Curran Associates, URL https://proceedings.neurips.cc/paper/2020/hash/\n\nand Qinghua Liu.\n\nIn Advances\n\nin Neural\n\nvolume\n\n2020.\n\n33,\n\npp.\n\nAbhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, and Dhruv Batra. Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance? IEEE Robotics and Automation Letters, 5(4):6670– 6677, October 2020. ISSN 2377-3766. doi: 10.1109/LRA.2020.3013848. Conference Name: IEEE Robotics and Automation Letters.\n\nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998. Publisher: Elsevier.\n\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. RECURRENT EXPERIENCE REPLAY IN DISTRIBUTED REINFORCEMENT LEARNING. pp. 19, 2019.\n\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ̧ois Fleuret. Transformers In Proceedings of the are RNNs: Fast Autoregressive Transformers with Linear Attention. 37th International Conference on Machine Learning, pp. 5156–5165. PMLR, November 2020. URL https://proceedings.mlr.press/v119/katharopoulos20a.html. ISSN: 2640-3498.\n\n12\n\nMicha\\l Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja ́skowski. ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning. In IEEE Conference on Computational Intelligence and Games, pp. 341–348, Santorini, Greece, September 2016. IEEE. URL http://arxiv.org/abs/1605.02097.\n\nJeongyeol Kwon, Yonathan Efroni, Constantine Caramanis,\n\nRL In Advances in NeuInformation Processing Systems, volume 34, pp. 24523–24534. Curran Associates, URL https://proceedings.neurips.cc/paper/2021/hash/\n\nfor Latent MDPs: Regret Guarantees and a Lower Bound. ral Inc., cd755a6c6b699f3262bcc2aa46ab507e-Abstract.html.\n\nand Shie Mannor.\n\n2021.\n\nTor Lattimore and Csaba Szepesv ́ari. Bandit algorithms. Cambridge University Press, 2020.\n\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998. ISSN 1558-2256. doi: 10.1109/5.726791. Conference Name: Proceedings of the IEEE.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems, November 2020. URL http://arxiv. org/abs/2005.01643. arXiv:2005.01643 [cs, stat].\n\nShuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao.\n\nIndependently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5457–5466, Salt Lake City, UT, June 2018. IEEE. ISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.2018.00572. URL https://ieeexplore. ieee.org/document/8578670/.\n\nEric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning, pp. 3053–3062. PMLR, 2018.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, and others. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015. Publisher: Nature Publishing Group.\n\nSteven Morad, Stephan Liwicki, Ryan Kortvelesy, Roberto Mecca, and Amanda Prorok. Modeling Partially Observable Systems using Graph-Based Memory and Topological Priors. In Proceedings of The 4th Annual Learning for Dynamics and Control Conference, pp. 59–73. PMLR, May 2022. URL https://proceedings.mlr.press/v168/morad22a.html. ISSN: 2640-3498.\n\nSteven D. Morad, Roberto Mecca, Rudra P. K. Poudel, Stephan Liwicki, and Roberto Cipolla. EmIEEE bodied Visual Navigation With Automatic Curriculum Learning in Real Environments. Robotics and Automation Letters, 6(2):683–690, April 2021. ISSN 2377-3766. doi: 10.1109/ LRA.2020.3048662. Conference Name: IEEE Robotics and Automation Letters.\n\nTianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs. In Proceedings of the 39th International Conference on Machine Learning, pp. 16691–16723. PMLR, June 2022. URL https://proceedings. mlr.press/v162/ni22a.html. ISSN: 2640-3498.\n\nIan Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesv ́ari, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, and Hado van Hasselt. Behaviour Suite for Reinforcement Learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=rygf-kSYwH.\n\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Rapha ̈el Lopez Kaufman, Aidan Clark, Seb Noury, Matthew Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing Transformers for Reinforcement Learning. In Hal Daum ́e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7487–7498. PMLR, July 2020. URL https://proceedings.mlr.press/v119/parisotto20a.html.\n\n13\n\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal of Machine Learning Research, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/ 20-1364.html.\n\nDhruv Ramani. A Short Survey On Memory Based Reinforcement Learning. Technical Report arXiv:1904.06736, arXiv, April 2019. URL http://arxiv.org/abs/1904.06736. arXiv:1904.06736 [cs] type: article.\n\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\n\nImanol Schlag, Kazuki Irie, and J ̈urgen Schmidhuber. Linear Transformers Are Secretly Fast Weight Programmers. In Proceedings of the 38th International Conference on Machine Learning, pp. 9355–9366. PMLR, July 2021. URL https://proceedings.mlr.press/v139/ schlag21a.html. ISSN: 2640-3498.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nDaniel Seita. Can RL From Pixels be as Efficient as RL From State?, July 2020. URL http:\n\n//bair.berkeley.edu/blog/2020/07/19/curl-rad/.\n\nVatsal Sharan, Sham M Kakade, Percy S Liang, and Gregory Valiant. Learning Overcomplete In Advances in Neural Information Processing Systems, volume 30. Curran AssoHMMs. ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 6aca97005c68f1206823815f66102863-Abstract.html.\n\nAleksandrs Slivkins and others. Introduction to multi-armed bandits. Foundations and Trends® in\n\nMachine Learning, 12(1-2):1–286, 2019. Publisher: Now Publishers, Inc.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAaron Voelker, Ivana Kaji ́c, and Chris Eliasmith. Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://papers.nips.cc/paper/ 2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html.\n\nZhihan Yang and Hai Nguyen. Recurrent Off-policy Baselines for Memory-based Continuous Control. Technical Report arXiv:2110.12628, arXiv, October 2021. URL http://arxiv.org/ abs/2110.12628. arXiv:2110.12628 [cs] type: article.\n\nKaiyu Zheng and Stefanie Tellex.\n\npomdp py: A Framework to Build and Solve In ICAPS 2020 Workshop on Planning and Robotics (PlanRob), POMDP Problems. 2020. URL https://icaps20subpages.icaps-conference.org/wp-content/ uploads/2020/10/14-PlanRob_2020_paper_3.pdf.\n\n14\n\nTable 2: PPO hyperparameters used in all of our experiments.\n\nHParam\n\nDecay factor γ Value fn. loss coef. Entropy loss coef. Learning rate Num. SGD iters Batch size Minibatch size GAE λ KL target KL coefficient PPO clipping Value clipping BPTT Truncation Length Maximum Episode Length\n\nValue\n\n0.99 1.0 0.0 5e-5 30 65536 8192 1.0 0.01 0.2 0.3 0.3 ∞\n1024\n\nA EXPERIMENTAL PARAMETERS\n\nGiven the number of environments and models, it is not computationally feasible to optimize hyperparameters in a structured way. Through trial and error, we evaluated all models on the Repeat First and Repeat Previous environments and found suitable values that worked across all models. We then picked a more conservative estimate (larger batch size, lower learning rate) to promote monotonic improvement and prevent catastrophic forgetting, at the expense of some sample efficiency.\n\nThere is no clear axis for a truly fair comparison between memory models – model throughput and parameter count vary drastically throughout the memory models. We decide to limit the amount of storage (i.e. recurrent state size) of each memory model to 256 dimensions, which is greater than the common values of 64 and 128 in literature (Ni et al., 2022). This results in lower parameter counts for models that produce the recurrent state using a tensor product (e.g. FWP, FART, S4D). We could make an exception for these models, allowing them to produce recurrent states of size 2562 = 63356 dimensions instead of 162 = 256 dimensions to bring up the parameter count, but we believe this is unfair to recurrent models. In this case, recurrent models would need to forget and compress information over longer episodes, while tensor product models could store every single observation in memory without any compression or forgetting. Storing everything is unlikely to scale to real-world applications where episodes could span hours, days, or run indefinitely.\n\nA.1 GENERAL MODEL HYPERPARAMETERS\n\nFor all our memory experiments, we use the same outer model, just swapping out the inner memory model. The outer model first projects observations from the environment to 128 zero-mean varianceone dimensions. Here, the positional encoding is applied if the memory model requests it. The projection goes through a single linear layer and LeakyReLU activation of size 128, then feeds into the memory model. Output from the memory model is projected to 128 dimensions, then split into the actor and critic head. The actor and critic heads are two layer MLPs of width 128 with LeakyReLU activation.\n\nA.2 MODEL-SPECIFIC HYPERPARAMETERS\n\nThe Elman, GRU, LSTM, and LMU RNNs use a single cell. We use a 2-cell IndRNN as they claim to utilize deeper networks. The FART and FWP models use a single attention block. We use the attention-only formulations of FWP, rather than the combined attention and RNN variant. We utilize a temporal window of four for frame stacking and TCN. LMU utilizes a θ window size of 64 timesteps.\n\n15\n\nB FULL EXPERIMENTAL RESULTS\n\nWe report our results in three forms:\n\n1. Bar charts denoting the standardized MMER split by environment (Figure 5-Figure 9)\n\n2. Line plots showing cumulative maximum episodic reward for each training epoch, split by\n\nenvironment (Figure 10-Figure 14)\n\n3. Tables reporting the mean and standard deviation of the MMER, split by model and envi-\n\nronment (Table 3)\n\nAll models and environments are from commit e397e5e except for the DiffNC experiments, which are from commit 33b0995. All experiments sample 15M timesteps from each environment, except for the DiffNC experiments which sample 10M timesteps.\n\nWe run each trial 3 times, aggregating results using the mean over trials. All raw data is available at https://wandb.ai/prorok-lab/popgym-public. The bar plots represent the mean and 95% bootstrap confidence interval. For the bar charts, we standardize the reward between 0 and 1. In the line plots, the solid region refers to the mean and the shaded region to the 95% bootstrap confidence interval. The table reports the MMER mean and standard deviation across trials.\n\n16\n\nFigure 5: POPGym baselines.\n\n17\n\nMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelAutoencodeEasyAutoencodeMediumAutoencodeHardMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelBattleshipEasyBattleshipMediumBattleshipHard0.00.20.40.60.81.0Standardized MMERMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelConcentrationEasy0.00.20.40.60.81.0Standardized MMERConcentrationMedium0.00.20.40.60.81.0Standardized MMERConcentrationHardFigure 6: POPGym baselines (continued)\n\n18\n\nMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelCountRecallEasyCountRecallMediumCountRecallHardMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelHigherLowerEasyHigherLowerMediumHigherLowerHard0.00.20.40.60.81.0Standardized MMERMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelLabyrinthEscapeEasy0.00.20.40.60.81.0Standardized MMERLabyrinthEscapeMedium0.00.20.40.60.81.0Standardized MMERLabyrinthEscapeHardFigure 7: POPGym baselines (continued)\n\n19\n\nMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelLabyrinthExploreEasyLabyrinthExploreMediumLabyrinthExploreHardMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelMineSweeperEasyMineSweeperMediumMineSweeperHard0.00.20.40.60.81.0Standardized MMERMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelMultiarmedBanditEasy0.00.20.40.60.81.0Standardized MMERMultiarmedBanditMedium0.00.20.40.60.81.0Standardized MMERMultiarmedBanditHardFigure 8: POPGym baselines (continued)\n\n20\n\nMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelNoisyStatelessCartPoleEasyNoisyStatelessCartPoleMediumNoisyStatelessCartPoleHardMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelNoisyStatelessPendulumEasyNoisyStatelessPendulumMediumNoisyStatelessPendulumHard0.00.20.40.60.81.0Standardized MMERMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelRepeatFirstEasy0.00.20.40.60.81.0Standardized MMERRepeatFirstMedium0.00.20.40.60.81.0Standardized MMERRepeatFirstHardFigure 9: POPGym baselines (continued)\n\n21\n\nMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelRepeatPreviousEasyRepeatPreviousMediumRepeatPreviousHardMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelStatelessCartPoleEasyStatelessCartPoleMediumStatelessCartPoleHard0.00.20.40.60.81.0Standardized MMERMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCModelStatelessPendulumEasy0.00.20.40.60.81.0Standardized MMERStatelessPendulumMedium0.00.20.40.60.81.0Standardized MMERStatelessPendulumHardFigure 10: POPGym baselines (continued)\n\nFigure 11: POPGym baselines (continued)\n\n22\n\n0.750.500.250.000.25Max Episodic RewardAutoencodeEasyAutoencodeMediumAutoencodeHard0.750.500.250.000.25Max Episodic RewardBattleshipEasyBattleshipMediumBattleshipHard050100150200Epoch0.750.500.250.000.25Max Episodic RewardConcentrationEasy050100150200EpochConcentrationMedium050100150200EpochConcentrationHardModelMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNC1.00.50.00.51.0Max Episodic RewardCountRecallEasyCountRecallMediumCountRecallHard1.00.50.00.51.0Max Episodic RewardHigherLowerEasyHigherLowerMediumHigherLowerHard050100150200Epoch1.00.50.00.51.0Max Episodic RewardLabyrinthEscapeEasy050100150200EpochLabyrinthEscapeMedium050100150200EpochLabyrinthEscapeHardModelMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCFigure 12: POPGym baselines (continued)\n\nFigure 13: POPGym baselines (continued)\n\n23\n\n1.00.50.00.51.0Max Episodic RewardLabyrinthExploreEasyLabyrinthExploreMediumLabyrinthExploreHard1.00.50.00.51.0Max Episodic RewardMineSweeperEasyMineSweeperMediumMineSweeperHard050100150200Epoch1.00.50.00.51.0Max Episodic RewardMultiarmedBanditEasy050100150200EpochMultiarmedBanditMedium050100150200EpochMultiarmedBanditHardModelMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNC0.50.00.51.0Max Episodic RewardNoisyStatelessCartPoleEasyNoisyStatelessCartPoleMediumNoisyStatelessCartPoleHard0.50.00.51.0Max Episodic RewardNoisyStatelessPendulumEasyNoisyStatelessPendulumMediumNoisyStatelessPendulumHard050100150200Epoch0.50.00.51.0Max Episodic RewardRepeatFirstEasy050100150200EpochRepeatFirstMedium050100150200EpochRepeatFirstHardModelMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCFigure 14: POPGym baselines (continued)\n\n24\n\n0.50.00.51.0Max Episodic RewardRepeatPreviousEasyRepeatPreviousMediumRepeatPreviousHard0.50.00.51.0Max Episodic RewardStatelessCartPoleEasyStatelessCartPoleMediumStatelessCartPoleHard050100150200Epoch0.50.00.51.0Max Episodic RewardStatelessPendulumEasy050100150200EpochStatelessPendulumMedium050100150200EpochStatelessPendulumHardModelMLPPosMLPFWPFARTS4DTCNFr.StackLMUIndRNNElmanGRULSTMDNCTable 3: Full table of results, denoting the MMER mean and standard deviation by environment and model.\n\nEnv.\n\nAutoencodeEasy\n\nAutoencodeMedium\n\nAutoencodeHard\n\nBattleshipEasy\n\nModel\n\nMMER\n\nμ\n\nσ\n\n-0.489 DNC -0.306 Elman -0.447 FART -0.322 FWP -0.422 Fr.Stack -0.283 GRU -0.334 IndRNN -0.370 LMU -0.312 LSTM -0.470 MLP -0.458 PosMLP -0.490 S4D -0.410 TCN -0.488 DNC -0.443 Elman -0.478 FART -0.449 FWP -0.466 Fr.Stack GRU -0.425 IndRNN -0.420 -0.474 LMU -0.423 LSTM -0.482 MLP -0.474 PosMLP -0.490 S4D -0.464 TCN -0.489 DNC -0.481 Elman -0.481 FART -0.472 FWP -0.478 Fr.Stack GRU -0.456 IndRNN -0.448 -0.480 LMU -0.467 LSTM -0.488 MLP -0.483 PosMLP -0.489 S4D -0.476 TCN -0.427 DNC -0.290 Elman -0.413 FART -0.389 FWP -0.378 Fr.Stack -0.320 GRU -0.287 IndRNN -0.323 LMU -0.376 LSTM MLP -0.325 PosMLP -0.226 -0.432 S4D\n\n0.002 0.022 0.014 0.020 0.010 0.029 0.004 0.015 0.008 0.004 0.003 0.005 0.006 0.002 0.007 0.002 0.005 0.002 0.018 0.011 0.003 0.004 0.002 0.001 0.001 0.002 0.002 0.005 0.001 0.011 0.004 0.009 0.010 0.006 0.004 0.002 0.003 0.003 0.004 0.002 0.013 0.005 0.007 0.015 0.013 0.005 0.027 0.007 0.012 0.077 0.002\n\nContinued on next page\n\n25\n\nEnv.\n\nBattleshipMedium\n\nBattleshipHard\n\nConcentrationEasy\n\nConcentrationMedium\n\nConcentrationHard\n\nμ\n\nσ\n\nModel\n\n-0.333 TCN -0.394 DNC -0.373 Elman -0.392 FART -0.386 FWP -0.390 Fr.Stack -0.367 GRU IndRNN -0.337 -0.387 LMU -0.379 LSTM -0.356 MLP -0.344 PosMLP -0.406 S4D -0.363 TCN -0.380 DNC -0.377 Elman -0.384 FART -0.380 FWP -0.381 Fr.Stack GRU -0.377 IndRNN -0.369 -0.381 LMU -0.380 LSTM -0.383 MLP -0.382 PosMLP -0.389 S4D -0.376 TCN -0.182 DNC -0.098 Elman -0.185 FART -0.188 FWP -0.146 Fr.Stack -0.039 GRU 0.142 IndRNN -0.057 LMU -0.080 LSTM -0.050 MLP -0.048 PosMLP -0.190 S4D 0.253 TCN -0.182 DNC -0.186 Elman -0.186 FART -0.185 FWP -0.185 Fr.Stack -0.189 GRU -0.256 IndRNN -0.176 LMU -0.185 LSTM -0.178 MLP -0.175 PosMLP -0.186 S4D -0.157 TCN -0.830 DNC\n\n0.007 0.003 0.007 0.003 0.003 0.008 0.008 0.005 0.010 0.006 0.020 0.030 0.003 0.003 0.004 0.005 0.003 0.005 0.004 0.008 0.009 0.000 0.001 0.002 0.003 0.005 0.004 0.004 0.012 0.002 0.001 0.001 0.005 0.004 0.009 0.012 0.015 0.010 0.004 0.004 0.001 0.003 0.002 0.002 0.002 0.003 0.012 0.005 0.001 0.004 0.002 0.002 0.005 0.002\n\nContinued on next page\n\n26\n\nEnv.\n\nCountRecallEasy\n\nCountRecallMedium\n\nCountRecallHard\n\nHigherLowerEasy\n\nμ\n\nσ\n\nModel\n\n-0.830 Elman -0.830 FART -0.831 FWP -0.829 Fr.Stack -0.829 GRU -0.849 IndRNN -0.828 LMU -0.829 LSTM MLP -0.833 PosMLP -0.831 -0.830 S4D -0.830 TCN -0.913 DNC 0.016 Elman -0.300 FART -0.399 FWP -0.365 Fr.Stack 0.177 GRU -0.042 IndRNN -0.214 LMU 0.509 LSTM -0.847 MLP -0.409 PosMLP -0.911 S4D -0.385 TCN -0.907 DNC -0.540 Elman -0.541 FART -0.541 FWP -0.529 Fr.Stack -0.528 GRU -0.535 IndRNN -0.555 LMU -0.538 LSTM MLP -0.907 PosMLP -0.519 -0.920 S4D -0.524 TCN -0.815 DNC -0.541 Elman -0.501 FART -0.477 FWP -0.475 Fr.Stack -0.475 GRU -0.481 IndRNN -0.522 LMU -0.478 LSTM -0.867 MLP PosMLP -0.470 -0.858 S4D -0.470 TCN 0.505 DNC 0.520 Elman 0.522 FART\n\n0.001 0.001 0.001 0.001 0.003 0.005 0.002 0.001 0.001 0.001 0.002 0.003 0.017 0.091 0.145 0.047 0.022 0.005 0.050 0.058 0.062 0.011 0.011 0.005 0.010 0.023 0.001 0.003 0.016 0.002 0.001 0.014 0.009 0.005 0.001 0.003 0.000 0.003 0.042 0.033 0.008 0.006 0.008 0.006 0.004 0.036 0.006 0.002 0.003 0.013 0.002 0.002 0.002 0.002\n\nContinued on next page\n\n27\n\nEnv.\n\nHigherLowerMedium\n\nHigherLowerHard\n\nLabyrinthEscapeEasy\n\nLabyrinthEscapeMedium\n\nModel\n\nFWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack\n\nμ\n\nσ\n\n0.520 0.504 0.529 0.528 0.494 0.526 0.505 0.505 0.479 0.503 0.501 0.503 0.511 0.504 0.503 0.511 0.513 0.466 0.504 0.504 0.505 0.420 0.503 0.498 0.501 0.507 0.499 0.499 0.506 0.509 0.453 0.502 0.504 0.502 0.387 0.501 0.958 0.956 -0.043 -0.078 0.521 0.959 -0.218 0.814 0.920 0.961 0.544 -0.305 0.773 -0.414 -0.122 -0.602 -0.604 -0.384\n\n0.002 0.001 0.002 0.000 0.003 0.003 0.001 0.001 0.018 0.000 0.003 0.001 0.001 0.006 0.000 0.002 0.001 0.004 0.003 0.001 0.002 0.057 0.001 0.005 0.001 0.001 0.002 0.002 0.001 0.001 0.005 0.001 0.002 0.001 0.037 0.001 0.002 0.002 0.266 0.062 0.055 0.001 0.136 0.076 0.024 0.000 0.023 0.033 0.054 0.493 0.388 0.057 0.015 0.271\n\nContinued on next page\n\n28\n\nEnv.\n\nLabyrinthEscapeHard\n\nLabyrinthExploreEasy\n\nLabyrinthExploreMedium\n\nLabyrinthExploreHard\n\nModel\n\nGRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN\n\nμ\n\nσ\n\n0.230 -0.735 -0.525 -0.513 -0.093 -0.413 -0.719 -0.494 -0.827 -0.780 -0.828 -0.848 -0.733 -0.839 -0.907 -0.762 -0.789 -0.245 -0.806 -0.897 -0.787 0.956 0.964 0.424 -0.152 0.957 0.960 0.961 0.812 0.587 0.968 0.964 -0.265 0.962 0.905 0.873 -0.197 -0.464 0.847 0.893 0.440 0.423 0.025 0.924 0.516 -0.580 0.903 0.720 0.612 -0.407 -0.611 0.437 0.796 0.315\n\n0.642 0.135 0.081 0.123 0.857 0.049 0.064 0.046 0.085 0.039 0.018 0.016 0.040 0.032 0.030 0.033 0.049 0.948 0.078 0.034 0.011 0.006 0.001 0.465 0.033 0.001 0.001 0.001 0.048 0.076 0.000 0.001 0.018 0.000 0.005 0.052 0.130 0.041 0.063 0.008 0.350 0.185 0.009 0.001 0.191 0.035 0.003 0.212 0.286 0.032 0.044 0.287 0.003 0.412\n\nContinued on next page\n\n29\n\nEnv.\n\nMineSweeperEasy\n\nMineSweeperMedium\n\nMineSweeperHard\n\nMultiarmedBanditEasy\n\nModel\n\nLMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM\n\nμ\n\nσ\n\n-0.068 -0.206 0.858 -0.018 -0.721 0.559 0.293 0.640 0.474 0.520 0.516 0.693 0.626 0.689 0.686 0.251 0.403 -0.071 0.582 -0.385 -0.009 -0.176 -0.151 -0.177 -0.006 -0.024 -0.108 -0.110 -0.158 -0.117 -0.365 0.018 -0.429 -0.230 -0.390 -0.338 -0.338 -0.206 -0.247 -0.294 -0.303 -0.289 -0.278 -0.430 -0.191 0.302 0.631 0.453 0.556 0.476 0.619 0.625 0.332 0.527\n\n0.048 0.030 0.002 0.016 0.008 0.203 0.102 0.008 0.030 0.025 0.028 0.009 0.003 0.005 0.022 0.019 0.023 0.024 0.012 0.016 0.034 0.020 0.041 0.029 0.013 0.023 0.008 0.012 0.006 0.042 0.019 0.030 0.002 0.021 0.009 0.036 0.028 0.027 0.006 0.021 0.012 0.011 0.001 0.002 0.018 0.106 0.014 0.042 0.045 0.052 0.007 0.014 0.141 0.006\n\nContinued on next page\n\n30\n\nEnv.\n\nMultiarmedBanditMedium\n\nMultiarmedBanditHard\n\nNoisyStatelessCartPoleEasy\n\nNoisyStatelessCartPoleMedium\n\nModel\n\nMLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP\n\nμ\n\nσ\n\n0.377 0.342 0.095 0.586 0.260 0.450 0.278 0.388 0.357 0.538 0.591 0.055 0.476 0.350 0.299 0.136 0.598 0.278 0.297 0.208 0.234 0.264 0.516 0.567 0.033 0.419 0.391 0.177 0.118 0.574 0.615 0.991 0.983 0.966 0.856 0.995 0.994 0.921 0.987 0.515 0.537 0.259 0.871 0.312 0.640 0.590 0.547 0.449 0.642 0.659 0.519 0.638 0.302 0.353\n\n0.055 0.068 0.008 0.004 0.066 0.043 0.109 0.087 0.074 0.036 0.026 0.018 0.019 0.041 0.055 0.006 0.022 0.075 0.103 0.102 0.048 0.096 0.083 0.033 0.002 0.057 0.040 0.028 0.024 0.049 0.094 0.000 0.001 0.010 0.003 0.000 0.001 0.020 0.006 0.003 0.006 0.021 0.002 0.025 0.014 0.006 0.017 0.004 0.012 0.006 0.012 0.007 0.002 0.004\n\nContinued on next page\n\n31\n\nEnv.\n\nModel\n\nμ\n\nσ\n\nNoisyStatelessCartPoleHard\n\nS4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN NoisyStatelessPendulumMedium DNC\n\nNoisyStatelessPendulumEasy\n\nNoisyStatelessPendulumHard\n\nElman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN\n\n0.211 0.462 0.233 0.386 0.366 0.354 0.326 0.390 0.404 0.352 0.393 0.229 0.288 0.207 0.330 0.456 0.818 0.610 0.532 0.832 0.894 0.654 0.654 0.786 0.387 0.467 0.291 0.811 0.435 0.622 0.570 0.521 0.621 0.717 0.526 0.570 0.653 0.369 0.446 0.286 0.633 0.440 0.614 0.553 0.498 0.561 0.657 0.521 0.563 0.617 0.351 0.433 0.289 0.573\n\n0.025 0.007 0.009 0.009 0.002 0.001 0.006 0.007 0.005 0.019 0.002 0.002 0.000 0.007 0.004 0.040 0.056 0.005 0.028 0.015 0.002 0.188 0.055 0.031 0.010 0.011 0.022 0.015 0.020 0.031 0.001 0.006 0.010 0.004 0.061 0.012 0.024 0.002 0.011 0.011 0.013 0.017 0.017 0.007 0.008 0.008 0.002 0.109 0.014 0.010 0.012 0.009 0.011 0.009\n\nContinued on next page\n\n32\n\nEnv.\n\nRepeatFirstEasy\n\nRepeatFirstMedium\n\nRepeatFirstHard\n\nRepeatPreviousEasy\n\nRepeatPreviousMedium\n\nModel\n\nDNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman\n\nμ\n\nσ\n\n0.716 1.000 1.000 0.813 0.997 1.000 1.000 1.000 1.000 0.489 0.736 -0.194 1.000 -0.349 -0.468 0.962 0.830 -0.467 1.000 0.998 0.641 0.926 -0.389 -0.472 -0.241 -0.449 -0.334 -0.464 0.867 0.915 -0.448 0.940 0.969 -0.406 0.275 -0.355 -0.455 -0.178 -0.457 -0.223 1.000 0.060 0.200 1.000 1.000 0.957 1.000 1.000 -0.320 -0.336 -0.473 1.000 -0.490 -0.394\n\n0.207 0.000 0.000 0.224 0.005 0.000 0.000 0.000 0.000 0.391 0.209 0.098 0.000 0.041 0.014 0.048 0.237 0.007 0.000 0.003 0.457 0.068 0.057 0.007 0.100 0.033 0.039 0.002 0.142 0.045 0.016 0.012 0.026 0.060 0.677 0.111 0.009 0.146 0.010 0.075 0.000 0.040 0.052 0.000 0.000 0.012 0.000 0.000 0.007 0.014 0.003 0.000 0.001 0.025\n\nContinued on next page\n\n33\n\nEnv.\n\nRepeatPreviousHard\n\nStatelessCartPoleEasy\n\nStatelessCartPoleMedium\n\nStatelessCartPoleHard\n\nModel\n\nFART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP\n\nμ\n\nσ\n\n-0.468 -0.345 -0.484 -0.315 -0.304 0.789 -0.284 -0.486 -0.485 -0.490 -0.478 -0.490 -0.481 -0.485 -0.443 -0.485 -0.428 -0.384 0.191 -0.397 -0.486 -0.486 -0.491 -0.486 0.960 1.000 1.000 1.000 1.000 1.000 1.000 0.996 1.000 0.722 0.967 0.514 1.000 0.956 1.000 1.000 0.989 1.000 1.000 1.000 0.995 1.000 0.398 0.593 0.205 1.000 0.778 1.000 0.996 0.900\n\n0.011 0.033 0.003 0.017 0.014 0.288 0.024 0.001 0.001 0.002 0.004 0.002 0.003 0.003 0.017 0.001 0.002 0.013 0.041 0.008 0.002 0.004 0.001 0.001 0.027 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.001 0.006 0.076 0.000 0.061 0.000 0.000 0.007 0.000 0.000 0.000 0.001 0.000 0.006 0.049 0.011 0.000 0.330 0.000 0.000 0.092\n\nContinued on next page\n\n34\n\nEnv.\n\nStatelessPendulumEasy\n\nStatelessPendulumMedium\n\nStatelessPendulumHard\n\nModel\n\nFr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN DNC Elman FART FWP Fr.Stack GRU IndRNN LMU LSTM MLP PosMLP S4D TCN\n\nμ\n\nσ\n\n0.989 1.000 1.000 0.987 1.000 0.265 0.443 0.127 0.998 0.427 0.903 0.652 0.556 0.906 0.913 0.798 0.819 0.878 0.448 0.486 0.290 0.906 0.436 0.880 0.660 0.579 0.881 0.884 0.719 0.858 0.875 0.455 0.509 0.282 0.874 0.420 0.819 0.698 0.663 0.824 0.828 0.804 0.806 0.819 0.477 0.543 0.303 0.822\n\n0.017 0.000 0.000 0.007 0.000 0.002 0.018 0.026 0.003 0.022 0.007 0.010 0.007 0.005 0.002 0.151 0.107 0.048 0.005 0.011 0.007 0.003 0.012 0.004 0.013 0.033 0.002 0.002 0.216 0.003 0.005 0.025 0.011 0.003 0.002 0.033 0.005 0.077 0.051 0.002 0.001 0.023 0.006 0.006 0.030 0.020 0.014 0.005\n\n35",
    "reference": "# Summary Of The Paper\n\nThe paper performs benchmarks on partially observable RL environments and a bunch of network architectures on memory. The authors make sure the learning of the POPGym environments is affordable with consumer-grade GPU to ensure the experiments are feasible for researchers. They conclude that GRU is the best general-purpose memory model.\n\n# Strength And Weaknesses\n\nStrength\n+ A very good and in-depth summary of the current RL benchmarks, especially on the partially observable environments. After analysis of the shortcomings of the current benchmarks, they motivate the work;\n+ The author conducted an extensive set of experiments with 14 partially observable environments and with 13 memory model baselines;\n+ Considerations have been made for wide adoption of the proposed benchmark, including low compute cost for a wide community, compatible APIs with the popular training library, etc;\n\nWeaknesses\n- I would expect the analysis part of a benchmark paper would take the largest portion of the paper and will have a very detailed analysis of the benchmarks and experiments. However, the major parts of the paper are about the task description and baseline description; \n- Though the authors have run very extensive experiments, the results do not seem dramatically different than what people already understood;\n- I  found limited insights from the benchmark results;\n- The authors should point out potential directions for tackling the partially observable tasks after the benchmarking.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity is high, the quality is decent, novelty is on a borderline level. The reproducibility is good.\n\n# Summary Of The Review\n\nI found unsatisfactory that a benchmark paper does not provide much content on the in-depth analysis of experiment results, even though the authors conduct an extensive set of experiments.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nUSER-INTERACTIVE OFFLINE REINFORCEMENT LEARNING\n\nPhillip Swazinna Siemens & TU Munich Munich, Germany swazinna@in.tum.de\n\nSteffen Udluft Siemens Technology Munich, Germany steffen.udluft@siemens.com\n\nThomas Runkler Siemens & TU Munich Munich, Germany thomas.runkler@siemens.com\n\nABSTRACT\n\nOffline reinforcement learning algorithms still lack trust in practice due to the risk that the learned policy performs worse than the original policy that generated the dataset or behaves in an unexpected way that is unfamiliar to the user. At the same time, offline RL algorithms are not able to tune their most important hyperparameter - the proximity of the learned policy to the original policy. We propose an algorithm that allows the user to tune this hyperparameter at runtime, thereby addressing both of the above mentioned issues simultaneously. This allows users to start with the original behavior and grant successively greater deviation, as well as stopping at any time when the policy deteriorates or the behavior is too far from the familiar one.\n\n1\n\nINTRODUCTION\n\nRecently, offline reinforcement learning (RL) methods have shown that it is possible to learn effective policies from a static pre-collected dataset instead of directly interacting with the environment (Laroche et al., 2019; Fujimoto et al., 2019; Yu et al., 2020; Swazinna et al., 2021b). Since direct interaction is in practice usually very costly, these techniques have alleviated a large obstacle on the path of applying reinforcement learning techniques in real world problems.\n\nA major issue that these algorithms still face is tuning their most important hyperparameter: The proximity to the original policy. Virtually all algorithms tackling the offline setting have such a hyperparameter, and it is obviously hard to tune, since no interaction with the real environment is permitted until final deployment. Practitioners thus risk being overly conservative (resulting in no improvement) or overly progressive (risking worse performing policies) in their choice.\n\nAdditionally, one of the arguably largest obstacles on the path to deployment of RL trained policies in most industrial control problems is that (offline) RL algorithms ignore the presence of domain experts, who can be seen as users of the final product - the policy. Instead, most algorithms today can be seen as trying to make human practitioners obsolete. We argue that it is important to provide these users with a utility - something that makes them want to use RL solutions. Other research fields, such as machine learning for medical diagnoses, have already established the idea that domain experts are crucially important to solve the task and complement human users in various ways Babbar et al. (2022); Cai et al. (2019); De-Arteaga et al. (2021); Fard & Pineau (2011); Tang et al. (2020). We see our work in line with these and other researchers (Shneiderman, 2020; Schmidt et al., 2021), who suggest that the next generation of AI systems needs to adopt a user-centered approach and develop systems that behave more like an intelligent tool, combining both high levels of human control and high levels of automation. We seek to develop an offline RL method that does just that. Furthermore, we see giving control to the user as a requirement that may in the future be much more enforced when regulations regarding AI systems become more strict: The EU’s high level expert group on AI has already recognized “human autonomy and oversight” as a key requirement for trustworthy AI in their Ethics Guidelines for Trustworthy AI (Smuha, 2019). In the future, solutions found with RL might thus be required by law to exhibit features that enable more human control.\n\nIn this paper, we thus propose a simple method to provide users with more control over how an offline RL policy will behave after deployment. The algorithm that we develop trains a conditional policy, that can after training adapt the trade-off between proximity to the data generating policy on\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nthe one hand and estimated performance on the other. Close proximity to a known solution naturally facilitates trust, enabling conservative users to choose behavior they are more inclined to confidently deploy. That way, users may benefit from the automation provided by offline RL (users don’t need to handcraft controllers, possibly even interactively choose actions) yet still remain in control as they can e.g. make the policy move to a more conservative or more liberal trade-off. We show how such an algorithm can be designed, as well as compare its performance with a variety of offline RL baselines and show that a user can achieve state of the art performance with it. Furthermore, we show that our method has advantages over simpler approaches like training many policies with diverse hyperparameters. Finally, since we train a policy conditional on one of the most important hyperparameters in offline RL, we show how a user could potentially use it to tune this hyperparameter. In many cases of our evaluations, this works almost regret-free, since we observe that the performance as a function of the hyperparameter is mostly a smooth function.\n\n2 RELATED WORK\n\nOffline RL Recently, a plethora of methods has been published that learn policies from static datasets. Early works, such as FQI and NFQ (Ernst et al., 2005; Riedmiller, 2005), were termed batch instead of offline since they didn’t explicitly address issue that the data collection cannot be influenced. Instead, similarly to other batch methods (Depeweg et al., 2016; Hein et al., 2018; Kaiser et al., 2020), they assumed a uniform random data collection that made generalization to the real environment simpler.\n\nAmong the first to explicitly address the limitations in the offline setting under unknown data collection were SPIBB(-DQN) (Laroche et al., 2019) in the discrete and BCQ (Fujimoto et al., 2019) in the continuous actions case. Many works with different focuses followed: Some treat discrete MDPs and come with provable bounds on the performance at least with a certain probability Thomas et al. (2015); Nadjahi et al. (2019), however many more focused on the continuous setting: EMaQ, BEAR, BRAC, ABM, various DICE based methods, REM, PEBL, PSEC-TD-0, CQL, IQL, BAIL, CRR, COIL, O-RAAC, OPAL, TD3+BC, and RvS (Ghasemipour et al., 2021; Kumar et al., 2019; Wu et al., 2019; Siegel et al., 2020; Nachum et al., 2019; Zhang et al., 2020; Agarwal et al., 2020; Smit et al., 2021; Pavse et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021; Chen et al., 2019; Wang et al., 2020; Liu et al., 2021; Urpí et al., 2021; Ajay et al., 2020; Brandfonbrener et al., 2021; Emmons et al., 2021) are just a few of the proposed model-free methods over the last few years. Additionally, many model-based as well as hybrid approaches have been proposed, such as MOPO, MOReL, MOOSE, COMBO, RAMBO, and WSBC (Yu et al., 2020; Kidambi et al., 2020; Swazinna et al., 2021b; Yu et al., 2021; Rigter et al., 2022; Swazinna et al., 2021a). Even approaches that train policies purely supervised, by conditioning on performance, have been proposed (Peng et al., 2019; Emmons et al., 2021; Chen et al., 2021). Model based algorithms more often use model uncertainty, while model-free methods use a more direct behavior regularization approach.\n\nOffline policy evaluation or offline hyperparameter selection is concerned with evaluating (or at least ranking) policies that have been found by an offline RL algorithm, in order to either pick the best performing one or to tune hyperparameters. Often, dynamics models are used to evaluate policies found in model-free algorithms, however also model-free evaluation methods exist (Hans et al., 2011; Paine et al., 2020; Konyushova et al., 2021; Zhang et al., 2021b; Fu et al., 2021). Unfortunately, but also intuitively, this problem is rather hard since if any method is found that can more accurately assess the policy performance than the mechanism in the offline algorithm used for training, it should be used instead of the previously employed method for training. Also, the general dilemma of not knowing in which parts of the state-action space we know enough to optimize behavior seems to always remain. Works such as Zhang et al. (2021a); Lu et al. (2021) become applicable if limited online evaluations are allowed, making hyperparameter tuning much more viable.\n\nOffline RL with online adaptation Other works propose an online learning phase that follows after offline learning has conceded. In the most basic form, Kurenkov & Kolesnikov (2021) introduce an online evaluation budget that lets them find the best set of hyperparameters for an offline RL algorithm given limited online evaluation resources. In an effort to minimize such a budget, Yang et al. (2021) train a set of policies spanning a diverse set of uncertainty-performance trade-offs. Ma et al. (2021) propose a conservative adaptive penalty, that penalizes unknown behavior more during the beginning and less during the end of training, leading to safer policies during training. In Pong et al.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n(2021); Nair et al. (2020); Zhao et al. (2021) methods for effective online learning phases that follow the offline learning phase are proposed. In contrast to these methods, we are not aiming for a fully automated solution. Instead, we want to provide the user with a valuable tool after training, so we do not propose an actual online phase, also since practitioners may find any performance deterioration inacceptable. To the best of our knowledge, no prior offline RL method produces policies that remain adaptable after deployment without any further training.\n\n3 LION: LEARNING IN INTERACTIVE OFFLINE ENVIRONMENTS\n\nIn this work, we address two dilemmas of the offline RL setting: First and foremost, we would like to provide the user with a high level control option in order to influence the behavior of the policy, since we argue that the user is crucially important for solving the task and not to be made obsolete by an algorithm. Further we address the issue that in offline RL, the correct hyperparameter controlling the trade-off between conservatism and performance is unknown and can hardly be tuned. By training a policy conditioned in the proximity hyperparameter, we aim to enable the user to find a good trade-off hyperparameter. Code will be made available at https://github.com/pswazinna/LION.\n\nAs mentioned, behavior cloning, will most likely yield the most trustworthy solution due to its familiarity, however the solution is of very limited use since it does not outperform the previous one. Offline RL on the other hand is problematic since we cannot simply evaluate policy candidates on the real system and offline policy evaluation is still an open problem (Hans et al., 2011; Paine et al., 2020; Konyushova et al., 2021; Zhang et al., 2021b; Fu et al., 2021). In the following, we thus propose a solution that moves the hyperparameter choice from training to deployment time, enabling the user to interactively find the desired trade-off between BC and offline optimization. A user may then slowly move from conservative towards better solutions.\n\n3.1 TRAINING\n\nDuring training time, we optimize three components: A model of the original policy βφ(s), an ensemble of transition dynamics models {f i (s, a)|i ∈ 0, . . . , N − 1}, as well as the user adaptive ψi policy πθ(s, λ). The dynamics models {f i} as well as the original policy β are trained in isolation before the actual policy training starts. Both π and β are always simple feedforward neural networks which map states directly to actions in a deterministic fashion (practitioners likely favor deterministic policies over stochastic ones due to trust issues). β is trained to simply imitate the behavior present in the dataset by minimizing the mean squared distance to the observed actions:\n\nL(φ) =\n\n1 N\n\n(cid:88)\n\n[at − βφ(st)]2\n\nst,at∼D\n\n(1)\n\nDepending on the environment, the transition models are either also feedforward networks or simple recurrent networks with a single recurrent layer. The recurrent networks build their hidden state over G steps and are then trained to predict a window of size F into the future (similarly to (Hein et al., 2017b)), while the feedforward dynamics simply predict single step transitions. Both use mean squared error as loss:\n\nL(ψi) =\n\nL(ψi) =\n\n1 N\n\n1 N\n\n(cid:88)\n\n(cid:2)st+1 − f i\n\nψi\n\n(st, at)(cid:3)2\n\nst,at,st+1∼D\n\n(2)\n\n(cid:88)\n\nF (cid:88)\n\nt∼D\n\nf =1\n\n[st+G+f +1 − f i ψi\n\n(st, at, . . . st+G, at+G, . . . ˆst+G+f , at+G+f )]2\n\nwhere ˆst+H+f are the model predictions that are fed back to be used as input again. For simplicity, in this notation we assume the reward to be part of the state. Also we do not explicitly show the recurrence and carrying over of the hidden states.\n\nAfter having trained the two components βφ(s) and {f i (s, a)}, we can then move on to policy ψi training. Similarly to MOOSE and WSBC, we optimize the policy πθ by sampling start states from D and performing virtual rollouts throughout the dynamics ensemble using the current policy candidate. In every step, the ensemble predicts the reward as the minimum among its members and the next state\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nthat goes with it. At the same time we collect the mean squared differences between the actions that πθ took in the rollout and the one that βφ would have taken. The loss is then computed as a weighted sum of the two components. Crucially, we sample the weighting factor λ randomly and pass it to the policy as an additional input - the policy thus needs to learn all behaviors ranging from pure behavior cloning to entirely free optimization:\n\nL(θ) = −\n\n(cid:88)\n\nT (cid:88)\n\ns0∼D\n\nt\n\nγt[λe(st, at) − (1 − λ)p(at)]\n\nat = πθ(st, λ)\n\n(3)\n\nwhere we sample λ between 0 & 1, e(st, at) = min{r(f i ψi(st, at))|i ∈ 0, ..., N − 1} denotes the output of the ensemble prediction for reward (we omit explicit notation of recurrence for simplicity) and p(at) = [βψ(st) − at]2 denotes the penalty based on the mean squared distance between the original policy and the actions proposed by πθ. See Fig. 1 for a visualization of our proposed training procedure.\n\nFigure 1: Schematic of LION policy training. During policy training (Eq. 3) only πθ (in green) is adapted, while the original policy model βφ (orange) and the dynamics ensemble {fψi} (blue) are already trained and remain unchanged. From left to right, we first sample a start state (black) from the dataset and a λ value from its distribution. Then, we let the original policy (orange) as well as the currently trained policy (green) predict actions - note that the newly trained policy is conditioned on λ. Both actions are then compared to calculate the penalty for that timestep (red). The action from the currently trained policy is then also fed into the trained transition model (blue) together with the current state (black / blue), to get the reward for that timestep (yellow) as well as the next state (blue). This procedure is repeated until the horizon of the episode is reached. The rewards and penalties are then summed up and weighted by λ to be used as a loss function for policy training.\n\nWe motivate our purely model-based approach (no value function involved) with the fact that we have fewer moving parts: Our ensemble can be kept fixed once it is trained, while a value function has to be learned jointly with πθ, which is in our case more complex than usual. See experimental results in Fig. 10 a brief attempt at making our approach work in the model-free domain.\n\nIn addition to Eq. 3, we need to penalize divergence not only from the learned model of the original policy during virtual roll-outs, but also from the actual actions in the dataset at λ = 0. It seems that if this is not done, the trained policy π sticks to the (also trained) original policy β during the rollouts, but during those rollouts, there are states that did not appear in the original dataset, enabling π to actually diverge from the true trajectory distribution. We thus penalize both rollout as well as data divergence at λ = 0:\n\nL(θ) = −\n\n(cid:88)\n\nT (cid:88)\n\ns0∼D\n\nt\n\nγt[λe(st, at) − (1 − λ)p(at)] + η\n\n(cid:88)\n\ns,a∼D\n\n[a − π(s, λ = 0)]2\n\n(4)\n\nwhere η controls the penalty weight for not following dataset actions at λ = 0, see Appendix A for more details. Furthermore, we normalize states to have zero mean and unit variance during every\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nforward pass through dynamics model or policy, using the mean and standard deviation observed in the dataset. We also normalize the rewards provided by the ensemble rt = e(st, at), so that they live in the same magnitude as the action penalties (we assume actions to be in [−1, 1]D, so that the penalty can be in [0, 4]D where D is the action dimensionality).\n\nIntuitively, one might choose to sample λ uniformly between zero and one, however instead we choose a beta distribution with parameters (0.1, 0.1), which could be called bathtub-shaped. Similarly to (Seo et al., 2021), we find that it is important to put emphasis on the edge cases, so that the extreme behavior is properly learned, rather than putting equal probability mass on each value in the [0, 1] range. The interpolation between the edges seems to be easier and thus require less samples. Fig. 11 shows policy results for different lambda distributions during training.\n\n3.2 DEPLOYMENT\n\nAt inference time, the trained policy can at any point be influenced by the user that would otherwise be in control of the system, by choosing the λ that is passed to the policy together with the current system state to obtain an action:\n\nAlgorithm 1 LION (Training)\n\n1: Require Dataset D = {τi}, randomly initialized parameters θ, φ, ψ, lambda distribution parameters Beta(a, b), horizon H, number of policy updates U\n\nψi with D and Equation 2\n\nsample start states S0 ∼ D sample lambda values λ ∼ Beta(a, b) initialize policy loss L(θ) = 0 for t in 0..H do\n\n2: // dynamics and original policy models can be trained supervised and independently of other components 3: train original policy model βφ using D and Equation 1 4: train dynamics models f i 5: for j in 1..U do 6: 7: 8: 9: 10: 11: 12: 13:\n\nat = πθ(st, λ) λ ∈ User(st). (5) He or she may choose to be conservative or adventurous, observe the feedback and always adjust the proximity parameter of the policy accordingly. At this point, any disliked behavior can immediately be corrected without any time loss due to re-training and deploying a new policy, even if the user’s specific preferences were not known at training time. We propose to initially start with λ = 0 during deployment, in order to check whether the policy is actually able to reproduce the original policy and to gain the user’s trust in the found solution. Then, depending on how critical failures are and how much time is at hand, λ may be increased in small steps for as long as the user is still comfortable with the observed behavior. Figure 3 shows an example of how the policy behavior changes over the course of λ. Once the performance stops to increase or the user is otherwise not satisfied, we can immediately return to the last satisfying λ value.\n\nψi(st, at) i = arg mini{r(f i L(θ)+ = −γt[λrt − (1 − λ)p(at)] update πθ using gradient ∇θL(θ) and Adam\n\ncalculate policy actions at = πθ(st, λ) calculate behavioral actions bt = βφ(st) calculate penalty term p(at) = [βψ(st) − at]2 rt, st+1 = f i\n\n15: 16: return πθ;\n\nψi(st, at))}\n\ns.t.\n\n14:\n\n4 EXPERIMENTS\n\nAt first, we intuitively showcase LION in a simple 2D-world in order to get an understanding of how the policy changes its behavior based on λ. Afterwards, we move to a more serious test, evaluating our algorithm on the 16 industrial benchmark (IB) datasets (Hein et al., 2017a; Swazinna et al., 2021b). We aim to answer the following questions:\n\n• Do LION policies behave as expected, i.e. do they reproduce the original policy at λ = 0\n\nand deviate more and more from it with increased freedom to optimize for return?\n\n• Do LION policies at least in parts of the spanned λ space perform better or similarly well to\n\nstate of the art offline RL algorithms?\n\n• Is it easy to find the λ values that maximize return for practitioners? That is, are the\n\nperformance courses smooth or do they have multiple local mini- & maxima?\n\n• Is it possible for users to exploit the λ regularization at runtime to restrict the policy to only\n\nexhibit behavior he or she is comfortable with?\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n4.1\n\n2D-WORLD\n\nσ\n\n√\n\n2π\n\nWe evaluate the LION approach on a simplistic 2D benchmark. The states are x & y coordinates in the environment and rewards are given based on the position of the agent, following a Gaussian distribution around a fixed point in the e−0.5((st−μ)/σ)2 state space, i.e. r(st) = 1 .\nIn this example we set μ = (3, 6)T and σ = (1.5, 1.5)T. A visualization of the reward distribution can be seen in Fig. 2 (b). We collect data from the environment using a simple policy that moves either to position (2.5, 2.5)T or to (7.5, 7.5)T, depending on which is closer to the randomly drawn start state (shown in Fig. 2(a)), adding ε = 10% random actions as exploration. Then we follow the outlined training procedure, by training a transition model, original policy model and finally a new policy that can at runtime change its behavior based on the desired proximity to the original policy. Fig. 3 shows policy maps for λ ∈ {0.0, 0.6, 0.65, 0.7, 0.85, 1.0}, moving from simply imitating the original policy, over different mixtures, to pure return optimization. Since the task is easy and accurately modeled by the dynamics ensemble, one may give absolute freedom to the policy and optimize for return only. As it can be seen, the policy moves quickly to the center of the reward distribution for λ = 1.\n\nFigure 2: (a) Original policy for data collection and - color represents action direction (b) reward distribution in the 2D environment - color represents reward value\n\nFigure 3: Policy maps for increasing values of λ in the 2D environment - colors represent action direction. Initially, the policy simply imitates the original policy (see Fig. 2 (a)). With increased freedom, the policy moves less to the upper right and more to the bottom left goal state of the original policy, since that one is closer to the high rewards. Then, the policy moves its goal slowly upwards on the y-axis until it is approximately at the center of the reward distribution. Since enough data was available (1,000 interactions) and the environment so simple, the models capture the true dynamics well and the optimal solution is found at λ = 1. This is however not necessarily the case if not enough or not the right data was collected (e.g. due to a suboptimal original policy - see Fig. 4).\n\n4.2\n\nINDUSTRIAL BENCHMARK\n\nDatasets We evaluate LION on the industrial benchmark datasets initially proposed in (Swazinna et al., 2021b). The 16 datasets are created with three different baseline original policies (optimized, mediocre, bad) mixed with varying degrees of exploration. The optimized baseline is an RL trained policy and simulates an expert practitioner. The mediocre baseline moves the system back and forth around a fixed point that is rather well behaved, while the bad baseline steers to a point on the edge of the state space in which rewards are deliberately bad. Each baseline is combined with ε ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}-greedy exploration to collect a dataset (making the ε = 0.0 datasets extreme cases of the narrow distribution problem). Together, they constitute a diverse set of offline RL settings. The exact baseline policies are given by:\n\nπbad =\n\n \n\n\n\n100 − vt 100 − gt 100 − ht\n\nπmed =\n\n \n\n\n\n25 − vt 25 − gt 25 − ht\n\nπopt =\n\n \n\n\n\n− ̃vt−5 − 0.91 2 ̃ft−3 − ̃p + 1.43 −3.48 ̃ht−3 − ̃ht−4 + 2 ̃p + 0.81\n\nThe datasets contain 100,000 interactions collected by the respective baseline policy combined with the ε-greedy exploration. The IB is a high dimensional and partially observable environment - if access to the full Markov state were provided, it would contain 20 state variables. Since only six of those are observable, and the relationship to the other variables and their subdynamics are complex\n\n6\n\n02468100246810(a) Behavioral Policy w/o noise02468100246810(b) 2D Env reward distribution00.51.01.52.0angle to (1,0)00.51.0reward02468100246810Policymap - =0.00246810Policymap - =0.60246810Policymap - =0.650246810Policymap - =0.70246810Policymap - =0.850246810Policymap - =1.000.51.52Published as a conference paper at ICLR 2023\n\nand feature heavily delayed components, prior work Hein et al. (2017b) has stated that up to 30 past time steps are needed to form a state that can hope to recover the true dynamics, so the state can be considered 180 dimensional. In our case we thus set the number of history steps G = 30. The action space is 3 dimensional. The benchmark is not supposed to mimic a single industrial application, but rather exhibit common issues observable in many different applications (partial observability, delayed rewards, multimodal and heteroskedastic noise, ...). The reward is a weighted combination of the observable variables fatigue and consumption, which are conflicting (usually move in opposite directions and need trade-off) and are influenced by various unobservable variables. As in prior work Hein et al. (2018); Depeweg et al. (2016); Swazinna et al. (2021b) we optimize for a horizon of 100. The datasets are available at https://github.com/siemens/industrialbenchmark/ tree/offline_datasets/datasets under the Apache License 2.0.\n\nFigure 4: Evaluation performance (top portion of each graph) and distance to the original policy (lower portion of each graph) of the LION approach over the chosen λ hyperparameter. Various state of the art baselines are added as dashed lines with their standard set of hyperparameters (results from (Swazinna et al., 2022)). Even though the baselines all exhibit some hyperparameter that controls the distance to the original policy, all are implemented differently and we can neither map them to a corresponding lambda value of our algorithm, nor change the behavior at runtime, which is why we display them as dashed lines over the entire λ-spectrum. See Fig. 12 for the 100% exploration dataset.\n\nBaselines We compare performances of LION with various state of the art offline RL baselines:\n\n• BEAR, BRAC, BCQ, CQL and TD3+BC (Kumar et al., 2019; Wu et al., 2019; Fujimoto et al., 2019; Kumar et al., 2020; Fujimoto & Gu, 2021) are model-free algorithms. They mostly regularize the policy by minimizing a divergence to the original policy. BCQ samples only likely actions and CQL searches for a Q-function that lower bounds the true one.\n\n• MOOSE and WSBC (Swazinna et al., 2021a;b) are purely model based algorithms that optimize the policy via virtual trajectories through the learned transition model. MOOSE penalizes reconstruction loss of actions under the original policy (learned by an autoencoder), while WSBC constrains the policy directly in weight space. MOOSE is from the policy training perspective the closest to our LION approach.\n\n• MOPO and MOReL (Yu et al., 2020; Kidambi et al., 2020) are hybrid methods that learn a transition model as well as a value function. Both use the models to collect additional data and regularize the policy by means of model uncertainty. MOPO penalizes uncertainty directly, while MOReL simply stops episodes in which future states become too unreliable. MOReL uses model-disagreement and MOPO Gaussian outputs to quantify uncertainty.\n\n7\n\n300200100returnIB bad 0.00.000.250.500.751.00- Optimization Mixture Parameter01dist. to org.300200100IB bad 0.20.000.250.500.751.00- Optimization Mixture Parameter01200150100IB bad 0.40.000.250.500.751.00- Optimization Mixture Parameter0.250.500.7515012510075IB bad 0.60.000.250.500.751.00- Optimization Mixture Parameter0.250.5012010080IB bad 0.80.000.250.500.751.00- Optimization Mixture Parameter0.250.500.75150100returnIB mediocre 0.00.000.250.500.751.00- Optimization Mixture Parameter12dist. to org.150100IB mediocre 0.20.000.250.500.751.00- Optimization Mixture Parameter1215012510075IB mediocre 0.40.000.250.500.751.00- Optimization Mixture Parameter12908070IB mediocre 0.60.000.250.500.751.00- Optimization Mixture Parameter0.51.01.575706560IB mediocre 0.80.000.250.500.751.00- Optimization Mixture Parameter0.20.47060returnIB optimized 0.00.000.250.500.751.00- Optimization Mixture Parameter0.51.01.5dist. to org.6560IB optimized 0.20.000.250.500.751.00- Optimization Mixture Parameter0.51.062.560.057.5IB optimized 0.40.000.250.500.751.00- Optimization Mixture Parameter0.250.500.7565.062.560.057.5IB optimized 0.60.000.250.500.751.00- Optimization Mixture Parameter0.00.56560IB optimized 0.80.000.250.500.751.00- Optimization Mixture Parameter0.00.5LION (ours)MOOSEWSBCBRAC-vBEARBCQTD3+BCCQLMOPOMOReL (below Y-range)LION (distance to original)Published as a conference paper at ICLR 2023\n\nEvaluation In order to test whether the trained LION policies are able to provide state of the art performance anywhere in the λ range, we evaluate them for λ from 0 to 1 in many small steps. Figs. 4 and 12 show results for the 16 IB datasets. We find that the performance curves do not exhibit many local optima. Rather, there is usually a single maximum before which the performance is rising and after which the performance is strictly dropping. This is a very desirable characteristic for usage in the user interactive setting, as it enables users to easily find the best performing λ value for the policy. In 13 out of 16 datasets, users can thus match or outperform the current state of the art method on that dataset, and achieve close to on-par performance on the remaining three. The distance-to-original-policy curves are even monotonously increasing from start to finish, making it possible for the practitioner to find the best solution he or she is still comfortable with in terms of distance to the familiar behavior.\n\nDiscrete baseline A simpler approach might be to train an existing offline RL algorithm for many trade-offs in advance, to provide at least discrete options. Two downsides are obvious: (a) we wouldn’t be able to handle the continuous case, i.e. when a user wants a trade-off that lies between two discrete policies, and (b) the computational cost increases linearly with the number of policies trained. We show that a potentially even bigger issue exists in Figure 5: When we train a discrete collection of policies with different hyperparameters, completely independently of each other, they often exhibit wildly different behaviors even when the change in hyperparameter was small. LION instead expresses the collection as a single policy network, training them jointly and thus forcing them to smoothly interpolate among each other. This helps to make the performance a smooth function of the hyperparameter (although this must not always be the case) and results in a performance landscape that is much easier to navigate for a user searching for a good trade-off.\n\nFigure 5: Prior offline RL algorithms like MOPO do not behave consistently when trained across a range of penalizing hyperparameters.\n\nReturn conditioning baseline Another interesting line of work trains policies conditional on the return to go, such as RvS (Emmons et al., 2021) (Reinforcement Learning via Supervised Learning) or DT (Chen et al., 2021) (Decision Transformer). A key advantage of these methods is their simplicity - they require neither transition model nor value function, just a policy suffices, and the learning can be performed in an entirely supervised fashion. The resulting policies could be interpreted in a similar way as LION policies: Conditioning on returns close to the original performance would result in the original behavior, while choosing to condition on higher returns may lead to improved performance if the extrapolation works well. In Fig. 6 we report results of the RvS algorithm on the same datasets as the discrete baseline. The returns in the datasets do not exhibit a lot of variance, so it is unsurprising that the approach did not succeed in learning a lot of different behaviors.\n\nFigure 6: Return conditioned policies did not learn many different behaviors on the IB datasets.\n\n8\n\n0.00.51.01.52.02.5 (Penalty Parameter of MOPO)1401301201101009080Evaluation ReturnBad-0.4 - Multi Hyperparameter Baseline0.00.51.01.52.02.5 (Penalty Parameter of MOPO)120110100908070Evaluation ReturnMediocre-0.0 - Multi Hyperparameter Baseline0.00.51.01.52.02.5 (Penalty Parameter of MOPO)95908580757065Evaluation ReturnOptimized-0.6 - Multi Hyperparameter Baselinemopo300275250225200175150125100Conditioned Return20018016014012010080Evaluation ReturnBad-0.4RvS-R8078767472706866Conditioned Return120110100908070Evaluation ReturnMediocre-0.072.570.067.565.062.560.057.555.0Conditioned Return1009590858075706560Evaluation ReturnOptimized-0.6Published as a conference paper at ICLR 2023\n\nFinding a suitable λ We would like to emphasize that we neither want to optimize all offline hyperparameters with our solution, nor are we interested in a fully automated solution. Users may thus adopt arbitrary strategies to finding their personal trade-off of preference. We will however provide a conservative example strategy: The operator starts with the most conservative value available and then moves in small, but constant steps towards more freedom. Whenever the performance drops below the previous best or the baseline performance, he immediately stops and uses the last λ before that. Table 1 summarizes how this strategy would perform.\n\nDataset Bad-0.4 Mediocre-0.0 Optimized-0.6\n\nRvS\n\nLION MOPO Final λ Final λ Final ˆR Return -102.4 -70.8 -58.9\n\nLION MOPO Return -107.2 -87.8 -72.4\n\n-287 -81 -71\n\n1.0 0.2 0.35\n\n2.4 2.4 2.5\n\nRvS Return -158.2 -86.6 -61.9\n\nTable 1: If a user adopts the simple strategy of moving in small steps (0.05 for LION, 0.1 for MOPO since its range is larger, 10.0 / 1.0 for RvS) from conservative towards better solutions, immediately stopping when a performance drop is observed, LION finds much better solutions due to the consistent interpolation between trade-offs. Note that in MOPO, we start with large λ = 2.5 (1.0 is the default) since there it controls the penalty, while we start with λ = 0 in LION, where it controls the return.\n\n5 DISCUSSION & CONCLUSION\n\nIn this work we presented a novel offline RL approach that, to the best of our knowledge, is the first to let the user adapt the policy behavior after training is finished. We let the user tune the behavior by allowing him to choose the desired proximity to the original policy, in an attempt to solve two issues: (1) The problem that practitioners cannot tune the hyperparameter in offline RL & (2) the general issue that users have no high level control option when using RL policies (they might even have individual preferences with regards to the behavior of a policy that go beyond just performance).\n\nWe find that effectively, LION provides a high level control option to the user, while still profiting from a high level of automation. It furthermore takes much of the risk that users normally assume in offline RL away since deployments can always start with a BC policy when they start at λ = 0, before moving to better options. While behavior cloning does not have to work in general, we did not experience any issues with it in our experiments, and it should be easier than performing RL since it can be done entirely in a supervised fashion. Given that BC works, deployments can thus start with minimal risk. In prior offline algorithms, users experienced the risk that the algorithm did not produce a satisfying policy on the particular dataset they chose. E.g.: WSBC produces state of the art results for many of the IB datasets, however for mediocre-0.6 it produces a catastrophic -243 (original performance is -75). Similarly, CQL is the prior best method on optimized-0.8, however the same method produces a performance of -292 on bad-0.2 (MOOSE, MOPO, and WSBC get between -110 & -130). Due to the smoothness of the interpolation of behaviors in LION, practitioners should be able to use it to find better trade-offs with lower risk than prior methods. Adaptable policies are thus likely a step towards more deployments in industrial applications.\n\nFuture Work As outlined at the end of Section C of the appendix, we were unable to incorporate value functions into our approach. This can be seen as a limiting factor, since there exist environments with sparse or very delayed rewards or that for other reasons exhibit long planning horizons. The industrial benchmark features delayed rewards and evaluation trajectories are 100 steps long, however other environments can be more extreme in their characteristics. At some point, even the best dynamics models suffer from compounding errors and cannot accurately predict the far away future. We do not believe that it is in principle not possible to combine the LION approach with value functions, however future work will likely need to find methods to stabilize the learning process.\n\nOther potential limitations of our approach include difficulties with the behavior cloning, e.g. when the original policy is stochastic or was not defined in the same state space as we use (e.g. different human operators controlled the system at different times in the dataset), as well as difficulties when interpolating between vastly different behaviors on the Pareto front spanned by proximity and performance. We mention these potential limitations only for the sake of completeness since we were unable to observe them in our practical experiments.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n6 ETHICS STATEMENT\n\nGenerally, (offline) reinforcement learning methods can be used to optimize control policies for many different systems beyond the control of the researchers. Malicious individuals may thus use findings in manners that negatively impact others or society as a whole (e.g. by using it in weapons). Faulty policies could also potentially result in other safety concerns (e.g. policies for heavy machinery). In ML research, there is also always the concern that increased automation may lead to rising unemployment. While some institutions like the UN and the WEF are convinced AI will lead to long term job growth, Bordot (2022) found that a slightly increased unemployment risk can be found empirically.\n\nBeyond these usual concerns that mostly apply to ML/RL research as a whole, rather than our individual paper, we believe that no additional considerations are necessary for our method. If at all, the societal impact should be positive, since our solution fosters trust by practitioners and should thus increase the usage and human-machine collaborative deployment of offline RL policies in practice, enabling more efficient control strategies in many industrial domains.\n\nREFERENCES\n\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR, 2020.\n\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline primitive discovery for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.\n\nVarun Babbar, Umang Bhatt, and Adrian Weller. On the utility of prediction sets in human-AI teams.\n\narXiv preprint arXiv:2205.01411, 2022.\n\nFlorent Bordot. Artificial intelligence, robots and unemployment: Evidence from OECD countries.\n\nJournal of Innovation Economics Management, 37(1):117–138, 2022.\n\nDavid Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline RL without off-policy evaluation. Advances in Neural Information Processing Systems, 34:4933–4946, 2021.\n\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems, pp. 1–14, 2019.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021.\n\nXinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. BAIL: Best-action imitation learning for batch deep reinforcement learning. arXiv preprint arXiv:1910.12179, 2019.\n\nMaria De-Arteaga, Artur Dubrawski, and Alexandra Chouldechova. Leveraging expert consistency\n\nto improve algorithmic decision support. arXiv preprint arXiv:2101.09648, 2021.\n\nStefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, and Steffen Udluft. Learning and policy search in stochastic dynamical systems with Bayesian neural networks. arXiv preprint arXiv:1605.07127, 2016.\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. RvS: What is essential for\n\noffline RL via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n\nDamien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. J.\n\nMach. Learn. Res., 6:503–556, December 2005. ISSN 1532-4435.\n\nM Milani Fard and Joelle Pineau. Non-deterministic policies in Markovian decision processes.\n\nJournal of Artificial Intelligence Research, 40:1–24, 2011.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJustin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R Zhang, Yutian Chen, Aviral Kumar, et al. Benchmarks for deep off-policy evaluation. arXiv preprint arXiv:2103.16596, 2021.\n\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\n\nAdvances in Neural Information Processing Systems, 34, 2021.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.\n\nSeyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max Q-learning operator for simple yet effective offline and online RL. In International Conference on Machine Learning, pp. 3682–3691. PMLR, 2021.\n\nAlexander Hans, Siegmund Duell, and Steffen Udluft. Agent self-assessment: Determining policy quality without execution. In 2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), pp. 84–90. IEEE, 2011.\n\nDaniel Hein, Stefan Depeweg, Michel Tokic, Steffen Udluft, Alexander Hentschel, Thomas A Runkler, and Volkmar Sterzing. A benchmark environment motivated by industrial control problems. In 2017 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 1–8. IEEE, 2017a.\n\nDaniel Hein, Steffen Udluft, Michel Tokic, Alexander Hentschel, Thomas A Runkler, and Volkmar Sterzing. Batch reinforcement learning on the industrial benchmark: First experiences. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 4214–4221. IEEE, 2017b.\n\nDaniel Hein, Steffen Udluft, and Thomas A Runkler. Interpretable policies for reinforcement learning by genetic programming. Engineering Applications of Artificial Intelligence, 76:158–169, 2018.\n\nMarkus Kaiser, Clemens Otte, Thomas A Runkler, and Carl Henrik Ek. Bayesian decomposition of\n\nmulti-modal dynamical systems for reinforcement learning. Neurocomputing, 2020.\n\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. Advances in neural information processing systems, 33: 21810–21823, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nKsenia Konyushova, Yutian Chen, Thomas Paine, Caglar Gulcehre, Cosmin Paduraru, Daniel J Mankowitz, Misha Denil, and Nando de Freitas. Active offline policy selection. Advances in Neural Information Processing Systems, 34, 2021.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\n\nq-learning. arXiv preprint arXiv:2110.06169, 2021.\n\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy Q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, pp. 11761–11771, 2019.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n\nVladislav Kurenkov and Sergey Kolesnikov. Showing your offline reinforcement learning work:\n\nOnline evaluation budget matters. arXiv preprint arXiv:2110.04156, 2021.\n\nRomain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with baseline bootstrapping. In International Conference on Machine Learning, pp. 3652–3661. PMLR, 2019.\n\nMinghuan Liu, Hanye Zhao, Zhengyu Yang, Jian Shen, Weinan Zhang, Li Zhao, and Tie-Yan Liu. Curriculum offline imitating learning. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nCong Lu, Philip Ball, Jack Parker-Holder, Michael Osborne, and Stephen J Roberts. Revisiting design choices in offline model based reinforcement learning. In International Conference on Learning Representations, 2021.\n\nYecheng Jason Ma, Andrew Shen, Osbert Bastani, and Dinesh Jayaraman. Conservative and adaptive penalty for model-based safe reinforcement learning. arXiv preprint arXiv:2112.07701, 2021.\n\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in Neural Information Processing Systems, 32, 2019.\n\nKimia Nadjahi, Romain Laroche, and Rémi Tachet des Combes. Safe policy improvement with soft baseline bootstrapping. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 53–68. Springer, 2019.\n\nAshvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online\n\nreinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.\n\nTom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement learning. arXiv preprint arXiv:2007.09055, 2020.\n\nBrahma Pavse, Ishan Durugkar, Josiah Hanna, and Peter Stone. Reducing sampling error in batch temporal difference learning. In International Conference on Machine Learning, pp. 7543–7552. PMLR, 2020.\n\nXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n\nVitchyr H Pong, Ashvin Nair, Laura Smith, Catherine Huang, and Sergey Levine. Offline metareinforcement learning with online self-supervision. arXiv preprint arXiv:2107.03974, 2021.\n\nMartin Riedmiller. Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning, pp. 317–328. Springer, 2005.\n\nMarc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-RL: Robust adversarial model-based offline\n\nreinforcement learning. arXiv preprint arXiv:2204.12581, 2022.\n\nAlbrecht Schmidt, Fosca Giannotti, Wendy Mackay, Ben Shneiderman, and Kaisa Väänänen. Artificial intelligence for humankind: A panel on how to create truly interactive and human-centered AI for the benefit of individuals and society. In IFIP Conference on Human-Computer Interaction, pp. 335–339. Springer, 2021.\n\nSungyong Seo, Sercan Arik, Jinsung Yoon, Xiang Zhang, Kihyuk Sohn, and Tomas Pfister. Controlling neural networks with rule representations. Advances in Neural Information Processing Systems, 34, 2021.\n\nBen Shneiderman. Human-centered artificial intelligence: three fresh ideas. AIS Transactions on\n\nHuman-Computer Interaction, 12(3):109–124, 2020.\n\nNoah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, and Martin Riedmiller. Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396, 2020.\n\nJordi Smit, Canmanie T Ponnambalam, Matthijs TJ Spaan, and Frans A Oliehoek. PEBL: Pessimistic ensembles for offline deep reinforcement learning. In Robust and Reliable Autonomy in the Wild Workshop at the 30th International Joint Conference of Artificial Intelligence, 2021.\n\nNathalie Smuha. Ethics guidelines for trustworthy AI. European Commission, Directorate-General\n\nfor Communications Networks, Content and Technology Publications Office, 2019.\n\nPhillip Swazinna, Steffen Udluft, Daniel Hein, and Thomas Runkler. Behavior constraining in weight\n\nspace for offline reinforcement learning. arXiv preprint arXiv:2107.05479, 2021a.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nPhillip Swazinna, Steffen Udluft, and Thomas Runkler. Overcoming model bias for robust offline deep reinforcement learning. Engineering Applications of Artificial Intelligence, 104:104366, 2021b.\n\nPhillip Swazinna, Steffen Udluft, Daniel Hein, and Thomas Runkler. Comparing model-free and model-based algorithms for offline reinforcement learning. arXiv preprint arXiv:2201.05433, 2022.\n\nShengpu Tang, Aditya Modi, Michael Sjoding, and Jenna Wiens. Clinician-in-the-loop decision making: Reinforcement learning with near-optimal set-valued policies. In International Conference on Machine Learning, pp. 9387–9396. PMLR, 2020.\n\nPhilip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement. In International Conference on Machine Learning, pp. 2380–2388. PMLR, 2015.\n\nNúria Armengol Urpí, Sebastian Curi, and Andreas Krause. Risk-averse offline reinforcement\n\nlearning. arXiv preprint arXiv:2102.05371, 2021.\n\nZiyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. arXiv preprint arXiv:2006.15134, 2020.\n\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\n\narXiv preprint arXiv:1911.11361, 2019.\n\nYijun Yang, Jing Jiang, Tianyi Zhou, Jie Ma, and Yuhui Shi. Pareto policy pool for model-based offline reinforcement learning. In International Conference on Learning Representations, 2021.\n\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: Model-based offline policy optimization. In Advances in Neural Information Processing Systems, volume 33, pp. 14129–14142, 2020.\n\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in Neural Information Processing Systems, 34, 2021.\n\nBaohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, André Biedenkapp, Kurtland Chua, Frank Hutter, and Roberto Calandra. On the importance of hyperparameter optimization for model-based reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pp. 4015–4023. PMLR, 2021a.\n\nMichael R Zhang, Tom Le Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, Ziyu Wang, and Mohammad Norouzi. Autoregressive dynamics models for offline policy evaluation and optimization. arXiv preprint arXiv:2104.13877, 2021b.\n\nShangtong Zhang, Bo Liu, and Shimon Whiteson. Gradientdice: Rethinking generalized offline estimation of stationary values. In International Conference on Machine Learning, pp. 11194– 11203. PMLR, 2020.\n\nYi Zhao, Rinu Boney, Alexander Ilin, Juho Kannala, and Joni Pajarinen. Adaptive behavior cloning\n\nregularization for stable offline-to-online reinforcement learning. 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA OTHER HYPERPARAMETERS\n\nSimilarly to other methods, we introduce a set of hyperparameters in our method either explicitly or implicitly. Any hyperparameter that is involved in training the original policy model βφ or the transition dynamics models {fψi } can obviously be tuned offline, by holding out a validation set of transitions from the original dataset and evaluating the trained models on them to test how well they predict dataset actions or future states and rewards. This is true for example for the architecture of the transition models, their learning rate, or the number of epochs to train. When we train the policy in offline reinforcement learning however, the equivalent of an evaluation dataset does not exist, since we cannot evaluate the policy on the true environment dynamics. While efforts are being made to make hyperparameter tuning possible via offline policy evaluation, the problem is hard for obvious reasons and we cannot really rely on them yet. Especially the arguably most important hyperparameter, how closely the newly trained policy needs to follow the actions proposed by the original policy, cannot realistically be tuned since it involves knowing when the evaluation method itself cannot be trusted any more. This is precisely why we propose the selection of this hyperparameter at runtime via user interaction. Our method however introduces some hyperparameters that are used during policy training and that could thus be seen as critical hyperparameters that cannot be tuned: η, controlling the penalty magnitude for not following the dataset actions at λ = 0, and the parameters of the Beta distribution from which λ is sampled during training. We did not tune them, since we simply copied the parameters from (Seo et al., 2021) so that λ ∼ Beta(0.1, 0.1) and also set η = 0.1 without any experiments. We would however like to point out, that these parameters are at least partially tunable even in the offline setting, since they do not depend on the accuracy of the transition models: η controls how strongly one adheres to the actions of the dataset, and we can easily asses whether this is working properly, by evaluating whether the final trained policy still adheres to the actions of the dataset at λ = 0. We evaluated policies for different η in Figure 7 (a) and show that there is not a great difference between different η, as long as it is not zero, so it seems reasonable to choose something small that is still doing the trick. Similarly, we show an evaluation for different parameters of the beta distribution in Fig. 7 (b). The parameters control how often which parts of the λ-spectrum are seen during training (see Fig. 11). As can be seen, the more the distribution tends towards a uniform distribution, the more the policy deviates from the original policy model at λ = 0, which is undesired. When more probability mass is focused on the edges of the distribution, we can see that the original policy model is reproduced more accurately. Both η and β can thus be reasonably well tuned entirely offline, even without using the transition models.\n\nFigure 7: Some hyperparameters like η and the parameters of the Beta distribution can be adjusted entirely offline, since they do not require evaluation on the true transition dynamics.\n\n14\n\n0.00.20.40.60.81.00.300.320.340.360.380.40Data Distance Impact @ =00.20.40.60.81.0x in Beta(x,x)0.0450.0500.0550.0600.0650.0700.0750.080Distance to Original Policy ModelBeta Distribution Impact @ =0Published as a conference paper at ICLR 2023\n\nB COMPLETE POLICY VISUALIZATION IN 2D-ENV\n\nIn Fig. 3, we chose only a set of interesting λ values for visualization for page limit reasons. See Fig 8 for a more fine-grained evaluation.\n\nFigure 8: Visualization for more λ values of the trained policy in the simple 2D Env than in Fig. 3.\n\nC MODEL-FREE EXPERIMENTS\n\nModel-free We motivate our purely model-based implementation with the idea that it should be simpler to implement compared to model-free policy training. When learning a policy with a value function, we have always two models that are trained jointly - the policy and the Q-function interact with each other and both are essentially a moving target. While it should be possible to derive a similar algorithm as LION in the model-free world, we were not quite able to do so: Figs. 9 and 10 show experiments in which we augment TD3+BC with our approach by sampling λ from the same distribution as in LION and providing it to the policy, only that now it controls the influence the value function has on the optimization of the policy and (1-λ) controls the distance to the behavior actions. Interestingly however, the trained policy is unable to alter its behavior anywhere in the λ-range. Instead it seems that the policy training is rather unstable and the policy collapses to a solution that behaves always the same, regardless of λ. Similarly, the distance to the original policy does not change with varying λ. We chose TD3+BC due to its simplicity - it exhibits fewer trained models and implementational pitfalls than others. The original Q-function and policy optimization objectives in TD3+BC are: Es,a,s′∼D\n\nQ = arg min\n\nQ\n\n(cid:2)[Q(s, a) − (r + γQt(s′, π(s))]2)(cid:3) ,\n\nFigure 9: Model-free experiment on the 100% exploration dataset.\n\nwhere Qt denotes the slow moving target Q-function used for computing the Q-targets. And:\n\nπ = arg max\n\nπ\n\nEs,a∼D\n\n(cid:34)\n\nα\n\n1 N\n\n(cid:80)\n\ns |Q(s, π(s))|\n\nQ(s, π(s)) − (π(s) − a)2\n\n(cid:35)\n\nIn our extension, denoted by λ-TD3+BC, we provide the sampled λ to the policy and use it to balance the two optimization terms, similarly to our LION approach. The new objectives are:\n\nQ = arg min\n\nQ\n\nEs,a,s′∼D\n\n(cid:104)\n\n[Q(s, a, λ ) − (r + γQt(s′, π(s, λ ), λ )]2)\n\n(cid:105)\n\n,\n\n15\n\n02468100246810Policymap - =0.00246810Policymap - =0.050246810Policymap - =0.10246810Policymap - =0.150246810Policymap - =0.20246810Policymap - =0.250246810Policymap - =0.302468100246810Policymap - =0.350246810Policymap - =0.40246810Policymap - =0.450246810Policymap - =0.50246810Policymap - =0.550246810Policymap - =0.60246810Policymap - =0.6502468100246810Policymap - =0.70246810Policymap - =0.750246810Policymap - =0.80246810Policymap - =0.850246810Policymap - =0.90246810Policymap - =0.950246810Policymap - =1.000.51.01.52.0angle to (1,0)00.51.01.52.0angle to (1,0)00.51.01.52.0angle to (1,0)666462IB global 1.0LION (ours)-TD3+BC0.000.250.500.751.00- Optimization Mixture Parameter0.20.4Published as a conference paper at ICLR 2023\n\nand:\n\nπ = arg max\n\nπ\n\nEs,a∼D\n\nλ\n\n(cid:34)\n\nα\n\n1 N\n\n(cid:80)\n\ns,a |Q(s, π(s, λ ), λ )|\n\nQ(s, π(s, λ ), λ ) − (1 − λ) (π(s, λ ) − a)2\n\n(cid:35)\n\nWe show complete experiments for all of the IB datasets in Figs. 10 & 9. As already mentioned in Section C, we are unable to observe that the policy is able to alter its behavior anywhere in the λ-range. We hypothesize that methods for stabilizing the Q-function are needed in order for the policy to not collapse.\n\nFigure 10: Model-free experiments incorporating the LION approach. The trained policies are unable to alter their behavior for different inputs of λ.\n\nD SAMPLING λ\n\nAs mentioned, we do not sample λ uniformly since doing so leads to less accurate learning at the edges of the λ-spectrum. In Fig 11, we show visualizations of different symmetric Beta distributions from which we draw λ together with the policy results on the bad-0.2 dataset. The Beta(1, 1) case corresponds to the uniform distribution and has a significant discrepancy at the λ = 0 end of the range: The original policy (its performance) is not accurately reproduced. Generally, it seems the flatter the distribution becomes, the more are the two extreme cases moved together. We observe this phenomenon even though the policy has plenty of capacity and even in the simple 2D environment.\n\nFigure 11: Beta distributions and resulting performances on bad-0.2. Edge cases become more accurate with increased probability mass.\n\n16\n\n300200100returnIB bad 0.00.000.250.500.751.00- Optimization Mixture Parameter02dist. to org.200100IB bad 0.20.000.250.500.751.00- Optimization Mixture Parameter01300200100IB bad 0.40.000.250.500.751.00- Optimization Mixture Parameter02300200100IB bad 0.60.000.250.500.751.00- Optimization Mixture Parameter02300200100IB bad 0.8LION (ours)-TD3+BC0.000.250.500.751.00- Optimization Mixture Parameter0215012510075returnIB mediocre 0.00.000.250.500.751.00- Optimization Mixture Parameter12dist. to org.150100IB mediocre 0.20.000.250.500.751.00- Optimization Mixture Parameter1215012510075IB mediocre 0.40.000.250.500.751.00- Optimization Mixture Parameter12300200100IB mediocre 0.60.000.250.500.751.00- Optimization Mixture Parameter0290807060IB mediocre 0.80.000.250.500.751.00- Optimization Mixture Parameter02150100returnIB optimized 0.00.00.20.40.60.81.0- Optimization Mixture Parameter02dist. to org.8060IB optimized 0.20.00.20.40.60.81.0- Optimization Mixture Parameter02626058IB optimized 0.40.00.20.40.60.81.0- Optimization Mixture Parameter0.250.500.7564626058IB optimized 0.60.00.20.40.60.81.0- Optimization Mixture Parameter0.00.564626058IB optimized 0.80.00.20.40.60.81.0- Optimization Mixture Parameter0.00.50.00.20.40.60.81.0xp(x)Beta PDFs0.00.20.40.60.81.0275250225200175150125100Evaluation ReturnLION performancesbehavior performanceBeta (0.05, 0.05)Beta (0.1, 0.1)Beta (0.5, 0.5)Beta (1.0, 1.0) [=U(0, 1)]Published as a conference paper at ICLR 2023\n\nE EXPERIMENTAL DETAILS\n\nWe conducted experiments on a system with a Xeon Gold 5122 CPU (4 × 3.6 GHz, no GPU support used). A single policy training took about 45 minutes, a single dynamics model about 2 hours. We train dynamics models using a 90/10 random data split and select the best models according to their validation performance. The final ensemble contains four models. The recurrent models for the industrial benchmark have an RNN cell with size 30 and an output layer mapping from the cell state to the state space. We use G = 30 history steps to build up the hidden state of the RNN and then predict F = 50 steps into the future. The feedforward models of the 2D env have two layers of size 20 & 10. We use adam (Kingma & Ba, 2014) with standard parameters and an exponentially decaying learning rate with factor 0.99 and train for up to 3000 epochs. The models of the original policy have a single hidden layer of size 30 and the final policies have two layers of size 1024. We use ReLU nonlinearities throughout all experiments. During policy and original policy model training we use the entire dataset instead of just the 90% split from the dynamics training. We use a discount factor of γ = 0.97 and perform rollouts of length H = 100 to train and evaluate the policy.\n\nFigure 12: LION results as presented in Fig. 4 for the 100% exploration dataset.\n\nF HYPERPARAMETERS OF THE TRANSITION ENSEMBLE\n\nInstead of using the minimum over the predicted rewards during rollouts, one might as well use other forms of aggregation, like the mean. In Fig. 13 we show results for such mean reward experiments - the future state was then randomly selected among the predictions of the transition ensemble. The resulting performance curves look similar as those in 4, except that in the mediocre and the optimized setting, the performance decrease on the λ = 1 end is even steeper than with the minimum reward prediction. This makes intuitively sense, since the mean is much easier exploited than the minimum.\n\nFigure 13: Results as in Fig. 4 for the mean reward rollout experiments.\n\nSimilar effects can be observed when the number of ensemble members is reduced. In Fig. 14, we show results on the same datasets when only a single model instead of an ensemble with four members is used (thus mean and min become the same). Here, the performance drops even earlier as well as stronger in the mediocre and optimized settings.\n\nFigure 14: Results as in Fig. 4 for the single model rollout experiments.\n\n17\n\n67.565.062.560.0IB global 1.00.000.250.500.751.00- Optimization Mixture Parameter0.20.415012510075returnIB bad 0.60.00.20.40.60.81.0 - Optimization Mixture Parameter0.250.50dist. to org.100200300IB mediocre 0.20.00.20.40.60.81.0 - Optimization Mixture Parameter0265.062.560.057.5IB optimized 0.40.00.20.40.60.81.0 - Optimization Mixture Parameter0.51.0LION (mean)MOOSEWSBCBRAC-vBEARBCQTD3+BCCQLMOPO15012510075returnIB bad 0.60.00.20.40.60.81.0 - Optimization Mixture Parameter0.250.50dist. to org.100200300IB mediocre 0.20.00.20.40.60.81.0 - Optimization Mixture Parameter0265.062.560.057.5IB optimized 0.40.00.20.40.60.81.0 - Optimization Mixture Parameter0.10.20.3LION (single)MOOSEWSBCBRAC-vBEARBCQTD3+BCCQLMOPOPublished as a conference paper at ICLR 2023\n\nG PRELIMINARIES ON OFFLINE RL\n\nIn offline reinforcement learning, we are trying to optimize the sum of discounted rewards, called return, R = (cid:80) t γtrt obtained by acting in the MDP M =< S, A, R, T, γ, s0 >, where S is the set of states, s0 ⊆ S the set of starting states, A the set of actions, R : S × A → R the reward function, T : S × A → S the transition dynamics function, and γ ∈ [0, 1) the discount factor. We train a policy πθ : S → A to obtain an action a when in state s, in order to produce trajectories of length H: τ = (s0, a0, r0, s1, a1, r1, . . . , sH , aH , rH ). While online RL methods are able to continuously explore their environment, collect new data and test new behaviors, offline RL methods need to optimize the return R by learning only from a fixed dataset of interactions D = {τi}, i = 1, ..., K. Often, this is done by solving the approximate MDP ˆM =< S, A, R, TD, γ, s0 >, where TD is directly estimated with a transition model using the transitions from D. However, also model-free offline approaches exist, that do not estimate TD and directly try to optimize a policy with a value function that is only trained on D. Since neither model-free nor model-based variants can be sure to accurately assess the policy’s performance in unknown parts of the state-action space, offline RL algorithms regularize their policy to stay in a way ’close’ to the policy that generated the data. The crucial question - how close should πθ be to the original policy? - is an open problem. We address the dilemma that the user has no meaningful way of choosing the correct proximity since offline policy selection / evaluation remain generally unsolved problems.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper takes a user-centered approach to the problem of the behavior of off-line reinforcement learning models when put into use. It takes an idea that is simple in concept and practical to apply that addresses some of the risks of using RL models that have been trained off-line. The idea is that users be given a choice of how aggressive or conservative a policy they follow by parameterizing the choice as a single parameter learnt at training time.  Learning this parameter has a clear computation solution, and the method can be demonstrated to generate a smooth transition from a given behavioral policy to a more aggressive one.\n\n# Strength And Weaknesses\n\nGiven that the use of offline reinforcement learning, as is, remains problematic, this paper offers a both sound and practical way to put it to use. It's refreshing to see an idea with apparent merit demonstrated to the point where it can be implemented, And bringing the user back, to propose a user centric approach that touches on the \"representation\" theme of the conference is notable. \n\nI wish the authors had mentioned the computational effort needed for training.  Another thought is to be more specific about how choices are presented to the user -- do they chose only \\lambda, or do they also get to see the policy that their choice implies?  One might imagine that exposing the risk / reward tradeoff implicit in the choice, in terms of the reward distribution would be  useful. None of these wishes are flaws, but rather extensions should the authors wish to address them.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is one of the most approachable and significant papers I've reviewed.\n\n# Summary Of The Review\n\nBy considering a user-centric approach, this paper addresses an outstanding dilemma about implementing off-line RL policies in a way that is easily representable to the user and builds on current offline methods.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nADAN: ADAPTIVE NESTEROV MOMENTUM ALGORITHM FOR FASTER OPTIMIZING DEEP MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAdaptive gradient algorithms combine the moving average idea with heavy ball acceleration to estimate accurate first- and second-order moments of the gradient for accelerating convergence. However, Nesterov acceleration which converges faster than heavy ball acceleration in theory and also in many empirical cases, is much less investigated under the adaptive gradient setting. In this work, we propose the ADAptive Nesterov momentum algorithm, Adan for short, to speed up the training of deep neural networks effectively. Adan first reformulates the vanilla Nesterov acceleration to develop a new Nesterov momentum estimation (NME) method, which avoids the extra computation and memory overhead of computing gradient at the extrapolation point. Then Adan adopts NME to estimate the first- and second-order moments of the gradient in adaptive gradient algorithms for convergence acceleration. Besides, we prove that Adan finds an ε-approximate first-order stationary point within O(cid:0)ε−3.5(cid:1) stochastic gradient complexity on the non-convex stochastic problems (e.g. deep learning problems), matching the bestknown lower bound. Extensive experimental results show that Adan surpasses the corresponding SoTA optimizers on vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, e.g. ResNet, ConvNext, ViT, Swin, MAE, LSTM, Transformer-XL, and BERT. More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, ResNet, MAE, etc, and also shows great tolerance to a large range of minibatch size, e.g. from 1k to 32k. We hope Adan can contribute to developing deep learning by reducing training costs and relieving the engineering burden of trying different optimizers on various architectures.\n\n1\n\nINTRODUCTION\n\nDeep neural networks (DNNs) have made remarkable success in many fields, e.g. computer vision (Szegedy et al., 2015; He et al., 2016) and natural language processing (Sainath et al., 2013; Abdel-Hamid et al., 2014). A noticeable part of such success is contributed by the stochastic gradient-based optimizers, which find satisfactory solutions with high efficiency. Among current deep optimizers, SGD (Robbins & Monro, 1951) is the earliest and also the most representative stochastic optimizer, with dominant popularity for its simplicity and effectiveness. It adopts a single common learning rate for all gradient coordinates but often suffers unsatisfactory convergence speed on sparse data or ill-conditioned problems. In recent years, adaptive gradient algorithms, e.g. Adam (Kingma & Ba, 2014) and AdamW (Loshchilov & Hutter, 2018), have been proposed, which adjust the learning rate for each gradient coordinate according to the current geometry curvature of the loss objective. These adaptive algorithms, e.g. Adam, often offer a faster convergence speed than SGD in practice.\n\nHowever, none of the above optimizers can always stay undefeated among all its competitors across different network architectures and application settings. For instance, for vanilla ResNet (He et al., 2016), SGD often achieves better generalization performance than adaptive gradient algorithms such as Adam, whereas on vision transformers (ViTs) (Touvron et al., 2021), SGD often fails, and AdamW is the dominant optimizer with higher and more stable performance. Moreover, these commonly used optimizers usually fail for large-batch training, which is a default setting of the prevalent distributed training. Although there is some performance degradation, we still tend to choose the large-batch setting for large-scale deep learning training tasks due to the unaffordable training time. For example, training the ViT-B with the batch size of 512 usually takes several days, but when the batch size comes to 32K, we may finish the training within three hours (Liu et al., 2022a). Although some\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison of different adaptive gradient algorithms on nonconvex stochastic problems. “Separated Reg.” refers to whether the l2 regularizer (weight decay) can be separated from the loss objective like AdamW. “Complexity\" denotes stochastic gradient complexity to find an εapproximate first-order stationary point. Adam-type methods (Guo et al., 2021) includes Adam, and AdaGrad (Duchi et al., 2011), etc. AdamW has no available convergence result. For SAM (Foret et al., 2020), A-NIGT (Cutkosky & Mehta, 2020) and Adam+ (Liu et al., 2020), we compare their adaptive versions. d is the variable dimension. The lower bound is proven in (Arjevani et al., 2020).\n\nSmooth Condition\n\nLipschitz\n\nGradient\n\nLipschitz\n\nHessian\n\nOptimizer\n\nAdam-type RMSProp AdamW Adabelief Padam LAMB Adan (ours)\n\nA-NIGT Adam+ Adan (ours)\n\nSeparated Reg.\n\nBatch Size Condition\n\nGrad Bound Complexity\n\n(cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34)\n\n(cid:37) (cid:37) (cid:34)\n\n(cid:37) (cid:37)\n\n— (cid:37) (cid:37) O(cid:0)ε−4(cid:1) (cid:37)\n\n(cid:37) O(cid:0)ε−1.625(cid:1) (cid:37)\n\nl∞ ≤ c∞ l∞ ≤ c∞ —\nl2 ≤ c2 l∞ ≤ c∞ l2 ≤ c2 l∞ ≤ c∞\n\nl2 ≤ c2 l2 ≤ c2 l∞ ≤ c∞\n\n∞dε−4(cid:1) c∞dε−4(cid:1)\n\nO(cid:0)c2 O(cid:0)√ —\n2ε−4(cid:1) O(cid:0)c6 O(cid:0)√ c∞dε−4(cid:1) O(cid:0)c2 2dε−4(cid:1) O(cid:0)c2.5 ∞ ε−4(cid:1) O(cid:0)ε−3.5 log c2 O(cid:0)ε−3.625(cid:1) O(cid:0)c1.25\n\n∞ ε−3.5(cid:1)\n\nε\n\nLower Bound\n\nΩ(cid:0)ε−4(cid:1) Ω(cid:0)ε−4(cid:1) —\nΩ(cid:0)ε−4(cid:1) Ω(cid:0)ε−4(cid:1) Ω(cid:0)ε−4(cid:1) Ω(cid:0)ε−4(cid:1) (cid:1) Ω(cid:0)ε−3.5(cid:1) Ω(cid:0)ε−3.5(cid:1) Ω(cid:0)ε−3.5(cid:1)\n\nmethods, e.g. LARS (You et al., 2017) and LAMB (You et al., 2019), have been proposed to handle large batch sizes, their performance often varies significantly across batch sizes. This performance inconsistency increases the training cost and engineering burden, since one usually has to try various optimizers for different architectures or training settings.\n\nWhen we rethink the current adaptive gradient algorithms, we find that they mainly combine the moving average idea with the heavy ball acceleration technique to estimate the first- and second-order moments of the gradient, e.g. Adam, AdamW and LAMB. However, previous studies (Nesterov, 1983; 1988; 2003) have revealed that Nesterov acceleration can theoretically achieve a faster convergence speed than heavy ball acceleration, as it uses gradient at an extrapolation point of the current solution and sees a slight “future\". Moreover, recent work (Nado et al., 2021; He et al., 2021) have shown the potential of Nesterov acceleration for large-batch training. Thus we are inspired to consider efficiently integrating Nesterov acceleration with adaptive algorithms.\n\nContributions: 1) We propose an efficient DNN optimizer, named Adan. Adan develops a Nesterov momentum estimation method to estimate stable and accurate first- and second-order moments of the gradient in adaptive gradient algorithms for acceleration. 2) Moreover, Adan enjoys a provably faster convergence speed than previous adaptive gradient algorithms such as Adam. 3) Empirically, Adan shows superior performance over the SoTA deep optimizers across vision, language, and reinforcement learning (RL) tasks. Our detailed contributions are highlighted below.\n\nk) at the extrapolation point θ′\n\nFirstly, we propose an efficient Nesterov-acceleration-induced deep learning optimizer termed Adan. Given a function f and the current solution θk, Nesterov acceleration (Nesterov, 1983; 1988; 2003) estimates the gradient gk = ∇f (θ′ k = θk − η(1 − β1)mk−1 with the learning rate η and momentum coefficient β1 ∈ (0, 1), and updates the moving gradient average as mk = (1 − β1)mk−1 + gk. Then it runs a step by θk+1 = θk − ηmk. However, the inconsistency of the positions for parameter updating at θk and gradient estimation at θ′ k leads to the additional cost of model parameter reloading during back-propagation (BP), which is unaffordable especially for large DNNs. To avoid the model reloading during BP, we propose an alternative Nesterov momentum estimation (NME). We compute the gradient gk = ∇f (θk) at the current solution θk, and estimate the moving gradient average as mk = (1 − β1)mk−1 + g′ k, where g′ k = gk + (1 − β1)(gk − gk−1). Our NME is provably equivalent to the vanilla one yet can Then by regarding g′ avoid the extra model reloading. k as the current stochastic gradient in adaptive gradient algorithms, e.g. Adam, we accordingly estimate the first- and second-moments as k)2 respectively. Finally, we update mk = (1 − β1)mk−1 + β1g′ θk+1 = θk − ηmk/ nk + ε. In this way, Adan enjoys the merit of Nesterov acceleration, namely faster convergence speed and tolerance to large mini-batch size (Lin et al., 2020), which is verified in our experiments in Sec. 5.\n\nk and nk = (1 − β2)nk−1 + β2(g′\n\n√\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nSecondly, as shown in Table 1, we theoretically justify the advantages of Adan over previous SoTA adaptive gradient algorithms on nonconvex stochastic problems, e.g. deep learning problems.\n\n√\n\n1) Given Lipschitz gradient condition, to find an ε-approximate first-order stationary point, Adan has the stochastic gradient complexity O(cid:0)c2.5 ∞ ε−4(cid:1) which accords with the lower bound Ω(ε−4) 2ε−4(cid:1) of This complexity is lower than O(cid:0)c6 (up to a constant factor) (Arjevani et al., 2019). Adabelief (Zhuang et al., 2020) and O(cid:0)c2 2dε−4(cid:1) of LAMB, especially on over-parameterized networks. Specifically, for the d-dimensional gradient, compared with its l2 norm c2, its l∞ norm c∞ is usually d× smaller for the best case. Moreover, different from Adam-type much smaller, and can be optimizers (e.g. Adam), Adan can separate the l2 regularizer with the loss objective like AdamW whose generalization benefits have been validated in many works (Touvron et al., 2021). 2) Given the Lipschitz Hessian condition, Adan has a complexity O(cid:0)c1.25 ∞ ε−3.5(cid:1) which also matches (cid:1) the lower bound Ω(ε−3.5) in Arjevani et al. (2020). This complexity is superior to O(cid:0)ε−3.5 log c2 of A-NIGT (Cutkosky & Mehta, 2020) and also O(cid:0)ε−3.625(cid:1) of Adam+ (Liu et al., 2020). Indeed, Adam+ needs the minibatch size of order O(cid:0)ε−1.625(cid:1) which is prohibitive in practice. For other optimizers, e.g. Adam, their convergence has not been provided yet under Lipschitz Hessian condition.\n\nε\n\nFinally, Adan simultaneously surpasses the corresponding SoTA optimizers across vision, language, and RL tasks, and establishes new SoTAs for many networks and settings, e.g. ResNet, ConvNext (Liu et al., 2022b), ViT (Touvron et al., 2021), Swin (Liu et al., 2021), MAE (He et al., 2022), LSTM (Schmidhuber et al., 1997), Transformer-XL (Dai et al., 2019) and BERT (Devlin et al., 2018). More importantly, with half of the training cost (epochs) of SoTA optimizers, Adan can achieve higher or comparable performance. Besides, Adan works well in a large range of minibatch size, e.g. from 1k to 32k on ViTs. The improvement of Adan for various architectures and settings can greatly relieve the engineering burden by avoiding trying different optimizers.\n\n2 RELATED WORK Current DNN optimizers can be grouped into two families: SGD and its accelerated variants, and adaptive gradient algorithms. SGD computes stochastic gradient and updates the variable along the gradient direction. Later, heavy-ball acceleration (Polyak, 1964) movingly averages stochastic gradient in SGD for faster convergence. Nesterov acceleration runs a step along the moving gradient average and then computes gradient at the new point to look ahead for correction. Typically, Nesterov acceleration converges faster both empirically and theoretically at least on convex problems, and also has superior generalization resutls on DNNs (Foret et al., 2020; Kwon et al., 2021).\n\nUnlike SGD, adaptive gradient algorithms, e.g. AdaGrad, RMSProp and Adam, view the second moment of gradient as a preconditioner and also use moving gradient average to update the variable. Later, many variants have been proposed to estimate a more accurate and stable first moment of gradient or its second moment, e.g. AMSGrad, Adabound, and Adabelief. To improve generalization, AdamW splits the objective and trivial regularization, and its effectiveness is validated across many applications; SAM and its variants (Kwon et al., 2021) aim to find flat minima but need forward and backward twice per iteration. LARS and LAMB train DNNs with a large batch but suffer unsatisfactory performance on small batch. Xie et al. (2022) reveal the generalization and convergence gap between Adam and SGD from the perspective of diffusion theory and propose the optimizers, Adai, which accelerates the training and provably favors flat minima. Padam (Chen et al., 2021a) provides a simple but effective way to improve the generalization performance of Adam by adjusting the second-order moment in Adam. The most related work to ours is NAdam. It simplifies Nesterov acceleration to estimate the first moment of gradient in Adam. But its acceleration does not use any gradient from the extrapolation points and thus does not look ahead for correction. Moreover, there is no theoretical result to ensure its convergence. See more difference discussion in Sec. 3.2.\n\n3 METHODOLOGY In this work, we study the following regularized nonconvex optimization problem:\n\nminθ F (θ) := Eζ∼D [f (θ, ζ)] +\n\nλ 2\n\n∥θ∥2 2,\n\n(1)\n\nwhere loss f (·, ·) is differentiable and possibly nonconvex, data ζ is drawn from an unknown distribution D, θ is learnable parameters, and ∥·∥ is the classical l2 norm. At below, we first introduce the key motivation of Adan in Sec. 3.1, and then give detailed algorithmic steps in Sec. 3.2.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3.1 PRELIMINARIES\n\nAdaptive gradient algorithms, Adam and AdamW, have become the default choice to train CNNs and ViTs. Unlike SGD which uses one learning rate for all gradient coordinates, adaptive algorithms adjust the learning rate for each gradient coordinate according to the current geometry curvature of the objective function, and thus converge faster. Take RMSProp and Adam as examples. Given stochastic gradient estimator gk := Eζ∼D[∇f (θk, ζ)] + ξk, e.g. minibatch gradient, where ξk is the gradient noise, RMSProp updates the variable θ as follows:\n\nRMSProp:\n\n(cid:40)\n\nnk = (1 − β)nk−1 + βg2 θk+1 = θk − η/(\n\nk nk + ε) ◦ gk,\n\n√\n\n⇒ Adam:\n\n \n\n\n\nmk = (1 − β1)mk−1 + β1gk nk = (1 − β2)nk−1 + β2g2 θk+1 = θk − η/(\n\nk nk + ε) ◦ mk,\n\n√\n\nwhere m0 = g0, n0 = g2 0, the scalar η is the base learning rate, and ◦ denotes the element-wise product. Based on RMSProp, Adam1 replaces the estimated gradient gk with a moving average mk of all previous gradient gk. By inspection, one can easily observe that the moving average idea in Adam is similar to the classical (stochastic) heavy-ball acceleration (HBA) technique (Polyak, 1964):\n\nHBA: gk = ∇f (θk) + ξk,\n\nmk = (1 − β1)mk−1 + gk,\n\nθk+1 = θk − ηmk.\n\nBoth Adam and HBA share the spirit of moving gradient average, though HBA does not have the factor β1 on the gradient gk. That is, given one gradient coordinate, if its gradient directions are more consistent along the optimization trajectory, Adam/HBA accumulates a larger gradient value in this direction and thus goes ahead for a bigger gradient step, which accelerates convergence.\n\nIn addition to HBA, Nesterov’s accelerated (stochastic) gradient descent (AGD) (Nesterov, 1983; 1988; 2003) is another popular acceleration technique in the optimization community:\n\nAGD: gk = ∇f (θk −η(1 − β1)mk−1)+ξk, mk = (1 − β1)mk−1+gk, θk+1 = θk −ηmk. (2)\n\nUnlike HBA, AGD uses the gradient at the extrapolation point θ′ k = θk − η(1 − β1)mk−1. Hence when the adjacent iterates share consistent gradient directions, AGD sees a slight future to converge faster. Indeed, AGD theoretically converges faster than HBA and achieves optimal convergence rate on the general smooth convex problems (Nesterov, 2003). Meanwhile, since the over-parameterized DNNs have been observed/proved to have many convex-alike local basins (Hardt & Ma, 2016; Xie et al., 2017; Li & Yuan, 2017), AGD seems more suitable than HBA for DNNs. For large-batch training, Nado et al. (2021) showed that AGD has the potential to achieve comparable performance to some specifically designed optimizers, e.g. LARS and LAMB. With its advantage in convergence and large-batch training, we consider applying AGD to improve adaptive algorithms.\n\n3.2 ADAPTIVE NESTEROV MOMENTUM ALGORITHM\n\nk and preserving both θk and θ′\n\nMain Iteration. We temporarily set λ = 0 in Eqn. (1). As aforementioned, AGD computes gradient at an extrapolation point θ′ k instead of the current iterate θk, which however brings extra computation and memory overhead for computing θ′ k. To solve the issue, Lemma 1 with proof in Appendix D reformulates AGD (2) into its equivalent but more DNN-efficient version. Lemma 1. Assume E(ξk) = 0, Cov(ξi, ξj) = 0 for any k, i, j > 0, ̄θk and ̄mk be the iterate and momentum of the vanilla AGD in Eqn. (2), respectively. Let θk+1 := ̄θk+1 − η(1 − β1) ̄mk and mk := (1 − β1)2 ̄mk−1 + (2 − β1)(∇f (θk) + ξk). The vanilla AGD in Eqn. (2) becomes AGD-II: gk = Eζ∼D[∇f (θk, ζ)]+ξk, mk = (1−β1)mk−1 +[gk + (1−β1)(gk − gk−1)], θk+1 = θk −ηmk. Moreover, if vanilla AGD in Eqn. (2) converges, so does AGD-II, and E(θ∞) = E( ̄θ∞).\n\nThe main idea in Lemma 1 is that we maintain (θk − η(1 − β1)mk−1) rather than θk in vanilla AGD at each iteration, since there is no difference between them when the algorithm converges. Like other adaptive optimizers, by regarding g′ k = gk + (1 − β1)(gk − gk−1) as the current stochastic gradient and movingly averaging g′\n\nVanilla Adan:\n\nk to estimate the first- and second-moments of gradient, we obtain \n\n\nmk = (1 − β1)mk−1 + β1[gk + (1 − β1)(gk − gk−1)] nk = (1 − β3)nk−1 + β3[gk + (1 − β1)(gk − gk−1)2] θk+1 = θk − ηk ◦ mk with ηk = η/(\n\nnk + ε).\n\n\n\n√\n\n1For presentation convenience, we omit the de-bias term in adaptive gradient methods.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nThe main difference of Adan with Adam-type methods and Nadam (Dozat, 2016) is that as compared in Eqn. (3), the momentum mk of Adan is the average of {gt + (1 − β1)(gt − gt−1)}k t=1 while those of Adam-type and Nadam are the average of {gt}k\n\nt=1. So is their second-order term nk.\n\nmk =\n\n \n\n(cid:80)k (cid:80)k\n\n\n\nμk+1 μ′\n\nk+1\n\nt=0 ck,t[gt + (1 − β1)(gt − gt−1)], Adan, Adam, t=0 ck,tgt, (cid:0) (cid:80)k Nadam,\n\n(cid:1) + 1−μk\n\nt=0 ck,tgt\n\ngk,\n\nμ′ k\n\nck,t =\n\n \n\nβ1(1 − β1)(k−t)\n\nt > 0,\n\n\n\n(1 − β1)k\n\nt = 0,\n\n(3)\n\nt=1 is a predefined exponentially decaying sequence, μ′\n\nwhere {μt}∞ t=1 μt. So Nadam is more like Adam than Adan, as their mk movingly averages the historical gradients instead of gradient differences in Adan. For a large k (i.e. small μk), mk in Nadam and Adam are almost the same.\n\nk = 1 − (cid:81)k\n\nAs shown in Eqn. (3), the moment mk in Adan consists of two terms, i.e. gradient term gt and gradient difference term (gt − gt−1), which actually have different physic meanings. So here we decouple them for greater flexibility and also better trade-off between them. Specifically, we estimate\n\n(θk+1 − θk)/ηk =\n\n(cid:88)k\n\nt=0\n\n(cid:2)ck,tgt + (1 − β2)c′\n\nk,t(gt − gt−1)(cid:3) = mk + (1 − β2)vk,\n\n(4)\n\nwhere c′\n\nk,t = β2(1 − β2)(k−t) for t > 0, c′ mk = (1 − β1)mk−1 + β1gk,\n\nk,t = (1 − β2)k for t = 0, and mk and vk are defined as\n\nvk = (1 − β2)vk−1 + β2(gk − gk−1).\n\nThis change for a flexible estimation does not impair convergence speed. As we show in Theorem 1, the complexity of Adan under this change matches the lower complexity bound. We do not separate the gradients and their difference in the second-order moment nk, since E(nk) contains the correlation term Cov(gk, gk−1) ̸= 0 which may have statistical significance.\n\nDecay Weight by Proximation. As observed in AdamW, decoupling the optimization objective and simple-type regularization (e.g. l2 regularizer) can largely improve the generalization performance. Here we follow this idea but from a rigorous optimization perspective. Intuitively, at each iteration, we minimize the first-order approximation of F (·) at the point θk:\n\nθk+1 = θk − ηk ◦ ̄mk = argminθ F (θk) + ⟨ ̄mk, θ − θk⟩ +\n\n1 2η\n\n∥θ − θk∥2√\n\n,\n\nnk\n\n:= (cid:10)x, (cid:0)√\n\nnk + ε(cid:1) ◦ x(cid:11) and ̄mk := mk + (1 − β2)vk is the first-order derivative of where ∥x∥2√ F (·) in some sense. Follow the idea of proximal gradient descent (Parikh & Boyd, 2014; Zhuang et al., 2022), we decouple the l2 regularizer from F (·) and only linearize the loss function f (·):\n\nnk\n\nθk+1 = argmin\n\nθ\n\n(cid:16) λk 2\n\n∥θ∥2√\n\nnk\n\n+ ⟨ ̄mk, θ − θk⟩ +\n\n1 2η\n\n∥θ − θk∥2√\n\nnk\n\n(cid:17)\n\n=\n\nθk − ηk ◦ ̄mk 1 + λkη\n\n,\n\n(5)\n\nwhere λk > 0 is the weight decay at the k-th iteration. One can find that the optimization objective of at the k-th iteration is changed from the vanilla “static\" function F (·) in Eqn. (1) to a “dynamic\" function Fk(·) in Eqn. (6), which adaptively regularizes the coordinates with larger gradient more:\n\nFk(θ) := Eζ∼D [f (θ, ζ)] +\n\nλk 2\n\n∥θ∥2√\n\n.\n\nnk\n\n(6)\n\nWe summarize our Adan in Algorithm 1. We reset the momentum term properly by the restart condition, a common trick to stabilize optimization and benefit convergence (Li & Lin, 2022; Jin et al., 2018). But to make Adan simple, in all experiments except Table 8, we do not use this restart strategy although it can improve performance as shown in Table 8.\n\n4 CONVERGENCE ANALYSIS\n\nFor analysis, we make several mild assumptions used in many works, e.g. (Guo et al., 2021). Assumption 1 (L-smoothness). The function f (·, ·) is L-smooth w.r.t. the parameter, if ∃L > 0,\n\n∥∇ Eζ[f (x, ζ)] − ∇ Eζ[f (y, ζ)]∥ ≤ L∥x − y∥,\n\n∀x, y.\n\nAssumption 2 (Unbiased and bounded gradient oracle). The stochastic gradient oracle gk = Eζ[∇f (θk, ζ)] + ξk is unbiased, and its magnitude and variance are bounded with probability 1:\n\nE (ξk) = 0,\n\n∥gk∥∞ ≤ c∞/3, E\n\n(cid:16)\n\n∥ξk∥2(cid:17)\n\n(cid:16)\n\n= E\n\n∥∇ Eζ[f (θk, ζ)] − gk∥2(cid:17)\n\n≤ σ2,\n\n∀k ∈ [T ].\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Adan (Adaptive Nesterov Momentum Algorithm) Input: initialization θ0, step size η, weight decay λk > 0, restart condition. Output: some average of {θk}K\n\nk=1.\n\n1 while k < K do\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\ncompute the stochastic gradient estimator gk at θk; mk = (1 − β1)mk−1 + β1gk vk = (1 − β2)vk−1 + β2(gk − gk−1) nk = (1 − β3)nk−1 + β3[gk + (1 − β2)(gk − gk−1)]2; θk+1 = (1 + λkη)−1[θk − ηk ◦ (mk + (1 − β2)vk)] with ηk = η/(cid:0)√\n\n/* set m0 = g0 */;\n\n/* set v1 = g1 − g0 */;\n\nnk + ε(cid:1);\n\nif restart condition holds then\n\nget stochastic gradient estimator g0 at θk+1; m0 = g0, v0 = 0, n0 = g2 0,\n\nupdate θ1 by Line 6,\n\nk = 1;\n\nend if 10 11 end while\n\nAssumption 3 (ρ-Lipschitz continuous Hessian). The function f (·, ·) has ρ-continuous Hessian:\n\n(cid:13) ≤ ρ∥x − y∥, where ∥·∥ is the spectral norm for matrix and the l2 norm for vector.\n\n(cid:13)∇2 Eζ[f (x, ζ)] − ∇2 Eζ[f (x, ζ)](cid:13) (cid:13)\n\n∀x, y,\n\nFor a general nonconvex problem, if Assumptions 1 and 2 hold, the lower bound of the stochastic gradient complexity to find an ε-approximate first-order stationary point (ε-ASP) is Ω(ε−4) (Arjevani et al., 2019; 2020). Moreover, if Assumption 3 further holds, the lower complexity bound becomes Ω(ε−3.5) for a non-variance-reduction algorithm (Arjevani et al., 2019; 2020).\n\nLipschitz Gradient. Theorem 1 with proof in Appendix E proves the convergence of Adan on problem (6) with lipschitz gradient condition. Theorem 1. Suppose Assumptions 1 and 2 hold. Let max {β1, β2} = O(cid:0)ε2(cid:1), μ := η = O(cid:0)ε2(cid:1), and λk = λ(1 − μ)k. Algorithm 1 runs at most K = Ω(cid:0)c2.5\n\n2β3c∞/ε ≪ 1, ∞ ε−4(cid:1) iterations to achieve\n\n√\n\nThat is, to find an ε-ASP, the stochastic gradient complexity of Adan on problem (6) is O(cid:0)c2.5\n\n∞ ε−4(cid:1).\n\n1 K + 1\n\n(cid:88)K\n\nk=0\n\n(cid:16)\n\nE\n\n∥∇Fk(θk)∥2(cid:17)\n\n≤ 4ε2.\n\nTheorem 1 shows that under Assumptions 1 and 2, Adan can converge to an ε-ASP of a nonconvex ∞ ε−4(cid:1) which accords with the lower stochastic problem with stochastic gradient complexity O(cid:0)c2.5 bound Ω(ε−4) in Arjevani et al. (2019). For this convergence, Adan has no requirement on minibatch size and only assumes gradient estimation to be unbiased and bounded. Moreover, as shown in Table 1 in Sec. 1, the complexity of Adan is superior to those of previous adaptive gradient algorithms. For Adabelief and LAMB, Adan always has lower complexity and respectively enjoys d3× and d2× lower complexity for the worst case. Adam-type optimizers (e.g. Adam and AMSGrad) enjoy the same complexity as Adan. But they cannot separate the l2 regularizer with the objective like AdamW and our Adan. The regularizer separation can boost generalization performance (Touvron et al., 2021; Liu et al., 2021) and already helps AdamW dominate training of ViT-alike architectures. Besides, some previous analyses (Luo et al., 2018; Zaheer et al., 2018; Liu et al., 2019a; Shi et al., 2020) need the momentum coefficient (i.e. βs) to be close or increased to one, which contradicts with the practice that βs are close to zero. In contrast, Theorem 1 assumes that all βs are very small, which is more consistent with the practice. Note that when μ = c/T , we have λk/λ ∈ [(1 − c), 1] during training. Hence we could choose the λk as a fixed constant in the experiment for convenience.\n\nLipschitz Hessian. With Assumption 3, we further need a restart condition. Consider an extension point yk+1 := θk+1 + ηk ◦ [mk + (1 − β2)vk − βgk], and a restart condition: (cid:88)k\n\n(k + 1)\n\n∥yt+1 − yt∥2√\n\nnt\n\n> R2,\n\nt=0\n\n(7)\n\nwhere the constant R controls the restart frequency. Intuitively, when the parameters have accumulated enough updates, the iterate may reach a new local basin. Resetting the momentum at this moment nk + ε(cid:1)\n\nhelps Adan to better use the local geometric information. Besides, we change ηk from η/(cid:0)√ to η/(cid:0)√ nk−1 + ε(cid:1) to ensure ηk to be independent of noise ζk. See its proof in Appendix F.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Top-1 accuracy (%) of ResNet and ConvNext on ImageNet under their official settings. ∗ and ⋄ are respectively reported in (Wightman et al., 2021; Liu et al., 2022b).\n\nResNet-50\n\nResNet-101\n\nEpoch\n\nSAM SGD-M Adam AdamW LAMB Adan (ours)\n\n100\n\n77.3 77.0 76.9 77.0 77.0 78.1\n\n200\n\n78.7 78.6 78.4 78.9 79.2 79.7\n\n300\n\n79.4 79.3 78.8 79.3 79.8∗ 80.2\n\n100\n\n79.5 79.3 78.4 78.9 79.4 79.9\n\n200\n\n81.1 81.0 80.2 79.9 81.1 81.6\n\n300\n\n81.6 81.4 80.6 80.4 81.3∗ 81.8\n\nEpoch\n\nAdamW Adan (ours)\n\nEpoch\n\nAdamW Adan (ours)\n\n82.2 82.5\n\nConvNext Tiny 150\n\n81.2 81.7 ConvNext Small 150\n\n300 82.1⋄ 82.4\n\n300 83.1⋄ 83.3\n\nTheorem 2. Suppose Assumptions 1-3 hold. Let R = O(cid:0)ε0.5(cid:1), max {β1, β2} = O(cid:0)ε2(cid:1), β3 = O(cid:0)ε4(cid:1), η = O(cid:0)ε1.5(cid:1), K = O(cid:0)ε−2(cid:1), λ = 0. Then Algorithm 1 with restart condition Eqn.(7) satisfies:\n\nE (cid:0)(cid:13)\n\n(cid:13)∇Fk( ̄θ)(cid:13) (cid:1) = O(cid:0)c0.5 (cid:13)\n\n∞ ε(cid:1),\n\n(cid:80)K0\n\nwhere ̄θ := 1 K0 ASP, Algorithm 1 restarts at most O(cid:0)c0.5 K = O(cid:0)ε−2(cid:1) iterations, and hence needs at most O(cid:0)c1.25\n\nk=1 θk, K0 = argmin⌊ K\n\n2 ⌋≤k≤K−1 ∥yt+1 − yt∥2√\n\n. Moreover, to find an ε- ∞ ε−1.5(cid:1) times in which each restarting cycle has at most\n\n∞ ε−3.5(cid:1) stochastic gradient complexity.\n\nnt\n\nFrom Theorem 2, one can observe that with an extra smooth Hessian condition in Assumption 3 and ∞ ε−4(cid:1) a restart condition (7), Adan improves its vanilla stochastic gradient complexity from O(cid:0)c2.5 ∞ ε−3.5(cid:1), which also matches the corresponding lower bound Ω(ε−3.5). This complexity to O(cid:0)c1.25 (cid:1) of A-NIGT and O(cid:0)ε−3.625(cid:1) of Adam+. For other DNN optimizers, is lower than O(cid:0)ε−3.5 log c2 e.g. Adam, their convergence under Lipschitz Hessian condition has not been proved yet.\n\nε\n\nMoreover, Theorem 2 still holds for the large batch size. For example, by using minibatch size b = O(cid:0)ε−1.5(cid:1), our results still hold when R = O(cid:0)ε0.5(cid:1), max {β1, β2} = O(cid:0)ε0.5(cid:1), β3 = O(ε), η = O(1), K = O(cid:0)ε−0.5(cid:1) and λ = 0. In this case, our η is of the order O(1), and is much larger than O(ploy(ε)) of other optimizers (e.g., LAMB and Adam+) for handling large minibatch. This large step size often boosts convergence speed in practice, which is actually desired.\n\n5 EXPERIMENTAL RESULTS\n\nWe evaluate Adan on vision, NLP and RL tasks. For vision tasks, we test Adan on several representative SoTA backbones under the supervised settings, including 1) CNN-type architectures (ResNets and ConvNexts (Liu et al., 2022b)) and 2) ViTs vanilla ViTs and Swins (Liu et al., 2021)). Moreover, we also investigate Adan via the self-supervised pretraining by using it to train MAE ViT (He et al., 2022). For NLP tasks, we train LSTM, Transformer-XL (Dai et al., 2019), and BERT (Devlin et al., 2018) for sequence modeling. On RL tasks, we evaluate Adan on four games in MuJoCo (Todorov et al., 2012). In all experiments, we only replace the optimizer with Adan and tune the step size, warmup epochs, and weight decay, etc, while fixing the optimizer-independent hyper-parameters, e.g. data augmentation and model architectures. Moreover, to make Adan simple, in all experiments except Table 8, we do not use the restart strategy in Algorithm 1. Due to space limitation, we defer the RL results and the ablation study into Appendix B.3 and B.5, respectively.\n\n5.1 EXPERIMENTS FOR VISION TASKS\n\nBesides the vanilla supervised training setting used in ResNets (He et al., 2016), we further consider two prevalent training settings on ImageNet, namely the following Training Setting I and II.\n\nTraining Setting I. The recently proposed “A2 training recipe” in (Wightman et al., 2021) has lifted the performance limits of many SoTA CNN-type architectures by stronger data augmentation. Specifically, for data augmentation, this setting uses random crop, horizontal flipping, Mixup (0.1)/CutMix (1.0) with probability 0.5, and RandAugment with M = 7, N = 2 and MSTD = 0.5. It sets stochastic depth (0.05), and adopts cosine learning rate decay and binary cross-entropy loss.\n\nTraining Setting II. For this setting, data augmentation includes random crop, horizontal flipping, Mixup (0.8), CutMix (1.0), RandAugment (M = 9, MSTD = 0.5) and Random Erasing (p = 0.25). It uses cross-entropy loss, cosine decay, and stochastic depth. For both settings, please refer to their details, e.g. data augmentation, in Appendix Sec. A.1.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Top-1 accuracy (%) of ViT and Swin on ImageNet. We use their official Training Setting II to train them. ∗ and ⋄ are respectively reported in (Touvron et al., 2021; Liu et al., 2021).\n\nEpoch\n\nViT Small 300\n\n150\n\nAdamW Adan (ours)\n\n78.3 79.6\n\n79.9∗ 80.9\n\nViT Base\n\n150\n\n79.5 81.7\n\n300\n\n81.8∗ 82.3\n\nSwin Tiny 300\n\n150\n\nSwin small 300\n\n150\n\nSwin Base 300\n\n150\n\n79.9 81.3\n\n81.2⋄ 81.6\n\n82.1 82.9\n\n83.2⋄ 83.7\n\n82.6 83.3\n\n83.5⋄ 83.8\n\nResults on CNN-type Architectures. To train ResNet and ConvNext, we respectively use their official Training Setting I and II. For SoTA ResNet/ConvNext, its default official optimizer is LAMB/AdamW. From Table 2, one can observe that on ResNet, 1) in most cases, Adan only running 200 epochs can achieve higher or comparable top-1 accuracy on ImageNet compared with the official SoTA result trained by LAMB with 300 epochs; 2) Adan gets more improvements over other optimizers, when training is insufficient, e.g. 100 epochs. The possible reason for observation 1) is the regularizer separation, which can dynamically adjust the weight decay for each coordinate instead of sharing a common one. For observation 2), this can be explained by the faster convergence speed of Adan than other optimizers. As shown in Figure 1, Adan converges faster than many adaptive gradient optimizers. This faster speed partially comes from its large learning rate guaranteed by Theorem 2, almost 3× larger than that of LAMB. The same as Nesterov acceleration, Adan could look ahead for possible corrections. More comparison on convergence speed and loss curve is in Appendix B.2. On ConvNext, one can observe similar comparison results on ResNets. Additional results in Appendix Sec. B.1 provide more comparison on ResNet-18 under the vanilla setting in (He et al., 2016).\n\nFigure 1: Training curves of various optimizers.\n\nResults on ViTs. 1) Supervised Training. We train ViT and Swin under their official training setting, i.e. Training Setting II. Table 3 shows that across different model sizes of ViT and Swin, Adan outperforms the official AdamW optimizer by a large margin. For ViTs, their gradient per iteration differs much from the previous one due to the much sharper loss landscape than CNNs (Chen et al., 2021b) and the strong random augmentations for training. So it is hard to train ViTs to converge within a few epochs. Thanks to its faster convergence, as shown in Figure 1, Adan is very suitable for this situation. Moreover, the direction correction term from the gradient difference vk of Adan can also better correct the first- and second-order moments. One piece of evidence is that the first-order moment decay coefficient β1 = 0.02 of Adan is much smaller than 0.1 used in other deep optimizers.\n\nTable 4: Top-1 Acc. (%) of ViT-B and ViT-L trained by MAE under the official Training Setting II. ∗ and ⋄ are respectively reported in (Chen et al., 2022; He et al., 2022).\n\n2) Self-supervised MAE Training (pretraining + finetuning). We follow the MAE training framework to pretrain and fine-tune ViT-B for 300/800 pretraining epochs and 100 fine-tuning epochs, and ViT-L for 800 pretraining epochs and 50 fine-tuning epochs, on ImageNet. Table 4 shows that 1) on ViT-B, Adan makes 0.5% improvement over AdamW under 300 pretraining epochs, and Adan pretrained 800 epochs surpasses AdamW pretrained 1,600 epochs by nontrivial 0.2%; 2) on ViT-L, Adan only uses 800 pretraining epochs to achieve the same performance of AdamW with 1,600 pretraining epochs. All these results show a superior performance of Adan.\n\nAdamW 82.9∗ — 83.6⋄ Adan\n\n83.8 — 85.9 —\n\nMAE-ViT-L 1600 800\n\nMAE-ViT-B 800\n\n85.4⋄ 85.9⋄\n\nEpoch\n\n1600\n\n83.4\n\n300\n\n3) Large-Batch Training. Though large batch size can increase parallelism to reduce training time and is heavily desired, optimizers often suffer performance degradation, or even fail. For instance, AdamW fails to train ViTs when batch size is beyond 4,096. How to solve the problem remains open (He et al., 2021). At present, LAMB is the most effective optimizer for large batch size. Table 5 reveals that Adan is robust to batch sizes from 2k to 32k, and shows higher performance and robustness than LAMB.\n\nTable 5: Top-1 Acc. (%) of ViT-S on ImageNet under Training Setting I.\n\n78.9 79.2 79.8 79.7 79.5 78.4 80.9 81.1 81.1 80.8 80.5 80.2\n\nLAMB Adan\n\nBatch Size\n\n16k\n\n32k\n\n4k\n\n1k\n\n8k\n\n2k\n\n8\n\n020406080100Training Epochs on ViT-S34567Training LossAdan(ours)AdamAdamWLAMBSGD-M020406080100Training Epochs on ResNet-50234567Training LossAdan(ours)AdamWLAMBNadamSAMSGD-MUnder review as a conference paper at ICLR 2023\n\nTable 6: Results (the higher, the better) of BERT-base model on the development set of GLUE. The first line is from (Wolf et al., 2020) while the second line is reproduced by us.\n\nBERT-base\n\nMNLI\n\nQNLI\n\nQQP\n\nAdam (official) Adam (reproduced) Adan (ours)\n\n83.7/84.8 84.9/84.9 85.7/85.6\n\n89.3 90.8 91.3\n\n90.8 90.9 91.2\n\nRTE\n\n71.4 69.3 73.3\n\nSST-2\n\nCoLA\n\nSTS-B\n\nAverage\n\n91.7 92.6 93.2\n\n48.9 58.5 64.6\n\n91.3 88.7 89.3\n\n81.5 82.5 84.3 (+1.8)\n\n5.2 EXPERIMENTS FOR NATURAL LANGUAGE PROCESSING TASKS\n\nResults on BERT. Similar to the pretraining experiments of MAE which is also a self-supervised learning framework on vision tasks, we utilize Adan to train BERT (Devlin et al., 2018) from scratch, which is one of the most widely used pretraining models/frameworks for NLP tasks. We employ the exact BERT training setting in the widely used codebase—Fairseq (Ott et al., 2019). See more training details in Appendix A.3.\n\nFrom Table 6, one can see that in the most commonly used BERT training experiment, Adan reveals much better advantage over Adam. Specifically, in all GLUE tasks, on BERT-base model, Adan achieves higher performance than Adam, and makes 1.8 average improvements on all tasks. In addition, on some tasks of Adan, BERT-base trained by Adan can outperform some large models. e.g., BERT-large which achieves 70.4% on RTE, 93.2% on SST-2 and 60.6 correlation on CoLA, and XLNet-large which has 63.6 correlation on CoLA. See (Liu et al., 2019b) for more results.\n\nResults on Transformer-XL. We evaluate Adan on Transformer-XL (Dai et al., 2019) which is often used to model long sequences. We follow the exact official setting 2 to train Transformer-XL-base on the WikiText-103 dataset that is the largest available word-level language modeling benchmark with long-term dependency. We only replace the default Adam optimizer of Transformer-XLbase by our Adan, and do not make other changes for the hyper-parameter. For Adan, we set β1 = 0.1, β2 = 0.1, and β3 = 0.001, and choose learning rate as 0.001. We test Adan and Adam with several training steps, including 50k, 100k, and 200k (official).\n\nTable 7: Test PPL (the lower, the better) for Transformer-XL-base model on the WikiText-103 dataset.\n\nTraining Steps 100k 25.5 24.2\n\nAdam Adan (ours)\n\nTransformer-XL\n\n200k 24.2 23.5\n\n50k 28.5 26.2\n\nTable 7 shows that on Transformer-XL-base, Adan surpasses its default Adam optimizer in terms of test PPL (the lower, the better) under all training steps. Surprisingly, Adan using 100k training steps can even achieve comparable results to Adam with 200k training steps. All these results demonstrate the superiority of Adan over the default SoTA Adam optimizer in Transformer-XL.\n\nResults on LSTM. In Appendix B.4, the results on LSTM shows the superiority of our Adan over several representative optimizers, e.g. SGD, Adam and AdamW, on the Penn TreeBank dataset.\n\n5.3 DISCUSSION ON RESTART STRATEGY\n\nHere we investigate the performance Adan with and without restart strategy on ViT and ConvNext under 300 training epochs. From the results in Table 8, one can observe that the restart strategy slightly improves test performance of Adan on both ViT and ConvNext. However, to make our Adan simple and avoid hyper-parameter tuning of the restart strategy (e.g., restart frequency), in all experiments except Table 8, we do not use this restart strategy.\n\nTable 8: Top-1 Acc. (%) of ViT-S and ConvNext-T on ImageNet under Training Setting II trained with 300 epochs.\n\nViT Small ConvNext Tiny\n\nAdan w/o restart\n\nAdan w/ restart\n\n80.71\n\n80.87\n\n81.38\n\n81.62\n\n6 CONCLUSION\n\nIn this paper, we propose a new deep optimizer, Adan. We reformulate the vanilla AGD to a more efficient version and use it to estimate the first- and second-order moments in adaptive optimization algorithms. We prove that the complexity of Adan matches the lower bounds and is superior to those of other adaptive optimizers. Finally, extensive experimental results demonstrate that Adan consistently surpasses other optimizers on many popular backbones and frameworks, including ResNet, ConvNext, ViT, Swin, MAE-ViT, LSTM, Transformer-XL and BERT.\n\n2https://github.com/kimiyoung/transformer-xl\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nO. Abdel-Hamid, A. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu. Convolutional neural networks for speech recognition. IEEE Trans. on audio, speech, and language processing, 22(10): 1533–1545, 2014.\n\nYossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.\n\nYossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Second-order information in non-convex stochastic optimization: Power and limitations. In Conference on Learning Theory, pp. 242–299. PMLR, 2020.\n\nJinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in training deep neural networks. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 3267–3275, 2021a.\n\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets\n\nwithout pre-training or strong data augmentations. arXiv preprint arXiv:2106.01548, 2021b.\n\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022.\n\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated In Proceedings of the IEEE Conference on\n\ndata augmentation with a reduced search space. Computer Vision and Pattern Recognition Workshops, pp. 702–703, 2020.\n\nAshok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International Conference\n\non Machine Learning, pp. 2260–2268. PMLR, 2020.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nTimothy Dozat. Incorporating nesterov momentum into adam. 2016.\n\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International conference on machine learning, pp. 1329–1338. PMLR, 2016.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of Machine Learning Research, 12(7), 2011.\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization\n\nfor efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.\n\nZhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for\n\nalgorithms of the adam family. arXiv preprint arXiv:2112.03459, 2021.\n\nM. Hardt and T. Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nXiaoxin He, Fuzhao Xue, Xiaozhe Ren, and Yang You. Large-scale deep learning optimizations: A\n\ncomprehensive survey. arXiv preprint arXiv:2111.00856, 2021.\n\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\n\nstochastic depth. In European Conference on Computer Vision, pp. 646–661, 2016.\n\nChi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. In Conference On Learning Theory, pp. 1042–1085. PMLR, 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning, pp. 5905–5914. PMLR, 2021.\n\nHuan Li and Zhouchen Lin. Restarted nonconvex accelerated gradient descent: No more polyloga-\n\nrithmic factor in the O(cid:0)ε−7/4(cid:1) complexity. arXiv preprint arXiv:2201.11411, 2022.\n\nZ. Li and Y. Yuan. Convergence analysis of two-layer neural networks with relu activation. In\n\nAdvances in Neural Information Processing Systems, 2017.\n\nZhouchen Lin, Huan Li, and Cong Fang. Accelerated optimization for machine learning. Springer,\n\n2020.\n\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations, 2019a.\n\nMingrui Liu, Wei Zhang, Francesco Orabona, and Tianbao Yang. Adam +: A stochastic method\n\nwith adaptive variance reduction. arXiv preprint arXiv:2011.11985, 2020.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019b.\n\nYong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable\n\nsharpness-aware minimization. arXiv preprint arXiv:2203.02714, 2022a.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\n\nA convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022b.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\n\nence on Learning Representations, 2018.\n\nLiangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic\n\nbound of learning rate. In International Conference on Learning Representations, 2018.\n\nMary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Using\n\nLarge Corpora, 273, 1994.\n\nZachary Nado, Justin M Gilmer, Christopher J Shallue, Rohan Anil, and George E Dahl. A large batch optimizer reality check: Traditional, generic optimizers suffice across batch sizes. arXiv preprint arXiv:2102.06356, 2021.\n\nYurii Nesterov. On an approach to the construction of optimal methods of minimization of smooth\n\nconvex functions. Ekonomika i Mateaticheskie Metody, 24(3):509–517, 1988.\n\nYurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer\n\nScience & Business Media, 2003.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYurii E Nesterov. A method for solving the convex programming problem with convergence rate o\n\n(1/k2). In Dokl. akad. nauk Sssr, volume 269, pp. 543–547, 1983.\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.\n\nNeal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in optimization, 1(3):\n\n127–239, 2014.\n\nBoris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computa-\n\ntional mathematics and mathematical physics, 4(5):1–17, 1964.\n\nH. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics,\n\npp. 400–407, 1951.\n\nTara N Sainath, Brian Kingsbury, Abdel-rahman Mohamed, George E Dahl, George Saon, Hagen Soltau, Tomas Beran, Aleksandr Y Aravkin, and Bhuvana Ramabhadran. Improvements to deep convolutional neural networks for LVCSR. In 2013 IEEE workshop on automatic speech recognition and understanding, pp. 315–320. IEEE, 2013.\n\nJürgen Schmidhuber, Sepp Hochreiter, et al. Long short-term memory. Neural Comput, 9(8):\n\n1735–1780, 1997.\n\nNaichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. Rmsprop converges with proper hyper-\n\nparameter. In International Conference on Learning Representations, 2020.\n\nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033. IEEE, 2012.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021.\n\nHugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. arXiv preprint\n\narXiv:2204.07118, 2022.\n\nJiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. arXiv preprint arXiv:2107.14171, 2021.\n\nRoss Wightman, Hugo Touvron, and Hervé Jégou. Resnet strikes back: An improved training\n\nprocedure in timm. arXiv preprint arXiv:2110.00476, 2021.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\n\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and Ross Girshick. Early convolutions help transformers see better. Advances in Neural Information Processing Systems, 34: 30392–30400, 2021.\n\nBo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Artificial\n\nIntelligence and Statistics, pp. 1216–1224. PMLR, 2017.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nZeke Xie, Xinrui Wang, Huishuai Zhang, Issei Sato, and Masashi Sugiyama. Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In International Conference on Machine Learning, pp. 24430–24459. PMLR, 2022.\n\nYang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv\n\npreprint arXiv:1708.03888, 2017.\n\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2019.\n\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6023–6032, 2019.\n\nManzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods\n\nfor nonconvex optimization. Advances in neural information processing systems, 31, 2018.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. In International Conference on Learning Representations, 2018.\n\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33:15383–15393, 2020.\n\nJuntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances in Neural Information Processing Systems, 33:18795–18806, 2020.\n\nZhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, and Francesco Orabona. Understanding adamw\n\nthrough proximal methods and scale-freeness. arXiv preprint arXiv:2202.00089, 2022.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nThe appendix contains some additional experimental results and the technical proofs of convergence results of the paper entitled “Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models”. It is structured as follows. Sec. A provides details of the training setting and Adan’s implementation. It also gives detailed steps to perform the experiment on BERT. Sec. B include the additional experimental results, which contains the results on ResNet-18 in Sec. B.1, convergence curve in Sec. B.2, experiments on RL tasks in Sec. B.3, results on LSTM in Sec. B.4, and the ablation study in Sec. B.5.\n\nAfter Sec. C, which summarizes the notations throughout this document, we provide the technical proofs of convergence results. Then Sec. D provides the proof of the equivalence between AGD and reformulated AGD, i.e., the proof of Lemma 1. And then, given Lipschitz gradient condition, Sec. E provides the convergence analysis in Theorem 1. Next, we show Adan’s faster convergence speed with Lipschitz Hessian condition in Sec. F, by first reformulating our Algorithm 1 and introducing some auxiliary bounds. Finally, we present some auxiliary lemmas in Sec. G.\n\nA TRAINING SETTING AND IMPLEMENTATION DETAILS\n\nA.1 TRAINING SETTING\n\nTraining Setting I. The recently proposed “A2 training recipe” in (Wightman et al., 2021) has pushed the performance limits of many SoTA CNN-type architectures by using stronger data augmentation and more training iterations. For example, on ResNet50, it sets new SoTA 80.4%, and improves the accuracy 76.1% under vanilla setting in (He et al., 2016). Specifically, for data augmentation, this setting uses random crop, horizontal flipping, Mixup (0.1) (Zhang et al., 2018)/CutMix (1.0) (Yun et al., 2019) with probability 0.5, and RandAugment (Cubuk et al., 2020) with M = 7, N = 2 and MSTD = 0.5. It sets stochastic depth (0.05) (Huang et al., 2016), and adopts cosine learning rate decay and binary cross-entropy (BCE) loss. For Adan, we use batch size 2048 for ResNet and ViT.\n\nTraining Setting II. We follow the same official training procedure of ViT/Swin/ConvNext. For this setting, data augmentation includes random crop, horizontal flipping, Mixup (0.8), CutMix (1.0), RandAugment (M = 9, MSTD = 0.5) and Random Erasing (p = 0.25). We use CE loss, the cosine decay for base learning rate, the stochastic depth (with official parameters), and weight decay. For Adan, we set batch size 2048 for Swin/ViT/ConvNext and 4096 for MAE. We follow MAE and tune β3 as 0.1.\n\nA.2\n\nIMPLEMENTATION DETAILS OF ADAN\n\nFor the large-batch training experiment, we use the sqrt rule to scale the learning rate: (cid:113) batch size\n\n256 × 6.25e-3, and respectively set warmup epochs {20, 40, 60, 100, 160, 200} for batch size bs = {1k, 2k, 4k, 8k, 16k, 32k}. For other remaining experiments, we use the hyper-parameters: learning rate 1.5e-2 for ViT/Swin/ResNet/ConvNext and MAE fine-tuning, and 2.0e-3 for MAE pre-training according to the official settings. We set β1 = 0.02, β2 = 0.08 and β3 = 0.01, and let weight decay be 0.02 unless noted otherwise. We clip the global gradient norm to 5 for ResNet training and do not clip the gradient for ViT, Swin, ConvNext, and MAE. In the implementation, to keep consistent with Adam-type optimizers, we utilize the de-bias strategy for Adan.\n\nlr =\n\nA.3 DETAILED STEPS FOR BERT\n\nWe replace the default Adam optimizer in BERT with our Adan for both pretraining and fune-tuning. Specifically, we first pretrain BERT-base on the Bookcorpus and Wikipedia datasets, and then finetune BERT-base separately for each GLUE task on the corresponding training data. Note, GLUE is a collection of 9 tasks/datasets to evaluate natural language understanding systems, in which the tasks are organized as either single-sentence classification or sentence-pair classification.\n\nHere we simply replace the Adam optimizer in BERT with our Adan and do not make other changes, e.g. random seed, warmup steps and learning rate decay strategy, dropout probability, etc. For\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Top-1 accuracy (%) of ResNet18 under the official setting in (He et al., 2016). ∗ are reported in (Zhuang et al., 2020).\n\nAdan\n\nSGD\n\nNadam AdaBound Adam Radam Padam LAMB AdamW AdaBlief Adai\n\n70.60\n\n70.23∗\n\n68.82\n\n68.13∗\n\n63.79∗\n\n67.62∗\n\n70.07\n\n68.46\n\n67.93∗\n\n70.08∗\n\n69.68\n\nTable 10: Top-1 accuracy (%) of different optimizers when training ViT-S on ImageNet trained under training setting II. * is reported in (Touvron et al., 2021).\n\nEpoch\n\nAdamW (default) Adam Adai SGD-M (AGD) LAMB Adan (ours)\n\n100\n\n76.1 62.0 66.4 64.3 69.4 77.5\n\n150\n\n78.9 64.0 72.6 68.7 73.8 79.6\n\n200\n\n79.2 64.5 75.3 71.4 75.9 80.0\n\n300\n\n79.9∗ 66.7 77.4 73.9 77.7 80.9\n\npretraining, we use Adan with its default weight decay (0.02) and βs (β1 = 0.02, β2 = 0.08, and β3 = 0.01), and choose learning rate as 0.001. For fine-tuning, we consider a limited hyper-parameter sweep for each task, with a batch size of 16, and learning rates ∈ {2e − 5, 4e − 5} and use Adan with β1 = 0.02, β2 = 0.01, and β3 = 0.01 and weight decay 0.01. Following the conventional setting, we run each fine-tuning experiment three times and report the median performance in Table 6.\n\nSame as the official setting, on MNLI, we report the mismatched and matched accuracy. And we report Matthew’s Correlation and Person Correlation on the task of CoLA and STS-B, respectively. The performance on the other tasks is measured by classification accuracy.\n\nThe performance of our reproduced one (second row) is slightly better than the vanilla results of BERT reported in Huggingface-transformer (Wolf et al., 2020) (widely used codebase for transformers in NLP), since the vanilla Bookcorpus data in (Wolf et al., 2020) is not available and thus we train on the latest Bookcorpus data version.\n\nB ADDITIONAL EXPERIMENTAL RESULTS\n\nB.1 RESUTLS ON RESNET-18\n\nSince some well-known deep optimizers also test ResNet-18 for 90 epochs under the official vanilla training setting in (He et al., 2016), we also run Adan 90 epochs under this setting for more comparison. Table 9 shows that Adan consistently outperforms SGD and all compared adaptive optimizers. Note for this setting, it is not easy for adaptive optimizers to surpass SGD due to the absence of heavy-tailed noise, which is the crucial factor helping adaptive optimizers beat AGD (Zhang et al., 2020).\n\nB.2 DETAILED COMPARISON AND CONVERGENCE CURVE\n\nBesides AdamW, we also compare Adan with several other popular optimizers, including Adam, SGD-M, and LAMB, on ViT-S. Table 10 shows that SGD, Adam, and LAMB perform poorly on ViT-S, which is also observed in the works (Xiao et al., 2021; Nado et al., 2021). These results demonstrate that the decoupled weight decay in Adan and AdamW is much more effective than 1) the vanilla weight decay, namely the commonly used l2 regularization in SGD, and 2) the one without any weight decay, since as shown in Eqn. (6), the decoupled weight decay is a dynamic regularization along the training trajectory and could better regularize the loss. Compared with AdamW, the advantages of Adan mainly come from its faster convergence shown in Figure 2 (b). We will discuss this below.\n\nIn Figure 2 (a), we plot the curve of training and test loss along with the training epochs on ResNet50. One can observe that Adan converges faster than the compared baselines and enjoys the smallest training and test losses. This demonstrates its fast convergence property and good generalization ability. To sufficiently investigate the fast convergence of Adan, we further plot the curve of training and test loss on the ViT-Small in Figure 2 (b). From the results, we can see that Adan consistently\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Training and test curves on ResNet-50 under Training Setting I.\n\n(b) Training and test curves on ViT-S under Training Setting II.\n\nFigure 2: Training and test curves of various optimizers on ImageNet dataset. Training loss is larger due to its stronger data argumentation.\n\nshows faster convergence behaviors than other baselines in terms of both training loss and test loss. This also partly explains the good performance of Adan over other optimizers.\n\nDiscussion about convergence complexity Under the corresponding assumptions, most compared optimizers already achieve the optimal complexity in terms of the dependence on optimization ε, and their complexities only differ from their constant factors, e.g. c2, c∞ and d. For instance, with Lipschitz gradient but without Lipschitz Hessian, most optimizers have complexity O(cid:0) x (cid:1) which (cid:1) in Arjevani et al. (2019), where the constant factor x varies from matches the lower bound O(cid:0) 1 ε4 different optimizers, e.g.x = c2 2d in LAMB, and x = c2.5 ∞ in Adan. So under the same conditions, one cannot improve the complexity dependence on ε but can improve the constant factors, which are significant, especially for the network.\n\n∞d in Adam-type optimizer, x = c6\n\n2 in Adabelief, x = c2\n\nε4\n\nActually, we empirically find c∞ = O(8.2), c2 = O(430), d = 2.2 × 107 in the ViT-small across different optimizers, e.g., AdamW, Adam, Adan, LAMB. In the extreme case, under the widely used Lipschitz gradient assumption, the complexity bound of Adan is 7.6 × 106 smaller than the one of Adam, 3.3 × 1013 smaller than the one of AdaBlief, 2.1 × 1010 smaller than the one of LAMB, etc. For ResNet50, we also observe c∞ = O(78), c2 = O(970), d = 2.5 × 107 which also means a large big improvement of Adan over other optimizers.\n\nB.3 RESULTS ON REINFORCEMENT LEARNING TASKS\n\nHere we evaluate Adan on reinforcement learning tasks. Specifically, we replace the default Adam optimizer in PPO (Duan et al., 2016) which is one of the most popular policy gradient method, and do not many any other change in PPO. For brevity, we call this new PPO version “PPO-Adan\". Then we test PPO and PPO-Adan on several games which are actually continuous control environments simulated by the standard and widely-used engine, MuJoCo (Todorov et al., 2012). For these test games, their agents receive a reward at each step. Following standard evaluation, we run each game under 10 different and independent random seeds (i.e. 1 ∼ 10), and test the performance\n\n16\n\n020406080100Epochs234567Training LossResNet50Adan(ours)AdamWLAMBNadamSAMSGD-M020406080100Epochs123456Test LossResNet50Adan(ours)AdamWLAMBNadamSAMSGD-M020406080100Epochs1234567Test LossViT-SAdan(ours)AdamAdamWLAMBSGD-M020406080100Epochs34567Training LossViT-SAdan(ours)AdamAdamWLAMBSGD-MUnder review as a conference paper at ICLR 2023\n\nFigure 3: Comparison of PPO and our PPO-Adan on several RL games simulated by MuJoCo. Here PPO-Adan simply replaces the Adam optimizer in PPO with our Adan and does not change others.\n\nTable 11: Test perplexity (the lower, the better) on Penn Treebank for one-, two- and three-layered LSTMs. All results except Adan and Padam in the table are reported by AdaBelief.\n\nLSTM Adan AdaBelief SGD AdaBound Adam AdamW Padam RAdam Yogi\n\n1 layer 2 layers 3 layers\n\n83.6 65.2 59.8\n\n84.2 66.3 61.2\n\n85.0 67.4 63.7\n\n84.3 67.5 63.6\n\n85.9 67.3 64.3\n\n84.7 72.8 69.9\n\n84.2 67.2 63.2\n\n86.5 72.3 70.0\n\n86.5 71.3 67.5\n\nfor 10 episodes every 30,000 steps. All these experiments are based on the widely used codebase Tianshou3 (Weng et al., 2021). For fairness, we use the default hyper-parameters in Tianshou, e.g. batch size, discount, and GAE parameter. We use Adan with its default βs (β1 = 0.02, β2 = 0.08, and β3 = 0.01). Following the default setting, we do not adopt the weight decay and choose the learning rate as 3e-4.\n\nWe report the results on four test games in Figure 3, in which the solid line denotes the averaged episodes rewards in the evaluation and the shaded region is its 75% confidence intervals. From Figure 3, one can observe that on the four test games, PPO-Adan achieves much higher rewards than vanilla PPO which uses Adam as its optimizer. These results demonstrate the advantages of Adan over Adam, since PPO-Adan simply replaces the Adam optimizer in PPO with our Adan and does not make other changes.\n\nB.4 RESULTS ON LSTM\n\nTo begin with, we test our Adan on LSTM (Schmidhuber et al., 1997) by using the Penn TreeBank dataset (Marcinkiewicz, 1994), and report the perplexity (the lower, the better) on the test set in\n\n3https://github.com/thu-ml/tianshou\n\n17\n\n01 M2 M3 M4 M5 M6 MTimesteps0100020003000400050006000Episode Rewardppoppo_adanAnt01 M2 M3 M4 M5 M6 MTimesteps0200040006000800010000Episode Rewardppoppo_adanHalfCheetah01 M2 M3 M4 M5 M6 MTimesteps010002000300040005000Episode Rewardppoppo_adanWalker2d01 M2 M3 M4 M5 M6 MTimesteps010002000300040005000Episode Rewardppoppo_adanHumanoidUnder review as a conference paper at ICLR 2023\n\nFigure 4: Effects of momentum coefficients (β1, β2, β3) to top-1 accuracy (%) of Adan on ViT-B under MAE training framework (800 pretraining and 100 fine-tuning epochs on ImageNet).\n\nTable 12: Top-1 accuracy (%) of ViT-S on ImageNet trained under Training Setting I and II. ∗ is reported in (Touvron et al., 2021).\n\nTraining epochs\n\nTraining Setting I Adan AdamW\n\nTraining Setting II Adan AdamW\n\n150 300\n\n76.4 77.9\n\n80.2 81.1\n\n78.3 79.9∗\n\n79.6 80.7\n\nTable 11. We follow the exact experimental setting in Adablief (Zhuang et al., 2020). Indeed, all our implementations are also based on the code provided by Adablief (Zhuang et al., 2020)4. We use the default setting for all the hyper-parameters provided by Adablief, since it provides more baselines for a fair comparison. For Adan, we utilize its default weight decay (0.02) and βs (β1 = 0.02, β2 = 0.08, and β3 = 0.01). We choose the learning rate as 0.01 for Adan.\n\nTable 11 shows that on the three LSTM models, Adan always achieves the lowest perplexity, making about 1.0 overall average perplexity improvement over the runner-up. Moreover, when the LSTM depth increases, the advantage of Adan becomes more remarkable.\n\nB.5 ABLATION STUDY\n\nB.5.1 ROBUSTNESS TO IN MOMENTUM COEFFICIENTS\n\nHere we choose MAE to investigate the effects of the momentum coefficients (βs) to Adan, since as shown in MAE, its pre-training is actually sensitive to momentum coefficients of AdamW. To this end, following MAE, we pretrain and fine tune ViT-B on ImageNet for 800 pretraining and 100 fine-tuning epochs. We also fix one of (β1, β2, β3) and tune others. Figure 4 shows that by only pretraining 800 epochs, Adan achieves 83.7%+ in most cases and outperforms the official accuracy 83.6% obtained by AdamW with 1600 pretraining epochs, indicating the robustness of Adan to βs. We also observe 1) Adan is not sensitive to β2; 2) β1 has a certain impact on Adan, namely the smaller the (1.0 − β1), the worse the accuracy; 3) similar to findings of MAE, a small second-order coefficient (1.0 − β3) can improve the accuracy. The smaller the (1.0 − β3), the more current landscape information the optimizer would utilize to adjust the coordinate-wise learning rate. Maybe the complex pre-training task of MAE is more preferred to the local geometric information.\n\nB.5.2 ROBUSTNESS TO TRAINING SETTINGS\n\nIn convention, many works (Liu et al., 2021; 2022b; Touvron et al., 2022; Wightman et al., 2021; Touvron et al., 2021) often preferably chose LAMB/Adam/SGD for Training Setting I and AdamW for Training Setting II. Table 12 investigates Adan under both settings and shows consistent improvement of Adan. Moreover, one can also observe that Adan under Setting I largely improves the accuracy of Adan under Setting II. It actually surpasses the best-known accuracy 80.4% on ViT-small in (Touvron et al., 2022) trained by advanced layer scale strategy and stronger data augmentation.\n\n4https://github.com/juntang-zhuang/Adabelief-Optimizer.\n\nThe reported results in (Zhuang et al., 2020) slightly differ from the those in (Chen et al., 2021a) because of their different settings for LSTM and training hyper-parameters.\n\n18\n\n0.980.940.921.01=0.980.960.940.901.0383.6583.6783.6883.7383.7283.7083.6983.5983.751.020.980.970.951.03=0.90.980.940.921.0283.6983.6483.5783.5983.5483.5183.7583.7283.601.0183.483.583.683.783.8Acc. (%)0.980.970.951.02=0.920.960.940.901.0383.6883.5283.4583.7083.5883.5083.7583.7283.601.01Under review as a conference paper at ICLR 2023\n\nC NOTATION\n\nWe provide some notation that are frequently used throughout the paper. The scale c is in normal font. And the vector is in bold lowercase. Give two vectors x and y, x ≥ y means that (x − y) is a non-negative vector. x/y or x y represents the element-wise vector division. x ◦ y means the element-wise multiplication, and (x)2 = x ◦ x. ⟨·, ·⟩ is the inner product. Given a non-negative n + ε ◦ x(cid:11). Unless otherwise specified, ∥x∥ is the vector l2 vector n ≥ 0, we let ∥x∥2√ norm. Note that E(x) is the expectation of random random vector x.\n\nn := (cid:10)x,\n\n√\n\nD PROOF OF LEMMA 1: EQUIVALENCE BETWEEN THE AGD AND AGD II\n\nIn this section, we show how to get AGD II from AGD. For convenience, we omit the noise term ζk. Note that, let α := 1 − β1:\n\nAGD:\n\n \n\n\n\ngk = ∇f (θk − ηαmk−1) mk = αmk−1 + gk θk+1 = θk − ηmk\n\n.\n\nWe can get:\n\nθk+1 − ηαmk = θk − ηmk − ηαmk\n\n=θk − η(1 + α)(αmk−1 + ∇f (θk − ηαmk−1)) =θk − ηαmk−1 − ηα2mk−1 − η(1 + α)(∇f (θk − ηαmk−1)).\n\nLet\n\n(cid:40) ̄θk+1 := θk+1 − ηαmk,\n\n ̄mk := α2mk−1 + (1 + α)∇f (θk − ηαmk−1) = α2mk−1 + (1 + α)∇f ( ̄θk)\n\nThen, by Eq.(8), we have:\n\n ̄θk+1 = ̄θk − η ̄mk.\n\nOn the other hand, we have ̄mk−1 = α2mk−2 + (1 + α)∇f ( ̄θk−1) and :\n\n ̄mk − α ̄mk−1 = α2mk−1 + (1 + α)∇f ( ̄θk) − α ̄mk−1\n\n= (1 + α)∇f ( ̄θk) + α2(cid:0)αmk−2 + ∇f ( ̄θk−1)(cid:1) − α ̄mk−1 = (1 + α)∇f ( ̄θk) + α(cid:0)α2mk−2 + α∇f ( ̄θk−1) − ̄mk−1 (cid:1) = (1 + α)∇f ( ̄θk) + α(cid:0)α2mk−2 + α∇f ( ̄θk−1)(cid:1) − α ̄mk−1 = (1 + α)∇f ( ̄θk) − α∇f ( ̄θk−1) = ∇f ( ̄θk) + α(cid:0)∇f ( ̄θk) − ∇f ( ̄θk−1)(cid:1).\n\nFinally, due to Eq.(9) and Eq.10, we have:\n\n(8)\n\n(9)\n\n(10)\n\n \n\n\n\n(cid:16)\n\n∇f ( ̄θk) + α(cid:0)∇f ( ̄θk) − ∇f ( ̄θk−1)(cid:1)(cid:17)\n\n ̄mk = α ̄mk−1 + ̄θk+1 = ̄θk − η ̄mk\n\nE CONVERGENCE ANALYSIS WITH LIPSCHITZ GRADIENT\n\nBefore starting the proof, we first provide several notations. Let Fk(θ) := Eζ[f (θ, ζ)] + λk and μ :=\n\n2β3c∞/ε,\n\n√\n\n2 ∥θ∥2√\n\nnk\n\n∥x∥2√\n\nnk\n\n√\n\n:= ⟨x, (\n\nnk + ε) ◦ x⟩ ,\n\nλk = λ(1 − μ)k.\n\nMoreover, we let\n\n√\n\n ̃θk := (\n\nnk + ε) ◦ θk.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 2. Assume f (·) is L-smooth. For\n\nθk+1 = argmin\n\nθ\n\n(cid:18) λk 2\n\n∥θ∥2√\n\nnk\n\n+ f (θk) + ⟨uk, θ − θk⟩ +\n\n∥(θ − θk)∥2√\n\nnk\n\n(cid:19)\n\n.\n\n1 2η\n\nWith η ≤ min{ ε\n\n3L , 1\n\n10λ }, then we have:\n\nFk+1(θk+1) ≤ Fk(θk) −\n\nη 4c∞\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nη 2ε\n\n∥gk − uk∥2,\n\nwhere gk := ∇f (θk).\n\nProof. We denote pk := uk/(cid:0)√\n\nnk + ε(cid:1). By the optimality condition of θk+1, we have\n\nλkθk + pk =\n\nλk √\n\n ̃θk + uk nk + ε\n\n=\n\n1 + ηλk η\n\n(θk − θk+1).\n\n(11)\n\nThen for η ≤ ε\n\n3L , we have:\n\nFk+1(θk+1) ≤ f (θk) + ⟨∇f (θk), θk+1 − θk⟩ +\n\nL 2\n\n∥θk+1 − θk∥2 +\n\nλk+1 2\n\n∥θk+1∥2\n\n√\n\nnk+1\n\n(a) ≤ f (θk) + ⟨∇f (θk), θk+1 − θk⟩ +\n\n(b) ≤Fk(θk) +\n\n(cid:28)\n\nθk+1 − θk, λkθk +\n\nL 2\n\n∥θk+1 − θk∥2 +\n\n∥θk+1∥2√\n\nnk\n\nλk 2\nL/ε + λk 2\n\ngk√\n\nnk + ε\n\n(cid:29)\n\n+\n\n√\n\nnk\n\n∥θk+1 − θk∥2√\n\nnk\n\n=Fk(θk) +\n\n(c) = Fk(θk) +\n\nL/ε + λk 2\n(cid:18) L/ε + λk 2\n\n∥θk+1 − θk∥2√\n\n(cid:28)\n\n+\n\nθk+1 − θk, λkθk + pk +\n\nnk\n\ngk − uk √\nnk + ε\n\n(cid:29)\n\n−\n\n1 + ηλk η\n\n(cid:19)\n\n∥θk+1 − θk∥2√\n\nnk\n\n(cid:28)\n\n+\n\nθk+1 − θk,\n\ngk − uk √\nnk + ε\n\nnk\n\n√\n\n(cid:29)\n\n√\n\nnk\n\n(d) ≤ Fk(θk) +\n\n(cid:18) L/ε 2\n\n−\n\n1 η\n\n(cid:19)\n\n∥θk+1 − θk∥2√\n\nnk\n\n+\n\n1 2η\n\n∥θk+1 − θk∥2√\n\n+\n\nnk\n\nη 2ε\n\n∥gk − uk∥2\n\n≤Fk(θk) −\n\n≤Fk(θk) −\n\n1 3η η\n4c∞\n\n∥θk+1 − θk∥2√\n\n+\n\nnk\n\n∥gk − uk∥2\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n∥gk − uk∥2,\n\nη 2ε η\n2ε\n\nwhere (a) comes from the fact λk+1(1 − μ)−1 = λk and Proposition 3:\n\n(cid:18) √ √\n\nnk + ε nk+1 + ε\n\n(cid:19)\n\ni\n\n≥ 1 − μ,\n\nwhich implies:\n\nand (b) is from:\n\nλk+1∥θk+1∥2\n\n√\n\n≤\n\nnk+1\n\nλk+1 1 − μ\n\n∥θk+1∥2√\n\nnk\n\n= λk∥θk+1∥2√\n\n,\n\nnk\n\n∥θk+1∥2√\n\n=\n\nnk\n\n(cid:16)\n\n∥θk∥2√\n\nnk\n\n+ 2 ⟨θk+1 − θk, θk⟩√\n\nnk\n\n+ ∥θk+1 − θk∥2√\n\n(cid:17)\n\n,\n\nnk\n\n(c) is due to Eqn. (11), and for (d), we utilize:\n\n(cid:28)\n\nθk+1 − θk,\n\n(cid:29)\n\ngk − uk √\nnk + ε\n\n√\n\nnk\n\n≤\n\n1 2η\n\n∥θk+1 − θk∥2√\n\n+\n\nnk\n\nη 2ε\n\n∥gk − uk∥2,\n\nthe last inequality comes from the fact in Eqn. (11) and η ≤ 1\n\n10λ , such that: (cid:13) η\n2 (cid:13) (cid:13) 4c∞\n\n ̃θk\n\n≥\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\n1 3η\n\n∥(θk+1 − θk)∥2√\n\n=\n\nnk\n\n3(cid:0)√\n\nη\n\nnk + ε(cid:1)(1 + ηλk)\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 1. Suppose Assumptions 1 and 2 hold. Let cl := 1 c∞\n\nη2 ≤\n\n1\n\nclβ2 uL2 , max {β1, β2} ≤ 8c3\n\nclε2 96cuσ2 , T ≥ max\n\nand cu := 1 (cid:26) 24∆0 ηclε2 ,\n\nε . With β3c∞/ε ≪ 1, (cid:27)\n\n24cuσ2 β1clε2\n\n,\n\nwhere ∆0 := F (θ0) − f ∗ and f ∗ := minθ Eζ[∇f (θk, ζ)], then we let uk := mk + (1 − β1)vk and have:\n\n1 T + 1\n\nT (cid:88)\n\nk=0\n\nE\n\n(cid:18)(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n(cid:13) (cid:13) (cid:13)\n\n2(cid:19)\n\n≤ ε2,\n\nand\n\n1 T + 1\n\nT (cid:88)\n\nk=0\n\nHence, we have:\n\n(cid:18)(cid:13) (cid:13)\n\nE\n\n(cid:13)mk − gf ull\n\nk\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n≤\n\nε2 4\n\n,\n\n1 T + 1\n\nT (cid:88)\n\nk=0\n\n(cid:16)\n\nE\n\n∥vk∥2(cid:17)\n\n≤\n\nε2 4\n\n.\n\n1 T + 1\n\nT (cid:88)\n\nk=0\n\nE\n\n(cid:32)(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇θk\n\n(cid:18) λk 2\n\n∥θ∥2√\n\nnk\n\n+ Eζ[∇f (θk, ζ)]\n\n2(cid:33)\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ 4ε2.\n\nProof. For convince, we let uk := mk + (1 − β1)vk and gf ull\n\nk\n\n:= Eζ[∇f (θk, ζ)]. We have:\n\n(cid:13) (cid:13)\n\n(cid:13)uk − gf ull\n\nk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\n≤ 2\n\n(cid:13)mk − gf ull\n\nk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 2(1 − β1)2∥vk∥2.\n\nBy Lemma 2, Lemma 5, and Lemma 6, we already have:\n\nFk+1(θk+1) ≤ Fk(θk) −\n\nηcl 4\n\n(cid:18)(cid:13) (cid:13)\n\nE\n\n(cid:13)mk+1 − gf ull\n\nk+1\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)uk + λk (cid:18)(cid:13) (cid:13)\n\n≤ (1 − β1)E\n\n(cid:13)mk − gf ull\n\nk\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ ηcu\n\n(cid:13) (cid:13)\n\n(cid:13)gf ull k − mk 2(cid:19)\n\n(cid:13) 2\n(cid:13) (cid:13) (1 − β1)2L2 β1\n\n(cid:13) (cid:13) (cid:13)\n\n+\n\n+ ηcu(1 − β1)2∥vk∥2,\n\n(12)\n\n(cid:16)\n\nE\n\n∥θk+1 − θk∥2(cid:17)\n\n+ β2\n\n1 σ2\n\n(cid:16)\n\nE\n\n∥vk+1∥2(cid:17)\n\n≤ (1 − β2)E\n\n(cid:16)\n\n∥vk∥2(cid:17)\n\n+ 2β2E\n\n(cid:18)(cid:13) (cid:13)\n\n(cid:13)gf ull\n\nk+1 − gf ull\n\nk\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ 3β2\n\n2 σ2\n\n(13)\n\n(14)\n\nThen by adding Eq.(12) with ηcu β1\n\n× Eq.(13) and ηcu(1−β1)2\n\nβ2\n\n× Eq.(14), we can get:\n\n(cid:32)\n\nE(Φk+1) ≤ E\n\nΦk −\n\nηcl 4\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nηcu β1\n\n(cid:32)\n\n(1 − β1)2L2 β1\n\n∥θk+1 − θk∥2 + β2\n\n1 σ2\n\n(cid:33)(cid:33)\n\nηcu(1 − β1)2 β2\n\n+\n\n(cid:32)\n\n(cid:16)\n\nE\n\n2β2L2∥θk+1 − θk∥2 + 3β2\n\n2 σ2(cid:17)\n\n≤E\n\nΦk −\n\nηcl 4\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ ηcuL2\n\n(cid:32)\n\n(1 − β1)2 β2 1\n\n(cid:33)\n\n(cid:33)\n\n+ 2(1 − β1)2\n\n∥θk+1 − θk∥2\n\n+ (β1 + 3β2)ηcuσ2 (cid:18)\n\n(a)\n\n≤ E\n\n(cid:18)\n\n(b)\n\n≤E\n\nΦk −\n\nΦk +\n\n(cid:18)\n\n≤E\n\nΦk −\n\nwhere we let:\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\nηcl 4\n(cid:18) (ηcu)3L2 β2 1\n\nηcl 8\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\n−\n\nηcl 4\n(cid:13) (cid:13) (cid:13)\n\n ̃θk\n\n2(cid:19)\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nηcuL2 β2 1\n\n∥θk+1 − θk∥2\n\n(cid:19)\n\n+ 4βmηcuσ2\n\n(cid:19)(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ 4βmηcuσ2\n\n+ 4βmηcuσ2,\n\nΦk := Fk(θk) − f ∗ +\n\nβm = max {β1, β2} ≤\n\nηcu β1 2\n3\n\n,\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nηcu(1 − β1)2 β2\n\n∥vk∥2,\n\n(cid:13) (cid:13)\n\nk\n\n(cid:13)mk − gf ull clβ2 uL2 , 8c3\n\nη ≤\n\n1\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nand for (a), when β1 ≤ 2\n\n3 , we have:\n\n(1 − β1)2 β2 1\n\n+ 2(1 − β1)2 <\n\n1 β2 1\n\n,\n\nand (b) is due to Eq.(11) from Lemma 2. And hence, we have:\n\nT (cid:88)\n\nk=0\n\nE(Φk+1) ≤\n\nT (cid:88)\n\nk=0\n\nE(Φk) −\n\nηcl 8\n\nT (cid:88)\n\nk=0\n\n(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ (T + 1)4ηcuβmσ2.\n\nHence, we can get:\n\n1 T + 1\n\nT (cid:88)\n\nk=0\n\nE\n\n(cid:18)(cid:13) (cid:13) (cid:13)uk + λk\n\n ̃θk\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n≤\n\n8Φ0 ηclT\n\n+\n\n32cuβσ2 cl\n\n=\n\n8∆0 ηclT\n\n+\n\n8cuσ2 β1clT\n\n+\n\n32cuβmσ2 cl\n\n≤ ε2,\n\nwhere\n\n∆0 := F (θ0) − f ∗,\n\nβm ≤\n\nclε2 96cuσ2 , T ≥ max\n\n(cid:26) 24∆0 ηclε2 ,\n\n24cuσ2 β1clε2\n\n(cid:27)\n\n.\n\nWe finish the first part of the theorem. From Eq.(13), we can conclude that:\n\n1 T + 1\n\nT (cid:88)\n\nk=0\n\n(cid:18)(cid:13) (cid:13)\n\nE\n\n(cid:13)mk − gf ull\n\nk\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n≤\n\nσ2 βT\n\n+\n\nL2η2c2\n\nuε2\n\nβ2 1\n\n+ β1σ2 <\n\nε2 4\n\n.\n\nFrom Eq.(14), we can conclude that:\n\n1 T + 1\n\nT (cid:88)\n\nk=0\n\n(cid:16)\n\nE\n\n∥vk∥2(cid:17)\n\n≤ 2L2η2c2\n\nuε2 + 3β2σ2 <\n\nε2 4\n\n.\n\nFinally we have:\n\n1 T + 1\n\n≤\n\n1 T + 1\n\nT (cid:88)\n\nE\n\nk=0 (cid:32) T\n\n(cid:88)\n\nk=0\n\n(cid:32)(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇θk\n\n(cid:18) λk 2\n\n∥θ∥2√\n\nnk\n\n+ Eζ[f (θk, ζ)]\n\n2(cid:33)\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:18)\n\nE\n\n(cid:13) (cid:13) (cid:13)uk + λk 2\n\n ̃θk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 4\n\n(cid:13) (cid:13)\n\n(cid:13)mk − gf ull\n\nk\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 4∥vk∥2\n\n(cid:19)(cid:33)\n\n≤ 4ε2.\n\nNow, we have finished the proof.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nF FASTER CONVERGENCE WITH LIPSCHITZ HESSIAN\n\nFor convince, we let λ = 0, β1 = β2 = β and β3 = β2 in the following proof. To consider the weight decay term in the proof, we refer to the previous section for more details. For the ease of notation, we denote x instead of θ the variable needed to be optimized in the proof, and abbreviate Eζ[f (θk, ζ)] as f (θk).\n\nF.1 REFORMULATION\n\nAlgorithm 2: Nesterov Adaptive Momentum Estimation Reformulation Input: initial point θ0, stepsize η, average coefficients β, and ε.\n\n1 begin\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\nwhile k < K do\n\nget stochastic gradient estimator gk at xk; ˆmk = (1 − β) ˆmk−1 + β(gk + (1 − β)(gk − gk−1)); nk = (cid:0)1 − β2(cid:1)nk−1 + β2(gk−1 + (1 − β)(gk−1 − gk−2))2; ηk = η/(cid:0)√ nk + ε(cid:1); yk+1 = xk − ηkβgk; xk+1 = yk+1 + (1 − β)[(yk+1 − yk) + (ηk−1 − ηk)( ˆmk−1 − βgk−1)]; if (k + 1) (cid:80)k\n\n≥ R2 then\n\nt=0\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:0)√\n\n(cid:13) 2\nnt + ε(cid:1)1/2 (cid:13) (cid:13) get stochastic gradient estimator g0 at xk+1; ˆm0 = g0, n0 = g2\n\n◦ (yt+1 − yt)\n\n0, x0 = y0 = xk+1, x1 = y1 = x0 − η\n\nˆm0√\n\nn0+ε ,\n\nk = 1;\n\nend if end while\n\nK0 = argmin⌊ K\n\n2 ⌋≤k≤K−1\n\n(cid:0)√\n\n(cid:13) (cid:13) (cid:13)\n\nnk + ε(cid:1)1/2\n\n◦ (yk+1 − yk)\n\n(cid:13) (cid:13) (cid:13);\n\n15 end\n\nOutput: ̄x := 1 K0\n\n(cid:80)K0\n\nk=1 xk\n\nWe first prove the equivalent form between Algorithm 1 and Algorithm 2. The main iteration in Algorithm 1 is:\n\n \n\n\n\nmk = (1 − β)mk−1 + βgk, vk = (1 − β)vk−1 + β((gk − gk−1)), xk+1 = xk − ηk ◦ (mk + (1 − β)vk).\n\nLet ˆmk := (mk + (1 − β)vk), we can simplify the variable:\n\n(cid:26) ˆmk = (1 − β) ˆmk−1 + β(gk + (1 − β)(gk − gk−1)),\n\nxk+1 = xk − ηk ◦ ˆmk.\n\nWe let yk+1 := xk+1 + ηk( ˆmk − βgk), then we can get:\n\nyk+1 = xk+1 + ηk ˆmk − βηkgk = xk+1 + xk − xk+1 − βηkgk = xk − βηkgk.\n\nOn one hand, we have:\n\nxk+1 = xk − ηk ˆmk = yk+1 − ηk( ˆmk − βgk).\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nOn the other hand:\n\nηk( ˆmk − βgk) = (1 − β)ηk( ˆmk−1 + β(gk − gk−1))\n\n=(1 − β)ηk( ˆmk−1 + β(gk − gk−1)) (cid:18) xk−1 − xk ηk−1\n\n=(1 − β)ηk\n\n+ β(gk − gk−1)\n\n(cid:19)\n\n=(1 − β)\n\n=(1 − β)\n\n=(1 − β)\n\nηk ηk−1 ηk ηk−1 (cid:20) ηk ηk−1\n\n(xk−1 − xk + βηk−1(gk − gk−1))\n\n(yk − xk + βηk−1gk)\n\n(yk − yk+1 − β(ηk − ηk−1)gk)\n\n(cid:21)\n\n(cid:20)\n\n=(1 − β)\n\n(yk − yk+1) +\n\n(cid:20)\n\n=(1 − β)\n\n(yk − yk+1) +\n\nηk − ηk−1 ηk−1 ηk − ηk−1 ηk−1\n\n(yk − yk+1 − βηkgk)\n\n(cid:21)\n\n(cid:21)\n\n(yk − xk)\n\n=(1 − β)[(yk − yk+1) + (ηk − ηk−1)(mk−1 − βgk−1)].\n\nHence, we can conclude that:\n\nxk+1 = yk+1 + (1 − β)[(yk+1 − yk) + (ηk−1 − ηk)( ˆmk−1 − βgk−1)].\n\nThe main iteration in Algorithm 1 becomes:\n\n \n\n\n\nyk+1 = xk − βηkgk,\n\n(cid:20)\n\nxk+1 = yk+1 + (1 − β)\n\n(yk+1 − yk) +\n\nηk−1 − ηk ηk−1\n\n(yk − xk)\n\n(cid:21) .\n\n(15)\n\nF.2 AUXILIARY BOUNDS\n\nWe first show some interesting property. Define K to be the iteration number when the ’if condition’ triggers, that is,\n\nK := min\n\nk\n\n(cid:40)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk\n\nk\n\nk−1 (cid:88)\n\nt=0\n\n√\n\n(cid:13) (cid:13) (cid:13)(\n\nnt + ε)1/2 ◦ (yt+1 − yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:41)\n\n> R2\n\n.\n\nProposition 1. Given k ≤ K and β ≤ ε/(cid:0)√\n\n2c∞ + ε(cid:1), we have:\n\n√\n\n(cid:13) (cid:13) (cid:13)(\n\nnk + ε)1/2 ◦ (xk − yk)\n\n(cid:13) (cid:13) (cid:13) ≤ R.\n\nProof. First of all, we let ˆnk := (cid:0)√\n\nnk + ε(cid:1)1/2\n\n. Due to Proposition 3, we have:\n\nthen, we get:\n\n(cid:18) √ nk−1 + ε √\nnk + ε\n\n(cid:19)\n\ni\n\n(cid:34)\n\n√\n\n∈\n\n1 −\n\n2β2c∞ ε\n\n, 1 +\n\n√\n\n(cid:35)\n\n,\n\n2β2c∞ ε\n\n(cid:32)\n\n√\n\nˆnk ≤\n\n1 −\n\n2β2c∞ ε\n\n(cid:33)−1/2\n\nˆnk−1 ≤ (1 − β)−1/4 ˆnk−1,\n\nwhere we use the fact β ≤ ε/(cid:0)2\n\n√\n\n2c∞ + ε(cid:1).For any 1 ≤ k ≤ K, we have:\n\n∥ˆnk ◦ (yk − yk−1)∥2 ≤ (1 − β)−1/2∥ˆnk−1 ◦ (yk − yk−1)∥2\n\n≤(1 − β)−1\n\nk−1 (cid:88)\n\nt=1\n\n∥ˆnt ◦ (yt+1 − yt)∥2 ≤\n\nR2 k(1 − β)\n\n,\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nhence, we can conclude that:\n\n∥ˆnk ◦ (yk − yk−1)∥2 ≤\n\nR2 k(1 − β)\n\n.\n\n(16)\n\nOn the other hand, by Eq.(15), we have:\n\nxk+1 − yk+1 = (1 − β)\n\n(yk+1 − yk) +\n\n(cid:20)\n\nηk − ηk−1 ηk−1\n\n(xk − yk)\n\n(cid:21) ,\n\nand hence,\n\n∥ˆnk ◦ (xk − yk)∥ ≤ (1 − β)\n\n(cid:20)\n\n∥ˆnk ◦ (yk − yk−1)∥ +\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nηk−1 − ηk−2 ηk−2\n\n(cid:13) (cid:13) (cid:13) (cid:13)∞\n\n∥ˆnk ◦ (xk−1 − yk−1)∥\n\n(cid:21)\n\n(a)\n\n≤ (cid:112)1 − β\n\nR √\n\nk\n\n+ (1 − β)\n\n√\n\n2β2c∞ ε\n\n(cid:32)\n\n√\n\n1 −\n\n2β2c∞ ε\n\n(cid:33)−1/2\n\n∥ˆnk−1 ◦ (xk−1 − yk−1)∥\n\n≤ (cid:112)1 − β\n\nR √\n\n≤ (cid:112)1 − βR\n\nk (cid:32)\n\n+ β(1 − β)3/4∥ˆnk−1 ◦ (xk−1 − yk−1)∥\n\n1 √\n\nk\n\n+\n\nβ(1 − β)3/4 k − 1\n\n√\n\n(cid:16)\n\nβ(1 − β)3/4(cid:17)k−1(cid:33)\n\n+ · · · +\n\n(b)\n\n≤ (cid:112)1 − βR\n\n(cid:32)k−1 (cid:88)\n\nt=1\n\n1 t2\n\n(cid:33)1/4(cid:32) k\n\n(cid:88)\n\n(cid:16)\n\nβ(1 − β)3/4(cid:17)4t/3(cid:33)3/4\n\n(c) < R,\n\nt=0\n\nwhere (a) comes from Eq.(16) and the proposition 3, (b) is the application of Hölder’s inequality and (c) comes from the facts when β ≤ 1/2:\n\n∞ (cid:88)\n\nt=1\n\n1\n\nt2 =\n\nπ2 6\n\n, (cid:112)1 − β\n\n(cid:32) k\n\n(cid:88)\n\n(cid:16)\n\nβ(1 − β)3/4(cid:17)4t/3(cid:33)3/4\n\n≤\n\nt=0\n\n(cid:18) (1 − β)2/3\n\n1 − β4/3(1 − β)\n\n(cid:19)3/4\n\n.\n\nF.3 DECREASE OF ONE RESTART CYCLE\n\nLemma 3. Suppose that Assumptions 1-2 hold. Let R = O(cid:0)ε0.5(cid:1), β = O(cid:0)ε2(cid:1), η = O(cid:0)ε1.5(cid:1), K ≤ K = O(cid:0)ε−2(cid:1). Then we have:\n\nE (f (yK) − f (x0)) = −O(cid:0)ε1.5(cid:1).\n\n(17)\n\nProof. Recall Eq.(15) and denote gf ull\n\n:= ∇f (θk) for convenience:\n\nyk+1 = xk − βηk ◦\n\nk (cid:16)\n\n(cid:17)\n\ngf ull\n\nk + ξk (cid:20)\n\nxk+1 − yk+1 = (1 − β)\n\n(yk+1 − yk) +\n\n \n\n\n\nIn this proof, we let ˆnk := (cid:0)√\n\nnk + ε(cid:1)1/2\n\nfor 1 ≤ k ≤ K, we have:\n\n(cid:18) ηk − ηk−1 ηk−1\n\n(cid:19)(cid:21)\n\n(18)\n\n◦ (xk − yk)\n\n,\n\n, and hence ηk = η/ˆn2\n\nk. By the L-smoothness condition,\n\nE (f (yk+1) − f (xk)) ≤ E\n\n(cid:18)\n\n⟨gk, yk+1 − xk⟩ +\n\n∥yk+1 − xk∥2\n\n(cid:19)\n\nL 2\n\n(cid:18)\n\n−\n\n(cid:28) yk+1 − xk βηk\n\n+ ξk, yk+1 − xk\n\n+\n\n(cid:29)\n\n∥yk+1 − xk∥2\n\n(cid:19)\n\nL 2\n\n∥ˆnk ◦ (yk+1 − xk)∥2 +\n\n∥yk+1 − xk∥2\n\n(cid:19)\n\n+\n\nL 2\n\n∥ˆnk ◦ (yk+1 − xk)∥2 +\n\n∥ˆnk ◦ (yk+1 − xk)∥2\n\n+\n\nηβσ2 ε\n(cid:19)\n\n(19)\n\nηβσ2 ε\n\n= E\n\n(a) ≤ E\n\n(cid:18)\n\n(cid:18)\n\n(cid:18)\n\n≤ E\n\n≤ E\n\n−\n\n−\n\n1 ηβ\n\n1 ηβ\n\n∥ˆnk ◦ (yk+1 − xk)∥2\n\n(cid:19)\n\n−\n\n1 2ηβ\n\nηβσ2 ε\n\n,\n\nL 2ε\n\n+\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nwhere (a) comes from the facts:\n\nE (⟨ξk, yk+1 − xk⟩) = E (⟨ξk, xk − βηk ◦ (gk + ξk)⟩) = E (⟨ξk, βηk ◦ ξk⟩) ≤\n\nηβσ2 ε\n\n.\n\nand the last inequality is due to Lη ≤ ε. On the other hand, we have: (cid:19)\n\n(cid:18)\n\nE(f (xk) − f (yk)) ≤ E\n\n⟨∇f (yk), xk − yk⟩ +\n\n∥xk − yk∥2\n\nL 2\n\n(cid:18)\n\n⟨gk, xk − yk⟩ + ⟨∇f (yk) − ∇f (xk), xk − yk⟩ +\n\n(cid:19)\n\n∥xk − yk∥2\n\nL 2\n\n(cid:18)\n\n⟨gk, xk − yk⟩ +\n\n(cid:18)\n\n⟨gk, xk − yk⟩ +\n\n1 2L\n\n3L 2\n\n∥∇f (yk) − ∇f (xk)∥2 +\n\nL 2\n\n∥xk − yk∥2 +\n\n(cid:19)\n\n∥xk − yk∥2\n\nL 2\n\n(cid:19)\n\n∥xk − yk∥2\n\n(cid:18)\n\n−\n\n(cid:28) yk+1 − xk βηk\n\n(cid:29)\n\n+ ξk, xk − yk\n\n+\n\n(cid:19)\n\n∥xk − yk∥2\n\n3L 2\n\n(cid:10)ˆn2\n\nk ◦ (yk+1 − xk), yk − xk\n\n(cid:11) +\n\n3L 2\n\n(cid:19)\n\n∥xk − yk∥2\n\n= E\n\n≤ E\n\n≤ E\n\n= E\n\n= E\n\n(a) ≤ E\n\n(b) ≤ E\n\n(cid:18) 1 ηβ (cid:18) 1 2ηβ (cid:18) 1 2ηβ\n\n(cid:16)\n\n∥ˆnk ◦ (yk+1 − xk)∥2 + ∥ˆnk ◦ (yk − xk)∥2 − ∥ˆnk ◦ (yk+1 − yk)∥2(cid:17)\n\n+\n\n(cid:19)\n\n∥xk − yk∥2\n\n3L 2\n\n(cid:16)\n\n∥ˆnk ◦ (yk+1 − xk)∥2 − ∥ˆnk ◦ (yk+1 − yk)∥2(cid:17)\n\n+\n\n1 + β/2 2ηβ\n\n∥ˆnk ◦ (yk − xk)∥2\n\n(cid:19)\n\nwhere (a) comes from the following facts, and in (b), we use 3Lη ≤ ε 2 : 2 (cid:10)ˆn2\n\nk ◦ (yk+1 − xk), yk − xk\n\n(cid:11) = ∥ˆnk ◦ (yk+1 − xk)∥2+∥ˆnk ◦ (yk − xk)∥2−∥ˆnk ◦ (yk+1 − yk)∥2.\n\n(20)\n\nBy combing Eq.(19) and Eq.(20), we have:\n\nE (f (yk+1) − f (yk)) ≤ E\n\n(cid:18)\n\n−\n\n1 2ηβ\n\n∥ˆnk ◦ (yk+1 − yk)∥2 +\n\n1 + β/2 2ηβ\n\n∥ˆnk ◦ (yk − xk)∥2\n\n(cid:19)\n\n+\n\n(a) ≤ E\n\n(cid:18)\n\n−\n\n1 2ηβ\n\n∥ˆnk ◦ (yk+1 − yk)∥2 +\n\n1 − β/2 − β2/2 2ηβ\n\n∥ˆnk−1 ◦ (yk − yk−1)∥2\n\n(cid:19)\n\n+\n\n4β2R2c2 ∞\nηε2\n\nηβσ2 ε\nηβσ2 ε\n\n+\n\n,\n\nwhere (a) comes from the following fact, and note that by Proposition 1 we already have ˆnk ≤ (1 − β)−1/4 ˆnk−1:\n\n∥ˆnk ◦ (xk − yk)∥2 (cid:20) (1 + α)∥ˆnk ◦ (yk − yk−1)∥2 + (1 +\n\n≤(1 − β)2\n\n) ˆβ2∥ˆnk ◦ (xk−1 − yk−1)∥2\n\n(cid:21)\n\n1 α\n\n≤(1 − β)3/2\n\n(cid:20) (1 + α)∥ˆnk−1 ◦ (yk − yk−1)∥2 + (1 +\n\n) ˆβ2∥ˆnk−1 ◦ (xk−1 − yk−1)∥2\n\n(cid:21)\n\n1 α\n\n≤(1 − β)∥ˆnk−1 ◦ (yk − yk−1)∥2 +\n\n≤(1 − β)∥ˆnk−1 ◦ (yk − yk−1)∥2 +\n\nˆβ2(1 − β)3/2 1 − (1 − β)1/2 2 ˆβ2 β\n\n∥ˆnk−1 ◦ (xk−1 − yk−1)∥2\n\n∥ˆnk−1 ◦ (xk−1 − yk−1)∥2\n\n(21)\n\n≤(1 − β)∥ˆnk−1 ◦ (yk − yk−1)∥2 + 4β3R2c2\n\n∞/ε2,\n\n√\n\nwhere we let ˆβ := 2β2c∞/ε, α = (1 − β)−1/2 − 1, and the last inequality we use the results in Proposition 1. Summing over k = 2, · · · , K − 1, and note that y1 = x1, and hence we have E (f (y2) − f (x1)) = E (f (y2) − f (y1)) ≤ ηβσc∞/\n\nε due to Eq. (19), then we get:\n\n√\n\nE (f (yK) − f (y1)) ≤ E\n\n−\n\n(cid:32)\n\n∥ˆnk ◦ (yt+1 − yt)∥2\n\n+\n\n(cid:33)\n\n1 4η\n\nK−1 (cid:88)\n\nt=1\n\n4Kβ2R2c2 ∞\nηε2\n\n+\n\nKηβσ2 ε\n\n.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nOn the other hand, similar to the results given in Eq.(19), we have:\n\nE (f (y1) − f (y0)) = E (f (x1) − f (x0)) ≤ E\n\n(cid:18)\n\n−\n\n1 2η\n\n∥ˆnk ◦ (y1 − y0)∥2\n\n(cid:19)\n\n+\n\nησ2 ε\n\n.\n\nTherefore, using βK = O(1) and the restart condition\n\nK\n\nK−1 (cid:88)\n\nt=0\n\n√\n\n(cid:13) (cid:13) (cid:13)(\n\nnt + ε)1/2 ◦ (yt+1 − yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≥ R2,\n\nwe can get:\n\nE (f (yK) − f (y0)) ≤ E\n\n−\n\n(cid:32)\n\nK−1 (cid:88)\n\n∥ˆnk ◦ (yk+1 − yk)∥2\n\n+\n\n(cid:33)\n\n1 4η\n\n4Kβ2R2c2 ∞\nηε2\n\n+\n\n(Kβ + 1)ησ2 ε\n\n≤ −\n\nR2 4Kη\n\n+\n\n4Kβ2R2c2 ∞\nηε2\n\n+\n\nt=0 (Kβ + 1)ησ2 ε\n\n= −O\n\n(cid:18) R2 Kη\n\n−\n\nβR2 η\n\n(cid:19)\n\n− η\n\n= −O(cid:0)ε1.5(cid:1).\n\nNow, we finish the proof of this claim.\n\nF.4 GRADIENT IN THE LAST RESTART CYCLE\n\nBefore showing the main results, we first provide several definitions for the convenience of proof. Note that, for any k < K we already have:\n\n(ε)1/2∥yk − y0∥ ≤ (ε)1/2\n\n(cid:118) (cid:117) (cid:117) (cid:116)k\n\nk−1 (cid:88)\n\nt=0\n\n∥yt+1 − yt∥2 ≤ R.\n\nand we have:\n\nE (∥xk − x0∥) ≤ E (∥yk − xk∥ + ∥yk − x0∥) ≤\n\n2R ε1/2\n\n,\n\n(22)\n\nwhere we utilize the results from Proposition 1. For each epoch, denote H := ∇2f (x0). We then define:\n\nh(y) :=\n\nRecall the Eq. (15):\n\n(cid:68)\n\ngf ull\n\n0\n\n, y − x0\n\n+\n\n(y − x0)⊤H(y − x0).\n\n(cid:69)\n\n1 2\n\n \n\nyk+1 = xk − βηk ◦\n\n(cid:17)\n\n(cid:16)\n\ngf ull\n\nk + ξk (cid:20)\n\n= xk − βηk ◦ (∇h(xk) + δk + ξk) (cid:19)(cid:21)\n\n\n\nxk+1 − yk+1 = (1 − β)\n\n(yk+1 − yk) +\n\n◦ (xk − yk)\n\n,\n\n(cid:18) ηk − ηk−1 ηk−1\n\nwhere we let δk := gf ull\n\nk − ∇h(xk), and we can get that:\n\nE (∥δk∥) = E\n\nk − gf ull\n\n0 − H(xk − x0)\n\n(cid:16)(cid:13) (cid:13)gf ull (cid:13)\n\n(cid:17)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:18)(cid:90) 1\n\n∇2h(x0 + t(xk − x0)) − H\n\n(cid:19)\n\n(xk − x0)dt\n\n(cid:19)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:18)(cid:13) (cid:13) (cid:13) (cid:13) (cid:16)\n\nE\n\n= E\n\n≤\n\nρ 2\n\n0\n\n∥xk − x0∥2(cid:17)\n\n≤\n\n2ρR2 ε\n\n.\n\n(23)\n\n(24)\n\nIterations in Eq.(23) can be viewed as applying the proposed optimizer to the quadratic approximation h(x) with the gradient error δk, which is in the order of O(cid:0)ρR2/ε(cid:1). Lemma 4. Suppose that Assumptions 1-3 hold. Let B = O(cid:0)ε0.5(cid:1), β = O(cid:0)ε2(cid:1), η = O(cid:0)ε1.5(cid:1), K ≤ K = O(cid:0)ε−2(cid:1). Then we have:\n\nwhere ̄x := 1\n\nK0−1\n\n(cid:80)K0\n\nk=1 xk.\n\nE (∥∇f ( ̄x)∥) = O(ε),\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Since h(·) is quadratic, then we have:\n\nE (∥∇h( ̄x)∥) = E\n\n(cid:32)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 K0 − 1\n\nK0(cid:88)\n\nk=1\n\n∇h(xk)\n\n(cid:33)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nK0(cid:88)\n\n(βηk)−1 ◦ (yk+1 − xk) + ξk + δk\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=\n\n1 K0 − 1\n\nE\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\n1 (K0 − 1)β\n\n(a) ≤\n\n1 (K0 − 1)β\n\n=\n\n1 (K0 − 1)β\n\nK0(cid:88)\n\nk=1\n\nK0(cid:88)\n\nk=1\n\nK0(cid:88)\n\nE\n\nk=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nE\n\nE\n\nk=1 2ρR2 ε\nK0(cid:88)\n\n+\n\n√\n\nσ K0 − 1\n\n(b) ≤\n\n1 (K0 − 1)β\n\n(c) ≤\n\n1 (K0 − 1)β\n\nE\n\nE\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nk=1\n\nK0(cid:88)\n\nk=1\n\nK0(cid:88)\n\nk=1\n\nE\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) K0(cid:88)\n\n√\n\n(\n\nk=1 4R\n\n√\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n(d) ≤\n\n≤\n\n≤\n\n1 (K0 − 1) √\n\n2c∞ ηK\n\nE\n\n√\n\n2c∞R ηK (cid:18) R ηK\n\n=O\n\n(βηk)−1 ◦ (yk+1 − xk)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n1 (K0 − 1)\n\nE\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nK0(cid:88)\n\nξk\n\nk=1\n\n(ηk)−1 ◦ (yk+1 − xk)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n√\n\nσ K0 − 1\n\n+\n\n2ρR2 ε\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n1 (K0 − 1)\n\nE\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nK0(cid:88)\n\nδk\n\nk=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nyk+1 − yk − (1 − β)(yk − yk−1) ηk\n\nyk+1 − yk − (1 − β)(yk − yk−1) ηk\n\n(cid:18) yk+1 − yk ηk\n\n−\n\n(1 − β)(yk − yk−1) ηk−1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 (K0 − 1)\n\nK0−1 (cid:88)\n\nE\n\nk=1\n\n≤\n\n1 (K0 − 1)β\n\nE\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nyK0 − yK0−1 ηK0\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n− (1 − β)\n\nηk−1 − ηk−2 ηk−2ηk\n\n(xk−1 − yk−1)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n2βc1.5 ∞ R ηε\n\n+\n\n√\n\nσ K0 − 1\n\n+\n\n2ρR2 ε\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n4βc1.5 ∞ R ηε\n\n+\n\n√\n\nσ K0 − 1\n\n+\n\n2ρR2 ε\n\nyk+1 − yk ηk\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n4βc1.5 ∞ R ηε\n\n+\n\n√\n\nσ K0 − 1\n\n+\n\n2ρR2 ε\n\nyk+1 − yk ηk\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n√\n\n4R\n\nc∞ βηK 2 +\n\n4βc1.5 ∞ R ηε\n\n+\n\n√\n\nσ K0 − 1\n\n+\n\n2ρR2 ε\n\nnk + ε)1/2 ◦ (yk+1 − yk)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n√\n\n4R\n\nc∞ βηK 2 +\n\n4βc1.5 ∞ B ηε\n\n+\n\n√\n\nσ K0 − 1\n\n+\n\n2ρR2 ε\n\nc∞ βηK 2 +\n\n4βc1.5 ∞ R ηε\n\n+\n\n√\n\nσ K0 − 1\n\n+\n\n2ρR2 ε\n\n+\n\nβR η\n\n+\n\n1 √\n\nK\n\n(cid:19)\n\n+ R2\n\n= O(ε),\n\nwhere (a) is due to the independence of ξk’s and Eq.(24), (b) comes from Propositions 1 and 2:\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nηk−1 − ηk−2 ηk−2ηk\n\n(xk−1 − yk−1)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\nη(cid:0)√\n\n(cid:0)√\n\nnk + ε(cid:1)1/2\n\n√\n\nη\n\n2β2c∞ ε\n\n(cid:32)\n\n√\n\n1 −\n\n2β2c∞ ε\n\n≤\n\n√\n\nnk + ε nk−1 + ε(cid:1)1/2 (cid:33)−1/2\n\nR\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nηk−1 − ηk−2 ηk−2\n\n(cid:13) (cid:13) (cid:13) (cid:13)∞\n\n∥ˆnk−1 ◦ (xk−1 − yk−1)∥\n\n√\n\n≤\n\n(c∞ + ε)1/2 η\n\n2β2c∞ ε\n\nR (1 − β)1/4\n\n≤\n\n(cid:18) 1\n\n(cid:19)1/4 2β2c1.5\n\n∞ R\n\n1 − β\n\nηε\n\n,\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nwe use the following bounds in (c):\n\n(yk − yk−1) ηk−1\n\n−\n\n(yk − yk−1) ηk\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n=\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nηk − ηk−1 ηk−1ηk\n\n(yk − yk−1)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:0)√\n\nnk−1 + ε(cid:1)1/2\n\nη\n\nηk − ηk−1 ηk\n\n√\n\n(cid:13) (cid:13) (cid:13) (cid:13)∞\n\n(cid:13) (cid:13) (cid:13)(\n\n(cid:13) (cid:13) (cid:13) (cid:13) √\n\n(cid:0)√\n\nnk−1 + ε(cid:1)1/2\n\nη\n\n2β2c∞ ε\n\nR k\n\n≤\n\n≤\n\n≤\n\nnk−1 + ε)1/2 ◦ (yk − yk−1)\n\n(cid:13) (cid:13) (cid:13)\n\n(c∞ + ε)1/2 η\nnk + ε(cid:1)1/2\n\n(cid:0)√\n\n(cid:13) (cid:13) (cid:13)\n\n√\n\n2β2c∞ ε\n\nR k\n\n≤\n\n∞ R\n\n2β2c1.5 ηεk\n\n,\n\n◦ (yk+1 − yk)\n\n(cid:13) (cid:13) (cid:13) and restart condition:\n\n(d) is implied by K0 = argmin⌊ K (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nyK0 − yK0−1 ηK0\n\nnK0 + ε η2\n\n√\n\n≤\n\n2 ⌋≤k≤K−1\n\n(cid:0)√\n\n(cid:13) (cid:13) (cid:13)\n\nnK0 + ε(cid:1)1/2\n\n◦ (yK0 − yK0−1)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:0)√\n\n(cid:13) (cid:13) (cid:13)\n\nnK0 + ε(cid:1)1/2\n\n◦ (yK0 − yK0−1)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤\n\n1 K − ⌊K/2⌋\n\nK−1 (cid:88)\n\n√\n\n(cid:13) (cid:13) (cid:13)(\n\nk=⌊K/2⌋\n\nnk + ε)1/2 ◦ (yk+1 − yk)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤\n\n1 K − ⌊K/2⌋\n\nK (cid:88)\n\nk=1\n\n√\n\n(cid:13) (cid:13) (cid:13)(\n\nFinally, we have:\n\nnk + ε)1/2 ◦ (yk+1 − yk)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤\n\n1 K − ⌊K/2⌋\n\nR2 K\n\n≤\n\n2R2 K 2 .\n\nE (∥∇f ( ̄x)∥) = E (∥∇h( ̄x)∥) + E (∥∇f ( ̄x) − ∇h( ̄x)∥) = O(ε) +\n\n2ρR2 ε\n\n= O(ε),\n\nwhere we use the results from Eq.(24), namely:\n\nE (∥∇f ( ̄x) − ∇h( ̄x)∥) = E\n\nand we also note that, by Eq.(22):\n\n(cid:16)(cid:13) (cid:13)∇f ( ̄x) − gf ull (cid:13)\n\n0 − H( ̄x − x0)\n\n(cid:17)\n\n(cid:13) (cid:13) (cid:13)\n\n≤\n\nρ 2\n\n(cid:16)\n\n∥ ̄x − x0∥2(cid:17)\n\n,\n\nE\n\nE ∥ ̄x − x0∥ ≤\n\n1 K0 − 1\n\nK0(cid:88)\n\nk=1\n\nE ∥xk − x0∥ ≤\n\n2R ε1/2\n\n.\n\nF.5 PROOF FOR MAIN THEOREM\n\nTheorem 2. Suppose that Assumptions 1-3 hold. Let B = O(cid:0)ε0.5(cid:1), β = O(cid:0)ε2(cid:1), η = O(cid:0)ε1.5(cid:1), K ≤ K = O(cid:0)ε−2(cid:1). Then Algorithm 1 find an ε-approximate first-order stationary point within at most O(cid:0)ε−3.5(cid:1) iterations. Namely, we have:\n\nE (f (yK) − f (x0)) = −O(cid:0)ε1.5(cid:1), E (∥∇f ( ̄x)∥) = O(ε).\n\nProof. Note that at the beginning of each restart cycle in Algorithm 2, we set x0 to be the last iterate xK in the previous restart cycle. Due to Lemma 3, we already have: E (f (yK) − f (x0)) = −O(cid:0)ε1.5(cid:1).\n\nSumming this inequality over all cycles, say N total restart cycles, we have:\n\nf (x) − f (xinit) = −O(cid:0)N ε1.5(cid:1),\n\nmin x\n\nHence, the Algorithm 2 terminates within at most O(cid:0)ε−1.5∆f (cid:1) restart cycles, where ∆f := f (xinit)− minx f (x). Note that each cycle contain at most K = O(cid:0)ε−2(cid:1) iteration step, therefore, the total iteration number must be less than O(cid:0)ε−3.5∆f\n\n(cid:1).\n\nOn the other hand, by Lemma 4, in the last restart cycle, we have:\n\nE (∥∇f ( ̄x)∥) = O(ε).\n\nNow, we obtain the final conclusion for the theorem.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nG AUXILIARY LEMMAS\n\nProposition 2. If Assumption 2 holds. We have: ∥mk∥∞ ≤ c∞,\n\n∥nk∥∞ ≤ c2\n\n∞.\n\nProof. By the definition of mk, we can have that: k\n(cid:88)\n\nmk =\n\nck,tgt,\n\nt=0\n\nwhere\n\nSimilar, we also have:\n\nwhere\n\nIf is obvious that:\n\nhence, we get:\n\nck,t =\n\n \n\nβ1(1 − β1)(k−t) when t > 0,\n\n\n\n(1 − β1)k\n\nwhen t = 0.\n\nnk =\n\nk (cid:88)\n\nt=0\n\nk,t(gt + (1 − β2)(gt − gt−1))2, c′\n\nc′ k,t =\n\n \n\nβ3(1 − β3)(k−t) when t > 0,\n\n\n\n(1 − β3)k\n\nwhen t = 0.\n\nk (cid:88)\n\nt=0\n\nck,t = 1,\n\nk (cid:88)\n\nt=0\n\nc′ k,t = 1,\n\n∥mk∥∞ ≤\n\nk (cid:88)\n\nt=0\n\nck,t∥gt∥∞,\n\n∥nk∥∞ ≤\n\nk (cid:88)\n\nt=0\n\nk,t∥gt + (1 − β2)(gt − gt−1)∥2 c′\n\n∞ ≤ c2\n\n∞.\n\nProposition 3. If Assumption 2 holds, we have: (cid:13) (cid:13) (cid:13) (cid:13)∞\n\nηk − ηk−1 ηk−1\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n√\n\n≤\n\n2β3c∞ ε\n\n.\n\nProof. Give any index i ∈ [d] and the definitions of ηk, we have: (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:18) √ nk−1 + ε √\nnk + ε\n\n(cid:18) ηk − ηk−1 ηk−1\n\n− 1\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n=\n\n=\n\n(cid:19)\n\n(cid:19)\n\n(cid:18) √\n\ni\n\ni\n\n√\n\n(cid:19)\n\nnk\n\nnk−1 − √\n\nnk + ε\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\ni\n\nNote that, by the definition of nk, we have: (cid:18) √\n\n√\n\n(cid:19)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nnk\n\nnk−1 − √\n\nnk + ε\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ni\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:32) (cid:112)|nk−1 − nk|\n\n√\n\nnk + ε\n\n(cid:33)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ni\n\n(cid:114)(cid:12) (cid:12)\n\n(cid:12)nk−1 − (gk + (1 − β2)(gk − gk−1))2(cid:12)\n\n(cid:12) (cid:12)\n\n√\n\nnk + ε\n\n√\n\n≤\n\n2β3c∞ ε\n\n,\n\n\n\n \n\n\ni\n\n\n\n \n\n\n=β3\n\nhence, we have:\n\nWe finish the proof.\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:18) ηk − ηk−1 ηk−1\n\n(cid:19)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ni\n\n(cid:34)\n\n√\n\n∈\n\n0,\n\n(cid:35) .\n\n2β3c∞ ε\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 5. Consider a moving average sequence:\n\nwhere we note that:\n\nmk = (1 − β)mk−1 + βgk,\n\ngk = Eζ[∇f (θk, ζ)] + ξk,\n\nand we denote gf ull 2(cid:19)\n\nk\n\n(cid:13)mk − gf ull\n\nk\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:18)(cid:13) (cid:13)\n\nE\n\n:= Eζ[∇f (θk, ζ)] for convenience. Then we have: (1 − β)2L2 β\n\n(cid:13)mk−1 − gf ull\n\n≤ (1 − β)E\n\n(cid:18)(cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\nk−1\n\n+\n\n(cid:16)\n\n∥θk−1 − θk∥2(cid:17)\n\n+ β2σ2.\n\nE\n\nProof. Note that, we have:\n\nmk − gf ull\n\nk =(1 − β)\n\n(cid:16)\n\nmk−1 − gf ull\n\nk−1\n\n(cid:16)\n\n=(1 − β)\n\nmk−1 − gf ull\n\nk−1\n\nThen, take expectation on both sides:\n\n(cid:17)\n\n(cid:17)\n\n(cid:18)(cid:13) (cid:13)\n\nE\n\n(cid:13)mk − gf ull\n\nk\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ (1 − β)gf ull\n\nk−1 − gf ull (cid:16)\n\nk + βgk (cid:17)\n\nk−1 − gf ull gf ull\n\nk\n\n+ β\n\n+ (1 − β)\n\n(cid:16)\n\ngk − gf ull\n\nk\n\n(cid:17)\n\n.\n\n=(1 − β)2E\n\n(cid:13)mk−1 − gf ull\n\nk−1\n\n(cid:18)(cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ (1 − β)2E\n\n(cid:13)gf ull\n\nk−1 − gf ull\n\nk\n\n(cid:18)(cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ β2σ2+\n\n2(1 − β)2E\n\n(cid:16)(cid:68)\n\n(cid:16)\n\n≤\n\n(1 − β)2 + (1 − β)2a\n\nk−1, gf ull mk−1 − gf ull (cid:18)(cid:13) (cid:13)\n\nE\n\n(cid:17)\n\nk−1 − gf ull\n\nk\n\n(cid:69)(cid:17)\n\n(cid:13)mk−1 − gf ull\n\nk−1\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:18)\n\n1 +\n\n(cid:19)\n\n1 a\n\n(1 − β)2E\n\n(cid:13)gf ull\n\nk−1 − gf ull\n\nk\n\n(cid:18)(cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ β2σ2\n\n(a)\n\n≤ (1 − β)E\n\n(cid:13)mk−1 − gf ull\n\nk−1\n\n≤(1 − β)E\n\n(cid:13)mk−1 − gf ull\n\nk−1\n\n(cid:18)(cid:13) (cid:13)\n\n(cid:18)(cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+\n\n+\n\n(1 − β)2 β\n\n(cid:18)(cid:13) (cid:13)\n\nE\n\n(cid:13)gf ull\n\nk−1 − gf ull\n\nk\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ β2σ2\n\n(1 − β)2L2 β\n\n(cid:16)\n\nE\n\n∥θk−1 − θk∥2(cid:17)\n\n+ β2σ2,\n\nwhere for (a), we set a = β\n\n1−β .\n\nLemma 6. Consider a moving average sequence:\n\nwhere we note that:\n\nvk = (1 − β)vk−1 + β(gk − gk−1),\n\ngk = Eζ[∇f (θk, ζ)] + ξk,\n\nand we denote gf ull\n\nk\n\n:= Eζ[f (θk, ζ)] for convenience. Then we have:\n\n(cid:16)\n\n∥vk∥2(cid:17)\n\nE\n\n≤ (1 − β)E\n\n(cid:16)\n\n∥vk−1∥2(cid:17)\n\nProof. Take expectation on both sides:\n\n(cid:18)(cid:13) (cid:13)\n\n+ 2βE\n\n(cid:13)gf ull\n\nk − gf ull\n\nk−1\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ 3β2σ2.\n\n(cid:16)\n\nE\n\n∥vk∥2(cid:17)\n\n(a)\n\n= (1 − β)2E\n\n(b)\n\n≤(1 − β)2E\n\n(c)\n\n≤ (1 − β)2E\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\n∥vk−1∥2(cid:17)\n\n= (1 − β)2E ∥vk−1∥2(cid:17)\n\n(cid:16)\n\n+ β2E\n\n+ β2E ∥gk − gk−1∥2(cid:17) (cid:18)(cid:13) (cid:13)\n\nk − gf ull\n\n(cid:13)gf ull\n\nk−1\n\n+ 2β2E\n\n∥vk−1∥2(cid:17)\n\n∥vk−1∥2(cid:17)\n\n+ 2β2E\n\n(cid:18)(cid:13) (cid:13)\n\n(cid:13)gf ull\n\nk − gf ull\n\nk−1\n\n(cid:16)\n\n∥gk − gk−1∥2(cid:17)\n\n+ 2β(1 − β)E(⟨vk−1, gk − gk−1⟩)\n\n+ 2β(1 − β)E\n\n(cid:16)(cid:68)\n\nvk−1, gf ull\n\nk − gk−1\n\n(cid:69)(cid:17)\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n+ 2β(1 − β)E\n\n(cid:16)(cid:68)\n\nvk−1, gf ull\n\nk − gk−1\n\n+ 2β(1 − β)E\n\n(cid:16)(cid:68)\n\nvk−1, gf ull\n\nk − gf ull\n\nk−1\n\n(cid:69)(cid:17)\n\n(cid:69)(cid:17)\n\n+ 3β2σ2\n\n+ 3β2σ2\n\n(d)\n\n≤ (1 − β)E\n\n(cid:16)\n\n∥vk−1∥2(cid:17)\n\n+ 2βE\n\n(cid:18)(cid:13) (cid:13)\n\n(cid:13)gf ull\n\nk − gf ull\n\nk−1\n\n+ 3β2σ2,\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nwhere for (a), we utilize the independence between gk and vk−1, while for (b): (cid:18)(cid:13) (cid:13)\n\n∥gk − gk−1∥2(cid:17)\n\n+ 2E\n\n+ 2E\n\n(cid:18)(cid:13) (cid:13)\n\n(cid:18)(cid:13) (cid:13)\n\n≤ E\n\n2(cid:19)\n\n2(cid:19)\n\nk−1 − gk−1\n\n(cid:13)gk − gf ull\n\n(cid:13)gf ull\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)\n\nE\n\n(cid:16)\n\nk\n\n(cid:13)gf ull\n\nk − gf ull\n\nk−1\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n,\n\nfor (c), we know:\n\n(cid:16)(cid:68)\n\nE\n\n(cid:16)(cid:68)\n\n=E\n\nvk−1, gf ull\n\nk−1 − gk−1\n\n(cid:69)(cid:17)\n\n(cid:16)(cid:68)\n\n= E\n\n(1 − β)vk−2 + β(gk−1 − gk−2), gf ull\n\nk−1 − gk−1\n\n(cid:69)(cid:17)\n\n(1 − β)vk−2 − βgk−2, gf ull\n\nk−1 − gk−1\n\n(cid:69)(cid:17)\n\n(cid:16)(cid:68)\n\n+ βE\n\ngk−1 − gf ull\n\nk−1 + gf ull\n\nk−1, gf ull\n\nk−1 − gk−1\n\n(cid:69)(cid:17)\n\n= − βE\n\n(cid:16)(cid:13) (cid:13)gf ull (cid:13)\n\nk−1 − gk−1\n\n2(cid:17)\n\n(cid:13) (cid:13) (cid:13)\n\n,\n\n(cid:16)(cid:68)\n\nand thus E nally, for (d), we use:\n\nvk−1, gf ull\n\nk − gk−1\n\n(cid:69)(cid:17)\n\n(cid:16)(cid:68)\n\n= E\n\nvk−1, gf ull\n\nk − gf ull\n\nk−1\n\n(cid:69)(cid:17)\n\n− βE\n\n(cid:16)(cid:13) (cid:13)gf ull (cid:13)\n\nk−1 − gk−1\n\n2(cid:17)\n\n(cid:13) (cid:13) (cid:13)\n\n. Fi-\n\n(cid:18)(cid:13) (cid:13)\n\n+ E\n\n(cid:13)gf ull\n\nk − gf ull\n\nk−1\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n.\n\n(cid:16)(cid:68)\n\n2E\n\nvk−1, gf ull\n\nk − gf ull\n\nk−1\n\n(cid:69)(cid:17)\n\n(cid:16)\n\n≤ E\n\n∥vk−1∥2(cid:17)\n\n32",
    "reference": "# Summary Of The Paper\n\nThe paper introduce a new adaptive method that built upon ADAM and Nesterov momentum. First, it reformulate Nesterov momentum update and combine it with ADAM; Second, it provides $O(\\epsilon^{-4})$ complexity for Lipschitz-smooth case; Third, it provides $O(\\epsilon^{-3.5})$ complexity for Hessian Lipschitz case.\n\n# Strength And Weaknesses\n\n### Strength\n1. The idea is clear and straightforward\n2. The experiments are conducted for multiple tasks\n\n### Weaknesses\n1. The claim that $O(\\epsilon^{-3.5})$ is the lower bound is not correct. In Assumption 1, it assumes the function is smooth with respect to each value of $\\zeta$, which will already lead to a lower bound of $O(\\epsilon^{-3})$ even without Lipschitz Hessian. I also found that Assumption 2 is stronger than usual, as it assume $\\mathbb{E}||\\xi|| \\leq \\sigma$ rather than $\\mathbb{E}||\\xi||^2 \\leq \\sigma^2$. So I think lower bound could be much better than $O(\\epsilon^{-4})$ and $O(\\epsilon^{-3.5})$ for each case. \n2. From the change in equation (4), the algorithm introduce a new hyperparameter $\\beta_2$. In the new algorithm, there are three momentum hyperparameters to pick: $\\beta_1, \\beta_2$ and $\\beta_3$. When $\\beta_2 = 0$, the algorithm basically reduces to the vanilla Adam. In the experiment, it performs much better than Adam, so I am not sure whether it is simply a result of tuning $\\beta_2$. However, the ablation study in B.5 indicate that Adan is not sensitive to the value of $\\beta_2$, which contradicts that it is much better than Adam.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: I think the paper is easy to read. I also suggest to add reference for Table 1, Nesterov acceleration in the last paragraph of page 2 and lower bound $\\epsilon^{-4}$ in the first paragraph of page 3. \n\nNovelty: The algorithm seems new to me and the construction is interesting.\n\n# Summary Of The Review\n\nOverall, the algorithm looks promising, but does not match the lower bounds in theory and I am not fully convinced by the experiments.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMBRAIN: A MULTI-CHANNEL SELF-SUPERVISED LEARNING FRAMEWORK FOR BRAIN SIGNALS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nBrain signals are important quantitative data for understanding physiological activities and diseases of human brain. Meanwhile, rapidly developing deep learning methods offer a wide range of opportunities for better modeling brain signals, which has attracted considerable research efforts recently. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, in this paper, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifically, we capture the temporal correlation by designing the delayed-time-shift prediction task; we represent the spatial correlation by a graph structure, which is built with proposed multi-channel CPC whose goal is to maximize the mutual information of each channel and its correlated ones. We further theoretically prove that our design can lead to a better predictive representation and propose the instantaneou-time-shift prediction task based on it. Finally, replace-discriminative-learning task is designed to preserve the characteristics of each channel. Extensive experiments of seizure detection on both EEG and SEEG large-scale real-world datasets demonstrate our model outperforms several state-of-the-art time series SSL and unsupervised models.\n\n1\n\nINTRODUCTION\n\nBrain signals are foundational quantitative data for the study of human brain in the field of neuroscience. The patterns of brain signals can greatly help us to understand the normal physiological function of the brain and the mechanism of related diseases. There are many applications of brain signals, such as cognitive research (Ismail & Karwowski, 2020; Kuanar et al., 2018), emotion recognition (Song et al., 2020; Chen et al., 2019), neurological disorders (Alturki et al., 2020; Yuan et al., 2019) and so on. Brain signals can be measured by noninvasive or invasive methods (Paluszek et al., 2015). The noninvasive methods, like electroencephalography (EEG), cannot simultaneously consider temporal and spatial resolution along with the deep brain information, but they are easier to implement without any surgery. As for invasive methods like stereoelectroencephalography (SEEG), they require extra surgeries to insert the recording devices, but have access to more precise and higher signal-to-noise data. For both EEG and SEEG data, there are multiple electrodes with contacts (also called channels) that are sampled at a fixed frequency to record brain signals.\n\nRecently, discoveries in the field of neuroscience have inspired advances of deep learning techniques, which in turn promotes neuroscience research. According to the literature, most deep learning-based studies of brain signals focus on supervised learning (Shoeibi et al., 2021; Rasheed et al., 2020; Zhang et al., 2021; Craik et al., 2019), which relies on a large number of clinical labels. However, obtaining accurate and reliable clinical labels requires a high cost. In the meantime, the emergence of self-supervised learning (SSL) and its great success (Chen & He, 2021; Grill et al., 2020; He et al., 2020; Brown et al., 2020; Devlin et al., 2018; Raffel et al., 2020; Van den Oord\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\net al., 2018) makes it a predominant learning paradigm in the absence of labels. Therefore, some recent studies have introduced the means of SSL to extract the representations of brain signal data. For example, Banville et al. (2021) directly applies general SSL tasks to pre-train EEG data, including relative position prediction (Doersch et al., 2015), temporal shuffling (Misra et al., 2016) and contrastive predictive coding (Van den Oord et al., 2018). Mohsenvand et al. (2020) designs data augmentation methods, and extends the self-supervised model SimCLR (Chen et al., 2020) in computer vision to EEG data. In contrast to the numerous works investigating EEG, few studies focus on SEEG data. Martini et al. (2021) proposes a self-supervised learning model for real-time epilepsy monitoring in multimodal scenarios with SEEG data and video recordings.\n\nDespite the advances on representation learning of brain signals, two main issues remain to be overcome. Firstly, almost all existing methods are designed for a particular type of brain signal data, and there is a lack of a unified method for handling both EEG and SEEG data. The challenge mainly lies in the different clinical patterns of brain signals that need to be measured in different ways. On the one hand, EEG collects noisy and rough brain signals on the scalp; differently, SEEG collects deeper signals with more stereo spatial information, which indicates more significant differences of different brain areas (Perucca et al., 2014). On the other hand, in contrast to EEG with a goldstandard collection location, the monitoring areas of SEEG vary greatly between patients, leading to different number and position of channels. Therefore, how to find the commonalities of EEG and SEEG data to design a unified framework is challenging.\n\nAnother issue is about the gap between existing methods and the real-world applications. In clinical scenarios, doctors typically locate brain lesions by analyzing signal patterns of each channel and their holistic correlations. A straight-forward way for this goal is to model each of the channels separately by single-channel time series models, which, however, cannot exploit correlations between brain areas (Davis et al., 2020; Lynn & Bassett, 2019). As for the existing multivariable time series models, most of them can only capture implicit correlation patterns (Zerveas et al., 2021; Chen & Shi, 2021), whereas explicit correlations are required by doctors for identifying lesions. Moreover, although some graph-based methods have been proposed to explicitly learn correlations, they focus on giving an overall prediction for all channels at a time but overlook the prediction on one specific channel (Zhang et al., 2022; Shang et al., 2021). Therefore, how to explicitly capture the spatial and temporal correlations while giving channel-wise prediction is another issue to be overcome.\n\nIn this paper, our main contribution is to propose a multi-channel self-supervised learning framework MBrain, which can be generally applied for learning representations of both EEG and SEEG data. In addition, we pay special attention to its application in seizure detection. Based on domain knowledge and data observations, we propose to learn the correlation graph between channels as the common cornerstone for both two types of brain signals. In particular, we employ Contrastive Predictive Coding (CPC) (Van den Oord et al., 2018) as the backbone model of our framework by extending it for handling multi-channel data with theoretically guaranteed effectiveness. Based on the multichannel CPC, we propose the instantaneous time shift task to learn the spatial correlations between channels, and the delayed time shift task and the replace discriminative task are designed to capture the temporal correlation patterns and to preserve the characteristics of each channel respectively. Extensive experiments show that MBrain outperforms several state-of-the-art baselines on largescale real-world EEG and SEEG datasets for the seizure detection task.\n\n2 PRELIMINARY: THEORETICAL ANALYSIS OF MULTI-CHANNEL CPC\n\nWe employ Contrastive Predictive Coding (CPC) (Van den Oord et al., 2018) as the basis of our framework. The pretext task of CPC is to predict low-level local representations by high-level global contextual representations ct at the t-th time step. Theoretically, the optimal InfoNCE loss proposed by CPC with N − 1 negative samples Lopt N is a lower bound of the mutual information between contextual semantic distribution p(ct) and raw data distribution p(xt+k), i.e., Lopt N ≥ −I(xt+k; ct) + log N , where k is the prediction step size. CPC is originally designed for single-channel sequence data only, and there are two natural ways to extend single channel CPC to multi-channel version. The first one is to use CNNs with multiple kernels to encode all channels simultaneously, which cannot offer explicit correlation patterns for doctors to identify lesions. The second one is to train a shared CPC regarding all channels as one, which has no ability to capture the correlation patterns. Taking a comprehensive consideration, we propose multi-channel CPC in this paper. Our motivation is to explicitly aggregate the semantic information of multiple channels\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nto predict the local representations of one channel. Formally, we propose the following proposition as our basic starting point. Proposition 1. Introducing the contextual information of the correlated channels increases the amount of mutual information with the raw data of the target channel. t }j̸=i)) ≥ I(xi t+k; Φ(ct)) = I(xi\n\n(1) where i and j are indexes of the channels. Φ(·) represents some kinds of aggregate function, which has no additional formal constraints other than the need to retain information of the target channel.\n\nt, Φ({cj\n\nt+k; ci\n\nt+k; ci\n\nI(xi\n\nt),\n\nProof. We use the linear operation of mutual information to obtain: I(xi I(xi t }j̸=i)|ci information, we complete the proof.\n\nt }j̸=i)) = t). According to the non-negativity of the conditional mutual\n\nt+k; Φ({cj\n\nt) + I(xi\n\nt+k; ci\n\nt+k; ci\n\nt, Φ({cj\n\nIt seems natural that the predictive ability of multiple channels is stronger than that of a single channel, which is also consistent with the assumption of Granger causality (Granger, 1969) to some extent. Therefore, we choose to approximate the more informative I(xi t+k; Φ(ct)) to obtain more expressive representations. Specifically, followed by InfoNCE, we define our loss function LN as\n\nLN = −\n\n(cid:88)\n\ni\n\n(cid:34)\n\nEX i\n\nlog\n\nfk(xt+k, Φ(ct)) xj ∈X fk(xj, Φ(ct))\n\n(cid:80)\n\n(cid:35)\n\n,\n\n(2)\n\nwhere X i denotes the data sample set consisting of one positive sample and N − 1 negative samples of the i-th channel. We then establish the relationship between LN and I(xi Theorem 1. Given a sample set for each channel X i = {xi one positive sample from p(xi is the number of channels. The optimal Lopt\n\nN }, i = 1, . . . , n consisting of t+k)/n, where n\n\nt+k|Φ(ct)) and N − 1 negative samples from (cid:80)\n\nt+k; Φ(ct)).\n\nj p(xj t+k; Φ(ct)):\n\n1, . . . , xi\n\ni I(xi\n\nN is the lower bound of (cid:80) t+k; Φ(ct)) + log N (cid:3) . (cid:2)−I(xi\n\nLopt\n\nN ≥\n\n(cid:88)\n\ni\n\n(3)\n\nProof. The optimal fk(xt+k, Φ(ct)) is proportional to p(xi t+k)/n), which is the same as single-channel CPC. And we can directly replace the data distributions in the proof of single-channel CPC (see details in Appendix B) to obtain the inequality below: (cid:35) (cid:80)\n\n(cid:80)\n\nt+k|Φ(ct))/((cid:80)\n\nj p(xj\n\n(cid:34)\n\n(cid:35)\n\n(cid:35)\n\nLopt\n\nN ≥\n\n(cid:88)\n\ni\n\nEX i log\n\n(cid:34) 1 n\np(xi\n\nj p(xj t+k) t+k|Φ(ct))\n\n+ log N\n\n= EX 1,X 2,...,X n log\n\n(cid:34) [ 1\n\nn\n\nj p(xj t+k)]n t+k|Φ(ct))\n\nΠjp(xj\n\n+n log N.\n\nAccording to the Jensen Inequality, we obtain that ((cid:80) exponentiating the two equations, we have\n\nj log p(xj\n\nt+k))/n ≤ log ((cid:80)\n\nj p(xj\n\n(4) t+k)/n). By\n\nΠjp(xj\n\nt+k) ≤ [\n\n1 n\n\n(cid:88)\n\nj\n\np(xj\n\nt+k)]n.\n\n(5)\n\nWith the help of equation 5, we can further obtain the lower bound of equation 4:\n\nLopt\n\nN ≥ EX 1,X 2,...,X n log\n\n(cid:34)\n\n(cid:35)\n\nΠjp(xj\n\nt+k) t+k|Φ(ct))\n\nΠjp(xj\n\n+ n log N =\n\n(cid:88)\n\ni\n\n(cid:2)−I(xi\n\nt+k; Φ(ct)) + log N (cid:3) . (6)\n\nThen we complete the proof.\n\nt+k; Φ(ct)), if the optimal loss function for each channel has log N gap with I(xi\n\nWe next analyze the advantages of multi-channel CPC over single-channel CPC. Our loss function LN leads to a better predictive representation because we approximate a more informative objective I(xi t+k; Φ(ct)), which is the same in single-channel CPC. Moreover, with the same GPU memory, the more channels, the smaller the batch size that can be accommodated. But we can randomly sample negative samples across all channels, which increases the diversity of negative samples. However, in order to narrow the approximation gap, equation 5 should be considered. The equality sign in this inequality holds if and only if samples from each channel follows the same distribution. In fact, for many largescale multi-channel time series data (e.g., brain signal data used in this paper), by normalizing each channel, they all exhibit close normal distributions leading to small gaps in equation 5.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 PROPOSED METHOD\n\nIn this section, we introduce the details of our proposed self-supervised learning framework MBrain. For the commonality between EEG and SEEG, we are inspired by the synergistic effect of brain that is, different connectivity patterns correspond to different brain function and nerve cells, states (Lynn & Bassett, 2019). In particular, for brain signals, nerve cells will spontaneously generate traveling waves and spread them out (Davis et al., 2020), maintaining some characteristics such as shape during the process. Therefore, the degree of channel similarity implies different propagation patterns of traveling waves, reflecting the differences in connectivity patterns to some extent. Both EEG and SEEG brain signals follow the inherent physiological mechanism. Therefore, we propose to extract the correlation graph structure between channels (brain areas) as the cornerstone of unifying EEG and SEEG data (Section 3.1). Next, we introduce three self-supervised learning tasks to model brain signals in Section 3.2. We propose instantaneous time shift task based on multichannel CPC and delayed time shift task to capture the spatial and temporal correlation patterns. Then Replace discriminative task is further designed to preserve characteristics of each channel.\n\nNotations. For both EEG and SEEG data, there are multiple electrodes with C channels. We use X = {xl ∈ RC}L l=1 to represent raw time series data with L time points. i and j denote the index of channels. Yl,i ∈ {0, 1} is the label for the l-th time point and i-th channel. We use a W-length window with no overlap to obtain the time segments S = {st}|S| t=1 (see details in Appendix A). The label corresponding to the t-th time segment and the i-th channel is denoted as Y s t,i.\n\n3.1 LEARNING CORRELATIONS BETWEEN CHANNELS\n\nAs mentioned above, the correlation patterns between different brain areas can help us to distinguish brain activities in downstream tasks to a large extent. Taking the seizure detection task as an example, when seizures occur, more rapid and significant propagation of spike-and-wave discharges will appear (Proix et al., 2018), which greatly enhances the correlation between channels. This phenomenon is also verified by data observations in Appendix C, which supports us to treat correlation graph structure learning as the common cornerstone of our framework. However, correlations between brain regions are difficult to be observed and recorded directly. Therefore, for each time step t, our goal is to learn the structure of the correlation graph, whose adjacency matrix is At, where nodes in the graph indicate channels and weighted edges denote the correlations between channels.\n\nConsidering that the brain is in normal and stable state most of the time, we first define the coarsegrained correlation graph as the prior graph for a particular individual as\n\nAcoarse(i, j) = Est[Cosine(st,i, st,j)], where the expectation operation averages over all the correlation matrices computed in only one time segment st, and Cosine(·, ·) denotes the cosine similarity function.\n\n(7)\n\nNext, based on Acoarse, for each pair of channels, we further model their fine-grained short-term correlation within each time segment. We assume that the fine-grained correlations follow a Gaussian distribution element-wise, whose location parameters are elements of Acoarse and scale parameters will be learned from the data. By means of the reparameterization trick, the short-term correlation matrix of the t-th time segment is sampled from the learned Gaussian distribution:\n\nσt(i, j) = SoftPlus(MLP(cself nt(i, j) ∼ N (0, 1),\n\nt,τ,i, cself\n\nt,τ,j)),\n\nAfine t\n\n(i, j) = Acoarse(i, j) + σt(i, j) × nt(i, j).\n\n(8)\n\n(9)\n\n(10)\n\nSoftPlus(·) is a commonly used activation function to ensure the learned standard deviation is positive. cself t,τ is the contextual representation of raw time segments extracted by encoders (see details in Section 3.2). To remove the spurious correlations caused by low frequency signals and enhance the sparsity, which is a common assumption in neuroscience (Yu et al., 2017), we filter the edges by a threshold-based function to obtain the final correlation graph structure At: (cid:40)\n\nAt(i, j) =\n\n(11)\n\nAfine t\nAfine t\n\n(i, j) ≥ θ1,\n\n(i, j) < θ1.\n\nAfine t\n\n(i, j),\n\n0,\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Overview of MBrain. The leftmost is the raw multi-channel brain signals. We use an encoder to map the raw data into a low-dimensional representation space. To capture the spatial and temporal correlation patterns, we propose three SSL tasks to guide the encoder to learn informative and distinguishable representations.\n\n3.2 SELF-SUPERVISED LEARNING TASKS FOR BRAIN SIGNALS\n\nTo capture the correlation patterns in space and time, we propose two self-supervised tasks: instantaneous time shift that is based on multi-channel CPC and captures the short-term correlations focusing on spatial patterns; and delayed time shift for temporal patterns in broader time scales.\n\nInstantaneous Time Shift. For spatial patterns, we aim to leverage the contextual information of correlated channels to better predict future data of the target channel. Therefore, we apply multichannel CPC and utilize the fine-grained graph structure At obtained in Section 3.1 as the correlations between channels.\n\nWe first use a non-linear encoder genc (1D-CNN with d kernels) mapping the observed time segments to the local latent d-dimensional representations zt = genc(st) ∈ RT ×C×d for each channel separately. T is the sequential length after down sampling by genc. Then an autoregressive model gar is utilized to summarize the historical τ -length local information of each channel itself to obtain the respective contextual representations:\n\ncself t,τ = gar(zt,1, · · · , zt,τ ).\n\n(12)\n\nIn this step, we only extract the contextual information of all channels independently. Based on the graph structure At, we instantiate the aggregate function Φ(·) in equation 4 as GNNs due to their natural message-passing ability on a graph. Here we use a one-layer directed GCN (Yun et al., 2019) to show the process:\n\ncother t,τ,i = ReLU\n\n(cid:32) (cid:80)\n\nj̸=i At(i, j) · cself (cid:80) j̸=i At(i, j)\n\nt,τ,j\n\n(cid:33)\n\n· Θ\n\n,\n\n(13)\n\nwhere Θ is the learnable matrix. Considering that we only aggregate other channels’ information, the self-loop in GCN is removed here. Finally, by combining both cself to obtain the global representations ct,τ , the model can predict the local representations k1-step away zt,τ +k1 based on the multi-channel CPC loss:\n\nt,τ and cother\n\nt,τ\n\nct,τ = Concat(cself\n\nL1 = LN = −Et,i,k1\n\nt,τ , cother t,τ ), (cid:34) c⊤ t,τ,iWk1 zt,τ +k1,i c⊤ t,τ,iWk1 zj\n\nlog\n\nzj ∈X i\n\n(cid:80)\n\nt\n\n(cid:35)\n\n,\n\n(14)\n\n(15)\n\nwhere X i samples. Wk1 is the learnable bilinear score matrix of the k1-th step prediction.\n\nt denotes the random noise set including one positive sample zt,τ +k1,i and N − 1 negative\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nDelayed Time Shift. For brain areas far apart, there exists delayed brain signal propagation, which is confirmed by the data observations showed in Appendix C. We should consider these significant temporal correlations across several time steps in our model.\n\nOur motivation is that if a simple classifier can easily predict whether two time segments are highly correlated, the segment representations will be significantly different from those with weaker correlations. We thus define the delayed time shift task to encourage more distinguishable segment representations. Similar with instantaneous time shift, we first compute the cosine similarity matrix based on raw data between time segments across several time steps. For the i-th channel in the t-th time segment, the long-term correlation matrix Bi\n\nt is computed as\n\nt(k2, j) = Cosine(st,i, st+k2,j), where j traverses all channels including the target channel and k2 traverses at most K2 prediction t according to Bi steps. Then we construct pseudo labels Y i t to encourage the segment representations with higher correlations to be closer. A predefined threshold θ2 is set to assign pseudo labels:\n\n(16)\n\nBi\n\nY i\n\nt (k2, j) =\n\n(cid:40)\n\n1,\n\n0,\n\nBi Bi\n\nt(k2, j) ≥ θ2, t(k2, j) < θ2.\n\n(17)\n\nWith the pseudo labels, we define the cross entropy loss of the delayed time shift prediction task:\n\nht = Pooling(cself\n\nt,1 , · · · , cself\n\nt,T ),\n\nˆp = Softmax(MLP(Concat(ht,i, ht+k2,j))),\n\n(18)\n\n(19)\n\nL2 = −Et,i,k2,j\n\nt (k2, j) log ˆp + (1 − Y i where ˆp is the predicted probability that the two segments are highly correlated. In practical application, we randomly choose 50% labels from each Y i\n\nt (k2, j)) log(1 − ˆp)(cid:3)\n\nt for efficient training.\n\n(cid:2)Y i\n\n(20)\n\nReplace Discriminative Learning. Consistently exploiting correlation for all channels will weaken the specificity between channels. However, there are significant differences in the physiological signal patterns of different brain areas recorded by channels. Therefore, retaining the characteristics of each channel cannot be ignored for the modeling of brain signals. For this purpose, we further design the replace discriminative learning task.\n\nFollowing BERT (Devlin et al., 2018), we randomly replace r% local representations throughout zt by ˆzt, which is sampled from any T sequences and any C channels in zt. We use the notation I(ˆzt) to represent the new local representations after replacement and the corresponding channel indexes of ˆzt in the original sequence. We generate pseudo labels Yt of the task as below:\n\nYt(τ, i) =\n\n(cid:26)1, 0,\n\nI(ˆzt,τ,i) ̸= i, I(ˆzt,τ,i) = i.\n\n(21)\n\nτ and i traverse T sequences and C channels of ˆzt. After obtaining ˆzt, we put it into the autoregressive model to get the new contextual representations ˆct = gar(ˆzt). Finally, a simple discriminator implemented by an MLP is utilized to classify whether ˆct are replaced by other channels or not:\n\nL3 = −Et,τ,i [Yt(τ, i) log ˆq + (1 − Yt(τ, i)) log(1 − ˆq)] ,\n\n(22)\n\nwhere ˆq is the predicted probability that ˆct,τ,i is replaced. When the accuracy of discrimination increases, different channel representations output by the autoregressive model are easier to distinguish. Therefore, the task preserves the unique characteristics of each channel.\n\nCombining the multi-task loss functions equation 15, equation 20 and equation 22, we jointly train MBrain with L = (1−λ1 −λ2)L1 +λ1L2 +λ2L3. After the SSL stage, the segment representations ht obtained from equation 18 are used for downstream tasks.\n\n4 EXPERIMENTS\n\n4.1 DATASETS AND BASELINES\n\nSEEG dataset. The SEEG dataset used in our experiment is anonymized and provided by a firstclass hospital we cooperate with. For a patient suffering from epilepsy, 4 to 10 invasive electrodes\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nwith 52 to 124 channels are used for recording signals. It is worth noting that since SEEG data are collected in a high frequency (1,000Hz or 2,000Hz) through multiple channels, our data is massive. In total, we have collected 470 hours of SEEG signals with a total capacity of 550GB. Professional neurosurgeons help us label the epileptic segments for each channel. For the self-supervised learning stage, we randomly sample 5,000 10-second SEEG clips for training and validation. As for the downstream seizure detection task, we first obtain 5,000 sampled 10-second SEEG clips (80% for training and 20% for validation). For the testing, we sample another 2,550 10-second SEEG clips with positive and negative sample ratio of 1:50. We use a 1-second window to segment each clip without overlap and our target is to make predictions for all channels in each 1-second segment.\n\nEEG dataset. We use the Temple University Hospital EEG Seizure Corpus (TUSZ) v1.5.2 (Shah et al., 2018) as our EEG dataset. It is the largest public EEG seizure database, containing 5,612 EEG recordings, 3,050 annotated seizures from clinical recordings, and eight seizure types. We include 19 EEG channels in the standard 10-20 system. For experimental efficiency, we generate a smaller dataset from TUSZ. We randomly sample 3,000 12-second EEG clips for self-supervised learning. As for the downstream seizure detection task, we first obtain 3,000 sampled 12-second EEG clips (80% for training and 20% for validation). For the testing, we sample another 3,900 12-second EEG clips with positive and negative sample ratio of 1:10. It is worth noting that the labels of EEG data are coarse-grained, which means we only have the label of whether epilepsy occurs in an EEG clip.\n\nBaselines. We compare MBrain with state-of-the-art models including one supervised time series classification model MiniRocket (Dempster et al., 2021) and several self-supervised and unsupervised models: CPC (Van den Oord et al., 2018), SimCLR (Chen et al., 2020), TripletLoss (T-Loss) (Franceschi et al., 2019), Time Series Transformer (TST) (Zerveas et al., 2021), GTS (Shang et al., 2021), TS-TCC (Eldele et al., 2021) and TS2Vec (Yue et al., 2021). See details of the baselines in Appendix E.\n\n4.2 EXPERIMENTAL SETUP\n\nTo demonstrate the effectiveness of MBrain, we first define the seizure detection task and perform the experiments on EEG and SEEG datasets respectively. To examine the generalization of our model, we further conduct experiments corresponding to two clinically feasible schemes on SEEG dataset. The ablation study, hyperparameter analysis, variance and more experimental results are showed in Appendix G, H and I. We also show the case study of the correlation graph learned by Section 3.1 in Appendix F to confirm the ability of identifying different brain states.\n\nTask 1 (Seizure Detection). Given a time-ordered set including IS consecutive time segments with the index of the first segment being t0: S = {st0, . . . , st0+IS }, models predict the labels ˆY s t,i for all time segments in S (i.e., t = t0, . . . , t0 + IS ) and all channels in each segment (i.e., i = 1, . . . , C).\n\nSeizure detection experiment. In this experiment, we first perform the self-supervised learning of the model on unlabeled data. Then the segment representations learned by the model are used for downstream seizure detection task (see details in Appendix D). During the training phase of downstream task, the encoder of SSL model will be fine-tuned with a very low learning rate. For EEG data, there is no overlap between the patients of training and testing sets. In addition, since EEG labels are coarse-grained, the segment representations in each 12-second EEG clip are pooled to one representation and then be used for seizure detection. For SEEG data, considering the difference in the number and location of implanted electrodes for patients, we conduct experiments for each patient independently and report the average performance.\n\nTransfer learning experiments. To meet practical clinical needs, we design two clinically feasible schemes for SEEG data. The first is the domain generalization experiment, that is, training the model on data of existing patients and directly predicting data of unknown patients. More specifically, we follow the “3-1-1” setting, where 3 patients are used for training, 1 patient is used for validation and 1 patient is used for testing. We conduct experiments for all combinations, pick up the best result for each patient, and report the average results over all patients. The second is the domain adaptation experiment (Motiian et al., 2017). In this experiment, we first perform SSL on one patient (i.e., source domain) and then fine-tuning is performed using partial labeled data from another patient (i.e., target domain) in the testing set. Finally, we perform seizure detection on the\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nremaining data of the target domain. We report the results on 12 cross-domain scenarios covering all combinations of four patients with typical seizure patterns in the SEEG dataset.\n\n4.3 RESULTS OF SEIZURE DETECTION EXPERIMENT\n\nThe average performance of seizure detection on the SEEG dataset are presented in Table 1. Since the positive-negative sample ratio of SEEG dataset is imbalanced, F -score is a more appropriate metric to evaluate the performance of models than only considering precision or recall. Especially in clinical applications, doctors pay more attention to finding as much seizures as possible, we thus choose F1-score and F2-score in the experiment. Overall, MBrain improves the F1-score by 28.92% and the F2-score by 26.85% on SEEG dataset compared to the best result of baseline methods, demonstrating that MBrain can learn informative representations from SEEG data.\n\nTable 1: The average performance of the seizure detection experiment on SEEG and EEG datasets.\n\nModels\n\nSEEG\n\nPre.\n\nRec.\n\nF1\n\nF2\n\nPre.\n\nRec.\n\nEEG\n\nF1\n\nF2\n\nAUROC\n\nMiniRocket\n\n22.98\n\n66.24\n\n31.79\n\n43.58\n\n22.86\n\n63.08\n\n33.56\n\n46.66\n\nCPC SimCLR T-Loss TST GTS TS-TCC TS2Vec\n\nMBrain\n\n27.65 11.06 29.29 13.60 24.29 22.10 30.56\n\n37.97\n\n55.07 51.54 51.55 44.65 40.39 49.94 52.83\n\n65.07\n\n34.20 16.60 36.00 19.80 29.16 25.32 36.03\n\n42.73 25.41 43.13 28.41 34.17 32.74 43.35\n\n46.45\n\n55.28\n\n22.81 12.63 20.72 15.65 18.86 15.55 21.40\n\n22.13\n\n58.31 74.88 69.25 28.59 62.51 39.76 58.31\n\n32.50 21.33 31.82 19.65 28.88 21.89 31.24\n\n44.02 36.78 47.00 23.87 42.54 29.60 43.24\n\n76.99\n\n34.32\n\n51.34\n\n75.30\n\n74.53 55.86 75.88 58.20 71.69 58.63 73.35\n\n77.96\n\nTable 1 also shows the results of seizure detection experiment on EEG dataset. Following the common evaluation scheme on EEG dataset (Tang et al., 2022), we also add Area Under the Receiver Operating Characteristic (AUROC) metric in our experiment. Our model is designed to learn the representation for each channel, while there is only one label for an EEG clip. Therefore, it requires the pooling operation to aggregate representations output by our model over channels and time segments for seizure detection. This setting makes the performance improvement of our model not as significant as that in the SEEG experiment. Nevertheless, MBrain still outperforms all baselines on F1-score, F2-score and AUROC with an increase of 2.26%, 9.23% and 2.74%, respectively.\n\nTable 2: The average performance of the domain generalization experiment on SEEG dataset.\n\nModels\n\nCPC T-Loss TS2Vec\n\nMBrain\n\nPre.\n\nRec.\n\n22.88±5.06 21.38±4.25 27.93±5.23\n\n23.92±3.90 28.50±4.07 29.49±3.97\n\nF1\n\n20.11±3.27 23.48±3.30 26.78±3.29\n\nF2\n\n21.23±2.49 25.90±3.06 27.88±3.52\n\n30.69±5.92\n\n38.94±4.34\n\n32.61±3.60\n\n35.64±3.04\n\n4.4 RESULTS OF DOMAIN GENERALIZATION EXPERIMENT\n\nIn this experiment, we compare the baseline models that perform well in Table 1. This is because, to some extent, the results in Table 1 represent an upper bound on the performance of these models. We point out that although GTS and MBrain are both graph-based models, GTS cannot be trained on multiple patients, since the DCRNN (Li et al., 2018) encoder used in GTS can only process graphs with fixed nodes. In contrast, our model is designed to learn the correlations between each pair of nodes and utilizes inductive GNN, so it can easily handle graphs with different numbers of input nodes. In general, the performance of models under the domain generalization setting decreases significantly (40.37% on average in terms of F2-score) compared with that in epilepsy detection\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nexperiment. The drop for recall metric is more pronounced, confirming that the distribution shift of patients in SEEG data is more significant than that in EEG. This results from the fact that different brain regions and different types of epileptic waves have different physiological properties and patterns. MBrain in this experiment still improves F1-score and F2-score by 21.77% and 27.83% respectively, compared to the best baseline. The results prove that MBrain has a superior generalization ability benefiting from rational inductive assumption of model design.\n\nTable 3: The performance of the domain adaptation experiment on SEEG dataset in terms of F2score. DA row denotes the performance of MBrain in the domain adaptation experiment setting. Max-base and Non-DA rows represent the best performance of baselines and MBrain in the seizure detection experiment. Bold numbers and * indicate the best and the second best performance.\n\nSetting\n\nDA Max-base Non-DA\n\nGroup A\n\nGroup B\n\nGroup C\n\nGroup D\n\nB→A C→A D→A A→B C→B D→B A→C B→C D→C A→D B→D C→D\n\n68.55 69.14* 68.78 41.08 46.06 46.12* 40.04 39.34 48.64 80.82* 79.90 80.72\n\n62.49 70.63\n\n39.78 46.62\n\n33.59 46.09*\n\n75.35 83.27\n\n4.5 RESULTS OF DOMAIN ADAPTATION EXPERIMENT\n\nAccording to the results of domain generalization experiment, it is difficult for MBrain to achieve competitive performance with Table 1. Due to the long record, clinical SEEG data contains tens or even hundreds of seizures, allowing our model to use a subset of data to fine-tune and then to predict the remaining data. Therefore, as a clinical alternative to domain generalization, we conduct a compromise domain adaptation experiment. Table 3 shows the performance of the domain adaptation (DA) experiment for four patients with typical seizure patterns provided by doctors from SEEG dataset. More specifically, we train MBrain on one patient and fine-tune it on all other three patients. B→A denotes that the SSL model is trained on Patient-B and then performs seizure detection experiment with the SSL model being fine-tuned on Patient-A. The results of “Max-base” and “Non-DA” rows correspond to the performance of the best baseline and MBrain respectively in scenarios A→A, B→B, C→C and D→D.\n\nCompared with the results in the condition that the self-supervised model and downstream model are both trained on the same patient, the F2-scores of all 12 cross-domain scenarios reduce by less than 15%. Additionally, it can be observed that in all cross-domain scenarios, MBrain beats the It is worth noting that best baseline in the corresponding scenarios without domain adaptation. D→C scenario outperforms corresponding “Non-DA” result. The possible reason is that the signal patterns on Patient-D are more significant and recognizable than those on Patient-C. Therefore, the SSL model trained on higher quality source domain can better distinguish signal states when performing downstream tasks on target domain. Overall, the domain adaptation experiment makes MBrain achieve competitive performance with Table 1 by fine-tuning it on only a subset of the target domain. The results suggest that MBrain captures the inherent features and outputs generalized representations between patients, because we fine-tune the SSL model with a very low learning rate (1e-6). From the perspective of pre-training, the SSL model trained on the source patient gives good initial parameters for the fine-tuning stage on the target patient.\n\n5 CONCLUSION AND DISCUSSION\n\nIn this paper, we propose a general multi-channel SSL framework MBrain, which can be applied for learning representations of both EEG and SEEG brain signals. Based on domain knowledge and data observations, we succeed to use the correlation graph between channels as the cornerstone of our model. The proposed instantaneous and delayed time shift tasks help us capture the correlation patterns of brain signals spatially and temporally. Extensive experiments of seizure detection on large-scale real-world datasets demonstrate the superior performance and generalization ability of MBrain. However, there are still some limitations of our work. For example, negative sampling of multi-channel CPC consumes certain memory and time. Besides, we lack a more automatic way to determine the time range of long-term temporal patterns. As for the future work, we plan to collect more types of brain signals and extend MBrain to more downstream tasks.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY STATEMENT\n\nWe provide the source code of our model MBrain in the Supplementary Material. Some implementation details of MBrain can be found in Appendix D and default hyperparameters can be found in the code. Users can run MBrain on their own datasets, following the same generation method of database mentioned in Section 4.1.\n\n7 ETHICS STATEMENT\n\nThis paper proposes a novel self-supervised learning framework MBrain for brain signals, and conduct experiments on real-world large-scale EEG and SEEG datasets. The EEG dataset is public and SEEG dataset is non-public but anonymous. Overall, this work inherits some of the risks of the existing works implementing the EEG dataset and does not introduce any new ethical or future social concerns for SEEG dataset.\n\nREFERENCES\n\nFahd A. Alturki, Khalil AlSharabi, Akram M. Abdurraqeeb, and Majid Aljalal. Eeg signal analysis for diagnosing neurological disorders using discrete wavelet transform and intelligent techniques. Sensors, 20(9), 2020.\n\nAnthony Bagnall, Jason Lines, Aaron Bostrom, James Large, and Eamonn Keogh. The great time series classification bake off: A review and experimental evaluation of recent algorithmic advances. Data mining and knowledge discovery, 31(3):606–660, 2017.\n\nHubert Banville, Omar Chehab, Aapo Hyv ̈arinen, Denis-Alexander Engemann, and Alexandre Gramfort. Uncovering the structure of clinical EEG signals with self-supervised learning. Journal of Neural Engineering, 18(4):046020, 2021.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NIPS, pp. 1877–1901, 2020.\n\nJ. X. Chen, P. W. Zhang, Z. J. Mao, Y. F. Huang, D. M. Jiang, and Y. N. Zhang. Accurate eegbased emotion recognition on combined features using deep convolutional neural networks. IEEE Access, 7:44317–44328, 2019.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, pp. 1597–1607, 2020.\n\nWei Chen and Ke Shi. Multi-scale attention convolutional neural network for time series classifica-\n\ntion. Neural Networks, 136:126–140, 2021.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning.\n\nIn CVPR, pp.\n\n15745–15753, 2021.\n\nAlexander Craik, Yongtian He, and Jose L Contreras-Vidal. Deep learning for electroencephalogram\n\n(eeg) classification tasks: a review. Journal of neural engineering, 16(3):031001, 2019.\n\nZachary W Davis, Lyle Muller, Julio Martinez-Trujillo, Terrence Sejnowski, and John H Reynolds. Spontaneous travelling cortical waves gate perception in behaving primates. Nature, 587(7834): 432–436, 2020.\n\nAngus Dempster, Franc ̧ois Petitjean, and Geoffrey I Webb. Rocket: exceptionally fast and accurate time series classification using random convolutional kernels. Data Mining and Knowledge Discovery, 34(5):1454–1495, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAngus Dempster, Daniel F Schmidt, and Geoffrey I Webb. Minirocket: A very fast (almost) deter-\n\nministic transform for time series classification. In KDD, pp. 248–257, 2021.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\n\ncontext prediction. In ICCV, pp. 1422–1430, 2015.\n\nEmadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In IJCAI, pp. 2352–2359, 2021.\n\nJean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation\n\nlearning for multivariate time series. In NIPS, 2019.\n\nClive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.\n\nEconometrica: journal of the Econometric Society, pp. 424–438, 1969.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In NIPS, pp. 21271–21284, 2020.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\n\nunsupervised visual representation learning. In CVPR, pp. 9726–9735, 2020.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9:1735–\n\n80, 1997.\n\nLina Elsherif Ismail and Waldemar Karwowski. Applications of eeg indices for the quantification of human cognitive performance: A systematic review and bibliometric analysis. PloS one, 15(12): e0242857, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nShiba Kuanar, Vassilis Athitsos, Nityananda Pradhan, Arabinda Mishra, and K.R. Rao. Cognitive analysis of working memory load from eeg, by a deep recurrent neural network. In ICASSP, pp. 2576–2580, 2018.\n\nYaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-\n\nwork: Data-driven traffic forecasting. In ICLR, 2018.\n\nChristopher W Lynn and Danielle S Bassett. The physics of brain network structure, function and\n\ncontrol. Nature Reviews Physics, 1(5):318–332, 2019.\n\nMichael L Martini, Aly A Valliani, Claire Sun, Anthony B Costa, Shan Zhao, Fedor Panov, Saadi Ghatan, Kanaka Rajan, and Eric Karl Oermann. Deep anomaly detection of seizures with paired stereoelectroencephalography and video recordings. Scientific Reports, 11(1):1–11, 2021.\n\nIshan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and learn: unsupervised learning\n\nusing temporal order verification. In ECCV, pp. 527–544. Springer, 2016.\n\nMostafa Neo Mohsenvand, Mohammad Rasool Izadi, and Pattie Maes. Contrastive representation\n\nlearning for electroencephalogram classification. In PMLR, pp. 238–253, 2020.\n\nSaeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep super-\n\nvised domain adaptation and generalization. In ICCV, pp. 5715–5725, 2017.\n\nM. Paluszek, D. Avirovik, Y. Zhou, S. Kundu, A. Chopra, R. Montague, and S. Priya. 11 - magnetoelectric composites for medical application. In Composite Magnetoelectrics, pp. 297–327. Woodhead Publishing, 2015.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nPiero Perucca, Franc ̧ois Dubeau, and Jean Gotman. Intracranial electroencephalographic seizure-\n\nonset patterns: effect of underlying pathology. Brain, 137(1):183–196, 2014.\n\nTimoth ́ee Proix, Viktor K Jirsa, Fabrice Bartolomei, Maxime Guye, and Wilson Truccolo. Predicting the spatiotemporal diversity of seizure propagation and termination in human focal epilepsy. Nature communications, 9(1):1–15, 2018.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\n\nKhansa Rasheed, Adnan Qayyum, Junaid Qadir, Shobi Sivathamboo, Patrick Kwan, Levin Kuhlmann, Terence O’Brien, and Adeel Razi. Machine learning for predicting epileptic seizures using eeg signals: A review. IEEE Reviews in Biomedical Engineering, 14:139–155, 2020.\n\nPatrick Sch ̈afer. The boss is concerned with time series classification in the presence of noise. Data\n\nMining and Knowledge Discovery, 29(6):1505–1530, 2015.\n\nVinit Shah, Eva Von Weltin, Silvia Lopez, James Riley McHugh, Lillian Veloso, Meysam Golmohammadi, Iyad Obeid, and Joseph Picone. The temple university hospital seizure detection corpus. Frontiers in Neuroinformatics, 12:83, 2018.\n\nChao Shang, Jie Chen, and Jinbo Bi. Discrete graph structure learning for forecasting multiple time\n\nseries. In ICLR, pp. 1–14, 2021.\n\nAfshin Shoeibi, Marjane Khodatars, Navid Ghassemi, Mahboobeh Jafari, Parisa Moridian, Roohallah Alizadehsani, Maryam Panahiazar, Fahime Khozeimeh, Assef Zare, Hossein Hosseini-Nejad, et al. Epileptic seizures detection using deep learning techniques: a review. International Journal of Environmental Research and Public Health, 18(11):5780, 2021.\n\nTengfei Song, Wenming Zheng, Peng Song, and Zhen Cui. Eeg emotion recognition using dynamical graph convolutional neural networks. IEEE Transactions on Affective Computing, 11(3): 532–541, 2020.\n\nChang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I Webb. Monash university,\n\nuea, ucr time series regression archive. arXiv preprint arXiv:2006.10996, 2020.\n\nSiyi Tang, Jared Dunnmon, Khaled Kamal Saab, Xuan Zhang, Qianying Huang, Florian Dubost, Daniel Rubin, and Christopher Lee-Messer. Self-supervised graph neural networks for improved electroencephalographic seizure analysis. In ICLR, 2022.\n\nAaron Van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv e-prints, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n\nRenping Yu, Han Zhang, Le An, Xiaobo Chen, Zhihui Wei, and Dinggang Shen. Connectivity strength-weighted sparse group representation-based brain network construction for m ci classification. Human brain mapping, 38(5):2370–2383, 2017.\n\nYe Yuan, Guangxu Xun, Kebin Jia, and Aidong Zhang. A multi-view deep learning framework for eeg seizure detection. IEEE Journal of Biomedical and Health Informatics, 23(1):83–94, 2019.\n\nZhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. arXiv preprint arXiv:2106.10466, 2021.\n\nSeongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph trans-\n\nformer networks. In NIPS, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nGeorge Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In KDD, pp. 2114–2124, 2021.\n\nXiang Zhang, Lina Yao, Xianzhi Wang, Jessica Monaghan, David Mcalpine, and Yu Zhang. A survey on deep learning-based non-invasive brain signals: recent advances and new frontiers. Journal of Neural Engineering, 18(3):031002, 2021.\n\nXiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, and Marinka Zitnik. Graph-guided network for irregularly sampled multivariate time series. In ICLR, 2022. URL https://openreview. net/forum?id=Kwm8I7dU-l5.\n\nZhe Zhu. Change detection using landsat time series: A review of frequencies, preprocessing, algorithms, and applications. ISPRS Journal of Photogrammetry and Remote Sensing, 130:370– 384, 2017.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA PRELIMINARIES\n\nBrain signal data. For both EEG and SEEG data, there are multiple electrodes with C contacts that are sampled at a fixed frequency to record the brain signals. We also call these contacts channels. For every sampling point, each channel records the potential value of the brain region in which they are located, constituting abstract multi-channel time series data. A complete record file contains a total of L time points, for which we use the notation X = {xl ∈ RC}L l=1 to represent. In the reminder of this paper, we use i and j to denote the index of channels, such as xl = {xl,i}C i=1. For every xl,i, we assign a binary label Yl,i ∈ {0, 1} to it according to the start and end time of epileptic brain signals marked by doctors. The time points are in the epileptic state with positive labels (Yl,i = 1), while zero labels (Yl,i = 0) represent the normal data.\n\nPreprocessing. Following the existing time series works (Zhu, 2017; Bagnall et al., 2017; Sch ̈afer, 2015) with the common preprocessing of segmentation, we use a W-length window to divide the original brain signal data X into time segments S = {st ∈ RW×C}|S| t=1 without overlapping. The number of segments |S| = ⌊L/W⌋. The segment label is obtained from the time points of the whole segment, i.e., Y s\n\nt,i = max{Yt×W+1,i, . . . , Y(t+1)×W,i}.\n\nB SINGLE-CHANNEL CPC\n\nContrastive Predictive Coding (CPC), a pioneering model for self-supervised contrastive learning, sets the pretext task to predict low-level local representations by high-level global contextual information ct. In this way, the model can avoid learning too many details of the raw data and pay more attention to the contextual semantic information of the sequence data. The InfoNCE loss proposed in CPC has become the basic design of the contrastive learning loss function. Specifically, given a raw data sample set X = {x1, . . . , xN } consisting of one positive sample from p(xt+k|ct) and N − 1 negative samples from the noisy distribution p(xt+k), InfoNCE will optimize: (cid:34)\n\n(cid:35)\n\nLN = −EX\n\nlog\n\n(cid:80)\n\nfk(xt+k, ct) xj ∈X fk(xj, ct)\n\n.\n\n(23)\n\nIn order to obtain the best classification probability of the positive sample with the cross entropy loss function, the optimal fk(xt+k, ct) is proportional to p(xt+k|ct)/p(xt+k). Furthermore, the optimal loss function is also closely related to mutual information, as below:\n\n(cid:34)\n\nLopt\n\nN = −EX\n\nlog\n\np(xt+k|ct)/p(xt+k)\n\np(xt+k|ct)/p(xt+k) + (cid:80) (cid:21)\n\nxj ∈Xneg\n\np(xj|ct)/p(xj)\n\n(cid:20)\n\nlog\n\n≥ EX\n\np(xt+k) p(xt+k|ct)\n\nN\n\n= −I(xt+k; ct) + log N.\n\n(cid:35)\n\n(24)\n\n(25)\n\nTherefore, we can conclude that while minimizing the loss function LN , we are also constantly approximating the mutual information of raw data distribution p(xt+k) and contextual semantic distribution p(ct). It turns out that InfoNCE is indeed a well-established loss function designed for self-supervised contrastive learning.\n\nC DATA OBSERVATIONS\n\nAs Figure 2 shows, for both EEG and SEEG data, we can observe that the correlation matrices are almost the same on two normal segments without overlapping in the same patient. To the opposite, the correlation matrix in the epileptic states differs from the normal ones greatly. These data observations verify the conclusion that correlation patterns can help us to distinguish different brain states, and support us to use the correlation matrix as the cornerstone of EEG and SEEG data.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The normal and seizure correlation matrices of EEG and SEEG brain signals. The top row is for SEEG and the bottom row is for EEG. For clear presentation, we sample some channels in SEEG data. The leftmost column including two figures are the base correlation matrices on normal data. The two figures in the middle column represent the matrices after subtracting another normal correlation matrices from the base matrices, and the rightmost column includes matrices after subtracting seizure correlation matrices from the base matrices.\n\nFigure 3: The correlation matrices of delayed time shift of SEEG data. The top figure shows the average correlation matrix over all 10-second SEEG clips of one particular patient. And the bottom figure represents the correlation matrix of one particular sampled 10-second clip. We compute the cosine similarity matrix between the first time segment in the clip of the first channel and the time segments of other channels in the next consecutive 7 time steps. For clear presentation, we sample 26 channels and set correlations below 0.5 to 0 for the bottom figure.\n\nThe data observations showed in Figure 3 and Figure 4 confirm that there still exist significant correlations between time segments across several time steps. Unlike instantaneous time shift, delayed correlations are not stable. This can be concluded from the numerical difference between the aver-\n\n15\n\nCorrelation matrices of delayed time shift (SEEG)ChannelsTemporal stepsUnder review as a conference paper at ICLR 2023\n\nFigure 4: The correlation matrices of delayed time shift of EEG data. The top figure shows the averaged correlation matrix over all 12-second EEG clips. And the bottom figure represents the correlation matrix of one particular 12-second clip which is randomly sampled. The computation and operation are the same as Figure 3.\n\naged correlation matrix and the one-clip correlation matrix in both figures. Therefore, we design a self-supervised task different from that in the instantaneous time shift to learn the delayed correlations.\n\nD IMPLEMENTATION DETAILS OF MBrain\n\nThe non-linear encoder genc used in MBrain is composed of three 1-D convolution layers and a one-layer LSTM model (Hochreiter & Schmidhuber, 1997) is used as the autogressive model gar. The model is optimized using Adam optimizer (Kingma & Ba, 2015) with a learning rate of 2e-4 and weight decay of 1e-6 for the self-supervised learning stage. And for the downstream seizure detection stage, the downstream model is optimized with a learning rate of 5e-4 and weight decay of 1e-6 while the SSL model is fine-tuned with a low learning rate of 1e-6. For the hyperparameters of MBrain, we set θ1 = 0.5 and θ2 = 0.5. We set the maximum value of k1 in instantaneous time shift task as 8. As Figure 5 shows, we set K2 = 7 so as to take into account the step with the most significant correlation in delayed time shift task. Lastly, we build our model using PyTorch 1.8 (Paszke et al., 2019) and trained it on a workstation with four NVIDIA TESLA T4 GPUs.\n\nFor the downstream task, we first utilize an LSTM model (Hochreiter & Schmidhuber, 1997) to encode the segment representations of each channel in chronological order (10-second in SEEG clips and 12-second in EEG clips) independently. One-layer self-attention (Vaswani et al., 2017) is then adopted to all channels within the same time step. Finally, a two-layer MLP classifier is used to predict whether epilepsy is occurring in the time segments. All baselines share the same downstream model in our experiments.\n\nE IMPLEMENTATION DETAILS OF BASELINES\n\n• MiniRocket (Dempster et al., 2021): Rocket (Dempster et al., 2020) is a state of the art supervised time series classification method based on evaluations on public benchmarks (Bagnall et al., 2017; Tan et al., 2020), involves training a linear classifier on top of features extracted by a flat collection of numerous and various random convolutional kernels. MiniRocket is a variant of Rocket which improves processing time, while offering essentially the same accuracy. We uese the open source\n\n16\n\nCorrelation matrices of delayed time shift (EEG)ChannelsTemporal stepsUnder review as a conference paper at ICLR 2023\n\nFigure 5: The data observation of how to choose hyperparameter K2. We first average the correlations of all channels of each channel in each time step. Then we average those of all channels in the same time step. As the figure shows, we empirically choose K2 = 7.\n\ncode from https://github.com/angus924/minirocket. For each subject, we use the features obtained through MiniRocket to train an independent logistic regression classifier for each channel and test it on the test set of that channel.\n\n• CPC (Van den Oord et al., 2018): This is a self-supervised learning method based on a contrastive loss InfoNCE. The pretext task of CPC is set to predict future local low-level representations obtained from multi-layer CNNs by contextual high-level representations obtained from an autoregressive model. This is the backbone model in this paper. We use the open source code of the corrected version from https://github.com/facebookresearch/CPC audio.\n\n• SimCLR (Chen et al., 2020): This is a simple yet effective framework for contrastive learning of visual representations and we use time-series specific augmentations to adapt it to our application. We implemented SimCLR on time series data by ourselves. We use the same encoder architecture and parameter configuration as TS-TCC. In the meantime, we also follow TS-TCC and use scaling (sigma=1.1) as the data augmentation way.\n\n• Triplet-Loss (T-Loss) (Franceschi et al., 2019): The approach employs time-based negative sampling and a triplet loss to learn representations for time series segments. We use the default model architecture from the source code provided by the author (https://github.com/WhiteLink/UnsupervisedScalableRepresentationLearningTimeSeries). For the sampling method of negative samples, we use the data of the previous batch as the candidate set of negative samples of the current batch data (the negative sample candidate set for the first batch is itself). Since the dataloader is shuffled at the end of each epoch, there is no need to worry about the case where the set of sampled negative samples does not change.\n\n• Time Series Transformer (TST) (Zerveas et al., 2021): This is a unsupervised representation learning framework for multivariate time series by training a transformer model to extract dense vector representations of time series through an input denoising objective. We use the default model architecture from the source code provided by the author (https://github.com/gzerveas/mvts transformer).\n\n• GTS (Shang et al., 2021): This is a time series forecasting model that learns a graph structure among multiple time series and forecasts them simultaneously with a GNN. In view of this, this model can learn useful representations from unlabeled time series data. We use the default model architecture from the source code provided by the author (https://github.com/chaoshangcs/GTS). In the pre-training stage, we divide each time series segment into 10 parts on average, and learn a time series forecasting model that predicts the next 2 steps based on the previous 8 steps. In the downstream task stage, we use the representation after step 10 as the representation of the time series segment for the seizure detection task.\n\n• TS-TCC (Eldele et al., 2021): This is an unsupervised time-series representation learning framework, applying a temporal contrasting module and a contextual contrasting module to learn robust\n\n17\n\n123456789Temporal steps0.0100.0050.0000.0050.010Cosine correlationsAnalysis of hyperparameter K2Under review as a conference paper at ICLR 2023\n\nand discriminative representations. We use the default model architecture from the open source code provided by the author (https://github.com/emadeldeen24/TS-TCC).\n\n• TS2Vec (Yue et al., 2021): This is a universal representation learning framework for time series, that applies hierarchical contrasting to learn scale-invariant representations within augmented context views. We use the default model architecture from the source code provided by the author (https://github.com/yuezhihan/ts2vec).\n\nF CASE STUDY\n\n(a) Normal correlation graph.\n\n(b) Seizure correlation graph.\n\nFigure 6: Case study on correlation graphs learned by MBrain.\n\nIn this section, we study the correlation graphs between the channels learned by MBrain. We randomly sample normal and seizure SEEG clips of one particular patient, and visualize their correlation graphs. The correlation graphs showed in Figure 6, we use the threshold θ1 defined in Section 3.1 for the preservation of the edges. In addition, the thickness of an edge represents the edge weight, and the size of a node represents the sum of the edge weights of all the edges linked to that node. It can be observed that during the normal phase, the correlation is sparser and the weights between edges are smaller, indicating a weaker correlation between channels. In contrast, during the seizure phase, the pattern between channels varies, with the correlation becomes denser and the edge weights become larger. Furthermore, in the correlation graph of the seizure phase, edges with larger weights are usually connected to 2 seizure channels, like Channel-2, Channel-35 and Channel-38 in Figure 6(b), which can help surgeons to better localize the seizure lesions.\n\nG ABLATION STUDY\n\nWe study the effectiveness of each component in MBrain. Specifically, we compare MBrain with different model variants removing following different components. We firstly remove the correlation graph structure learning module from the instantaneous time shift task and degenerate the task to single-channel CPC while still uniformly sampling negative samples in all channels. This variant is denoted as “MBrain-Graph”. Next, we respectively remove the whole instantaneous time shift task, the delayed time shift task and replace discriminative task. These model variants are denoted as “MBrain-Instant”, “MBrain-Delay” and “MBrain-Replace”. Finally, we consider the condition of preserving only one self-supervised task. “MBrain-onlyInstant”, “MBrain-onlyDelay” and\n\n18\n\n01234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950510123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051Under review as a conference paper at ICLR 2023\n\n“MBrain-onlyReplace” indicate that MBrain only performs instantaneous time shift task, delayed time shift task and replace discriminative task respectively.\n\nWe repeat the experiments five times in the fine-tuning stage of the downstream seizure detection task. Table 4 shows the results of ablation study on SEEG dataset. It can be observed that the complete MBrain achieves the best performance on F1 and F2 metrics, demonstrating the effectiveness of each component in our model design. For “MBrain-Instant”, the significant decrease in performance illustrates that capturing the spatial and short-term patterns is quite important and is the key to learning the essential representations in multi-channel brain signals. For “MBrain-Graph”, the decrease in performance demonstrates that multi-channel CPC can greatly help learn more informative representations. Additionally, the performance in “MBrain-Delay” and “MBrain-Replace” also decreases significantly, illustrating that modeling long-term temporal patterns and preserving the characteristics of channels can help learn more distinguishable representations. In the condition where only one self-supervised task is preserved, it can be observed that the instantaneous time shift task is the most important, which is as expected, and the delayed time shift task and the replace discriminative task contribute similarly to the performance of the complete model.\n\nTable 4: The results of ablation study.\n\nModels\n\nCPC CPC-Conv CPC-MLP\n\nMBrain-Graph MBrain-Instant MBrain-Delay MBrain-Replace\n\nMBrain-onlyInstant MBrain-onlyDelay MBrain-onlyReplace\n\nPre.\n\nRec.\n\nF1\n\nF2\n\n27.65±4.49 6.39±0.77 25.84±3.07\n\n36.72±4.59 34.49±4.37 35.00±4.49 36.08±5.35\n\n36.43±4.44 31.59±4.24 34.13±6.84\n\n55.07±3.52 33.21±4.00 52.70±3.65\n\n60.48±4.47 55.41±3.90 65.61±2.94 63.67±4.24\n\n63.66±2.12 55.03±5.26 56.06±3.68\n\n34.20±3.40 10.53±1.07 32.18±2.46\n\n43.61±3.08 41.57±3.48 42.97±3.61 43.66±3.66\n\n43.35±3.83 38.56±2.84 40.02±4.47\n\n42.73±2.57 17.46±1.42 40.34±2.05\n\n51.47±2.68 48.38±2.52 52.51±1.93 52.49±2.32\n\n51.82±2.67 46.05±2.26 47.44±2.40\n\nMBrain\n\n37.97±2.75\n\n65.07±2.68\n\n46.45±2.25\n\n55.28±1.77\n\nIn addition to removing some components and tasks from MBrain, we also design some ablation experiments to verify the effectiveness of our proposed graph structure learning. We have proposed two ideas on how to directly implement the multi-channel CPC in Section 2. For the second idea, we have reported the results of a shared CPC regarding all channels as one on the CPC row of Table 1. For the first idea, we design two strategies to combine multi-channel CNN or MLP into CPC respectively to learn representations for each channel.\n\n• Directly use 1-Dimension CNN to encode the whole time series data and the number of channels during the process is C → 256 → 256 → C × 256, and split the output into C representations, each of which is a 256-dimensional representation. Then an LSTM is implemented to it. Then we execute the self-supervised task and the downstream task of CPC based on the representations for each channel as MBrain does, this variant is denoted as “CPC-Conv”.\n\n• We use the contextual representations of all n channels as input to an MLP in a fixed order, but we set the representation of the target channel to 0 tensor when we aggregate them. By using the output of MLP as the aggregated representation of other channels, we perform subsequent experiments following exactly the same steps as MBrain. We name this variant as “CPC-MLP”.\n\nWe can observe that the performance of “CPC-Conv” decreases dramatically. We speculate that this is because the channels are relatively independent, and the correlation between most channels is weak or even non-existent. Direct adoption of multi-channel convolution may introduce spurious and noisy correlations. However, the graph structure learning proposed by us has a sparsity assumption, and the representation extraction of each channel is relatively independent, so it can effectively learn and aggregate more significant information. For “CPC-MLP”, we use an MLP to aggregate\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nthe representations of other channels, and then concatenate it with the representation of the target channel to predict future data. Unlike “CPC-Con” which directly adopts multi-channel convolution for the raw data to obtain the “mixed” low-level representations, “CPC-MLP”, like MBrain, learns the correlation of channels based on the “separate” high-level representations. Therefore, the performance of “CPC-MLP” does not drop as dramatically as that of “CPC-Conv”. It can be observed that “CPC-MLP” outperforms “CPC” on the EEG dataset. This may result from the fact that the number of channels in EEG dataset is only 19, while that in SEEG dataset is 52 to 124. Consequently, the ablation results show that the graph structure learning we design has a reasonable and parameter-efficient inductive assumption.\n\nH HYPERPARAMETER ANALYSIS EXPERIMENTS\n\n(a) Weights search for L2.\n\n(b) Weights search for L3.\n\nFigure 7: Sensitivity analysis on loss weights.\n\nSensitivity analysis on loss weights. Our loss function is defined as: L = (1−λ1−λ2)L1+λ1L2+ λ2L3, where L1, L2 and L3 are the loss of instantaneous time shift prediction task, delayed time shift prediction task and replace discriminative task respectively, and λ1 and λ2 are hyperparameters to balance the three pre-training tasks. We search both of the weights of λ1 and λ2 in the set {0.1, 0.2, 0.3, 0.4, 0.5} and report the tuning results with F2-score for seizure detection task on patient-A from SEEG dataset. In 7(a) and 7(b), we can see that λ1 = 0.5 and λ2 = 0.3 lead to the optimal performance. In addition, MBrain consistently performs better than the best baseline.\n\nFigure 8: Sensitivity analysis on replace ratio r%.\n\nSensitivity analysis on replace ratio. We perform sensitivity analysis on replace ratio r% from replace discriminative task. We search the replace ratio from 5% to 95% and report the tuning results with F2-score for seizure detection task on patient-A from SEEG dataset. As Figure 8 shows, when the replace ratio is set as 45%, MBrain has the best performance of 71.06±3.41. While MBrain gets\n\n20\n\n0.10.20.30.40.510.5500.5750.6000.6250.6500.6750.7000.7250.750F2-score2=0.3MBrainMax-base0.10.20.30.40.520.5500.5750.6000.6250.6500.6750.7000.7250.750F2-score1=0.5MBrainMax-base05152535455565758595100Replace ratio (%)0.500.550.600.650.700.750.80F2-scoreSensitivity analysis of replace ratioMBrainUnder review as a conference paper at ICLR 2023\n\nthe smallest standard deviation and the second best performance of 70.63±1.41 when the replace ratio is set as 15%.\n\nI FULL RESULTS\n\nI.1 FULL RESULTS OF SEIZURE DETECTION EXPERIMENT\n\nTable 5: The average performance of the seizure detection experiment on the SEEG dataset.\n\nModels\n\nSEEG\n\nPre.\n\nRec.\n\nF1\n\nF2\n\nMiniRocket\n\n22.98±0.15\n\n66.24±0.26\n\n31.79±0.19\n\n43.58±0.22\n\nCPC SimCLR T-Loss TST GTS TS-TCC TS2Vec\n\nMBrain\n\n27.65±4.49 11.06±3.95 29.29±2.65 13.60±3.48 24.29±4.26 22.10±7.65 30.56±2.17\n\n55.07±3.52 51.54±5.87 51.55±2.53 44.65±4.21 40.39±5.80 49.94±5.41 52.83±2.89\n\n34.20±3.40 16.60±4.68 36.00±1.97 19.80±3.73 29.16±2.97 25.32±8.02 36.03±1.72\n\n42.73±2.57 25.41±4.95 43.13±1.57 28.41±3.29 34.17±2.36 32.74±7.95 43.35±1.59\n\n37.97±2.75\n\n65.07±2.68\n\n46.45±2.25\n\n55.28±1.77\n\nTable 6: The average performance of the seizure detection experiment on the SEEG dataset with the encoder of the SSL model frozen.\n\nModels\n\nSEEG\n\nPre.\n\nRec.\n\nF1\n\nF2\n\nMiniRocket\n\n22.98±0.15\n\n66.24±0.26\n\n31.79±0.19\n\n43.58±0.22\n\nCPC SimCLR T-Loss TST GTS TS-TCC TS2Vec\n\nMBrain\n\n26.99±4.35 10.36±2.76 29.20±3.21 13.32±3.96 23.13±3.51 23.07±7.45 30.79±3.08\n\n54.16±4.49 50.89±6.68 50.24±3.52 45.59±4.37 42.34±4.95 50.32±7.11 51.43±3.36\n\n32.98±3.25 15.69±3.21 35.03±1.73 19.40±3.91 28.92±2.70 26.29±6.08 35.99±2.78\n\n41.17±2.45 24.19±3.34 41.85±1.99 28.24±2.86 34.78±2.08 34.39±5.64 42.88±2.44\n\n37.72±5.46\n\n63.34±4.27\n\n44.47±4.57\n\n53.00±4.26\n\nSeizure detection experiment. First, we keep the initial parameters of the trained self-supervised model unchanged, repeat the experiments five times in the fine-tuning stage of the downstream seizure detection task, and report the mean and standard deviation results in Table 5 and 7. Next, we freeze the entire encoder of SSL model and only train the downstream model and the classifier in the seizure detection task. The same reproducible experimental results are shown in Table 6 and Table 8. We can see that the performance of most models drops slightly compared with the results without freezing the SSL model. Nevertheless, our model still has a competitive performance in F2-score. For the supervised model MiniRocket without a learnable representation extractor, the results remain unchanged in the two experimental settings.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: The average performance of the seizure detection experiment on the EEG dataset.\n\nModels\n\nPre.\n\nRec.\n\nEEG\n\nF1\n\nF2\n\nAUROC\n\nMiniRocket\n\n22.86±0.84\n\n63.08±1.47\n\n33.56±1.11\n\n46.66±1.33\n\n75.30±0.77\n\nCPC SimCLR T-Loss TST GTS TS-TCC TS2Vec\n\nMBrain\n\n22.81±2.04 12.63±1.62 20.72±1.26 15.65±1.54 18.86±1.09 15.55±0.88 21.40±0.63\n\n58.31±7.55 74.88±16.77 69.25±3.99 28.59±12.93 62.51±5.04 39.76±11.08 58.31±6.14\n\n32.50±1.24 21.33±1.95 31.82±1.08 19.65±4.36 28.88±0.88 21.89±1.20 31.24±1.18\n\n44.02±2.43 36.78±2.61 47.00±0.50 23.87±8.09 42.54±1.48 29.60±4.64 43.24±2.78\n\n74.53±1.00 55.86±5.36 75.88±0.49 58.20±4.27 71.69±1.88 58.63±1.62 73.35±1.02\n\n22.13±1.03\n\n76.99±4.49\n\n34.32±0.90\n\n51.34±0.97\n\n77.96±0.97\n\nTable 8: The average performance of the seizure detection experiment on the EEG dataset with the encoder of the SSL model frozen.\n\nModels\n\nPre.\n\nRec.\n\nEEG\n\nF1\n\nF2\n\nAUROC\n\nMiniRocket\n\n22.86±0.84\n\n63.08±1.47\n\n33.56±1.11\n\n46.66±1.33\n\n75.30±0.77\n\nCPC SimCLR T-Loss TST GTS TS-TCC TS2Vec\n\nMBrain\n\n23.00±1.87 11.69±0.49 21.09±2.61 13.42±2.92 18.07±0.78 14.03±2.50 20.23±1.17\n\n53.69±7.47 71.73±11.90 65.54±6.26 30.96±13.66 65.01±2.92 34.34±10.20 64.82±13.92\n\n31.90±1.40 20.05±1.20 31.63±2.24 17.62±5.05 28.27±1.07 19.35±3.06 30.43±1.77\n\n41.95±3.19 35.22±3.25 45.59±1.51 23.11±8.09 42.76±1.53 25.89±5.68 44.21±5.29\n\n73.46±1.35 51.35±1.16 75.25±1.10 53.82±5.07 71.42±2.13 54.80±4.93 73.43±2.60\n\n23.09±1.96\n\n66.02±5.05\n\n34.08±1.82\n\n47.88±1.52\n\n76.27±1.91\n\nSignificant Analysis. Due to the large variance of the experimental results, we further conduct the significance test of the mean values to show that the performance of MBrain is indeed significantly better than that of other baseline models. We would like to emphasize that we are primarily concerned with the F2-score of the models, because in the clinical practice, doctors focus on finding as many seizures as possible. Thus, we focus on the significant analysis of F2-score for the results of Table 1.\n\nWe perform significance analysis according to the following procedures: (1) For SEEG data, we first combine the repeat results of each model for each patient into one vector. For EEG data, we directly use the vector of repeat results. Significance analysis is performed on F2 score vectors of all models pair by pair. (2) We use Levene’s test1 to test whether two populations have homogeneity of variance, which is a critical property in significant analysis. (3) On the condition of homogeneity of variance, we conduct independent sample T-test2 for the two populations. We show the p-values of Table 5 and Table 7 in Figure 9 in the form of thermal maps respectively. Considering the symmetry of the significance test, we only show the half of the p-value matrix.\n\nFrom these two figures, we can see that for EEG data, MBrain significantly outperforms other baselines. For SEEG data, although the performance of T-Loss and TS2Vec is not significantly different\n\n1https://en.wikipedia.org/wiki/Levene’s_test 2https://en.wikipedia.org/wiki/Student%27s_t-test#Unpaired_and_paired_\n\ntwo-sample_t-tests\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\n(a) SEEG dataset.\n\n(b) EEG dataset.\n\nFigure 9: Significant analysis.\n\nfrom that of MBrain at the tolerance of 0.05, the p-value was only slightly above 0.05. Therefore, in general, MBrain is superior to other baselines models in terms of F2-score. For baselines, we can see that MiniRocket, CPC, T-Loss, TS2Vec and GTS have similar performance on the two datasets. And SimCLR, TST and TS-TCC perform relatively poorly.\n\nI.2 FULL RESULTS OF DOMAIN ADAPTATION EXPERIMENT\n\nTable 9: The performance of the domain adaptation experiment on SEEG dataset in terms of F2score. DA row denotes the performance of MBrain in the domain adaptation experiment setting. Max-base and Non-DA rows represent the best performance of baselines and MBrain in the seizure detection experiment. Bold numbers and * indicate the best and the second best performance.\n\nSetting\n\nGroup A\n\nGroup B\n\nB→A\n\nC→A\n\nD→A\n\nA→B\n\nC→B\n\nD→B\n\nDA\n\n68.55±4.27 69.14±6.54* 68.78±4.12\n\n41.08±2.59\n\n46.06±3.05 46.12±2.04*\n\nMax-base\n\nNon-DA\n\nSetting\n\n62.49±2.30\n\n70.63±1.41\n\nGroup C\n\n39.78±2.04\n\n46.62±2.42\n\nGroup D\n\nA→C\n\nB→C\n\nD→C\n\nA→D\n\nB→D\n\nC→D\n\nDA\n\n40.04±3.98\n\n39.34±2.11\n\n48.64±5.48 80.82±0.65* 79.90±1.11\n\n80.72±1.31\n\nMax-base\n\nNon-DA\n\n33.59±2.23\n\n46.09±2.35*\n\n75.35±0.79\n\n83.27±0.95\n\nIn this experiment, we still keep the self-supervised model trained on the source domain unchanged and repeat the experiments on the target domain five times in the fine-tuning stage of the downstream seizure detection task. The mean and standard deviation results are presented in Table 9. It can be observed that the variances of the results under DA setting are generally higher than those under Non-DA setting, indicating a large variation in the patterns of patients in SEEG dataset. Nevertheless, high-quality source patient data can still be used to pre-train an SSL encoder with good average performance on the target patient. In practice, we can make predictions in an ensemble way by training multiple classifiers.\n\n23\n\nMBrainMiniRocketCPCSimCLRT-LossTSTGTSTS-TCCTS2VecMBrainMiniRocketCPCSimCLRT-LossTSTGTSTS-TCCTS2Vec0.0280.0280.869<0.01<0.01<0.010.0600.9670.855<0.01<0.01<0.01<0.010.500<0.01<0.010.2210.3080.0700.2650.177<0.010.0190.0470.1050.0460.3000.5410.0540.9670.918<0.010.942<0.010.2980.0580.00.20.40.60.81.0MBrainMiniRocketCPCSimCLRT-LossTSTGTSTS-TCCTS2VecMBrainMiniRocketCPCSimCLRT-LossTSTGTSTS-TCCTS2Vec<0.01<0.010.093<0.01<0.01<0.01<0.010.6470.069<0.01<0.01<0.01<0.010.053<0.01<0.01<0.010.329<0.01<0.01<0.01<0.01<0.01<0.010.080<0.010.434<0.01<0.010.0570.6850.0160.053<0.010.669<0.010.00.20.40.60.81.0",
    "reference": "# Summary Of The Paper\n\nThe paper proposes an SSL framework for EEG and evaluate for seizure detection.\n\n# Strength And Weaknesses\n\nStrengths:\n- Nicely written\n- Baselines are chosen well\n- Outperforms all baselines for both F1 and F2 (weaknesses highlight why this is not really valid)\n- Evaluated on both SEEG and EEG datasets\n- Includes clinical collaboration further signifying the significance of the work\n- The domain adaptation experiment in Table 2 is also a great way to show the significance of the work\n\nWeaknesses:\n- Split table 3 into two tables?\n- Domain adaptation not shown on TUSZ dataset\n- Including the same subject in both testing and training invalidates the results\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writeup is clear and the work has sufficient novelty. Code is included for reproducibility.\n\n# Summary Of The Review\n\nThe paper is nicely written with good results on two different datasets. The demonstrated application is important and the strong performance indicates the significance of the work. However, including the same subjects in training and testing invalidates the results. It is not clear if that was done for other approaches as well. Even if it was, both should be done by selecting different subjects for training and testing.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nISS: IMAGE AS STEPPING STONE FOR TEXT-GUIDED 3D SHAPE GENERATION\n\nZhengzhe Liu1 1The Chinese University of Hong Kong\n\nPeng Dai2 Ruihui Li3 Xiaojuan Qi2∗ Chi-Wing Fu1∗\n\n2The University of Hong Kong\n\n3Hunan University\n\nABSTRACT\n\nText-guided 3D shape generation remains challenging due to the absence of large paired text-shape dataset, the substantial semantic gap between these two modalities, and the structural complexity of 3D shapes. This paper presents a new framework called Image as Stepping Stone (ISS) for the task by introducing 2D image as a stepping stone to connect the two modalities and to eliminate the need for paired text-shape data. Our key contribution is a two-stage feature-space-alignment approach that maps CLIP features to shapes by harnessing a pre-trained single-view reconstruction (SVR) model with multi-view supervisions: first map the CLIP image feature to the detail-rich shape space in the SVR model, then map the CLIP text feature to the shape space and optimize the mapping by encouraging CLIP consistency between the input text and the rendered images. Further, we formulate a textguided shape stylization module to dress up the output shapes with novel structures and textures. Beyond existing works on 3D shape generation from text, our new approach is general for creating shapes in a broad range of categories, without requiring paired text-shape data. Experimental results manifest that our approach outperforms the state-of-the-arts and our baselines in terms of fidelity and consistency with text. Further, our approach can stylize the generated shapes with both realistic and fantasy structures and textures. Codes are available at https://github.com/ liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation.\n\nFigure 1: Our novel “Image as Stepping Stone” framework (a) is able to connect the text space (the CLIP Text feature) and the 3D shape space (the SVR feature) through our two-stage feature-space alignment, such that we can generate plausible 3D shapes from text (b) beyond the capabilities of the existing works (CLIP-Forge and Dream Fields), without requiring paired text-shape data.\n\n1\n\nINTRODUCTION\n\n3D shape generation has a broad range of applications, e.g., in Metaverse, CAD, games, animations, etc. Among various ways to generate 3D shapes, a user-friendly approach is to generate shapes from natural language or text descriptions. By this means, users can readily create shapes, e.g., to add/modify objects in VR/AR worlds, to design shapes for 3D printing, etc. Yet, generating shapes from texts is very challenging, due to the lack of large-scale paired text-shape data, the large semantic gap between the text and shape modalities, and the structural complexity of 3D shapes.\n\nExisting works (Chen et al., 2018; Jahan et al., 2021; Liu et al., 2022) typically rely on paired textshape data for model training. Yet, collecting 3D shapes is already very challenging on its own, let alone the tedious manual annotations needed to construct the text-shape pairs. To our best knowledge, the largest existing paired text-shape dataset (Chen et al., 2018) contains only two categories, i.e., table and chair, thus severely limiting the applicability of the existing works.\n\n1\n\nA green bedside lamp.Ours(b) Text: A {green,rainbow, mushroom}bedside lamp.Clip-Forge Dream Fieldsrainbowvs.mushroom MapperMapperSVR decoderCLIP Text feature spaceCLIP Image feature space3D shape feature spaceSVR Stage 1 Stage 2 Stage-1 feature Target shape feature (a) Our “Image as Stepping Stone” FrameworkgreenCLIP Textfeature fT3D shapeSVRfeature fsCLIP imagefeature fIM(fT)M(fI)M′(fT)Published as a conference paper at ICLR 2023\n\nVery recently, two annotation-free approaches, CLIP-Forge (Sanghi et al., 2022) and Dream Fields (Jain et al., 2022), were proposed to address the dataset limitation. These two state-ofthe-art approaches attempt to utilize the joint text-image embedding from the large-scale pre-trained language vision model, i.e., CLIP (Radford et al., 2021), to eliminate the need of requiring paired text-shape data in model training. However, it is still extremely challenging to generate 3D shapes from text without paired texts and shapes for the following reasons. First, the range of object categories that can be generated are still limited due to the scarcity of 3D datasets. For example, Clip-Forge (Sanghi et al., 2022) is built upon a shape auto-encoder; it can hardly generate plausible shapes beyond the ShapeNet categories. Also, it is challenging to learn 3D prior of the desired shape from texts. For instance, Dream Field (Jain et al., 2022) cannot generate 3D shapes like our approach due to the lack of 3D prior, as it is trained to produce only multi-view images with a neural radiance field. Further, with over an hour of optimization for each shape instance from scratch, there is still no guarantee that the multi-view consistency constraint of Dream Field (Jain et al., 2022) can enforce the model for producing shapes that match the given text; we will provide further investigation in our experiments. Last, the visual quality of the generated shapes is far from satisfactory due to the substantial semantic gap between the unpaired texts and shapes. As shown in Figure 1 (b), the results generated by Dream Field typically look surrealistic (rather than real), due to insufficient information extracted from text for the shape structures and details. On the other hand, CLIP-Forge (Sanghi et al., 2022) is highly restricted by the limited 643 resolution and it lacks colors and textures, further manifesting the difficulty of generating 3D shapes from unpaired text-shape data.\n\nGoing beyond the existing works, we propose a new approach to 3D shape generation from text without needing paired text-shape data. Our key idea is to implicitly leverage 2D image as a stepping stone (ISS) to connect the text and shape modalities. Specifically, we employ the joint text-image embedding in CLIP and train a CLIP2Shape mapper to map the CLIP image features to a pre-trained detail-rich 3D shape space with multi-view supervisions; see Figure 1 (a): stage 1. Thanks to the joint text-image embedding from CLIP, our trained mapper is able to connect the CLIP text features with the shape space for text-guided 3D shape generation. Yet, due to the gap between the CLIP text and CLIP image features, the mapped text feature may not align well with the destination shape feature; see the empirical analysis in Section 3.2. Hence, we further fine-tune the mapper specific to each text input by encouraging CLIP consistency between the rendered images and the input text to enhance the consistency between the input text and the generated shape; see Figure 1 (a): stage 2.\n\nOur new approach advances the frontier of 3D shape generation from text in the following aspects. First, by taking image as a stepping stone, we make the challenging text-guided 3D shape generation task more approachable and cast it as a single-view reconstruction (SVR) task. Having said that, we learn 3D shape priors from the adopted SVR model directly in the feature space. Second, benefiting from the learned 3D priors from the SVR model and the joint text-image embeddings, our approach can produce 3D shapes in only 85 seconds vs. 72 minutes of Dream Fields (Jain et al., 2022). More importantly, our approach is able to produce plausible 3D shapes, not multi-view images, beyond the generation capabilities of the state-of-the-art approaches; see Figure 1 (b).\n\nWith our two-stage feature-space alignment, we already can generate shapes with good fidelity from texts. To further enrich the generated shapes with vivid textures and structures beyond the generative space of the pre-trained SVR model, we additionally design a text-guided stylization module to generate novel textures and shapes by encouraging consistency between the rendered images and the text description of the target style. We then can effectively fuse with the two-stage feature-space alignment to enable the generation of both realistic and fantasy textures and also shapes beyond the generation capability of the SVR model; see Figure 1 (b) for examples. Furthermore, our approach is compatible with different SVR models (Niemeyer et al., 2020; Alwala et al., 2022). For example, we can adopt SS3D (Alwala et al., 2022) to generate shapes from single-view in-the-wild images to broaden the range of categorical 3D shapes that our approach can generate, going beyond Sanghi et al. (2022), which can only generate 13 categories of ShapeNet. Besides, our approach can also work with the very recent approach GET3D (Gao et al., 2022) to generate high-quality 3D shapes from text; see our results in Section 4.\n\n2 RELATED WORKS\n\nText-guided image generation. Existing text-guided image generation approaches can be roughly cast into two branches: (i) direct image synthesis (Reed et al., 2016a;b; Zhang et al., 2017; 2018;\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nXu et al., 2018; Li et al., 2019; 2020; Qiao et al., 2019; Wang et al., 2021) and (ii) image generation with a pre-trained GAN (Stap et al., 2020; Yuan & Peng, 2019; Souza et al., 2020; Wang et al., 2020; Rombach et al., 2020; Patashnik et al., 2021; Xia et al., 2021). Yet, the above works can only generate images for limited categories. To address this issue, some recent works explore zero-shot text-guided image generation (Ramesh et al., 2021; Ding et al., 2021; Nichol et al., 2021; Liu et al., 2021; Ramesh et al., 2022) to learn to produce images of any category. Recently, Zhou et al. (2022) and Wang et al. (2022b) leverage CLIP for text-free text-to-image generation. Text-guided shape generation is more challenging compared with text-to-image generation. First, it is far more labor-intensive and difficult to prepare a large amount of paired text-shape data than paired text-image data, which can be collected from the Internet on a large scale. Second, the text-to-shape task requires one to predict full 3D structures that should be plausible geometrically and consistently in all views, beyond the needs in single-view image generation. Third, 3D shapes may exhibit more complex spatial structures and topology, beyond regular grid-based 2D images.\n\nText-guided 3D generation. To generate shapes from text, several works (Chen et al., 2018; Jahan et al., 2021; Liu et al., 2022) rely on paired text-shape data for training. To avoid paired text-shape data, two very recent works, CLIP-Forge (Sanghi et al., 2022) and Dream Fields (Jain et al., 2022), attempt to leverage the large-scale pre-trained vision-language model CLIP. Yet, they still suffer from various limitations, as discussed in the third paragraph of Section 1. Besides 3D shape generation, some recent works utilize CLIP to manipulate a shape or NeRF with text (Michel et al., 2022; Jetchev, 2021; Wang et al., 2022a) and to generate 3D avatars (Hong et al., 2022). In this work, we present a new framework for generating 3D shape from text without paired text-shape data by our novel two-stage feature-space alignment. Our experimental results demonstrate the superiority of this work beyond the existing ones in terms of fidelity and text-shape consistency.\n\nSingle-view reconstruction. Another topic related to this work is single-view reconstruction (SVR). Recently, researchers have explored SVR with meshes (Agarwal & Gopi, 2020), voxels (Zubi ́c & Liò, 2021), and 3D shapes (Niemeyer et al., 2020). Further, to extend SVR to in-the-wild categories, Alwala et al. (2022) propose SS3D to learn 3D shape reconstruction using single-view images in hundreds of categories. In our work, we propose to harness an SVR model to map images to shapes, such that we can take 2D image as a stepping stone for producing shapes from texts. Yet, we perform the mapping and feature alignment implicitly in the latent space rather than explicitly.\n\n3 METHODOLOGY\n\n3.1 OVERVIEW\n\nThis work aims to generate 3D shape S from text T . Overall, our idea is to map the CLIP features to the shape space of a pre-trained SVR model, such that we can leverage the joint text-image embeddings from CLIP and also the 3D generation capability of the SVR model to enhance the generation of 3D shape from text. Hence, our method only needs to be trained with multi-view RGB or RGBD images and the associated camera poses without paired text-shape data. As Figure 2 shows, our framework includes (i) image encoder ES, which maps input image I to SVR shape space ΩS, (ii) pre-trained CLIP text and image encoders ET and EI, which map text T and image I to CLIP spaces ΩT and ΩI, respectively, (iii) mapper M with 12 fully-connected layers, each followed by a Leaky-ReLU, and (iv) decoder D to generate the final shape S. Specifically, we use DVR (Niemeyer et al., 2020) as the SVR model when presenting our method, unless otherwise specified.\n\nOverall, we introduce a novel two-stage feature-space-alignment approach to connect the text, image, and shape modalities. In detail, we first train CLIP2Shape mapper M to connect the CLIP image space ΩI and the shape space ΩS from the pre-trained SVR model (see Figure 2 (a)). Then, we fine-tune M at test time using a CLIP consistency loss Lc to further connect the CLIP text space ΩT with ΩS (see Figure 2 (b)). Last, we may further optimize the texture and structure style of S by fine-tuning the decoders (see Figure 2 (c)).\n\nIn the following, we first introduce two empirical studies on the CLIP feature space in Section 3.2, then present our two-stage feature-space-alignment approach in Section 3.3. Further, Section 3.4 presents our text-guided shape stylization method and Section 3.5 discusses the compatibility of our approach with different SVR models and our extension to generate a broad range of categories.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Overview of our text-guided 3D shape generation framework, which has three major stages. (a) Leveraging a pre-trained SVR model, in stage-1 feature-space alignment, we train the CLIP2Shape mapper M to map the CLIP image feature space ΩI to shape space ΩS of the SVR model with ES, EI frozen, and fine-tune decoder D with an additional background loss Lbg. M and D are trained with their own losses separately at the same time by stopping the gradients from SVR loss LD and background loss Lbg to propagate to M . (b) In stage-2 feature-space alignment, we fix D and fine-tune M into M ′ by encouraging CLIP consistency between input text T and the rendered images at test time. (c) Last, we optimize the style of the generated shape and texture of S for T . At the inference, we use stage 2 to generate 3D shape from T and (c) is optional.\n\nFigure 3: Empirical studies on the CLIP feature space for text-guided 3D shape generation.\n\n3.2 EMPIRICAL STUDIES AND MOTIVATIONS\n\nExisting works (Sanghi et al., 2022; Zhou et al., 2022; Wang et al., 2022b) mostly utilize CLIP directly without analyzing how it works and discussing its limitations. To start, we investigate the merits and drawbacks of leveraging CLIP for text-guided 3D shape generation by conducting the following two empirical studies to gain more insight into the CLIP feature space.\n\n3.2.1 WHETHER CLIP FEATURE SPACE SUFFICIENTLY GOOD FOR 3D SHAPE GENERATION?\n\nFirst, we study the representative capability of the CLIP image feature space ΩI by trying to generate 3D shapes from this space. Specifically, we replace the SVR image encoder ES with the CLIP image encoder EI, and optimize implicit decoder D using multi-view losses like DVR (Niemeyer et al., 2020) with EI frozen. This approach can be extended to text-to-shape generation by replacing EI with CLIP text encoder ET during the inference. To compare the performance of ES and EI, we evaluate 3D mIoU between their generated shapes and GTs. The results are as follows: the standard SVR pipeline ES+D achieves 47.86% mIoU while replacing the SVR encoder ES with CLIP encoder EI (EI+D) degrades the performance to 40.09%. From the results and qualitative comparison shown in Figures 3 (a, b), we can see that the CLIP image space ΩI has inferior representative capability to capture details of the input image for 3D shape generation. This is not surprising, since the pre-trained EI from CLIP is targeted to extract semantic-aligned features from texts rather than extracting details from images. Hence, image details relevant to 3D reconstruction are lost, e.g., textures. On the contrary, ES from the SVR model is optimized for 3D generation from images, so it maintains more\n\n4\n\n( )( )A wooden table. ΩIΩSΩTA mahogany table.(a) Stage 1: LD+Lbg(b) Stage 2(c) Text-Guided Stylization: LCESEIETDMFrozenUpdateTTSISM→M′M′DoDcETΩT: LMR1R2Rr...SR1R2Rr...R1R2Rr...[ ]: LC+LP( ) Texture stylization[ ] Shape-and-texture stylizationDStop gradient: LC+LbgSD(a) 3D reconstruction visualization (b) 3D reconstruction results (c) Average feature distance on the test set0.58M(fT)M(fI)fSM′(fT)fS: target shape featureM(fI):mapped image featureM(fT):mapped text featureM′(fT):mapped text feature after stage-2 304050Es+DEI+D47.8640.09mIoU(%)Input image ES+DEI+DPublished as a conference paper at ICLR 2023\n\nnecessary details. The above result motivates us to design a mapper M from ΩI to ΩS and then generate shapes from ΩS instead of ΩI for better generative fidelity.\n\n3.2.2 HOW THE CLIP IMAGE AND TEXT FEATURE GAP INFLUENCES 3D SHAPE GENERATION?\n\nSecond, we investigate the gap between the normalized CLIP image feature fI ∈ ΩI and normalized CLIP text feature fT ∈ ΩT; (see also the CLIP image and text feature spaces in Figure 1 (a)) and how such gap influences text-guided 3D shape generation. Specifically, we randomly sample 300 textshape pairs from the text-shape dataset (Chen et al., 2018), then evaluate the cosine distance between fI and fT, i.e., d = 1 − cosine_similarity(fI, fT), where fI is the CLIP feature of the rendered images from the corresponding shape. We repeat the experiment and obtain d(fT, fI) = 0.783 ± 0.004. The result reveals a certain gap between the CLIP text and image features in this dataset, even though they are paired. Also, the angle in the feature space between the two features is around arccos(1 − 0.783) = 1.35 rad in this dataset (Chen et al., 2018). Having said that, directly replacing fI with fT like Sanghi et al. (2022); Zhou et al. (2022) in inference may harm the consistency between the output shape and the input text. As demonstrated in Figure 3 (c), directly replacing fI with fT causes a cosine distance of 0.45 to fS ∈ ΩS (see Figure 3 (c)), which is significantly larger than the distance between M (fI) and fS (0.21). Our finding is consistent with the findings in Liang et al. (2022). It motivates us to further fine-tune M into M ′ at test time, such that we can produce feature M ′(fT), which is closer to fS than M (fT).\n\n3.3 TWO-STAGE FEATURE-SPACE ALIGNMENT\n\nFollowing the above findings, we propose a two-stage feature-space-alignment approach to first connect image space ΩI and shape space ΩS and further connect text space ΩT to shape space ΩS with the image space ΩI as the stepping stone.\n\nStage-1 alignment: CLIP image-to-shape mapping. Given multi-view RGB or RGBD images for training, the stage-1 alignment is illustrated in Figure 2 (a). Considering that shape space ΩS contains richer object details than the image space ΩI, while ΩI provides a joint text-image embedding with the input text space ΩT, we introduce a fully-connected CLIP2Shape mapper M to map image feature fI to shape space ΩS. Taking a rendered image I as input, M is optimized with an L2 regression between M (fI) and standard SVR feature fS = ES(I) according to Equation (1) below:\n\nLM =\n\nN (cid:88)\n\ni=1\n\n||M (fI,i) − ES(Ii)||2\n\n2\n\n(1)\n\nwhere N is the number of images in the training set and fI,i is the normalized CLIP feature of Ii.\n\nAlso, we fine-tune decoder D to encourage it to predict a white background, which helps the model to ignore the background and extract object-centric feature (see Figure 4), while maintaining its 3D shape generation capability. To this end, we propose a new background loss Lbg in Equation (2) below to enhance the model’s foreground object awareness to prepare for the second-stage alignment.\n\nLbg =\n\n(cid:88)\n\np\n\n||Dc(p) − 1||2\n\n2\n\n1(ray(o, p) ∩ F = ∅)\n\n(2)\n\nwhere 1 is the indicator function; F = {p : Do(p) > t} indicates the foreground region, in which the occupancy prediction Do(p) is larger than threshold t; p is a query point; ray(o, p) ∩ F = ∅ means the ray from camera center o through p does not intersect the foreground object marked by F ; and Dc(p) is the color prediction at query point p. In a word, Lbg encourages D to predict the background region as white color (value 1), such that EI can focus on and better capture the foreground object. In addition, to preserve the 3D shape generation capability of D, we follow the loss LD, with which D has been optimized in the SVR training. In this work, we adopt Niemeyer et al. (2020).\n\nHence, the overall loss in stage-1 training is λM LM for mapper M and λbgLbg + LD for decoder D, where λM and λbg are weights. The stage-1 alignment provides a good initialization for the test-time optimization of stage 2.\n\nStage-2 alignment: text-to-shape optimization. Given a piece of text, the stage-2 alignment aims to further connect the text and shape modalities. Specifically, it searches for shape S that best matches\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Effect of generating shapes from the same text with/without background loss Lbg.\n\nthe input text T . To this end, we formulate a fast test-time optimization to reduce the gap between the text and image CLIP features fT and fI, as discussed earlier in the second empirical study.\n\nAs shown in Figure 2 (b), given input text T , we replace image encoder EI with text encoder ET to extract CLIP text feature fT, then fine-tune M with CLIP consistency loss between input text T and m images {Ri}m\n\ni=1 rendered with random camera poses from output shape S; see Equation 3:\n\nLC =\n\nm (cid:88)\n\ni=1\n\n⟨fT ·\n\nEI(Ri) ||EI(Ri)||\n\n⟩\n\n(3)\n\nwhere ⟨·⟩ indicates the inner-product.\n\nIn stage-2 alignment, we still adopt Lbg to enhance the model’s foreground awareness. Comparing Figures 4 (a) and (b), we can see that the stage-2 alignment is able to find a rough shape with Lbg in around five iterations, yet failing to produce a reasonable output without Lbg, since having the same color prediction on both foreground and background hinders the object awareness of the model.\n\nThanks to the joint text-image embedding of CLIP, the gap between text feature fT and shape feature fS has already been largely narrowed by M . Therefore, the stage-2 alignment only needs to fine-tune M with 20 iterations using the input text, taking only around 85 seconds on a single GeForce RTX 3090 Ti, compared with 72 minutes taken by Dream Fields (Jain et al., 2022) at test time. After this fine-tuning, we can readily obtain a plausible result; see, e.g., the “result” shown in Figure 4 (b). Our ISS is a novel and efficient approach for 3D shape generation from text.\n\nDiversified generation. In general, shape generation from text is one-to-many. Hence, we further extend our approach with diversified 3D shape generation from the same piece of input text. Unlike the existing works, which require additional and complex modules, e.g., GANs (Chen et al., 2018), IMLE (Liu et al., 2022), and normalizing flow network (Sanghi et al., 2022), we can simply perturb the image and text features for diversified generation. Specifically, after stage-1 alignment, we randomly permute fI as an initialization and fT as the ground truth by adding normalized Gaussian noises z1 = h1/||h1||, z2 = h2/||h2||, where h1, h2 ∼ N (0, 1) to derive diversified features\n\nˆfI = τ1fI + (1 − τ1)z1 and ˆfT = τ2fT + (1 − τ2)z2,\n\n(4)\n\nwhere τ1, τ2 are hyperparameters to control the degrees of permutation. With permuted ˆfI and ˆfT in stage-2 alignment, our model can converge to different 3D shapes for different noise.\n\n3.4 TEXT-GUIDED STYLIZATION\n\nThe two-stage feature-space alignment is already able to generate plausible 3D shapes; see, e.g., Figures 2 (b) and 4 (b). However, the generative space is limited by the representation capability of the employed SVR model, e.g., DVR (Niemeyer et al., 2020) can only generate shapes with limited synthetic patterns as those in ShapeNet. However, a richer and wider range of structures and textures are highly desired. To this end, we equip our model with a text-guided stylization module to enhance the generated shapes with novel structure and texture appearances, as shown in Figures 2 (c) and 1.\n\nSpecifically, for texture stylization, we first duplicate D (except for the output layer) to be Do and Dc, then put the output occupancy prediction layer and color prediction layer on top of Do and Dc, respectively. Further, we fine-tune Dc with the same CLIP consistency loss as in Equation (3), encouraging the consistency between input text T and the m rendered images {Ri}m\n\ni=1.\n\nBesides textures, novel structures are also desirable for shape stylization. Hence, we further incorporate a shape-and-texture stylization strategy to create novel structures. To enable shape sculpting, we fine-tune D with the same CLIP consistency loss in Equation 3. At the same time, to maintain the\n\n6\n\n(a) without Lbg(b) with LbgA red car.iter 0 iter 5 iter 10 iter 15 resultiter 0 iter 5 iter 10 iter 15 resultPublished as a conference paper at ICLR 2023\n\noverall structure of the initial shape S, we propose a 3D prior loss LP shown in Equation (5), aiming at preserving the 3D shape prior learned by the two-stage feature-space alignment.\n\nLP =\n\n(cid:88)\n\np\n\n|Do(p) − D′\n\no(p)|\n\n(5)\n\nwhere p is the query point, and Do, D′ o are the occupancy predictions of the initial D and the finetuned D in the stylization process, respectively. To improve the consistency between the generated texture and the generated shape, we augment the background color of Ri with a random RGB value in each iteration. Please find more details in the supplementary material.\n\n3.5 COMPATIBILITY WITH DIFFERENT SVR MODELS\n\nBesides DVR (Niemeyer et al., 2020), our ISS framework is compatible with different SVR models. For example, we can adapt it with the most recent SVR approach SS3D (Alwala et al., 2022) that leverages in-the-wild single images for 3D generation. With this model, our framework can generate a wider range of shape categories by using SS3D’s encoder and decoder as shape encoder ES and decoder D in our framework, respectively. Here, we simply follow the same pipeline as in Figure 2 to derive a text-guided shape generation model for the in-the-wild categories; see our results in Section 4.4. Notably, we follow the losses in Alwala et al. (2022) in place of LD (see Section 3.3) in stage-1 training, requiring only single-view images without camera poses. More importantly, our approach’s high compatibility suggests that it is orthogonal to SVR, so its performance can potentially be further upgraded with more advanced SVR approaches in the future.\n\n4 EXPERIMENTS\n\n4.1 DATASETS, IMPLEMENTATION DETAILS, AND METRICS\n\nWith multi-view RGB or RGBD images and camera poses, we can train ISS on the synthetic dataset ShapeNet (Chang et al., 2015) (13 categories) and the real-world dataset CO3D (Reizenstein et al., 2021) (50 categories). To evaluate our generative performance, we create a text description set with four texts per category on ShapeNet and two texts per category on CO3D. SS3D (Alwala et al., 2022) takes single-view in-the-wild images in training; as their data has not been released, we only evaluate our method on some of their categories. To evaluate the performance, we employ Fréchet Inception Distance (FID) (Heusel et al., 2017), Fréchet Point Distance (FPD) (Liu et al., 2022) to measure shape generation quality, and conduct a human perceptual evaluation to further assess text-shape consistency. Please refer to the supplementary material for more details on the metrics and implementation details.\n\n4.2 COMPARISONS WITH STATE-OF-THE-ART METHODS\n\nWe compare our approach with existing works (Sanghi et al., 2022; Jain et al., 2022) both qualitatively and quantitatively. For a fair comparison, we use their official codes on GitHub to generate shapes on our text set. Table 1 shows quantitative comparisons, whereas Figure 5 shows the qualitative comparisons. Comparing existing works and ours in Table 1, we can see that our approach outperforms two state-of-the-art works by a large margin for both generative quality and text-shape consistency scores. On the other hand, the qualitative comparisons in Figure 5 (a,b) show that CLIP-Forge (Sanghi et al., 2022) produces low-resolution shapes without texture, and some generated shapes are inconsistent with the input text, e.g., “a wooden boat.” Dream Fields (Jain et al., 2022) cannot generate reasonable shapes from the input text on the top row and its generated shape on the bottom row is also inconsistent with the associated input text. On the contrary, our approach (Figure 5 (i)) can generate high-fidelity shapes that better match the input text. Note that we only utilize two-stage feature-space alignment without stylization in producing our results. Please refer to the supplementary file for more results and visual comparisons.\n\n4.3 ABLATION STUDIES\n\nTo manifest the effectiveness of our approach, we conduct ablation studies on the following baselines (see Table 1 and Figure 5): generate shapes from ΩI (EI + D), optimize stage-2 alignment without stage-1 (w/o stage 1), conduct stage-1 alignment without stage-2 (w/o stage 2), disable the background loss in stage 1, stage 2 and both (w/o Lbg_1, w/o Lbg_2, w/o Lbg), and two additional baselines that first create images and then 3D shapes (GLIDE+DVR, LAFITE+DVR). More details on the setup and analysis are provided in the supplementary material Section 3. Our results outperform all the existing works and baselines in terms of fidelity and text-shape consistency by a large margin.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Quantitative comparisons with existing works and baselines.\n\nMethod Type\n\nMethod\n\nFID (↓)\n\nConsistency Score (%) (↑)\n\nFPD (↓)\n\nA/B/C Test\n\nExisting works\n\nAblation Studies\n\nText2Image+SVR\n\nCLIP-Forge Dream Fields\n\nEI+D w/o Stage 1 w/o Stage 2 w/o Lbg_1 w/o Lbg_2 w/o Lbg\n\nGLIDE+DVR LAFITE+DVR\n\n162.87 181.25\n\n181.88 222.96 202.33 149.45 156.52 178.34\n\n212.41 135.01\n\n41.83 ± 17.62 25.38 ± 12.33\n\n20.97 ± 13.59 1.92 ± 2.22 29.52 ± 14.86 29.45 ± 14.67 31.55 ± 8.87 30.96 ± 15.49\n\n8.85 ± 7.94 52.12 ± 11.05\n\n37.43 N.A.\n\n38.61 79.41 41.71 40.85 38.31 40.98\n\n41.33 37.55\n\n8.90 ± 4.12 N.A.\n\nN.A. N.A. N.A. N.A. N.A. N.A.\n\nN.A. 11.70 ± 4.11\n\nOurs\n\nISS\n\n124.42 ± 5.11\n\n60.0 ± 10.94\n\n35.67 ± 1.09\n\n21.70 ± 5.19\n\nFigure 5: Qualitative comparisons with existing works and baselines.\n\nFigure 6: Our approach is able to generate novel shapes, not in the training set. (a) shows our results and (b,c,d) are the top-three shapes retrieved from the training set.\n\nFigure 7: Our approach can generate diversified results from the same input text.\n\nFigure 8: Results on CO3D dataset. We show two different views of each result.\n\nFigure 9: Text-guided stylization. Left: texture stylization. Right: shape-and-texture stylization.\n\n4.4 MORE ANALYSIS ON GENERATIVE RESULTS OF ISS\n\nNext, we present evaluations on the generative novelty and diversity, as well as the scalability of our two-stage feature-space alignment. Then, we show more text-guided stylization results and how our ISS approach generalizes to a wide range of categories and generates shapes with better fidelity.\n\nGeneration novelty. Our approach is able to generate novel shapes beyond simple retrieval from the training data. As shown in Figure 6, from the input text, we first generate our result in (a) and then take our generated shape to retrieve the top-three closest shapes in the associated training set\n\n8\n\nSofa with legs.(a) CLIP-Forge (b) Dream Fields (c) ECLIP_I+D(d) w/o stage-1 (e) w/o stage-2 (f) w/o Lbg_1(g) w/o Lbg_2(h) w/o Lbg(i) GLIDE+DVR (j) Lafite + DVR (k) OursA green SUV.A wooden boat.A metal bench.A red recliner seems comfortable.(a) (b) (c) (d) (a) (b) (c) (d) A car.A chair.A blue tall vase. A sausage hotdog.A white bowl. A wooden baseball bat. GlacierchairFlamechairMaho-gany chair.Woodenchair.Banana boatAvocadoboatLeafboatTulipboatPublished as a conference paper at ICLR 2023\n\nFigure 10: With single images for training (without camera poses), our approach can produce results for a broad range of categories, by adopting (Alwala et al., 2022). Two different views are rendered.\n\nFigure 11: Results of ISS built upon the SVR model of IM-Net (left two) and GET3D (right four).\n\nbased on the cosine similarity between the CLIP features of the rendered images fI and input text fT as the retrieval metric. Our results with the two-stage feature-space alignment are not in the training sets, showing that our ISS approach can generate novel shapes beyond the training set, even without stylization. It is not surprising, as our approach shares the generative space with the pre-trained SVR model and can potentially generate all shapes that the pre-trained SVR model can produce.\n\nGeneration diversity. By perturbing the features and injecting randomness on initialization, ISS is able to generate diversified results from the same text input. As shown in Figure 7, ISS produces various cars and chairs from the same piece of text. Quantitative results are presented in the supplementary material Section 3.4 where our method achieves a high PS (Point Score).\n\nGeneration fidelity. To assess the capability of ISS in generating realistic real-world 3D shapes, we train the SVR model on the CO3D dataset (Reizenstein et al., 2021) which is a real-world dataset, and ISS leverages the learned feature space for text to shape generation without paired data. As shown in Figure 8, our model is able to generate real-world shapes. As far as know, this is first work that investigates text-guided shape generation and on real-world datasets can generate realistic 3D shapes.\n\nGeneration beyond the capability of SVR model. Our text-guided stylization module equips our model with the capability to generate 3D shapes beyond the SVR model. As shown in Figure 9 and Figure 2 (c), our model is able to create realistic and fantasy novel structures and textures that match text descriptions. Please refer to the supplementary material Section 4 for more details.\n\nGenerality and Scalability of ISS on other SVR models. Our model is generic and can work together with other SVR models. To evaluate the generality and scalablity of our model, we employ SS3D (Alwala et al., 2022), IM-Net (Chen & Zhang, 2019), and GET3D (Gao et al., 2022) as SVR models to provide the feature space. It is worth noting that SS3D is capable of generating shapes of more categories and IM-Net, GET3D can produce high fidelity results. First, as shown in Figure 10, built upon SS3D, our approach can generate shapes of more real-world categories, e.g., bird. Note that our model can generate shapes with comparable or even better qualities compared with initial SS3D model that takes an image as input. Second, when combined with IM-Net and GET3D, our model can fully exploit their generative capabilities and produces high-quality 3D shapes as shown in Figure 11. The above manifests that ISS is generic and compatible to advanced models for generating shapes of more categories and higher qualities.\n\n5 CONCLUSION\n\nIn this paper, we present a novel approach for text-guided 3D shape generation by leveraging the image modality as a stepping stone. Leveraging the joint text-image embeddings from CLIP and 3D shape priors from a pre-trained SVR model, our approach eliminates the need for the paired text and shape data. Technically, we have the following contributions. First, we step-by-step reduce the semantic gap among the text, image and shape modalities through our two-stage feature-space alignment approach. Second, our text-guided stylization technique effectively enriches our generated shapes with novel structures and textures in various styles. Third, our approach is compatible with various single-view reconstruction approaches and can be further extended to generate a wide range of categories with only single images without camera poses in training. Experiments on ShapeNet, CO3D, and multiple single-image categories manifest the superiority of our framework over the two state-of-the-art methods and various baselines. Limitations are discussed in the supplementary files.\n\n9\n\n(a) SS3D with image inputYellow hairdryer with black handle.(b) Ours with text inputRed yellow green parrot.Blue earth.Long table has four thin legs and connections on legs sides.Square front view,square shape chair. A greenmotorcycle.A bluejeep.A black swivel chair.Atall grey rectangular shape table with a unique base.Published as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nThe work has been supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Project no. CUHK 14206320), General Research Fund of Hong Kong (No. 17202422), Hong Kong Research Grant Council - Early Career Scheme (Grant No. 27209621), and National Natural Science Foundation of China (No. 62202151).\n\nREFERENCES\n\nNitin Agarwal and M Gopi. Gamesh: Guided and augmented meshing for deep point networks. In\n\n3DV, 2020.\n\nKalyan Vasudev Alwala, Abhinav Gupta, and Shubham Tulsiani. Pre-train, self-train, distill: A\n\nsimple recipe for supersizing 3D reconstruction. CVPR, 2022.\n\nAngel X. Chang, Thomas Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], 2015.\n\nKevin Chen, Christopher B Choy, Manolis Savva, Angel X Chang, Thomas Funkhouser, and Silvio Savarese. Text2shape: Generating shapes from natural language by learning joint embeddings. In ACCV, 2018.\n\nZhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR, 2019.\n\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 2021.\n\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. GET3D: A generative model of high quality 3D textured shapes learned from images. NeurIPS, 2022.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. NIPS, 2017.\n\nFangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. ACM TOG (SIGGRAPH), 2022.\n\nTansin Jahan, Yanran Guan, and Oliver van Kaick. Semantics-guided latent space exploration for\n\nshape generation. In COMPUT GRAPH FORUM, 2021.\n\nAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\n\nobject generation with Drefam Fields. In CVPR, 2022.\n\nNikolay Jetchev. ClipMatrix: Text-controlled creation of 3D textured meshes. arXiv preprint\n\narXiv:2109.12922, 2021.\n\nBowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip H. S. Torr. Controllable text-to-image\n\ngeneration. NeurIPS, 2019.\n\nBowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip H. S. Torr. ManiGAN: Text-guided image\n\nmanipulation. In CVPR, 2020.\n\nWeixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. arXiv preprint arXiv:2203.02053, 2022.\n\nXingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. FuseDream: Training-free text-to-image generation with improved CLIP+ GAN space optimization. arXiv preprint arXiv:2112.01573, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZhengzhe Liu, Yi Wang, Xiaojuan Qi, and Chi-Wing Fu. Towards implicit text-guided 3D shape\n\ngeneration. In CVPR, 2022.\n\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven\n\nneural stylization for meshes. In CVPR, 2022.\n\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n\nMichael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric\n\nrendering: Learning implicit 3D representations without 3D supervision. In CVPR, 2020.\n\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. StyleCLIP:\n\nText-driven manipulation of StyleGAN imagery. ICCV, 2021.\n\nTingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao. MirrorGAN: Learning text-to-image\n\ngeneration by redescription. In CVPR, 2019.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\n\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.\n\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\n\nGenerative adversarial text to image synthesis. In ICML, 2016a.\n\nScott E. Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee.\n\nLearning what and where to draw. NIPS, 2016b.\n\nJeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction. In ICCV, 2021.\n\nRobin Rombach, Patrick Esser, and Björn Ommer. Network-to-network translation with conditional\n\ninvertible neural networks. NeurIPS, 2020.\n\nAditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, and Marco Fumero.\n\nCLIP-Forge: Towards zero-shot text-to-shape generation. In CVPR, 2022.\n\nDouglas M. Souza, Jônatas Wehrmann, and Duncan D. Ruiz. Efficient neural architecture for\n\ntext-to-image synthesis. In IJCNN, 2020.\n\nDavid Stap, Maurits Bleeker, Sarah Ibrahimi, and Maartje ter Hoeve. Conditional image generation\n\nand manipulation for user-specified content. CVPRW, 2020.\n\nCan Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. CLIP-NeRF: Text-and-\n\nimage driven manipulation of neural radiance fields. In CVPR, 2022a.\n\nHao Wang, Guosheng Lin, Steven Hoi, and Chunyan Miao. Cycle-consistent inverse GAN for\n\ntext-to-image synthesis. ACM MM, 2021.\n\nZihao Wang, Wei Liu, Qian He, Xinglong Wu, and Zili Yi. CLIP-GEN: Language-free training of a\n\ntext-to-image generator with CLIP. In arXiv preprint arXiv:2203.00386, 2022b.\n\nZixu Wang, Zhe Quan, Zhi-Jie Wang, Xinjian Hu, and Yangyang Chen. Text to image synthesis with\n\nbidirectional generative adversarial network. In ICME, 2020.\n\nWeihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. TediGAN: Text-guided diverse face image\n\ngeneration and manipulation. In CVPR, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks. In CVPR, 2018.\n\nMingkuan Yuan and Yuxin Peng. Bridge-GAN: Interpretable representation learning for text-to-image\n\nsynthesis. IEEE TCSVT, 2019.\n\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N. Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017.\n\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N. Metaxas. StackGAN++: Realistic image synthesis with stacked generative adversarial networks. IEEE TPAMI, 2018.\n\nYufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: Towards language-free training for text-to-image generation. In CVPR, 2022.\n\nNikola Zubi ́c and Pietro Liò. An effective loss function for generating 3D models from single 2D\n\nimage without rendering. arXiv preprint arXiv:2103.03390, 2021.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe paper works on text2shape generation. It proposes to use images as a 'stepping stone' of the generation process. The authors use two mapping processes to map image-shape latent space and image-text latent space sequentially. Furthermore, the authors show the model can stylize both texture and shape structure. The experiment shows the method outperforms two zero-shot text2shape generation baselines.\n\n# Strength And Weaknesses\n\nStrength:\n\n+ The paper proposes a solution to using images as a stepping method for text2shape generation. \n\n+ As a zero-shot generation method, it can generate shapes of diverse categories relatively quickly (85 seconds).\n\n+ Using latent space makes the generation and stylization step very flexible. \n\nWeakness:\n\n- The paper focuses on 'latent space mapping'. However, it didn't provide a systematic evaluation/visualization of how the latent space is mapped. Figure 3 (c) provides only one instance which is not convincing enough for me. \n\n- The paper is hard to follow for me because of the writing. For example: (1) In equation 3, the symbol p jumped out without explanation. To make the paper self-contained, it's better to provide an explanation for all the symbols in the equations. (2) The paper lacks enough background knowledge about the SVR model, like the rendering method and reconstruction losses.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: \n+ Is the image encoder Es fixed or optimized during step 1? In Figure 2, it is better to provide the \"lock/unlock\" icon to Es, Ei, and ET as well, to make the notations consistent. \n\n+ Is the vector direction in Figure 3 (c) reasonable? In step 2, the 𝑀(𝑓𝑇) is optimized to 𝑀(𝑓𝐼), but the vector direction is moving to some other point in the latent space.\n\n+ Analyse and Evaluation: How many shapes are generated from the text prompt at the evaluation step? I'm also concerned with the number of instances used for evaluation. If only four texts per ShapeNet category are used, there will be a total of 52 shapes, which is a very limited number of instances to calculate FPD and FID. I suggest the author run the metric multiple times and provide a mean and a derivation.  \n\n+ In the paper, the author mentioned referring to Figure 2 (c)  & Figure 8 (b) for some results, but there is no  Figure 2 (c) or Figure 8 (b) in the paper. Is that false referred? \n\n+ Ablations of background loss. Is the background loss removed at stage 1, stage 2, or both?\n\nQuality\n+ Fair. But the paper is hard to follow for me because of lacking background, unexplained symbols and notations, and false references. \n\nNovelty. \n+ Good. \n\nReproducibility.\n+ Good.\n\n# Summary Of The Review\n\nThe paper proposes an interesting solution for the text2shape generation problem. I like the method itself. However, the author could better explain the background, notation, and equations better. I'm willing to improve the score if the author could clarify my concerns.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nD4AM: A GENERAL DENOISING FRAMEWORK FOR DOWNSTREAM ACOUSTIC MODELS\n\nChi-Chang Lee1,2, Yu Tsao2, Hsin-Min Wang2, Chu-Song Chen1,2 1National Taiwan University, Taipei, Taiwan 2Academia Sinica, Taipei, Taiwan\n\nABSTRACT\n\nThe performance of acoustic models degrades notably in noisy environments. Speech enhancement (SE) can be used as a front-end strategy to aid automatic speech recognition (ASR) systems. However, existing training objectives of SE methods are not fully effective at integrating speech-text and noisy-clean paired data for training toward unseen ASR systems. In this study, we propose a general denoising framework, D4AM, for various downstream acoustic models. Our framework fine-tunes the SE model with the backward gradient according to a specific acoustic model and the corresponding classification objective. In addition, our method aims to consider the regression objective as an auxiliary loss to make the SE model generalize to other unseen acoustic models. To jointly train an SE unit with regression and classification objectives, D4AM uses an adjustment scheme to directly estimate suitable weighting coefficients rather than undergoing a grid search process with additional training costs. The adjustment scheme consists of two parts: gradient calibration and regression objective weighting. The experimental results show that D4AM can consistently and effectively provide improvements to various unseen acoustic models and outperforms other combination setups. Specifically, when evaluated on the Google ASR API with real noisy data completely unseen during SE training, D4AM achieves a relative WER reduction of 24.65% compared with the direct feeding of noisy input. To our knowledge, this is the first work that deploys an effective combination scheme of regression (denoising) and classification (ASR) objectives to derive a general pre-processor applicable to various unseen ASR systems. Our code is available at https://github.com/ChangLee0903/D4AM.\n\n1\n\nINTRODUCTION\n\nSpeech enhancement (SE) aims to extract speech components from distorted speech signals to obtain enhanced signals with better properties (Loizou, 2013). Recently, various deep learning models (Wang et al., 2020; Lu et al., 2013; Xu et al., 2015; Zheng et al., 2021; Nikzad et al., 2020) have been used to formulate mapping functions for SE, which treat SE as a regression task trained with noisy-clean paired speech data. Typically, the objective function is formulated using a signal-level distance measure (e.g., L1 norm (Pandey & Wang, 2018; Yue et al., 2022), L2 norm (Ephraim & Malah, 1984; Yin et al., 2020; Xu et al., 2020), SI-SDR (Le Roux et al., 2019; Wisdom et al., 2020; Lee et al., 2020), or multiple-resolution loss (Défossez et al., 2020)). In speech-related applications, SE units are generally used as key pre-processors to improve the performance of the main task in noisy environments. To facilitate better performance on the main task, certain studies focus on deriving suitable objective functions for SE training.\n\nFor human-human oral communication tasks, SE aims to improve speech quality and intelligibility, and enhancement performance is usually assessed by subjective listening tests. Because largescale listening tests are generally prohibitive, objective evaluation metrics have been developed to objectively assess human perception of a given speech signal (Rix et al., 2001; Taal et al., 2010; Jensen & Taal, 2016; Reddy et al., 2021). Perceptual evaluation of speech quality (PESQ) (Rix et al., 2001) and short-time objective intelligibility (STOI) (Taal et al., 2010; Jensen & Taal, 2016) are popular objective metrics designed to measure speech quality and intelligibility, respectively. Recently, DNSMOS (Reddy et al., 2021) has been developed as a non-instructive assessment tool that predicts human ratings (MOS scores) of speech signals. In order to obtain speech signals with\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nimproved speech quality and intelligibility, many SE approaches attempt to formulate objective functions for SE training directly according to speech assessment metrics (Fu et al., 2018; Fu et al., 2019; 2021). Another group of approaches, such as deep feature loss (Germain et al., 2019) and HiFi-GAN (Su et al., 2020), propose to perform SE by mapping learned noisy latent features to clean ones. The experimental results show that the deep feature loss can enable the enhanced speech signal to attain higher human perception scores compared with the conventional L1 and L2 distances.\n\nAnother prominent application of SE is to improve automatic speech recognition (ASR) in noise (Seltzer et al., 2013; Weninger et al., 2015b; Li et al., 2014; Cui et al., 2021). ASR systems perform sequential classification, mapping speech utterances to sequences of tokens. Therefore, the predictions of ASR systems highly depend on the overall structure of the input utterance. When regard to noisy signals, ASR performance will degrade significantly because noise interference corrupts the content information of the structure. Without modifying the ASR model, SE models can be trained separately and “universally” used as a pre-processor for ASR to improve recognition accuracy. Several studies have investigated the effectiveness of SE’s model architecture and objective function in improving the performance of ASR in noise (Geiger et al., 2014a; Wang et al., 2020; Zhang et al., 2020; Chao et al., 2021; Meng et al., 2017; Weninger et al., 2015a; Du et al., 2019; Kinoshita et al., 2020; Meng et al., 2018). The results show that certain specific designs, including model architecture and input format, are favorable for improving ASR performance. However, it has also been reported that improved recognition accuracy in noise is not always guaranteed when the ASR objective is not considered in SE training (Geiger et al., 2014b). A feasible approach to tune the SE model parameters toward the main ASR task is to prepare the data of (noisy) speech-text pairs and backpropagate gradients on the SE model according to the classification objective provided by the ASR model. That is, SE models can be trained on a regression objective (using noisy-clean paired speech data) or/and a classification objective (using speech-text paired data). Ochiai et al. (2017a;b) proposed a multichannel end-to-end (E2E) ASR framework, where a mask-based MVDR (minimum variance distortionless response) neural beamformer is estimated based on the classification objective. Experimental results on CHiME-4 (Jon et al., 2017) confirm that the estimated neural beamformer can achieve significant ASR improvements under noisy conditions. Meanwhile, Chen et al. (2015) and Ma et al. (2021) proposed to train SE units by considering both regression and classification objectives, and certain works (Chen et al., 2015; Ochiai et al., 2017a;b) proposed to train SE models with E2E-ASR classification objectives. A common way to combine regression and classification objectives is to use weighting coefficients to combine them into a joint objective for SE model training.\n\nNotwithstanding promising results, the use of combined objectives in SE training has two limitations. First, how to effectively combine regression and classification objectives remains an issue. A largescale grid search is often employed to determine optimal weights for regression and classification objectives, which requires exhaustive computational costs. Second, ASR models are often provided by third parties and may not be accessible when training SE models. Moreover, due to various training settings in the acoustic model, such as label encoding schemes (e.g., word-piece (Schuster & Nakajima, 2012), byte-pair-encoding (BPE) (Gage, 1994; Sennrich et al., 2016), and character), model architectures (e.g., RNN (Chiu et al., 2018; Rao et al., 2017; He et al., 2019; Sainath et al., 2020), transformer (Vaswani et al., 2017; Zhang et al., 2020), and conformer (Gulati et al., 2020)), and objectives (e.g., Connectionist Temporal Classification (CTC) (Graves et al., 2006), Attention (NLL) (Chan et al., 2016), and their hybrid version (Watanabe et al., 2017)), SE units trained according to a specific acoustic model may not generalize well to other ASR systems. Based on the above limitations, we raise the question: Can we effectively integrate speech-text and noisy-clean paired data to develop a denoising pre-processor that generalizes well to unseen ASR systems?\n\nIn this work, we derive a novel denoising framework, called D4AM, to be used as a “universal” pre-processor to improve the performance of various downstream acoustic models in noise. To achieve this goal, the proposed framework focuses on preserving the integrity of clean speech signals and trains SE models with regression and classification objectives jointly. By using the regression objective as an auxiliary loss, we circumvent the need to require additional training costs to grid search the appropriate weighting coefficients for the regression and classification objectives. Instead, D4AM applies an adjustment scheme to determine the appropriate weighting coefficients automatically and efficiently. The adjustment scheme is inspired by the following concepts: (1) we attempt to adjust the gradient yielded by a proxy ASR model so that the SE unit can be trained to improve the general recognition capability; (2) we consider the weighted regression objective as a regularizer and, thereby,\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nprevent over-fitting while training the SE unit. For (1), we derive a coefficient αgclb (abbreviation for gradient calibration) according to whether the classification gradient conflicts with the regression gradient. When the inner product of the two gradient sets is negative, αgclb is the projection of the classification gradient on the regression gradient; otherwise, it is set to 0. For (2), we derive a coefficient αsrpr (abbreviation for surrogate prior) based on an auxiliary learning method called ARML (auxiliary task reweighting for minimum-data learning) (Shi et al., 2020) and formulate the parameter distribution induced by the weighted regression objective as the surrogate prior when training the SE model. From the experimental results on two standard speech datasets, we first notice that by properly combining regression and classification objectives, D4AM can effectively improve the recognition accuracy of various unseen ASR systems and outperform the SE models trained only with the classification objective. Next, by considering the regression objective as an auxiliary loss, D4AM can be trained efficiently to prevent over-fitting even with limited speech–text paired data. Finally, D4AM mostly outperforms grid search. The main contribution of this study is two-fold: (1) to the best of our knowledge, this is the first work that derives a general denoising pre-processor applicable to various unseen ASR systems; (2) we deploy a rational coefficient adjustment scheme for the combination strategy and link it to the motivation for better generalization ability.\n\n2 MOTIVATION AND MAIN IDEA\n\nNoise Robustness Strategies for ASR. Acoustic mismatch, often caused by noise interference, is a long-standing problem that limits the applicability of ASR systems. To improve recognition performance, multi-condition training (MCT) (Parihar et al., 2004; Rajnoha, 2009; Zhang et al., 2020) is a popular approach when building practical ASR systems. MCT takes noise-corrupted utterances as augmented data and jointly trains an acoustic model with clean and noisy utterances. In addition to noise-corrupted utterances, enhanced utterances from SE models can also be included in the training data for MCT training. Recently, Prasad et al. (2021) studied advanced MCT training strategies and conducted a detailed investigation of different ASR fine-tuning techniques, including freezing partial layers (Zhang et al., 2020), multi-task learning (Saon et al., 2017; Prasad et al., 2021), adversarial learning (Serdyuk et al., 2016; Denisov et al., 2018; Zhang et al., 2020), and training involving enhanced utterances from various SE models (Braithwaite & Kleijn, 2019; Zhang et al., 2020; Défossez et al., 2020). The results reveal that recognition performance in noise can be remarkably improved by tuning the parameters in the ASR system. However, an ASR system is generally formed by a complex structure and estimated with a large amount of training data. Also, for most users, ASR systems provided by third parties are black-box models whose acoustic model parameters cannot be fine-tuned to improve performance in specific noisy environments. Therefore, tuning the parameters in an ASR system for each specific test condition is usually not a feasible solution for real-world scenarios.\n\nIn contrast, employing an SE unit to process noisy speech signals to generate enhanced signals, which are then sent to the ASR system, is a viable alternative for externally improving the recognition accuracy in noise as a two-stage strategy of robustness. The main advantage of this alternative is that SE units can be pre-prepared for direct application to ASR systems without the need to modify these ASR systems or to know the system setups. Furthermore, ASR is usually trained with a large amount of speech-text pairs, requiring expensive annotation costs; on the contrary, for SE, the required large number of noisy-clean training pairs can be easily synthesized. In this study, we propose to utilize the regression objective as a regularization term, combined with the classification objective, for SE unit training to reduce the need for speech-text data and improve the generalization ability.\n\nRegression Objective as an Auxiliary Loss. Since our proposed framework focuses on improving ASR performance in noise, the classification objective is regarded as the main task, and the regression objective is used as an auxiliary loss. Generally, an auxiliary loss aims to improve the performance of the main task without considering the performance of the auxiliary task itself. The auxiliary task learning framework searches for adequate coefficients to incorporate auxiliary information and thus improve the performance of the main task. Several studies (Hu et al., 2019; Chen et al., 2018; Du et al., 2018; Lin et al., 2019; Dery et al., 2021; Shi et al., 2020) have investigated and confirmed the benefits of auxiliary task learning. In D4AM, given the SE model parameter θ, we consider the classification objective Lcls(θ) as the main task and the regression objective Lreg(θ) as the auxiliary task. Then, we aim to jointly fine-tune the SE model with Lcls(θ) and Lreg(θ), where\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ntheir corresponding training data are Tcls (speech-text paired data) and Treg (noisy-clean paired speech data), respectively. Here, D4AM uses − log p(Tcls|θ) and − log p(Treg|θ), respectively, as the classification objective Lcls(θ) and the regression objective Lreg(θ). Fig. 1a shows the overall training flow of D4AM. Notably, D4AM partially differs from conventional auxiliary task learning algorithms, where the main and auxiliary tasks commonly share the same model parameters. In D4AM, the main task objective Lcls(θ) cannot be obtained with only the SE model. Instead, a proxy acoustic model φ is employed. That is, the true classification objective Lcls(θ) is not attainable, and alternatively, with the proxy acoustic model φ, a surrogate term − log p(Tcls|θ, φ), termed Lφ cls(θ), is used to update the SE parameters. Since there is no guarantee that reducing Lφ cls(θ) will reduce Lcls(θ), a constraint is needed to guide the gradient of Lφ cls(θ) toward improving general recognition performance.\n\nMeanwhile, we believe that, for any particular downstream acoustic recognizer, the best enhanced output produced by the SE unit from any noisy speech input is the corresponding clean speech. Therefore, any critical point of Lcls(θ) should be “covered” by the critical point space of Lreg(θ), and we just need guidance from speech-text paired data to determine the critical point in the Lreg(θ) space. That is, when an update of SE model parameters θ decreases both Lφ cls(θ) and Lreg(θ), that update should be beneficial to general ASR systems. On the other hand, compared to Lcls(θ), the calculation of Lreg(θ) does not require the proxy model when we use it to update SE model parameters. Accordingly, we derive the update of θ at the t-th step as:\n\nθt+1 = arg min\n\nθ\n\nLφ\n\ncls(θ) subject to Lreg(θ) ≤ Lreg(θt).\n\n(1)\n\nAs indicated in Lopez-Paz & Ranzato (2017) and Chaudhry et al. (2019), the model function is supposed to be locally linear around small optimization steps. For ease of computation, we replace Lreg(θ) ≤ Lreg(θt) with ⟨∇Lφ cls(θt), ∇Lreg(θt)⟩ ≥ 0. That is, we take the alignment between ∇Lφ cls(θt) and ∇Lreg(θt) as the constraint to guarantee the generalization ability to unseen ASR systems at each training step. When the constraint is violated, ∇Lφ cls(θt) needs to be calibrated. Following the calibration method proposed in Chaudhry et al. (2019) to search for the non-violating candidate closest to the original ∇Lφ\n\ncls(θt), we formulate the calibrated gradient g∗ as:\n\ng∗ = arg min\n\ng\n\n1 2\n\n∥g − ∇Lφ\n\ncls(θt)∥2 subject to ⟨g, ∇Lreg(θt)⟩ ≥ 0.\n\n(2)\n\nIn addition to guiding SE unit training toward improving ASR performance, D4AM also aims to alleviate over-fitting in the classification task. In ARML (Shi et al., 2020), auxiliary task learning was formulated as a problem of minimizing the divergence between the surrogate prior and true prior. It is shown that when a suitable surrogate prior is identified, the over-fitting issue caused by the data limitation of the main task can be effectively alleviated. Following the concept of ARML, D4AM aims to estimate an appropriate coefficient of Lreg to approximate the log prior of the main task objective Lcls to alleviate over-fitting. Finally, D4AM combines the calibrated gradient g∗ and auxiliary task learning (ARML) to identify a critical point that ensures better generalization to unseen ASR systems and alleviates over-fitting in the classification task. In Sec. 3, we illustrate the details of our proposed framework.\n\n3 METHODOLOGY\n\nD4AM aims to estimate an SE unit that can effectively improve the recognition performance of general downstream ASR systems. As shown in Fig. 1a, the overall objective consists of two parts, the classification objective Lφ cls(θ) given the proxy model φ and the regression objective Lreg(θ). As shown in Fig. 1b, we intend to guide the update of the model from θi to θo with the regression objective Lreg(θ) to improve the performance of ASR in noise. In this section, we present an adjustment scheme to determine a suitable weighting coefficient to combine Lφ cls(θ) and Lreg(θ) instead of a grid search process. The adjustment scheme consists of two parts. First, we calibrate the direction of ∇Lφ cls(θ) to promote the recognition accuracy, as described in Eq. 2. Second, we follow the ARML algorithm to derive a suitable prior by weighting the regression objective. Finally, the gradient descent of D4AM at the t-th step for SE training is defined as:\n\nθt+1 ← θt − εt(∇Lφ\n\ncls(θt) + (αgclb + αsrpr) · ∇Lreg(θt)) + ηt,\n\n(3)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n(a) Training flowchart\n\n(b) Schematic diagram of training trajectory\n\nFigure 1: (a) Training flow of D4AM. Given a proxy acoustic model (AM) parameterized by φ, θ is updated by training with both Lcls and Lreg weighted by α. (b) Illustration of the training trajectories. The dashed line (from θi to θb) represents the training process without considering Lreg. The solid line (from θi to θo) represents the training process considering Lreg.\n\ncls(θt), ∇Lreg(θt)⟩ ≥ 0. Mathematically, αgclb is the result of projecting ∇Lφ\n\nwhere εt is the learning rate, and ηt ∼ N (0, 2εt) is a Gaussian noise used to formulate the sampling of θ. The coefficient αgclb is responsible for calibrating the gradient direction when Lφ cls(θt) does not satisfy ⟨∇Lφ cls(θt) into the space formed by using ∇Lreg(θt) as a basis. The connection between the projection process and the constraint ⟨∇Lφ cls(θt), ∇Lreg(θt)⟩ ≥ 0 is introduced in Section 3.1. The coefficient αsrpr is responsible for weighting Lreg(θt) to approximate the log prior of parameters and is estimated to minimize the divergence between the parameter distributions of the surrogate and true priors. The determination of αsrpr, the sampling mechanism for θ, and the overall framework proposed in ARML are described in Section 3.2. Finally, we introduce the implementation of the D4AM framework in Section 3.3.\n\n3.1 GRADIENT CALIBRATION FOR IMPROVING GENERAL RECOGNITION ABILITY\n\nThis section details the gradient calibration mechanism. First, we assume that any critical point of Lcls(θ) is located in the critical point space of Lreg(θ). When the derived g∗ satisfies ⟨g∗, ∇Lreg(θt)⟩ ≥ 0, the direction of g∗ can potentially yield recognition improvements to general acoustic models. The constrained optimization problem described in Eq. 2 can be solved in a closed form while considering only a mini-batch of samples. We then design a criterion C = ⟨∇Lφ\n\ncls(θt), ∇Lreg(θt)⟩ to examine the gradient direction and formulate the solution as:\n\ng∗ = ∇Lφ\n\ncls(θt) + αgclb · ∇Lreg(θt) , where αgclb = −\n\nC < 0\n\n·\n\nC ∥∇Lreg(θt)∥2\n\n2\n\n,\n\n(4)\n\n(cid:75)\n\n(cid:74)\n\n. (cid:74)\n\nis an indicator function whose output is 1 if the argument condition is satisfied; otherwise, where the output is 0. αgclb is derived based on the Lagrangian of the constrained optimization problem. The details of the derivation are presented in Appendix A.7.\n\n(cid:75)\n\n3.2 REGRESSION OBJECTIVE WEIGHTING AS THE SURROGATE PRIOR\n\nIn this section, we discuss the relationship between the classification and regression objectives with the main ASR task. A weighted regression objective is derived to approximate the log prior for SE training by modifying the ARML (Shi et al., 2020) algorithm, which considers the weighted sum of auxiliary losses as a surrogate prior and proposes a framework for reweighting auxiliary losses based on minimizing the divergence between the parameter distributions of the surrogate and true priors. We aim to identify a coefficient αsrpr such that αsrpr = arg minα Eθ∼p∗ [log p∗(θ) − α · log(p(Treg|θ))], where p∗ is the true prior distribution of θ, and pαsrpr (Treg|θ) is the surrogate prior corresponding to the weighted regression objective −αsrpr · log(p(Treg|θ)). However, p∗(θ) is not practically available. To overcome the unavailability of p∗(θ), ARML uses an alternative solution for p∗(θ) and a sampling scheme of θ in p∗(θ). For the alternative solution of p∗(θ), ARML assumes that the main task distribution p(Tcls|θ) will cover p∗(θ) since the samples of θ from p∗(θ) are likely to induce high values in p(Tcls|θ). Thus, ARML takes log p(Tcls|θ) as the alternative of log p∗(θ). For the sampling scheme of θ in p∗(θ), for convenience, ARML acquires the derived parameters from the joint loss at\n\n5\n\nPublished as a conference paper at ICLR 2023\n\neach training step and combines them with Langevin dynamics (Neal et al., 2011; Welling & Teh, 2011) to approximate the sampling of θ with an injected noise ηt ∼ N (0, 2εt). Thus, the optimization problem of αsrpr becomes minα Eθt∼pJ [log(p(Tcls|θt)) − α · log(p(Treg|θt))], where pJ represents the distribution for sampling θ from the above described joint training process with respect to the original sampling. More details about the ARML algorithm are available in Shi et al. (2020). Finally, at the t-th step, the overall parameter change ∆θ(g) of the auxiliary learning framework given an arbitrary gradient g of the main task (ASR) can be written as:\n\n∆θ(g) = εt(g + αsrpr · ∇Lreg(θt)) + ηt.\n\n(5)\n\nNonetheless, the aforementioned problem remains: we can obtain only ∇Lφ cls(θt) instead of the ideal ASR task gradient ∇Lcls(θt). Therefore, the gradient should be calibrated, as described in Section 3.1. To this end, we update θ by accordingly taking ∆θ(g∗), as described in Eq. 3.\n\nAlgorithm 1 D4AM (A General Denoising Framework for Downstream Acoustic Models)\n\nLearnable-Parameter: model parameter θ (initialized by θ0); task weight αsrpr (initialized by 1) Hyper-Parameter: learning rate of the t-th iteration εt, learning rate of task weight β\n\n3:\n\n1: for iteration t = 0 to T − 1 do αgclb ← project(∇Lφ 2: θt+1 ← θt − εt(∇Lφ gα ← ∂ αsrpr ← αsrpr − β · clamp(gα, max = +1, min = −1)\n\ncls(θt), ∇Lreg(θt)) cls(θt) + (αgclb + αsrpr) · ∇Lreg(θt)) + ηt\n\ncls(θt) + (αgclb − αsrpr) · ∇Lreg(θt))∥2\n\n∥∇Lφ\n\n∂αsrpr\n\n4:\n\n2\n\n5: 6: end for\n\n3.3 ALGORITHM\n\nIn this section, we detail the implementation of the proposed D4AM framework. The complete algorithm is shown in Algorithm 1. The initial value of αsrpr is set to 1, and the learning rate β is set to 0.05. We first update θt by the mechanism presented in Eq. 3 and collect θ samples at each iteration. As described in Section 3.2, αsrpr is optimized to minimize the divergence between p∗(θ) and pαsrpr (Treg|θ). Referring to ARML (Shi et al., 2020), we use the Fisher divergence to measure the difference between the surrogate and true priors. That is, we optimize α by minα Eθt∼pJ [∥∇Lcls(θt) − α · ∇Lreg(θt))∥2 cls(θt), we use the calibrated version g∗ as the alternative of ∇Lcls(θt). Then, the optimization of α becomes minα Eθt∼pJ [∥∇Lφ 2]. In addition, we use gradient descent to approximate the above minimization problem, since it is not feasible to collect all θ samples. Practically, we only have access to one sample of θ per step, which may cause instability while updating αsrpr. Therefore, to increase the batch size of θ, we modify the update scheme by accumulating the gradients of αsrpr. The update period of αsrpr is set to 16 steps. Unlike ARML (Shi et al., 2020), we do not restrict the value of αsrpr to the range 0 to 1, because Lφ cls and Lreg have different scales. In addition, we clamp the gradient of αsrpr with the interval [−1, 1] while updating αsrpr to increase training stability. Further implementation details are provided in Section 4 and Supplementary Material.\n\n2]. Because we can obtain only Lφ\n\ncls(θt) + (αgclb − α) · ∇Lreg(θt))∥2\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\nThe training datasets used in this study include: noise signals from DNS-Challenge (Reddy et al., 2020) and speech utterances from LibriSpeech (Panayotov et al., 2015). DNS-Challenge, hereinafter DNS, is a public dataset that provides 65,303 background and foreground noise signal samples. LibriSpeech is a well-known speech corpus that includes three training subsets: Libri-100 (28,539 utterances), Libri-360 (104,014 utterances), and Libri-500 (148,688 utterances). The mixing process (of noise signals and clean utterances) to generate noisy-clean paired speech data was conducted during training, and the signal-to-noise ratio (SNR) level was uniformly sampled from -4dB to 6dB.\n\nThe SE unit was built on DEMUCS (Défossez et al., 2019; 2020), which employs a skip-connected encoder-decoder architecture with a multi-resolution STFT objective function. DEMUCS has been\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nshown to yield state-of-the-art results on music separation and SE tasks, We divided the whole training process into two stages: pre-training and fine-tuning. For pre-training, we only used Lreg to train the SE unit. At this stage, we selected Libri-360 and Libri-500 as the clean speech corpus. The SE unit was trained for 500,000 steps using the Adam optimizer with β1 = 0.9 and β2 = 0.999, learning rate 0.0002, gradient clipping value 1, and batch size 8. For fine-tuning, we used both Lreg and Lφ cls to re-train the SE model initialized by the checkpoint selected from the pre-training stage. We used Libri-100 to prepare the speech-text paired data for Lφ cls and all the training subsets (including Libri-100, Libri-360, and Libri-500) to prepare the noisy-clean paired speech data for Lreg. The SE unit was trained for 100,000 steps with the Adam optimizer with β1 = 0.9 and β2 = 0.999, learning rate 0.0001, gradient clipping value 1, and batch size 16. For the proxy acoustic model φ, we adopted the conformer architecture and used the training recipe provided in the SpeechBrain toolkit (Ravanelli et al., 2021). The model was trained with all training utterances in LibriSpeech, and the training objective was a hybrid version (Watanabe et al., 2017) of CTC (Graves et al., 2006) and NLL (Chan et al., 2016).\n\nSince the main motivation of our study was to appropriately adjust the weights of the training objectives to better collaborate with the noisy-clean/speech-text paired data, we focused on comparing different objective combination schemes on the same SE unit. To investigate the effectiveness of each component in D4AM, our comparison included: (1) without any SE processing (termed NOIS), (2) solely trained with Lreg in the pre-training stage (termed INIT), (3) solely trained with Lφ cls in the fine-tuning stage (termed CLSO), (4) jointly trained with Lφ cls and Lreg by solely taking αgclb in the fine-tuning stage (termed GCLB), and (5) jointly trained with Lφ cls and Lreg by solely taking αsrpr in the fine-tuning stage (termed SRPR). Note that some of the above schemes can be viewed as partial comparisons with previous attempts. For example, CLSO is trained only with the classification objective, the same as (Ochiai et al., 2017a;b); SRPR, GCLB, and D4AM use joint training, the same as (Chen et al., 2015; Ma et al., 2021), but (Chen et al., 2015; Ma et al., 2021) used predefined weighting coefficients. In our ablation study, we demonstrated that our adjustment achieves optimal results without additional training costs compared with grid search. Since our main goal was to prepare an SE unit as a pre-processor to improve the performance of ASR in noise, the word error rate (WER) was used as the main evaluation metric. Lower WER values indicate better performance.\n\nTo fairly evaluate the proposed D4AM framework, we used CHiME-4 (Jon et al., 2017) and Aurora4 (Parihar et al., 2004) as the test datasets, which have been widely used for robust ASR evaluation. For CHiME-4, we evaluated the performance on the development and test sets of the 1-st track. For Aurora-4, we evaluated the performance on the noisy parts of the development and test sets. There are two types of recording process, namely wv1 and wv2. The results of these two categories are reported separately for comparison. For the downstream ASR systems, we normalized all predicted tokens by removing all punctuation, mapping numbers to words, and converting all characters to their upper versions. To further analyze our results, we categorized the ASR evaluation into out-of-domain and in-domain scenarios. In the out-of-domain scenario, we aim to verify the generalization ability of ASR when the training and testing data originate from different corpora. That is, in this scenario, the testing data is unseen to all downstream ASR systems. In the in-domain scenario, we aim to investigate the improvements achievable in standard benchmark settings. Therefore, we set up training and testing data for ASR from the same corpus. Note that ASR systems typically achieve better performance in the in-domain scenarios than in the out-domain scenario (Likhomanenko et al., 2021).\n\n4.2 EXPERIMENTAL RESULTS\n\nD4AM Evaluated with Various Unseen Recognizers in the Out-of-Domain Scenario. We first investigated the D4AM performance with various unseen recognizers. We prepared these recognizers by the SpeechBrain toolkit with all the training data in LibriSpeech. The recognizers include: (1) a conformer architecture trained with the hybrid loss and encoded by BPE-5000 (termed CONF), (2) a transformer architecture trained with the hybrid loss and encoded by BPE-5000 (termed TRAN), (3) an RNN architecture trained with the hybrid loss and encoded by BPE-5000 (termed RNN), and (4) a self-supervised wav2vec2 (Baevski et al., 2020) framework trained with the CTC loss and encoded by characters (termed W2V2). To prevent potential biases caused by language models, CONF, TRAN, and RNN decoded results only from their attention decoders, whereas W2V2 greedily\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: WER (%) results on CHiME-4 and Aurora-4.\n\nCHiME-4\n\ndt05-real\n\ndt05-simu\n\net05-real\n\net05-simu\n\nCONF TRAN RNN W2V2 CONF TRAN RNN W2V2 CONF TRAN RNN W2V2 CONF TRAN RNN W2V2\n\nNOIS\n\nINIT\n\n40.53\n\n30.98\n\n34.56 56.76 29.84\n\n29.61 38.61 26.93\n\nCLSO 26.66\n\n27.83 35.25 24.63\n\nSRPR 26.88\n\n26.73 34.83 24.51\n\nGCLB 26.76\n\n26.51 34.62 23.55\n\nD4AM 26.70\n\n26.07 34.43 23.30\n\n45.40\n\n35.02\n\n29.09\n\n30.12\n\n29.28\n\n29.45\n\n38.86 58.75 35.37\n\n32.49 41.76 31.84\n\n29.89 38.00 29.16\n\n29.22 38.73 29.86\n\n28.73 37.55 28.54\n\n28.61 37.49 28.40\n\n57.23\n\n40.32\n\n34.00\n\n34.53\n\n34.18\n\n33.71\n\n46.10 72.16 45.61\n\n38.77 49.67 39.37\n\n35.19 46.02 36.08\n\n33.93 46.08 35.21\n\n33.76 45.75 35.15\n\n33.20 45.07 34.43\n\n52.06\n\n42.78\n\n35.02\n\n36.68\n\n35.69\n\n35.97\n\n44.96 64.93 44.10\n\n40.51 49.83 41.04\n\n36.62 45.66 37.23\n\n36.55 46.27 38.80\n\n36.09 45.37 37.18\n\n35.75 45.36 37.30\n\nAurora-4\n\ndev-wv1\n\ndev-wv2\n\ntest-wv1\n\ntest-wv2\n\nCONF TRAN RNN W2V2 CONF TRAN RNN W2V2 CONF TRAN RNN W2V2 CONF TRAN RNN W2V2\n\nNOIS\n\nINIT\n\n21.69\n\n18.97\n\n19.66 27.36 14.44\n\n18.24 21.71 13.59\n\nCLSO 17.75\n\n18.23 20.98 13.31\n\nSRPR 17.77\n\n17.20 20.40 12.97\n\nGCLB 17.70\n\n17.16 20.35 12.91\n\nD4AM 17.73\n\n17.08 20.30 12.84\n\n31.56\n\n27.85\n\n24.76\n\n24.67\n\n24.65\n\n24.31\n\n27.96 42.12 25.47\n\n26.16 34.58 24.38\n\n25.05 33.08 23.48\n\n23.20 31.66 21.57\n\n22.99 31.73 21.92\n\n22.78 31.06 20.90\n\n19.27\n\n16.76\n\n15.43\n\n15.52\n\n15.51\n\n15.55\n\n17.64 23.98 13.60\n\n16.25 18.45 12.26\n\n17.03 17.53 12.16\n\n15.50 16.82 11.89\n\n15.66 16.82 12.11\n\n15.44 17.18 11.66\n\n29.77\n\n26.55\n\n24.24\n\n23.83\n\n24.51\n\n23.35\n\n27.54 38.91 26.37\n\n25.23 31.98 24.65\n\n25.85 32.33 25.79\n\n23.00 29.74 22.95\n\n23.19 30.39 23.36\n\n22.68 29.65 22.02\n\nTable 2: WER (%) results of CHiME-4’s baseline ASR system.\n\nmethod\n\nNOIS\n\nINIT\n\nCLSO SRPR GCLB D4AM\n\ndt05-real\n\n11.57\n\n10.28\n\n7.95\n\ndt05-simu\n\n12.98\n\n11.80\n\n10.07\n\net05-real\n\n23.71\n\n19.91\n\n16.28\n\net05-simu\n\n20.85\n\n20.39\n\n18.37\n\n8.28\n\n10.28\n\n16.33\n\n18.18\n\n8.15\n\n9.56\n\n16.32\n\n16.86\n\n7.94\n\n9.48\n\n15.66\n\n16.31\n\ndecoded results following the CTC rule. Table 1 shows the WER results under different settings. Note that we cascaded CONF to train our SE model, so CONF is not a completely unseen recognizer in our evaluation. Therefore, we marked the results of CONF in gray and paid more attention to the results of other acoustic models. Obviously, NOIS yielded the worst results under all recognition and test conditions. INIT performed worse than others because it did not consider the ASR objective in the pre-training stage. CLSO was worse than GCLB, SRPR, and D4AM, verifying that training with Lφ cls alone does not generalize well to other unseen recognizers. Finally, D4AM achieved the best performance on average, confirming the effectiveness of gradient calibration and using regression objective weighting as a surrogate prior in improving general recognition ability.\n\nD4AM Evaluated with a Standard ASR System in the In-Domain Scenario. In this experiment, we selected the official baseline ASR system from CHiME-4, a DNN-HMM ASR system trained in an MCT manner using the CHiME-4 task-specific training data, as the evaluation recognizer. This baseline ASR system is the most widely used to evaluate the performance of SE units. As observed in Table 2, the result of INIT is comparable to the state-of-the-art results of CHiME-4. Meanwhile, D4AM achieved notable reductions in WER compared with NOIS. This trend is similar to that observed in Table 2 and again validates the effectiveness of D4AM.\n\nD4AM Evaluated with a Practical ASR API. We further evaluated D4AM using the Google ASR API, a well-known black-box system for users. To prepare a “real” noisy testing dataset, we merged the real parts of the CHiME-4 development and test sets. Table 3 shows the average WER values obtained by the Google ASR API. The table reveals trends similar to those observed in Table 1, but two aspects are particularly noteworthy. First, NOIS still yielded the worst result, confirming that all SE units contribute to the recognition accuracy of the Google ASR API on our testing data. Second, CLSO was worse than INIT, confirming that an SE unit trained only by the classification\n\nTable 3: WER (%) results of the Google ASR API on the CHiME-4 real sets.\n\nmethod\n\nNOIS\n\nINIT\n\nCLSO SRPR GCLB D4AM\n\nWER (%)\n\n35.09\n\n28.02\n\n29.26\n\n26.79\n\n26.54\n\n26.44\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nobjective according to one ASR model is not guaranteed to contribute to other unseen recognizers. In contrast, improved recognition results (confirmed by the improvements produced by SRPR, GCLB, and D4AM) can be obtained by well considering the combination of objectives.\n\n4.3 ABLATION STUDIES\n\nRegression Objective Weighting for Handling Over-fitting. As mentioned earlier, ARML addresses the over-fitting issue caused by the limited labeled data of the main task by introducing an appropriate auxiliary loss. For D4AM, we considered the regression objective as an auxiliary loss to alleviate the over-fitting issue. To verify this feature, we reduced the speech-text paired data to only 10% (2853 utterances) of the Libri-100 dataset. The WER learning curves of CLSO and D4AM on a privately mixed validation set within 100,000 training steps are shown in Fig. 2a. The figure illustrates that the learning curve of CLSO ascends after 50,000 training steps, which is a clear indication of over-fitting. In contrast, D4AM displays a different trend. Fig. 2a confirms the effectiveness of using the regression objective to improve generalization and mitigate over-fitting.\n\n(a) Learning curves.\n\n(b) WER (%) results of grid search, SRPR, and D4AM.\n\nFigure 2: (a) Learning curves of CLSO and D4AM trained with 10% transcribed data of Libri-100. (b) Comparison of WER (%) results of grid search, SRPR, and D4AM. The dashed line represents the average performance of the best seven results of grid search.\n\nD4AM versus Grid Search. One important feature of D4AM is that it automatically and effectively determines the weighting coefficients to combine regression and classification objectives. In this section, we intend to compare this feature with the common grid search. Fig. 2b compares the performance of D4AM, SRPR, and grid search with different weights on the CHiME-4 dt05-real set. For the grid search method, the figure shows the results for different weights, including 0, 0.1, 1.0, 10, 20, 30, 40, 50, and 60. The average performance of the best seven results is represented by the dashed green line. From Fig. 2b, we first note that D4AM evidently outperformed SRPR. Furthermore, the result of D4AM is better than most average results of grid search. The results confirm that D4AM can efficiently and effectively estimate coefficients without an expensive grid search process.\n\nTo further investigate its effectiveness, we tested D4AM on high SNR testing conditions, MCTtrained ASR models, and another acoustic model setup. Additionally, we tested human perception assessments using the D4AM-enhanced speech signals. All the experimental results confirm the effectiveness of D4AM. Owing to space limitations, these results are reported in APPENDIX.\n\n5 CONCLUSION\n\nIn this paper, we proposed a denoising framework called D4AM, which aims to function as a preprocessor for various unseen ASR systems to boost their performance in noisy environments. D4AM employs gradient calibration and regression objective weighting to guide SE training to improve ASR performance while preventing over-fitting. The experimental results first verify that D4AM can effectively reduce WERs for various unseen recognizers, even a practical ASR application (Google API). Meanwhile, D4AM can alleviate the over-fitting issue by effectively and efficiently combining classification and regression objectives without the exhaustive coefficient search. The promising results confirm that separately training an SE unit to serve existing ASR applications may be a feasible solution to improve the performance robustness of ASR in noise.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nThe source code for additional details of the model architecture, training process, and pre-processing steps, as well as a demo page, are provided in the supplemental material. Additional experiments are provided in Appendix.\n\nACKNOWLEDGEMENT\n\nThis work was partly supported by the National Science and Technology Council, Taiwan (NSTC 111-2634-F-002-023).\n\nREFERENCES\n\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework\n\nfor self-supervised learning of speech representations. In Proc. NeurIPS, 2020.\n\nD.T. Braithwaite and W. Bastiaan Kleijn. Speech enhancement with variance constrained autoen-\n\ncoders. In Proc. Interspeech, 2019.\n\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural network\n\nfor large vocabulary conversational speech recognition. In Proc. ICASSP, 2016.\n\nFu-An Chao, Shao-Wei Fan-Jiang, Bi-Cheng Yan, Jeih-Weih Hung, and Berlin Chen. Tenet: A time-reversal enhancement network for noise-robust asr. arXiv preprint arXiv:2107.01531, 2021.\n\nArslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient\n\nlifelong learning with a-GEM. In Proc. ICLR, 2019.\n\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient\n\nnormalization for adaptive loss balancing in deep multitask networks. In Proc. ICML, 2018.\n\nZhuo Chen, Shinji Watanabe, Hakan Erdogan, and John R. Hershey. Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks. In Proc. Interspeech, 2015.\n\nChung-Cheng Chiu, Tara Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, and Michiel Bacchiani. State-of-the-art speech recognition with sequence-to-sequence models. In Proc. ICASSP, 2018.\n\nXiaodong Cui, Songtao Lu, and Brian Kingsbury. Federated acoustic modeling for automatic speech\n\nrecognition. In Proc. ICASSP, 2021.\n\nAlexandre Défossez, Nicolas Usunier, Léon Bottou, and Francis Bach. Music source separation in\n\nthe waveform domain. arXiv preprint arXiv:1911.13254, 2019.\n\nAlexandre Défossez, Gabriel Synnaeve, and Yossi Adi. Real time speech enhancement in the\n\nwaveform domain. In Proc. Interspeech, 2020.\n\nPavel Denisov, Ngoc Thang Vu, and Marc Ferras. Unsupervised domain adaptation by adversarial learning for robust speech recognition. In ITG Symposium on Speech Communication, 2018.\n\nLucio M. Dery, Yann Dauphin, and David Grangier. Auxiliary task update decomposition: The good,\n\nthe bad and the neutral. In Proc. ICLR, 2021.\n\nYunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Mehrdad Farajtabar, Razvan Pascanu, and Balaji Lakshminarayanan. Adapting auxiliary losses using gradient similarity. arXiv preprint arXiv:1812.02224, 2018.\n\nZhihao Du, Xueliang Zhang, and Jiqing Han. Investigation of monaural front-end processing for\n\nrobust speech recognition without retraining or joint-training. In Proc. APSIPA ASC, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nY. Ephraim and D. Malah. Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1109–1121, 1984.\n\nS. Fu, T. Wang, Y. Tsao, X. Lu, and H. Kawai. End-to-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1570–1584, 2018.\n\nSzu-Wei Fu, Chien-Feng Liao, Yu Tsao, and Shou-De Lin. MetricGAN: Generative adversarial networks based black-box metric scores optimization for speech enhancement. In Proc. ICML, 2019.\n\nSzu-Wei Fu, Cheng Yu, Tsun-An Hsieh, Peter Plantinga, Mirco Ravanelli, Xugang lu, and Yu Tsao. MetricGAN+: An improved version of MetricGAN for speech enhancement. In Proc. Interspeech, 2021.\n\nPhilip Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, 1994.\n\nJürgen T Geiger, Jort F Gemmeke, Björn Schuller, and Gerhard Rigoll. Investigating NMF speech\n\nenhancement for neural network based acoustic models. In Proc. Interspeech, 2014a.\n\nJürgen T. Geiger, Felix Weninger, Jort F. Gemmeke, Martin Wöllmer, Björn Schuller, and Gerhard Rigoll. Memory-enhanced neural networks and NMF for robust asr. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1037–1046, 2014b.\n\nFrançois Germain, Qifeng Chen, and Vladlen Koltun. Speech denoising with deep feature losses. In\n\nProc. Interspeech, 2019.\n\nAlex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proc. ICML, 2006.\n\nEric Guizzo, Christian Marinoni, Marco Pennese, Xinlei Ren, Xiguang Zheng, Chen Zhang, Bruno Masiero, Aurelio Uncini, and Danilo Comminiello. L3das22 challenge: Learning 3d audio sources in a real office environment. In Proc. ICASSP, 2022.\n\nAnmol Gulati, Chung-Cheng Chiu, James Qin, Jiahui Yu, Niki Parmar, Ruoming Pang, Shibo Wang, Wei Han, Yonghui Wu, Yu Zhang, and Zhengdong Zhang. Conformer: Convolution-augmented transformer for speech recognition. In Proc. Interspeech, 2020.\n\nYanzhang He, Tara N. Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Álvarez, Ding Zhao, David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, Qiao Liang, Deepti Bhatia, Yuan Shangguan, Bo Li, Golan Pundak, Khe Chai Sim, Tom Bagby, Shuo yiin Chang, Kanishka Rao, and Alexander Gruenstein. Streaming end-to-end speech recognition for mobile devices. Proc. ICASSP, 2019.\n\nGuoning Hu and DeLiang Wang. A tandem algorithm for pitch estimation and voiced speech segregation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 2067–2079, 2010.\n\nHanzhang Hu, Debadeepta Dey, Martial Hebert, and J. Andrew Bagnell. Learning anytime predictions\n\nin neural networks via adaptive loss balancing. In Proc. AAAI, 2019.\n\nJ. Jensen and C. H. Taal. An algorithm for predicting the intelligibility of speech masked by modulated noise maskers. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 2009–2022, 2016.\n\nBarker Jon, Marxer Ricard, Vincent Emmanuel, and Watanabe Shinji. The third ‘CHiME’ speech separation and recognition challenge: Analysis and outcomes. Computer Speech & Language, pp. 605–626, 2017.\n\nKeisuke Kinoshita, Tsubasa Ochiai, Marc Delcroix, and Tomohiro Nakatani. Improving noise robust automatic speech recognition with single-channel time-domain enhancement network. In Proc. ICASSP, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R Hershey. SDR-half-baked or well\n\ndone? In Proc. ICASSP, 2019.\n\nChi-Chang Lee, Yu-Chen Lin, Hsuan-Tien Lin, Hsin-Min Wang, and Yu Tsao. SERIL: noise adaptive speech enhancement using regularization-based incremental learning. In Proc. Interspeech, 2020.\n\nJinyu Li, Li Deng, Yifan Gong, and Reinhold Haeb-Umbach. An overview of noise-robust automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 745–777, 2014.\n\nTatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello, Jacob Kahn, Gilad Avidov, Ronan Collobert, and Gabriel Synnaeve. Rethinking Evaluation in ASR: Are Our Models Robust Enough? In Proc. Interspeech, 2021.\n\nXingyu Lin, Harjatin Baweja, George Kantor, and David Held. Adaptive auxiliary task weighting for\n\nreinforcement learning. In Proc. NeurIPS, 2019.\n\nPhilipos C. Loizou. Speech Enhancement: Theory and Practice. CRC Press, Inc., USA, 2nd edition,\n\n2013. ISBN 1466504218.\n\nDavid Lopez-Paz and Marc' Aurelio Ranzato. Gradient episodic memory for continual learning. In\n\nProc. NeurIPS, 2017.\n\nXugang Lu, Yu Tsao, Shigeki Matsuda, and Chiori Hori. Speech enhancement based on deep\n\ndenoising autoencoder. In Proc. Interspeech, 2013.\n\nDuo Ma, Nana Hou, Van Tung Pham, Haihua Xu, and Eng Siong Chng. Multitask-based joint learning approach to robust asr for radio communication speech. In Proc. APSIPA ASC, 2021.\n\nZhong Meng, Shinji Watanabe, John R. Hershey, and Hakan Erdogan. Deep long short-term memory adaptive beamforming networks for multichannel robust speech recognition. In Proc. ICASSP, 2017.\n\nZhong Meng, Jinyu Li, Yifan Gong, and Biing-Hwang (Fred) Juang. Cycle-consistent speech\n\nenhancement. In Proc. Interspeech, 2018.\n\nRadford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,\n\n2(11):2, 2011.\n\nMohammad Nikzad, Aaron Nicolson, Yongsheng Gao, Jun Zhou, Kuldip K. Paliwal, and Fanhua\n\nShang. Deep residual-dense lattice network for speech enhancement. In Proc. AAAI, 2020.\n\nTsubasa Ochiai, Shinji Watanabe, Takaaki Hori, and John R. Hershey. Multichannel end-to-end\n\nspeech recognition. In Proc. ICML, 2017a.\n\nTsubasa Ochiai, Shinji Watanabe, and Shigeru Katagiri. Does speech enhancement work with end-to-end asr objectives?: Experimental analysis of multichannel end-to-end asr. In Proc. IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP), 2017b.\n\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus\n\nbased on public domain audio books. In Proc. ICASSP, 2015.\n\nAshutosh Pandey and DeLiang Wang. A new framework for supervised speech enhancement in the\n\ntime domain. In Proc. Interspeech, 2018.\n\nN. Parihar, J. Picone, D. Pearce, and H. G. Hirsch. Performance analysis of the Aurora large vocabulary baseline system. In Proc. The 12th European Signal Processing Conference, 2004.\n\nArchiki Prasad, Preethi Jyothi, and Rajbabu Velmurugan. An investigation of end-to-end models for\n\nrobust speech recognition. In Proc. ICASSP, 2021.\n\nJosef Rajnoha. Multi-condition training for unknown environment adaptation in robust ASR under\n\nreal conditions. 2009.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nKanishka Rao, Ha ̧sim Sak, and Rohit Prabhavalkar. Exploring architectures, data and units for\n\nstreaming end-to-end speech recognition with rnn-transducer. In Proc. ASRU, 2017.\n\nMirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, SungLin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, François Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio. Speechbrain: A general-purpose speech toolkit. arXiv preprint arXiv:2106.04624, 2021.\n\nChandan KA Reddy, Vishak Gopal, Ross Cutler, Ebrahim Beyrami, Roger Cheng, Harishchandra Dubey, Sergiy Matusevych, Robert Aichner, Ashkan Aazami, Sebastian Braun, et al. The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results. In Proc. Interspeech, 2020.\n\nChandan KA Reddy, Vishak Gopal, and Ross Cutler. Dnsmos: A non-intrusive perceptual objective\n\nspeech quality metric to evaluate noise suppressors. In Proc. ICASSP, 2021.\n\nA. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra. Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs. In Proc. ICASSP, 2001.\n\nTara N Sainath, Yanzhang He, Bo Li, Arun Narayanan, Ruoming Pang, Antoine Bruguier, Shuo-yiin Chang, Wei Li, Raziel Alvarez, Zhifeng Chen, et al. A streaming on-device end-to-end model surpassing server-side conventional model quality and latency. In Proc. ICASSP, 2020.\n\nGeorge Saon, Gakuto Kurata, Tom Sercu, Kartik Audhkhasi, Samuel Thomas, Dimitrios Dimitriadis, Xiaodong Cui, Bhuvana Ramabhadran, Michael Picheny, Lynn-Li Lim, Bergul Roomi, and Phil Hall. English conversational telephone speech recognition by humans and machines. In Proc. Interspeech, 2017.\n\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In Proc. ICASSP, 2012.\n\nMichael L. Seltzer, Dong Yu, and Yongqiang Wang. An investigation of deep neural networks for\n\nnoise robust speech recognition. In Proc. ICASSP, 2013.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\n\nsubword units. In Proc. ACL, 2016.\n\nDmitriy Serdyuk, Kartik Audhkhasi, Philémon Brakel, Bhuvana Ramabhadran, Samuel Thomas, Invariant representations for noisy speech recognition. arXiv preprint\n\nand Yoshua Bengio. arXiv:1612.01928, 2016.\n\nBaifeng Shi, Judy Hoffman, Kate Saenko, Trevor Darrell, and Huijuan Xu. Auxiliary task reweighting\n\nfor minimum-data learning. In Proc. NeurIPS, 2020.\n\nJiaqi Su, Zeyu Jin, and Adam Finkelstein. HiFi-GAN: High-fidelity denoising and dereverberation\n\nbased on speech deep features in adversarial networks. In Proc. Interspeech, 2020.\n\nC. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. A short-time objective intelligibility measure\n\nfor time-frequency weighted noisy speech. In Proc. ICASSP, 2010.\n\nAndrew Varga and Herman J. M. Steeneken. Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems. Speech Communication, pp. 247–251, 1993.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n\nKaiser, and Illia Polosukhin. Attention is all you need. In Proc. NeurIPS, 2017.\n\nPeidong Wang, Ke Tan, and De Liang Wang. Bridging the gap between monaural speech enhancement and recognition with distortion-independent acoustic modeling. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 39–48, 2020.\n\nS. Wang, W. Li, S. M. Siniscalchi, and C. Lee. A cross-task transfer learning approach to adapting deep speech enhancement models to unseen background noise using paired senone classifiers. In Proc. ICASSP, 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nShinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi. Hybrid ctc/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics in Signal Processing, pp. 1240–1253, 2017.\n\nMax Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proc.\n\nICML, 2011.\n\nFelix Weninger, Hakan Erdogan, Shinji Watanabe, Emmanuel Vincent, Jonathan Le Roux, John R. Hershey, and Björn Schuller. Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr. In International Conference on Latent Variable Analysis and Signal Separation, pp. 91–99, 2015a.\n\nFelix Weninger, Hakan Erdogan, Shinji Watanabe, Emmanuel Vincent, Jonathan Le Roux, John R Hershey, and Björn Schuller. Speech enhancement with LSTM recurrent neural networks and its application to noise-robust asr. In International Conference on Latent Variable Analysis and Signal Separation, 2015b.\n\nScott Wisdom, Efthymios Tzinis, Hakan Erdogan, Ron Weiss, Kevin Wilson, and John Hershey.\n\nUnsupervised sound separation using mixture invariant training. In Proc. NeurIPS, 2020.\n\nRuilin Xu, Rundi Wu, Yuko Ishiwaka, Carl Vondrick, and Changxi Zheng. Listening to sounds of\n\nsilence for speech denoising. In Proc. NeurIPS, 2020.\n\nY. Xu, J. Du, L. Dai, and C. Lee. A regression approach to speech enhancement based on deep neural\n\nnetworks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015.\n\nDacheng Yin, Chong Luo, Zhiwei Xiong, and Wenjun Zeng. PHASEN: A phase-and-harmonics-\n\naware speech enhancement network. In Proc. AAAI, 2020.\n\nHuanjing Yue, Wenxin Duo, and Jingyu Yang. Reference-based speech enhancement via feature\n\nalignment and fusion network. In Proc. AAAI, 2022.\n\nQ. Zhang, A. Nicolson, M. Wang, K. K. Paliwal, and C. Wang. DeepMMSE: A deep learning approach to MMSE-based noise power spectral density estimation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1404–1415, 2020.\n\nQian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and Shankar Kumar. Transformer transducer: A streamable speech recognition model with transformer encoders and RNN-T loss. In Proc. ICASSP, 2020.\n\nS. Zhang, C. T. Do, R. Doddipatla, and S. Renals. Learning noise invariant features through transfer\n\nlearning for robust end-to-end speech recognition. In Proc. ICASSP, 2020.\n\nShucong Zhang, Cong-Thanh Do, Rama Doddipatla, and Steve Renals. Learning noise invariant features through transfer learning for robust end-to-end speech recognition. In Proc. ICASSP, 2020.\n\nChengyu Zheng, Xiulian Peng, Yuan Zhang, Sriram Srinivasan, and Yan Lu. Interactive speech and\n\nnoise modeling for speech enhancement. In Proc. AAAI, 2021.\n\nA APPENDIX\n\nA.1 D4AM EVALUATED ON HIGH SNR DATA\n\nMost SE research focuses on “average” denoising on the target ASR task, which implies that we consider the distortion caused by the SE model as tolerable if it is sufficiently low. Accordingly, we conducted additional experiments using cleaner speech utterances as testing data. The WERs of the Google ASR API are shown in Table 4, where test-clean represents the clean LibriSpeech test set, and the others are test-clean mixed with noisex92 (Varga & Steeneken, 1993) and Nonspeech (Hu & Wang, 2010) noise sources, uniformly sampled at high SNR levels between 10dB to 20dB (average 15 dB). From Table 4 illustrates that the WER of the clean speech processed by D4AM is marginally\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nhigher than that of the original clean speech (ORIG). The result is 16.61% versus 16.27%. However, for both types of high SNR noisy speech, using D4AM reduced the WER (19.24% versus 19.67% for noisex92 (Varga & Steeneken, 1993) and 20.19% versus 20.55% for Nonspeech (Hu & Wang, 2010)). The results show that D4AM can yield performance comparable (better) to that achieved using the original clean (high SNR noisy) speech.\n\nTable 4: WER (%) results on high SNR test sets.\n\nmethod\n\ntest-clean\n\ntest-noisex\n\ntest-nonspeech\n\nORIG\n\nD4AM\n\n16.27\n\n16.61\n\n19.67\n\n19.24\n\n20.55\n\n20.19\n\nA.2 D4AM EVALUATED WITH MCT-TRAINED ASR MODELS\n\nThe results of the Google ASR API in Table 3 are supposed to be representative results of the MCTtrained ASR systems. We conducted additional ASR experiments on CHiME-4 with MCT-CONF, MCT-W2V2, and MCT-TRAN corresponding to the ASR model architectures in Table 1 and trained on the LibriSpeech and DNS noise datasets with the SNR levels -4 to 6. Note that W2V2 consists of a wav2vec2 module and a linear layer. During training, the wav2vec2 module was fixed and used as a feature extractor, and only the linear layer was updated. It is evident from Table 5 that D4AM consistently outperformed NOIS for all MCT-trained ASR models.\n\nTable 5: WER (%) results of MCT-trained ASR models on the CHiME-4 dt-05 real set.\n\nmethod MCT-CONF MCT-W2V2 MCT-TRAN\n\nNOIS\n\nD4AM\n\n25.69\n\n24.22\n\n28.86\n\n22.89\n\n22.82\n\n22.59\n\nA.3 D4AM TRAINED WITH ANOTHER ACOUSTIC MODEL\n\nWe selected the conformer-based ASR model as the proxy acoustic model because it has a lower memory requirement than other model architectures in SpeechBrain. In terms of implementation, model complexity is a key concern, particularly for researchers similar to us with limited computing resources. We believe our claim that any critical point of Lcls(θ) should be “covered” by the critical point space of Lreg(θ) applies to most cases. By considering this appropriately, we can reduce the bias of different settings of the proxy acoustic model used for training. In addition, previous work (Ochiai et al., 2017a;b) has shown empirically that training an SE unit with one acoustic model can improve another ASR system. To show that the proposed D4AM is insensitive to the selection of proxy acoustic model, we conducted additional experiments using the W2V2-CTC model as the proxy acoustic model. The results evaluated with a LibriSpeech-trained ASR system on the CHiME-4 dt-05 real set are shown in Table 6, where Proxy-CONF and Proxy-W2V2-CTC denote conformerand W2V2-CTC-based proxy acoustic models, respectively.\n\nAs shown in Table 6, D4AM can effectively reduce the bias between proxy acoustic models and notably improve the performance. Since Conformer and W2V2-CTC have different model architectures and use different label types, the results confirm that D4AM can consistently produce promising results using different types of proxy acoustic models.\n\nA.4 D4AM EVALUATED WITH THE L3DAS22 MICRO-A SETS\n\nThe input of L3DAS22 Guizzo et al. (2022) is a multi-channel signal. In our experiments, we focused on the single-channel SE task and considered different channels as separate signals for training and testing. Also, please note that the WER definition of L3DAS22 differs from the regular version. In L3DAS22, the reference target for computing WER is NOT a true transcription. The WER is the word-level distance between the ASR model’s predicted transcripts for the clean and enhanced\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: WER (%) results of different setups on the CHiME-4 dt-05 real set.\n\nmethod\n\nCONF TRAN RNN\n\nProxy-CONF-CLSO\n\nProxy-CONF-SRPR\n\nProxy-CONF-GCLB\n\nProxy-CONF-D4AM Proxy-W2V2-CTC-CLSO\n\nProxy-W2V2-CTC-SRPR\n\nProxy-W2V2-CTC-GCLB\n\n26.66\n\n26.88\n\n26.76\n\n26.7 30.61\n\n29.09\n\n29.25\n\nProxy-W2V2-CTC-D4AM 28.46\n\nINIT\n\n30.98\n\n27.83\n\n26.73\n\n27.51\n\n26.07 28.98\n\n27.57\n\n27.91\n\n26.89\n\n29.61\n\n35.25\n\n34.83\n\n34.62\n\n34.43 40.30\n\n36.67\n\n36.92\n\n35.33\n\n38.61\n\nsignals. Furthermore, the WER value is calculated on a sample basis. In contrast, the conventional WER value is calculated using ground truth transcripts as targets and is derived by summing the errors across all testing samples.\n\nBy training our SE model with their pre-mixed training data, we executed our algorithm on the set with the following results:\n\nTable 7: WER results on the L3DAS22 micro-A sets.\n\nL3DAS22-dev-microA\n\nL3DAS22-test-microA\n\nmethod\n\nTRAN\n\nNOISY\n\nINIT\n\nCLSO\n\nSRPR\n\nGCLB\n\nD4AM\n\n31.14\n\n22.93\n\n19.31\n\n19.03\n\n19.40\n\n18.68\n\nRNN\n\n60.80\n\n35.65\n\n32.88\n\n31.82\n\n32.07\n\n31.61\n\nW2V2\n\nTRAN\n\n38.85\n\n28.00\n\n24.34\n\n23.68\n\n23.75\n\n23.59\n\n27.59\n\n21.94\n\n18.35\n\n18.08\n\n18.50\n\n17.88\n\nRNN\n\n57.98\n\n33.72\n\n30.99\n\n29.95\n\n30.17\n\n29.71\n\nW2V2\n\n35.70\n\n26.67\n\n22.78\n\n22.54\n\n22.55\n\n22.19\n\nA.5 HUMAN PERCEPTION EVALUATION\n\nFinally, we evaluated the enhanced speech signal using an objective perceptual metric, DNSMOS (Reddy et al., 2021). The DNSMOS value represents the perception score of human hearing, with higher scores indicating better quality. Table 8 shows the DNSMOS scores of enhanced speech produced by various SE methods. The real sets of CHiME-4 described in Section 4.2 were used as the testing data. Table 8 reveals that D4AM produced the highest DNSMOS score among all models trained with the ASR objective, and its DNSMOS score is marginally lower than that of INIT (only considering the regression objective for training). The result again substantiates our conjecture that any critical point of Lcls(θ) should be “covered” by the critical point space of Lreg(θ).\n\nTable 8: DNSMOS results on the CHiME-4 real sets.\n\nmethod\n\nNOIS\n\nINIT\n\nCLSO\n\nSRPR GCLB D4AM\n\nDNSMOS\n\n2.5574\n\n3.2401\n\n2.7665\n\n3.0598\n\n3.0398\n\n3.1450\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nA.6 VARIATION IN αsrpr DURING TRAINING\n\nFig. 3 shows the variation in αsrpr with and without gradient calibration applied over training steps. It is evident that the αsrpr values of both SRPR and D4AM converge rapidly, indicating that their learning process is stable.\n\nFigure 3: The change in the αsrpr value during training. The dashed lines represent the average αsrpr values.\n\nA.7 DERIVATION OF αgclb\n\nTo derive αgclb, the Lagrangian of the constrained optimization problem defined in Eq. 2 can be written as:\n\nL(g, α) =\n\n1 2\n\n∥g − ∇Lφ\n\ncls(θt)∥2 − α⟨g, ∇Lreg(θt)⟩\n\n(6)\n\nTo determine the optimal value g∗ = arg ming L(g, α), we set the derivatives of L(g, α) with respect to g to 0. By solving for ∇gL(α, g) = 0, we obtain g = ∇Lφ cls(θt) + α · ∇Lreg(θt). Then, we can substitute g = ∇Lφ\n\ncls(θt) + α · ∇Lreg(θt) into L(α, g) and get the minimum L∗(α):\n\nL∗(α) = −α⟨∇Lφ\n\ncls(θt), ∇Lreg(θt)⟩ −\n\n1 2\n\nα2∥∇Lreg(θt)∥2\n\n2 + const\n\n(7)\n\nwhere const is the summation of those terms regardless of α. Here, the solution of αgclb is the critical point of L∗(α). Therefore, by deriving α such that ∇αL∗(α) = 0, we can obtain the final result of αgclb:\n\n⟨∇Lφ\n\ncls(θt), ∇Lreg(θt)⟩ ∥∇Lreg(θt)∥2\n\n2\n\n(8)\n\nαgclb = −\n\n⟨∇Lφ (cid:74)\n\ncls(θt), ∇Lreg(θt)⟩ < 0 (cid:75)\n\n·\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a denoising frontend for automatic speech recognition. The method combines speech enhancement (regression) and recognition (classification) loss. The paper presents a scheme for automatically balancing the regression and classification losses. The paper shows the effectiveness of the proposed method in the efficacy of noisy speech recognition experiments.\n\n# Strength And Weaknesses\n\nStrength\n- consistent improvements of the ASR performance even with a black box ASR API. The problem setup of developing universal speech enhancement techniques for various ASR systems is crucial. \n- automatic balancing scheme of regression and classification losses seem to be novel\n- showing the robustness of the balancing scheme experimentally.\n\nWeaknesses\n- optimization of speech enhancement with ASR loss is not novel. Of course, a part of dealing with unseen ASR systems is novel, but it is not a sufficient novelty.\n- the method is specific to speech processing problems and would not attract general machine learning and AI researchers. The automatic balancing scheme seems to be a general scheme, and if this has more critical or different applications, this part will be improved more.\n- Although the balancing scheme is sophisticated, the result is not very different from a conventional grid search. It is not difficult to tune a single hyper-parameter with the dev set.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity\n- It is difficult to read Section 3, especially Section 3.1, due to many inline equations. It is better to provide more high-level derivations; some details can be moved to the appendix sections.\n\nQuality\n- I cannot fully agree with the \"SE approaches do not consider the generalization ability to unseen ASR systems.\" I think it is valid for weak speech enhancement models, but the latest state-of-the-art speech enhancement shows the generalized performance without changing the ASR system, e.g., the L3DAS challenge https://www.l3das.com/icassp2022/index.html fixes the ASR backend, but the many speech enhancement systems improve the ASR performance without using the ASR backend as a loss function. I understand the claim of \"unseen ASR systems\" to some extent, but some strong speech enhancement may achieve such properties. \n- The effectiveness of the automatic balancing scheme compared with the grid search is marginal. To show the effectiveness of the proposed method, it may find more complex cases for the grid search (e.g., more tuning parameters).\n\nNovelty\n- The optimization of speech enhancement with ASR loss is not novel. Of course, a part of dealing with unseen ASR systems is novel, but it is not sufficiently novel, and the paper requires more clarification of the technical novelty of this paper.\n- I think the automatic balancing scheme seems to be novel.\n\nReproducibility\n- Training data seems to be based on their own simulation methods. It is better to release this setup for reproducibility\n\n# Summary Of The Review\n\nThe paper does not have sufficient novelty in terms of the technical/algorithm part. Various researchers have already studied the joint training of both enhancement and recognition losses (as shown in Section 2). The automatic balancing scheme seems novel, but the effectiveness compared with the grid search is marginal.\n\nOther suggestions\n- Please discuss the reverberation cases. This is very critical for actual ASR deployment.\n- Can we use this method without the regression loss? In the practical scenario (real data situations), we cannot prepare the clean data, and we cannot construct the regression loss. I want the authors to discuss such cases and how the proposed method works.\n- Abstract: It is better to provide more concrete descriptions (e.g., with some numbers) to claim the effectiveness of the proposed method experimentally. \n- Can you discuss whether the proposed automatic balancing scheme more generally applies to the other machine learning problems? I think such a discussion makes the potential effectiveness of this scheme strong.\n- The introduction (section 1) and motivation (section 2) have some similar logical flows, and this part can be refined.\n- Why did you use DEMUCS? Can you add some references about the state-of-the-art performance of DEMUCS in some SE tasks?\n- Table 1. Please use the same decimal place for all numbers (2nd decimal place).\n- Section 4.2 \"As seen in Table 2, the result of INIT is comparable to the state-of-the-art results of CHiME-4\" Can you add a reference? Which system are you comparing?\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTOWARDS PERFORMANCE-MAXIMIZING NETWORK PRUNING VIA GLOBAL CHANNEL ATTENTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNetwork pruning has attracted increasing attention recently for its capability of significantly reducing the computational complexity of large-scale neural networks while retaining the high performance of referenced deep models. Compared to static pruning removing the same network redundancy for all samples, dynamic pruning could determine and eliminate model redundancy adaptively and obtain different sub-networks for each input that achieve state-of-the-art performance with a higher compression ratio. However, since the system has to preserve the complete network information for running-time pruning, dynamic pruning methods are usually not memory-efficient. In this paper, our interest is to explore a static alternative, dubbed GlobalPru, to conventional static pruning methods that can take into account both compression ratio and model performance maximization. Specifically, we propose a novel channel attention-based learn-to-rank algorithm to learn the optimal consistent (global) channel attention prior among all sample-specific (local) channel saliencies, based on which Bayesian-based regularization forces each samplespecific channel saliency to reach an agreement on the global channel ranking simultaneously with model training. Hence, all samples can empirically share the same pruning priority of channels to achieve channel pruning with minimal performance loss. Extensive experiments demonstrate that the proposed GlobalPru can achieve better performance than state-of-the-art static and dynamic pruning methods by significant margins.\n\n1\n\nINTRODUCTION\n\nConvolutional neural networks (CNNs) have achieved great success in many visual recognition tasks including image classification He et al. (2016), object detection Ren et al. (2015), image segmentation Dai et al. (2016), etc. The success of CNNs is inseparable from an excessive number of parameters that are well organized to perform sophisticated computations, which conflicts with the increasing demand for deploying these resource-consuming applications on resource-limited devices.\n\nNetwork pruning has been proposed to effectively reduce the deep model’s resource cost without a significant accuracy drop. Unstructured pruning methods Han et al. (2015b;a) usually reach a higher compression rate, while relying on dedicated hardwares/libraries to achieve the actual effect. In contrast, structured pruning methods Li et al. (2016); He et al. (2017b) preserve the original convolutional structure and are more hardware-friendly. Considering the greater reduction in terms of floating-point operations (FLOPs) and hardware commonality, this research focuses on channel pruning. Existing methods perform channel pruning either statically or dynamically. Static pruning methods remove the same channels for all images Molchanov et al. (2019); Tang et al. (2020) while dynamic pruning removes different channels for different images Rao et al. (2018); Tang et al. (2021b). However, both existing static methods and dynamic methods have some limitations. Particularly, given the fact that the channel redundancy is highly sample-dependent, static pruning methods may remove some channels that are not redundant for certain images. Consequently, static methods refrain from a larger pruning rate to avoid a significant accuracy drop. To tackle the issue of image-specific redundant channels, dynamic pruning methods remove image-specific channels. In this way, dynamic methods achieve the state-of-the-art pruning ratio without significantly sacrificing performance. Despite the significant advantage, dynamic pruning usually requires preserving the full original model during inference, which restricts its practical deployment on resource-limited devices.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nIn this paper, we propose a new paradigm of static pruning method named GlobalPru. Although born from static, GlobalPru tackles the issue of image-specific redundant channels by making all images share the same ranking of channels with respect to redundancy. In other words, GlobalPru forces all images to agree on the same ranking of channel saliency (referred to as global channel ranking) to reduce the image-specific channel redundancy to the greatest extent possible. By removing channels with the lowest global rankings, GlobalPru can avoid the problem of existing static pruning methods, which remove more important channels while retaining less important channels for specific images with a high probability. More specifically, we first propose a novel global channel attention mechanism. Channel attention is local in the sense that the ranking of channels with respect to their importance is image-specific. Different from existing ones, our proposed global channel attention mechanism can identify the global channel ranking across all different samples in the training set, particularly through a learn-to-rank regularization. In detail, to make the static GlobalPru approach the maximum image-specific compression ratio of dynamic pruning as well as stabilize the training process, we first use a majority-voting-based strategy to specify the global ranking to make the static GlobalPru approach the maximum image-specific compression ratio of dynamic pruning. Then, given a certain ranking, all the image-specific channel rankings are forced to agree on the ranking via learn-to-rank regularization. When all the image-specific channel rankings are the same as the given ranking, the ranking becomes the global ranking. As a result of exposing global ranking for all images during training stages, GlobalPru can also avoid the disadvantage of existing dynamic pruning which needs to store the entire model for deciding image-specific channel ranking during inference and perform more efficient pruning on globally ordered channels.\n\nOur contributions are summarized as follows:\n\n• We propose GloablPru, a static network pruning method. GlobalPru tackles the issue of image-specific channel redundancy faced by existing static methods by learning a global ranking of channels w.r.t redundancy. GlobalPru produces a pruned network such that GlobalPru is a more memory-efficient solution than existing dynamic methods.\n\n• To the best of our knowledge, we for the first time propose a global channel attention mechanism where all the images share the same ranking of channels w.r.t. importance. Instead of repeatedly computing image-specific channel rankings under existing local attention mechanisms, our proposed global attention enriches the representation capacity of models and therefore greatly improves the pruning efficiency.\n\n• Extensive experimental results show that GlobalPru can achieve state-of-the-art performance\n\nwith almost all popular convolution neural network architectures.\n\n2 RELATED WORK\n\n2.1 STATIC PRUNING & DYNAMIC PRUNING\n\nAs the most traditional and classic model pruning method, static pruning shares a compact model among all different samples Wen et al. (2016); Liu et al. (2017a); Liebenwein et al. (2019); Molchanov et al. (2019); Tang et al. (2020). To be specific, static methods select pruning results through trade-offs on different samples, which leads to final compact models having limited representation capacity and thus suffering an obvious accuracy drop with large pruning rates. Recently, some works turn their attention to the pursuit of the ultimate pruning rate and focus on excavating sample-wise model redundancy, named dynamic pruning. Dynamic pruning generates different compact models for different samples Dong et al. (2017); Gao et al. (2018); Hua et al. (2019); Rao et al. (2018); Tang et al. (2021b). Actually, dynamic methods learn a path-decision module to find the optimal model path for each input during inference. For example, state-of-the-art work Liu et al. (2019) investigates a feature decay regularization to identify informative features for different samples, and therefore achieves an intermediate feature map to the model sparsity. Tang et al. (2021a) further improves the dynamic pruning efficiency by embedding the manifold information of all samples into the space of pruned networks. Despite dynamic methods achieve higher compression rate, most of them are not memory-efficient cause most of them requires to deploy the full model even in inference stage.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2.2 CHANNEL ATTENTION\n\nChannel attention is similar to the scale coefficients added on feature channels to enhance the important features and also weaken the unimportant ones. It is usually implemented through extra channel attention modules, which can exploit the inter-channel relationship for input feature maps. An typical channel attention mechanism is SENet Hu et al. (2017), it uses the “Squeeze-and-Excitation” (SE) block which can be stacked together to adaptively recalibrate channel-wise feature responses on each convolutional layer by explicitly modelling inter-dependencies between channels. Li et al. (2019) proposes a dynamic selection mechanism in CNNs that allows each neuron to adaptively adjust its receptive field size based on multiple scales of input information, which is named “Selective Kernel” (SK) unit. By dynamically calculating channel attention on the different kernels, SK realizes parameter sharing and thus improves the model efficiency. In summary, nearly all the existing channel attention are local in the sense that the channel saliency is image-specific and can not identify a global channel attention over the entire dataset.\n\n3 GLOBAL ATTENTION-BASED CHANNEL PRUNING\n\nIn this section, we will give a detailed formulation and theoretical explanation of the proposed Global Channel Attention Pruning (GlobalPru). As illustrated in Section 3.1, the GlobalPru is formulated as a static alternative regularized simultaneously by local channel-wise saliency and the global channel rank loss and can be optimized in an end-to-end manner. Then, the training process of GlobalPru is divided into two stages including Global Channel Attention Election (stage 1) and Learn-to rank Regularization Pruning (stage 2), which would be illustrated and proven theoretically in Section 3.2 and Section 3.3, respectively. In stage 1, GlobalPru explicitly elects the global channel attention that benefits most samples from the observations of the locally image-specific channel dependencies. After that, in stage 2, the proposed learn-to-rank algorithm forces the channels to be ordered towards the global channel attention concurrently during the model training process.\n\n(a) Global channel attention.\n\n(b) Learn-to-rank regularization.\n\nFigure 1: The overview of the training process of Channel Rank Network.\n\n3.1 PROBLEM FORMULATION\n\ni=1 with the corresponding labels Y = {yi}N\n\nGiven a dataset with N samples as X = {xi}N i=1. For a convolution neural network (CNN) with L convolution layers and parameters set Θ, W l ∈ Rcl×cl−1×kl×kl denotes the convolution parameters in the l-th layer, where cl is the channel numbers of l-th layer and kl represents the corresponding kernel size. F l(xi) ∈ Rb×cl×h×w is the output feature map of the l-th layer for the input xi, where b, h, w are the batch size, height and width of the output feature map, respectively. Given the input feature F l−1(xi), the output of layer l can be calculated as F l(xi) = Relu(Bn(W l ⊗ F l−1)), where Relu and Bn represent the activation and\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nbatch normalization operation respectively. Finally, we use f (X, Θ) to represent the output of the neural network over input samples X.\n\nStatic channel pruning eliminates the same redundancies and discovers a compact network among all images. And most static methods are guided by empirical risk minimization and numeric-based regular terms, i.e., parameter magnitudes, channel saliencies or reconstruction errors, etc. Typical static methods can be formulated as follows:\n\n(cid:80)N\n\ni=1\n\nmin Θ\n\nL (f (xi, Θ), yi) + λ · N orm(Θ),\n\n(1)\n\nwhere L denotes the loss function and N orm(·) is the regularization term which is usually a human-designed criterion for inducing model sparsity. And λ is used as a knob to strike the different trade-offs between model accuracy and sparsity ratio.\n\nConversely, dynamic channel pruning discovers effective sub-networks for each input dependently during the inference stage, which is usually implemented through additional model path-finding functions. Define a scoring module S (a squeeze and excitation module Hu et al. (2017) in our work), for a specific input xi, the channel saliencies on layer l can be computed as πl(xi) = Sl(xi) ∈ Rcl ,\nj(xi) indicates the relative importance score of the j-th channel, and a smaller πl where πl j indicates that channel j is less important. Given a pruning threshold εl, the variable maskl j to indicate the removal of the j-th channel of layer l is initialized to 1 and set to 0 when πl j(xi) < εl. The channel saliencies for the pruned model would be re-calculated as:\n\nˆπl(xi) = πl(xi) ⊗ maskl(xi)\n\nAnd the feature output of the l-th layer would be:\n\nF l(xi) = Relu\n\n(cid:16)\n\nBN\n\n(cid:16)(cid:16)\n\nmaskl(xi) · ˆF l−1(xi)\n\n(cid:17)\n\n⊗ W l(cid:17)(cid:17)\n\n(2)\n\n(3)\n\nThus, a general dynamic pruning paradigm could be formulated as EQ. 4, wherein the regularization is used to induce the instant-wise network sparsity.\n\n(cid:80)N\n\ni=1\n\nmin Θ\n\nL (f (xi, Θ), yi) + λ\n\nL (cid:88)\n\nl=1\n\n||πl(xi)||\n\n(4)\n\nDynamic pruning has achieved higher compression rates than static methods by removing instancespecific model redundancy, however, the calculation of πl(xi) depends on the complete referenced model along with additional path-addressing units being deployed on resource-limited devices. These unavoidable dependencies make dynamic methods not memory and computation-efficient, thus without great applicability.\n\nTo utilize advantages from both static and dynamic methods while avoiding the corresponding bottlenecks, GlobalPru expects all these samples to reach an agreement on the channel importance ranking simultaneously with model training, thus achieving a dynamic-like static channel pruning. To this end, GlobalPru first investigates the local channel attention of different samples via a preheated scoring module S, based on which it could identify a global channel attention as the channel importance ranking prior through the Majority Vote Mechanism. After that, GlobalPru forces all image-specific channel rankings to converge to the certain global channel ranking by the novel Learn-to-Rank regularization. The detailed formulation of GlobalPru is as follows.\n\nLet Rl denote the medium results of channel importance ranking of layer l most voted by all input samples during the training process, and T l is the corresponding channel ranking prior. Hence he channel ranking loss could be computed as Φ(Rl, Tl), wherein Φ is the learn-to-rank regularization defined in Eq. 6,7. The optimization target of GlobalPru can be formulated as:\n\nmin Θ\n\nN (cid:88)\n\nL (cid:88)\n\nL (f (xi, Θ), yi) + α (cid:13)\n\n(cid:13)πl(xi)(cid:13)\n\n(cid:13)1 + β\n\ni=1\n\nl=1\n\nL (cid:88)\n\nl=1\n\nΦ(Rl, T l),\n\n(5)\n\nj and Rl\n\nLet T l j represent the importance ranking index of the j-th channel in the prior and the global attention during training, respectively. We elaborately use the previously defined channel score |πl j| to measure the similarity between the current position and the target position of the j-th channel on\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nlayer l, and have P (Rl as:\n\nj = T l\n\nj ) =\n\nj )\n\nexp(πl j =1cl exp(πl j )\n\n(cid:80)\n\n. Hence Φ can be formulated in Bayesian perspective\n\nΦ(T l, Rl) = − log\n\ncl (cid:89)\n\nj=1\n\nP (Rl\n\nj = T l\n\nj |T l\n\nj ) = −\n\ncl (cid:88)\n\nj=1\n\nlog\n\nP (T l\n\nj |Rl\n\nj = T l\n\nj = T l j )\n\nj )P (Rl P (T l j )\n\n(6)\n\nSince T l is defined as the prior of the channel rank and independent of the current channel ranking, we have P (T l) = 1 and P (T l|Rl = T l) = 1, hence we can rewrite EQ. 6 as:\n\nΦ(T l, Rl) = −\n\ncl (cid:88)\n\nj=1\n\nlog P (Rl\n\nj = T l\n\nj ) = − −\n\ncl (cid:88)\n\nj=1\n\nlog\n\nexp(πl\n\nj)\n\n(cid:80)cl\n\nj=1 exp(πl j)\n\n(7)\n\nTo simulate the impact of model pruning and reduce computational overhead, we use ˆπl πl\n\nj in actual optimization.\n\nj instead of\n\n3.2 GLOBAL CHANNEL ATTENTION\n\nDefinition 1 Channel local attention refers to the channel attention of a single input (channel importance ranking), while channel global attention refers to the channel attention consistent with the most samples for the current data domain.\n\nLots of previous works have proven that the importance ranking of channels Tang et al. (2021b) is highly sample-dependent, which means the model redundancy for different samples could be quite different with high probability. In fact, natural images are a mixture of some intrinsic features such as color, edges, textures, etc., in different ratios, resulting in different requirements of different samples for the model. Channel attention actually adjusts the weight of each channel of the output features for each sample to approximate the target feature mapping. Thus, we give the following corollary.\n\nCorollary 1 Conventional channel attention could indicate the quantitative channel redundancy for each sample through the learnable feature scaling factors.\n\nCorollary 2 Local channel attention is highly sample-related, which implies that redundant channels are disordered and unpredictable for the whole sample domain.\n\nTo implement global channel attention across samples and therefore eliminate the dependency of dynamic pruning on sample-wise path-finding, we propose to explore the “Global Channel Attention” for all different samples. As shown in Figure 1(a), the global channel attention is identified by the majority vote mechanism. Specifically, a deep neural network with channel attention modules is first pre-warmed. After that, in the training stage, every image-specific local channel attention is collected as the electoral college to be voted for global channel attention, which would be the prior of the channel importance ranking in the learn-to-rank regularization. In this way, as a static alternative, GlobalPru obtains the most supported channel importance ordering in the current data domain to achieve high-quality dynamic-like pruning, achieving a higher model compression rate and reducing the precision decline from pruning.\n\n3.3 LEARN-TO-RANK REGULARIZATION\n\nThe learn-to-rank regularization is explicitly modeled from Bayesian perspective as a joint probability maximization problem. As shown in Figure 1(b) and EQ. 8, specifically, regarding the global channel attention as the prior of the channel importance ranking, the channel ranking loss can be formalized as the negative value of the probability that the current channel ranking is equal to the given prior. Thus, in addition to empirical risk minimization, GlobalPru trains a model distribution towards the probabilistic model which is most likely to generate the target channel ranking. However, the huge number of channels in CNNs brings unique challenges for computing the ranking loss for each channel. We use the unique advantages of model pruning to address the problem of huge computational costs in EQ. 8, given the pruning rate p, the computation cost could be further reduced by maximizing the probability that only the first (1 − p)cl channels in T l are ranked correctly. The\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nproposed learn-to-rank regularization could be reformatted to Eq. 8, and:\n\nΦ(T l, Rl) = −\n\n(1−p)cl (cid:88)\n\nj=1\n\n \n\n\n\nlog exp(πl\n\nj) − log\n\n(1−p)cl (cid:88)\n\nj=1\n\nexp(πl\n\nj)\n\n \n\n\n\n(8)\n\n4 EXPERIMENT\n\nIn this section, GlobalPru is empirically tested on the most representative neural network architectures: plain architecture, residual structure, and lightweight depthwise convolution networks to verify the effectiveness of the proposed “Global Attention Mechanism” and “Learn-To-Rank” method.\n\nDatasets: The effectiveness of the proposed GlobalPru is verified on most popular image datasets of model pruning, including MiniImageNet, CIFAR-10, CIFAR100 and SVHN. Diverse datasets can help effectively demonstrate the broad applicability of our GlobalPru.\n\nModels: We explore the effectiveness of GlobalPru on three mainstream neural network architectures includes: plain architecture - VGG-16, residual structure - ResNet with different depth, and lightweight depthwise convolution MobileNet-V2. These three representative architectures cover almost all the most popular convolutional networks currently.\n\nTasks: Most model pruning works only demonstrate the accuracy of the pruned models on the image classification task, and ignores the robustness of the pruned networks. To further demonstrate the applicability of our method, we also evaluate the performance of the pruned network against adversarial perturbations on the adversarial samples detection task.\n\nImplementation Details: Standard data argumentation RandomSizedCrop and RandomHorizontalFlip are used in all datasets we used. The coefficient α to regulate the channel saliency score is set as 0.0001 in our work. And another coefficient β to balance the weight of the channel rank loss is also set as 0.0001 empirically. The stochastic gradient descent (SGD) is used for all training processes. Two pruning modes are tested in GlobalPru, named “Fixed” and “Mixed”, respectively. Under the fixed mode, all layers use the same pruning rate with the Lottery Ticket of model pruning which corresponds to the highest test accuracy Frankle & Carbin (2019). For ResNet-18 on CIFAR10 in this work, the range of pruning rates which can achieve higher test accuracy than the original model is [38.9%, 100.0%], and the optimal pruning rate corresponds to the highest test accuracy is 61.1% Under the mixed mode, the layer-wise pruning rates are set according to the sensitivity of each layer to model perturbation. Dong et al. (2020) has theoretically proved that the right sensitivity metric for model perturbation is the average Hessian trace. Align the value of the average Hessian trace of all layers with the optimal pruning rate, and the fluctuation of the pruning rate of each layer relative to the optimal pruning rate is set according to the standard deviation of the Hessian trace of this layer relative to the model average . The larger the Hessian trace value, the lower the pruning rate. All of the experiments are conducted on NVIDIA GeForce GPUs.\n\n4.1 COMPARISON ON PLAIN ARCHITECTURE\n\nThe proposed method is first compared with the state-of-the-art network pruning algorithm on the most intuitive plain architecture, which is the initially successful way to construct a convolutional neural network. The typical plain network VGG-16 we used consists of a few building blocks (convolutional layers, activation layers, etc.) stacked in a vertical manner. A “Squeeze-and-Excitation” module is added following the convolutional layer to measure its relative channel saliency. Both static methods including Luo et al. (2017); Li et al. (2017); He et al. (2017a); Liu et al. (2017b), and the dynamic methods including Lin et al. (2017); Gao et al. (2019) are compared. All of the algorithms are tested on the Cifar-10 dataset and the comparison results are shown in Table 1. The pruning performance is reported over three main metrics: Memory Access Cost (MAC), FLoating point Operations (FLOPs), and Classification Accuracy Drop. When using the fixed pruning rate for all the computing layers, our method achieves 55.2% FLOPs decrease with a negligible accuracy drop of −0.21%. And when using the empirically mixed pruning rate for each layer, our method can prune more 1.5% FLOPs than the fixed mode, while with less accuracy degradation. The results show that our approach achieves the highest compression ratio on VGG-16 with competitive model performance. Compared to the SOTA static pruning method, undoubtedly, our method wins an\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison of pruning performance on VGG-16 architecture and Cifar-10 dataset. Our method is compared with different channel pruning algorithms including both traditional static methods and sample-wise dynamic methods. “Dy?” means if the current method is a sample-wise dynamic method. “Fixed” and “Mixed” represent the above-mentioned fixed pruning mode and mixed pruning mode, respectively. “*” denotes the new static pruning paradigm, which approximates the maximum compression ratio of dynamic methods.\n\nCite\n\nDy?\n\n↓MAC FLOPs(M)\n\n↓FLOPs\n\n-Acc\n\nMethod\n\nBaseline\n\nLIWS\n\nThiNet\n\nL1-norm\n\nCP\n\nNS\n\nRNP\n\nFBS\n\nWang et al. (2020)\n\nLuo et al. (2017)\n\nLi et al. (2017)\n\nHe et al. (2017a)\n\nLiu et al. (2017b)\n\nLin et al. (2017)\n\nGao et al. (2019)\n\n0\n\n53.2%\n\n50%\n\n34%\n\n50%\n\n51%\n\n50%\n\n50%\n\n55%\n\n56.7%\n\nY\n\nN\n\nN\n\nN\n\nN\n\nY\n\nY\n\n*\n\n*\n\n626.4\n\n184\n\n313\n\n413.2\n\n313\n\n306.8\n\n313\n\n310.4\n\n280.87\n\n271.26\n\n0\n\n0\n\n70.6%\n\n-0.8%\n\n50%\n\n34%\n\n50%\n\n51%\n\n50%\n\n-0.14%\n\n-0.5%\n\n-0.32%\n\n-0.19%\n\n-0.85%\n\n50.4% -0.47%\n\n55.2% -0.21%\n\n56.7% -0.15%\n\nGlobalPru(Fixed)\n\nGlobalPru(Mixed)\n\nOurs\n\nOurs\n\nTable 2: Comparison of pruning performance on Residual architecture: ResNet-20 architecture and Cifar-10 dataset.\n\nCite\n\nDy?\n\nFLOPs(M)\n\n↓FLOPs\n\n-Acc\n\nMethod\n\nBaseline\n\nSFP\n\nFPGM\n\nDSA\n\nHinge\n\nDHP\n\nManinp\n\nFBS\n\nHe et al. (2018)\n\nHe et al. (2019)\n\nNing et al. (2020)\n\nLi et al. (2020a)\n\nLi et al. (2020b)\n\nTang et al. (2021a)\n\nGao et al. (2019)\n\n43.17\n\n24.95\n\n19.86\n\n21.46\n\n23.53\n\n20.80\n\n19.77\n\n19.97\n\n15.77\n\n16.07\n\n0\n\n0\n\n42.2% -1.39%\n\n54%\n\n-1.78%\n\n50.3% -0.84%\n\n45.5% -0.38%\n\n51.8% -0.68%\n\n54.2% -0.17%\n\n53.1% -1.25%\n\n60.84% -0.14%\n\n60.1% -0.26%\n\nN\n\nN\n\nN\n\nN\n\nN\n\nY\n\nY\n\n*\n\n*\n\nGlobalPru(Fixed)\n\nGlobalPru(Mixed)\n\nOurs\n\nOurs\n\nabsolute advantage in pruning rate and only has a slight accuracy drop, i.e., −0.07% and −0.01 respectively for fixed and mixed modes, than state-of-the-art ThiNet. And when compared to the pioneering dynamic pruning method, our method gains and obviously less accuracy loss than the newest Liu et al. (2019), and outperforms Lin et al. (2017); Gao et al. (2019) in both compression rate and accuracy.\n\n4.2 COMPARISON ON RESIDUAL ARCHITECTURE\n\nWe further explore the effectiveness of GlobalPru on the residual neural networks, where GlobalPru explores global attention inside residual blocks by adding the vanilla input feature representations to the output feature maps scaled by channel attention. To be specific, we introduce compression on each convolutional layer inside the residual block without changing the transfer of residual information. The compared methods include both SOTA static method He et al. (2018; 2019); Ning et al. (2020); Li\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Comparison of pruning performance on DepthWise architecture: MobileNet-V2 architecture and Cifar-10 dataset.\n\nCite\n\nDy?\n\nFLOPs(M)\n\n↓FLOPs\n\n-Acc\n\nMethod\n\nBaseline\n\nWM\n\nDCP\n\nNPPM\n\nZhuang et al. (2018)\n\nZhuang et al. (2018)\n\nGao et al. (2021)\n\nGlobalPru(Fixed)\n\nGlobalPru(Fixed)\n\nOurs\n\nOurs\n\n89.9\n\n66.53\n\n66.53\n\n47.65\n\n47.47\n\n35.83\n\n0\n\n26%\n\n26%\n\n47%\n\n0\n\n-3.68%\n\n-3.16%\n\n-3.1%\n\n47.2% -0.69%\n\n60.14% -0.55%\n\nN\n\nN\n\nN\n\n*\n\n*\n\net al. (2020a;b) and state-of-the-art dynamic pruning method Tang et al. (2021a) and Gao et al. (2019). The results are tested on ResNet-20 and Cifar-10. As shown in Table 2, the FLOPs, FLOPs decrease relative to the baseline and post pruning accuracy drop are reported. Our method under both fixed pruning rate and mixed pruning rate setting achieve the highest compression ratio at the same time with the most negligible accuracy drop. For example, even under the highest pruning rate 68% for all layers, the accuracy of our method still reaches 98.01% with only −0.34 degradation. Besides, we also compare the performance of GlobalPru with the classic three-stage “train-prune-fine-tune” static pruning at different pruning rates. According to Figure ??, we choose 40.1% and 61.1% as model pruning rates. They correspond to the point where the generalization performance is the highest and the point where the generalization accuracy starts to suffer, respectively. All of the comparisons show that GlobalPru can work efficiently on residual neural networks.\n\n(a) Pruning rate is 40.1%.\n\n(b) Pruning rate is 61.1%.\n\nFigure 2: Comparison of pruning performance under different pruning rates on ResNet-20 and Cifar-10. “Lottery” represents an typical three-stage static pruning paradigm.\n\n4.3 COMPARISON ON DEPTHWISE ARCHITECTURE\n\nAnother popular model architecture using “Depthwise Separable Convolution” is also included in our experiment to cover another class of models in the spotlight. Also, the depthwise architecture is originally designed for resource-limited mobile devices and is already very light-weight. To validate whether our method can further compress these already compressed models, we compare it with the most widely used efficient MobileNet-V2. As shown in Table 3, while outperforming all methods in computing reduction, our model achieves the closest model accuracy to the baseline. The advantage of our method on lightweight DepthWise architecture is obvious which proves that our model enables further model compression in an almost lossless mode over current state-of-the-art methods.\n\n8\n\n2004006008001000Interation4050607080Accuracy (%)LotteryGlobalPru2004006008001000Interation405060708090Accuracy (%)LotteryGlobalPruUnder review as a conference paper at ICLR 2023\n\nTable 4: Comparison of pruning performance on MiniImageNet (M-ImageNet) and SVHN datasets with popular model pruning SOTAs.\n\nAccuracy (%)\n\nDataset\n\nMethod\n\nTop 1\n\nTop 5\n\n↓Size\n\nPolarPrun\n\n85.857\n\n96.068\n\n0.59\n\nSlimPrun\n\n70.06\n\nM-ImageNet\n\nGlobalPru\n\n89.227\n\n90.44\n\n96.97\n\nPolarPrun\n\n97.099\n\n98.199\n\nSlimPrun\n\n96.848\n\n99.228\n\nSVHN\n\nGlobalPru\n\n97.543\n\n99.3\n\n0.60\n\n0.60\n\n0.59\n\n0.50\n\n0.60\n\nTable 5: Comparison of AUROC (%) of GlobalPru with typical adversarial detection methods on Fast Gradient Sign Method (FGSM)-based CIFAR100 and SVHN datasets .\n\nNon Prune\n\nPrune\n\nAUROC (%) FGSM SVHN CIFAR100\n\nMAHA\n\nFBS\n\nDPIC\n\nGlobalPru\n\n99.63 99.77\n\n99.95 100.00\n\n99.96 100.00\n\n100.00 100.00\n\n4.4 GENERALIZATION VERIFICATION\n\nIn addition to model accuracy, a good pruning method also needs to have good generalization and robustness, which is usually ignored in previous work. To show that GlobalPru has these favorable properties, we test the performance of GlobalPru on a more diverse vision dataset and extra visual tasks with added adversarial samples.\n\nAs shown in Table.4, we compare the performance of GlobalPru and two static pruning SOTAs on typical image datasets SVHN and MiniImageNet. SVHN contains large-scaled labeled data (over 600,000 digit images) for a more difficult, unsolved real-world problem. MiniImageNet, drawn from ImageNet, has complex samples and multi-classes, more suitable for prototyping and experimental research. Obviously, as a static alternative, GlobalPru achieves a higher model compression/task loss ratio than static pruning SOTA by removing the global redundancy of the model for all samples.\n\nAs shown in Table.5, we verify the robustness of our method by observing the performance of GlobalPru when faced with adversarial perturbations. It can be seen that GlobalPru exhibits extraordinary perturbation resistance, even surpassing some typical adversarial sample detection methods. We speculate that this is because the model redundancy removed by GlobalPru is selected by a majority vote of all samples, thereby weakening the effect of small image disturbances and making GlobalPru more robust than the previous pruning criterion only guided by task loss.\n\n5 CONCLUTION\n\nTo sum up, we propose a novel pruning method, i.e., the Channel Attention-based Learn-to-Rank Network, based on the shortcomings of current dynamic pruning methods, such as the need to save the complete model locally and repeat the forward computation. Our approach first explores the channel saliency rank of each sample, and then selects the most suitable channel rank supported by all current inputs through a majority voting mechanism. We define this channel rank as global channel attention. Next, we obtain a Channel Rank Network by proposing an efficient channel sorting algorithm to incorporate the knowledge of global channel attention into the training of the model, so as to quickly give an appropriate pruning response when new samples or sparse requirements arrive. While obtaining the advantages of dynamic pruning, our method avoids the defects of the original dynamic pruning method, and achieves better pruning performance than most state-of-the-art methods.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJifeng Dai, Kaiming He, and Jian Sun. Instance-aware semantic segmentation via multi-task network cascades. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3150–3158, 2016.\n\nXuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicated network with less inference complexity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5840–5848, 2017.\n\nZhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. HAWQ-V2: hessian aware trace-weighted quantization of neural networks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nShangqian Gao, Feihu Huang, Weidong Cai, and Heng Huang. Network pruning via performance maximization. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 9270–9280. Computer Vision Foundation / IEEE, 2021.\n\nXitong Gao, Yiren Zhao, Łukasz Dudziak, Robert Mullins, and Cheng-zhong Xu. Dynamic channel\n\npruning: Feature boosting and suppression. arXiv preprint arXiv:1810.05331, 2018.\n\nXitong Gao, Yiren Zhao, Lukasz Dudziak, Robert D. Mullins, and Cheng-Zhong Xu. Dynamic channel pruning: Feature boosting and suppression. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015b.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pp. 2234–2240. ijcai.org, 2018.\n\nYang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4340–4349, 2019.\n\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pp. 1389–1397, 2017a.\n\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pp. 1389–1397, 2017b.\n\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. CoRR, abs/1709.01507, 2017.\n\nWeizhe Hua, Yuan Zhou, Christopher M De Sa, Zhiru Zhang, and G Edward Suh. Channel gating\n\nneural networks. Advances in Neural Information Processing Systems, 32, 2019.\n\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\n\nefficient convnets. arXiv preprint arXiv:1608.08710, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\nXiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 510–519, 2019.\n\nYawei Li, Shuhang Gu, Christoph Mayer, Luc Van Gool, and Radu Timofte. Group sparsity: The hinge between filter pruning and decomposition for network compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8018–8027, 2020a.\n\nYawei Li, Shuhang Gu, Kai Zhang, Luc Van Gool, and Radu Timofte. Dhp: Differentiable meta pruning via hypernetworks. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VIII 16, pp. 608–624. Springer, 2020b.\n\nLucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning for efficient neural networks. In International Conference on Learning Representations, 2019.\n\nJi Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In Proceedings of the 31st\n\nInternational Conference on Neural Information Processing Systems, pp. 2178–2188, 2017.\n\nChuanjian Liu, Yunhe Wang, Kai Han, Chunjing Xu, and Chang Xu. Learning instance-wise sparsity for accelerating deep models. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 3001–3007. ijcai.org, 2019.\n\nZhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pp. 2736–2744, 2017a.\n\nZhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pp. 2755–2763. IEEE Computer Society, 2017b.\n\nJian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, pp. 5058–5066, 2017.\n\nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11264–11272, 2019.\n\nXuefei Ning, Tianchen Zhao, Wenshuo Li, Peng Lei, Yu Wang, and Huazhong Yang. Dsa: More efficient budgeted pruning via differentiable sparsity allocation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, pp. 592–607. Springer, 2020.\n\nYongming Rao, Jiwen Lu, Ji Lin, and Jie Zhou. Runtime network routing for efficient image classification. IEEE transactions on pattern analysis and machine intelligence, 41(10):2291–2304, 2018.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n\nYehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang Xu. Scop: Scientific control for reliable neural network pruning. Advances in Neural Information Processing Systems, 33:10936–10947, 2020.\n\nYehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, and Chang Xu. Manifold regularized dynamic network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5018–5028, 2021a.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, and Chang Xu. Manifold regularized dynamic network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5018–5028, 2021b.\n\nYulong Wang, Xiaolu Zhang, Xiaolin Hu, Bo Zhang, and Hang Su. Dynamic network pruning with interpretable layerwise channel selection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 6299–6306, 2020.\n\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\n\ndeep neural networks. Advances in neural information processing systems, 29, 2016.\n\nZhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, and Jin-Hui Zhu. Discrimination-aware channel pruning for deep neural networks. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 883–894, 2018.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe proposed work initially obtains a majority vote-based prior on the global rank of channel saliencies before forcing each sample-level channel saliency to match the global prior. In this way, the proposed work aims to use the platform of static pruning yet match the high pruning levels similar to dynamic pruning while maintaining a common channel saliency across all samples.\n\n# Strength And Weaknesses\n\nStrengths\n- The context and explanation provided for static and dynamic pruning are well done.\n\nWeaknesses\n- Quantitatively, on a channel to channel comparison, could the authors provide more insight in to the difference in performance and saliency between dynamic pruning approaches and GlobalPru? This could expose both the positive and negative aspects of both approaches.\n- Channel level attention spans multiple ideologies and cannot be solely categorized as methods that are \"local\", especially over datasets, since certain methods learn inter-channel relationships over the dataset as opposed to sample-specific properties. Could the authors justify their statement in Pg. 3, Section 2.2, Lines 8-10?\n- I encourage the authors to take a closer look at Figure 1 and revise it slightly so that it can be a common reference to the underlying process, especially across later sections. As constituted currently, there are certain missing elements and the flow of processes in the diagram is confusing.\n- Equation 1 emphasizes objective functions which learn the mask to be applied on the weight matrices. Could the authors clarify if subsequent comparisons in the experiments section maintain this characteristic?\n- The nomenclature of \"prior\" and \"global attention during training\" need to be clearly defined before being put to use. As constituted currently, they are clarified just before Section 3.3. I encourage the authors to revise the explanation in Section 3 to ensure preliminary terms are well defined before they are put to use.\n- Could the authors clarify in some detail the reasoning behind the choice of expressions and formulation for the $\\phi()$, and all relevant information beyond Equation 5? On first glance, there seems to be some inconsistency in notation in Equations 6 and 7.\n- After establishing $\\alpha, \\beta$ as balancing coefficients in the main loss function, the experimental setup highlights their values to be 0.0001. Could the authors justify the choice of small values, including a comparison of the impact of varying them across a range of values?\n- Results from Table 1 consistently compare against ThiNet. However, there exist a number of more advanced methods, even static pruning, which improve upon ThiNet. Could the authors provide comparisons against current works that improve upon the performance of ThiNet?\n- Could the authors clarify the baseline performance across the result tables provided and whether they match the relative drop in accuracy values across baselines?\n- Figure 2, the X-axis is incorrectly labelled \"Interation\". Please revise the label.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity\nConceptually, the ideas are clear. However, the explanation of $\\phi()$ and related formulae is unclear.\n\nQuality and Originality\nA one-to-one relative comparison of dynamic vs. the proposed approach is not available. This direct comparison could serve to highlight a number of aspects of the proposed work, in terms of bringing concepts from dynamic pruning and meshing them into static approaches. In addition, the tables of results can be further updated to reflect state-of-the-art methods in the pruning domain. \nWhile the idea of a common channel attention rank is interesting, the above issues detract from the current work.\n\n# Summary Of The Review\n\nJustifying and addressing the points addressed in the weaknesses mentioned above could serve to highlight interesting comparisons between the proposed and existing methods.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nVISUALLY-AUGMENTED LANGUAGE MODELING\n\nWeizhi Wang†, Li Dong‡, Hao Cheng‡, Haoyu Song‡, Xiaodong Liu‡, Xifeng Yan†, Jianfeng Gao‡, Furu Wei‡ †University of California, Santa Barbara ‡Microsoft Research weizhiwang@ucsb.edu, {lidong1, haocheng}@microsoft.com\n\nABSTRACT\n\nHuman language is grounded on multimodal knowledge including visual knowledge like colors, sizes, and shapes. However, current large-scale pre-trained language models rely on text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary. To address this, we propose a novel pre-training framework, named VALM, to Visually-augment text tokens with retrieved relevant images for Language Modeling. Specifically, VALM builds on a novel latent text-image alignment method via an image retrieval module to fetch corresponding images given a textual context. With the visually-augmented context, VALM uses a visual knowledge fusion layer to enable multimodal grounded language modeling by attending to both text context and visual knowledge in images. We evaluate VALM on various visual knowledge-intensive commonsense reasoning tasks, which require visual information to excel. The experimental results illustrate that VALM outperforms all strong language-only and vision-language baselines with substantial gains in reasoning object commonsense including color, size, and shape. Our code is available at https://github.com/Victorwz/VaLM.\n\n1\n\nINTRODUCTION\n\nLarge-scale pre-trained language models (PLMs) have achieved great success in promoting state of the art on various natural language understanding and generation tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Yang et al., 2019; Brown et al., 2020; Wang et al., 2022). PLM self-supervision training largely benefits from harvesting local context information in the pre-training corpus. To further strengthen such contextual self-supervision, recent seminal works, e.g. GPT-3 (Brown et al., 2020) and Megatron-LM (Narayanan et al., 2021), focus on increasing the model size and the scale of pre-training corpus. With billions of parameters, these tremendous PLMs exhibit incredible ability as zero-shot or few-shot learners. More remarkably, PLMs can achieve human-parity performance on various downstream tasks, even without any task-specific supervision. Another major research line of PLMs is to enhance the language model with auxiliary knowledge (Wei et al., 2021), including entity knowledge (Yu et al., 2020), relational knowledge (Zhang et al., 2019; Qin et al., 2021), text chunk (Lewis et al., 2020; Wu et al., 2022; Borgeaud et al., 2021), etc. The incorporation of various knowledge resources to PLMs mitigates the drawbacks of local contextual attention, bringing additional relevant global context that benefits both language understanding and generation tasks.\n\nSince current unimodal PLMs lack visual knowledge grounding, they inevitably suffer from the hallucination problem, which refers to the inconsistent or false statements generated by PLMs with respect to the world knowledge (Logan et al., 2019). For instance, the PLMs may predict the color of the sky as red only due to the statistical contextual correlations between the token “color” and “red” in the pre-training corpus, neglecting the commonsense facts.\n\nIn this paper, we propose a novel framework to enable language model pre-training to take full advantage of both local text context and corresponding visual knowledge. Recent work on joint visionlanguage model (VLM) pre-training (Su et al., 2020; Tan & Bansal, 2020) relies on explicit alignments between text and image, e.g. supervised image captioning data, which limits the cross-modality fusion during fine-tuning/inference over text without accompanying images. As a consequence, later in our experiments (section 3), those prominent VLMs are found to achieve unsatisfactory performance on visual knowledge-intensive commonsense reasoning tasks. Instead, we design a flexible text-image\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nalignment mechanism via an image retrieval module that gathers related images for each token as visual augmentation. To achieve better language-vision grounding, we propose a visual knowledge fusion layer to enable joint attention across visually-augmented context including both textual tokens and retrieved images. Based on this, we build up a Visually-augmented Language Model, VALM, with flexible on-the-fly visual knowledge enhancement.\n\nWe evaluate the effectiveness of the proposed VALM on various commonsense reasoning and language-only benchmarks. Experimental results demonstrate that our model consistently outperforms the unimodal and multimodal baselines in terms of object commonsense reasoning. Remarkably, our method substantially improves +14.50%, +17.80%, and +11.68% accuracy on MEMORYCOLOR, RELATIVESIZE and OBJECTSHAPE datasets, respectively. Additional experiments on natural language understanding tasks also validate that the proposed visually-augmented language modeling framework could be helpful to improve the fundamental natural language understanding capability of PLMs.\n\nOur contributions are summarized as follows:\n\n• We propose a novel visually-augmented casual language model, VALM, to enable the language model to utilize visual knowledge flexibly and effectively. Through the proposed visual knowledge fused language modeling, VALM is capable of accomplishing tasks with the high demand of cross-modality knowledge, such as visual commonsense reasoning.\n\n• We design a framework to construct flexible on-the-fly text-image alignments and fuse augmented images into the context of language modeling. We implement an image retrieval module to query token-level representation in a large-scale cached image database and retrieve its nearest neighbors as the augmentation. With the proposed visual knowledge fusion layer, VALM can effectively take full advantage of both language information from local text context and visual information from retrieved images.\n\n• Experimental results demonstrate that VALM effectively alleviates the hallucination problem of PLMs via introducing visual knowledge in language model pre-training. VALM achieves significant performance improvements in inferring the commonsense object properties.\n\n2 METHODS\n\nWe propose a novel multi-modal pre-trained language model, which is augmented with retrieved images, named VALM. The architecture of VALM is presented in Figure 1. VALM augments each token in pre-training text corpus with k retrieved related images. VALM uses an image retrieval module to retrieve corresponding images for each token. The image retrieval module deploys a pre-trained CLIP model, which is capable of unifying the textual query and image candidates into a joint embedding space. VALM constructs a cached large-scale image knowledge base using image encoder of CLIP, and uses the contextual representation of each token as textual query to search its nearest neighbors in image knowledge base. With the help of the unified text and image embedding space provided by CLIP, the image nearest neighbors are taken as augmented images of each token to construct text and image alignments. We then propose a visual-knowledge fusion layer to enable learned hidden state to attend to both texts and augmented images.\n\n2.1 VALM: VISUALLY-AUGMENTED LANGUAGE MODELING\n\ni=1, the embedding layer first encodes input vector {xi}N\n\nGiven an input text sequence {xi}N i=1 into embedding space and outputs the initial hidden state H0 to the successive Transformer decoder layers. Then the proposed VALM model encodes H0 into visual knowledge fused contextual representations at difference levels H = {Hl}L l=1 via L − 1 Transformer decoder layers and one special visual knowledge fusion layer. Each Transformer decoder layer is identical to Vaswani et al. (2017), which outputs the contextual representations at different semantic levels given the representation from the previous layer Hl = Layerl(Hl−1), l ∈ [1, L]. The visual knowledge fusion layer is proposed as a variant of the Transformer decoder layer to incorporate visual knowledge in contextual learning via joint attention on both text contexts and augmented images. The visual knowledge fusion layer is injected in the second-to-last layer of VALM. The visual knowledge is stored in corresponding augmented image representations, obtained from\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Overview of visually-augmented language modeling (VALM). We conduct dense retrieval to get top-k images for the input context at each time step. Then the visual knowledge fusion layer attends to both text tokens and retrieved images. The vision-language fused representation is fed back to Transformer for language modeling.\n\nimage retrieval module {{zij}K j=1} = frt(xi). Then the visual knowledge fusion layer takes the input including both contextual representation of the previous layer and augmented image sets and outputs a visual-knowledge fused contextual representation HL−1 = VisualLayer({HL−2 i=1). Finally, the output contextual representations are passed into the output projection layer and a softmax function is used to compute the token probability P (xi|x1, · · · , xi−1) = softmax(W HL + b).\n\n, {zij}K\n\nj=1}N\n\ni\n\nWe conduct generative unsupervised pre-training (Radford et al., 2019) for VALM on a large-scale text corpus. The training objective of VALM is the standard left-to-right language modeling objective, which maximizes the likelihood of the next word token based on the left context:\n\nmax\n\n(cid:88)\n\n|x| (cid:88)\n\nx∈D\n\ni=1\n\nlog P (xi|x1, · · · , xi−1),\n\n(1)\n\nwhere x represents a sentence randomly sampled from the large-scale pre-training text corpus D.\n\n2.2\n\nIMAGE RETRIEVAL\n\nThe visual knowledge corresponding to a specific token is stored in its correlated images. Therefore, to prepare the fused visual knowledge, VALM deploys an image retrieval module to retrieve augmented images, denoted as frt(·). In order to achieve multi-modality text-image retrieval, it is of great importance to building up a discriminator to assess the correlation of every image in the extremely large-scale open image knowledge bases to the specific text representation. CLIP (Radford et al., 2021) proposed a simple-yet-effective method to connect images and texts into a unified multi-modal embedding space. We directly deploy the pre-trained CLIP model to encode the images and texts to enable a nearest neighbor text-image retrieval. Specifically, the pre-trained CLIP model we use in constructing the image retrieval module includes a ResNet-50x16 (He et al., 2016) model as an image encoder and a Transformer (Vaswani et al., 2017) model as a text encoder. Here, we only use the CLIP model as the backbone of our image retrieval module, and the CLIP parameters are not updated during the pre-training process of VALM.\n\nImage Knowledge Base Creation. The image knowledge base of the retrieval module is the cache of a set of image keys, which are the high-level visual representations of images. Given an image z ∈ Dimg, such visual representation can be obtained via forwarding image z to the pre-trained CLIP image encoder. Then the whole image knowledge base (Z) is constructed by taking the output hidden state fθI (x) as image keys: Z = (cid:83) {fθI (z)}, where θI represents the image encoder parameters.\n\nz∈Dimg\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nTextual Query. We take the contextual representation of each token as the query in the nearest neighbor search. For each sentence x ∈ D, the contextual representation of i-th token is computed via fθT (x<i), where θT represents the text encoder parameters. As the input sequence length of VALM generally exceeds the input length limitation of 75 tokens of CLIP text encoder, the long context x<i\n\n(cid:26) x[t,i−1],\n\nx[i−75,i−1],\n\ni − t < 75, i − t ≥ 75,\n\nis cut off into a context-chunk yi for fitting in CLIP text encoder: yi =\n\nwhere t is the index of the closest stop character before i-th token. Then the textual query for i-th token is computed as its context-chunk representation as fθT (yi).\n\nkNN Text-Image Retrieval. The retrieval module uses the contextual representation to search the cached image knowledge base (Z) and retrieves k nearest neighbor image keys w.r.t. dot product distance. As the pre-trained CLIP model has learned a joint embedding space for text and image domain, the retrieved images {zij}K j=1 are thus regarded as the top-k relevant images to the query.\n\n2.3 VISUAL KNOWLEDGE FUSION\n\nWith the help of the image retrieval module, each token in the pre-training corpus is augmented with k corresponding images, and these augmented images are represented in the joint embedding space with texts. Then the augmented image representations are directly treated as auxiliary “context” in the learning process.\n\nAs the conventional Transformer decoder layer uses the multi-head self-attention (Vaswani et al., 2017) to learn the contextual representation, we extend it to a joint-attention mechanism and propose a novel visual knowledge fusion layer to enable each token to attend to both contexts and retrieval images jointly. In addition, due to the inconsistency in magnitude and distribution between contextual hidden states and retrieved image representations, we apply Layer Normalization (Ba et al., 2016) on retrieved K image representations to alleviate such inconsistency, denoted as LNimg. Assume that the hidden state output for i-th token is hi and the corresponding retrieved images are {zij}K j=1, the hidden state HL−1\n\nis computed as:\n\ni\n\nQ = HL−2W Q + bQ, K = HL−2W K + bK, V = HL−2W V + bV , ̇kik = LNimg(zik)W K + bK\n\nimg, ̇vik = LNimg(zik)W V + bV\n\nimg,\n\nei =\n\nQiKT √\n\nd\n\n, ai =\n\neik =\n\nQi\n\n ̇kT ik√ d\n\n, aik =\n\n(cid:80)L\n\nexp (ei) j=1 exp (eij) + (cid:80)K exp (eik) j=1 exp (eij) + (cid:80)K\n\n(cid:80)L\n\nk=1 exp (eik)\n\nk=1 exp (eik)\n\n,\n\n,\n\nHL−1\n\ni\n\n= aiV +\n\n(cid:88)\n\nk\n\naik ̇vik,\n\n(2)\n\n(3)\n\n(4)\n\n(5)\n\n(6)\n\ni\n\nwhere Qi, ̇kik, ̇vik ∈ RE, K, V ∈ R|x|×E, ei, ai ∈ R|x|. The hidden state output from the previous layer HL−1 is linearly projected into contextual queries, keys, and values Q, K, V separately. K is the number of retrieved images for each token, and E is the embedding dimension for both context and image representations. In order to generate image-specific attention keys and values, we adopt image-specific bias bK img in linear projections and reuse the contextual projection weights W K, W V to generate image-specific attention keys and values. Moreover, it is vital to mention that the image-specific attention keys and values are distinct for each query token, which is highly different from self-attention where the contextual keys and values are kept the same for each token. A secondary subscript k is used to denote different image representations for the i-th token.\n\nimg, bV\n\n3 EXPERIMENTS\n\n3.1 PRETRAINING SETUP\n\nText Corpus. We use the English corpus of CC-100 (Conneau et al., 2020) as the pre-training text corpus for both VALM and baseline GPT-2∗. CC-100 corpus is one of the largest high-quality\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTask\n\nExample Prompt\n\nObject / Pair Answer\n\nObject Color Reasoning Object Shape Reasoning Object Size Reasoning\n\nThe color of [object] is [answer] The shape of [object] is [answer] Is [Item1] larger than [Item2]? [answer]\n\nthe sky apple (sofa, cat)\n\nblue round Yes\n\nTable 1: Evaluation examples of object color, shape, and size reasoning.\n\nweb-crawl text data. The English monolingual dataset of CC-100 contains about 55 billion tokens, stored in 301 GiBs disk storage. Due to the limitation of computing resources, we only consume 15% of CC-100 English monolingual corpus for pre-training VALM and baseline GPT-2∗.\n\nImage Data. We use the LAION Open Image Dataset (Schuhmann et al., 2021) as the image knowledge base for dense retrieval. To the best of our knowledge, the LAION Open Dataset is one of the world’s largest openly available image-text-pair dataset with 400 million samples. Due to the disk space limitation, we randomly select half of LAION images for the dense text-image retrieval, which is 200M images in total.\n\nPre-training Hyperparameters. The proposed model deploys transformer decoder architecture with 124M trainable parameters. Hyperparameter setting and training details are presented in Appendix B.1.\n\nRetrieval Module. For the implementation of the dense text-image retrieval module, we use the faiss (Johnson et al., 2021) toolkit to construct the efficient index. The faiss index contains the whole 200M image keys and provides the efficient nearest neighbor search. For efficiency purposes, we quantize all image keys to 32 bytes. faiss index stores image keys in clusters to speed up the search, which requires the additional training process to learn the cluster centroids. We use 10M keys for learning 131k cluster centroids and search 32 clusters to find the nearest neighbors during inference. We load the faiss index to GPU to achieve efficient dense text-image retrieval.\n\n3.2 VISUAL KNOWLEDGE INTENSIVE TASKS\n\nThe visual information stored in retrieved images can play a useful role in providing relevant visual knowledge to help language models perform better grounded commonsense reasoning. Such helpful visual information can be colors, positions, sizes, spatial relations, etc. The task of object commonsense reasoning requires language models to predict the correct visual property for a given object. To excel these tasks typically require models to capture and utilize intensive visual knowledge without any explicit text demonstrations or external knowledge bases. Due to reporting biases, such descriptive text of object properties rarely appears in text corpora, likely making this type of knowledge absent from language models. Thus, those visual knowledge-intensive tasks are likely challenging for both language models and vision-language models.\n\nWe first compared VALM and recent baselines on four object commonsense reasoning datasets, MEMORYCOLOR (Norlund et al., 2021), COLORTERMS (Bruni et al., 2012), OBJECTSHAPE (Zhang et al., 2022a) reasoning, and RELATIVESIZE (Bagherinezhad et al., 2016). In addition, we use another physical interaction question answering dataset (PIQA) (Bisk et al., 2020), to evaluate whether such visual commonsense knowledge could be implicitly encoded and utilized in the question answering process. In Table 1, we provide examples for different visual commonsense reasoning tasks.\n\nMEMORYCOLOR and COLORTERMS Dataset. The memory color of a concrete object is the typical color an object appears in, e.g. the color of banana is mostly memorized as yellow. Norlund et al. (2021) proposed this dataset for evaluating visual knowledge transfer in multi-modal language models. The dataset contains 109 objects paired with their memory color labels, an illustrating picture, and a descriptor. The COLORTERMS dataset also contains a list of common items manually labeled with their commonsense color. Both datasets hold a set of 11 color labels.\n\nOBJECTSHAPE Dataset. Zhang et al. (2022a) proposed a visual commonsense dataset with a set of object attributes like shape. The dataset of object shapes contains 140 objects with their shape label. The OBJECTSHAPE dataset consists of 12 shape categories.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nRELATIVESIZE Dataset. Bagherinezhad et al. (2016) proposed the RELATIVESIZE dataset, which includes a total of 486 object pairs between 41 physical objects. The task of object size reasoning requires the model to predict the size relations between two given objects, e.g., an ant is smaller than an elephant. The size information is again rarely included and described in text, while it is much easier to capture from the images. We convert the size relation reasoning task into a binary question-answering form with \"Yes\"/\"No\" answers.\n\nPHYSICAL INTERACTION QUESTION ANSWERING. Physical Interaction Question Answering (PIQA) is proposed and designed to investigate the physical commonsense knowledge of existing language models (Bisk et al., 2020). Completing such question answering tasks requires the language model to effectively utilize physical commonsense knowledge, i.e. knowledge of basic properties of the objects (flexibility, curvature, and being porous). Language models are supposed to first achieve the perception of objects and later encode such physical knowledge into the language modeling process. Each data sample in PIQA contains one goal and two solutions. The model is supposed to figure out and predict the more reasonable and appropriate solution between two candidates.\n\nEvaluation Setting. We evaluate VALM and all baseline methods in a zero-shot manner without any task-specific tuning. Specifically, VALM takes the input consisting of textual prompts and objects during inference and predicts the property label as the last token. The prompts used in evaluating object color, shape, and size reasoning performance are listed in Appendix Table 11. We use the top-1 accuracy as the evaluation metric and compute the average accuracy of all listed prompts to increase evaluation robustness. For PIQA, we follow Shwartz et al. (2020) to use the cross-entropy loss as the scorer for each potential solution score(sij) = CE([gi, sij]), j ∈ [0, 1]. Then the solution with lower scores is selected as the prediction. The classification accuracy is used as the evaluation metric.\n\nBaselines. We consider both pretrained language-only and vision-language models as baselines. In particular, three strong language models are considered for comparison with VALM, including 1) GPT-2∗ (Radford et al., 2019); 2) BERT Devlin et al. (2019); and 3) CaptionBERT (Zhang et al., 2022a), a pre-trained auto-encoding language model on Oscar’s (Li et al., 2020) caption-based text data. Here, GPT-2∗ is re-implemented and trained from scratch using the identical training data, hyper-parameter settings, and model size as the proposed VALM. Additionally, we also compare VALM with prominent vision-language models, including 1) OSCAR (Li et al., 2020), a pre-trained vision-language model with learned representations that capture channel-invariant factors (i.e. object tags) at the semantic level; 2) VisualBERT (Li et al., 2019), a vision-language model with learned joint contextualized representations across vision and language; 3) CLIP (Radford et al., 2021), a vision-language system with one image encoder and one text encoder which are mapped into a same cross-modal embedding space. We directly use OSCAR and VisualBERT as auto-encoding language models for zero-shot evaluations. For CLIP, we first retrieve the corresponding image using the concatenated query prompt and the given object. Then, the dot-product similarity of the retrieved image vector and the candidate-aware text vector (including the query prompt, the given object, and one candidate label) is used to rank. Finally, the top-ranked candidate label is regarded as the prediction for evaluation.\n\nResults. The main results on four object commonsense reasoning datasets are summarized in Table 2. The two variants of VALM (K = 4, 8) significantly outperform all considered language models and vision-language models on object color and shape reasoning datasets, with an improvement of +14.50%, +13.56%, and +11.68% on MEMORYCOLOR, COLORTERMS, and OBJECTSHAPE respectively. Moreover, the proposed VALM with K = 4 achieves an encouraging result with +17.80% accuracy gain over the best baseline, VisualBERT on RELATIVESIZE. The substantial improvements on these datasets demonstrate that VALM takes full advantage of visual knowledge (object visual property) to complete the corresponding visual commonsense reasoning. Surprisingly, the zero-shot evaluation results of all auto-encoding language models and vision-language models are below 40% accuracy on object color and shape reasoning datasets. Although pretrained with aligned text-image pairs, those vision-language models cannot effectively utilize relevant visual knowledge from their jointly contextualized vision-language representations. Among language models, the auto-regressive PLMs significantly outperform auto-encoding PLMs, suggesting that auto-regressive PLMs are likely better at zero-shot reasoners. We also observe that retrieving more images for each token results in a performance drop for object size and shape reasoning. We attribute the degradation\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nColor (ACC↑)\n\nShape (ACC↑) COLORTERMS OBJECTSHAPE\n\nModel\n\nK\n\nN/A GPT-2* BERT N/A CaptionBERT N/A N/A CLIP N/A OSCAR N/A VisualBERT\n\nVALM VALM\n\n4 8\n\nMEMORYCOLOR\n\n44.14% 24.34% 24.84% 26.25% 20.32% 26.68%\n\n53.99% 58.64%\n\n39.10% 26.33% 28.40% 23.08% 16.86% 38.02%\n\n52.66% 50.19%\n\n51.09% 31.86% 38.14% 13.66% 33.14% 11.14%\n\n62.77% 59.41%\n\nSize (ACC↑) RELATIVESIZE\n\n47.22% 34.78% 66.05% 47.99% 50.14% 67.23%\n\n85.03% 62.35%\n\nTable 2: Accuracy on object commonsense reasoning datasets. GPT-2∗ is the re-implemented model with identical pre-training data and hyper-parameter settings to VALM. K represents the number of images augmented to each token. Best performance is marked with bold.\n\nModel\n\nGPT-2*\n\nBERT\n\nCaptionBERT VALM (K=4) VALM (K=8)\n\nPIQA (ACC↑)\n\n62.53% 54.73%\n\n53.97%\n\n64.31%\n\n64.64%\n\nTable 3: Accuracy on Physical-Interaction Question-Answering benchmark.\n\nto the increased noise brought by augmenting with more images which causes model confusion when differentiating relevant visual information from irrelevant one.\n\nPIQA is a more challenging task that requires the model to reason useful implicit object properties and utilize these commonsense in the question answering process. The results on PIQA are presented in Table 3. As is shown, VALM outperforms all baseline language models with +2.11% accuracy improvement. The two variants of VALM achieve almost identical performance because the selection for the correct solution is based on the language modeling perplexity, indicating that the two variants demonstrate similar language modeling capability.\n\n3.3 NATURAL LANGUAGE UNDERSTANDING AND LANGUAGE MODELING TASKS\n\nThe casual language modeling pre-training task enables PLMs to naturally perform natural language understanding (NLU) and long-text modeling. Therefore, the zero-shot natural language understanding and language modeling performance are widely adopted to evaluate the capability of PLMs (Radford et al., 2019). Here, we evaluate VALM and the most relevant language model baseline GPT-2∗ on four NLU datasets, SST-2 (Socher et al., 2013), MPQA (Wiebe et al., 2005), DBPeida (Auer et al., 2007), and AGNews (Zhang et al., 2015). The prediction accuracy is used as the evaluation metric. In addition, following Radford et al. (2019), Wikitext-103 (Merity et al., 2017) and Lambada corpus (Paperno et al., 2016) are considered to study the language modeling performance in a zero-shot manner. We report perplexity for two corpora and also report last-word prediction accuracy for Lambada corpus.\n\nThe results on natural language understanding are summarized in Table 4. It is easy to see that VALM achieves decent improvements on all four NLU tasks, indicating that the cross-modality knowledge learned in our model is likely helpful for typical natural language understanding. Thus, our visually-augmented language modeling framework can be further explored to enhance the natural language understanding ability of PLMs. Table 5 illustrates the results of language modeling tasks. Again, VALM slightly improves the perplexity on both datasets, +0.68 on Wikitext-103 and +0.08 on Lambda. A similar trend is observed for the final word prediction accuracy on Lambada. Different from previous visual knowledge intensive commonsense reasoning tasks (subsection 3.2), we find that VALM models with different numbers of retrieved images (K = 8 vs K = 4) perform similarly on the intrinsic language modeling task, suggesting that VALM can effectively ignore irrelevant visual information when the task is unlikely to benefit from visual knowledge. In other words, visual commonsense reasoning tasks require more fine-grained fusions of text and image, i.e. locating the text object in the image set, extracting relevant vision information, and verbalizing reasoning output. In contrast, a certain portion of text from general language modeling corpora s is probably not visually related. Thus, only a coarse-grained fusion is sufficient here (e.g. deciding if the image set is\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nModel\n\nK\n\nSST-2 MPQA DBPedia AGNews ACC↑\n\nACC↑\n\nACC↑\n\nACC↑\n\nMajority N/A 50.90% 50.00% N/A 68.04% 71.25% GPT-2*\n\nVALM VALM\n\n4 8\n\n70.12% 78.70% 67.33% 77.35%\n\n9.4% 67.20%\n\n72.27% 68.48%\n\n25.0% 53.51%\n\n53.81% 59.77%\n\nTable 4: Zero-shot evaluation results on natural language understanding tasks (SST-2, MPQA, DBPedia, AGNews). Majority: majority class.\n\nModel\n\nK\n\nGPT-2∗ N/A\n\nVALM VALM\n\n4 8\n\nWikitext-103 Lambada Lambada\n\nPPL↓\n\n36.44\n\n35.78 35.76\n\nPPL↓\n\n42.46\n\n42.51 42.38\n\nACC↑\n\n42.17%\n\n42.65% 42.87%\n\nTable 5: Zero-shot evaluation results on language modeling tasks. We report perplexity (PPL) on Wikitext-103 and Lambada and final word prediction accuracy (ACC) on Lambada.\n\nuseful), making the language modeling evaluation less affected by the retrieval noise from augmented images.\n\n3.4 ABLATION STUDIES\n\nSo far, we empirically verify the effectiveness and superiority of VALM in utilizing visual knowledge for both visual knowledge-intensive tasks and traditional language understanding and modeling. To figure out how the visual information takes effect in our model, we focus on two questions here: 1) Is the model capable of using the retrieved image representations as \"auxiliary\" contexts? What is the effect of disabling such retrieved image representations during inference? To evaluate this, we design an ablation model which set K = 0 and disables image retrieval and fusion during inference. 2) Does the model learn to leverage visual knowledge in the retrieved images? What is the effect of directly augmenting randomly-retrieved image representations during inference? Thus, an ablation model which retrieves random images as augmentations during inference is used for probing. The results of the two ablation models, Randomly-Retrieval and Disable-Retrieval during the inference stage, are listed in the first two rows of Table 6. As we can see, both changes to the image retrieval result in noticeable performance degradation on all evaluation tasks. In particular, we find that disabling the image retrieval and augmenting no image during inference also makes a huge difference to the language modeling perplexity on two corpora, which is more related to pure text corpus rather than augmented images. Therefore, it suggests that VALM is able to effectively capture rich semantics from both pretraining sources, i.e. text corpus as well as augmented images. In other words, the improved zero-shot task transferability of VALM relies on visual information from augmented images, which complements the linguistic knowledge learned via text-based self-supervised learning. The results of the Randomly-Retrieval ablation model further illustrate that achieving the capability of integrating visual knowledge cannot be realized by only augmenting unrelated images to language models, while only context-relevant images can make a true difference.\n\nVALM proposes a novel visual knowledge fusion layer with a joint context-image attention mechanism as a key component to fuse visual knowledge into the language model. The separate linear projection layers are regarded as important components to map contexts into different embedding spaces for attention keys and values. Therefore, the proposed joint self-attention mechanism naturally holds three variations to generate image keys and values: establish image-specific linear projections, reuse contextual linear projections, and only use specific linear bias for augmented images. We conduct the ablation study to evaluate the effect of these three alternatives on image linear projections. The results in Table 6 demonstrate that adopting image-specific projection bias outperforms directly sharing the contextual projection bias. Introducing additional image-specific linear projection weights does not lead to further performance improvement. Thus, we take the strategy of only adding additional linear bias for augmented images and reuse contextual linear weights in generating visual attention keys and values for implementation convenience and parameter efficiency.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nAblation Model\n\nRandomly-Retrieval Disable-Retrieval\n\nK\n\n4 0\n\nimg\n\nW K,V img , bK,V W K,V , bK,V W K,V , bK,V\n\n4 4\nimg (VALM) 4\n\nMemoryColor ColorTerms RelativeSize Wikitext-103 Lambada Lambada\n\nACC↑\n\n41.48% 43.12%\n\n48.62% 47.85% 53.99%\n\nACC↑\n\n38.46% 40.53%\n\n41.86% 46.30% 52.66%\n\nACC↑\n\n49.63% 59.36%\n\n83.54% 62.04% 85.03%\n\nPPL↓\n\n37.89 39.22\n\n35.95 35.95 35.78\n\nPPL↓\n\n43.56 44.59\n\n42.28 42.33 42.51\n\nACC↑\n\n41.35% 41.20%\n\n42.67% 41.74% 42.65%\n\nTable 6: Ablation studies on the effects of Randomly-Retrieval and Disabling-Retrieval during inference stage (Upper part). Second ablation study on the effects of introducing extra image-specific attention key and value projection weights W K,V img in Equation 3 for augmented images. The proposed model VALM is shown in the last row which introduces only image-specific bias and reuses contextual weight in attention key and value projection layers.\n\nimg or bias bK,V\n\n4 RELATED WORK\n\nPre-trained Language Models. Pre-trained language models (PLMs) revolutionized NLP research. Enabled by attention mechanism (Bahdanau et al., 2015) and Transformer architecture (Vaswani et al., 2017), the state-of-the-art PLMs, including BERT (Liu et al., 2019), GPT (Radford et al., 2018; 2019), RoBERTa (Liu et al., 2019), ELECTRA (Clark et al., 2020), T5 (Raffel et al., 2020), and OPT (Zhang et al., 2022b), have become the dominant approach in NLP tasks via the paradigm of pre-training on large-scale text corpora and fine-tuning on downstream tasks. With the exponential scaling up of model size, a surprising fact emerged that the PLMs like GPT-3 (Brown et al., 2020) can work as few-shot or zero-shot learners.\n\nVision-Language Models. Vision-language tasks are at the intersection area of both modalities of vision and language, like visual-question answering (Agrawal et al., 2015), and image captioning (Chen et al., 2015). ViL-BERT (Lu et al., 2019) firstly proposed to generate image region features via object detection and then learn joint multi-modal representations via an interacted two-stream model. OSCAR (Li et al., 2020) proposed to introduce object tags detected in images as anchor points to solve the issue of high demand for image-text alignments. Another significant pathway for VLMs is to construct a unified embedding space for texts and images and use textual prompts to extract task-specific labels during inference, of which the representative models are CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021).\n\nVisually-Grounded Language Learning. Visually-grounded language learning is an emerging research topic in vision-language learning, in which the proposed VALM can be categorized in this area with other prior works like Vokenization (Tan & Bansal, 2020), VidLanKD (Tang et al., 2021), and iACE (Lu et al., 2022). Visual information and knowledge can be memorized by the PLMs via fusion layer or concatenated inputs. However, extracting and utilizing the visual information efficiently and effectively is still difficult for uni-modal language models. Vokenization concatenated tokens and token-related images as “vokens\", transferring sentence-level caption text to token-level “voken\" with a Vokenizer model.\n\n5 CONCLUSION\n\nIn this paper, we propose a multi-modal framework VALM to enable auto-regressive language modeling to effectively utilize visual knowledge. Specifically, an effective text-to-image retrieval module is designed to construct latent text-image alignments for visually-grounded language modeling. Empowered by pre-training, VALM achieves improved zero-shot task transfer on downstream tasks. Experiments on various visual knowledge-intensive tasks demonstrate the effectiveness of our model over recent vision-language models. VALM also achieves decent improvements over language models on multiple representative natural language understanding tasks. For future work, we plan to adapt the model architecture to encoder-only and encoder-decoder Transformer backbones. We are also interested in more input modalities for VALM.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe would like to thank the anonymous reviewers for the helpful comments. We appreciate Zewen Chi and Hangbo Bao for the fruitful discussions, and Yaru Hao for helpful suggestions on evaluation benchmarks.\n\nREFERENCES\n\nAishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. VQA: Visual question answering. International Journal of Computer Vision, 123:4–31, 2015.\n\nSaeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal S. Mian, Fahad Shahbaz Khan, and Abdul Wahab\n\nMuzaffar. Image colorization: A survey and dataset. ArXiv, abs/2008.10774, 2020.\n\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia: A nucleus for a web of open data. In The semantic web, pp. 722–735. Springer, 2007.\n\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv, abs/1607.06450,\n\n2016.\n\nHessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants bigger than\n\nbutterflies? reasoning about sizes of objects. ArXiv, abs/1602.00753, 2016.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. CoRR, abs/1409.0473, 2015.\n\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\n\ncommonsense in natural language. In Proc. of AAAI, 2020.\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. W. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. ArXiv, abs/2112.04426, 2021.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\n\nElia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. Distributional semantics in\n\ntechnicolor. In ACL, 2012.\n\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. ArXiv, abs/1504.00325, 2015.\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training\n\ntext encoders as discriminators rather than generators. ArXiv, abs/2003.10555, 2020.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In ACL, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\n\nbidirectional transformers for language understanding. In NAACL, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pp. 3887–3896. PMLR, 2020.\n\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016.\n\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021.\n\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE\n\nTransactions on Big Data, 7:535–547, 2021.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,\n\nabs/1412.6980, 2015.\n\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. ArXiv, abs/2005.11401, 2020.\n\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple\n\nand performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\n\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pretraining for vision-language tasks. In ECCV, 2020.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.\n\nRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5962–5971, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1598.\n\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.\n\nYujie Lu, Wanrong Zhu, Xin Wang, Miguel P. Eckstein, and William Yang Wang. Imagination-\n\naugmented natural language understanding. ArXiv, abs/2204.08535, 2022.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\n\nmodels. ArXiv, abs/1609.07843, 2017.\n\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Ali Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei A. Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021.\n\nTobias Norlund, Lovisa Hagström, and Richard Johansson. Transferring knowledge from vision to\n\nlanguage: How to achieve it and how to measure it? ArXiv, abs/2109.11321, 2021.\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\n\nMichael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In NAACL, 2019.\n\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and R. Fernández. The lambada dataset: Word prediction requiring a broad discourse context. ArXiv, abs/1606.06031, 2016.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie Huang, Maosong Sun, and Jie Zhou. Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning. In ACL, 2021.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\n\nstanding with unsupervised learning. 2018.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.\n\nColin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683, 2020.\n\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. ArXiv, abs/2111.02114, 2021.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\n\nsubword units. ArXiv, abs/1508.07909, 2016.\n\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised\n\ncommonsense question answering with self-talk. arXiv preprint arXiv:2004.05483, 2020.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.\n\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training\n\nof generic visual-linguistic representations. ArXiv, abs/1908.08530, 2020.\n\nHao Tan and Mohit Bansal. Vokenization: Improving language understanding via contextualized,\n\nvisually-grounded supervision. In EMNLP, 2020.\n\nZineng Tang, Jaemin Cho, Hao Tan, and Mohit Bansal. Vidlankd: Improving language understanding via video-distilled knowledge transfer. Advances in Neural Information Processing Systems, 34: 24468–24481, 2021.\n\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n\nWeizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, and Weihua Luo. Task-oriented dialogue system as natural language generation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2698–2703, 2022.\n\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew O. Arnold. Knowledge enhanced pretrained language models: A compreshensive survey. ArXiv, abs/2110.08455, 2021.\n\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions\n\nin language. Language resources and evaluation, 39(2):165–210, 2005.\n\nYuhuai Wu, Markus N. Rabe, DeLesley S. Hutchins, and Christian Szegedy. Memorizing transformers.\n\nArXiv, abs/2203.08913, 2022.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019.\n\nDonghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng.\n\nJaket: Joint pre-training of\n\nknowledge graph and language understanding. ArXiv, abs/2010.00796, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nChenyu Zhang, Benjamin Van Durme, Zhuowan Li, and Elias Stengel-Eskin. Visual commonsense\n\nin pretrained unimodal and multimodal models. arXiv preprint arXiv:2205.01850, 2022a.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022b.\n\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text\n\nclassification. Advances in neural information processing systems, 28, 2015.\n\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced\n\nlanguage representation with informative entities. In ACL, 2019.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA ADDITIONAL RESULTS\n\nA.1 TIME-COST EFFECTS OF RETRIEVAL AND IMAGESET SIZE\n\nIntroducing efficient image retrieval on GPU brings a linear increase in inference time cost (about 2.1 times of text-only GPT-2∗ baseline), shown in Table 7. This cost is negligible with larger-size language models because the model forward cost will increase many times while the retrieval cost will not change with the model size. The retrieval cost can be further improved by searching fewer clusters or decreasing the number of encoding bytes for approximate image keys, with a minor trade-off on the performance. Moreover, efficient nearest neighbor search is an active research area (Guo et al., 2020) and we could try other efficient search tools to accelerate the retrieval.\n\nAs the introduced retrieval time cost is proportional to the size of imageset for dense retrieval, we provide more details on the relationship between retrieval time cost and imageset size, presented in Table 7. Concluded from Table 7, there is no significant performance decrease with the smaller imageset size from the original 200M down to 10M. As the 10M set is still large and sufficient for providing enough visual knowledge, we will consider deploying a 10M size imageset to train VALM for potential real-time industry applications.\n\nImage Size\n\n200M 100M 10M 1M\n\nMEMORYCOLOR\n\nColor (ACC↑)\n\nShape (ACC↑) COLORTERMS OBJECTSHAPE\n\nSize (ACC↑) RELATIVESIZE\n\nTimecost (GPT2∗ as 1x)\n\n53.99% 53.50% 51.79% 51.87%\n\n52.66% 49.71% 47.49% 46.31%\n\n62.77% 61.39% 62.18% 48.51%\n\n85.03% 81.84% 82.15% 82.35%\n\n2.06x 1.91x 1.79x 1.74x\n\nTable 7: Accuracy on object commonsense reasoning datasets of VALM (K = 4) with variants of retrieval imageset size. K represents the number of images augmented to each token.\n\nA.2 COMPARISONS WITH ADDITIONAL STRONG BASELINES\n\nWe compare VALM with Vokenization (Tan & Bansal, 2020) on four visual-knowledge-intensive tasks, and the results are shown in Table 8. In addition, we evaluate the performance of large language models on the visual–knowledge-intensive tasks for stronger and more fair comparisons. We evaluate the OPT (1.3B parameters) (Zhang et al., 2022b) model on these visual–knowledgeintensive tasks and the results are presented in Table 8. VALM(124M parameters) significantly outperforms the OPT-1.3B on four datasets, which further demonstrates the challenge of solving those visual-knowledge-intensive tasks and the effectiveness of our method.\n\nModel\n\nK\n\nOPT-1.3B\n\nN/A\n\nVokenization N/A\n\nVALM VALM\n\n4 8\n\nMEMORYCOLOR\n\nColor (ACC↑)\n\nShape (ACC↑) COLORTERMS OBJECTSHAPE\n\n39.25%\n\n14.18%\n\n53.99% 58.64%\n\n41.03%\n\n19.97%\n\n52.66% 50.19%\n\n19.21%\n\n48.35%\n\n62.77% 59.41%\n\nSize (ACC↑) RELATIVESIZE\n\n50.78%\n\n43.28%\n\n85.03% 62.35%\n\nTable 8: Accuracy on object commonsense reasoning datasets. K represents the number of images augmented to each token. Best performance is marked with bold.\n\nA.3 SCALING EFFECT OF VALM\n\nWe train the 355M model (GPT-2 Medium Size) of VALM (k=8) to evaluate the effects of scaling up model parameters. The results are presented in Table 9 and the model performance is significantly improved on four visual knowledge-intensive datasets. We will seek more computation resources to train large size VALM models.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nModel\n\nK\n\nMEMORYCOLOR\n\nColor (ACC↑)\n\nShape (ACC↑) COLORTERMS OBJECTSHAPE\n\nSize (ACC↑) RELATIVESIZE\n\nVALM-124M 8 VALM-355M 8\n\n58.64% 65.82%\n\n50.19% 55.36%\n\n59.41% 70.0%\n\n62.35% 72.79%\n\nTable 9: Accuracy on object commonsense reasoning datasets. K represents the number of images augmented to each token. Best performance is marked with bold.\n\nA.4 ABLATION STUDY OF K\n\nWe further conduct another ablation study by setting the number of augmented images K = 1 for VALM, which is very similar to the CLIP (Radford et al., 2021) inference. The results are presented in Table 10. VALM (k=1) significantly outperforms CLIP in all visual-knowledge-intensive tasks, validating the effectiveness of our method.\n\nModel\n\nK\n\nMEMORYCOLOR\n\nColor (ACC↑)\n\nShape (ACC↑) COLORTERMS OBJECTSHAPE\n\nSize (ACC↑) RELATIVESIZE\n\nCLIP VALM\n\nN/A 1\n\n26.25% 52.43%\n\n23.08% 46.01%\n\n13.66% 64.55%\n\n47.99% 86.73%\n\nTable 10: Accuracy on object commonsense reasoning datasets. K represents the number of images augmented to each token.\n\nA.5 CASE STUDIES\n\nWe provide a case study in the object color reasoning task for VALM. In order to reason the correct commonsense color of objects sky and parsley, VALM takes the input combination of the prompt and the object as “the color of [object] is”. Then we present the retrieval results of top-4 corresponding images to the textual query in Figure 2.\n\n(a)\n\n(b)\n\nFigure 2: The attention matrix visualization given the query prompt “the color of [object] is” for VALM. VALM achieves accurate image retrieval of top-4 images corresponding to the objects of sky and parsley as augmented images, shown in the horizontal index of each subfigure.\n\nA.6 COLORIZATION EFFECT\n\nWe conduct another interesting ablation case study to evaluate the effect of image color changes in the object color reasoning task. Specifically, VALM predicts the color label of an apple as red based on the commonsense in both contexts and retrieved images. The original prediction probability distribution is presented in Blue Bars of Figure 3(b). Then we replace the retrieved images with K unusual images of green apples in OBJECTCOLORIZATION dataset (Anwar et al., 2020), shown in\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3(a). The predicted probability distribution for 11 color types given replaced colorization objects is presented in Orange Bars of Figure 3(b). We could observe a clear probability increase in the color type of green and a decrease in that of red, which is confronted with the colorization process. This ablation study demonstrates VALM is capable of extracting useful visual knowledge from retrieved object images and inferring correct semantics based on that. Given retrieved object images in different colors, VALM could extract the correct color knowledge and adopt it in its semantic inference stage.\n\n(a) Images for green apples in OBJECTCOLORIZATION dataset as replacements.\n\n(b) Probability Distribution Visualization for retrieved images and colorization images.\n\nFigure 3: The visualization of the predicted probability distribution on 11 object color types with retrieved images and colorization images, respectively. The adopted prompt for reasoning the object color of an apple is “the color of [object] is”.\n\nB EXPERIMENTAL DETAILS\n\nB.1 PRE-TRAINING HYPERPARAMETERS AND TRAINING DETAILS\n\nThe implementation of models and all experiments are based on the fairseq (Ott et al., 2019) toolkit. The proposed model deploys transformer decoder architecture with 124M trainable parameters, in which nlayer = 12, nhead = 12, dembed = 768. We deploy Adam (Kingma & Ba, 2015) (β1 = 0.9, β2 = 0.98) optimizer and train all models with lr = 0.0005, twarmup = 4000, dropout = 0.1, bsz = 128, len = 512. The layer normalization over the retrieved image keys is initialized with ε of 0.00001. VALM reuses the identical lower-cased byte pair encoding (BPE) (Sennrich et al., 2016) representation with a 49152 vocab size of CLIP text encoder. The proposed VALM and re-implemented GPT-2∗ are trained for 500k steps using 16 Nvidia Tesla V100-SXM2-32GB GPUs. The encoded 200M image knowledge base takes up 274GiBs disk storage and the trained faiss approximate retrieval index takes another 14GiBs storage.\n\nB.2 PROBE TEMPLATES\n\nWe present all zero-shot query prompts and labels for 4 object commonsense reasoning datasets and 4 natural language understanding benchmarks in Tabele 11.\n\n16\n\nredwhiteorangegreenyellowpurpleblackbluebrownpinkgreycolor0.0000.0250.0500.0750.1000.1250.1500.1750.200probclassVaLMVaLM_ColorizationPublished as a conference paper at ICLR 2023\n\nTask\n\nPrompt\n\nLabels\n\nObject Color Reasoning\n\nObject Shape Reasoning\n\nObject Size Reasoning\n\nQ: What is the color of [DESCRIPTOR] [ITEM]? A: It is [Label] Q: What is the colour of [DESCRIPTOR] [ITEM] ? A: It is [Label] What is the color of [DESCRIPTOR] [ITEM]? It is [Label] What is the colour of [DESCRIPTOR] [ITEM]? [Label] The color of [DESCRIPTOR] [ITEM] is [Label] The usual color of [DESCRIPTOR] [ITEM] is [Label] [DESCRIPTOR] [ITEM] usually has the color of [Label] What is the usual color of [DESCRIPTOR] [ITEM]? [Label] What is the typical color of [DESCRIPTOR] [ITEM]? [Label]\n\nIs [ITEMA] larger than [ITEMB]? [Label] [ITEM] can be shape [Label] [ITEM] has shape [Label] [ITEM] is of shape [Label] The shape of [ITEM] can be [Label] The shape of the [ITEM] is [Label]\n\nIs [ITEMA] larger than [ITEMB]? [Label] Is [ITEMA] taller than [ITEMB]? [Label] Is [ITEMA] higher than [ITEMB]? [Label] [ITEMA] is larger than [ITEMB], is it true? [Label] [ITEMA] is taller than [ITEMB], is it true? [Label] [ITEMA] is larger than [ITEMB], is it true? [Label]\n\nSST-2 MPQA DBPedia AGNews\n\nReview: [Sentence] Sentiment: [Label] Review: [Sentence] Sentiment: [Label] input: [Sentence] type: [Label] input: [Sentence] type: [Label]\n\n{red, white, orange, green, blue, yellow, purple, black, pink, grey, brown}\n\n{cross, heart, octagon, oval, polygon, rectangle, rhombus, round, semicircle, square, star, triangle}\n\n{Yes, No}\n\n{Positive, Negative} {Positive, Negative} {company, school, ..., book} {world, ..., technology}\n\nTable 11: The prompts and prediction labels used to query the model predictions on the zero-shot evaluation of 4 object commonsense reasoning and 4 natural language understanding benchmarks. The labels for DBPedia are {company, school, artist, athlete, politics, transportation, building, nature, village, animal, plant, album, film, book} and the labels for AGNews are {world, sports, business, technology}.\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a pre-training framework, called VALM, to jointly train on image-text data. The novelty of this work, compared to previous works in similar field, is how the image-text pairs are created. While previous works use pre-curated image-text aligned pairs, this work instead uses images retrieved using text as a query and then jointly fuses them through attention layers. The claim is that this will help the model perform better on tasks requiring visual commonsense reasoning.\n\n# Strength And Weaknesses\n\n**Strengths**:\n\n-- The empirical results in the paper are quite strong and surpass pre-trained text models (GPT) as well as vision-language models (VisualBERT) on 4 reasoning task datasets and several language understanding/modeling tasks. While the differences in language-only tasks is small, the gains compared to baselines in visual reasoning tasks is large as claimed.\n\n-- The proposed model (esp visual knowledge fusion component) is novel and elegant and simple, which will serve as motivation for following works.\n\n-- The paper is clearly written and easy to follow.\n\n**Weaknesses**:\n\n-- The main weakness in the method is the use of frozen image retrieval component. The concern is that if this component is not end-to-end trained with the rest of the model, will the model quality be limited by the quality of image retrieval. While the reviewer acknowledges that empirical results show a large gap between CLIP and VALM on all tasks, it is worth wondering if the gains are due to embedding multiple images with text in VALM (compared to single image-text pair used originally in CLIP). An ablation with different values of k (number of images retrieved) will be helpful here.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is quite clear, quality of writing and results is high. The method is novel and authors mention they will be releasing the code on github later.\n\n# Summary Of The Review\n\nOverall, quite an interesting and novel idea in the space of vision-language models. The claims are verified by empirical results. VALM outperforming on tasks compared to other vision-language models is a strong result. The weakness related to understanding why this method works better than other vision-language models and whether frozen image retrieval will limit the method's effectiveness is something the reviewer would like to understand and engage with the authors further.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nUNIKGQA: UNIFIED RETRIEVAL AND REASONING FOR SOLVING MULTI-HOP QUESTION ANSWERING OVER KNOWLEDGE GRAPH\n\nJinhao Jiang1,3 *, Kun Zhou2,3 *, Wayne Xin Zhao1,3 (cid:66) and Ji-Rong Wen1,2,3 1Gaoling School of Artificial Intelligence, Renmin University of China. 2School of Information, Renmin University of China. 3Beijing Key Laboratory of Big Data Management and Analysis Methods. jiangjinhao@ruc.edu.cn, francis kun zhou@163.com, batmanfly@gmail.com, jrwen@ruc.edu.cn\n\nABSTRACT\n\nMulti-hop Question Answering over Knowledge Graph (KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately. Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model (PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. For parameter learning, we design a shared pre-training task based on questionrelation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies. Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our codes and data are publicly available at https://github.com/RUCAIBox/UniKGQA.\n\n1\n\nINTRODUCTION\n\nWith the availability of large-scale knowledge graphs (KGs), such as Freebase (Bollacker et al., 2008) and Wikidata (Tanon et al., 2016), knowledge graph question answering (KGQA) has become an important research topic that aims to find the answer entities of natural language questions from KGs. Recent studies (Lan et al., 2021) mainly focus on multi-hop KGQA, a more complex scenario where sophisticated multi-hop reasoning over edges (or relations) is required to infer the correct answer on the KG. We show an example in Figure 1(a). Given the question “Who is the wife of the nominee for The Jeff Probst Show”, the task goal is to find a reasoning path from the topic entity “The Jeff Probst Show” to the answer entities “Shelley Wright” and “Lisa Ann Russell”.\n\nFaced with the vast search space in large-scale KGs, previous work (Sun et al., 2018; 2019) typically adopts a retrieval-then-reasoning approach, to achieve a good trade-off. Generally, the retrieval stage aims to extract relevant triples from the large-scale KG to compose a relatively smaller question-relevant subgraph, while the reasoning stage focuses on accurately finding the answer entities from the retrieved subgraph. Although the purposes of the two stages are different, both stages\n\n* Equal contribution. (cid:66) Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustrative examples and learning procedure of our work.\n\nneed to evaluate the semantic relevance of a candidate entity with respect to the question (for removal or reranking), which can be considered as a semantic matching problem in essence. For measuring the entity relevance, relation-based features, either direct relations (Miller et al., 2016) or composite relation paths (Sun et al., 2018), have been shown to be particularly useful for building the semantic matching models. As shown in Figure 1(a), given the question, it is key to identify the semantically matched relations and the composed relation path in the KG (e.g., “nominee → spouse”) for finding the correct answer entities. Since the two stages cope with different scales of search space on KGs (e.g., millions v.s. thousands), they usually adopt specific technical solutions: the former prefers more efficient methods focusing on the recall performance (Sun et al., 2018), while the latter prefers more capable methods for modeling fined-grained matching signals (He et al., 2021).\n\nConsidering the same essence for both stages, this work aims to push forwards the research on multihop KGQA by investigating the following problem: can we design a unified model architecture for both stages to derive a better performance? To develop a unified model architecture for multi-hop KGQA, a major merit is that we can tightly relate the two stages and enhance the sharing of the relevance information. Although the two stages are highly related, previous studies usually treat them separately in model learning: only the retrieved triples are passed from the retrieval stage to the reasoning stage, while the rest of the useful signal for semantic matching has been neglected in the pipeline framework. Such an approach is likely to lead to sub-optimal or inferior performance, since multi-hop KGQA is a very challenging task, requiring elaborate solutions that sufficiently leverage various kinds of relevance information from the two stages.\n\nHowever, there are two major issues about developing a unified model architecture for multi-hop KGQA: (1) How to cope with very different scales of search space for the two stages? (2) How to effectively share or transfer useful relevance information across the two stages? For the first issue, instead of letting the same model architecture directly fit very different data distributions, we propose a new subgraph form to reduce the node scale at the retrieval stage, namely abstract subgraph that is composed by merging the nodes with the same relations from the KG (see Figure 1(b)). For the second issue, based on the same model architecture, we design an effective learning approach for the two stages, so that we can share the same pre-trained parameters and use the learned retrieval model to initialize the reasoning model (see Figure 1(c)).\n\nTo this end, in this paper, we propose UniKGQA, a unified model for multi-hop KGQA task. Specifically, UniKGQA consists of a semantic matching module based on a PLM for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. In order to learn these parameters, we design both pre-training (i.e., question-relation matching) and fine-tuning (i.e., retrieval- and reasoning-oriented learning) strategies based on the unified architecture. Compared with previous work on multi-hop KQGA, our approach is more unified and simplified, tightly relating the retrieval and reasoning stages.\n\nTo our knowledge, it is the first work that unifies the retrieval and reasoning in both model architecture and learning for the multi-hop KGQA task. To evaluate our approach, we conduct extensive experiments on three benchmark datasets. On the difficult datasets, WebQSP and CWQ, we outperform existing state-of-the-art baselines by a large margin (e.g., 8.1% improvement of Hits@1 on WebQSP and 2.0% improvement of Hits@1 on CWQ).\n\n2\n\nTheJeffProbstShownomineeJeff ProbstShelley WrightLisa Ann Russellis_aTalkshowCBS Television DistributiondistributedbyWhois the wifeofthe nomineeforTheJeffProbstShow?Lisa WhelchelnomineeSurvivorspousenomineeis_adistributedbyWhois the wifeofthe nomineeforTheJeffProbstShow?TheJeffProbstShowTalkshowCBS Television DistributionJeff ProbstLisa WhelchelLisa Ann RussellShelley WrightSurvivorΘpΘoΓpΓoRetrievalReasoningPre-trainingFine-tuningupdate by QRM(c)the overall learning procedure(b) an example of abstract subgraph(a) an example of multi-hop KGQA TV producerTVproducerupdate byRAS initialize Γowith Θoand update it byRRSinitialize Γpwith Θpand fix itPublished as a conference paper at ICLR 2023\n\n2 PRELIMINARY\n\nIn this section, we introduce the notations that will be used throughout the paper and then formally define the multi-hop KGQA task.\n\nKnowledge Graph (KG). A knowledge graph typically consists of a set of triples, denoted by G = {⟨e, r, e′⟩|e, e′ ∈ E, r ∈ R}, where E and R denote the entity set and relation set, respectively. A triple ⟨e, r, e′⟩ describes the fact that a relation r exists between head entity e and tail entity e′. Furthermore, we denote the set of neighborhood triples that an entity e belongs to by Ne = {⟨e, r, e′⟩ ∈ G} ∪ {⟨e′, r, e⟩ ∈ G}. Let r−1 denote the inverse relation of r, and we can represent a triple ⟨e, r, e′⟩ by its inverse triple ⟨e′, r−1, e⟩. In this way, we can simplify the definition of the neighborhood triples for an entity e as Ne = {⟨e′, r, e⟩ ∈ G}. We further use E ∈ Rd×|E| and R ∈ Rd×|R| to denote the embedding matrices for entities and relations in KG, respectively.\n\nMulti-hop Knowledge Graph Question Answering (Multi-hop KGQA). Given a natural language question q and a KG G, the task of KGQA aims to find answer entitie(s) to the question over the KG, denoted by the answer set Aq ∈ E. Following previous work (Sun et al., 2018; 2019), we assume that the entities mentioned in the question (e.g., “The Jeff Probst Show” in Figure 1(a)) are marked and linked with entities on KG, namely topic entities, denoted as Tq ⊂ E. In this work, we focus on solving the multi-hop KGQA task where the answer entities are multiple hops away from the topic entities over the KG. Considering the trade-off between efficiency and accuracy, we follow existing work (Sun et al., 2018; 2019) that solves this task using a retrieval-then-reasoning framework. In the two-stage framework, given a question q and topic entities Tq, the retrieval model aims to retrieve a small subgraph Gq from the large-scale input KG G, while the reasoning model searches answer entities Aq by reasoning over the retrieved subgraph Gq.\n\nAbstract Subgraph. Based on KGs, we further introduce the concept of abstract graph, which is derived based on the reduction from an original subgraph. Specifically, given a subgraph related to question q, denoted as Gq ⊂ G, we merge the tail entities from the triples with the same prefix (i.e., the same head entity and relation: ⟨e, r, ?⟩), and then generate a corresponding abstract node (cid:101)e to represent the set of tail entities, so we have ̃e = {e′|⟨e, r, e′⟩ ∈ G}. Similarly, we can also perform the same operations on the head entities. To unify the notations, we transform an original node that can’t be merged into an abstract node by creating a set only including the node itself. In this way, the corresponding abstract subgraph Gq can be denoted as: (cid:101)Gq = {⟨ ̃e, r, ̃e′⟩|∃e ∈ ̃e, ∃e′ ∈ ̃e′, ⟨e, r, e′⟩ ∈ Gq}, where each node ̃e is an abstract node representing a set of original nodes (one or multiple). We present illustrative examples of the original subgraph and its abstract subgraph in Figure 1(a) and Figure 1(b).\n\n3 APPROACH\n\nIn this section, we present our proposed UniKGQA, which unifies the retrieval and reasoning for multi-hop KGQA. The major novelty is that we introduce a unified model architecture for both stages (Section 3.1) and design an effective learning approach involving both specific pre-training and fine-tuning strategies (Section 3.2). Next, we describe the two parts in detail.\n\n3.1 UNIFIED MODEL ARCHITECTURE\n\nWe consider a general input form for both retrieval and reasoning, and develop the base architecture by integrating two major modules: (1) the semantic matching (SM) module that employs a PLM to perform the semantic matching between questions and relations; (2) the matching information propagation (MIP) module that propagates the semantic matching information on KGs. We present the overview of the model architecture in Figure 2. Next, we describe the three parts in detail.\n\nGeneral Input Formulation. In order to support both retrieval and reasoning stages, we consider a general form for evaluating entity relevance, where a question q and a subgraph Gq of candidate entities are given. For the retrieval stage, Gq is an abstract subgraph that incorporates abstract nodes to merge entities from the same relation. For the reasoning stage, Gq is constructed based on the\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: The illustration of updating entity representation e at step t by aggregating the semantic matching information from the set of directed relations pointing to e in the subgraph (i.e., {r1, r2, r3}) in our UniKGQA.\n\nretrieved subgraph from the retrieval stage, without abstract nodes. Such a general input formulation In what enables the development of the unified model architecture for the two different stages. follows, we will describe the approach in a general way, without considering specific stages.\n\nSemantic Matching (SM). The SM module aims to produce the semantic matching features between the question q and a triple ⟨e′, r, e⟩ from the given subgraph Gq. Considering the excellent modeling capacity of the PLM, we leverage the PLM to produce text encoding as the representations of question q and relation r. Specifically, we first utilize the PLM to encode the texts of q and r, and employ the output representation of the [CLS] token as their representations:\n\nhq = PLM(q), hr = PLM(r).\n\n(1)\n\nBased on hq and hr, inspired by the NSM model (He et al., 2021), we obtain the vector capturing semantic matching features m(t) ⟨e′,r,e⟩ between question q and triple ⟨e′, r, e⟩ at the t-th step by adopting corresponding projection layers:\n\nm(t)\n\n⟨e′,r,e⟩ = σ\n\n(cid:16)\n\nhqW (t)\n\nQ ⊙ hrW (t)\n\nR\n\n(cid:17)\n\n,\n\n(2)\n\n⟨e′,r,e⟩ ∈ Rd, W (t)\n\nwhere m(t) R ∈ Rh×d are parameters of the t-step projection layers, h, d are the hidden dimensions of PLM and the feature vector, respectively, σ is the sigmoid activation function, and ⊙ is the hadamard product.\n\nQ , W (t)\n\nMatching Information Propagation (MIP). Based on the generated semantic matching features, the MIP module first aggregates them to update the entity representation and then utilizes it to obtain the entity match score. To initialize the match score, given a question q and a subgraph Gq, for each entity ei ∈ Gq, we set the match score between q and ei as follows: s(1) ei = 1 if ei is a topic entity and s(1) ei = 0 otherwise. At the t-th step, we utilize the match scores of the head entities computed at the last step s(t−1) as the weights and aggregate the matching features from neighboring triples to obtain the representation of the tail entity:\n\ne′\n\ne(t) = W (t)\n\nE\n\n\n\n\n\n e(t−1);\n\n(cid:88)\n\ns(t−1)\n\ne′\n\n· m(t)\n\n⟨e′,r,e⟩\n\n\n\n ,\n\n\n\n\n\n⟨e′,r,e⟩∈Ne\n\n(3)\n\nwhere e(t) ∈ Rd is the representation of the entity e in the t-th step, and the W (t) E ∈ R2d×d is a learnable matrix. At the first step, since there are no matching scores, following the NSM (He et al., 2021) model, we directly aggregate the representations of its one-hop relations as the entity representation: e(1) = σ((cid:80) r · U ), where the U ∈ R2d×d is a learnable matrix. Based on the representations of all entities E(t) ∈ Rd×n, we update their entity match scores using the softmax function as:\n\n⟨e′,r,e⟩∈Ne\n\ns(t) = softmax(cid:0)E(t)⊤\n\nv(cid:1),\n\n(4)\n\n4\n\nPLMQuestion:Relation:LinearLinearhqme1,r1,e(t−1)me3,r3,e(t−1)Nese1t−1se3t−1෤etScore updateetstE(t−1)S(t−1)E(t)S(t)Stept−1SteptLinearEmbedding updateScore updateSemantic MatchingMatching Information Propagationqee1e2e3e4e5r1r2r3r5r4h1h3: hadamard product operation: sum operatione6e7r6r7r1r3r2h2me2,r2,e(t−1)Published as a conference paper at ICLR 2023\n\nwhere v ∈ Rd is a learnable vector.\n\nAfter T -step iterations, we can obtain the final entity match scores s(T ), which is a probability distribution over all entities in the subgraph Gq. These match scores can be leveraged to measure the possibilities of the entities being the answers to the given question q, and will be used in both the retrieval and reasoning stages.\n\n3.2 MODEL TRAINING\n\nIn our approach, we have both the retrieval model and the reasoning model for the two stages of multi-hop KGQA. Since the two models adopt the same architecture, we introduce Θ and Γ to denote the model parameters that are used for retrieval and reasoning stages, respectively. As shown in Section 3.1, our architecture contains two groups of parameters, namely the underlying PLM and the other parameters for matching and propagation. Thus, Θ and Γ can be decomposed as: Θ = {Θp, Θo} and Γ = {Γp, Γo}, where the subscripts p and o denote the PLM parameters and the other parameters in our architecture, respectively. In order to learn these parameters, we design both pre-training (i.e., question-relation matching) and fine-tuning (i.e., retrieval- and reasoning-oriented learning) strategies based on the unified architecture. Next, we describe the model training approach.\n\nPre-training with Question-Relation Matching (QRM). For pre-training, we mainly focus on learning the parameters of the underlying PLMs (i.e., Θp and Γp). In the implementation, we let the two models share the same copy of PLM parameters, i.e., Θp = Γp. As shown in Section 3.1, the basic capacity of the semantic matching module is to model the relevance between a question and a single relation (Eq. 2), which is based on the text encoding from the underlying PLM. Therefore, we design a contrastive pre-training task based on question-relation matching. Specifically, we adopt the contrastive leaning objective (Hadsell et al., 2006) to align the representations of relevant question-relation pairs while pushing apart others. To collect the relevant question-relation pairs, given an example consisting of a question q, the topic entities Tq and answer entities Aq, we extract all the shortest paths between the Tq and Aq from the entire KG and regard all of the relations within these paths as relevant to q, denoted as R+. In this way, we can obtain a number of weaksupervised examples. During pre-training, for each question q, we randomly sample a relevant relation r+ ∈ R+, and utilize a contrastive learning loss for pre-training:\n\nLP T = − log\n\n(cid:16)\n\n(cid:80)M\n\nj=1\n\nesim(qi,r+\n\ni )/τ\n\nesim(qi,r+\n\nj )/τ + esim(qi,r−\n\nj )/τ (cid:17)\n\n(5)\n\nwhere τ is a temperature hyperparameter, r− is a randomly sampled negative relation, and sim (q, r) is the cosine similarity, and q, r is the question and relation encoded by the PLM from the SM module (Eq. 1). In this way, the question-relation matching capacity will be enhanced by pretraining the PLM parameters. Note that the PLM parameters will be fixed after pre-training.\n\ni\n\nFine-tuning for Retrieval on Abstract Subgraphs (RAS). After pre-training, we first fine-tune the entire model for learning the parameters Θo according to the retrieval task. Recall that we transform the subgraphs into a form of abstract subgraphs, where abstract nodes are incorporated for merging entities from the same relation. Since our MIP module (Section 3.1) can produce the matching scores sA of nodes in a subgraph (Eq. 4), where the subscript A denotes that the nodes are from an abstract subgraph. Furthermore, we utilize the labeled answers to get the ground-truth vectors, denoted by s∗ A to 1 if it contains the answer entity. Then we minimize the KL divergence between the learned and ground-truth matching score vectors as:\n\nA. We set an abstract node in s∗\n\nLRAS = DKL\n\n(cid:0)sA, s∗\n\nA\n\n(cid:1).\n\n(6)\n\nAfter fine-tuning the RAS loss, the retrieval model can be effectively learned. We further utilize it to retrieve the subgraph for the given question q, by selecting the top-K ranked nodes according to their match scores. Note that only the node within a reasonable distance to the topic entities will be selected into the subgraph, which ensures a relatively small yet relevant subgraph Gq for the subsequent reasoning stage to find answer entities.\n\nFine-tuning for Reasoning on Retrieved Subgraphs (RRS). After fine-tuning the retrieval model, we continue to fine-tune the reasoning model by learning the parameters Γo. With the fine-tuned\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nMethods\n\nRetrieval\n\nReasoning\n\nTable 1: Comparison of different methods. Parameters Transferring ✗\n✗ ✗\n✗ ✓\n\nGraftNet PPR PullNet LSTM NSM PPR PLM SR+NSM UniKGQA UniKGQA UniKGQA\n\nGraftNet GraftNet NSM NSM\n\nTable 2: Statistics of all datasets.\n\nDatasets MetaQA-1hop MetaQA-2hop MetaQA-3hop WebQSP CWQ\n\n#Train 96,106 118,980 114,196 2,848 27,639\n\n#Valid 9,992 14,872 14,274 250 3,519\n\n#Test Max #hop 9,947 14,872 14,274 1,639 3,531\n\n1 2\n3 2\n4\n\nretrieval model, we can obtain a smaller subgraph Gq for each question q. In the reasoning stage, we focus on performing accurate reasoning to find the answer entities, so that we recover the original nodes in the abstract nodes and the original relations among them. Since the retrieval and reasoning stages are highly dependent, we first initialize the parameters of the reasoning model with those from the retrieval model: Θo → Γo. Then, following Eq. 4, we employ a similar approach to fitting the learned matching scores (denoted by sR) with the ground-truth vectors (denoted by s∗ R) according to the KL loss:\n\nLRRS = DKL\n\n(cid:0)sR, s∗\n\nR\n\n(cid:1),\n\n(7)\n\nwhere the subscript R denotes that the nodes come from a retrieved subgraph. After fine-tuning with the RRS loss, we can utilize the learned reasoning model to select the top-n ranked entities as the answer list according to the match scores.\n\nAs shown in Figure 1(c), the overall training procedure is composed by: (1) pre-training Θp with question-relation matching, (2) fixing Θp and fine-tuning Θo for retrieval on abstract subgraphs, and (3) fixing the Γp initialized by Θp and fine-tuning Γo initialized by Θo for reasoning on subgraphs.\n\nOur work provides a novel unified model for the retrieval and reasoning stages to share the reasoning capacity. In Table 1, we summarize the differences between our method and several popular methods for multi-hop KGQA, including GraphfNet (Sun et al., 2018), PullNet (Sun et al., 2019), NSM (He et al., 2021), and SR+NSM (Zhang et al., 2022). As we can see, existing methods usually adopt different models for the retrieval and reasoning stages, while our approach is more unified. As a major benefit, the information between the two stages can be effectively shared and reused: we initialize the reasoning model with the learned retrieval model.\n\n4 EXPERIMENT\n\n4.1 EXPERIMENTAL SETTING\n\nDatasets. Following existing work on multi-hop KGQA (Sun et al., 2018; 2019; He et al., 2021; Zhang et al., 2022), we adopt three benchmark datasets, namely MetaQA (Zhang et al., 2018), WebQuestionsSP (WebQSP) (Zhang et al., 2018; Yih et al., 2015), and Complex WebQuestions 1.1 (CWQ) (Talmor & Berant, 2018) for evaluating our model. Table 2 shows the statistics of the three datasets. Since previous work has achieved nearly full marks on MetaQA, WebQSP and CWQ are our primarily evaluated datasets. We present a detailed description of these datasets in Appendix A.\n\nEvaluation Protocol. For the retrieval performance, we follow Zhang et al. (2022) that evaluate the models by the answer coverage rate (%). It is the proportion of questions whose retrieved subgraphs contain at least one answer. For the reasoning performance, we follow Sun et al. (2018; 2019) that regard the reasoning as a ranking task for evaluation. Given each test question, we rely on the predictive probabilities from the evaluated model to rank all candidate entities and then evaluate whether the top-1 answer is correct with Hits@1. Since a question may correspond to multiple answers, we also adopt the widely-used F1 metric.\n\nBaselines. We consider the following baselines for performance comparison: (1) reasoning-focused methods: KV-Mem (Miller et al., 2016), GraftNet (Sun et al., 2018), EmbedKGQA (Saxena et al., 2020), NSM (He et al., 2021), TransferNet (Shi et al., 2021); (2) retrieval-augmented methods: PullNet (Sun et al., 2019), SR+NSM (Zhang et al., 2022), SR+NSM+E2E (Zhang et al., 2022). We present a detailed description of these baselines in Appendix B.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Performance comparison of different methods for KGQA (Hits@1 and F1 in percent). We copy the results for TransferNet from Shi et al. (2021) and others from Zhang et al. (2022). Bold and underline fonts denote the best and the second-best methods, respectively.\n\nModels\n\nKV-Mem GraftNet PullNet EmbedKGQA NSM TransferNet SR+NSM SR+NSM+E2E UniKGQA w QU w QU,RU\n\nWebQSP\n\nCWQ\n\nHits@1 46.7 66.4 68.1 66.6 68.7 71.4 68.9 69.5 75.1 77.0 77.2\n\nF1 34.5 60.4 -\n- 62.8 -\n64.1 64.1 70.2 71.0 72.2\n\nHits@1 18.4 36.8 45.9 -\n47.6 48.6 50.2 49.3 50.7 50.9 51.2\n\nF1 15.7 32.7 -\n- 42.4 -\n47.1 46.3 48.0 49.4 49.0\n\nMetaQA-1 MetaQA-2 MetaQA-3 Hits@1 82.7 94.8 99.9 98.8 99.9 100 -\n- 99.0 99.9 99.9\n\nHits@1 48.9 77.7 91.4 94.8 98.9 100 -\n- 99.1 99.5 99.9\n\nHits@1 96.2 97.0 97.0 97.5 97.1 97.5 -\n- 97.5 97.6 98.0\n\n4.2 EVALUATION RESULTS\n\nTable 3 shows the results of different methods on 5 multi-hop KGQA datasets. It can be seen that:\n\nFirst, most baselines perform very well on the three MetaQA datasets (100% Hits@1). It is because these datasets are based on a few hand-crafted question templates and have only nine relation types for the given KG. Thus, the model can easily capture the relevant semantics between the questions and relations to perform reasoning. To further examine this, we conduct an extra one-shot experiment on MetaQA datasets and present the details in Appendix E. Second, TransferNet performs better than GraftNet, EmbedKGQA, and NSM with the same retrieval method. It attends to question words to compute the scores of relations and transfers entity scores along with the relations. Such a way can effectively capture the question-path matching semantics. Besides, SR+NSM and SR+NSM+E2E outperform NSM and PullNet in a large margin. The reason is that they both leverage a PLM-based relation paths retriever to improve the retrieval performance and then reduce the difficulty of the later reasoning stage.\n\nFinally, on WebQSP and CWQ, our proposed UniKGQA is substantially better than all other competitive baselines. Unlike other baselines that rely on independent models to perform retrieval and reasoning, our approach can utilize a unified architecture to accomplish them. Such a unified architecture can pre-learn the essential capability of question-relation semantic matching for both stages, and is also capable of effectively transferring relevance information from the retrieval stage to the reasoning stage, i.e., initializing the reasoning model with the parameters of the retrieval model.\n\nIn our approach, we fix the parameters of the PLM-based encoder for efficiency. Actually, updating its parameters can further improve our performance. Such a way enables researchers to trade off the efficiency and effectiveness when employing our approach in real-world applications. Here, we study it by proposing two variants of our UniKGQA: (1) w QU that updates the parameters of the PLM encoder only when encoding questions, (2) w QU, RU that updates the parameters of the PLM encoder both when encoding questions and relations. Indeed, both variants can boost the performance of our UniKGQA. And only updating the PLM encoder when encoding questions can obtain a comparable even better performance to update both. A possible reason is that updating the PLM encoder owns when encoding questions and relations may lead to overfitting on the downstream tasks. Therefore, it is promising for our UniKGQA to just update the PLM encoder when encoding questions, as it can achieve better performance with relative less additional computation cost.\n\n4.3 FURTHER ANALYSIS\n\nRetrieval Evaluation. We evaluate the effectiveness of our UniKGQA to retrieve a smaller but better answer coverage rate subgraph for a given question. Following the evaluation principles of SR (Zhang et al., 2022), we measure such a capacity from three aspects: the direct subgraph size, answer coverage rate, and the final QA performance. Concretely, we first compare UniKGQA with SR (Zhang et al., 2022) and PPR-based heuristic retrieval method (Sun et al., 2018) based on the answer coverage rate curve w.r.t. the number of graph nodes. Then, we compare UniKGQA with\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: The evaluation of retrieval and fine-tuning efficiency: the answer coverage rate under various subgraph sizes (Left), the Hits@1 scores under various answer coverage rates (Middle), and the Hits@1 scores at different epochs on WebQSP (Right).\n\nSR+NSM (Zhang et al., 2022) and PPR+NSM (He et al., 2021) based on their final QA performance. To further study the effectiveness of our approach, we add an extra variant of our UniKGQA, namely UniKGQA+NSM, which relies on UniKGQA for retrieval while NSM for performing reasoning. The left and middle of Figure 3 show the comparison results of the above methods. As we can see, under the same size of retrieved subgraphs, UniKGQA and SR have significantly larger answer coverage rates than PPR. It demonstrates the effectiveness and necessity of training a learnable retrieval model. Besides, although the curves of UniKGQA and SR are very similar, our UniKGQA can achieve a better final reasoning performance than SR+NSM. The reason is that UniKGQA can transfer the relevance information from the retrieval stage to the reasoning stage based on the unified architecture, learning a more effective reasoning model. Such a finding can be further verified by comparing our UniKGQA with UniKGQA+NSM.\n\nTable 4: Ablation study of our training strategies. WebQSP\n\nCWQ\n\nModels\n\nUniKGQA w QU w/o Pre w/o Trans w/o Pre, Trans\n\nHits@1 77.0 75.4 75.8 72.5\n\nF1 71.0 70.6 70.6 60.0\n\nHits@1 50.9 49.2 49.8 48.1\n\nF1 49.4 48.8 49.3 48.4\n\nAblation Study. Our UniKGQA contains two important training strategies to improve performance: (1) pre-training with question-relation matching, (2) initializing the parameters of the reasoning model with the retrieval model. Here, we conduct the ablation study to verify their effectiveness. We propose three variants as: (1) w/o Pre removing the pre-training procedure, (2) w/o Trans removing the initialization with the parameters of retrieval model, (3) w/o Pre, Trans removing both the pretraining and initialization procedures. We show the results of the ablation study in Table 4. We can see that all these variants underperform the complete UniKGQA, which indicates that the two training strategies are both important for the final performance. Besides, such an observation also verifies that our UniKGQA is indeed capable of transferring and reusing the learned knowledge to improve the final performance.\n\nFine-tuning Efficiency. As our UniKGQA model can transfer the learned knowledge from the pretraining stage and the retrieval task, it can be easily adapted into downstream reasoning tasks. In this way, we can perform a more efficient fine-tuning on the reasoning task with a few fine-tuning steps. To explore it, we compare the performance changes of our UniKGQA with a strong baseline NSM w.r.t. the increasing of fine-tuning epochs based on the same retrieved subgraphs. The results are presented on the right of Figure 3. First, we can see that before fine-tuning (i.e., when the epoch is zero), our UniKGQA has achieved a comparable performance as the best results of NSM at the last epoch. It indicates that the reasoning model has successfully leveraged the knowledge from prior tasks based on the parameters initialized by the retrieval model. After fine-tuning with two epochs, our UniKGQA has already achieved a good performance. It verifies that our model can be fine-tuned in an efficient way with very few epochs. To further investigate our UniKGQA model, we conduct parameter sensitivity analysis w.r.t. pre-training steps, hidden dimensions, and the number of retrieved nodes K, shown in Appendix H.\n\n8\n\n0250500750100012501500Subgraph size7075808590Answer Coverage Rate (%)SRPPRUniKGQA7075808590Answer coverage rate (%)50607080Hits@1 (%)SR+NSMPPR+NSMUniKGQA+NSMUniKGQA+UniKGQA0246810Epoch253545556575Hits@1 (%)UniKGQAUniKGQA w/o PreNSMPublished as a conference paper at ICLR 2023\n\n5 RELATED WORK\n\nMulti-hop Knowledge Graph Question Answering. Multi-hop KGQA aims to seek answer entities that are multiple hops away from the topic entities in a large-scale KG. Considering the efficiency and accuracy, existing work (Sun et al., 2018; 2019; Zhang et al., 2022) typically first retrieves a question-relevant subgraph to reduce the search space and then performs multi-hop reasoning on it. Such a retrieval-and-reasoning paradigm has shown superiority over directly reasoning on the entire KG (Chen et al., 2019; Saxena et al., 2020).\n\nThe retrieval stage focuses on extracting a relatively small subgraph involving the answer entities. A commonly-used approach is to collect entities with nearer hops around the topic entities to compose the subgraph and filter the ones with low Personalized PageRank scores to reduce the graph size (Sun et al., 2018; He et al., 2021). Despite the simplicity, such a way neglects the question semantics, limiting the retrieval efficiency and accuracy. To address it, several work (Sun et al., 2019; Zhang et al., 2022) devises retrievers based on semantic matching using neural networks (e.g., LSTMs or PLMs). Starting from the topic entities, these retrievers iteratively measure the semantic relevance between the question and neighbouring entities or relations, and add proper ones into the subgraph. In this way, a smaller but more question-relevant subgraph would be constructed.\n\nThe reasoning stage aims to accurately find the answer entities of the given question by walking along the relations starting from the topic entities. Early work (Miller et al., 2016; Sun et al., 2018; 2019; Jiang et al., 2022) relies on the special network architectures (e.g., Key-Value Memory Network or Graph Convolution Network) to model the multi-hop reasoning process. Recent work further enhances the reasoning capacity of the above networks from the perspectives of intermediate supervision signals (He et al., 2021), knowledge transferring (Shi et al., 2021), etc. However, all these methods design different model architectures and training methods for the retrieval and reasoning stages, respectively, neglecting the similarity and intrinsic connection of the two stages.\n\nRecently, some work parses the question into structured query language (e.g., SPARQL) (Lan et al., 2021; Das et al., 2021; Huang et al., 2021) and executes it by a query engine to get answers. In this way, the encoder-decoder architecture (i.e., T5 (Raffel et al., 2020)) is generally adopted to produce the structured queries, where the annotated structured queries are also required for training.\n\nDense Retrieval. Given a query, the dense retrieval task aims to select relevant documents from a large-scale document pool. Different from the traditional sparse term-based retrieval methods, e.g., TF-IDF (Chen et al., 2017) and BM25 (Robertson & Zaragoza, 2009), dense retrieval methods (Karpukhin et al., 2020; Zhou et al., 2022a;b) rely on a bi-encoder architecture to map queries and documents into low-dimensional dense vectors. Then their relevance scores can be measured using vector distance metrics (e.g., cosine similarity), which supports efficient approximate nearest neighbour (ANN) search algorithms. In multi-hop KGQA, starting from the topic entities, we need to select the relevant neighboring triples from a large-scale KG, to induce a path to reach the answer entities, which can be seen as a constrained dense retrieval task. Therefore, in this work, we also incorporate a bi-encoder architecture to map questions and relations into dense vectors, and then perform retrieval or reasoning based on their vector distances.\n\n6 CONCLUSION\n\nIn this work, we proposed a novel approach for the multi-hop KGQA task. As the major technical contribution, UniKGQA introduced a unified model architecture based on PLMs for both retrieval and reasoning stages, consisting of the semantic matching module and the matching information propagation module. To cope with the different scales of search space in the two stages, we proposed to generate abstract subgraphs for the retrieval stage, which can significantly reduce the number of nodes to be searched. Furthermore, we designed an effective model learning method with both pre-training (i.e., question-relation matching) and fine-tuning (i.e., retrieval- and reasoning-oriented learning) strategies based on the unified architecture. With the unified architecture, the proposed learning method can effectively enhance the sharing and transferring of relevance information between the two stages. We conducted extensive experiments on three benchmark datasets, and the experimental results show that our proposed unified model outperforms the competitive methods, especially on more challenging datasets (i.e., WebQSP and CWQ).\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098. And this work is also partially supported by the Outstanding Innovative Talents Cultivation Funded Programs 2022 of Renmin University of China. Xin Zhao is the corresponding author.\n\nREFERENCES\n\nKurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008, pp. 1247–1250. ACM, 2008.\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pp. 1870–1879. Association for Computational Linguistics, 2017.\n\nYu Chen, Lingfei Wu, and Mohammed J. Zaki. Bidirectional attentive memory networks for question answering over knowledge bases. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 2913–2923. Association for Computational Linguistics, 2019.\n\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. Case-based reasoning for natural language queries over knowledge bases. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 9594–9611. Association for Computational Linguistics, 2021.\n\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), 2:1735–1742, 2006.\n\nGaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. Improving multi-hop knowledge base question answering by learning intermediate supervision signals. In WSDM ’21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021, pp. 553–561. ACM, 2021.\n\nXin Huang, Jung-Jae Kim, and Bowei Zou. Unseen entity handling in complex question answering over knowledge base via language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 547–557. Association for Computational Linguistics, 2021.\n\nDrew A. Hudson and Christopher D. Manning. Learning by abstraction: The neural state machine. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5901–5914, 2019.\n\nJinhao Jiang, Kun Zhou, Ji-Rong Wen, and Xin Zhao. $great truths are always simple: $ A rather simple knowledge encoder for enhancing the commonsense reasoning capacity of pre-trained In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, models. WA, United States, July 10-15, 2022, pp. 1730–1741. Association for Computational Linguistics, 2022.\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 6769–6781. Association for Computational Linguistics, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nYunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. A survey on complex knowledge base question answering: Methods, challenges and solutions. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pp. 4483–4491. ijcai.org, 2021.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.\n\nAlexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 1400–1409. The Association for Computational Linguistics, 2016.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.\n\nStephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and\n\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, 2009.\n\nApoorv Saxena, Aditay Tripathi, and Partha P. Talukdar. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 4498–4507. Association for Computational Linguistics, 2020.\n\nJiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, and Hanwang Zhang. Transfernet: An effective and transparent framework for multi-hop question answering over relation graph. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 4149–4158. Association for Computational Linguistics, 2021.\n\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W. Cohen. Open domain question answering using early fusion of knowledge bases In Proceedings of the 2018 Conference on Empirical Methods in Natural Language and text. Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 4231–4242. Association for Computational Linguistics, 2018.\n\nHaitian Sun, Tania Bedrax-Weiss, and William W. Cohen. Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 2380–2390. Association for Computational Linguistics, 2019.\n\nAlon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 641–651. Association for Computational Linguistics, 2018.\n\nThomas Pellissier Tanon, Denny Vrandecic, Sebastian Schaffert, Thomas Steiner, and Lydia In Proceedings of the 25th InterPintscher. From freebase to wikidata: The great migration. national Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016, pp. 1419–1428. ACM, 2016.\n\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pp. 1321–1331. The Association for Computer Linguistics, 2015.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. Subgraph retrieval enhanced model for multi-hop knowledge base question answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 5773–5784. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.396. URL https://doi.org/10.18653/v1/ 2022.acl-long.396.\n\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. Variational reasoning for question answering with knowledge graph. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 6069–6076. AAAI Press, 2018.\n\nKun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong Wen, Nan Duan, et al. Simans: Simple ambiguous negatives sampling for dense text retrieval. In EMNLP, 2022a.\n\nKun Zhou, Xiao Liu, Yeyun Gong, Wayne Xin Zhao, Daxin Jiang, Nan Duan, and Ji-Rong Wen. Master: Multi-task pre-trained bottlenecked masked autoencoders are better dense retrievers. 2022b.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: Analysis of MetaQA datasets.\n\nDataset\n\n# of templates\n\nMetaQA-1 MetaQA-2 MetaQA-3\n\n161 210 150\n\naverage # of training cases per template 597 567 761\n\n# of used relations for question construction 9\n9 9\n\nA DATASETS\n\nWe adopt three widely-used multi-hop KGQA datasets in this work:\n\n• MetaQA (Zhang et al., 2018) contains more than 400k questions in the domain of movie and the answer entities are up to 3 hops away from the topic entities. According to the number of hops, this dataset is split into three sub-datasets, i.e., MetaQA-1hop, MetaQA-2hop, and MetaQA-3hop.\n\n• WebQuestionsSP (WebQSP) (Yih et al., 2015) contains 4,737 questions and the answer entities require up to 2-hop reasoning on the KG Freebase (Bollacker et al., 2008). We use the same train/valid/test splits as GraftNet (Sun et al., 2018).\n\n• Complex WebQuestions 1.1 (CWQ) (Talmor & Berant, 2018) is constructed based on WebQSP by extending the question entities or adding constraints to answers. These questions require up to 4-hop reasoning on the KG Freebase (Bollacker et al., 2008).\n\nExisting work has demonstrated that the training data for MetaQA is more than sufficient (Shi et al., 2021; He et al., 2021), hence all the comparison methods in our experiments can achieve very high performance. We conduct further analysis of the three MetaQA datasets about the number of templates, the average number of training cases per template, and the number of relations used for constructing questions, and show them in Table 5. In summary, more training cases and simpler questions make the MetaQA easier to be solved.\n\nB BASELINES\n\nWe consider the following baseline methods for performance comparison:\n\n• KV-Mem (Miller et al., 2016) maintains a key-value memory table to store KG facts, and conducts multi-hop reasoning by performing iterative read operations on the memory.\n\n• GraftNet (Sun et al., 2018) first retrieves the question-relevant subgraph and text sentences from the KG and Wikipedia respectively with a heuristic method. Then it adopts a graph neural network to perform multi-hop reasoning on a heterogeneous graph built upon the subgraph and text sentences.\n\n• PullNet (Sun et al., 2019) trains a graph retrieval model composed of a LSTM and a graph neural network instead of the heuristic way in GraftNet for the retrieval task, and then conducts multi-hop reasoning with GraftNet.\n\n• EmbedKGQA (Saxena et al., 2020) reformulates the multi-hop reasoning of GraftNet as a link prediction task by matching pre-trained entity embeddings with question representations from a PLM.\n\n• NSM (He et al., 2021) first conducts retrieval following GraftNet and then adapt the neural state machine (Hudson & Manning, 2019) used in visual reasoning for multi-hop reasoning on the KG.\n\n• TransferNet (Shi et al., 2021) first conducts retrieval following GraftNet and then performs the multi-hop reasoning on a KG or a text-formed relation graph in a transparent framework. The reasoning model consists of a PLM for question encoding and a graph neural network for updating the relevance scores between entities and the question.\n\n• SR+NSM (Zhang et al., 2022) first learns a PLM-based relation path retriever to conduct effectively retrieval and then leverages NSM reasoner to perform multi-hop reasoning.\n\n• SR+NSM+E2E (Zhang et al., 2022) further fine-tunes the SR+NSM by an end-to-end way.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: The results of ablation study on WebQSP. The performance on WebQSP of varying pretraining steps (Left), hidden dimensions (Middle), and the number of retrieved nodes K (Right).\n\nC KNOWLEDGE GRAPH PREPROCESSING DETAILS\n\nWe preprocess the full Freebase following existing work (Sun et al., 2018; He et al., 2021). For MetaQA, we directly use the subset of WikiMovies provided by the datasets, and the size is about 134,741. For WebQSP and CWQ datasets, we set the max hops of retrieval and reasoning as two and four, respectively. Based on the topic entities labeled in original datasets, we reserve the neighborhood subgraph consisting of entities within the four hops of the topic entities for each sample. After such simple preprocessing, the size of KG we used is 147,748,092 for WebQSP and 202,358,414 for CWQ. Based on the preprocessed KG, we conduct the retrieval and reasoning using our proposed approach.\n\nD IMPLEMENTATION DETAILS.\n\nDuring pre-training, we collect question-relation pairs based on the shortest relation paths between topic entities and answer entities, and then use these pairs to pre-train the RoBERTa-base (Liu et al., 2019) model with the contrastive learning objective. We set the temperature τ as 0.05, and select the best model by evaluating Hits@1 on the validation set. For retrieval and reasoning, we initialize the PLM module of our UniKGQA model with the contrastive learning pre-trained RoBERTa, and set the hidden size of other linear layers as 768. We optimize parameters with the AdamW optimizer, where the learning rate is 0.00001 for the PLM module, and 0.0005 for other parameters. The batch size is set to 40. The reasoning step is set to 4 for CWQ dataset, 3 for WebQSP and MetaQA-3 datasets, 2 for MetaQA-2 dataset, and 1 for MetaQA-1 dataset. We preprocess the KGs for each datasets following existing work (Sun et al., 2018; He et al., 2021).\n\nE ONE-SHOT EXPERIMENT FOR METAQA\n\nTable 6: One-shot experiment results on MetaQA (Hits@1 in percent).\n\nModel NSM TransferNet UniKGQA\n\nMetaQA-1 MetaQA-2 MetaQA-3 97.0 97.5 98.2\n\n94.8 96.5 97.1\n\n91.0 90.1 92.6\n\nSince the samples in MetaQA are more than sufficient, all the comparison methods in our experiments have achieved very high performance. For example, our method and previous work (e.g., TransferNet and NSM) have achieved more than 98% Hits@1 on MetaQA, which shows that this dataset’s performance may have been saturated. To examine this assumption, we consider conducting few-shot experiments to verify the performance of different methods. Specially, we follow the NSM paper (He et al., 2021) that conducts the one-shot experiment. We randomly sample just one training case for each question template from the original training set, to form a one-shot training dataset. In this way, the numbers of training samples for MetaQA-1, MetaQA-2, and MetaQA-3 are 161, 210, and 150, respectively. We evaluate the performance of our approach and some strong\n\n14\n\n04001600280040005200Training steps70727476Hits@1 (%)UniKGQATransferNet641282565127681024Hidden dimensions70727476Hits@1 (%)UniKGQATransferNet15101520Top-k707580859095Answer Coverage Rate (%)UniKGQASRPublished as a conference paper at ICLR 2023\n\nbaselines (i.e., TrasnferNet and NSM) trained with this new training dataset. As shown in Table 6, our method can consistently outperform these baselines in all three subsets.\n\nF ABLATION STUDY OF OUR UNIFIED MODEL ARCHITECTURE\n\nTable 7: Ablation study by combining our UniKGQA with other models.\n\nModels PPR+NSM SR+NSM SR+UniKGQA UniKGQA+NSM UniKGQA+UniKGQA\n\nWebQSP (Hits@1) 68.7 68.9 70.5 69.1 75.1\n\nCWQ (Hits@1) 47.6 50.2 48.0 49.2 50.7\n\nThe unified model architecture is the key of our approach. Once the unified model architecture is removed, it would be hard to share the question-relation matching capability enhanced by pre-training in retrieval and reasoning stages, and also hard to transfer the relevance information for multi-hop KGQA learned in the retrieval stage to the reasoning stage. To verify it, we conduct an extra ablation study to explore the effect of only adopting the unified model architecture as the reasoning model or the retrieval model. We select the existing strong retrieval model (i.e., SR) and reasoning model (i.e., NSM), and compare the performance when integrated with our UniKGQA. As we can see in Table 7, all the variants underperform our UniKGQA. It indicates that the unified model used in the retrieval and reasoning stages simultaneously is indeed the key reason for improvement.\n\nG ANALYSIS OF THE PRE-TRAINING STRATEGY\n\nTable 8: Results of variants with or without pre-training strategy (Pre) and updating the PLM (QU). WebQSP (Hits@1) 75.1 77.0 75.4 67.3\n\nCWQ (Hits@1) 50.7 50.9 49.2 48.1\n\nModels UniKGQA w QU w/o Pre, w QU w/o Pre\n\nWe conduct the analysis experiments to investigate how the pre-training strategy (Pre) affects the performance with or without updating the PLM (QU). We show the results in Table 8. Once removing the pre-training strategy, the model performance would drop 10.4% (2.1%) in WebQSP and 5.1% (3.3%) in CWQ when fixing (not fixing) the PLM. It indicates that the pre-training strategy is an important component of our approach. After pre-training, the PLM can be fixed for more efficient parameters optimization during fine-tuning.\n\nH PARAMETER SENSITIVITY ANALYSIS\n\nPre-training Steps Although the pre-training strategy has shown effective in our approach, too many pre-training steps will be time-consuming and costly. Here, we investigate the performance with respect to varying pre-training steps. As shown in the left of Figure 4, we can see that our method can reach the best performance with only few pre-training steps (i.e., 2800) compared with the best baseline TransferNet. It shows that our approach does not require too many steps for pretraining. Instead, we can see that too many pre-training steps will hurt the model performance. The reason may be that the PLM has overfit into the contrastive learning objective.\n\nParameter Tuning. In our approach, we have two hyper-parameters required to tune: (1) the hidden size of linear layers d and (2) the number of retrieved nodes K. Here, we tune the d amongst {64, 128, 256, 512, 768, 1024} and K amongst {1, 5, 10, 15, 20}. We show the results in the middle and right of Figure 4 compared with the best results for the reasoning stage and the retrieval stage. Since K is a consistent hyper-parameter in the UniKGQA and SR, we also describe the various results of SR with different K to give a fair comparison. First, we can see that our method is robust to\n\n15\n\nPublished as a conference paper at ICLR 2023\n\ndifferent hidden sizes, as the performance is consistently nearby 77.0. As the PLM adopts 768 as the embedding size, we can see 768 is also slightly better than other numbers. Besides, we can see that with the increase of K, the answer coverage rate also improves consistently. However, when K increases to 15 or even 20, the performance gain becomes relatively small. It means that the retrieved subgraphs are likely saturated, and further increasing K could only bring marginal improvement.\n\n16",
    "reference": "# Summary Of The Paper\n\nThis paper proposed to learn a multi-hop KBQA model which contains a retrieval and reasoning model that shares the same architecture. Unifying the retrieval and reasoning module let the model share more learned knowledge. Experiments show great performance on three benchmark multi-hop reasoning datasets.\n\n# Strength And Weaknesses\n\nThe propose model is simple and effective. The results are also very impressive. It can be a go-to solution for multi-hop KBQA. I have some questions about the implementation.\n\n1. Without any of the proposed technique (w/o Pre, Trans), the model already outperforms the previous state-of-the-art. Do you know why this happen?\n2. The reasoning and retrieval module share the same input structure and same model architecture, but it seems they do not share the same parameters. This sounds weird to me, and I am not sure why this will lead to improvement in model's performance. How much improvement comes from the pretraining of question-relation matching (see also Fig 3(c))? You should consider emphasizing the pretraining strategy if it leads to a big improvement.\n3. What is the size of the KB you use for WebQSP and CWQ? Do you use the full Freebase?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nOverall the paper is clear to read.\n\nThe paper should include more details about key choices in the experiments, e.g. size of KB. Also, it makes me worried why the ablated results without any of the introduced technique (in Table 4) can outperform the previous state-of-the-art.\n\n# Summary Of The Review\n\nThe paper delivered good results in their experiments and the intuition of having a shared reasoning and retrieval model is promising. However, it is hard to tell whether the biggest improvement comes from the advertised techniques. Please consider emphasize on your contributions. Having good numbers itself does not warrant an acceptance of the paper.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCONNECTING REPRESENTATION AND GENERATION VIA MASKED VISION-LANGUAGE TRANSFORMER\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecently, there has been great progress in the self-supervised pre-training of multimodal representation models that understand image and language jointly. One particularly popular application of such models is text-to-image generation, which is typically obtained via a two-stage process: in the first stage, a representation model is trained via self-supervised objectives; then in the second stage, a conditional generative decoder is trained on top of the representation to generate natural images. In this work, we aim at bringing representation learning and conditional generation together by unifying the two stages into a single model and training objective. We present UPGen, a unified pre-trained model for both representation learning and generation. UPGen is trained with a simple masked token prediction objective on a flexible mixture of image and language data. We use a pre-trained VQGAN image tokenizer to convert images into discrete tokens, then train a masked token prediction model on both paired image-text datasets and unpaired language datasets, using randomly sampled mask ratios. We show that this masked token prediction model can be directly used to generate images and language by iteratively re-masking and predicting the masked tokens. We demonstrate empirically that UPGen serves as both a good representation learning model and a generative model for both image and language.\n\n1\n\nINTRODUCTION\n\nWith the rapid improvement of deep learning architecture and accelerator hardware, researchers have made significant progress in self-supervised representation learning from image and text data (Radford et al., 2021; Geng et al., 2022; Mu et al., 2021; Wang et al., 2022). Such models are trained to jointly understand language and image data and learn generalizable representations that transfer across image and language modalities, and thus can be applied to a wide variety of downstream tasks. One interesting task that has gained popularity recently is text-to-image generation, where the model is given a text prompt and generates an image that corresponds to the text prompt’s description (Saharia et al., 2022; Ramesh et al., 2021; Yu et al., 2022). This task is particularly attractive because it enables a human to directly interact with the model and inspect its understanding of language and image, providing great tools for artistic creations.\n\nDriven by the successes of representation learning, text-to-image models have achieved impressive progress (see e.g., Saharia et al., 2022; Ramesh et al., 2021; Yu et al., 2022). However, the drawbacks of these text-to-images models are due to the pipeline having two stages. In the first stage, a representation model is trained with self-supervised pre-training objectives. In the second stage, a diffusion (Saharia et al., 2022; Ramesh et al., 2021) or autoregressive (Yu et al., 2022) generative model is trained conditioned on top of the (typically frozen) pre-trained representation. Such a twostage training pipeline requires more hyperparameters to be tuned, and introduces extra complexity in developing models.\n\nTo address the above limitations, we propose UPGen, a simple but effective framework that unifies pre-training for representation and generation. Using a pre-trained VQGAN model (Esser et al., 2021), we convert an image into a sequence of discrete tokens and concatenate the image tokens and language tokens into a single sequence. We then train an encoder-only transformer model using a simple masked token prediction objective on the concatenated sequence. The masked token prediction objectives produces good representations for downstream tasks. Furthermore, we\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: UPGen consists of a pre-trained VQGAN tokenizer that converts images into discrete tokens and an encoder-only transformer that processes the image tokens and language tokens jointly. The concatenated image and language sequence are randomly masked according to a uniformly sampled ratio, and the transformer is trained to predict the masked tokens.\n\nshow that UPGen can be directly used for conditional and unconditional generation of images and language without having to train extra components. To achieve this, we apply an iterative refinement strategy by repeatedly re-masking and regenerating the masked tokens, following the approach of MaskGIT (Chang et al., 2022).\n\nIn this work, we provide a large-scale empirical study of UPGen on a mixture of paired image-text datasets and unpaired language datasets. We find that UPGen learns generalizable representations that transfer to a wide variety of tasks, including image classification, text-guided image inpainting, and image-to-text generation. We also demonstrate that UPGen can generate high-quality images with and without language conditioning on language prompts. While achieving competitive results, UPGen does not perform as well as state-of-the-art methods on each of the downstream tasks due to being trained on much smaller datasets. However, to the best of our knowledge, UPGen is the first model that combines representation learning, image-to-text generation, text-to-image-generation, and unconditioned image-generation into a single model and training objective. Scaling UPGen to larger datasets and model size is left as promising future work.\n\n2 RELATED WORKS\n\nSelf-supervised learning via masked modeling. Ever since the introduction of Transformers (Vaswani et al., 2017), self-supervised pre-training has made significant progress in the recent years. A particularly popular style of self-supervised learning is masked modeling, where the input example is partially masked and the model is trained to predict the masked part from the unmasked part. Masked modeling first sees its success in natural language processing (NLP), with large language models like BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020) and UL2 (Tay et al., 2022) that learns highly generalizable representations that transfer well to various of downstream tasks. Inspired by the effectiveness of these NLP models, researchers have taken the masked modeling ideas into computer vision and also met with great success. Vision transformers (Dosovitskiy et al., 2020) introduces the transformer architecture into computer vision and make the natural analogy between language tokens and patches of natural images. Building on this work, BEiT (Bao et al., 2021) proposes to apply the masked token prediction on vision transformers, taking image patches as input and predicting the output of a discrete tokens produced by a VQVAE (Van Den Oord et al., 2017) model. Recently, MAE (He et al., 2022) further removes the need of a discrete image tokenizer by directly predicting the patch pixel output using an encoder-decoder architecture. Our UPGen first tokenizes the image into discrete tokens by using a pre-trained VQGAN (Esser et al.,\n\n2\n\nVQGANEncoderVQGANDecoderVisual tokenEncoder-only Bidirectional Transformertok2tok3tok1tok4tok5tok6Language tokenMask tokentok7tok1tok4tok6tok7Under review as a conference paper at ICLR 2023\n\nImage-to-text\n\nText-to-image\n\nFigure 2: Pre-trained UPGen can be directly used for cross-modality generation: Image-to-text generation (left) and text-to-image generation (right) by inputting a sequence of masked tokens.\n\n2021) model, and then follows the simple masked token prediction objective of BERT. UPGen differs from BEiT mainly in the way that it uses the discretized image tokens as both input and output, which facilitates the generation process. In addition, our model is trained with both image and language data, which connects the information from different modalities together.\n\nMultimodal representation learning for language and image. While self-supervised learning on image or language data alone has proven great success in extracting useful information and producing generalizable representations, recent works have shown that the such capability can be greatly improved if we connect these two modalities modalities together. Multimodal self-supervised learning models achieves this by jointly learning on paired data of two modalities. One popular approach for multimodal pre-training is contrastive learning. Contrastive learning approaches such as CLIP (Radford et al., 2021), SLIP (Mu et al., 2021) and FLAVA (Singh et al., 2021) connects representation between different modalities together by maximizing the similarity between positive example pairs and minimizing the similarities between negative pairs. Such objectives are limited to paired image-text datasets, and extending the pre-training to unpaired data often requires the introduction of additional objectives. Recently, with the advancement of masked modeling techniques for images, masked modeling has gained a lot of popularity as an alternative objective for contrastive learning due to its simplicity and flexibility. M3AE (Geng et al., 2022) extends the MAE approach to both image and language by combining image patches and language tokens as a single sequence. BEiTv3 (Wang et al., 2022) also combines image patches and language tokens, but predict discretized image tokens from a VQVAE model as output. Both of these models uses simple masked token prediction objective. Unlike M3AE and BEiTv3, UPGen uses discretized image tokens for both model input and output in order to perform generation. Moreover, UPGen is trained with uniformly sampled mask ratios from 0 to 1, removing the need to tune mask ratio for each modality.\n\nText-to-image generation. Text-to-image generation has been a popular field with various approaches being proposed recently. Typically, text-to-image models follow the general pattern of pre-training a language encoder model via self-supervised objective and then training a generative image decoder conditioned on the language representation. Therefore, these models can be characterized by their language encoder type and their generative decoder type. From the language encoder perspective, DALL-E (Ramesh et al., 2021), DALL-E 2 (Ramesh et al., 2022) and Stable Diffusion (Rombach et al., 2021) uses the multimodal CLIP as their language encoder. Imagen (Saharia et al., 2022) uses T5 (Raffel et al., 2020), a pure language model, and Parti (Yu et al., 2022) trains a BERT-like transformer as its language encoder. GLIDE (Nichol et al., 2021) trains a transformer with the generative model in an end-to-end way to process language. From the generative model perspective, most approaches including DALL-E 2, GLIDE, Stable Diffusion and Imagen uses diffusion models (Ho et al., 2020) as its generative decoders. DALL-E and Parti uses autoregressive models in the discretized image token space as the generative decoders. Unlike these prior approaches, our model combines the representation encoder and generative decoder into a single transformer model. We show that the UPGen can complete both representation learning and generative tasks by training on a single objective of masked token prediction.\n\n3 UNIFYING REPRESENTATION PRE-TRAINING AND GENERATION\n\nIn this section we introduce our method UPGen, which is an encoder-only transformer model that takes the masked sequence of image and language tokens as input and predicts the masked tokens. We summarize the architecture and training objectives in Figure 1.\n\n3\n\nEncoder-only Bidirectional Transformertok2tok1tok3tok4tok5tok1tok2Encoder-only Bidirectional Transformertok2tok1tok3tok4tok5tok1tok2Encoder-only Bidirectional Transformertok2tok1tok3tok4tok5tok1tok2Encoder-only Bidirectional Transformertok2tok1tok3tok4tok5tok1tok2Under review as a conference paper at ICLR 2023\n\nVQGAN-based image tokenization. In order to apply the transformers architectures to images which do not exhibit a natural sequence structure, prior works typically use one of two approaches: slice the image into patches and treat these patches as a sequence, or use a pre-trained discrete representation model such as VQVAE (Van Den Oord et al., 2017), DVAE (Ramesh et al., 2021) or VQGAN (Esser et al., 2021) to convert the image into a sequence of discrete tokens. In this work, we choose the latter approach for many practical reasons. Firstly, the discrete representations bring the images closer to language in terms of format, allowing us to use the same input processing and cross-entropy loss function for both image tokens and language tokens. Moreover, the discrete-token classification output of our model naturally forms a probability distribution with high degree of freedom, allowing us to directly sample from the predicted token classes during generation without having to rely on extra probability distribution assumptions. For simplicity, we directly use the open-source pre-trained model from MaskGIT (Chang et al., 2022), which is a highly effective ConvNet-based VQGAN model trained on the ImageNet 1K dataset (Deng et al., 2009). We keep the VQGAN model fixed throughout our training process.\n\nUPGen Transformer architecture. After using the pre-trained VQGAN to convert an image into a sequence of tokens, we concatenate the image tokens and language tokens into a single sequence. For unpaired examples, we simply take the sequence of image or language tokens. We apply the standard trainable word embedding layer to obtain a sequence of vectors. In addition to the standard positional embeddings, we also add modality-type embeddings to enable the model to distinguish between two modalities, following the approach of M3AE (Geng et al., 2022). We feed the sequence of vectors into an encoder-only Transformer network, and apply a final linear projection on the output of the Transformer to obtain the classification logits.\n\nMasked token prediction training. We train UPGen with a simple masked token prediction objective. With the combined sequence of image and language tokens, we first randomly sample a mask ratio uniformly between 0 and 1 for each example in the batch, following the approach of MaskGIT (Chang et al., 2022), and then randomly replace the tokens in the combined sequence with a special mask token according to the ratio. We feed the masked sequence into the Transformer, and supervise the output with the ground-truth token only at the masked position, similar to BERT (Devlin et al., 2018).\n\nSpecifically, let X = [xi]N i=1 denote the latent tokens obtained by processing the image with the VQGAN encoder, where N is the length of the reshaped token matrix, and Y = [yi]K i=1 denote the text tokens obtained via the text tokenizer, where K is the number of text tokens. During training, we sample a subset of tokens and replace them with a special [MASK] token. Let M = {0, 1}(N +K) denote a randomly generated mask, we mask the token xi or yj if M[i] = 1 or M[j] = 1. Let Z = [X, Y] denote the sequence of image tokens and text tokens, and ˆZ = Z ⊙ M denote the sequence of unmasked tokens. The training objective is to minimize the negative log-likelihood of the masked tokens:\n\nL = − E\n\nZ∈D\n\n(cid:104)\n\n(cid:88)\n\nlog p(zi| ˆZ)\n\n(cid:105) ,\n\n∀i∈[1,N +K],M[i]=1\n\n(1)\n\nwhere Z ∈ D denotes over all image-text sequences. Note that when Z = X, Equation 1 reduces to MaskGIT (Chang et al., 2022) which does image-only masked token prediction. When Z = Y, Equation 1 reduces to training a BERT (Devlin et al., 2018) on language tokens with random mask ratio. Since the model is agnostic to what Z consists of, i.e., it can be image-text pair, text-only and image-only data, we train UPGen on not only image-text pairs but also text-only data to enable better language understanding.\n\nIterative cross-modality generation. During inference, since the model has been trained to model any set of unconditional and conditional probabilities, we can sample any subset of tokens per sampling iteration, from the extreme cases of sampling all tokens (independent) versus sampling each token at a time (autoregressive). We follow MaskGIT (Chang et al., 2022) to use a confidence-based sampling mechanism. Specifically, at each iteration, we only keep the predictions that the model is most confident with and discard the rest of predicted tokens and replace them with mask. We repeat this process until all tokens are generated.\n\nWe can control the model to condition on image tokens to predict text tokens by inputting a sequence of image tokens followed by a sequence of [MASK] tokens. The same can be done for text-to-image generation. The process is illustrated in Figure 2. For text-to-image generation, we visualize the tokens generated at intermediate iterations in Figure 3.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nA cup of coffee with latte art\n\nA large dog in the park Figure 3: Visualization of the iterative text-to-image generation process of UPGen. The text prompt is shown below the image generation process.\n\n4 EXPERIMENTS\n\nIn this section, we conduct an empirical study of UPGen and present the results. Since UPGen unifies representation learning and generation into a single model and training objective, it is important to evaluate its performance from both perspectives. Hence, in designing the experiments, we aim to answer the following questions: (1) As a representation learning model, does UPGen learn generalizable visual representations that transfer well to downstream tasks? (2) As a generative model, can UPGen generate high-quality image and text samples?\n\nTo answer these questions, we pre-train UPGen on a large and diverse dataset of image-language pairs. In addition to the paired data, we also use a combination of unpaired language-only data. We evaluate the representation quality of pre-trained UPGen models on image classification tasks, and we evaluate the image and text generation by inspecting the generated samples both qualitatively and quantitatively.\n\n4.1 UPGEN PRE-TRAINING\n\nDatasets. In pre-training UPGen, we use a combination of image-text paired datasets and text-only unpaired datasets. For the image-text paired dataset, we use the publicly available Conceptual Caption 12M (CC12M) (Changpinyo et al., 2021), Redcaps (Desai et al., 2021), and a 15M subset of the YFCC100M (Thomee et al., 2016) selected according to (Radford et al., 2021). For Conceptual Captions 12M, since the dataset is provided in the form of online image URLs, we are only able to retrieve 10M images due to some expired URLs. The combination of these three datasets give us a total of 36M image-text pairs.\n\nFor the unpaired text-only dataset, we follow the practice of BERT (Devlin et al., 2018) and use a mixture of multiple publicly available text corpus. Specifically, we combine the text datasets of Wikipedia (Foundation), CC-News (Hamborg et al., 2017), OpenWebText (Gokaslan & Cohen) and Toronto BookCorpus (Zhu et al., 2015).\n\nModel architecture and input processing. UPGen’s main model is a bi-directional encoder-only transformer (Vaswani et al., 2017) network. We adopt the convention of Vision Transformers (Dosovitskiy et al., 2020) and use a ViT-Large sized model. The pre-training takes the combined sequence of image tokens and language tokens as input after applying a fixed sinusoidal positional positional embedding and learnable type embedding. For the VQGAN image tokenizer, we use the pre-trained model from MaskGIT (Chang et al., 2022), which is a convnet-based encoder and decoder VQGAN pre-trained on ImageNet. The tokenizer converts a 256×256 image into 256 discrete tokens, where the codebook size is 1024. We list the detailed architecture and input processing information in Appendix A.\n\nPre-training. For all models, we pre-train for 50 epochs on the combined dataset with cross-entropy loss on the masked tokens. For paired image-text tokens, we sample a mask ratio uniformly between 0 and 1 every time and for unpaired text tokens we use a fixed mask ratio of 15%. For the default model, we apply a weighting of 1.0 for the loss on the image tokens, a weighting of 0.1 on the paired text tokens and a weighting of 0.01 on the unpaired text tokens. Detailed training hyperparameter information can be found in Appendix A.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nA group of animals standing in the snow\n\nGalaxy colored apple\n\nBlack forest cake with many cherries\n\nA nice cheese pizza\n\nA large seagull sitting on a post\n\nA crowd of people standing on top of a beach\n\nFigure 4: Examples of text-to-image generation of UPGen. The two images for the each prompt are generated with different random seeds.\n\n4.2 REPRESENTATION LEARNING\n\nFollowing the convention in prior works, we evaluate the representation learning performance of our model by performing linear classification on top of the learned representation on ImageNet 1K. We freeze the weights of the UPGen pre-train and take the embeddings of all the intermediate layers. We concatenate these embeddings and train a linear classification layer on top. We summarize the results in Table 1. The results show that UPGen achieves comparable performance to prior works in representation learning, indicating that UPGen can serve as a simple but strong alternative to existing visual representation learning methods. We do note that CLIP outperforms our model significantly, and hypothesize that this is the result of CLIP being trained on a proprietary dataset 10 times larger than the one we use.\n\nModel\n\nAccuracy\n\nUPGen\n\n70.18\n\nUPGen (paired data only) UPGen (image-only)\n\n70.18\n\n70.17\n\nModel\n\nCLIP (YFCC100M)\n\nMAE (ImageNet)\n\nM3AE (CC12M)\n\nAccuracy\n\n83.9\n\n73.5\n\n64.1\n\nTable 1: Linear classification accuracy on ImageNet 1K validation set. All models are based on ViT-L with the same number of parameters. In the top row, we present the accuracy of UPGen pre-trained with both paired and unpaired data, with unpaired data only and with image data only. In the second row, we show the performance of visual representation learning methods trained on other datasets. We see that UPGen reaches similar performance to other representation learning methods.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Examples of unconditioned image generation of UPGen. Even without conditioning on text prompts, UPGen is still able to generate images that contain diverse objects.\n\n4.3\n\nIMAGE GENERATION\n\nQualitative results. We perform qualitative evaluation on the image generation of UPGen. We present selected text-to-image generation results in Figure 4. We see UPGen is able to generate mostly coherent images that correspond well to the description of the text prompt. It can be observed that these images contain certain unnatural artifacts. We hypothesize that this is largely caused by the specific VQGAN image tokenizer we use, which is only trained on ImageNet and therefore is not able to encode objects not included in the ImageNet dataset. We also present unconditioned image generations in Figure 5. We see that even without conditioning on text prompts, UPGen can still generate images that contain natural objects. This further demonstrates the generality of our UPGen training objective.\n\nZero-Shot FID score on MS-COCO Following the convention in prior works of image-to-text generation, we use the FID (Heusel et al., 2017) score on MS-COCO (Lin et al., 2014) validation set as a quantitative metric of quality of generated images. To perform the evaluation, we sample 30K captions from the annotations of MS-COCO 2014 validation set and generate images from these captions. We then calculate the FID score against the original images in the validation set. This setting is known as zero-shot FID score since our model is not trained or finetuned on MS-COCO. We present the results in Table 2. While our model does not perform as well as prior methods for text-to-image generation, we note that our model is trained with a much smaller dataset, and to the best of our knowledge, UPGen is the first model that unifies representation learning and generation (of text and language, both conditioned and unconditioned) into a single model and training objective.\n\nModel\n\nUPGen DALL-E\n\nStableDiffusion GLIDE DALL-E 2\n\nImagen\n\nParti\n\nFID-30K score\n\n65.25\n\n28\n\n12.63\n\n12.24\n\n10.39\n\n7.27\n\n7.23\n\nTable 2: Zero shot FID-30K score evaluated on the MS-COCO validation set.\n\n4.4 TEXT GENERATION\n\nTo evaluate the quality of image-to-text generation of UPGen, we take several images from the MS-COCO validation set and use our model to generate captions for these images. We perform the exact same sampling process as text-to-image generation, starting from the image tokens instead of text tokens as unmasked input. We present the results in Figure 6. While the text is not fluent as those generated by an autoregressive language model, we see that UPGen is able to capture important keywords that describe the content of the image.\n\n4.5 ABLATION STUDIES\n\nSampling temperature for image generation. Following the practice of MaskGIT, we use a temperature to sample from the predicted logits during the generation process. This temperature hyperparameter has great influence on the quality of generated images. To find out the optimal\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nImage prompt\n\nDataset annotation\n\nModel output\n\nthe light display of the erupting volcano of stromboli.\n\nthe who vintage yellow guitar - 0\n\nthis violent volcano going out of the atmosphere will just erupt! an active volcano - - called the volcano ‘ ‘ thundering lightning - has glow only been abandoned for two weeks. the human sense of sound with its active bodies lies within, if ever in the parts, or event. < person >, lightning wallpaper.\n\nthis vintage 1950s electric guitar is available to elements from unique parts of the body. we will welcome this to your life. we are all unique. made of diameter. 3’s 6. 5 ‘ ‘, in gold top, in brass and engraved hardware, with custom hardware. electric guitar, guitar amp, guitar amp\n\ntower grove park : beautiful landscaping throughout the park.\n\naccording to < person >, the houston zoo is one of the most beautiful zoos of america, but the zoo does. if it cannot afford for any terms of the budget, it will cost you too much money. if it is going to be saving us, i would wonder if we can visit it. - -\n\nnorth sea beach, the hague, netherlands\n\ni was looking for a walk, but decided to conveniently cross - my - break when i received this lovely, brief breeze perfectly between my legs - either going for a stroll, or not to walk in it, whether you should walk on the beach or on the place. if you like me, then go out.\n\nFigure 6: Examples of image-to-text generation using our method. The images are sampled from the CC12M dataset. Left. The sampled image prompt from CC12M. Middle. Text annotation of the image from CC12M. Right. Model-generated text based on the image prompt.\n\ntemperature, we measure the FID score on MS-COCO validation set of generated images on different temperatures, and present the result in Figure 7. The result suggests an optimal temperature of 20. Qualitatively, we find that having too low temperature makes the generation process more greedy by only sampling the most probable token, which result in less diverse images. If the temperature is too high, the generation process becomes too chaotic and the generated images are less coherent.\n\nThe effect of unpaired text data. While the flexibility of UPGen enables the use of both paired and unpaired image and text data in training, it is important to understand the effects of using additional unpaired text data during pre-training. We conduct an ablation study comparing the performance of UPGen pre-trained with and without unpaired text data. We measure the visual representation quality of these two models via ImageNet linear classification accuracy, and test the quality of text-to-image generation with the FID score on MS-COCO validation set. We present the results in Table 3. We see\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTraining data\n\nWith unpaired text No unpaired text\n\nMS-COCO zero-shot FID-30K score\n\nImageNet linear classification accuracy\n\n65.25\n\n70.18\n\n67.32\n\n70.18\n\nTable 3: Ablation study of the effects of using additional unpaired language data in pre-training UPGen. We see that while the use of unpaired text data does not improve visual representation quality, it improves the quality of text-to-image generation which requires more complex language understanding.\n\nthat the use of unpaired data does not improve the quality of visual representations. This result is expected since the ImageNet classification task does not require understand of complex logic and reasoning in natural language, and simply matching keywords is sufficient. On the other hand, we observe that the use of unpaired text data improves the FID score for text-to-image generation, which is a task that requires stronger language understanding.\n\nThe effect of loss weights. During the pre-training of UPGen, although we use cross-entropy loss for both predicted image tokens and predicted language tokens, we find that it is important to balance the weight of these two losses. Specifically, we discover that having a lower weight on the language loss improves the quality of both visual representation and image generation. Following the previous ablation study, we present the effect of the language loss weights in Table 4. We hypothesize that this is caused by the text token prediction being a much simpler problem than image token prediction. This can also be observed by the training accuracy of text and token predictions. During the final steps of training, the masked text token prediction accuracy converges to 37% while the masked image token prediction accuracy stay around 15%. Hence, it is important to put more weights on the more difficult image token prediction problem.\n\nFigure 7: The effect of different sampling temperature on MS-COCO FID score.\n\nText loss weight of UPGen\n\n1.0\n\n0.1\n\nMS-COCO zero-Shot FID-30K score\n\n66.85\n\n65.25\n\nImageNet linear classification accuracy\n\n69.13\n\n70.18\n\nTable 4: Ablation study of the text loss weight during pre-training of UPGen. We see that using a smaller weight for language loss improves both visual representation and image generation.\n\n5 CONCLUSION AND LIMITATIONS\n\nIn this work, we present UPGen, a simple masked Transformer-based method for multimodal tasks, including representation learning, image-to-text generation, and text-to-image generation. We conducted quantitative and qualitative experimental evaluations on UPGen and demonstrate competitive performance on a wide range of tasks. As UPGen is scalable and flexible, an interesting future work would be scaling up the amount of image-text, image-only and text-only data. A limitation of UPGen is that incorporating text-only data does not improve visual representation learning, even though training on additional language data should enable the model to better leverage cross-modal information. We believe resolving this would further improve the results of UPGen, and leave this as a future work.\n\n9\n\n1510203040Token Sampling Temperature708090100110120FID ScoreUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315–11325, 2022.\n\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558–3568, 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nKaran Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated imagetext data created by the people, for the people. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/e00da03b685a0dd18fb6a08af0923de0-Abstract-round1.html.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873–12883, 2021.\n\nWikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.\n\nXinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey Levine, and Pieter Abbeel. Multimodal masked autoencoders learn transferable representations. arXiv preprint arXiv:2205.14204, 2022.\n\nAaron Gokaslan and Vanya Cohen. Openwebtext corpus.\n\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science, pp. 218–223, March 2017. doi: 10.5281/zenodo.4120316.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on\n\nautoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL http://github.com/google/flax.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nNorman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets\n\nlanguage-image pre-training. arXiv preprint arXiv:2112.12750, 2021.\n\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n\nresolution image synthesis with latent diffusion models, 2021.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. arXiv preprint arXiv:2112.04482, 2021.\n\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022.\n\nBart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016.\n\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\n\nneural information processing systems, 30, 2017.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA IMPLEMENTATIONS DETAILS\n\nWe provide our implementation and training details of UPGen in this section.\n\nA.1\n\nIMPLEMENTATION\n\nWe implement our UPGen Transformers using Flax (Heek et al., 2020) and JAX (Bradbury et al., 2018). Our implementation is largely based on open source codebases, such as the released code for MaskGIT (Chang et al., 2022) and M3AE (Geng et al., 2022). For the VQGAN tokenizer, we directly use the open source ImageNet pre-trained VQGAN from MaskGIT for 256x256 image size.\n\nA.2 HYPERPARAMETERS\n\nIn this section we provide the training hyperparameters of UPGen. These configurations largely follow the training hyperparameters of MaskGIT (Chang et al., 2022).\n\nHyperparameter\n\nValue\n\nModel size Dropout Optimizer Base learning rate Weight decay Optimizer momentum Batch size Learning rate schedule Epochs Warmup epochs Image data augmentation Paired image loss weight Paired text loss weight Unpaired text loss weight\n\nViT-Large 0.1 AdamW 0.005 0.05 β1 = 0.9, β2 = 0.95 4096 cosine decay 50 5\nRandom crop and flip 1.0 0.1 0.01\n\nTable 5: Hyperparameters for training UPGen.\n\nA.3 TRAINING AND COMPUTE RESOURCES\n\nWe train our model on Cloud TPUv3 pods on Google Cloud Platform through the TPU Research Cloud program. On a TPUv3-512 pod, UPGen using unpaired data takes 80 hours to train and UPGen with only paired data takes 45 hours to train.\n\n13",
    "reference": "# Summary Of The Paper\n\nThis paper presents a unified pre-trained model UPGen for both generation and representation learning.\nLike DALL-E/BEiTv1-3, the image data are tokenized into dVAE/VQ-VAE tokens, and then is used to learn MLM on mono-modal data or image-text pair data with a random probability.\n\n# Strength And Weaknesses\n\nStrength:\n\n  1.This paper proposes a simple idea.\n  2.This paper is well written and organized.\n\nWeaknesses:\n\n1.Lock of novelty. In contrast to DALL-E, this paper uses a BERT-like bidirectional encoding during the pretraining phase. And similar to BEiT-3, pretrain on both mono-modal and image-text pair data, and perform mask-and-reconstruct loss.\n2.The reviewer thought the result in this paper was not very satisfactory.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nLack of novelty, and please revise other mistakes carefully.\n\n# Summary Of The Review\n\nI prefer to reject this paper since the novelty, please check the weaknesses.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  }
]