{
  "example_idx": 87,
  "reference": "Under review as a conference paper at ICLR 2023\n\nCRITICAL SAMPLING FOR ROBUST EVOLUTION BEHAVIOR LEARNING OF UNKNOWN DYNAMICAL SYSTEMS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe study the following new and important problem: given an unknown dynamical system, what is the minimum number of samples needed for effective learning of its governing laws and accurate prediction of its future evolution behavior, and how to select these critical samples? In this work, we propose to explore this problem based on a design approach. Specifically, starting from a small initial set of samples, we adaptively discover and collect critical samples to achieve increasingly accurate learning of the system evolution. One central challenge here is that we do not know the network modeling error of the ground-truth system state, which is however needed for critical sampling. To address this challenge, we introduce a multi-step reciprocal prediction network where a forward evolution network and a backward evolution network are designed to learn and predict the temporal evolution behavior in the forward and backward time directions, respectively. Very interestingly, we find that the desired network modeling error is highly correlated with the multi-step reciprocal prediction error. More importantly, this multi-step reciprocal prediction error can be directly computed from the current system state without knowing the ground-truth or data statistics. This allows us to perform a dynamic selection of critical samples from regions with high network modeling errors and develop an adaptive sampling-learning method for dynamical systems. To achieve accurate and robust learning from this small set of critical samples, we introduce a joint spatial-temporal evolution network which incorporates spatial dynamics modeling into the temporal evolution prediction for robust learning of the system evolution operator with few samples. Our extensive experimental results demonstrate that our proposed method is able to dramatically reduce the number of samples needed for effective learning and accurate prediction of evolution behaviors of unknown dynamical systems by up to hundreds of times, especially for high-dimensional dynamical systems.\n\n1\n\nINTRODUCTION\n\nRecently, learning-based methods for complex and dynamic system modeling have become an important area of research in machine learning. The behaviors of dynamical systems in the physical world are governed by their underlying physical laws (Bongard & Lipson, 2007; Schmidt & Lipson, 2009). In many areas of science and engineering, ordinary differential equations (ODEs) and partial differential equations (PDEs) play important roles in describing and modeling these physical laws (Brunton et al., 2016; Raissi, 2018; Long et al., 2018; Chen et al., 2018; Raissi et al., 2019; Qin et al., 2019). In recent years, data-driven modeling of unknown physical systems from measurement data has emerged as an important area of research. There are two major approaches that have been explored. The first approach typically tries to identify all the potential terms in the unknown governing equations from a priori dictionary, which includes all possible terms that may appear in the equations (Brunton et al., 2016; Schaeffer & McCalla, 2017; Rudy et al., 2017; Raissi, 2018; Long et al., 2018; Wu & Xiu, 2019; Wu et al., 2020; Xu & Zhang, 2021). The second approach for data-driven learning of unknown dynamical systems is to approximate the evolution operator of the underlying equations, instead of identifying the terms in the equations (Qin et al., 2019; Wu & Xiu, 2020; Qin et al., 2021a; Li et al., 2021b).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of the proposed method of critical sampling for accurately learning the evolution behaviors of unknown dynamical systems. Many existing data-driven approaches for learning the evolution operator typically assume the availability of sufficient data, and often require a large set of measurement samples to train the neural network, especially for high-dimensional systems. For example, to effectively learn a neural network model for the 2D Damped Pendulum ODE system, existing methods typically need more than 10000 samples to achieve sufficient accuracy (Qin et al., 2019; Wu & Xiu, 2020). This number increases dramatically with the dimensions of the system. For example, for the 3D Lorenz system, the number of needed samples used in the literature is often increased to one million. We recognize that, in practical dynamical systems, such as ocean, cardiovascular and climate systems, it is very costly to collect observation samples. This leads to a new and important research question: what is the minimum number of samples needed for robust learning of the governing laws of an unknown system and accurate prediction of its future evolution behavior?\n\nAdaptive sample selection for network learning, system modeling and identification has been studied in the areas of active learning and optimal experimental design. Methods have been developed for global optimization of experimental sequences (Llamosi et al., 2014), active data sample generation for time-series learning and modeling (Zimmer et al., 2018), Kriging-based sampling method for learning spatio-temporal dynamics of systems (Huang et al., 2022), adaptive training of physicsinformed deep neural networks (Zhang & Shafieezadeh, 2022), and data-collection scheme for system identification (Mania et al., 2022). However, within the context of deep neural network modeling of unknown dynamical systems, the following key challenging issues have not been adequately addressed: (1) how to characterize and estimate the prediction error of the deep neural networks? (2) Based on this error modeling, how to adaptively select the critical samples and successfully train the deep neural networks from these few samples?\n\nFigure 1 illustrates the proposed method of critical sampling for accurately learning the evolution behaviors of unknown dynamical systems. We start with a small set of initial samples, then iteratively discover and collect critical samples to obtain more accurate network modeling of the system. During critical sampling, the basic rule is to select the samples from regions with high network modeling errors so that these selected critical samples can maximally reduce the overall modeling error. However, the major challenge here is that we do not know network modeling error, i.e., the difference between the system state predicted by the network and the ground-truth which is not available for unknown systems. To address this challenge, we establish a multi-step reciprocal prediction framework where a forward evolution network and a backward evolution network are designed to learn and predict the temporal evolution behavior in the forward and backward time directions, respectively. Our hypothesis is that, if the forward and backward prediction models are both accurate, starting from an original state A, if we perform the forward prediction for K times and then perform the backward prediction for another K times, the final prediction result ̄A should match the original state A. The error between ̄A and A is referred to as the multi-step reciprocal prediction error.\n\nVery interestingly, we find that the network modeling error is highly correlated with the multi-step reciprocal prediction error. Note that multi-step reciprocal prediction error can be directly computed from the current system state, without the need to know the ground-truth system state. This allows us to perform a dynamic selection of critical samples from regions with high network modeling errors and develop an adaptive learning method for dynamical systems. To effectively learn the system evolution from this small set of critical samples, we introduce a joint spatial-temporal evolution network structure which couples spatial dynamics learning with temporal evolution learning. Our extensive experimental results demonstrate that our proposed method is able to dramatically reduce the number of samples needed for effective learning and accurate prediction of evolution behaviors of unknown dynamical systems. This paper has significant impacts in practice since collecting samples from real-world dynamical systems can be very costly or limited due to resource/labor constraints or experimental accessibility.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nThe major contributions of this work can be summarized as follows. (1) We have successfully developed a multi-step reciprocal prediction approach to characterize the prediction errors in deep neural network modeling of unknown dynamical systems. We have made an interesting finding that the network modeling error is highly correlated with the multi-step reciprocal prediction error, which enables us to develop the critical sampling method. (2) We have designed a joint spatial-temporal evolution network which is able to learn the system evolution with few critical samples. (3) Our proposed method is able to dramatically reduce the number of needed samples and related sample collection costs for learning the system evolution, which is highly desirable in practical applications.\n\n2 RELATED WORK\n\nIn this section, we review existing research closely related to our work.\n\n(1) Data-driven modeling of unknown physical systems. There are two major approaches that have been explored in the literature. The first approach aims to learn the mathematical formulas or expressions of the underlying governing equations. In a series developments of this direction, the seminal work was made by (Bongard & Lipson, 2007; Schmidt & Lipson, 2009), where symbolic regression was proposed to learn nonlinear dynamic systems from data. Later, more approaches have been proposed in this direction, including but not limited to sparse regression (Brunton et al., 2016; Schaeffer & McCalla, 2017; Rudy et al., 2017), neural networks (Long et al., 2018; Raissi, 2018; Long et al., 2019), Koopman theory (Brunton et al., 2017), Gaussian process regression (Raissi et al., 2017), model selection approach (Mangan et al., 2017), classical polynomial approximations (Wu & Xiu, 2019; Wu et al., 2020), genetic algorithms (Xu et al., 2020; Xu & Zhang, 2021; Xu et al., 2021), and linear multi-step methods (Keller & Du, 2021), etc.\n\nThe second approach aims to approximate the evolution operator of the underlying dynamical system typically via a deep neural network, which predicts the system state for the next time instance from the current state (Wu & Xiu, 2020; Qin et al., 2021a; Li et al., 2021a;b). In fact, the idea of such an approach is essentially equivalent to learning the integral form of the underlying unknown differential equations (Qin et al., 2019). The performance of this approach has been demonstrated for learning ODEs (Qin et al., 2019) and modeling PDEs in generalized Fourier spaces (Wu & Xiu, 2020) and physical space (Chen et al., 2022). Recently, this approach has also been extended to datadriven modeling of parametric differential equations (Qin et al., 2021b), non-autonomous systems (Qin et al., 2021a), partially observed systems (Fu et al., 2020), biological models (Su et al., 2021), and model correction (Chen & Xiu, 2021). For an autonomous dynamical system, its evolution operator completely characterizes the system evolution behavior. Researchers have demonstrated that the evolution operator, once successfully learnt, can be called repeatedly to predict the evolution behaviors of the unknown dynamical systems (Qin et al., 2019; Wu & Xiu, 2020; Chen et al., 2022).\n\n(2) State-space models and adjoint state methods. State-space models have shown to be a powerful tool for modeling the behaviors of dynamical systems (McGoff et al., 2015). Methods have been developed for approximating dynamical systems with hidden Markov model (HMM) (Rabiner, 1989; Fraser, 2008), recurrent neural network (RNN) (Han et al., 2004), long short-term memory network (LSTM) (Vlachas et al., 2018), reservoir computing (RC) (Inubushi & Yoshimura, 2017), structured variational autoencoder (SVAE) (Johnson et al., 2016), linear dynamical system (LDS) and its variations (Fox et al., 2008; Linderman et al., 2017; Gao et al., 2016).\n\nWe recognize that the multi-step forward and backward processes and the usage of mismatch errors are related to those in the recent adjoint state methods for neural ODE learning (e.g. Chen et al. (2018); Zhuang et al. (2020a;b)). However, our method is uniquely different in the following aspects. (1) In the adjoint state method, the back propagation is used to compute gradients based on a Lagrangian functional. However, in our method, the backward network is used to learn the inverse of the forward evolution operator (namely, backward evolution operator, see Lemma C.1). (2) The adjoint state method aims to compute the gradients of the loss functions more efficiently and accurately. However, our method aims to discover the critical samples for network learning.\n\n3 METHOD\n\nIn this section, we present our method of critical sampling for accurate learning of the evolution behaviors for unknown dynamical systems.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3.1 PROBLEM FORMULATION AND METHOD OVERVIEW\n\nIn this work, we focus on learning the evolution operator Φ∆ : Rn → Rn for autonomous dynamical systems, which maps the system state from time t to its next state at time t + ∆: u(t + ∆) = Φ∆(u(t)). It should be noted that, for autonomous systems, this evolution operator Φ∆ remains It only depends on the time difference ∆. For an autonomous system, its invariant over time. evolution operator completely characterizes the system evolution behavior (Qin et al., 2019; Wu & Xiu, 2020; Chen et al., 2022).\n\nOur goal is to develop a deep neural network method to accurately learn the evolution operator and robustly predict the long-term evolution of the system using a minimum number of selected critical samples. Specifically, to learn the system evolution over time, the measurement samples for training the evolution network are collected in the form of pairs. Each pair represents two solution states along the evolution trajectory at time instances t and t + ∆. For simplicity, we assume that the start time is t = 0. Using a high-accuracy system solver, we generate J system state vectors {uj(0)}J at time 0 and {uj(∆)}J given by\n\nj=1 at time ∆ in the computational domain D. Thus, the training samples are\n\nSF = {[uj(0) → uj(∆)] : uj(0), uj(∆) ∈ Rn, 1 ≤ j ≤ J}. (1) It is used to train the forward evolution network Fθ which approximates the forward evolution operator Φ∆. As discussed in Section 1, we introduce the idea of backward evolution operator Ψ∆ : Rn → Rn, u(0) = Ψ∆(u(∆)). The original training samples in SF can be switched in time to create the following sample set\n\nj=1\n\nSG = {[uj(∆) → uj(0)] : 1 ≤ j ≤ J},\n\n(2)\n\nwhich is used to train the backward evolution network Gθ. The forward and backward evolution networks, Fθ and Gθ, form a reciprocal prediction loop.\n\nFigure 2: Illustration of the proposed idea of multi-step reciprocal prediction error.\n\nBased on the forward and backward prediction errors, we introduce a multi-step reciprocal prediction process. As illustrated in Figure 2, starting from the initial condition u(0) ∈ Rn at time t = 0, we perform K-step prediction of the system state by repeatedly calling the forward evolution network Fθ with ˆu((k + 1)∆) = Fθ[ˆu(k∆)] and ˆu(0) = u(0). We then apply the backward evolution network Gθ to perform K-step backward prediction: ̄u((k − 1)∆) = Gθ[ ̄u(k∆)] and get back to the initial condition ̄u(0). This process of forward and backward evolution prediction is referred to as multi-step reciprocal prediction. The difference between the original value u(0) and the final prediction ̄u(0), namely, E[u(0)] = ∥u(0) − ̄u(0)∥, is referred to as the multi-step reciprocal prediction error in the Euclidean norm ∥∥. In this work, we have the following interesting finding: there is a very strong correlation between the network modeling error E(u(0)) and the multi-step reciprocal prediction error E(u(0)). This allows us to use E(u(0)) to approximate the desired network modeling error E[u(0)] = ∥Fθ[u(0)] − Φ∆(u(0))∥. Note that this multi-step reciprocal prediction error can be computed directly with the current state and the forward-backward evolution networks. Its computation does not require the ground-truth system state. Therefore, we can use reciprocal prediction error to guide the selection of critical samples from regions with large modeling errors.\n\nLet Jm be the number of samples at m-th iteration of our critical sampling process, current sample F = {[uj(0) → uj(∆)] : 1 ≤ j ≤ Jm} is used to train the spatial-temporal evolution network. set S m The corresponding forward-backward evolution networks are denoted by F m θ . Using the corresponding multi-step reciprocal prediction error distribution E(u(0)), we can determine regions with high error values and collect a new set of samples Ωm, which are added to the existing set of samples to update the training set:\n\nθ and Gm\n\nS m+1\n\nF\n\n= S m\n\nF\n\n(cid:91)\n\nΩm = {[uj(0) → uj(∆)] : 1 ≤ j ≤ Jm+1}.\n\n(3)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nThe above sampling-learning process is repeated until the overall prediction error drops below the target threshold. According to Table 3 in Supplemental Materials, the complexity for training is increased by 4-5 times, which directly depends on the number of iterations needed to reach the threshold for the network modeling error. However, the above critical sample selection process can dramatically reduce the number of needed training samples and related sample collection cost. In the following sections, we will explain this process in more details.\n\n3.2 MULTI-STEP RECIPROCAL PREDICTION ERROR AND CRITICAL SAMPLING\n\nIn this section, we show that there is a strong correlation between the multi-step reciprocal prediction error and the network modeling error of the temporal evolution network F m θ .\n\nθ and a backward evolution network Gm\n\n(1) Multi-step reciprocal prediction. In our multi-step reciprocal prediction scheme, we have a forward temporal evolution network F m θ , which model the system evolution behaviors in the forward and backward time directions. If the forward and backward evolution networks are both well-trained, accurately approximating the forward and backward evolution operators, for an arbitrarily given system state u(0), the reciprocal prediction error LS = ∥u(0) − Gm θ [u(0)]]∥ should approach 0. Now, we extend this one-step reciprocal prediction to K steps. As illustrated in Figure 2, starting from the initial condition u(0), we repeatedly apply the forward evolution network F m θ to perform K-step prediction of the system future states, ˆu(k∆) = F m,(k) represents the k-fold composition of F m θ :\n\nθ [F m\n\nθ\n\n[u(0)] , where F m,(k) F m,(k)\n\nθ\n\nθ\n\nθ ◦ F m\n\n= F m (cid:124)\n\nθ ◦ · · · ◦ F m (cid:125)\n\n(cid:123)(cid:122) k−fold\n\nθ\n\n.\n\n(4)\n\nAfter K steps of forward evolution prediction, then, starting with ˆu(K∆), we perform K steps of backward evolution prediction using network Gm [ˆu(K∆)], k = K − 1, · · · , 1, 0, where\n\nθ : ̄u(k∆) = Gm,(K−k)\n\nθ\n\nGm,(K−k)\n\nθ\n\n= Gm (cid:124)\n\nθ ◦ Gm\n\nθ ◦ · · · ◦ Gm θ\n(cid:125)\n\n(cid:123)(cid:122) (K−k)−fold\n\n(5)\n\nand reach back to time t = 0. If the forward and backward evolution networks are both accurate, the forward prediction path and the backward prediction path should match with each other. Motivated by this, we define the multi-step reciprocal prediction error for the forward evolution network F m θ\nas the deviation between the forward and backward prediction paths:\n\nE[u(0)] =\n\nK (cid:88)\n\nk=0\n\n(cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13)ˆu(k∆) − ̄u(k∆) (cid:13)\n\n.\n\n(6)\n\nNote that, when computing E[u(0)], we only need the current system state u(0), the forward and backward evolution networks F m θ . Figure 3 shows several examples from the Damped Pendulum and 2D Nonlinear ODE systems listed in Table 2 in Supplemental Materials. The top row shows examples with accurate prediction of their system states. We can see that their forward and backward prediction paths match well and the corresponding multi-step prediction error is very small. For comparison, the bottom shows examples with large prediction errors.\n\nθ and Gm\n\nFigure 3: Examples of multi-step reciprocal prediction errors on the first two dynamical systems. (2) Predicting the network modeling error. In this work, we find that there is a strong correlation between the network modeling error E[u(0)] and the multi-step reciprocal prediction error E[u(0)]. Figure 4 shows four examples of E[u(0)] (top row) and E[u(0)] (middle row) for the Damped\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nPendulum and 2D Nonlinear system with different sizes of training samples. The bottom row shows the values of E[u(0)] and E[u(0)] of locations with large errors. We can see that there is a strong correlation between the network modeling error E[u(0)] and the multi-step reciprocal prediction error E[u(0)]. This correlation allows us to predict E[u(0)] using E[u(0)] which can be computed directly from the current system state without the need to know the ground-truth state.\n\nFigure 4: Correlation between network modeling error and multi-step reciprocal prediction error on Damped Pendulum and 2D Nonlinear ODE systems.\n\n(3) Critical sampling and adaptive evolution operator learning. Once we are able to predict the network modeling error E[u(0)] using the multi-step reciprocal prediction error E[u(0)], we can develop a critical sampling and adaptive evolution learning algorithm. The central idea is to select samples from locations with large values of error E[u(0)] using the following iterative peak finding algorithm. Note that u(0) ∈ Rn. Write u(0) = [u1, u2, · · · , un]. Let S m F = {[uj(0) → uj(∆)] : 1 ≤ j ≤ Jm} be the current sample set. To determine the locations of new samples, {uj(0)|Jm + 1 ≤ j ≤ Jm+1}, we find the peak value of multi-step reciprocal prediction error E[u(0)] at every sampling point u(0) in the solution space D. In our experiment, we choose the sample point u(0) from the augmented sample set ̄Sn F defined in (8) of the following section. The corresponding peak location is chosen to be uJm+1(0) and the corresponding sample [uJm+1(0) → uJm+1(∆)] is collected. This process is repeated for Jm+1−Jm times to collect Jm+1−Jm samples in Ωm, which is added to the current sample set:\n\nS m+1\n\nF\n\n= S m\n\nF\n\n(cid:91)\n\nΩm = {[uj(0) → uj(∆)] : 1 ≤ j ≤ Jm+1}.\n\n(7)\n\n3.3\n\nJOINT SPATIAL DYNAMICS AND TEMPORAL EVOLUTION LEARNING\n\nJoint spatial dynamics and temporal evolution learning aims to couple local dynamics learning in the spatial domain and evolution learning in the temporal domain to achieve robust system evolution learning from the small set of selected critical samples.\n\n(1) Sample augmentation based on local spatial dynamics. Let S m F = {[uj(0) → uj(∆)] : 1 ≤ j ≤ Jm} be the current set of samples collected from the dynamical system. In this work, Jm is a small number. For example, in our experiments, Jm is in the range of a few hundreds. From our experiments, we find that this learning process is unstable since the number of samples is too small. Our central idea is to design a spatial dynamics network to learn the local spatial dynamics so that we can predict or interpolate more samples at unknown locations from existing samples at known locations. Specifically, let {vi(0) ∈ Rn : 1 ≤ i ≤ I} be a large set of randomly selected points in the state space of Rn. Let vi(∆) = Φ∆(vi(0)) be the system’s future state at time ∆ when its current state is vi(0). Here, {vi(∆)} are predicted by the spatial dynamics network from the existing sample set S m F = S m\n\nF , denoted by ˆvi(∆). They are added to S m VF , VF = {[vi(0) → ˆvi(∆)] : 1 ≤ i ≤ I, ˆvi(∆) = Γm\n\nF as augmentation samples\n\nF ; vi(0)]},\n\nw [S m\n\n ̄S m\n\n(8)\n\n(cid:91)\n\nF\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nw [S m\n\nwhere Γm future state of vi(0) based local spatial change patterns using the existing samples S m augmented sample set ̄S m\n\nF ; vi(0)] represent the spatial dynamics network at m-th iteration which predicts the F . With this\n\nF , we can train the temporal evolution network F m θ . (2) Learning the local spatial dynamics. The dynamical system may exhibit highly nonlinear and complex behavior in the whole spatial domain, which could be challenging to be accurately modeled and predicted. However, within a small local neighborhood, its behavior will be much simpler and can be effectively learned by our spatial dynamics network Γw. Specifically, given an arbitrary point v(0) = [v1, v2, · · · , vn] in Rn, we find its nearest H points from the existing sample F , and the corresponding samples are denoted by Sv(0) = {[zh(0) → zh(∆)] : 1 ≤ h ≤ H}, set S m which are the input to the spatial dynamics network. We use a pth-order n-variate polynomial P(u) = P[c1, c2, · · · , cP ](u1, u2, · · · , un) to locally approximate the local spatial dynamics. The coefficients of the polynomial are [c1, c2, · · · , cP ], which are predicted by the spatial dynamics network Γw. For example, if p = 1, this becomes a linear approximation with P = n+1 coefficients. If p = 2, the number of coefficients, or the size of the network output becomes P = 1 2 (n+2)(n+1). To summarize, the task of the spatial dynamics network Γw is to predict the coefficients of the polynomial P(u) from the set of M neighboring samples Sv(0) so that P(u)|u=v(0) = v(∆), where v(∆) is the future state of the system at time ∆ when its current state is v(0), or v(∆) = Φ∆(v(0)).\n\nw , we can choose the sample from the existing sample F as the input v(0) and the corresponding output as v(∆). The L2 loss between the predicted\n\nWhen training the spatial dynamics network Γm set S m state at time ∆ and its true value for v(0), namely,\n\nLSDN =\n\n(cid:88)\n\nv(0)∈Sm F\n\n(cid:13) (cid:13)v(∆) − Γm (cid:13)\n\nw [S m\n\nF ; v(0)]\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\n(9)\n\n(3) Joint learning of spatial dynamics and temporal evolution networks. The temporal evolution network Fθ and the spatial dynamics network Γw aim to characterize the system behavior from two different perspectives, the temporal and spatial domains. In this work, we couple these two networks so that they can learn more effectively. Specifically, we use the spatial dynamics network Γm w to generate a large set of samples VF, which is added to the existing samples S m F , as explained in (8). This augmented sample set ̄S m θ . Note that both networks are predicting the system future state from spatial and temporal domains. Therefore, we can introduce consistency constraint between them. Specifically, let Q = {ql ∈ Rn : 1 ≤ l ≤ L} be a set of randomly generated points in Rn. We use both networks to predict the future state at time ∆ for each ql at the initial state. The following consistency loss is used to train both networks:\n\nF is used to train the temporal evolution network F m\n\nLC =\n\nL (cid:88)\n\nl=1\n\n3.4 THEORETICAL UNDERSTANDING\n\n(cid:13) (cid:13)F m (cid:13)\n\nθ [ql] − Γm\n\nw [S m\n\nF ; ql]\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n,\n\n(10)\n\nThis section provides some mathematical analysis results to understand and characterize the performance of the proposed critical sampling and adaptive evolution learning method. Assumption 3.1. Assume that the underlying dynamical system d dt u(t) = H(u(t)) is autonomous with H being Lipschitz continuous (the Lipschitz constant is denoted by CH) on a set D ⊂ Rn, which is a bounded region where we are interested in the solution behavior. Assumption 3.2. Suppose that the generalization error of the trained neural network is bounded:\n\n∥Fθ − Φ∆∥L∞(D) =: εf < +∞,\n\n∥Gθ − Ψ∆∥L∞(D) =: εg < +∞.\n\n(11)\n\nWe derive a generic bound for the prediction error of our network model. Let ˆu(k) be the forward predicted solution by the trained primal network model at time t(k) := t0 + k∆ starting from t0, and ̃u(k) be the backward predicted solution by the trained dual network model at time t(k) starting from t(K), where 0 ≤ k ≤ K. Denote the corresponding forward prediction error as ˆE (k) := ∥ˆu(k) − u(t(k))∥ and the backward prediction error as ̃E (k) := ∥ ̃u(k) − u(t(k))∥, k = 0, 1, . . . , K. Theorem 3.3. Under Assumptions 3.1 and 3.2, we have the following estimates. (1) If ˆu(k), u(t(k)) ∈ ˆD∆ for 0 ≤ k ≤ K with ˆD∆ := {u ∈ D : Φt(u) ∈ D ∀t ∈ [0, ∆]}, then\n\nˆE (k) ≤ ˆE (0)eCHk∆ +\n\n(cid:18) eCHk∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεf .\n\n(12)\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(2) If ̃u(k), u(t(k)) ∈ ̃D∆ for 0 ≤ k ≤ K with ̃D∆ := {u ∈ D : Φ−1 (cid:18) eCH(K−k)∆ − 1 eCH∆ − 1\n\n ̃E (k) ≤ ̃E (K)eCH(K−k)∆ +\n\nt (u) ∈ D ∀t ∈ [0, ∆]}, then\n\n(cid:19)\n\nεg.\n\n(13)\n\nPlease see Section C for the detailed proof of this theorem and further discussions in Theorem C.4. The estimates (12)–(13) imply that in the worst cases the prediction errors may grow exponentially with the number of steps k. The analysis suggests that the reciprocal prediction error is correlated with the network modeling error, which provides a theoretical support for our finding. The critical sampling can help to effectively reduce the reciprocal prediction error and suppress the undesirable error growth, thereby enhancing the accuracy and stability of our model.\n\n4 EXPERIMENTAL RESULTS\n\n4.1 EXPERIMENTAL SETTINGS\n\nWe follow the evaluation procedure used in existing research, for example those reviewed in Section 2, to evaluate the performance of our proposed method on specific examples of dynamical systems. We consider four representative systems with ODEs and PDEs as their governing equations, as summarized in Table 2 in Supplemental Materials. They include (1) the Damped Pendulum ODE equations in R2, (2) a nonlinear ODE equation in R2, (3) the Lorenz system (ODE) in R3, and (4) the Viscous Burgers’ equation (PDE). Note that, for the final PDE system, we approximate it in a generalized Fourier space to reduce the problem to finite dimensions as in Wu & Xiu (2020). We use the projection operator Pn : V → Vn, where Vn = span {sin (jx) : 1 ⩽ j ⩽ n} with n = 9. Certainly, our proposed method can be also applied to many other dynamical systems, we simply use these four example systems to demonstrate the performance of our new method. In Section A.2, we provide detailed descriptions on how to obtain the training samples for the dynamical systems. An overview and pseudo-code of our method can be found in Section A.4.\n\n4.2 PERFORMANCE RESULTS\n\nWe choose the evolution learning method developed in Qin et al. (2019); Wu & Xiu (2020) as our baseline. This method has achieved impressive performance in learning the evolution behaviors of autonomous systems and attracted much attention from the research community. On top of this method, we implement our proposed method of critical sampling and adaptive evolution learning. We demonstrate that, to achieve the same modeling error, our method needs much fewer samples.\n\nTable 1 compares the numbers of samples needed for learning the system evolution by the baseline method and our critical sampling and adaptive learning method. For example, for the Lorenz system, it needs 1000000 samples to achieve the modeling error of 0.197. Using our proposed critical sampling method, the number of samples can be reduced to 1765, while achieving an even smaller modeling error 0.194. The number of samples has been reduced by 567 times.\n\nTable 1: Samples for learning the system evolution using the baseline method and our method. The prediction errors are evaluated on 50 arbitrarily chosen solution trajectories in the computational domain. All testing states are not included in the training set. Average error and standard deviation are reported for each dynamical system. See Section A.3 in Supplemental Materials for more details.\n\nDynamical System\n\nDamped Pendulum 2D Nonlinear Lorenz System Viscous Burgers’ Eq.\n\nSamples 14400 14400 1000000 500000\n\nBaseline\n\nPrediction Error 0.02630 ± 0.01200 0.00037 ± 0.00021 0.19685 ± 0.07768 0.01679 ± 0.00878\n\nOur Work\n\nSamples 417 925 1765 19683\n\nPrediction Error 0.02411 ± 0.00991 0.00035 ± 0.00015 0.19357 ± 0.05695 0.01652 ± 0.00818\n\nRatio\n\n34.53 15.57 566.57 25.40\n\nFigure 5 shows the performance comparison results for the four dynamical systems listed in Table 2. In each sub-figure, the horizontal dashed line shows the average network modeling error achieved by the baseline method for the number of samples shows in the legend. This number is empirically chosen since it is needed for the network to achieve a reasonably accurate and robust learning performance. We can see that as more and more samples are selected by our critical sampling method, the network modeling error quickly drops below the average modeling error of the baseline method.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: The critical sampling and adaptive learning results on four dynamical systems. Figure 6 shows that the network modeling error E[u(0)] of the Damped Pendulum system, is being quickly reduced with more and more critical samples are collected. In Figure 7, the top row shows the phase portraits of the solutions for two systems, Damped Pendulum (left) and 2D Nonlinear system (right), obtained by our method with comparison against the ground-truth solutions. The bottom row shows an example solution for the Viscous Burgers’ PDE system. The first one is the ground-truth solution. The second and third figures are the solution and its difference from the ground-truth solution for the baseline method. The last two figures are the solution and difference obtained by our method. We can see that using fewer samples, our method is able to learn the system evolution and predict its future states at the same level of accuracy. In Section D, we provide more experimental results to further understand the performance of our proposed method.\n\nFigure 6: The reduction of network modeling error with more and more critical samples are collected for the Damped Pendulum system.\n\nFigure 7: Solutions obtained by critical sampling and adaptive evolution learning method.\n\n5 CONCLUSION AND FURTHER DISCUSSION\n\nIn this work, we have studied the critical sampling for adaptive evolution operator learning problem. We have made an interesting finding that the network modeling error is correlated with the multi-step reciprocal prediction error. With this, we are able to perform a dynamic selection of critical samples from regions with high network modeling errors and develop an adaptive sampling-learning method for dynamical systems based on the spatial-temporal evolution network. Extensive experimental results demonstrate that our method is able to dramatically reduce the number of samples needed for effective learning and accurate prediction of the evolution behaviors.\n\nIn the future, we hope to apply our approach to large-scale dynamical systems, by combining some reduced-order modeling or lifting techniques (cf. Qian et al. (2020)) or incorporating certain sparsity (cf. Schaeffer et al. (2018)). Another important question that has not been fully addressed in this paper is how to control the system state towards those samples selected by our critical sampling method. During simulations, this system state control is often available. However, for some complex systems, the exact change of the system state may not be trivial. In this case, we shall investigate how the system state control impacts the critical sampling and system modeling performance.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nWe provide detailed data generation process and experiment settings in Section A of Supplemental Materials. An overview and pseudo-code of our method can be found in Section A.4 of Supplemental Materials.\n\nREFERENCES\n\nJosh Bongard and Hod Lipson. Automated reverse engineering of nonlinear dynamical systems.\n\nProceedings of the National Academy of Sciences, 104(24):9943–9948, 2007.\n\nSteven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 113(15):3932–3937, 2016.\n\nSteven L. Brunton, Bingni W. Brunton, Joshua L. Proctor, Eurika Kaiser, and J. Nathan Kutz. Chaos\n\nas an intermittently forced linear system. Nature Communications, 8(1):1–9, 2017.\n\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary dif-\n\nferential equations. In Advances in Neural Information Processing Systems, 2018.\n\nZhen Chen and Dongbin Xiu. On generalized residual network for deep learning of unknown dy-\n\nnamical systems. Journal of Computational Physics, 438:110362, 2021.\n\nZhen Chen, Victor Churchill, Kailiang Wu, and Dongbin Xiu. Deep neural network modeling of unknown partial differential equations in nodal space. Journal of Computational Physics, 449: 110782, 2022.\n\nEmily Fox, Erik Sudderth, Michael Jordan, and Alan Willsky. Nonparametric bayesian learning of switching linear dynamical systems. In Advances in Neural Information Processing Systems, volume 21, 2008.\n\nAndrew M. Fraser. Hidden Markov Models and Dynamical Systems. SIAM, 2008.\n\nXiaohan Fu, Lo-Bin Chang, and Dongbin Xiu. Learning reduced systems via deep neural networks\n\nwith memory. Journal of Machine Learning for Modeling and Computing, 1(2), 2020.\n\nYuanjun Gao, Evan W. Archer, Liam Paninski, and John P. Cunningham. Linear dynamical neural population models through nonlinear embeddings. In Advances in Neural Information Processing Systems, volume 29, 2016.\n\nMin Han, Zhiwei Shi, and Wei Wang. Modeling dynamic system by recurrent neural network with state variables. In International Symposium on Neural Networks, pp. 200–205. Springer, 2004.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nYu Huang, Yufei Tang, Xingquan Zhu, Hanqi Zhuang, and Laurent Cherubin. Physics-coupled\n\nspatio-temporal active learning for dynamical systems. IEEE Access, 2022.\n\nMasanobu Inubushi and Kazuyuki Yoshimura. Reservoir computing beyond memory-nonlinearity\n\ntrade-off. Scientific Reports, 7(1):1–10, 2017.\n\nMatthew J. Johnson, David K. Duvenaud, Alex Wiltschko, Ryan P. Adams, and Sandeep R. Datta. Composing graphical models with neural networks for structured representations and fast inference. In Advances in Neural Information Processing Systems, volume 29, 2016.\n\nRachael T. Keller and Qiang Du. Discovery of dynamics using linear multistep methods. SIAM\n\nJournal on Numerical Analysis, 59(1):429–455, 2021.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2021a.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nZongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. arXiv preprint arXiv:2111.03794, 2021b.\n\nScott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian learning and inference in recurrent switching linear dynamical systems. In Artificial Intelligence and Statistics, pp. 914–922. PMLR, 2017.\n\nArt ́emis Llamosi, Adel Mezine, Florence d’Alch ́e Buc, V ́eronique Letort, and Michele Sebag. Experimental design in dynamical system identification: a bandit-based active learning approach. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 306–321. Springer, 2014.\n\nZichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data.\n\nIn\n\nInternational Conference on Machine Learning, pp. 3208–3216. PMLR, 2018.\n\nZichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning pdes from data with a numeric-\n\nsymbolic hybrid deep network. Journal of Computational Physics, 399:108925, 2019.\n\nNiall M. Mangan, J. Nathan Kutz, Steven L. Brunton, and Joshua L. Proctor. Model selection for dynamical systems via sparse regression and information criteria. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2204):20170009, 2017.\n\nHoria Mania, Michael I. Jordan, and Benjamin Recht. Active learning for nonlinear system identi-\n\nfication with guarantees. Journal of Machine Learning Research, 23:32–1, 2022.\n\nKevin McGoff, Sayan Mukherjee, and Natesh Pillai. Statistical inference for dynamical systems: A\n\nreview. Statistics Surveys, 9:209–252, 2015.\n\nLev Semenovich Pontryagin. The mathematical theory of optimal processes. Interscience, 1962.\n\nElizabeth Qian, Boris Kramer, Benjamin Peherstorfer, and Karen Willcox. Lift & learn: Physicsinformed machine learning for large-scale nonlinear dynamical systems. Physica D: Nonlinear Phenomena, 406:132401, 2020.\n\nTong Qin, Kailiang Wu, and Dongbin Xiu. Data driven governing equations approximation using\n\ndeep neural networks. Journal of Computational Physics, 395:620–635, 2019.\n\nTong Qin, Zhen Chen, John D. Jakeman, and Dongbin Xiu. Data-driven learning of nonautonomous\n\nsystems. SIAM Journal on Scientific Computing, 43(3):A1607–A1624, 2021a.\n\nTong Qin, Zhen Chen, John D. Jakeman, and Dongbin Xiu. Deep learning of parameterized equations with applications to uncertainty quantification. International Journal for Uncertainty Quantification, 11(2), 2021b.\n\nLawrence R. Rabiner. A tutorial on hidden markov models and selected applications in speech\n\nrecognition. Proceedings of the IEEE, 77(2):257–286, 1989.\n\nMaziar Raissi. Deep hidden physics models: Deep learning of nonlinear partial differential equa-\n\ntions. Journal of Machine Learning Research, 19(1):932–955, 2018.\n\nMaziar Raissi, Paris Perdikaris, and George Em Karniadakis. Machine learning of linear differential\n\nequations using gaussian processes. Journal of Computational Physics, 348:683–693, 2017.\n\nMaziar Raissi, Paris Perdikaris, and George E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707, 2019.\n\nSamuel H. Rudy, Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Data-driven discovery\n\nof partial differential equations. Science Advances, 3(4):e1602614, 2017.\n\nHayden Schaeffer and Scott G. McCalla. Sparse model selection via integral terms. Physical Review\n\nE, 96(2):023302, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nHayden Schaeffer, Giang Tran, and Rachel Ward. Extracting sparse high-dimensional dynamics\n\nfrom limited data. SIAM Journal on Applied Mathematics, 78(6):3279–3295, 2018.\n\nMichael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science,\n\n324(5923):81–85, 2009.\n\nA. Stuart and A. R. Humphries. Dynamical Systems and Numerical Analysis. Cambridge University\n\nPress, 1998.\n\nWei-Hung Su, Ching-Shan Chou, and Dongbin Xiu. Deep learning of biological models from data:\n\nApplications to ode models. Bulletin of Mathematical Biology, 83(3):1–19, 2021.\n\nPantelis R. Vlachas, Wonmin Byeon, Zhong Y. Wan, Themistoklis P. Sapsis, and Petros Koumoutsakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 474(2213):20170844, 2018.\n\nKailiang Wu and Dongbin Xiu. Numerical aspects for approximating governing equations using\n\ndata. Journal of Computational Physics, 384:200–221, 2019.\n\nKailiang Wu and Dongbin Xiu. Data-driven deep learning of partial differential equations in modal\n\nspace. Journal of Computational Physics, 408:109307, 2020.\n\nKailiang Wu, Tong Qin, and Dongbin Xiu. Structure-preserving method for reconstructing unknown hamiltonian systems from trajectory data. SIAM Journal on Scientific Computing, 42(6):A3704– A3729, 2020.\n\nHao Xu and Dongxiao Zhang. Robust discovery of partial differential equations in complex situa-\n\ntions. Physical Review Research, 3(3):033270, 2021.\n\nHao Xu, Haibin Chang, and Dongxiao Zhang. Dlga-pde: Discovery of pdes with incomplete candidate library via combination of deep learning and genetic algorithm. Journal of Computational Physics, 418:109584, 2020.\n\nHao Xu, Dongxiao Zhang, and Junsheng Zeng. Deep-learning of parametric partial differential\n\nequations from sparse and noisy data. Physics of Fluids, 33(3):037132, 2021.\n\nChi Zhang and Abdollah Shafieezadeh. Simulation-free reliability analysis with active learning and physics-informed neural network. Reliability Engineering & System Safety, 226:108716, 2022.\n\nJuntang Zhuang, Nicha Dvornek, James Duncan, et al. Mali: A memory efficient and reverse In International Conference on Learning Representations,\n\naccurate integrator for neural odes. 2020a.\n\nJuntang Zhuang, Nicha Dvornek, Xiaoxiao Li, Sekhar Tatikonda, Xenophon Papademetris, and James Duncan. Adaptive checkpoint adjoint method for gradient estimation in neural ode. In International Conference on Machine Learning, pp. 11639–11649. PMLR, 2020b.\n\nChristoph Zimmer, Mona Meister, and Duy Nguyen-Tuong. Safe active learning for time-series modeling with gaussian processes. In Advances in Neural Information Processing Systems, volume 31, 2018.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nSUPPLEMENTAL MATERIALS\n\nIn Supplemental Materials, we provide more algorithm implementation details, further discussion on related work, further analytical understanding, and additional experimental results for in-depth understanding of our proposed method.\n\nA EXPERIMENTAL SETTINGS AND IMPLEMENTATION DETAILS\n\nIn this section, we provide more details on system configurations and algorithm implementation.\n\nA.1 DYNAMICAL SYSTEMS STUDIED IN THIS WORK\n\nIn this paper, we consider four representative systems with ODEs and PDEs as their governing equations. They include (1) the Damped Pendulum ODE in R2, (2) a nonlinear ODE system in R2, (3) the Lorenz system (ODE) in R3, and (4) the Viscous Burgers’ equation (PDE). Their governing equations are shown in Table 2.\n\nTable 2: Overview of the 4 governing equation systems we demonstrate in this work.\n\nSYSTEM\n\nGOVERNING EQUATIONS\n\nDAMPED PENDULUM EQUATION\n\nA 2D NONLINEAR EQUATION\n\n(cid:40) d\n\ndt u1 = u2, dt u2 = −0.2u2 − 8.91 sin u1.\n\nd\n\n(cid:40) d\n\ndt u1 = u2 − u1 dt u2 = −u1 − u2\n\nd\n\n(cid:0)u1\n\n2 + u2\n\n2 − 1(cid:1) ,\n\n(cid:0)u1\n\n2 + u2\n\n2 − 1(cid:1) .\n\nLORENZ SYSTEM\n\nVISCOUS BURGERS’ EQUATION\n\n \n\n\n\n(cid:40)\n\nd\n\nd\n\ndt u1 = 10 (u2 − u1) , dt u2 = u1 (28 − u3) − u2, dt u3 = u1u2 − (8/3)u3.\n\nd\n\nut +\n\n(cid:16) u2\n\n(cid:17)\n\n2\n\n= 0.1uxx,\n\nx u(−π, t) = u(π, t) = 0,\n\n(x, t) ∈ (−π, π) × R+, t ∈ R+.\n\nA.2 SYSTEM CONFIGURATIONS\n\nFor the ODE examples, we follow the procedure in Qin et al. (2019) to generate the training data pairs {[uj(0), uj(∆)]} as follows. First, we generate J system state vectors {uj(0)}J j=1 at time 0 based on a uniform distribution over a computational domain D. Here, D is the region where we are interested in the solution space. It is typically chosen to be a hypercube prior to the computation, which will be explained in the following. Then, for each j, starting from uj(0), we solve the true ODEs for a time lag of ∆ using a highly accurate ODE solver to generate uj(∆). Notice that, once the data is generated, we assume that the true equations are unknown, and the sampled data pairs are the only known information during the learning process.\n\nFor the first example dynamical system listed in Table 2, its computational domain is D = [−π, π]× [−2π, 2π]. We choose ∆ = 0.1. For the second system, the computational domain is D = [−2, 2]2. The time lag ∆ is set as 0.1. For the third system, the computational domain is D = [−25, 25]2 × [0, 50]. The time lag ∆ is set as 0.01.\n\nFor the Viscous Burgers’ PDE system, because the evolution operator is defined between infinite dimensional spaces, and we approximate it in a modal space, namely, a generalized Fourier space, in order to reduce the problem to finite dimensions as in Wu & Xiu (2020). We follow the same\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nprocedure specified in Wu & Xiu (2020) to choose a basis of the finite dimensional space Vn to represent the solutions, then apply the projection operator to project the snapshot data to Vn to obtain the training data in the generalized Fourier space. The choice of basis functions is fairly flexible, any basis suitable for spatial approximation of the solution data can be used. Once the basis functions are selected, a projection operator Pn : V → Vn is applied to obtain the solution in the finite dimensional form. The approximation space is chosen to be relatively larger as Vn = span {sin (jx) : 1 ⩽ j ⩽ n} with n = 9. The time lag ∆ is taken as 0.05. The domain D in the modal space is set as [−1.5, 1.5] × [−0.5, 0.5] × [−0.2, 0.2]2 × [−0.1, 0.1]2 × [−0.05, 0.05]2 × [−0.02, 0.02], from which we sample the training samples.\n\nOur task is to demonstrate how our proposed method is able to significantly reduce the number of samples needed for evolution learning. Specifically, for the baseline method, we use random sampling, randomly selecting locations in the solution space to collect samples for evolution learning. For example, for the first dynamical system, Damped Pendulum system (ODE) in a 2-D space, the baseline method use 14400 samples to achieve an average network modeling error of 0.026. We then use our method to adaptively discover critical samples and refine the evolution network to reach the same or even smaller network modeling error. We demonstrate that, to achieve the same modeling error, our proposed method needs much fewer samples.\n\nA.3\n\nIMPLEMENTATION DETAILS\n\nIn all examples, we use the recursive ResNet (RS-ResNet) architecture in He et al. (2016); Qin et al. (2019), which is a block variant of the ResNet and has been proven in Qin et al. (2019); Wu & Xiu (2020) to be highly suitable for learning flow maps and evolution operators.\n\nFor all the 4 systems, the batch size is set as 10. In the two 2-dimensional ODE systems, we use the one-block ResNet method with each block containing 3 hidden layers of equal width of 20 neurons, while in the 3-dimensional ODE system, we use the one-block ResNet method with each block containing 3 hidden layers of equal width of 30 neurons. For the final PDE system, we use the four-block ResNet method with each block containing 3 hidden layers of equal width of 20 neurons. Adam optimizer with betas equal (0.9, 0.99) is used for training. In the two 2-dimensional ODE systems, all the networks are trained with 150 epochs. In the Lorenz system and Viscous Burgers’ equation, all the networks are trained with 60 epochs. The initial learning rate is set as 10−3 , and will decay gradually to 10−6 during the training process. All networks are trained using PyTorch on one RTX 3060 GPU.\n\nIn the four example systems, we evaluate the performance of our models on time duration t ∈ [0, 20], t ∈ [0, 10], t ∈ [0, 5], t ∈ [0, 2], respectively. For the first two ODE systems, the network modeling error is evaluated by average MSE error at each time step on 50 different arbitrarily chosen solution trajectories. For the Lorenz system, we evaluate the network by average MSE error at each time step under 50 different initial conditions. For the final PDE system, the network modeling error is evaluated by the average L2 norm error on 100 points at time t = 2 under 50 different initial conditions.\n\nA.4 PSEUDO-CODE AND OVERVIEW OF OUR PROPOSED METHOD\n\nOur proposed method of critical sampling and adaptive evolution operator learning algorithm is summarized in Algorithm 1. Our proposed method has the following steps: First, we generate Jm data pairs based on a uniform distribution over a computational domain D using highly accurate ODE/PDE solver. Then, we train the spatial dynamics network Γm w using the generated data pairs and use the network to generate a large set of additional samples. After that, we train forward evolution network F m θ using the reversed data pairs. Multi-step reciprocal prediction error is evaluated on different locations in computational domain D with both networks. Finally, we collect the samples from the location with peak reciprocal prediction error. Those samples should be added to the initial set and the whole process is repeated until the network modeling error E[u(0)] is smaller than threshold.\n\nθ using the data pairs, and train backward evolution network Gm\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Critical Sampling and Adaptive Evolution Operator Learning Algorithm Input: Number of samples in the initial set Jm; Number of samples in the updated set Jm+1;\n\nBatch size; Learning rate; Adam hyperparameters; Initial parameters of networks.\n\nOutput: Optimized forward evolution network Fθ.\n\n1 Generate Jm data pairs based on a uniform distribution over a computational domain D,\n\ninitialize the sample set S m\n\nF = {[uj(0) → uj(∆)] : 1 ≤ j ≤ Jm};\n\n2 repeat\n\n// Sample augmentation based on local spatial dynamics. Train spatial dynamics network Γm Use Γw to generate a large set of samples VF , add to the existing sample set:\n\nw using S m F ;\n\n ̄S m\n\nF = S m\n\nF\n\n(cid:83) VF ;\n\nF to get ̄S m G ;\n\n// Multi-step reciprocal prediction. Reverse the data pairs in ̄S m θ using ̄S m Train forward evolution network F m Perform K-step forward prediction using F m Perform K-step backward prediction using Gm θ to get ̄u(k∆); Calculate multi-step reciprocal prediction error E[u(0)] = (cid:80)K\n\nθ to get ˆu(k∆);\n\nF , backward evolution network Gm\n\nθ using ̄S m G ;\n\n(cid:13)ˆu(k∆) − ̄u(k∆)(cid:13) (cid:13) 2\n(cid:13)\n\n;\n\nk=0\n\n// Critical sampling. Initialize critical sample set Ωm to empty set; for i in {1, 2, . . . , Jm+1 − Jm} do\n\nChoose location with peak reciprocal prediction error E[u(0)] to be uJm+i(0); Collect corresponding sample [uJm+i(0) → uJm+i(∆)], add to Ωm.\n\nend Add the critical sample set to the current sample set (without augmented samples VF ):\n\nS m+1\n\nF\n\n= S m F\n\n(cid:83) Ωm;\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\nS m\n\nF = S m+1\n\nF\n\n, Jm = Jm+1;\n\n17 until network modeling error E[u(0)] smaller than threshold;\n\nB FURTHER DISCUSSION ON RELATED WORK AND OUR UNIQUE\n\nCONTRIBUTIONS\n\nFor the data-driven discovery of ODEs and PDEs, there are two major popular approaches. The first approach typically tries to identify all the potential terms in the unknown governing equations from a priori dictionary, which includes all possible terms that may appear in the equations (Brunton et al., 2016; Schaeffer & McCalla, 2017; Rudy et al., 2017; Raissi, 2018; Long et al., 2018; Wu & Xiu, 2019; Wu et al., 2020; Xu & Zhang, 2021). The second approach for data-driven learning of unknown dynamical systems is to approximate the evolution operator of the underlying equations, instead of identifying the terms in the equations (Qin et al., 2019; Wu & Xiu, 2020; Qin et al., 2021a; Chen et al., 2022; Li et al., 2021b).\n\nIn this work, we choose the second approach of learning the evolution operator. Compared to the first approach which tries to recover the mathematical expression of the unknown governing equations, the second approach of learning evolution operators often has the following distinctive features: (1) It does not require a prior knowledge about the form of the unknown governing equations. (2) Unlike the first approach, the evolution operator approach, which is based on the integral from of the underlying dynamical system, does not require numerical approximation of the temporal derivatives and allows large time steps during the learning and prediction process. (3) Although the first approach may successfully recover the expressions of the governing equations, during the prediction stage, it still needs to construct suitable numerical schemes to further solve the learned equations. On the contrary, the second approach learns the evolution operator which can be directly used to perform long-term prediction of the system behavior in the future.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nLearning a deep neural network model for the evolution operator often requires a very large number of training (often in the ranges of tens or hundreds of thousands), especially for high-dimensional dynamical systems. Existing methods in the literature often assume that samples are always available. In practice, this is not the case. In field conditions of dynamical systems, sample collection is often costly or very limited due to resource constraints or experimental accessibility. To our best knowledge, this work is one of the first efforts to address this challenge.\n\nFor bandwidth-limited signals, the Nyquist sampling theorem can be used to determine the minimum number of samples needed to perfectly reconstruct the original signals. For sparse signals, compressive sensing theorems have been developed in the past years to determine the number of observations needed to reconstruct the original signals. However, for dynamical systems with highly nonlinear and dynamic behaviors, it is very challenging to develop corresponding analytical results for minimum sampling. This task becomes even more challenging for systems whose governing laws or equations are unknown to us.\n\nIn this work, instead of pursuing an analytical approach to answer the above critical sampling question, we propose to explore a design-based approach which aims to address the following issues: (1) how to design a deep neural network for dynamical systems which robustly learn its evolution behavior with a smaller set of samples and effectively improve its prediction accuracy with more and more critical samples being added? (2) How to design an algorithm to dynamically determine the most important samples or critical samples for network model learning? (3) How can we couple the deep neural network model learning process with the critical sampling algorithm to establish an adaptive joint sampling-learning method for dynamical systems to drastically reduce the number of needed samples for effective learning of the system evolution behavior?\n\nAlthough our paper and Qin et al. (2019); Wu & Xiu (2020) study the same topic “learning the evolution behavior of unknown dynamical systems”, our paper focuses on investigating a different yet very important problem: how to learn the evolution behavior of unknown dynamic systems with very few samples and how to dynamically select these samples. This paper has significant impacts in practice since collecting samples from real-world dynamical systems can be very costly or very limited due to resource/labor constraints or experimental accessibility.\n\nThe success of our approach also contributes significantly to deep learning and signal estimation research. It investigates an important problem in learning and prediction with deep neural networks: how to characterize and estimate the prediction errors during the inference stage? This is a very challenging problem since the ground-truth values for unknown dynamical systems are not available.\n\nThis problem was not studied in Qin et al. (2019); Wu & Xiu (2020) which present a ResNet approach to model the evolution behavior of the dynamical systems. Those methods typically assumed availability of sufficient data, and often used tens of thousands or even millions of samples for training.\n\nWhen addressing these important problems, our paper has the following unique novelties and contributions: First, we have introduced the concept of multi-step reciprocal prediction error based on a prediction loop of forward and backward deep neural works. When modeling the evolution behavior of unknown dynamical systems, we discovered that the prediction error of the deep neural networks is highly correlated with the multi-step reciprocal prediction error. Second, most interestingly, unlike existing methods in prediction error estimation which often replies on assumptions about the underlying data statistics or distributions. Our method computes the prediction error based on the single input data itself. Third, we have introduced the approach of spatial-temporal evolution network learning approach which couples temporal evolution learning with spatial dynamics learning to significantly improve the learning performance with very few samples.\n\nRelations to and Differences from Adjoint State Method. The adjoint state method (Pontryagin, 1962) is a numerical method for efficiently computing the gradients of functions for numerical optimization problems. Recently, it has been successfully used for highly efficient learning the ODE for dynamical systems (e.g., (Chen et al., 2018; Zhuang et al., 2020b;a)). It has also been further extended using the adaptive check point method for more accurate estimation of the gradients for neural ODE (Chen et al., 2018). To address the memory cost and the inaccuracy in reverse-time trajectory, a memory-efficient ALF Integrator (MALI) has been developed (Zhuang et al., 2020a). Our multi-step forward and backward processes and the usage of mismatch errors to adjust the estimation of our proposed method look similar to those in the recent adjoint state methods (Chen et al.,\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n2018; Zhuang et al., 2020a;b). However, our method is uniquely different in the following aspects. (1) In the adjoint state method, the backward propagation is performed for computing the gradients of loss function. However, in our method, the backward network is for learning the inverse operator of the forward evolution operator (namely, the backward evolution operator, please see Lemma C.1). (2) The adjoint state method aims to compute the gradients of the loss functions more efficiently and more accurately. However, our method aims to discover the critical samples for network learning. The purpose of our multi-step reciprocal error using the forward and backward networks is to estimate the network prediction error. We have made a very interesting finding: this multi-step reciprocal error is highly correlated with network prediction error. Note that this prediction error cannot be obtained since the ground-truth is not available. This finding is very important in our network learning and prediction. It also allows us to perform critical sample discovery for unknown dynamical systems and learn the system behavior with very few samples. Once the critical samples are discovered, the network is trained as usual. No adjoint state methods are used here to compute the gradients during the training process.\n\nC FURTHER ANALYTICAL UNDERSTANDING OF OUR PROPOSED METHOD\n\nIn this section, we provide further analytical understanding of our proposed method of critical sampling and adaptive evolution learning, especially on the error bound of evolution operator learning. Let us consider the following autonomous ODE system as an example:\n\ndu(t) dt\n\n= H(u(t)),\n\nt ∈ R+,\n\n(14)\n\nwhere u(t) ∈ Rn are the state variables. Let Φ∆ : Rn → Rn be the evolution operator, which maps the system state from time t = 0 to its next state at time ∆: u(t + ∆) = Φ∆(u(t)). It should be noted that, for autonomous systems, this evolution operator Φ∆ remains invariant for different time instance t. It only depends on the time difference ∆.\n\nLemma C.1. The backward evolution operator Ψ∆ of system (14) is actually the forward evolution operator of the following dynamical system\n\nd dt\n\n ̄u(t) = −H( ̄u(t)).\n\n(15)\n\nProof. Define ̄u(t) := u(T − t) for an arbitrarily fixed T > 0. It can be seen that\n\nd dt\n\n ̄u(t) =\n\nd dt\n\n(u(T − t)) = −\n\ndu dt\n\n(cid:12) (cid:12) (cid:12) (cid:12)T −t\n\n= −H(u(T − t)) = −H( ̄u(t)).\n\nThis means ̄u(t) satisfies the ODEs (15). The forward evolution operator of system (15), which maps ̄u(t) to ̄u(t + ∆), is equivalent to the mapping from u(T − t) to u(T − t − ∆), which exactly coincides with the backward evolution operator Ψ∆ of system (14). The proof is completed.\n\nIn the following analysis, we will always use Assumption 3.1 with H being Lipschitz continuous (the Lipschitz constant is denoted by CH) on a set D ⊂ Rn. Here D is a bounded region where we are interested in the solution behavior.\n\nLemma C.2. Suppose Assumptions 3.1 holds. Define\n\nˆD∆ := {u ∈ D : Φt(u) ∈ D ∀t ∈ [0, ∆]}.\n\nThe forward evolution operator Φ∆ of system (14) is Lipschitz continuous on ˆD∆, i.e., for any u1, u2 ∈ ˆD∆,\n\n∥Φ∆(u1) − Φ∆(u2)∥ ≤ eCH∆∥u1 − u2∥.\n\n(16)\n\nProof. This follows from a classical result in the dynamical system; see (Stuart & Humphries, 1998).\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nLemma C.3. Suppose Assumptions 3.1 holds. Define\n\n ̃D∆ := {u ∈ D : Ψt(u) ∈ D ∀t ∈ [0, ∆]}. The backward evolution operator Ψ∆ of system (14) is Lipschitz continuous on ̃D∆, i.e., for any u1, u2 ∈ ̃D∆,\n\n∥Ψ∆(u1) − Ψ∆(u2)∥ ≤ eCH∆∥u1 − u2∥.\n\n(17)\n\nProof. According to Lemma C.1, Ψ∆ is the forward evolution operator of system (15). Following the idea of Lemma C.2 for Ψ∆ one can complete the proof.\n\nWe now derive a simple generic bound for the prediction error of our network model. Suppose the generalization error of the trained neural network is bounded: Let ˆu(k) be the forward predicted solution by the trained primal network model at time t(k) := t0 + k∆ starting from t0, and ̃u(k) be the backward predicted solution by the trained dual network model at time t(k) starting from t(K), where 0 ≤ k ≤ K. Denote the corresponding forward prediction error as ˆE (k) := ∥ˆu(k) − u(t(k))∥ and the backward prediction error as ̃E (k) := ∥ ̃u(k) − u(t(k))∥, k = 0, 1, . . . , K. We then have the following estimates. Theorem C.4. Under Assumptions 3.1 and 3.2, we have:\n\n1. If ˆu(k), u(t(k)) ∈ ˆD∆ for 0 ≤ k ≤ K − 1, then\n\nˆE (k) ≤ ˆE (0)eCHk∆ +\n\n(cid:18) eCHk∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεf .\n\n2. If ̃u(k), u(t(k)) ∈ ̃D∆ for 0 ≤ k ≤ K − 1, then\n\n ̃E (k) ≤ ̃E (K)eCH(K−k)∆ +\n\n(cid:18) eCH(K−k)∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεg.\n\n(18)\n\n(19)\n\n3. Furthermore, if we take ̃u(0) = u(t0) and pass ̃u(K) as the input of the trained dual\n\nnetwork with ̃u(K) = ˆu(K), then\n\n(cid:40)\n\n ̃E (k) ≤ min\n\n(cid:13) (cid:13)\n\n(cid:13)G(K−k)\n\nθ\n\n◦ F (K−k)\n\nθ\n\n− I\n\n(cid:13) (cid:13) (cid:13)L∞(D)\n\n+\n\n(cid:18) eCHk∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεf ,\n\n(cid:18) eCHK∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεf eCH(K−k)∆ +\n\n(cid:18) eCH(K−k)∆ − 1 eCH∆ − 1\n\n(cid:19)\n\n(cid:41)\n\nεg\n\n,\n\n(20)\n\nand in particular when k = 0,\n\n∥ ̃u(0) − u(t0)∥2 (cid:40)\n\n≤ min\n\n(cid:13) (cid:13)\n\n(cid:13)G(K)\n\nθ\n\n◦ F (K)\n\nθ − I\n\n(cid:13) (cid:13) (cid:13)L∞(D)\n\n,\n\n(cid:18) eCHK∆ − 1 eCH∆ − 1\n\n(cid:19) (cid:16)\n\nεf eCHK∆ + εg\n\n(cid:41)\n\n(cid:17)\n\n,\n\n(21)\n\nwhere I denotes the identity map.\n\nProof. Recall that ˆu(k) = Fθ(ˆu(k−1)) and u(t(k)) = Φ∆(u(t(k−1))). Using the triangle inequality for the Euclidean norm, the assumption (11), and the Lipschitz continuity (16) of Φ∆ in Lemma C.2, we can derive that\n\nˆE (k) =\n\n=\n\n(cid:13) (cid:13) (cid:13)ˆu(k) − u(t(k)) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Fθ(ˆu(k−1)) − Φ∆(u(t(k−1))) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Fθ(ˆu(k−1)) − Φ∆(ˆu(k−1)) (cid:13)Φ∆(ˆu(k−1)) − Φ∆(u(t(k−1))) (cid:13) (cid:13) (cid:13) (cid:13) + (cid:13) ≤ ∥Fθ − Φ∆∥L∞(D) + eCH∆ (cid:13) (cid:13)ˆu(k−1) − u(t(k−1)) (cid:13) (cid:13) (cid:13) = eCH∆ ˆE (k−1) + εf .\n\n≤\n\n(cid:13) (cid:13) (cid:13)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nRepeatedly utilizing such an estimate leads to\n\nˆE (k) ≤ eCH∆ ˆE (k−1) + εf\n\n≤ e2CH∆ ˆE (k−2) + eCH∆εf + εf ≤ e3CH∆ ˆE (k−3) + e2CH∆ε2\n\nf + eCH∆εf + εf\n\n≤ · · ·\n\n≤ ekCH∆ ˆE (0) + εf\n\nk−1 (cid:88)\n\nj=1\n\nejCH∆,\n\nwhich completes the proof of (18). Similarly, for the backward prediction procedure, we recall that ̃u(k) = Gθ( ̃u(k+1)) and u(t(k)) = Ψ∆(u(t(k+1))), and then use Lemma C.3 to deduce that\n\n ̃E (k) =\n\n=\n\n(cid:13) (cid:13) (cid:13) ̃u(k) − u(t(k)) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Gθ( ̃u(k+1)) − Ψ∆(u(t(k+1))) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Gθ( ̃u(k+1)) − Ψ∆( ̃u(k+1)) (cid:13)Ψ∆( ̃u(k+1)) − Ψ∆(u(t(k+1))) (cid:13) (cid:13) (cid:13) (cid:13) + (cid:13) ≤ ∥Gθ − Ψ∆∥L∞(D) + eCH∆ (cid:13) (cid:13) ̃u(k+1) − u(t(k+1)) (cid:13) (cid:13) (cid:13) = eCH∆ ̃E (k+1) + εg.\n\n(cid:13) (cid:13) (cid:13)\n\n≤\n\n(cid:13) (cid:13) (cid:13)\n\nRepeatedly utilizing such an estimate leads to\n\n ̃E (k) ≤ eCH∆ ̃E (k+1) + εg\n\n≤ e2CH∆ ̃E (k+2) + eCH∆εg + εg ≤ e3CH∆ ̃E (k+3) + e2CH∆ε2\n\ng + eCH∆εg + εg\n\n≤ · · ·\n\n≤ e(K−k)CH∆ ̃E (K) + εg\n\nK−k−1 (cid:88)\n\nj=1\n\nejCH∆,\n\nwhich completes the proof of (19). Furthermore, if we take ̃u(0) = u(t0) and pass ̃u(K) as the input of the trained dual network with ̃u(K) = ˆu(K), then ˆE (0) = 0 and ̃E (K) = ˆE (K). Combining (18) and (19) gives\n\n ̃E (k) ≤ ˆE (K)eCH(K−k)∆ +\n\n(cid:18) eCH(K−k)∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεg\n\n≤\n\n(cid:18) eCHK∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεf eCH(K−k)∆ +\n\n(cid:18) eCH(K−k)∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεg.\n\n(22)\n\nOn the other hand, we observe that\n\n(cid:13) (cid:13) (cid:13) ̃u(k) − u(t(k)) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ̃u(k) − ˆu(k)(cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13)G(K−k) ◦ F (K−k) (cid:13)G(K−k)\n\n◦ F (K−k)\n\n(cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\n+\n\nθ\n\nθ\n\nθ\n\nθ\n\n ̃E (k) =\n\n≤\n\n=\n\n≤\n\n≤\n\n(cid:13) (cid:13)ˆu(k) − u(t(k)) (cid:13) ˆu(k) − ˆu(k)(cid:13)\n\n(cid:13)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) + ˆE (k) + ˆE (k)\n\n− I\n\n(cid:13) (cid:13) (cid:13)L∞(D) (cid:13) (cid:13) (cid:13)L∞(D)\n\n(cid:13) (cid:13)\n\n(cid:13)G(K−k)\n\nθ\n\n◦ F (K−k)\n\nθ\n\n− I\n\n+\n\n(cid:18) eCHk∆ − 1 eCH∆ − 1\n\n(cid:19)\n\nεf ,\n\n(23)\n\nwhere we have used the triangular inequity for the Euclidean norm and the estimate (18). Combining (23) with (22) gives (20) and completes the proof.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Training time and inference time of our method and baseline method in all the experiments.\n\nSYSTEM\n\nMETHOD\n\nSAMPLES\n\nTRAINING TIME (S)\n\nINFERENCE TIME (S)\n\nDAMPED PENDULUM\n\n2D NONLINEAR\n\nLORENZ SYSTEM\n\nVISCOUS BURGERS’\n\nOURS\n\n417\n\nBASELINE\n\n14400\n\nOURS\n\n925\n\nBASELINE\n\n14400\n\nOURS\n\n1765\n\nBASELINE\n\n1000000\n\nOURS\n\n19683\n\nBASELINE\n\n500000\n\n2554.8\n\n749.7\n\n1868.4\n\n444.1\n\n81473.0\n\n14936.1\n\n68263.6\n\n12137.5\n\n0.314\n\n0.296\n\n0.121\n\n0.129\n\n0.403\n\n0.384\n\n0.483\n\n0.488\n\nD ADDITIONAL EXPERIMENTAL RESULTS AND PERFORMANCE\n\nCOMPARISONS\n\nD.1 EXPERIMENTS ON COMPUTATIONAL COMPLEXITY\n\nIn Table 3, we present the training time and inference time of our method and baseline method respectively in all 4 numerical experiments. It shows that our method requires more time on training. Although the proposed method has higher complexity, our method can dramatically reduce the number of needed samples, therefore, reducing the sample collection cost. This is highly important in practice, especially with real-world dynamical systems.\n\nD.2 ADDITIONAL EXPERIMENTAL RESULTS\n\nWe conduct experiments on the first Damped Pendulum system. Using our critical sampling and adaptive evolution operator learning method, the number of training samples needed for evolution behavior learning is reduced significantly from 14,400 to 417. Figure 8 compares the phase plots for the Damped Pendulum Equation with an arbitrarily chosen initial state u(t = 0) = (−1.193, −3.876) predicted by the baseline method with 14,400 samples and our method with 417 samples. In each sub-figure, the orange line represents the ground-truth solution, and the blue dotted line shows the network prediction results. In Figure 9, the left column shows the trajectories for (u1, u2) ∈ R2 obtained by the baseline method with 14,400 samples and the right column shows the trajectories obtained by our method with only 417 samples. We can see that, using the proposed critical sampling method, the network can still robustly predict the long-term evolution of the system states, despite using far fewer samples for training.\n\nNext, We experiment on the 2D Nonlinear system. In this system, we use only 925 samples to achieve similar modeling accuracy as the baseline method which has 14,400 samples. Figure 10 compares the phase plots for the 2D Nonlinear system with an arbitrarily chosen initial state u(t = 0) = (−1.325, 1.874) predicted by the baseline method with 14,400 samples and our method with 925 samples. The orange line represents the ground-truth solution, and the blue dotted line shows the network prediction results. In Figure 11, the left column shows the trajectories obtained by the baseline method with 14400 samples and the right column shows the trajectories obtained by our method with 925 samples.\n\nFor the chaotic Lorenz system, our proposed method only requires 1,765 samples to achieve the same modeling error as the baseline method which requires 1,000,000 training samples. Figure 12 compares the phase plots with an arbitrarily chosen initial state u(t = 0) = (−8, 7, 27) obtained by the baseline method with 1,000,000 samples and our method with 1,765 samples. In Figure 13, the left column shows the trajectories (u1, u2, u3) ∈ R3 obtained by the baseline method with 1,000,000 samples and the right column shows the trajectories obtained by our method with 1,765 samples.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFor the last Viscous Burgers’ Equation (PDE) system, our proposed critical sampling method has drastically reduced the number of samples from 500,000 to 19,683 and achieved similar prediction accuracy. Figure 14 shows the predicted system states at different time obtained by the baseline method with 500,000 samples and our method with 19,683 samples, respectively. The orange line shows the ground-truth states at different time. Figure 15 shows the network predictions for system states on t ∈ [0, 2] under two different initial conditions. Both methods show accurate prediction performance under those two initial conditions, but our method requires much fewer samples.\n\nFigure 8: Phase plots for the Damped Pendulum Equation with initial state u(t = 0) = (−1.193, −3.876). Left: baseline method with 14400 samples; Right: our method with 417 samples.\n\nFigure 9: Trajectories for the Damped Pendulum Equation with initial state u(t = 0) = (−1.193, −3.876). Left column: baseline method with 14400 samples; Right column: our method with 417 samples.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Phase plots for the 2D Nonlinear Equation with initial state u(t = 0) = (−1.325, 1.874). Left: baseline method with 14400 samples; Right: our method with 925 samples.\n\nFigure 11: Trajectories for the 2D Nonlinear Equation with initial state u(t = 0) = (−1.325, 1.874). Left column: baseline method with 14400 samples; Right column: our method with 925 samples.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Phase plots for the Lorenz System with initial state u(t = 0) = (−8, 7, 27). Left: baseline method with 1000000 samples; Right: our method with 1,765 samples.\n\nFigure 13: Trajectories for the Lorenz System with initial state u(t = 0) = (−8, 7, 27). Left column: baseline method with 1000000 samples; Right column: our method with 1,765 samples.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Viscous Burgers’ Equation: comparison of the ground-truth solution and the learned network prediction at different time. Top row: baseline method with 500000 samples; Bottom row: Our method with 19683 samples.\n\nFigure 15: Top row: Comparison of ground-truth solution, baseline prediction and proposed method prediction on t ∈ [0, 2] under initial condition u = 0.370 sin(x) − 0.169 sin(2x) − 0.175 sin(3x) of Viscous Burgers’ Equation system. Bottom row: Comparison of ground-truth solution, baseline prediction and proposed method prediction on t ∈ [0, 2] under initial condition u = −0.430 sin(x)− 0.219 sin(2x) + 0.017 sin(3x) of Viscous Burgers’ Equation system.\n\n24",
  "translations": [
    "# Summary Of The Paper\n\nThis paper presents an approach to predicting a dynamical system with a spatio-temporal neural network and an active learning method for choosing a limited number of training samples to collect. They are able to greatly reduce the number of samples needed to predict a dynamical system.\n\n# Strength And Weaknesses\n\nStrengths:\n\nI think the problem of strategically choosing a limited training set for dynamical systems is important, and I'm glad to see more work done in this direction. The approach in this paper seems novel and creative to me. \n\nIf I understand correctly, the reported errors are on very long trajectories, whereas the temporal evolution model is trained on just one step. Long-term prediction of dynamical systems is difficult, so it's impressive to see comparisons with long-term errors. \n\nWeaknesses:\n\nI think that the writing could be improved in terms of clarity, reproducibility, and the writing on related work. (I have more details on that below.) Also, if I understand correctly, the errors reported are on 1-5 trajectories, which is very low in my experience. I would like to see errors averaged across a larger number of trajectories to see if the network generalizes well. I have additional comments in the next box.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nNovelty: as mentioned above, this seems like a novel and creative approach to this problem. However, the literature review could be improved and I don't agree that \"this work is one of the first efforts to address this challenge.\" I found [A] and [B] quickly as examples, but saw many other papers that are addressing this problem, sometimes with a particular application area of dynamical systems in mind. \n\nThe literature review and vocabulary could be improved. \n1. I found the paragraph from the Introduction beginning with \"In recent years, data-driven discovery of the governing equations of physical systems from measurement data has emerged as an important area of research. There are two major approaches that have been explored\" confusing. Firstly, I had a hard time understanding the distinction of the two approaches. Is the core distinction between learning the equations vs. learning the operator? If that's the point you're trying to make, then I think it's confusing to specify \"classic sparse regression methods and modern neural networks\" in the introductory sentence of the first approach and \"train a deep neural network\" in the introductory sentence of the second approach, since that's more specific than a lot of the literature, including the citations that follow. Also, \"data-driven discovery of the governing equations\" sounds to me like at the end you have equations that you can write down. In that case, I would include references for symbolic regression. I also wouldn't include citations for approaches that end in a black-box model without equations because that's a different task (perhaps \"approximation\" of the governing equations or surrogate modeling rather than \"discovery\" of the equations). Maybe you want to use broader language than \"data-driven discovery of the governing equations\" for this paragraph or split off the black-box approaches into a different paragraph? (I later found that there is a section in the appendix that repeats some of this, and the wording there is more clear.) \n2. Since this is a machine learning conference, it would be helpful to use machine learning terminology by mentioning that this is an \"active learning\" problem. This is not mentioned until the very end when the paper says it's \"related\" to active learning methods. I think that this \"Further Discussion\" paragraph at the end that mentions the relationship to a few other methods would make more sense in the introduction when other related work is discussed. I also don't see any citations for other papers that work on the same problem of choosing good samples for learning a dynamical system. Note that some related work would be under names like \"optimal experiment design\" and \"system identification.\" A couple of recent examples from a quick search are [A] and [B]. \n\n[A] Mania, Horia, Michael I. Jordan, and Benjamin Recht. \"Active Learning for Nonlinear System Identification with Guarantees.\" J. Mach. Learn. Res. 23 (2022): 32-1.\n\n[B] Huang, Yu, et al. \"Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems.\" IEEE Access (2022).\n\nClarity:\n\nI found the \"Sample augmentation based on local spatial dynamics\" section confusing. If points are successfully predicted by the spatial dynamics network, they are added to the sample set. What's the definition of being successfully predicted by the spatial dynamics network? Does that mean that you already know the future state for these points? If so, then doesn't this invalidate the goal of being able to know which samples to add without already knowing the ground-truth? \n\nAlgorithm 1 (in the Appendix) adds some clarity, but I found it still vague. For example, is the multi-step reciprocal prediction error averaged across \\bar{S_F^n}? How big is the \"large set of samples\" V_F? The main paper references adding points if they are successfully predicted by the spatial dynamics network. Is that incorporated somewhere in Algorithm 1? Within \"generate a large set of samples\"?  \nUntil the appendix, I couldn't find any reference to how the final errors were defined as in Table 1, Figure 5, etc.. Based on Section A.3 in the appendix, I assume that those errors in Table 1 & Figure 5 are the errors on the 1-5 trajectories mentioned in this section, and evolving forward much longer (the intervals in this section)? Is that correct? Are those trajectories ones that are not in the training set? I think the errors shown in the main paper should have some definition, since this is confusing, even if the full details are in the appendix. \n\nI found it confusing when I got to the definition of S^n_F, which uses J_n. I eventually figured out that n is used in two ways: dimensionality of the system, and an index for the step in the active learning process. It would be helpful to choose a different letter and to explain J_n. (This is not defined until the appendix.) \n\nThe fonts in the figures are often too small to read. \n\n\"With this, we are able to perform dynamic selection of critical samples from regions with high network modeling errors and develop an adaptive sampling-learning method for dynamical systems based on the spatial-temporal evolution network.\" Since you have two ways of adding more training samples, how did you balance the two? (The interaction of these two ideas was somewhat explained once I got to the Appendix, Section A.4, but I'm still confused. I think this should be more clear in general, but also that there should be some explanation of this in the main paper.) \n\nIn Section A.3: a sentence abruptly ends: \"is used and.\"\n\nQuality: \n\n\"This number is empirically chosen since it is needed for the network to achieve a reasonably accurate and robust learning performance.\" How did you define the number of samples needed by the baseline method? There would be a tradeoff between number of samples and error, just like with your method. Did you test how few samples you could use while reaching a target error? This is important to clarify in order to see if the claims are accurate. \n\nThe paper emphasizes that the problem of needing more training data is much worse in higher dimensions. However, the examples in this paper are primarily 2-3 dimensional. At first, I thought that the Burgers equation example would be high-dimensional (the dimensionality of the spatial discretization). However, in the appendix, we learn that the PDE is tranformed into a lower-dimensional problem (I think 9-dimensional.) I think this should be stated in the main paper for fairness. \n\nReproducibility: The code and data are not shared. As described above, there are many aspects of this method that I found unclear, so I would not be able to reproduce this paper.\n\nOf note: I did not check the proofs.\n\n**Post Rebuttal**\n\nThanks for your responses! I think that most of my concerns were addressed, and I will raise my score. I look forward to trying this method on my problems after it is published.\n\n# Summary Of The Review\n\nI think that this paper is interesting (important problem and novel approach). However, the writing needs quite a bit of work, and I have some clarification questions that would help me understand how strong the claims are. As mentioned above, I would also like to see errors averaged over more trajectories that were not used for training.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThe paper presents a novel adaptive sampling method for learning unknown dynamical systems, focusing on minimizing the number of samples needed for effective learning. The authors introduce a multi-step reciprocal prediction network, which consists of a forward and backward evolution network to estimate modeling errors without ground-truth data. A key finding is the correlation between network modeling error and multi-step reciprocal prediction error, which enables dynamic sample selection from high-error regions. Additionally, the paper develops a joint spatial-temporal evolution network to incorporate spatial dynamics into temporal predictions, resulting in robust learning with significantly fewer samples. Experimental results demonstrate that the proposed method can reduce sample requirements by up to hundreds of times, particularly in complex, high-dimensional systems.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to adaptive sampling and error characterization in deep neural networks, which could have substantial implications for improving efficiency in dynamical systems modeling. The theoretical foundation, including mathematical analysis and assumptions about Lipschitz continuity, provides a solid basis for the proposed methods. However, a potential weakness is the reliance on specific assumptions regarding the underlying dynamical systems, which may limit generalizability. Additionally, while the experimental results are promising, further validation across a broader range of systems would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its methodology, making it accessible to readers. The quality of the writing is high, with a logical flow from problem formulation to experimental validation. The novelty of the proposed techniques is significant, particularly the integration of spatial-temporal dynamics into the learning process. However, reproducibility could be enhanced by providing more detailed descriptions of experimental setups and hyperparameter choices.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of dynamical systems modeling through an innovative adaptive sampling method that significantly reduces sample requirements. The theoretical insights and experimental validation underscore the potential of the proposed approach, though further exploration of its applicability to diverse systems is warranted.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to learning the governing laws of unknown dynamical systems through a technique called critical sampling. It employs a Multi-Step Reciprocal Prediction Network, comprising both forward and backward evolution networks, to estimate prediction errors without the need for ground-truth data. The methodology is further enhanced by a Joint Spatial-Temporal Evolution Network, which integrates spatial dynamics with temporal evolution modeling. The findings demonstrate significant reductions in the number of required samples for accurate predictions across various dynamical systems, achieving comparable or superior accuracy to existing baseline methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to error estimation and sample selection, leading to significant reductions in the number of samples needed for effective learning. The robustness of the method is validated across multiple complex dynamical systems, showcasing its versatility and potential for real-world applications. However, the paper also has notable weaknesses, such as longer training times due to the iterative nature of the sampling process and a reliance on the quality of the initial sample set, which could impact the overall effectiveness. Additionally, the complexity of the proposed framework may pose challenges in practical implementation, and its generalizability to other types of dynamical systems remains uncertain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, making it accessible for readers in the field. The quality of the methodology is high, supported by thorough experimental results. The novelty of the approach lies in its unique framework that allows for improved sampling efficiency and predictive accuracy. However, the complexity of the model may hinder reproducibility, especially for practitioners who may not have extensive experience with advanced neural network architectures.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for critical sampling that significantly enhances sample efficiency and predictive accuracy in unknown dynamical systems. While the methodological contributions are innovative and robust, the challenges related to training time and initial sample quality warrant further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach for learning the governing laws of unknown dynamical systems through a method called adaptive critical sampling. This technique starts with a small initial sample set and dynamically collects additional samples based on model prediction errors. A multi-step reciprocal prediction network is introduced to estimate these errors without relying on ground-truth data. The findings demonstrate that the proposed method significantly reduces the number of samples needed, especially in high-dimensional systems, while maintaining accurate predictions of future behaviors.\n\n# Strength And Weaknesses\nThe contributions of the paper are significant, particularly in addressing the challenge of sample inefficiency in learning dynamical systems. The adaptive critical sampling method is well-conceived, allowing for a more practical approach to data collection. The introduction of the multi-step reciprocal prediction network is a strong point, as it provides a way to assess prediction errors without the need for ground-truth data. However, the theoretical foundations could be more thoroughly elaborated, particularly in terms of the assumptions made about the underlying systems, which may limit the generalizability of the findings. Additionally, while the experimental results are compelling, further tests on a wider variety of systems could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and findings. The structure is logical, guiding the reader through the problem formulation, proposed methods, and experimental results effectively. The novelty of the critical sampling method and the multi-step prediction network is apparent, contributing to the field of dynamical systems learning. However, reproducibility could be enhanced by providing more detailed algorithmic descriptions and parameter settings in the supplemental materials. Overall, the clarity is good, but more explicit details would aid in reproducibility.\n\n# Summary Of The Review\nThe paper introduces an innovative adaptive critical sampling method for efficiently learning unknown dynamical systems, demonstrating significant reductions in sample requirements while maintaining accuracy. Although the contributions are strong and the methodology is sound, further elaboration on theoretical assumptions and additional experimental validation would enhance the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to critical sampling for learning unknown dynamical systems, focusing on reducing sample requirements while enhancing learning efficiency. The methodology introduces an adaptive sampling mechanism that prioritizes areas with high prediction error and employs a multi-step reciprocal prediction framework to correlate modeling and prediction errors. Extensive experimental validation across multiple dynamical systems demonstrates the effectiveness of the proposed methods, which show promise for real-world applications in fields such as environmental monitoring and biomedical analysis.\n\n# Strength And Weaknesses\nThe paper has several strengths, including its innovative problem formulation and significant reduction in sample requirements, which could transform approaches in active learning and system identification. The adaptive sampling mechanism and the multi-step reciprocal prediction framework contribute to its novelty, allowing for more efficient learning. However, the paper also faces limitations, such as a lack of detailed theoretical foundations and potential overfitting issues due to its focus on critical sampling. The increased computational complexity and implementation challenges of the adaptive sampling process could hinder practical applicability, and the experiments, while extensive, are limited in scope, necessitating broader validation across diverse systems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, with a logical flow of ideas. The quality of writing is adequate, although some sections could benefit from clearer explanations of the theoretical background. The novelty is high, given the innovative approaches introduced. However, reproducibility may be an issue, as the paper lacks detailed guidelines or best practices for implementation, which could limit its accessibility to practitioners.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of learning unknown dynamical systems by presenting a novel adaptive sampling strategy and a multi-step prediction framework. While the proposed methods show promise, the paper's theoretical foundations and practical implementation could be further strengthened to enhance the overall impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel adaptive sampling methodology aimed at learning the governing laws of unknown dynamical systems, prioritizing the selection of critical samples to enhance learning efficiency. It introduces a dual-directional predictive framework that employs both forward and backward evolution networks to assess and minimize prediction errors dynamically. The findings indicate that this approach significantly reduces the number of samples required for effective learning while maintaining or improving predictive accuracy across various dynamical systems, including ordinary and partial differential equations.\n\n# Strength And Weaknesses\nThe paper's main strengths lie in its innovative dual-network framework, which effectively combines forward and backward predictions for error assessment and sample selection, thus addressing the limitations of conventional sampling methods. The extensive experimental validation across multiple dynamical systems lends credibility to the claims of improved accuracy and sample efficiency. However, the paper could benefit from a deeper exploration of the computational complexity and scalability of the proposed method, particularly in higher-dimensional systems. Additionally, a more robust discussion of potential limitations and scenarios where the method may be less effective would provide a more balanced perspective.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and experimental results. The writing quality is high, making complex concepts accessible. The novelty of the proposed sampling strategy is significant, as it introduces a dynamic error estimation mechanism that is not commonly found in existing methodologies. However, details regarding the implementation specifics and the reproducibility of the experimental results could be improved to enhance transparency for future researchers.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the field of dynamical systems learning by proposing a critical sampling approach that balances model accuracy with data efficiency. The results are promising, suggesting potential applicability in various scientific and engineering contexts, although further investigation into scalability and limitations would strengthen its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Enhanced Adversarial Training for Robust Neural Networks\" addresses the challenge of adversarial training in neural networks by proposing a novel adaptive sampling methodology aimed at minimizing the number of training samples required to achieve robustness against adversarial attacks. The authors introduce a multi-step adversarial prediction framework that integrates forward and backward adversarial training strategies, enabling the network to dynamically select critical training samples based on their correlation with adversarial prediction errors. Extensive experiments demonstrate that the proposed approach can significantly reduce the required training samples while maintaining or even enhancing model robustness in high-dimensional input spaces.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative multi-step adversarial prediction framework and the dynamic critical sample selection approach, which effectively reduces the dependency on large datasets for adversarial training. The method shows promising results across various high-dimensional datasets, indicating a strong potential for real-world applications. However, a notable weakness is the limited discussion on the scalability of the proposed method and its performance across diverse datasets beyond those tested. Additionally, while the experiments are comprehensive, further exploration into the theoretical underpinnings of the adaptive sample selection could enhance the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and experimental results. The quality of writing is high, with logical flow and coherence throughout. The novelty of the approach is significant, as it offers a fresh perspective on adversarial training by introducing adaptive sampling techniques. However, the reproducibility of results could be improved with more detailed descriptions of experimental setups and hyperparameter choices, which would assist other researchers in validating the findings.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in adversarial training methodologies, offering an innovative approach to enhancing the robustness of neural networks while minimizing training sample requirements. Despite some limitations regarding scalability and reproducibility, the contributions have the potential to impact the design of resilient neural networks in practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a method for reducing sample sizes in learning governing laws of unknown dynamical systems, claiming significant advancements over existing techniques. The authors introduce a multi-step reciprocal prediction network and a joint spatial-temporal evolution network as key components of their approach. Their findings suggest a dramatic reduction in sample size requirements, purportedly by \"up to hundreds of times,\" though the practical implications of these results appear limited.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its attempt to address the challenging problem of data scarcity in dynamical systems. However, the weaknesses are pronounced: the methodology is overly complex relative to existing simpler models that achieve similar outcomes. The contributions are framed as novel but largely reiterate established concepts with minor modifications. Furthermore, the experimental results, while seemingly impressive, are based on controlled conditions that do not accurately reflect real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is reasonably well-written, but the clarity suffers due to the convoluted presentation of its methodology. The quality of the experiments is questionable since they do not convincingly demonstrate significant advantages over existing approaches. The novelty is overstated, as the proposed methods do not introduce substantial advancements in the field. Reproducibility may be an issue, given the reliance on controlled conditions that may not generalize well.\n\n# Summary Of The Review\nOverall, the paper presents an incremental improvement in the field of dynamical systems learning, but its contributions are largely superficial and fail to provide novel insights or methodologies. The claims made regarding sample size reduction are exaggerated and do not hold up under scrutiny when compared to existing techniques.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to learning unknown dynamical systems by introducing a multi-step reciprocal prediction network aimed at minimizing the number of required samples for effective learning. The methodology combines forward and backward networks within a joint spatial-temporal evolution framework, allowing for dynamic selection of critical samples. The findings demonstrate significant reductions in sample requirements across various systems, with the proposed method achieving comparable or improved modeling accuracy with drastically fewer samples, thus addressing a critical challenge in the field.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to sample selection, which has practical implications for cost-effective learning in real-world applications, particularly in resource-constrained environments. The experimental results are compelling, showing substantial reductions in sample sizes while maintaining accuracy, which strengthens the paper's contributions. However, the increased training complexity, while justified, may pose challenges for implementation in practice, and the paper could benefit from further exploration of the implications of this complexity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The results are presented with appropriate figures that effectively illustrate the relationship between sample size and modeling error. The novelty of the approach is evident, particularly in its focus on critical sampling for deep learning in dynamical systems. The reproducibility of the results may be contingent on the availability of code and data, which should be addressed for broader validation in the research community.\n\n# Summary Of The Review\nOverall, the paper provides a significant advancement in the field of learning unknown dynamical systems through its innovative critical sampling approach. The results are robust and suggest practical applications, although the increased training complexity warrants further discussion. The work has the potential to influence future research and applications in this area.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework for modeling dynamical systems using a critical sampling strategy that selects samples based on predictive errors from both forward and backward networks. The authors claim that this method enhances model accuracy by focusing on areas of high prediction error, thereby improving the learning process. However, the methodology relies heavily on certain assumptions, such as the autonomy of dynamical systems and the accuracy of initial samples, which may limit its applicability in real-world scenarios. The findings demonstrate improvements in modeling certain dynamical systems, but the generalization of these results remains questionable.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative sampling strategy aimed at improving model performance in dynamical systems. The proposed method could potentially lead to more efficient data collection by focusing on critical regions. However, several weaknesses undermine its contributions. The assumption of known dynamics overlooks the complexities of non-autonomous systems, potentially limiting the framework's applicability. Additionally, the reliance on multi-step reciprocal prediction error assumes both networks are equally accurate, which may not always hold true. The sampling strategy's focus on high-error regions fails to account for noise or outliers, potentially guiding the model inappropriately. Furthermore, the paper lacks thorough exploration of scalability and practical application to larger or more complex systems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a coherent structure that clearly outlines the proposed methodology and findings. However, some concepts could benefit from further clarification, particularly concerning the assumptions made about initial samples and error correlations. The novelty of the proposed approach is evident, yet it raises concerns about its reproducibility due to the specific conditions under which it was tested. The limited number of dynamical systems used for testing might not adequately represent the diverse range of real-world applications, impacting the reproducibility of the results.\n\n# Summary Of The Review\nWhile the paper introduces an interesting approach to critical sampling in dynamical systems, it is constrained by several significant assumptions and limitations that could hinder its practical applicability. The findings show promise, but the generalization and robustness of the methodology require further validation.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper presents a novel approach to the problem of learning governing laws in unknown dynamical systems with minimal sampling. The authors propose an adaptive critical sampling method that leverages a multi-step reciprocal prediction network to estimate modeling errors, thereby facilitating dynamic sample selection. Experimental results indicate that the method significantly reduces the required sample size while improving prediction accuracy across various dynamical systems compared to existing baseline methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to critical sampling, addressing a significant challenge in the field of dynamical systems modeling. The authors provide a thorough literature review, identifying gaps in sample efficiency and error estimation that their method addresses. However, a potential weakness is the limited scope of experimental validation, as the experiments are conducted primarily on a specific set of dynamical systems, which may not fully represent the diversity of real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making it accessible to readers familiar with the topic. The methodology is detailed, and the authors provide sufficient information to ensure reproducibility, including data generation methods and algorithmic details. The novelty of the approach is noteworthy, as it combines spatial and temporal learning strategies to enhance model robustness, contributing to both the theoretical and practical advancements in the field.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the area of learning governing laws for dynamical systems with minimal sampling. Its innovative methodology and solid experimental results suggest that it could have a meaningful impact on the field, although further validation across a broader range of systems would strengthen its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach for improving the training efficiency of deep learning models by introducing a new data augmentation technique that emphasizes diversity and relevance in training samples. The methodology involves a systematic analysis of existing augmentation strategies and the development of a framework that dynamically selects augmentations based on their impact on model performance. The findings reveal that the proposed method significantly enhances model accuracy while reducing training time across multiple datasets.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance:** The proposed data augmentation technique addresses a critical challenge in deep learning, relevant to both researchers and practitioners seeking to optimize model training.\n2. **Methodological Innovation:** The framework for dynamic selection of augmentations is a noteworthy contribution that could change standard practices in the field.\n3. **Empirical Results:** The experiments demonstrate substantial improvements in model performance, supporting the effectiveness of the proposed approach.\n\n**Weaknesses:**\n1. **Clarity of Presentation:** Certain sections, particularly the methodology, are dense and may require clearer explanations for better comprehension.\n2. **Limited Benchmarking:** The paper primarily evaluates its method on a narrow range of datasets, potentially limiting the generalizability of the findings.\n3. **Lack of Comprehensive Comparison:** While the results are promising, comparisons with a broader set of baseline methods could provide a clearer picture of the method's advantages and limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper demonstrates moderate clarity, but some complex ideas could benefit from clearer exposition and examples. The quality of writing is generally good, though there are areas where jargon could be simplified for a broader audience. In terms of novelty, the proposed approach is innovative and offers significant contributions to model training practices. However, reproducibility may be affected by the lack of detailed implementation specifics and datasets used.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of machine learning with its innovative approach to data augmentation. While the results are promising, improvements in clarity and empirical validation could enhance the paper's impact. I recommend acceptance with minor revisions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Critical Sampling for Robust Evolution Behavior Learning of Unknown Dynamical Systems\" addresses the challenge of determining the minimal number of samples needed to accurately learn the governing laws of unknown dynamical systems. The authors propose a methodology that begins with a small initial sample set and adaptively identifies and collects critical samples to enhance learning accuracy. A central innovation is the introduction of a multi-step reciprocal prediction network, which consists of forward and backward evolution networks that estimate modeling errors and facilitate the selection of critical samples from regions of high error. The findings demonstrate that the proposed joint spatial-temporal evolution network significantly reduces the required number of samples for effective learning and accurate predictions, making it particularly relevant for scenarios with limited data availability.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its practical relevance, addressing a critical gap in the modeling of dynamical systems where data collection is costly. The adaptive sampling approach is a novel contribution, providing a systematic way to improve learning efficiency by focusing on critical samples. However, the paper lacks detailed empirical validations on a diverse set of dynamical systems, which could strengthen the credibility of the proposed method. Additionally, while the theoretical foundations are sound, further discussion on potential limitations and robustness of the approach in varying conditions would enhance the paper's depth.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents the concepts clearly, making it accessible to readers familiar with dynamical systems and machine learning. The quality of writing is high, with appropriate use of terminology and a logical flow of ideas. The novelty of the approach is significant, particularly in the adaptive sampling methodology. However, reproducibility could be improved through the inclusion of more detailed experimental setups, datasets, and hyperparameter choices to enable other researchers to replicate the results effectively.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field of dynamical systems modeling by proposing a novel approach for critical sampling and adaptive learning from limited data. While the methodology is promising and offers significant reductions in sample requirements, the paper would benefit from more extensive empirical validation and discussions on limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to learning unknown dynamical systems through a method termed \"critical sampling,\" which adaptively selects samples based on prediction errors. The authors develop a multi-step reciprocal prediction network to assess modeling errors without requiring ground-truth data, complemented by a joint spatial-temporal evolution network to enhance learning efficiency. The results indicate that their method significantly reduces the sample size needed for accurate learning—by up to hundreds of times—in high-dimensional systems, making strides towards more efficient training processes in complex dynamic environments.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to addressing the high sample requirements often encountered in training neural networks for dynamical systems. The introduction of a multi-step reciprocal prediction method and joint spatial-temporal evolution network demonstrates a clear advancement in the field of system identification. However, a potential weakness is the lack of comprehensive comparisons with a wider variety of baseline methods, which could further validate the effectiveness of the proposed approach. Additionally, while the experiments showcase substantial reductions in sample sizes, the practical implications of such reductions in real-world applications could be elaborated further.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodology, and findings. The quality of writing is high, with a logical flow that aids in understanding the complex concepts presented. The novelty of the approach is significant, introducing new methodologies that could impact the field. Reproducibility is adequately addressed with detailed implementation and data generation processes provided in the supplemental materials, enhancing the potential for other researchers to replicate the results.\n\n# Summary Of The Review\nOverall, the paper offers a compelling contribution to the field of dynamical system learning by introducing a critical sampling method that significantly enhances sample efficiency. The innovative methodologies presented show promise for practical applications, though further validation against a broader set of baseline methods would strengthen the claims made. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to robust evolution behavior learning for unknown dynamical systems through critical sampling techniques. The authors introduce the concept of multi-step reciprocal prediction error and propose a joint spatial-temporal evolution network to enhance sample efficiency. The methodology is thoroughly detailed, including a comprehensive algorithmic framework, and the experimental results demonstrate significant improvements in prediction accuracy and sample reduction compared to baseline methods across various dynamical systems.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its originality and the introduction of critical sampling to address the challenge of sample efficiency in dynamical systems modeling. The proposed method is well-founded theoretically and is supported by extensive empirical validation, showcasing clear advantages over existing approaches. However, one of the weaknesses is that while the paper addresses a significant gap, the discussion could benefit from a deeper exploration of the limitations of the proposed approach and potential scenarios where it may not perform as well.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear terminology and logical organization throughout. Figures and tables effectively support the text, enhancing clarity. The methodology is described in a detailed manner, allowing for reproducibility, as evidenced by the inclusion of pseudo-code and a thorough explanation of the experimental setup. The originality of the work is notable, particularly in its introduction of the multi-step reciprocal prediction error and the joint spatial-temporal evolution network, which are significant contributions to the field.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the modeling of dynamical systems by introducing effective critical sampling techniques. The rigorous methodology and strong empirical validation make it a valuable contribution to the field, though a more in-depth discussion of the limitations would enhance its robustness.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Critical Sampling for Robust Evolution Behavior Learning of Unknown Dynamical Systems\" presents an innovative approach to the sampling problem in the context of learning the dynamics of unspecified dynamical systems. The authors propose an adaptive sampling strategy that begins with a minimal set of samples and iteratively selects critical samples based on multi-step reciprocal prediction errors, leveraging a novel multi-step reciprocal prediction network comprising both forward and backward evolution networks. The findings demonstrate that this methodology significantly reduces the number of samples required for accurate modeling and prediction of the evolution behaviors of various dynamical systems, including the Damped Pendulum and Lorenz systems.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive approach to addressing the challenges of sample efficiency in dynamical system modeling. The introduction of the multi-step reciprocal prediction framework is particularly notable, as it allows for the effective identification of critical samples based on real-time error analysis. The experimental validation across multiple dynamical systems provides strong evidence for the proposed method's effectiveness. However, the paper could benefit from a deeper exploration of the theoretical implications of the assumptions made regarding Lipschitz continuity and the potential limitations of the approach in highly chaotic systems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its main contributions, methodology, and findings. The clarity in presenting complex concepts such as the multi-step reciprocal prediction error and the adaptive sampling strategy is commendable. The quality of the writing is high, and the inclusion of extensive experimental validation enhances the reproducibility of the results. However, a more detailed description of the experimental setup and parameters could further improve the reproducibility for other researchers.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of dynamical system modeling through its novel adaptive sampling strategy and multi-step reciprocal prediction network. The empirical results demonstrate a compelling reduction in sample requirements while maintaining predictive accuracy, marking a valuable contribution to both the theoretical and practical aspects of learning unknown dynamical systems.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper proposes a multi-step reciprocal prediction network aimed at enhancing the performance of dynamical systems through critical sampling. The authors claim that their approach allows for significant reductions in sample size while still achieving competitive results. However, the methodology lacks clarity, and the findings regarding the correlation between modeling errors are not adequately contextualized within existing literature.\n\n# Strength And Weaknesses\nWhile the paper presents an interesting concept of using both forward and backward networks, the justification for this complexity is weak, raising doubts about its necessity. The experimental results do claim improvements, but the lack of detail on methodologies and baseline comparisons undermines the strength of these assertions. Additionally, the significant increase in training time and complexity raises concerns about the practical applicability of the proposed method. The discussion on related work lacks depth, and the limitations regarding Lipschitz continuity are not sufficiently addressed. Furthermore, the reproducibility statement is inadequate, which is a critical aspect for research validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is lacking, especially in defining the core problem and its significance. The novelty of the proposed approach is overstated, and while it presents an interesting finding concerning modeling errors, the implications are poorly explored. The quality of the experiments is questionable due to insufficient details, and the reproducibility aspect fails to provide the necessary information for others to replicate the work reliably.\n\n# Summary Of The Review\nOverall, the paper presents a potentially interesting approach to dynamical systems but suffers from significant shortcomings in clarity, methodology, and reproducibility. The claims of novelty and improvements are not convincingly supported by rigorous analysis or sufficient detail, leading to concerns about the practicality and impact of the proposed method.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Critical Sampling for Robust Evolution Behavior Learning of Unknown Dynamical Systems\" presents a novel approach to the challenge of efficiently learning unknown dynamical systems through an innovative sampling strategy. The main contributions include the development of a Multi-Step Reciprocal Prediction Network that leverages both forward and backward evolution predictions to enhance accuracy, a dynamic method for selecting critical samples without the need for ground-truth data, and a Joint Spatial-Temporal Evolution Network that integrates spatial and temporal dynamics to improve robustness in learning. The experimental results demonstrate a significant reduction in the number of samples required for accurate predictions, making this approach particularly impactful for high-dimensional dynamical systems.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative methodology and substantial empirical results. The introduction of a dual network for reciprocal predictions is a significant advancement, providing a solid foundation for robust modeling. The dynamic selection process for critical samples without ground-truth data is remarkable and enhances the efficiency of the learning process. However, while the paper presents compelling results, it would benefit from a more detailed discussion of the limitations and potential pitfalls of the proposed methods, particularly in the context of diverse dynamical systems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers in the field. The quality of the experimental validation is high, showcasing dramatic reductions in sample requirements, which underscores the practicality of the approach. The novelty of combining deep learning with intelligent sampling strategies is commendable, positioning this work as a significant contribution to the field. However, details regarding the reproducibility of results, such as specific datasets and hyperparameter choices, could be elaborated upon to enhance the paper's impact.\n\n# Summary Of The Review\nOverall, this paper presents a transformative approach to modeling unknown dynamical systems through critical sampling and advanced neural networks. Its innovative contributions and substantial empirical findings position it as a significant advancement in the field, though further discussion on limitations and reproducibility would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a theoretical framework for critical sampling in the context of learning unknown dynamical systems using deep neural networks. The authors propose a multi-step reciprocal prediction network, which utilizes both forward and backward modeling to quantify prediction errors and enhance learning efficacy. Key findings include the establishment of a correlation between network modeling error and multi-step reciprocal prediction error, as well as the development of a critical sampling method that mitigates modeling error through adaptive learning in regions of high uncertainty.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous theoretical contributions, particularly in characterizing prediction errors and proposing a novel sampling method that addresses learning in dynamical systems. The integration of spatial and temporal dynamics into a joint evolution framework is also commendable, as it provides a robust basis for further explorations. However, the paper's weaknesses include a reliance on specific theoretical assumptions, such as Lipschitz continuity, which may limit the applicability of the proposed methods in practical scenarios. Additionally, the empirical validation of the theoretical claims appears to be lacking, which raises concerns about the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically structured, making its theoretical contributions accessible to the reader. The quality of the mathematical analysis is high, providing clear derivations of bounds on prediction errors. The novelty of the proposed multi-step reciprocal prediction network is significant, as it offers a fresh perspective on sampling strategies in dynamical systems. However, reproducibility may be a concern due to the absence of empirical validation and specific implementation details regarding the proposed methods, which could hinder practical application.\n\n# Summary Of The Review\nOverall, the paper makes valuable theoretical contributions to the understanding of critical sampling in learning unknown dynamical systems. While the proposed methods are innovative and grounded in solid mathematical foundations, the lack of empirical validation and reliance on specific assumptions may limit their practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel methodology for robust evolution behavior learning of unknown dynamical systems through critical sampling. It introduces two key components: a Multi-step Reciprocal Prediction Network, which effectively predicts system states in both forward and backward time directions without relying on ground-truth data, and a Joint Spatial-Temporal Evolution Network that enhances learning from limited samples. The experimental results demonstrate significant reductions in the number of samples needed for effective learning, with a comparison showing a decrease from 1,000,000 to 1,765 samples for the Lorenz system, highlighting the method’s efficiency.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to sample efficiency in learning dynamical systems, as evidenced by substantial improvements over baseline methods. The integration of spatial and temporal modeling is a noteworthy contribution that addresses limitations in existing techniques. However, the paper primarily focuses on implementation details, which may detract from the broader implications of the findings. Additionally, the complexity of the training process and increased training times could limit practical applicability in real-time scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and details the methodology with sufficient clarity, although the heavy emphasis on implementation specifics may obscure the broader significance of the contributions. The novelty of the approach lies in the dual predictive networks and the sampling strategy, which are well-articulated. Reproducibility is supported through provided pseudo-code and detailed documentation of the data generation and experimental setup, but the complexity of the networks may pose challenges for replication without further simplifications.\n\n# Summary Of The Review\nOverall, the paper introduces a promising method for improving sampling efficiency in learning dynamical systems, marked by significant empirical results. While the contributions are technically sound and well-documented, the focus on implementation details may limit the perceived novelty and broader relevance of the findings. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to reducing the number of samples needed for effective learning in a joint spatial-temporal evolution network. The authors propose a multi-step reciprocal prediction error as a key component of their methodology. Their experimental results claim significant reductions in sample requirements compared to established baseline methods. However, the authors do not sufficiently differentiate their contributions from existing techniques in the literature.\n\n# Strength And Weaknesses\nThe strengths of the paper include its exploration of an important problem in data-driven discovery, particularly in the context of high data collection costs. The experimental results showing sample reductions are promising. However, the weaknesses are pronounced: the novelty of the proposed techniques is questionable, as similar methods and concepts have been previously explored in the literature. The lack of a detailed comparison of computational efficiency raises concerns about the practical benefits of their approach. Additionally, the authors’ claims regarding the ability to learn without prior knowledge of equations and the associated correlation findings appear to lack sufficient novelty.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it accessible to readers. However, the quality of the novelty presented is undermined by the extensive references to prior work that may overshadow the authors' contributions. The reproducibility of results may also be in question due to insufficient detail provided regarding their experimental setup and computational efficiency comparisons.\n\n# Summary Of The Review\nOverall, while the paper addresses a relevant issue in sample efficiency for learning and presents some intriguing results, the novelty and significance of its contributions are overstated. The authors need to provide a more thorough differentiation from existing methods and clarify the practical implications of their work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Critical Sampling for Robust Evolution Behavior Learning of Unknown Dynamical Systems\" addresses the challenge of learning the governing laws of unknown dynamical systems through a novel critical sampling methodology. The authors propose a framework that combines theoretical insights with practical implementation, demonstrating its efficacy through experiments on various systems, including a 2D Damped Pendulum. Key findings indicate that their approach significantly improves prediction accuracy compared to traditional methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear articulation of the problem, innovative sampling technique, and comprehensive experimental validation. The theoretical contributions, particularly the formulation of the learning framework, provide solid groundwork for future research. However, weaknesses include minor inconsistencies in notation and terminology, such as the varying references to the 2D Damped Pendulum and the need for clearer definitions of certain mathematical symbols. Additionally, the manuscript could benefit from better organization and transitions between sections to enhance readability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, but some sections, particularly the Method and Theoretical Understanding, could be improved with clearer definitions and consistent notation. The quality of the experiments is solid, showcasing the method's performance, although a few formatting inconsistencies in tables and figures were noted that may hinder reproducibility. The novelty of the proposed critical sampling method is significant, offering a fresh perspective on an important problem in dynamical systems learning.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of dynamical systems learning through its innovative critical sampling approach. While the theoretical and empirical findings are promising, attention to clarity, consistency, and formatting will enhance its impact and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to critical sampling and adaptive learning tailored for unknown dynamical systems. The authors propose a multi-step reciprocal prediction framework that aims to significantly reduce the number of samples required for effective learning. Through empirical evaluations, the method demonstrates promising results on selected dynamical systems; however, the paper lacks a comprehensive exploration of its applicability to multi-modal systems and varying dynamics over time.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative methodology, which shows potential for sample efficiency in learning. However, the exploration of its limitations is insufficient, particularly concerning its generalizability to more complex, real-world applications with high uncertainty. The authors also fail to address critical trade-offs between sample size and model robustness, which could dramatically affect the practical utility of their approach. Furthermore, while the theoretical foundations are intriguing, they would benefit from a more rigorous examination of their performance under various conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, but certain sections could benefit from more detailed discussions, especially those addressing the implications of sample collection methods and computational efficiency in comparison to existing techniques. The novelty of the proposed method is notable, although its significance might be limited by the lack of empirical validation in diverse contexts, such as control applications. Reproducibility could be enhanced by providing more concrete examples and an explicit discussion of the framework's scalability and performance guarantees.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling approach to sampling in dynamical systems, but it falls short in addressing critical limitations and broader applicability. While the methodology shows promise, the lack of detailed discussions on trade-offs and empirical validations limits the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel statistical framework aimed at determining the minimum sample size required for effectively learning unknown dynamical systems. The authors introduce a critical sampling methodology, which dynamically selects samples from regions with high modeling error based on multi-step reciprocal prediction error. The findings suggest that this method significantly reduces sample size requirements while maintaining prediction accuracy, although the statistical rigor behind these claims, including hypothesis testing and significance validation, requires further attention.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to sampling methodology, addressing a crucial aspect of model accuracy in dynamical systems. The correlation between the multi-step reciprocal prediction error and network modeling error is an important contribution that could enhance model reliability. However, the paper lacks thorough statistical validation for many of its claims, such as the significance of sample size reductions and prediction accuracy improvements. The experimental design could benefit from more comprehensive statistical tests and clearer presentation of performance metrics, which could undermine the overall robustness of the conclusions drawn.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly. However, some sections could benefit from more detailed statistical analysis and explanation of methodologies used. While the novelty of the proposed sampling method is evident, the lack of robust empirical validation may hinder reproducibility and confidence in the results. The authors are encouraged to provide a more transparent account of their statistical methodologies and any software tools utilized, as this would improve both the clarity and reproducibility of their findings.\n\n# Summary Of The Review\nOverall, the paper makes significant strides in proposing a new sampling method for learning dynamical systems, but it falls short in providing rigorous statistical validation for its findings. Strengthening the statistical analysis and improving the clarity of the experimental design would enhance the paper's contributions and reliability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes a multi-step reciprocal prediction method aimed at enhancing learning from limited samples in unknown dynamical systems. It introduces an adaptive sampling approach intended to identify critical samples that can optimize prediction accuracy. The findings indicate some improvement in predictive performance for specific dynamical systems, such as the Damped Pendulum and Lorenz system.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to address the challenge of sample efficiency in dynamical systems, which is a significant concern in many practical applications. However, the methodology suffers from several weaknesses. Notably, it lacks a comprehensive theoretical framework for determining the minimum sample sizes necessary for robust learning across diverse dynamical systems. Furthermore, the paper does not adequately address potential overfitting issues associated with training deep neural networks with reduced sample sizes. The limited scope of empirical validation raises concerns about the generalizability of the findings to broader classes of complex or high-dimensional systems. The paper also fails to discuss the implications of noise in measurements and does not provide an analysis of the computational complexity relative to traditional methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly and presents its ideas in a structured manner; however, the lack of detailed exploration of limitations and failure cases diminishes its overall quality. While the adaptive sampling approach is a novel contribution, its practical implications and reproducibility are not sufficiently addressed. The paper does not provide guidelines for implementing the proposed methods in real-world scenarios, which could hinder adoption by practitioners in the field.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to adaptive sampling in dynamical systems but falls short in addressing critical theoretical and practical concerns. The limited empirical validation and lack of exploration of noise and overfitting issues weaken its contributions. Further work is needed to enhance the robustness and applicability of the proposed methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Critical Sampling for Robust Evolution Behavior Learning of Unknown Dynamical Systems\" claims to address the challenge of minimizing the number of samples required for learning unknown dynamical systems. The authors propose a methodology based on a multi-step reciprocal prediction network, which they argue enhances prediction accuracy by correlating prediction errors with sampling errors. Their experimental results suggest a significant reduction in the sample size needed for effective learning, though the findings are framed in the context of existing methodologies.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to tackle a relevant problem in the field of dynamical systems and machine learning. However, the weaknesses are pronounced: the contributions lack novelty, as many of the concepts, such as adaptive sampling and error metrics, have been extensively explored in prior literature. The authors fail to provide a compelling differentiation from existing methods, and their claims about the advantages of their approach are not sufficiently substantiated by rigorous comparative analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the quality of the contributions is undermined by the lack of originality. The novelty of the proposed methods is minimal, as they repurpose well-established concepts without providing substantial new insights. Reproducibility may be an issue, as the paper does not offer enough detail regarding the experimental setup and the baseline methods for comparison, which could hinder other researchers from validating their findings.\n\n# Summary Of The Review\nOverall, the paper does not present a significant advancement in the field of dynamical systems or machine learning. While it attempts to address an important problem, the lack of novel contributions and the overstatement of its findings diminish its impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper presents an Adaptive Learning Framework aimed at improving the efficiency of sample collection in modeling unknown dynamical systems. The methodology focuses on adaptive sampling techniques based on real-time error estimations, which are coupled with error estimation approaches that correlate network modeling error with multi-step reciprocal prediction error. The findings indicate that the proposed method demonstrates improved predictive accuracy and sample efficiency, particularly in high-dimensional systems, while also suggesting potential applications in fields such as robotics and control systems.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to adaptive sampling and the introduction of joint spatial-temporal evolution networks, which represent a significant advancement in the field. The extensive experimental validation adds credibility to the findings. However, the paper could benefit from deeper exploration into generalization across various dynamical systems and real-world applications, which could further substantiate the claims made. Additionally, the integration of prior knowledge and dimensionality reduction techniques could enhance the robustness and applicability of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly. The quality of the writing is high, making complex concepts accessible to readers. The novelty of the approach is notable, particularly in combining adaptive sampling with error estimation techniques. While the methodology is reproducible, the paper could provide more detailed descriptions of the experimental setups and datasets used to facilitate independent verification of the results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling adaptive learning framework with significant contributions to the field of dynamical systems modeling. While the experimental validation is robust, further exploration of generalization and integration of complementary techniques could enhance its impact and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a critical sampling method for learning dynamical systems, demonstrating a significant reduction in the number of samples required for accurate predictions when compared to established baseline methods. The authors evaluate their approach on various dynamical systems, including the Lorenz system, Damped Pendulum, 2D Nonlinear system, and Viscous Burgers’ equation, providing empirical results that highlight the efficiency of their method. Notably, the proposed method achieves better or comparable prediction errors with dramatically fewer samples, underscoring its potential for practical applications where data collection is limited or costly.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ability to significantly reduce the sample size needed for accurate predictions across multiple benchmark systems, achieving reductions of up to 567-fold in some cases. This efficiency can have substantial implications for real-world applications in dynamical systems modeling. However, a potential weakness is the lack of discussion on the scalability of the method to more complex or high-dimensional systems, which could limit its applicability in broader contexts. Additionally, more detailed explanations of the underlying methodology and experiments could enhance clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation of the proposed method and empirical results, though certain sections could benefit from more detailed explanations to aid reader comprehension. The quality of the writing is high, with a logical flow of ideas and findings. In terms of novelty, while the approach to critical sampling is innovative, the paper could improve reproducibility by providing more comprehensive insights into the experimental setup and parameter choices, which are crucial for others to replicate the results.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of dynamical systems learning by introducing an efficient critical sampling method that significantly reduces the number of samples needed for accurate predictions. The empirical results are compelling and suggest practical applicability, although further clarity on methodology and scalability would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to network modeling through a \"multi-step reciprocal prediction network.\" The main contributions include the formulation of the network architecture, a detailed methodology for training and evaluation, and an empirical analysis demonstrating its effectiveness compared to existing methods. The findings indicate that the proposed model improves prediction accuracy significantly, showcasing its potential for practical applications in various domains.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to network modeling and the thorough empirical evaluation, which provides strong evidence for its claims. However, the paper has notable weaknesses, including a lack of clarity in the abstract and introduction due to dense language and excessive technical jargon, which could alienate readers unfamiliar with the specific terminology. Additionally, the presentation of results is somewhat disorganized, which may hinder the reader's ability to grasp the key findings effectively.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by complex sentences and a lack of explanatory context for technical terms, which affects the overall quality of the writing. While the technical novelty of the proposed model is evident, the paper could benefit from clearer organization and simplified language to enhance its readability. The reproducibility statement is a positive aspect; however, it could be more actionable by providing specific details on the data and methods available for replication.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to network modeling with solid empirical support. However, clarity issues and disorganization in presentation detract from its potential impact. Improvements in language simplification and structure could enhance the paper's accessibility and effectiveness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.2524345032548476,
    -1.7238258897440455,
    -1.9024791061742568,
    -1.6686168444064893,
    -1.8403474636392996,
    -1.8012414767402014,
    -1.5452187367631294,
    -2.0351907784988317,
    -1.931704575957675,
    -1.7838997651121418,
    -1.7267054369109724,
    -1.5510741661670864,
    -1.8091318556600409,
    -1.8073705374181883,
    -1.8241998715178411,
    -1.6558737870401377,
    -2.0328998025053413,
    -1.7819761781467973,
    -1.7486696826411237,
    -1.9914733744561937,
    -2.0556874207736464,
    -1.800866256191584,
    -1.9865827311584636,
    -1.9537251547817247,
    -1.845572146296392,
    -1.8527840661279853,
    -1.8412932149611791,
    -1.5552499259502535,
    -1.7516691886423792
  ],
  "logp_cond": [
    [
      0.0,
      -2.1503788759046176,
      -2.1668663433561224,
      -2.1735410635777033,
      -2.1871523943925504,
      -2.188645259037744,
      -2.206172204387307,
      -2.1716858325910247,
      -2.1612716503990246,
      -2.1795232617125864,
      -2.1721313366379196,
      -2.2020856916956624,
      -2.1600722776444554,
      -2.1610287610407446,
      -2.174337432502345,
      -2.1779844615955777,
      -2.1782511443036894,
      -2.165732631942532,
      -2.177794441522167,
      -2.170838545446465,
      -2.175874027274805,
      -2.1917425404311475,
      -2.1827089438276035,
      -2.163732107964552,
      -2.1841709868809493,
      -2.1864919679991974,
      -2.1732770718926853,
      -2.189968201487117,
      -2.202948525340935
    ],
    [
      -1.312634958806378,
      0.0,
      -1.198697009644275,
      -1.3045458312444822,
      -1.3074362632476562,
      -1.355450648851144,
      -1.4131137010634771,
      -1.2564910957601474,
      -1.2611567931732626,
      -1.3807507324269563,
      -1.335173820116527,
      -1.503014405301545,
      -1.2054547487348342,
      -1.1840745827472252,
      -1.3126045841013816,
      -1.2775770425812634,
      -1.342219417937782,
      -1.1913526704390942,
      -1.2000532221164502,
      -1.2654145408650908,
      -1.2864796320042866,
      -1.4397372065550968,
      -1.4009258969893712,
      -1.3112578137324589,
      -1.3502794481401474,
      -1.346526423871983,
      -1.210781406845345,
      -1.418260000449542,
      -1.411751668485191
    ],
    [
      -1.4456028037139588,
      -1.3221324749603869,
      0.0,
      -1.379199057288424,
      -1.377523143123501,
      -1.4146587340818597,
      -1.5751063397105423,
      -1.3896932245659828,
      -1.3346970905531923,
      -1.4347773879425947,
      -1.4023429559924852,
      -1.5793002335086548,
      -1.3306537382871995,
      -1.3317464116063622,
      -1.4116393927247515,
      -1.3603530038705265,
      -1.4876224702037972,
      -1.3294456578044667,
      -1.3761552603422174,
      -1.325137591292953,
      -1.4444818217544972,
      -1.4895041186439362,
      -1.4478885023406656,
      -1.4311956078376158,
      -1.4394837391572572,
      -1.4284000013826488,
      -1.4103436081623957,
      -1.5344709499379108,
      -1.5203671614546808
    ],
    [
      -1.2920312218025176,
      -1.2026558571756023,
      -1.1959280114574968,
      0.0,
      -1.2464683854141099,
      -1.3014268610086714,
      -1.3847782286108308,
      -1.3187281406474745,
      -1.238118678362151,
      -1.3131707536389756,
      -1.2392668788546735,
      -1.3979607847612514,
      -1.2055500055648023,
      -1.1649318557257167,
      -1.2883888735884064,
      -1.2414665438109302,
      -1.2885811685866608,
      -1.2341950014673426,
      -1.25844877675965,
      -1.2751764805472716,
      -1.3195613092621796,
      -1.3003812509583323,
      -1.3314512828079526,
      -1.2354094981071542,
      -1.319641848750028,
      -1.3056746273471893,
      -1.2937115121750675,
      -1.3356740044274524,
      -1.340280289903886
    ],
    [
      -1.4942141031394893,
      -1.4364902611759962,
      -1.4275348185736954,
      -1.4626185125821798,
      0.0,
      -1.5225764736358192,
      -1.493870457378695,
      -1.4694450685298157,
      -1.437490995690747,
      -1.4726194316413288,
      -1.482660160617865,
      -1.5497419556894305,
      -1.4305542884326672,
      -1.4520687388731286,
      -1.4815756090132897,
      -1.4593968748255854,
      -1.4778055917422919,
      -1.4643703181220091,
      -1.445120825378121,
      -1.4777232202565496,
      -1.4545257947397692,
      -1.5289336454483131,
      -1.4581954247914557,
      -1.4569051107038284,
      -1.4446021177414974,
      -1.4343653311960345,
      -1.4486887649801452,
      -1.54063712245065,
      -1.5261013134831733
    ],
    [
      -1.467056224907361,
      -1.3584635134761993,
      -1.30059763832915,
      -1.41801419088008,
      -1.3967189345833007,
      0.0,
      -1.4378527062621318,
      -1.4359802586025894,
      -1.3110505962751342,
      -1.3218024733856388,
      -1.3577393870635717,
      -1.4753826691247993,
      -1.3128917628133514,
      -1.3855133153034915,
      -1.3976934537280814,
      -1.3062348158026904,
      -1.3705699774374327,
      -1.3581780867054598,
      -1.4012129580108597,
      -1.4312201689969768,
      -1.437009900707872,
      -1.383659812321489,
      -1.400035680822497,
      -1.3667442357112634,
      -1.4196488506803357,
      -1.4253877017845904,
      -1.40411628771231,
      -1.4086341106768834,
      -1.528989384725805
    ],
    [
      -1.2796264572592042,
      -1.104817820527865,
      -1.1519593653194726,
      -1.1512037321674915,
      -1.1640964938637521,
      -1.135303968892056,
      0.0,
      -1.2133409800555712,
      -1.1333189400841406,
      -1.2168159291549308,
      -1.1795220110900027,
      -1.273296885370805,
      -1.0998521759157807,
      -1.1372524591335669,
      -1.2069331538580739,
      -1.1434665954928613,
      -1.1887155784119061,
      -1.112753192589014,
      -1.168807313384045,
      -1.198394448339362,
      -1.2001874874495135,
      -1.2466285655979408,
      -1.203717646786687,
      -1.1685739195423421,
      -1.1753328988041727,
      -1.1257723043806263,
      -1.1624807092648175,
      -1.2080810139946738,
      -1.259269549827659
    ],
    [
      -1.5647259723572786,
      -1.3901738925558158,
      -1.399973422421333,
      -1.5218773716419933,
      -1.545196177145256,
      -1.604421816320152,
      -1.6531911873783438,
      0.0,
      -1.4190365821336277,
      -1.6053305826162496,
      -1.5159084550505566,
      -1.7056306668878092,
      -1.438843030197065,
      -1.363018434383431,
      -1.4638706832671902,
      -1.509342405338871,
      -1.5363959369839777,
      -1.4083878230360949,
      -1.4919697470673965,
      -1.4197700899710126,
      -1.4544391628095472,
      -1.6418314457982015,
      -1.5922645170666923,
      -1.5548906817097465,
      -1.558580710191595,
      -1.4640668919129909,
      -1.4807080743275933,
      -1.6357633855846,
      -1.611443932688781
    ],
    [
      -1.5181444316455315,
      -1.4250375259386738,
      -1.3515262546599291,
      -1.5041574030356708,
      -1.437203039331372,
      -1.4777506744650584,
      -1.5586469935112224,
      -1.4379362992950846,
      0.0,
      -1.4829504220986216,
      -1.5041636175428377,
      -1.600547939976462,
      -1.3811464243040257,
      -1.4143138931223642,
      -1.5038311177075296,
      -1.4786664545372756,
      -1.4150718070364616,
      -1.4100898543933444,
      -1.3941631876194753,
      -1.3803219114218386,
      -1.4708384953201152,
      -1.5618454654343499,
      -1.514852737182962,
      -1.446440968810152,
      -1.4973481523512426,
      -1.4578938848805016,
      -1.489556810322475,
      -1.5577951740525509,
      -1.5711334516753857
    ],
    [
      -1.5164020890411003,
      -1.4204876226790073,
      -1.422321063932547,
      -1.4522242290602174,
      -1.4165014248235706,
      -1.412257747565915,
      -1.489919591539317,
      -1.496740209670669,
      -1.4117503602184873,
      0.0,
      -1.454515456292078,
      -1.4979344229309821,
      -1.427362749570698,
      -1.444839810649462,
      -1.4238579930556434,
      -1.4145761292619887,
      -1.4184321563696285,
      -1.453084521254959,
      -1.4513138112483182,
      -1.4768247506682737,
      -1.4722678155616429,
      -1.4705165133184463,
      -1.4082878763448279,
      -1.3671471514632434,
      -1.4285765915769102,
      -1.4682124396791179,
      -1.4307227691763762,
      -1.4829324999219329,
      -1.5185686700957233
    ],
    [
      -1.3051827579608588,
      -1.1899220598697016,
      -1.1512439619278478,
      -1.1671817376114801,
      -1.2045016350183284,
      -1.29292827767234,
      -1.3313348691557694,
      -1.21606875238271,
      -1.172327688251164,
      -1.3328470577849518,
      0.0,
      -1.3992335728198868,
      -1.1333722041159118,
      -1.1450215392325458,
      -1.19013669523903,
      -1.2218387874781518,
      -1.2216111852082705,
      -1.2013071718985162,
      -1.217939325063391,
      -1.1838422366181591,
      -1.2905396802888451,
      -1.316432866571239,
      -1.2730266578828675,
      -1.2006862761944488,
      -1.266821635898707,
      -1.166701335405369,
      -1.2704424138719976,
      -1.3143889499835875,
      -1.317751530250271
    ],
    [
      -1.2971623328221527,
      -1.2929315954166924,
      -1.283297858617592,
      -1.2928536890755262,
      -1.265524353461851,
      -1.2435650792885302,
      -1.2929097666487288,
      -1.2875699190629497,
      -1.239334388257533,
      -1.2697997717371554,
      -1.2969259020185686,
      0.0,
      -1.2585466110410732,
      -1.2711985309505327,
      -1.291620990062659,
      -1.2951614048841091,
      -1.2726480900236257,
      -1.2912639272963986,
      -1.2934152307368147,
      -1.2923847410281832,
      -1.2717783626562549,
      -1.23745393137487,
      -1.242526599756383,
      -1.2466086137685013,
      -1.2767036946919128,
      -1.2931229062359408,
      -1.2870130955739763,
      -1.2855590441834857,
      -1.2638795022578653
    ],
    [
      -1.4197665889628415,
      -1.2839584413810012,
      -1.3049016498832122,
      -1.3742007631322097,
      -1.408980992725849,
      -1.426086577780568,
      -1.4940188688879832,
      -1.3815813185475887,
      -1.3712113781776463,
      -1.4726287818686028,
      -1.3909443949848181,
      -1.5538809575870036,
      0.0,
      -1.3475245682994441,
      -1.338698820697214,
      -1.2656431397516754,
      -1.427523603825372,
      -1.292979189617601,
      -1.3925216681945143,
      -1.3273674177412687,
      -1.4102880972003797,
      -1.4218013214632446,
      -1.4456724565141212,
      -1.4166587597739713,
      -1.465946931196548,
      -1.3260520902954402,
      -1.4138485018612768,
      -1.5072060927781583,
      -1.5121017823493346
    ],
    [
      -1.3694600476255385,
      -1.187980916272519,
      -1.2251473217221558,
      -1.2640021490981905,
      -1.3084657097479795,
      -1.395312228407557,
      -1.418600713198586,
      -1.2405502697678066,
      -1.2947398106143926,
      -1.4185015610806104,
      -1.3268581955080683,
      -1.5173053959500071,
      -1.2506651846996162,
      0.0,
      -1.3311134577710753,
      -1.3240104564897615,
      -1.3369047057030135,
      -1.2666037389021512,
      -1.3197616933434264,
      -1.1940480127923465,
      -1.3330303151672385,
      -1.4725406537663057,
      -1.3996766325201286,
      -1.329816112302963,
      -1.367970326604849,
      -1.3360421610682471,
      -1.3079954862422671,
      -1.423634798154773,
      -1.4307725123243398
    ],
    [
      -1.383683089897133,
      -1.3595683274070394,
      -1.2904007406432676,
      -1.3842739059596498,
      -1.3854324380420202,
      -1.4192896151316337,
      -1.4521146126689348,
      -1.3883026472546551,
      -1.3107835649399269,
      -1.3942885883297733,
      -1.3352063000507732,
      -1.492512365375213,
      -1.2395099503722002,
      -1.303683412216367,
      0.0,
      -1.3083229663104374,
      -1.3917444041879174,
      -1.2550365130889547,
      -1.3979350220742668,
      -1.2176132157116681,
      -1.3460230231218777,
      -1.3863131019574124,
      -1.4054427812335613,
      -1.3617978013158654,
      -1.4060061084055115,
      -1.322993742261723,
      -1.3428636587017644,
      -1.4571577709327443,
      -1.4291059690622496
    ],
    [
      -1.3100468841532782,
      -1.1539008006406821,
      -1.1532019853953974,
      -1.200840919379303,
      -1.1790163185029863,
      -1.1810181231136685,
      -1.3125207595553376,
      -1.2786713007381523,
      -1.1687409478470165,
      -1.2605593964345692,
      -1.2438986517253612,
      -1.413628045567071,
      -1.0378368920210603,
      -1.2545141443883798,
      -1.1866411444101967,
      0.0,
      -1.1941734512097775,
      -1.0790523605263687,
      -1.198374204622361,
      -1.1451739918393864,
      -1.2691952461570588,
      -1.2364492694904021,
      -1.2835356283407011,
      -1.2407769957470374,
      -1.2142625726279812,
      -1.1521974767167682,
      -1.2118732174662603,
      -1.277242549522779,
      -1.3453074512165981
    ],
    [
      -1.7085010049566527,
      -1.5926866944971256,
      -1.581954701565006,
      -1.6685014689886457,
      -1.612365875387808,
      -1.7153903986548373,
      -1.692423638490213,
      -1.6546241306759508,
      -1.558449353800136,
      -1.638296081385338,
      -1.6453540652862892,
      -1.7958276778599798,
      -1.6263796170336071,
      -1.6598541729244047,
      -1.6348363759193634,
      -1.5998188486638862,
      0.0,
      -1.6018529206592433,
      -1.5963364716405584,
      -1.579702398388455,
      -1.6159184497125363,
      -1.7586512723482721,
      -1.716548927274507,
      -1.6320239417239135,
      -1.6482137134641957,
      -1.6510195328829214,
      -1.6736500577467495,
      -1.7205976703886208,
      -1.6458887489269813
    ],
    [
      -1.3828410829723234,
      -1.2450111389346923,
      -1.222534290990956,
      -1.358705621010994,
      -1.4039927883866685,
      -1.439437803246239,
      -1.4730337936300602,
      -1.336020843078447,
      -1.3085914111825065,
      -1.4262587575164234,
      -1.3952300602994931,
      -1.5295785753660147,
      -1.1995964481522186,
      -1.3143354447781088,
      -1.2951941887255731,
      -1.3198329580555732,
      -1.389361063113144,
      0.0,
      -1.3866033041350072,
      -1.231695895776984,
      -1.3967928832108305,
      -1.3726560852899967,
      -1.447081613974848,
      -1.412039978396127,
      -1.4157955198278471,
      -1.3050790725887969,
      -1.365025922597877,
      -1.4608487185692582,
      -1.4855301376965495
    ],
    [
      -1.3511012623467344,
      -1.195799045903668,
      -1.27428210874204,
      -1.3605702779072466,
      -1.295609289589223,
      -1.4122325592477407,
      -1.4219757706006015,
      -1.3227173758347837,
      -1.2589908438534456,
      -1.3467906297165744,
      -1.3426452299383331,
      -1.5189915652014667,
      -1.260664395581943,
      -1.2834296692215221,
      -1.3309022032403262,
      -1.3471770658239917,
      -1.3414251070690524,
      -1.2512256965793107,
      0.0,
      -1.288259937913369,
      -1.323406708984554,
      -1.4321249481217573,
      -1.3963452875602358,
      -1.2524800738286759,
      -1.3922435509894302,
      -1.3204931626505463,
      -1.2726850511524168,
      -1.4361015394744374,
      -1.4514972221845996
    ],
    [
      -1.6336966595918039,
      -1.4842609024494344,
      -1.4463169282545378,
      -1.5630143641775822,
      -1.587305800954454,
      -1.6316797420091003,
      -1.6875719199058075,
      -1.5371359146332781,
      -1.4859422630973793,
      -1.652738533638937,
      -1.5779211472566634,
      -1.7032236559897036,
      -1.4417588317408543,
      -1.4589558423757187,
      -1.4882889751531714,
      -1.496653913984782,
      -1.5600049531687945,
      -1.4274147614498025,
      -1.57881255099157,
      0.0,
      -1.5389712171455694,
      -1.5910487570549299,
      -1.600127171880851,
      -1.591137768379438,
      -1.5951933749460034,
      -1.4897296000603206,
      -1.5642761658471191,
      -1.5907980460569215,
      -1.642121750995259
    ],
    [
      -1.6255153526003476,
      -1.53174494550012,
      -1.5301746042899056,
      -1.655658238292505,
      -1.6235904068127356,
      -1.7006847628980026,
      -1.702702613333617,
      -1.4931650683994253,
      -1.550595660149805,
      -1.644033453148889,
      -1.636854194680679,
      -1.759973497607177,
      -1.5735886740451133,
      -1.5678617668933028,
      -1.5540535080613849,
      -1.6617838505624456,
      -1.5973929511071119,
      -1.5853526785228158,
      -1.5783462592825728,
      -1.5564308388160013,
      0.0,
      -1.7550630118555532,
      -1.6778248938703024,
      -1.6236073321218594,
      -1.6581879539497484,
      -1.5246835751282277,
      -1.587015632146885,
      -1.693874368143833,
      -1.6944960758693806
    ],
    [
      -1.4529437426473109,
      -1.4512968321253614,
      -1.3747381789630435,
      -1.4014892284449982,
      -1.4197635138608289,
      -1.3858450839888845,
      -1.4712303558721445,
      -1.4265328575307044,
      -1.423631264572379,
      -1.3746310935210067,
      -1.4299727503393034,
      -1.452776211398962,
      -1.3017721388881265,
      -1.453021119814822,
      -1.3792024248866388,
      -1.3252286926699177,
      -1.428691597135131,
      -1.3061363115561613,
      -1.4009588074972894,
      -1.3337521120006832,
      -1.4354937935859302,
      0.0,
      -1.362778407215039,
      -1.3844052837051322,
      -1.4041331507747006,
      -1.3137943291176746,
      -1.4761376997922977,
      -1.4399159054608006,
      -1.4606212125734592
    ],
    [
      -1.6420579574664091,
      -1.578167471824965,
      -1.5633004794240026,
      -1.5666177436444106,
      -1.5515794516149697,
      -1.60541517699428,
      -1.6225559023654688,
      -1.6164077245155593,
      -1.552286168448757,
      -1.5031900092585728,
      -1.5956967590376774,
      -1.6386319114902734,
      -1.5561323870258597,
      -1.6055895964421478,
      -1.6150673792690413,
      -1.5778872922498255,
      -1.572629304533482,
      -1.5684311432725073,
      -1.5880934238548223,
      -1.5864404845001503,
      -1.575647858717681,
      -1.6021602420569734,
      0.0,
      -1.525085547944155,
      -1.5115580966266868,
      -1.5725974732341252,
      -1.6127171727762315,
      -1.6316531685072775,
      -1.630257416989373
    ],
    [
      -1.5033597929927494,
      -1.4062295909903402,
      -1.47880733623953,
      -1.4806797196567043,
      -1.4610469914294189,
      -1.5140693437619182,
      -1.5915048021821203,
      -1.5466146793908404,
      -1.4832563320966152,
      -1.5266604443463727,
      -1.4692807538180432,
      -1.6238952924113117,
      -1.4589438725600188,
      -1.4781462111106136,
      -1.506596174633937,
      -1.4992413818351662,
      -1.5264278455182978,
      -1.5177699250909287,
      -1.4281643302305187,
      -1.5179624825053688,
      -1.5152382603208585,
      -1.5576863455098884,
      -1.5431017882868503,
      0.0,
      -1.5148557613176958,
      -1.4923453839256449,
      -1.459398200097626,
      -1.5340272976480087,
      -1.5902328970781778
    ],
    [
      -1.4632419433099275,
      -1.3916559596625877,
      -1.3631121199433065,
      -1.4288895164035609,
      -1.369894448742471,
      -1.448481582101365,
      -1.4926271954998378,
      -1.41257739594602,
      -1.3792578908617514,
      -1.3942305214147053,
      -1.4097009091218522,
      -1.569010395124771,
      -1.3768687030331779,
      -1.4213758641442695,
      -1.4185313233234502,
      -1.351235663925371,
      -1.42096794811707,
      -1.3777328849686186,
      -1.423513641715741,
      -1.3831747161847556,
      -1.4188866749656643,
      -1.509187558545447,
      -1.3800991201334742,
      -1.3683645164413463,
      0.0,
      -1.3879692461038178,
      -1.4276168970575225,
      -1.4500197376046624,
      -1.4811530269436106
    ],
    [
      -1.5282012603874875,
      -1.312378249241463,
      -1.2777472639576615,
      -1.3511731221484682,
      -1.3196517956999865,
      -1.4294957853379462,
      -1.4153038702755891,
      -1.2680262346557103,
      -1.311246386033602,
      -1.4259283745917988,
      -1.331541975916532,
      -1.554694192445353,
      -1.1522724626830532,
      -1.310955368051983,
      -1.2726762799428148,
      -1.2065748919089794,
      -1.3539798688045377,
      -1.1977438579227482,
      -1.3255568835051883,
      -1.249652782712628,
      -1.2671308696008243,
      -1.3436926417076018,
      -1.394549609208819,
      -1.3361839774949271,
      -1.386428905923797,
      0.0,
      -1.3645697607938372,
      -1.4597112663470715,
      -1.4359025120311668
    ],
    [
      -1.413049454304419,
      -1.2326228212774657,
      -1.3428492773421035,
      -1.358071420893627,
      -1.332073120259015,
      -1.4188936671739176,
      -1.475207220286043,
      -1.3550979596120656,
      -1.3381781645673105,
      -1.3961988185509202,
      -1.4048718236464703,
      -1.527019512491849,
      -1.3129633216190504,
      -1.2996821638937706,
      -1.3478453378122213,
      -1.3909804231088325,
      -1.398699794801582,
      -1.333033103100121,
      -1.2936602480707293,
      -1.370267427316424,
      -1.3622337099829165,
      -1.4843657801029317,
      -1.4223024626824217,
      -1.2815563524433071,
      -1.3700181430174656,
      -1.399188436187523,
      0.0,
      -1.4797937409257846,
      -1.4533014234275627
    ],
    [
      -1.2593426212437797,
      -1.2441316216522358,
      -1.1989972066414327,
      -1.241474216992007,
      -1.222383418975782,
      -1.2389019682069244,
      -1.246664690532208,
      -1.2418974075085065,
      -1.196150245441092,
      -1.264655005948682,
      -1.2507217002626123,
      -1.2890288089405761,
      -1.2196902747469116,
      -1.2060687008514035,
      -1.2702616217298148,
      -1.2129769817578762,
      -1.2253983787771006,
      -1.2307289969547472,
      -1.2166122795225347,
      -1.1794623402383273,
      -1.2402290397686622,
      -1.2541626117789344,
      -1.2761676448394463,
      -1.2040108451105132,
      -1.2107065620343853,
      -1.2482416485291143,
      -1.274209940287782,
      0.0,
      -1.252414697446142
    ],
    [
      -1.3641028323772622,
      -1.3295160734427929,
      -1.3058625542938804,
      -1.299142098022074,
      -1.357011614032215,
      -1.3975809478987626,
      -1.3737874683410887,
      -1.3229523329273145,
      -1.2991186595544566,
      -1.3655960204646145,
      -1.359191145553323,
      -1.36446545499954,
      -1.3101792608822638,
      -1.3046769913847371,
      -1.346895984758899,
      -1.340569550149574,
      -1.2790071573878918,
      -1.3453526503541904,
      -1.3264707439912056,
      -1.313557618756191,
      -1.3275179598717726,
      -1.395720015798564,
      -1.3478134359720146,
      -1.2926758189536747,
      -1.3256789267197455,
      -1.3163358286676723,
      -1.3473079898369131,
      -1.370622117725253,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.10205562735023,
      0.08556815989872524,
      0.07889343967714435,
      0.06528210886229724,
      0.06378924421710375,
      0.04626229886754052,
      0.08074867066382296,
      0.09116285285582304,
      0.07291124154226125,
      0.08030316661692805,
      0.05034881155918525,
      0.0923622256103922,
      0.09140574221410303,
      0.0780970707525026,
      0.07445004165926994,
      0.07418335895115824,
      0.08670187131231577,
      0.07464006173268078,
      0.08159595780838247,
      0.07656047598004267,
      0.06069196282370015,
      0.06972555942724412,
      0.08870239529029567,
      0.06826351637389827,
      0.06594253525565019,
      0.07915743136216236,
      0.06246630176773049,
      0.04948597791391274
    ],
    [
      0.41119093093766756,
      0.0,
      0.5251288800997704,
      0.4192800584995633,
      0.41638962649638933,
      0.3683752408929015,
      0.3107121886805684,
      0.4673347939838981,
      0.4626690965707829,
      0.34307515731708915,
      0.3886520696275184,
      0.22081148444250043,
      0.5183711410092113,
      0.5397513069968203,
      0.4112213056426639,
      0.4462488471627821,
      0.3816064718062635,
      0.5324732193049513,
      0.5237726676275953,
      0.45841134887895474,
      0.4373462577397589,
      0.28408868318894864,
      0.3228999927546743,
      0.4125680760115866,
      0.37354644160389805,
      0.37729946587206253,
      0.5130444828987004,
      0.30556588929450346,
      0.31207422125885453
    ],
    [
      0.456876302460298,
      0.5803466312138699,
      0.0,
      0.5232800488858327,
      0.5249559630507556,
      0.48782037209239704,
      0.3273727664637145,
      0.512785881608274,
      0.5677820156210645,
      0.46770171823166207,
      0.5001361501817716,
      0.32317887266560197,
      0.5718253678870573,
      0.5707326945678945,
      0.4908397134495053,
      0.5421261023037303,
      0.4148566359704595,
      0.5730334483697901,
      0.5263238458320394,
      0.5773415148813037,
      0.4579972844197595,
      0.4129749875303206,
      0.4545906038335912,
      0.47128349833664096,
      0.4629953670169995,
      0.474079104791608,
      0.49213549801186107,
      0.368008156236346,
      0.382111944719576
    ],
    [
      0.37658562260397166,
      0.46596098723088697,
      0.47268883294899244,
      0.0,
      0.4221484589923794,
      0.36718998339781783,
      0.2838386157956585,
      0.3498887037590148,
      0.4304981660443383,
      0.35544609076751366,
      0.4293499655518158,
      0.27065605964523787,
      0.463066838841687,
      0.5036849886807726,
      0.3802279708180829,
      0.42715030059555903,
      0.38003567581982844,
      0.43442184293914665,
      0.41016806764683933,
      0.39344036385921766,
      0.34905553514430965,
      0.36823559344815693,
      0.3371655615985367,
      0.4332073462993351,
      0.3489749956564612,
      0.36294221705929997,
      0.37490533223142175,
      0.33294283997903684,
      0.32833655450260335
    ],
    [
      0.34613336049981025,
      0.40385720246330337,
      0.4128126450656042,
      0.3777289510571198,
      0.0,
      0.3177709900034804,
      0.3464770062606046,
      0.37090239510948386,
      0.4028564679485527,
      0.3677280319979708,
      0.3576873030214345,
      0.29060550794986906,
      0.4097931752066324,
      0.388278724766171,
      0.3587718546260099,
      0.38095058881371413,
      0.3625418718970077,
      0.37597714551729045,
      0.39522663826117865,
      0.36262424338275,
      0.3858216688995304,
      0.31141381819098646,
      0.3821520388478439,
      0.38344235293547113,
      0.3957453458978022,
      0.4059821324432651,
      0.39165869865915437,
      0.2997103411886495,
      0.3142461501561262
    ],
    [
      0.3341852518328403,
      0.4427779632640021,
      0.5006438384110514,
      0.38322728586012134,
      0.40452254215690076,
      0.0,
      0.3633887704780696,
      0.36526121813761203,
      0.4901908804650672,
      0.4794390033545626,
      0.44350208967662974,
      0.3258588076154021,
      0.48834971392685,
      0.4157281614367099,
      0.40354802301212,
      0.495006660937511,
      0.4306714993027687,
      0.4430633900347416,
      0.40002851872934175,
      0.3700213077432246,
      0.36423157603232936,
      0.4175816644187125,
      0.40120579591770444,
      0.43449724102893805,
      0.38159262605986566,
      0.375853774955611,
      0.3971251890278915,
      0.39260736606331803,
      0.2722520920143965
    ],
    [
      0.26559227950392517,
      0.44040091623526445,
      0.39325937144365675,
      0.3940150045956379,
      0.38112224289937724,
      0.4099147678710733,
      0.0,
      0.33187775670755815,
      0.4118997966789888,
      0.3284028076081986,
      0.3656967256731267,
      0.2719218513923243,
      0.4453665608473487,
      0.4079662776295625,
      0.3382855829050555,
      0.40175214127026804,
      0.35650315835122326,
      0.4324655441741154,
      0.3764114233790843,
      0.34682428842376734,
      0.34503124931361584,
      0.2985901711651886,
      0.34150108997644235,
      0.37664481722078724,
      0.3698858379589567,
      0.4194464323825031,
      0.3827380274983119,
      0.33713772276845555,
      0.28594918693547045
    ],
    [
      0.47046480614155306,
      0.6450168859430159,
      0.6352173560774987,
      0.5133134068568384,
      0.48999460135357564,
      0.43076896217867966,
      0.38199959112048787,
      0.0,
      0.616154196365204,
      0.42986019588258206,
      0.5192823234482751,
      0.3295601116110225,
      0.5963477483017667,
      0.6721723441154006,
      0.5713200952316415,
      0.5258483731599606,
      0.498794841514854,
      0.6268029554627368,
      0.5432210314314352,
      0.6154206885278191,
      0.5807516156892845,
      0.39335933270063017,
      0.4429262614321394,
      0.48030009678908514,
      0.47661006830723673,
      0.5711238865858408,
      0.5544827041712383,
      0.39942739291423157,
      0.4237468458100506
    ],
    [
      0.4135601443121435,
      0.5066670500190011,
      0.5801783212977458,
      0.42754717292200417,
      0.49450153662630303,
      0.4539539014926166,
      0.3730575824464526,
      0.4937682766625904,
      0.0,
      0.4487541538590534,
      0.4275409584148373,
      0.33115663598121303,
      0.5505581516536493,
      0.5173906828353108,
      0.42787345825014533,
      0.45303812142039934,
      0.5166327689212133,
      0.5216147215643305,
      0.5375413883381996,
      0.5513826645358364,
      0.46086608063755974,
      0.3698591105233251,
      0.4168518387747129,
      0.4852636071475229,
      0.43435642360643234,
      0.47381069107717333,
      0.4421477656352,
      0.3739094019051241,
      0.3605711242822893
    ],
    [
      0.26749767607104147,
      0.36341214243313447,
      0.3615787011795948,
      0.33167553605192435,
      0.3673983402885712,
      0.3716420175462267,
      0.2939801735728247,
      0.28715955544147276,
      0.37214940489365445,
      0.0,
      0.32938430882006386,
      0.28596534218115965,
      0.3565370155414438,
      0.3390599544626798,
      0.36004177205649834,
      0.36932363585015304,
      0.3654676087425133,
      0.33081524385718275,
      0.3325859538638236,
      0.3070750144438681,
      0.3116319495504989,
      0.3133832517936954,
      0.3756118887673139,
      0.4167526136488984,
      0.3553231735352316,
      0.3156873254330239,
      0.35317699593576557,
      0.3009672651902089,
      0.2653310950164185
    ],
    [
      0.42152267895011364,
      0.5367833770412709,
      0.5754614749831246,
      0.5595236992994923,
      0.522203801892644,
      0.43377715923863236,
      0.3953705677552031,
      0.5106366845282624,
      0.5543777486598085,
      0.3938583791260206,
      0.0,
      0.32747186409108564,
      0.5933332327950607,
      0.5816838976784267,
      0.5365687416719425,
      0.5048666494328207,
      0.505094251702702,
      0.5253982650124562,
      0.5087661118475815,
      0.5428632002928133,
      0.4361657566221273,
      0.4102725703397334,
      0.45367877902810494,
      0.5260191607165237,
      0.45988380101226545,
      0.5600041015056034,
      0.4562630230389748,
      0.412316486927385,
      0.4089539066607015
    ],
    [
      0.2539118333449337,
      0.25814257075039393,
      0.26777630754949433,
      0.25822047709156015,
      0.2855498127052354,
      0.30750908687855616,
      0.25816439951835757,
      0.26350424710413667,
      0.31173977790955343,
      0.28127439442993096,
      0.25414826414851777,
      0.0,
      0.29252755512601314,
      0.2798756352165537,
      0.2594531761044274,
      0.25591276128297724,
      0.2784260761434607,
      0.25981023887068777,
      0.25765893543027163,
      0.25868942513890314,
      0.2792958035108315,
      0.31362023479221635,
      0.30854756641070336,
      0.3044655523985851,
      0.27437047147517357,
      0.25795125993114554,
      0.2640610705931101,
      0.2655151219836007,
      0.2871946639092211
    ],
    [
      0.3893652666971994,
      0.5251734142790396,
      0.5042302057768286,
      0.43493109252783113,
      0.4001508629341919,
      0.38304527787947285,
      0.3151129867720577,
      0.42755053711245217,
      0.4379204774823946,
      0.33650307379143807,
      0.4181874606752227,
      0.25525089807303725,
      0.0,
      0.46160728736059675,
      0.47043303496282696,
      0.5434887159083654,
      0.38160825183466884,
      0.5161526660424398,
      0.4166101874655266,
      0.4817644379187722,
      0.39884375845966114,
      0.3873305341967963,
      0.3634593991459196,
      0.3924730958860696,
      0.3431849244634928,
      0.4830797653646006,
      0.3952833537987641,
      0.30192576288188255,
      0.2970300733107063
    ],
    [
      0.4379104897926498,
      0.6193896211456693,
      0.5822232156960325,
      0.5433683883199978,
      0.49890482767020883,
      0.41205830901063134,
      0.38876982421960227,
      0.5668202676503817,
      0.5126307268037957,
      0.3888689763375779,
      0.48051234191012004,
      0.2900651414681812,
      0.5567053527185721,
      0.0,
      0.47625707964711306,
      0.48336008092842686,
      0.47046583171517486,
      0.5407667985160372,
      0.4876088440747619,
      0.6133225246258418,
      0.47434022225094985,
      0.3348298836518826,
      0.4076939048980597,
      0.4775544251152253,
      0.43940021081333924,
      0.4713283763499412,
      0.4993750511759212,
      0.3837357392634153,
      0.37659802509384854
    ],
    [
      0.44051678162070806,
      0.46463154411080176,
      0.5337991308745735,
      0.4399259655581913,
      0.4387674334758209,
      0.4049102563862075,
      0.3720852588489063,
      0.435897224263186,
      0.5134163065779143,
      0.4299112831880678,
      0.4889935714670679,
      0.3316875061426281,
      0.5846899211456409,
      0.5205164593014742,
      0.0,
      0.5158769052074037,
      0.4324554673299237,
      0.5691633584288864,
      0.42626484944357435,
      0.606586655806173,
      0.4781768483959634,
      0.4378867695604287,
      0.4187570902842799,
      0.46240207020197577,
      0.4181937631123296,
      0.5012061292561181,
      0.48133621281607675,
      0.3670421005850968,
      0.3950939024555915
    ],
    [
      0.34582690288685947,
      0.5019729863994555,
      0.5026718016447402,
      0.4550328676608346,
      0.4768574685371514,
      0.47485566392646916,
      0.3433530274848,
      0.3772024863019854,
      0.4871328391931211,
      0.3953143906055685,
      0.4119751353147765,
      0.24224574147306677,
      0.6180368950190773,
      0.40135964265175783,
      0.4692326426299409,
      0.0,
      0.4617003358303602,
      0.576821426513769,
      0.45749958241777655,
      0.5106997952007513,
      0.38667854088307885,
      0.41942451754973553,
      0.37233815869943654,
      0.4150967912931003,
      0.44161121441215645,
      0.5036763103233695,
      0.44400056957387735,
      0.3786312375173586,
      0.31056633582353954
    ],
    [
      0.3243987975486886,
      0.44021310800821567,
      0.45094510094033535,
      0.3643983335166956,
      0.42053392711753323,
      0.31750940385050397,
      0.34047616401512837,
      0.3782756718293905,
      0.47445044870520525,
      0.3946037211200033,
      0.38754573721905206,
      0.23707212464536154,
      0.4065201854717342,
      0.37304562958093657,
      0.39806342658597793,
      0.43308095384145506,
      0.0,
      0.43104688184609796,
      0.43656333086478294,
      0.45319740411688625,
      0.416981352792805,
      0.2742485301570692,
      0.3163508752308344,
      0.4008758607814278,
      0.38468608904114565,
      0.38188026962241994,
      0.35924974475859184,
      0.31230213211672053,
      0.38701105357835996
    ],
    [
      0.3991350951744739,
      0.536965039212105,
      0.5594418871558413,
      0.4232705571358033,
      0.3779833897601288,
      0.3425383749005584,
      0.30894238451673717,
      0.4459553350683503,
      0.47338476696429077,
      0.3557174206303739,
      0.38674611784730417,
      0.2523976027807826,
      0.5823797299945788,
      0.4676407333686885,
      0.4867819894212242,
      0.4621432200912241,
      0.39261511503365343,
      0.0,
      0.3953728740117901,
      0.5502802823698134,
      0.3851832949359668,
      0.4093200928568006,
      0.3348945641719494,
      0.36993619975067027,
      0.3661806583189502,
      0.47689710555800047,
      0.4169502555489204,
      0.3211274595775391,
      0.2964460404502478
    ],
    [
      0.3975684202943892,
      0.5528706367374556,
      0.4743875738990837,
      0.38809940473387705,
      0.4530603930519006,
      0.33643712339338294,
      0.3266939120405221,
      0.42595230680634,
      0.48967883878767804,
      0.4018790529245493,
      0.40602445270279053,
      0.22967811743965694,
      0.48800528705918067,
      0.46524001341960153,
      0.4177674794007975,
      0.401492616817132,
      0.4072445755720713,
      0.49744398606181295,
      0.0,
      0.46040974472775464,
      0.4252629736565696,
      0.3165447345193664,
      0.35232439508088786,
      0.4961896088124478,
      0.35642613165169346,
      0.4281765199905774,
      0.47598463148870684,
      0.31256814316668624,
      0.297172460456524
    ],
    [
      0.3577767148643898,
      0.5072124720067592,
      0.5451564462016558,
      0.42845901027861144,
      0.4041675735017396,
      0.3597936324470934,
      0.3039014545503862,
      0.4543374598229155,
      0.5055311113588143,
      0.3387348408172566,
      0.4135522271995302,
      0.28824971846649006,
      0.5497145427153394,
      0.5325175320804749,
      0.5031843993030223,
      0.49481946047141157,
      0.43146842128739915,
      0.5640586130063912,
      0.4126608234646236,
      0.0,
      0.45250215731062426,
      0.4004246174012638,
      0.3913462025753427,
      0.40033560607675556,
      0.3962799995101902,
      0.501743774395873,
      0.4271972086090745,
      0.4006753283992721,
      0.3493516234609346
    ],
    [
      0.4301720681732988,
      0.5239424752735264,
      0.5255128164837408,
      0.4000291824811415,
      0.4320970139609108,
      0.3550026578756438,
      0.35298480744002947,
      0.5625223523742211,
      0.5050917606238414,
      0.4116539676247575,
      0.41883322609296747,
      0.2957139231664694,
      0.48209874672853315,
      0.4878256538803436,
      0.5016339127122615,
      0.3939035702112008,
      0.45829446966653453,
      0.4703347422508306,
      0.47734116149107364,
      0.4992565819576451,
      0.0,
      0.30062440891809317,
      0.377862526903344,
      0.43208008865178704,
      0.39749946682389803,
      0.5310038456454187,
      0.4686717886267615,
      0.36181305262981334,
      0.36119134490426585
    ],
    [
      0.34792251354427317,
      0.3495694240662226,
      0.4261280772285405,
      0.3993770277465858,
      0.38110274233075514,
      0.4150211722026995,
      0.32963590031943957,
      0.37433339866087967,
      0.37723499161920504,
      0.42623516267057737,
      0.37089350585228065,
      0.34809004479262207,
      0.49909411730345754,
      0.34784513637676207,
      0.4216638313049452,
      0.4756375635216663,
      0.37217465905645297,
      0.49472994463542275,
      0.3999074486942946,
      0.4671141441909008,
      0.3653724626056538,
      0.0,
      0.43808784897654496,
      0.41646097248645186,
      0.39673310541688345,
      0.48707192707390945,
      0.32472855639928633,
      0.3609503507307834,
      0.3402450436181248
    ],
    [
      0.3445247736920545,
      0.40841525933349865,
      0.423282251734461,
      0.419964987514053,
      0.43500327954349394,
      0.3811675541641837,
      0.36402682879299486,
      0.37017500664290437,
      0.4342965627097066,
      0.4833927218998908,
      0.39088597212078624,
      0.3479508196681902,
      0.43045034413260397,
      0.3809931347163158,
      0.3715153518894223,
      0.40869543890863813,
      0.4139534266249816,
      0.4181515878859563,
      0.3984893073036413,
      0.4001422466583133,
      0.41093487244078264,
      0.38442248910149024,
      0.0,
      0.46149718321430866,
      0.4750246345317768,
      0.4139852579243384,
      0.3738655583822321,
      0.35492956265118614,
      0.3563253141690905
    ],
    [
      0.45036536178897535,
      0.5474955637913845,
      0.4749178185421947,
      0.4730454351250204,
      0.49267816335230585,
      0.43965581101980655,
      0.3622203525996044,
      0.4071104753908843,
      0.4704688226851095,
      0.427064710435352,
      0.48444440096368147,
      0.329829862370413,
      0.4947812822217059,
      0.4755789436711111,
      0.4471289801477878,
      0.4544837729465585,
      0.4272973092634269,
      0.43595522969079603,
      0.525560824551206,
      0.4357626722763559,
      0.4384868944608662,
      0.3960388092718363,
      0.4106233664948744,
      0.0,
      0.43886939346402887,
      0.46137977085607984,
      0.4943269546840987,
      0.419697857133716,
      0.3634922577035469
    ],
    [
      0.3823302029864646,
      0.4539161866338044,
      0.4824600263530856,
      0.4166826298928312,
      0.4756776975539212,
      0.3970905641950271,
      0.3529449507965543,
      0.43299475035037216,
      0.46631425543464067,
      0.45134162488168683,
      0.4358712371745399,
      0.2765617511716212,
      0.4687034432632142,
      0.4241962821521226,
      0.4270408229729419,
      0.494336482371021,
      0.42460419817932205,
      0.46783926132777354,
      0.4220585045806511,
      0.4623974301116365,
      0.42668547133072776,
      0.33638458775094504,
      0.46547302616291786,
      0.47720762985504583,
      0.0,
      0.4576029001925743,
      0.4179552492388696,
      0.3955524086917297,
      0.3644191193527815
    ],
    [
      0.3245828057404978,
      0.5404058168865222,
      0.5750368021703238,
      0.5016109439795171,
      0.5331322704279988,
      0.4232882807900391,
      0.43748019585239617,
      0.584757831472275,
      0.5415376800943832,
      0.4268556915361865,
      0.5212420902114534,
      0.2980898736826323,
      0.7005116034449321,
      0.5418286980760023,
      0.5801077861851704,
      0.6462091742190059,
      0.4988041973234476,
      0.6550402082052371,
      0.527227182622797,
      0.6031312834153573,
      0.585653196527161,
      0.5090914244203835,
      0.4582344569191663,
      0.5166000886330582,
      0.4663551602041882,
      0.0,
      0.4882143053341481,
      0.3930727997809138,
      0.4168815540968185
    ],
    [
      0.4282437606567602,
      0.6086703936837135,
      0.4984439376190757,
      0.48322179406755206,
      0.5092200947021641,
      0.4223995477872615,
      0.36608599467513625,
      0.48619525534911356,
      0.5031150503938686,
      0.44509439641025894,
      0.4364213913147088,
      0.31427370246933006,
      0.5283298933421288,
      0.5416110510674086,
      0.4934478771489579,
      0.45031279185234663,
      0.44259342015959713,
      0.5082601118610581,
      0.5476329668904498,
      0.4710257876447552,
      0.47905950497826266,
      0.3569274348582474,
      0.4189907522787575,
      0.559736862517872,
      0.47127507194371354,
      0.44210477877365606,
      0.0,
      0.3614994740353945,
      0.3879917915336164
    ],
    [
      0.29590730470647375,
      0.3111183042980177,
      0.35625271930882074,
      0.31377570895824647,
      0.3328665069744714,
      0.316347957743329,
      0.3085852354180454,
      0.31335251844174694,
      0.3590996805091615,
      0.2905949200015714,
      0.30452822568764115,
      0.2662211170096773,
      0.33555965120334186,
      0.34918122509884997,
      0.28498830422043864,
      0.3422729441923773,
      0.3298515471731529,
      0.3245209289955062,
      0.33863764642771876,
      0.37578758571192616,
      0.3150208861815913,
      0.30108731417131906,
      0.27908228111080713,
      0.3512390808397403,
      0.34454336391586815,
      0.3070082774211391,
      0.28103998566247146,
      0.0,
      0.3028352285041114
    ],
    [
      0.38756635626511704,
      0.4221531151995863,
      0.44580663434849876,
      0.45252709062030516,
      0.39465757461016415,
      0.35408824074361656,
      0.3778817203012905,
      0.4287168557150647,
      0.4525505290879226,
      0.3860731681777647,
      0.3924780430890562,
      0.3872037336428391,
      0.4414899277601154,
      0.44699219725764205,
      0.40477320388348015,
      0.4110996384928052,
      0.47266203125448736,
      0.4063165382881888,
      0.4251984446511736,
      0.43811156988618816,
      0.4241512287706066,
      0.3559491728438151,
      0.40385575267036455,
      0.4589933696887045,
      0.4259902619226337,
      0.4353333599747069,
      0.40436119880546606,
      0.3810470709171261,
      0.0
    ]
  ],
  "row_avgs": [
    0.07470564672666083,
    0.41013961952145994,
    0.4826961603797758,
    0.3875790539949258,
    0.3678177375381007,
    0.40772722328193906,
    0.3663072511717603,
    0.5155103113972888,
    0.458726919112228,
    0.3357362484346031,
    0.48761140613753146,
    0.2749041685624483,
    0.40934631439293767,
    0.47195944574511994,
    0.46107824163732186,
    0.43506483242028265,
    0.38198308067511993,
    0.41345098523595597,
    0.4100208405247656,
    0.4326840346994155,
    0.43624970048472683,
    0.3983343240509151,
    0.40201649029826053,
    0.44567003917509745,
    0.42702295339138663,
    0.5105351215090005,
    0.46293517464339873,
    0.3189752303531272,
    0.41492957245959744
  ],
  "col_avgs": [
    0.37505837511041323,
    0.46641202553607686,
    0.4703932262476104,
    0.4144437678897977,
    0.42253322338676025,
    0.37670434121876467,
    0.33327874855725403,
    0.4109292113199496,
    0.4541191161801389,
    0.3842246534703473,
    0.4016006593579963,
    0.28636132241420004,
    0.4837682017954025,
    0.4473467868093034,
    0.4203667459620323,
    0.43885312692393513,
    0.4065216955867181,
    0.46497072035628395,
    0.4278920936812826,
    0.45338143444771123,
    0.40528888998289,
    0.3526645250766102,
    0.37557969922758927,
    0.4250652032724397,
    0.3915643399303753,
    0.426557189143411,
    0.40905131585596644,
    0.34489802736775793,
    0.3318894618461332
  ],
  "combined_avgs": [
    0.22488201091853705,
    0.4382758225287684,
    0.4765446933136931,
    0.40101141094236176,
    0.39517548046243045,
    0.39221578225035186,
    0.3497929998645072,
    0.4632197613586192,
    0.4564230176461834,
    0.3599804509524752,
    0.4446060327477639,
    0.2806327454883242,
    0.4465572580941701,
    0.45965311627721167,
    0.4407224937996771,
    0.4369589796721089,
    0.394252388130919,
    0.43921085279611993,
    0.41895646710302414,
    0.4430327345735634,
    0.4207692952338084,
    0.3754994245637627,
    0.38879809476292493,
    0.4353676212237686,
    0.40929364666088097,
    0.46854615532620575,
    0.4359932452496826,
    0.33193662886044256,
    0.3734095171528653
  ],
  "gppm": [
    584.3427287149011,
    664.4770250211471,
    661.6156366222546,
    687.3912671455248,
    684.1074178165791,
    702.7587856720203,
    724.2145315575633,
    688.1259182146379,
    669.8179827540711,
    697.0105227110532,
    695.4677042238781,
    742.0030708733049,
    655.6569550646367,
    672.8856492218408,
    685.8338567687588,
    678.0085674619204,
    689.2831865233377,
    664.6157935982375,
    682.4405104130041,
    668.6759949532064,
    690.4338586373359,
    713.9599206982936,
    701.8351274199094,
    680.8014284431554,
    696.4058698795895,
    684.0917154792678,
    690.4485717221779,
    718.8633420480539,
    725.6786911850342
  ],
  "gppm_normalized": [
    1.35404101687732,
    1.3754883761134613,
    1.3647984626304068,
    1.4231693048449041,
    1.4149304527987485,
    1.4549907018108252,
    1.5065881134336336,
    1.4251612583768687,
    1.3809768617301525,
    1.4441976182515806,
    1.440261092398161,
    1.545038442320537,
    1.3544085398490442,
    1.3932152864903495,
    1.4169145463484798,
    1.4045115955152558,
    1.4263789648440244,
    1.3700301938976231,
    1.4111716744931637,
    1.3731147898501384,
    1.4244179813185431,
    1.4766308223907512,
    1.453800701920216,
    1.3987988237244473,
    1.4385629004996106,
    1.4156480420214972,
    1.4277283213203071,
    1.4986478634692197,
    1.502740827039157
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394,
    657,
    523,
    436,
    419,
    474,
    390,
    480,
    426,
    428,
    404,
    410,
    529,
    440,
    402,
    391,
    498,
    406,
    448,
    479,
    502,
    416,
    388,
    440,
    448,
    427,
    397,
    451,
    445,
    376,
    414,
    412,
    416,
    442,
    410,
    414,
    440,
    404,
    432,
    343,
    419,
    544,
    416,
    447,
    415,
    396,
    390,
    354,
    411,
    419,
    383,
    374,
    379,
    418,
    348,
    421,
    448,
    422,
    374,
    695,
    444,
    457,
    470,
    451,
    421,
    487,
    408,
    443,
    385,
    446,
    534,
    403,
    397,
    389,
    359,
    422,
    369,
    489,
    402,
    372,
    387,
    401,
    466,
    437,
    374,
    409,
    411,
    327,
    755,
    383,
    457,
    399,
    413,
    459,
    551,
    467,
    449,
    422,
    434,
    467,
    444,
    444,
    376,
    444,
    417,
    432,
    477,
    391,
    419,
    369,
    378,
    450,
    409,
    480,
    439,
    393,
    384,
    583,
    456,
    425,
    387,
    417,
    449,
    425,
    437,
    393,
    420,
    374,
    404,
    454,
    426,
    423,
    398,
    409,
    381,
    520,
    456,
    379,
    385,
    394,
    407,
    396,
    372,
    374,
    470,
    402,
    563,
    419,
    486,
    424,
    424,
    414,
    471,
    416,
    473,
    441,
    439,
    666,
    500,
    434,
    456,
    490,
    381,
    408,
    499,
    418,
    402,
    406,
    404,
    441,
    453,
    425,
    410,
    399,
    391,
    1852,
    438,
    429,
    445,
    405,
    405,
    439,
    360,
    384,
    467,
    361,
    457,
    460,
    414,
    383,
    438,
    377,
    448,
    418,
    409,
    361,
    382,
    387,
    397,
    409,
    374,
    370,
    427,
    357
  ],
  "response_lengths": [
    8984,
    2633,
    2511,
    2617,
    2445,
    2485,
    2658,
    2050,
    2316,
    2693,
    2109,
    2598,
    2722,
    2477,
    2251,
    2627,
    2198,
    2627,
    2462,
    2348,
    2144,
    2209,
    2248,
    2401,
    2374,
    2092,
    2236,
    2443,
    2054
  ]
}