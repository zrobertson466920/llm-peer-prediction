{
  "example_idx": 47,
  "reference": "Under review as a conference paper at ICLR 2023\n\nOBJECT TRACKING BY HIERARCHICAL PART-WHOLE ATTENTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe present in this paper that hierarchical representations of objects can provide an informative and low-noisy proxy to associate objects of interest in multi-object tracking. This is aligned with our intuition that we usually only need to compare a little region of the body of target objects to distinguish them from other objects. We build the hierarchical representation in levels of (1) target body parts, (2) the whole target body, and (3) the union area of the target and other objects of overlap. Furthermore, with the spatio-temporal attention mechanism by transformer, we can solve the tracking in a global fashion and keeps the process online. We design our method by combining the representation with the transformer and name it Hierarchical Part-Whole Attention, or HiPWA for short. The experiments on multiple datasets suggest its good effectiveness. Moreover, previous methods mostly focus on leveraging transformers to exploit long temporal context during association which requires heavy computation resources. But HiPWA focuses on a more informative representation of objects on every single frame instead. So it is more robust with the length of temporal context and more computationally economic.\n\n1\n\nINTRODUCTION\n\nHow to represent the visual existence of an object in a discriminative fashion is a core question of computer vision. In this paper, we propose a hierarchical part-whole representation to represent the visual existence of objects. We adopt multi-object tracking as the application area since the distinguishable appearance feature is critical to avoid the mismatch among target objects when tracking across frames. To gather and process the visual information from different levels, we combine the hierarchical part-whole representation with the attention mechanism from transformers to summarize distinguishable and discriminative visual representations for objects of interest.\n\nIn the task of multi-object tracking, given a bounding box to localize objects of interest, how should we recognize the major object within the box and distinguish it from the background and other objects, especially some also having partial existence in the box? We believe the visual specificity of one object comes from three perspectives: the compositional, the semantic and the contextual. The compositional suggests the salient and unique visual regions on an object, such as a hat on a pedestrian whose color is different from all others in the same image. With a salient visual composition attached to an object, we can track it across frames even without seeing its full body. The semantic visual information is the commonly adopted one in modern computer vision such as a tight bounding box or instance segmentation mask. It defines the occupancy area of the object with the bond between its visual existence and semantic concept. Finally, contextual visual information describes the surroundings of an object. It helps to distinguish an object via contrast. For example, the bounding box might contain pixels from the background and secondary objects. However, a tight bounding box offers a strong underlying prior when combined with visual context: an object whose parts span across the boundary of the bounding box should not be the major object of this bounding box. Being the secondary object or not an object of interest, it should be regarded as noise when we generate a distinguishable visual representation for the major subject in the bounding box. The analysis above shows each level has its value to represent an object discriminatively. Motivated by the insight, we propose to represent an object by a three-level hierarchy: body parts, full body, and the union area including objects with overlap. We summarize it as a “Part-Body-Union” hierarchy.\n\nWith the hierarchy constructed, an ideal path to solving the target association in multi-object tracking is to leverage the salient information within the body area and discard mismatch by eliminating the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nnoise revealed by the contextual contrast. Without requiring more fine-grained data annotation, we propose to use transformers to process the hierarchical representation as the attention mechanism can discover important visual information. So, by combining the hierarchical visual representation and attention-based feature fusion, we finally propose our method as Hierarchical Part-Whole Attention, or HiPWA for short. In this work, we build a baseline model following this design and demonstrate its effectiveness in solving multi-object tracking problems. Through experiments on multiple multiobject tracking datasets, the proposed method achieves comparable or even better performance than the state-of-the-art transformer-based methods with a more lightweight implementation and better time efficiency during training and inference.\n\n2 RELATED WORKS\n\n2.1 REPRESENTING OBJECTS BY PARTS\n\nThe most commonly used object representation for multi-object tracking is bounding boxes. However, the bounding box is noisy by containing background pixels and pixels from secondary objects. On the other hand, our life experience demonstrates that, in many scenarios, it is not necessary to observe the full body of objects to specify an object visually and tracking targets by the distinguishable parts on it is usually more efficient. Therefore, researchers also have been studying object detection and tracking with more fine-grained representation. A common way is to use pre-defined certain parts on target bodies, such as only human head (Sundararaman et al., 2021; Shao et al., 2018), human joints (Andriluka et al., 2018; Xiu et al., 2018) or even every pixel (Voigtlaender et al., 2019; Weber et al., 2021). However, all these choices require more fine-grained data annotation beyond bounding boxes and more fine-grained perception modules beyond just normally available object detectors. In the contrast, the part-whole hierarchy we construct requires no additional annotations and we still solve tracking tasks at the granularity of bounding boxes. The idea of modeling objects with different levels is inspired by the hierarchical modeling of the human body (Marr, 2010) by David Marr when he explains how to construct the visual structure of an object from primal sketch to 2.5 sketch and further 3D representation. His classic three levels of visual information processing system concludes this in a higher-level: the computational, the algorithmic, and the implementational. A similar theory is also introduced by Fodor & Pylyshyn (1988) as the semantic, the syntactic, and the physical. Compared to these cognitive theories aiming to model general visual representation, the three perspectives we propose to recognize an object and distinguish it from others (the compositional, the semantic and the contextual) only apply to the specific problem of generating an effective visual descriptor to represent the objects of interest.\n\n2.2 TRANSFORMER-BASED MULTI-OBJECT TRACKING\n\nTransformer (Vaswani et al., 2017) is originally proposed for natural language processing. It shows a powerful capacity for information representation and processing. Later, DETR (Carion et al., 2020) introduces the transformer to the area of visual perception for object detection. It models object detection as solving a bipartite matching problem. Given that the matching-based strategy by DETR is quite similar to the target matching in the task of multi-object tracking, it is intuitive to further migrate transformer to this area. TransTrack (Sun et al., 2020) is the first work using the transformer to solve the MOT problem but it does not invent any association strategy by transformers. A concurrent work TrackFormer (Meinhardt et al., 2021) takes a further step to use the cross attention in transformer decoder in the stage of association by query passing. On the other hand, VisTR (Wang et al., 2021c) proposes a novel global association scheme upon transformer where a video clip of multiple frames is forward into the transformer at the same time to associate objects within the clip. More recently, many works (Zhou et al., 2022; Zeng et al., 2021) follow the global association scheme in either training or inference and achieve good performance. A key to their success is to process the information over a long temporal period, which can be hardly handled without the transformer. GTR (Zhou et al., 2022) makes a baseline model of using only appearance in associating objects and removing some secondary modules such as positional encoding and learnable object query. However, a downside of processing multiple frames as a batch by the transformer is the high requirement of computation resources. It has become a common practice to train the model on at least 4xV100 GPUs (Zhou et al., 2022; Sun et al., 2020; Zeng et al., 2021) or even 8xA100 GPUs (Cai et al., 2022). These methods usually suffer from significant performance drop if only limited computation resource is available. This is because they usually make improvements to association performance by taking advantage of a long temporal window and gathering more visual context within it. In\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The pipeline of our proposed method. Our method follows the tracking-by-detection scheme and conducts association in a global fashion. Our proposed hierarchical feature attention module fuses the features from three levels, i.e., object parts (compositional), object bodies (semantic) and union area of objects with overlap (contextual). The output features serve as tokens in the following transformer decoder for the global association.\n\nthis work, we focus on building a more computation and memory-efficient visual representation for objects from the scope of a single frame instead. This scheme is flexible to be integrated with transformers and more robust to short time windows during object association.\n\n3 METHOD\n\nIn this section, we introduce the method we propose to leverage a hierarchical part-whole visual representation with the attention mechanism from the transformer for multi-object tracking. In Section 3.1, we describe the overview structure of our method using global association. Then, in Section 3.2, we dive into the details of our proposed part-whole attention module. Finally, we talk about the details of training and inference by HiPWA in Section 3.3.\n\n3.1 GLOBAL ASSOCIATION\n\nBefore the transformer is introduced into this area, people usually solve multi-object tracking in a frame-by-frame fashion where the association is performed on only two frames. Recently, the transformer shows the advantage to gather and process information from multiple steps in parallel. To leverage this advantage, previous methods (Wang et al., 2021c; Zhou et al., 2022) propose to perform association in a video clip instead of just two frames. Therefore, the spatio-temporal attention capacity of the transformer leads to a new global association fashion. We follow this scheme in our proposed method. The overall pipeline of HiPWA is shown in the left-hand half of Figure 1. Now, we explain the three stages of it.\n\n1 , ..., OtN\n\nDetection and Feature Extraction. Given a video clip of T frames, i.e., T = {t, t + 1, ..., t + T }, we have the corresponding images I = {I t, I t+1, ..., I t+T }. Given a detector model, we could derive the detections of the object category of interest on all frames in parallel, noted as O = {Ot1 N }. N is the number of detections on the T frames and ti ∈ T (1 ≤ i ≤ N ) is the time step where the i-th detection, i.e., Oti , is detected. Then, we generate the representations of each detected object and note them as F = {F1, F2, ..., FN }. The most commonly adopted solution is to use the backbone output on the object area as the representation features while we adopt our proposed hierarchical part-whole representation instead whose details are to be introduced soon.\n\ni\n\nToken Generation by Hierarchical Part-Whole Attention. After being projected into vectors, the hierarchical representations of detections become tokens Tdet = {T kdet N },\n\n2 , ..., T kdet\n\n1 , T kdet\n\n3\n\n...frame tframe t+TObject Detector...Hierarchical Part-WholeAttentionImagesObjectsTokensGlobal Association.....................Global Association MatrixFeature EncoderconcatPart-aware feature pyramidTokenTkCross-AttPartsBodyUnionUnder review as a conference paper at ICLR 2023\n\nwhich are also terms as “object query” in previous works (Sun et al., 2020; Meinhardt et al., 2021). Concatenating the tokens makes Qdet ∈ RN ×D, where D is the feature dimension. If we aim to associate the new-coming detections with existing trajectories, we also need the tokens to represent the existing M trajectories, i.e., Ttraj = {T ktraj M }. The transformer has shown good power to generate more discriminative feature tokens for trajectories by iterative query passing (Zeng et al., 2021) or long-time feature buffering (Cai et al., 2022). But to make our method simple, we directly project the hierarchical representation of objects on existing trajectories to represent the trajectories. Given a historical horizon H to backtrack the objects on the previous time steps of a trajectory, we represent a trajectory, T ktraj j ∈ RH×D. The track query is the combination of the feature tokens of detections within the historical horizon on the corresponding trajectory.\n\n, with the “track query” Qtraj\n\n2 , ..., T ktraj\n\n1 , T ktraj\n\nj\n\nj\n\nas S(Qtraj\n\nj , Qdet) ∈ RH×N .\n\nGlobal Association. By cross-attention, we could get the association score between the set of detections and the trajectory T ktraj In practice, because we aim to associate between all M trajectories and N detections, we perform the cross-attention on all object queries and track queries at the same time, namely S(Qtraj, Qdet) ∈ RHM ×N . By averaging the score on the H frames selected from the historical horizon, we could finally get the association score between detections and trajectories as S ∈ RM ×N . Then, we need to make sure that a trajectory will never be associated with more than one object from the same frame. We normalize the association scores between a trajectory and objects from the same time step by softmax. So the normalized association score between the j-th trajectory and the i-th detection is exp(Sj,i)\n\nP (Masso\n\nj,i = 1|Qdet, Qtraj) =\n\n(cid:80)\n\nk∈{1,2,...,N } 1[tk = ti]exp(Sj,k)\n\n,\n\n(1)\n\nwhere the binary indicator function 1[tk = ti] indicates whether the i-th detection and the k-th detection are on the same time step. Masso ∈ R(M +1)×N is the final global association matrix. Its dimension is of (M + 1) × N because each detection can be associated with an “empty trajectory” to start a new track in practice. The query of the “empty trajectory” is represented by a query randomly drawn from previous unassociated tokens during training. Also, after the association, unassociated trajectories will be considered absent on the corresponding frames. In such a fashion, we can train over a large set of detections and trajectories in parallel and also conduct inference in an online manner by setting O as the set of the detections only from the new-coming frame.\n\n3.2 HIERARCHICAL PART-WHOLE ATTENTION\n\nFinally, we come to the details about constructing hierarchical part-whole visual representations. We name this process hierarchical part-whole attention as we use the attention mechanism to gather and process information from different levels in the hierarchy, which is illustrated in the right-hand half of Figure 1. We design this representation because we think there are three perspectives to describe the existence of an object and its identification over other objects: the compositional, the semantic, and the contextual. Correspondingly, we think the body part patches, the full object body, and the union of the occupancy of objects with interaction provide the knowledge from the three perspectives respectively. The insight behind this module is what we would like the most to deliver in this work.\n\nHierarchy Construction. We represent a detected object by a quintuple, i.e., O = [x, y, w, h, c], where the first four values describe its bounding box and c is the detection confidence. So its body area is B = [x, y, x + w, y + h]. Next, we divide the body into multiple sub-regions (parts). By default, similar to what ViT (Dosovitskiy et al., 2020) does upon images, we divide the bounding boxes into 2 × 2 bins, making a set of body parts as P = {P1, P2, P3, P4}. On the other hand, from a global scope, there are other targets interacting with O which are highly likely to be mismatched with O in the association stage. We crop the union area enclosing O and all other targets having overlap with it. We note the union area as U . Till now, we have derives the part-whole hierarchy {P, B, U } in a very straightforward way.\n\nFeature Fusion. Given the part-whole hierarchy, we have to fuse the features from different levels to get the final feature tokens for association. With a feature encoder, we can extract the CNN features from them as FP ∈ R4C×H×W , FB ∈ RC×H×W and FU ∈ RC×H×W . We simply concatenate the features from the first two levels as FP+B ∈ R5C×H×W . Then, by a two-layer\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nprojection network, we gain projected features VP +B ∈ R5×D. We also apply the projection to the union area features and get VU ∈ RD. Finally, we perform cross-attention between VP +B and VU and forward the output to an MLP network to get the tokens of shape R5×D. Before being forwarded to the global association stage, the tokens would be projected to the uniform dimension of D.\n\n3.3 TRAINING AND INFERENCE\n\nThe method we implement is a baseline model without complicated designs on “queries”. We simply use the hierarchical part-whole features of detected objects to serve as the representations of both detections and trajectories. And during training, we can associate between detections in the sampled video clips or between detections and existing trajectories. These two schemes of associations thus are implemented as the same and share all model modules. During inference, to keep the process online, we only perform association between detections from the new-coming frame and existing trajectories. We realize this by iterating a sliding window with the stride of one frame.\n\nTraining. We train the association module by maximizing the likelihood of associating detections belonging to the same ground truth trajectory as expressed in Eq. 1. But Eq. 1 happens on one time step ti only. To speed up training, we calculate the association score on all T frames of the sampled video clip at the same time and maximize the likelihood of the association aligned with the ground truths globally in the time window. The objective thus turns to\n\nt+T (cid:89)\n\nq=t\n\nP (Masso j,τ j\n\nq\n\n= 1|Qdet, Qtraj),\n\n(2)\n\nwhere τ j q is the ground truth index of the detection which should be associated with the j-th trajectory on the time step q. Therefore, by traversing the association of all trajectories, the training objective becomes the negative log-likelihood loss\n\nLasso = −\n\nM (cid:88)\n\nt+T (cid:88)\n\nj=1\n\nq=t\n\nlogP (Masso j,τ j\n\nq\n\n= 1|Qdet, Qtraj).\n\n(3)\n\nOn the other hand, trajectories can also be absent on some time steps because of occlusion or target disappearance. So similar to the practice of DETR (Carion et al., 2020) for detection and GTR (Zhou et al., 2022) for tracking, Eq. 3 has included the situation of associating a trajectory with “empty”. Moreover, the main reason why mismatch happens is the features of objects of different identities being indiscriminative. Therefore, to encourage the representations from objects of different identities to be distinguishable, we design a feature discrimination loss in the form of triplet loss as\n\nLfeat = max(0,\n\nNP min u=1\n\n||Att(f (FPu ), f (FB))−f (FB)||2−||Att(f (FB), f (F bg\n\nU ))−f (FB)||2+α), (4)\n\nwhere f (·) is the shared projection layers to project CNN features to feature vectors and NP is the number of part patches (NP = 4 in our default setting). Att(·, ·) is the operation of cross attention to generate attended features. α is the margin to control the distance between positive and negative pairs. FB and FPu (1 ≤ u ≤ NP ) are the extracted features of the body area and the part subregions as explained already. F bg U is the CNN features of the background area in the union box. We obtain the background features by setting the pixels of B in the area of U to be 0 and forward the processed patch of the union area into the feature encoder. We design Eq. 4 to encourage the projection network to pay more attention to the salient area on the body of target objects while less attention to the background area when processing the hierarchical part-whole representations. Also, it encourages the features of the background area in the union box, which probably belongs to another object target, to be distinguishable from the body features. This can be expected to decrease the chance of mismatch between neighboring objects. Finally, the training objective is\n\nL = Lasso + Lfeat + Ldet, (5) where Ldet is an optional detection loss. In our default implementation, we finetune the detector at the same time when training the association modules.\n\nInference. We adopt the traditional sliding-window style to realize online influence. With a window size T = 24 and stride 1, we start from the first frame of the input video. On the first frame, every detection is initialized as an original trajectory. In each time window, we would generate\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ntrajectories by detections within it. Then we use the association score in Eq. 1 to associate these trajectories with existing trajectories outside this time window. By averaging the detection-trajectory alongside detections of a trajectory, we get the trajectory-trajectory association scores, whose negative value serves as the entries in the cost matrix for the association assignment. And we adopt Hungarians matching to make sure the one-to-one mapping. Only when a pair of trajectories has the association score higher than a threshold β = 0.3, they are eligible to be associated. All unassociated detections on the new-coming frames will start new tracks.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENT SETUPS Datasets. We conduct quantitative experiments on multiple multi-object tracking datasets, including MOT17 (Milan et al., 2016), MOT20 (Dendorfer et al., 2020) and DanceTrack (Sun et al., 2021). We focus on pedestrian tracking in this paper so pedestrian is the only category of objects of interest on all datasets. MOT17 and MOT20 are the classic and popular datasets in the area of pedestrian tracking but their scales are relatively small and have no official validation sets. DanceTrack, on the contrary, is a recently proposed dataset that is of a much larger scale and provides an official validation set with no overlap with the training set. DanceTrack focuses on the scenarios where targets are in the foreground so detection is not considered as the bottleneck as it is on MOT20. And DanceTrack mainly contains videos where targets have heavy occlusion, complex motion patterns, and similar appearances so it provides a good platform to study the robustness of the tracking algorithm.\n\nEvaluation Metrics. The popular CLEAR evaluation protocol (Bernardin & Stiefelhagen, 2008) is based on single-frame-wise matching between the ground truth and predictions. This makes the metric emphasize single-frame detection quality rather than cross-frame association performance. MOTA, the main metric of CLEAR protocol, is also biased to the detection quality. To provide a more accurate sense of association performance in tracking, we mainly adopt the more recent HOTA (Luiten et al., 2021) metric set where the metric is calculated by the video-level association between ground truth and predictions. In the set of metrics, AssA emphasizes the association performance, and DetA stresses on the detection quality. HOTA is the main metric by taking both detection and association quality into consideration.\n\nImplementation. We follow the common practice (Sun et al., 2020; Zeng et al., 2021; Cai et al., 2022) to use ResNet-50 (He et al., 2016) as the backbone network, which is pretrained on Crowdhuman (Shao et al., 2018) dataset first. Though advanced detector (Zhang et al., 2021a) is demonstrated as a key to boosting tracking performance, we want our contribution to be more from the improvement of the association stage. Therefore, on MOT17, we follow the practice of another transformerbased global association tracking method GTR (Zhou et al., 2022) to use the classic CenterNet Zhou et al. (2019; 2020) as the detector and all training details are aligned with it to make fair comparisons with this close baseline method. The CenterNet detector is pretrained together with the backbone on Crowdhuman to align with the common practice on this dataset. For the fine-tuning of association modules, we use a 1:1 mixture of MOT17 and Crowdhuman for MOT17. We fine-tune with only the MOT20 training set for evaluation on MOT20. For DanceTrack, we use its official training set as the only training set during finetuning. The image size is set to be 1280 × 1280 during training and the test size is 1560 for the longer edge during the test. During finetuning, the detector head is also finetuned as mentioned already. The training iterations are set to be 20k on MOT17/MOT20 and 80k on DanceTrack. We use BiFPN (Tan et al., 2020) for the feature upsampling. For the implementation of the transformer, we follow the practice of Zhou et al. (2022) to use a stack of two layers of “Linear + ReLU” as the projection layers and one-layer encoders and decoders. We use AdamW (Loshchilov & Hutter, 2017) optimizer for training whose base learning rate is set to be 5e-5. The length of the video clip is T = 8 for training and T = 24 for inference in a sliding window. We use 4 × V100 GPUs as the default training device following some previous practice (Zhou et al., 2022; Zeng et al., 2021) but we will see that even using only one RTX 3090 GPU for training, our method can also achieve good performance. The training on MOT17 or MOT20 takes only 4 hours and the training on DanceTrack takes 11 hours.\n\n4.2 BENCHMARK RESULTS\n\nWe benchmark our proposed method with existing methods now. The results on the MOT17-test dataset are shown in Table 1. HiPWA achieves the highest HOTA score among all transformer-\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Results on MOT17 test set with the private detections. Bold numbers indicate the overall best result and underlined numbers are the best transformer-based results.\n\nTracker\n\nFairMOT (Zhang et al., 2021b) Semi-TCL (Li et al., 2021) CSTrack (Liang et al., 2020) GRTU (Wang et al., 2021a) QDTrack (Pang et al., 2021) MAA (Stadler & Beyerer, 2022) ReMOT (Yang et al., 2021) PermaTr (Tokmakov et al., 2021) TransMOT (Chu et al., 2021) ByteTrack (Zhang et al., 2021a)\n\nTransCt (Xu et al., 2021) TransTrk (Sun et al., 2020) MOTR (Zeng et al., 2021) TrackFormer (Meinhardt et al., 2021) GTR (Zhou et al., 2022) MeMOT (Cai et al., 2022) HiPWA (Ours)\n\nHOTA↑ AssA↑ MOTA↑ IDF1↑\n\nFP(104\n\n)↓ FN(104\n\n)↓ IDs↓\n\n59.3 59.8 59.3 62.0 53.9 62.0 59.7 55.5 61.7 63.1\n\n54.5 54.1 57.2 -\n59.1 56.9 60.8\n\n58.0 59.4 57.9 62.1 52.7 60.2 57.1 53.1 59.9 62.0\n\n73.7 73.3 74.9 74.9 68.7 79.4 77.0 73.8 76.7 80.3\n\n72.3 73.2 72.6 75.0 66.3 75.9 72.0 68.9 75.1 77.3\n\nTransformer-based Methods 62.2 63.5 68.4 63.9 75.1 69.0 75.7\n\n73.2 75.2 71.9 65.0 75.3 72.5 75.4\n\n49.7 47.9 55.8 -\n57.0 55.2 60.7\n\n2.75 2.29 2.38 3.20 2.66 3.73 3.32 2.90 3.62 2.55\n\n2.31 5.02 2.11 7.44 2.68 3,72 2,45\n\n11.7 12.5 11.4 10.8 14.7 7.77 9.36 11.5 9.32 8.37\n\n12.4 8.64 13.6 12.4 10.9 11.5 10,8\n\n3,303 2,790 3,567 1,812 3,378 1,452 2,853 3,699 2,346 2,196\n\n4,614 3,603 2,115 3,528 2,859 2,724 2,879\n\nFrag↓\n\n8,073 8,010 7,668 1,824 8,091 2,202 5,304 6,132 7,719 2,277\n\n9,519 4,872 3,897 -\n- -\n3,029\n\nTable 2: Results on MOT20 test set with the private detections. Bold numbers indicate the overall best result and underlined numbers are the best transformer-based results.\n\nTracker\n\nHOTA↑\n\nAssA↑\n\nMOTA↑\n\nIDF1↑\n\nFP(104\n\n)↓\n\nFN(104\n\n)↓\n\nFairMOT (Zhang et al., 2021b) CSTrack (Liang et al., 2020) GSDT (Wang et al., 2021b) RelationT (Yu et al., 2021) MAA (Stadler & Beyerer, 2022) ByteTrack (Zhang et al., 2021a) OC-SORT (Cao et al., 2022)\n\nTransCt (Xu et al., 2021) TransTrk (Sun et al., 2020) MeMOT (Cai et al., 2022) HiPWA (Ours)\n\n54.6 54.0 53.6 56.5 57.3 61.3 62.1\n\n43.5 48.5 54.1 53.0\n\n54.7 54.0 52.7 55.8 55.1 59.6 62.0\n\n61.8 66.6 67.1 67.2 73.9 77.8 75.5\n\n67.3 68.6 67.5 70.5 71.2 75.2 75.9\n\nTransformer-based Methods 37.0 45.2 55.0 51.1\n\n58.5 65.0 63.7 65.8\n\n49.6 59.4 66.1 64.4\n\n10.3 2.54 3.19 6.11 2.49 2.62 1.80\n\n6.42 2.72 4,79 3.64\n\n8.89 14.4 13.5 10.5 10.9 8.76 10.8\n\n14.6 15.0 13.8 13.7\n\nIDs↓\n\n5,243 3,196 3,131 4,243 1,331 1,223 913\n\n4,695 3,608 1,938 3,948\n\nbased methods. But our method only achieves a comparable MOTA score with TransTrack (Sun et al., 2020) and GTR (Zhou et al., 2022), suggesting the superior part of HiPWA does not lie in the detection stage. The higher AssA score of our method also demonstrates its superior association performance.\n\nMOT20 is a challenging dataset by containing scenes of crowded pedestrian flows. We report the results on the MOT20-test set in Table 2. Though HiPWA shows better performance than MeMOT (Cai et al., 2022) on MOT17, its performance is inferior on MOT20. This is probably related to the heavy and frequent occlusion on MOT20. It is common on MOT20 that a large portion of pedestrians’ bodies is occluded for a long time. If the occlusion period is longer than the horizon of associating existing trajectories and new-coming detections, HiPWA will be likely to fail. On the other hand, the much longer temporal buffer of object appearance history maintained by MeMOT turns out more effective in such scenarios. However, we note that we design HiPWA with the main goal of demonstrating the hierarchical part-whole representation and choosing the most naive implementation for association heads to make it a computationally efficient baseline model. In the contrast, MeMOT requires 8×A100 GPUs for training to support the long-time history context buffering (22 frames v.s. 8 frames by HiPWA ) uses COCO (Lin et al., 2014) dataset as the additional pretraining data.\n\nNext, we come to the benchmark on the DanceTrack dataset in Table 3. HiPWA achieves comparable performance with the best transformer-based methods. The association of HiPWA is inferior to MOTR (Zeng et al., 2021). MOTR has carefully designed global association and optimization modules. The global collective loss and query interaction module to propagate information frame by frame proposed by MOTR show good effectiveness. However, as a side-effect, its training and inference speed is much slower due to the heavy architecture. For example, training on MOT17 takes MOTR 2.5 days for MOTR on 8×V100 GPUs while only 4 hours on 4×V100 GPUs for our proposed method. And the inference speed is 6.3FPS for MOTR while 17.2FPS for our method on the same machine (V100 GPU). Compared to the close baseline GTR (Zhou et al., 2022), HiPWA\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The upper line shows the results by HiPWA on three randomly sampled frames of a video in the DanceTrack-test set. The video is challenging to show camera motion, heavy occlusion, nonlinear motion, and the crossover among targets at the same time. The bottom line shows results on a MOT20-test video where the pedestrians are in the crowd and heavily occluded.\n\nachieves a more significant gap of outperforming on DanceTrack. Such an observation suggests our proposed part-whole hierarchical representation can be more powerful when the occlusion is heavy.\n\nGiven the results shown on the three benchmarks, we have demonstrated the effectiveness of our proposed HiPWA to be comparable to the state-of-the-art transformer-based multi-object tracking algorithms with a lightweight design. It builds a new baseline for future research in this line of works. The commonly adopted techniques of query propagation and iteration (Meinhardt et al., 2021; Sun et al., 2020; Zeng et al., 2021), deformable attention (Sun et al., 2020; Cai et al., 2022) and long-time feature buffering (Cai et al., 2022) are all compatible to be integrated with HiPWA .\n\nTable 3: Results on DanceTrack test set. Bold numbers indicate the overall best result and underlined numbers are the best transformer-based results.\n\nTracker\n\nHOTA↑\n\nDetA↑\n\nAssA↑\n\nMOTA↑\n\nIDF1↑\n\nCenterTrack (Zhou et al., 2020) FairMOT (Zhang et al., 2021b) QDTrack (Pang et al., 2021) TraDes (Wu et al., 2021) ByteTrack (Zhang et al., 2021a) OC-SORT (Cao et al., 2022)\n\n41.8 39.7 45.7 43.3 47.3 55.7\n\n78.1 66.7 72.1 74.5 71.6 81.7\n\nTransformer-based Methods\n\nTransTrk(Sun et al., 2020) MOTR (Zeng et al., 2021) GTR (Zhou et al., 2022) HiPWA (Ours)\n\n45.5 54.2 48.0 52.1\n\n75.9 73.5 72.5 76.3\n\n22.6 23.8 29.2 25.4 31.4 38.3\n\n27.5 40.2 31.9 35.8\n\n86.8 82.2 83.0 86.2 89.5 92.0\n\n88.4 79.7 84.7 86.1\n\n35.7 40.8 44.8 41.2 52.5 54.6\n\n45.2 51.5 50.3 52.7\n\n4.3 ABLATION STUDY\n\nThough we provide the results on multiple benchmarks to show the efficiency and effectiveness of our proposed method, there are many variables in the design. We now ablate their contributions to the overall performance of HiPWA . Many previous works in the multi-object tracking community follow the practice of CenterTrack (Zhou et al., 2020) on MOT17 (Milan et al., 2016) to use the latter half of training video sequences as the validation set. However, this makes the ablation study on the validation set not always convincing because the data distribution of the training set and validation set is too close and the performance gap reflected on the validation set might shrink or even disappear on the test set. Therefore, we turn to DanceTrack (Sun et al., 2021) for the ablation study instead where an independent validation set is provided and of a much larger scale than previous MOT datasets.\n\nIn Table 4 and Table 5, we study the influence of video clip length in the training and inference stage respectively. The result suggests that training the association model with longer video clips\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Results of using different length of video clip during training.\n\nTable 5: Results of using different length of video clip during Inference.\n\nT\n\n6 8\n10 12\n\nHOTA↑\n\nDetA↑\n\nAssA↑ MOTA↑\n\nIDF1↑\n\n47.8 48.1 48.7 49.2\n\n70.0 70.2 70.0 71.1\n\n32.8 33.2 34.0 34.1\n\n81.1 80.6 80.3 82.6\n\n49.7 50.3 51.7 52.0\n\nT\n\n8 16 24 32\n\nHOTA↑\n\nDetA↑\n\nAssA↑ MOTA↑\n\nIDF1↑\n\n47.5 47.9 48.1 47.8\n\n69.8 70.1 70.2 70.1\n\n32.5 32.9 33.2 32.8\n\n80.1 81.4 80.6 81.2\n\n50.3 50.6 50.3 49.8\n\nTable 6: Results on DanceTrack validation set to study the contribution from each level in our hierarchical representations to the association performance.\n\nHOTA↑\n\nDetA↑\n\nAssA↑\n\nMOTA↑\n\nIDF1↑\n\nBody Body + Part Body + Union Body + Part + Union\n\n45.7 46.3 47.3 48.1\n\n69.5 69.5 70.1 70.2\n\n30.3 30.7 32.0 33.2\n\n81.6 80.0 81.2 80.6\n\n48.1 48.1 49.8 50.3\n\nTable 7: Results on DanceTrack validation set with different configurations for multiple training device choices. HiPWA shows good performance even given limited computation resources.\n\nTraining Device\n\nTrain len\n\nImage Size\n\nHOTA↑\n\nDetA↑\n\nAssA↑\n\nMOTA↑\n\nIDF1↑\n\n1x RTX 3090-24GB 1x V100-32GB 4x V100-32GB\n\n6 8\n8\n\n1280 × 1280 1560 × 1560 1280 × 1280\n\n47.8 48.0 48.1\n\n70.0 70.8 70.2\n\n32.8 32.6 33.2\n\n81.1 82.4 80.6\n\n49.7 50.1 50.3\n\ncan continuously improve performance. Limited by the GPU memory, we cannot increase the video clip length to longer than 12 frames here. On the contrary, during the inference stage, increasing the sliding window size does not significantly influence the tracking performance.\n\nThe hierarchical part-whole representation is the the main contribution of our proposed method. Considering that the hierarchical representation gathers information from three levels (Part, Body, Union), we study the contribution of each of them in Table 6. Compared to only using the features extracted from the bounding box (body) area, our hierarchical representation achieves a performance improvement of 2.4 points of HOTA and 2.9 points of AssA. On the challenging DanceTrack dataset, such improvement can be considered significant when they share the same detections. Also, integrating the features of the union area shows better effectiveness than solely integrating the features of body parts. This is probably because the cross attention between object body and union areas can provide critical information to compare object targets with their neighboring objects, which can prevent potential false association among them. On the other hand, the information about body parts is already contained in the object’s body features. By concatenating the part features and body features, we can’t introduce previously missing information pieces significantly.\n\nFinally, as we aim to build a baseline model for future research in this area, we hope the proposed method is more accessible and computationally economic. We try different parameter configurations in Table 7. Even with only a single RTX 3090 GPU for training and inference, its performance is still quite close to our default setting which requires 4 × V100 GPUs. We hope this makes the notorious computation barrier of transformer-based methods not that terrible anymore.\n\n5 CONCLUSION\n\nIn this paper, we propose to build discriminative hierarchical part-whole representations as the visual descriptor for objects in multi-object tracking. The representation is built upon only bounding box annotation and in three levels: Part, Body, and Union. They are designed to provide visual specificity of the object from the compositional, semantic, and contextual perspectives respectively. We further propose to use attention in the transformer to gather and process the visual features. The combination of these two aspects makes our method, namely Hierarchical Part-Whole Attention and HiPWA for short. The results on multiple datasets demonstrate its efficiency and effectiveness. We hope the study of this paper can provide new knowledge in the visual representation of objects and an advanced baseline model to solve multi-object tracking problems.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5167–5176, 2018. 2\n\nKeni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear\n\nmot metrics. EURASIP Journal on Image and Video Processing, 2008:1–10, 2008. 6\n\nJiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia, Zhuowen Tu, and Stefano Soatto. Memot: Multi-object tracking with memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8090–8100, 2022. 2, 4, 6, 7, 8\n\nJinkun Cao, Xinshuo Weng, Rawal Khirodkar, Jiangmiao Pang, and Kris Kitani. Observation-centric sort: Rethinking sort for robust multi-object tracking. arXiv preprint arXiv:2203.14360, 2022. 7, 8\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213–229. Springer, 2020. 2, 5\n\nPeng Chu, Jiang Wang, Quanzeng You, Haibin Ling, and Zicheng Liu. Transmot: Spatial-temporal\n\ngraph transformer for multiple object tracking. arXiv preprint arXiv:2104.00194, 2021. 7\n\nPatrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix ́e. Mot20: A benchmark for multi object tracking in crowded scenes. arXiv preprint arXiv:2003.09003, 2020. 6\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 4\n\nJerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analy-\n\nsis. Cognition, 28(1-2):3–71, 1988. 2\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 6\n\nWei Li, Yuanjun Xiong, Shuo Yang, Mingze Xu, Yongxin Wang, and Wei Xia. Semi-tcl: Semisupervised track contrastive representation learning. arXiv preprint arXiv:2107.02396, 2021. 7\n\nChao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, and Jianxiao Zou. Rethinking the competition between detection and reid in multi-object tracking. arXiv preprint arXiv:2010.12138, 2020. 7\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European\n\nDoll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. conference on computer vision, pp. 740–755. Springer, 2014. 7\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017. 6\n\nJonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taix ́e, and Bastian Leibe. Hota: A higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548–578, 2021. 6\n\nDavid Marr. Vision: A computational investigation into the human representation and processing of\n\nvisual information. MIT press, 2010. 2\n\nTim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer:\n\nMulti-object tracking with transformers. arXiv preprint arXiv:2101.02702, 2021. 2, 4, 7, 8\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAnton Milan, Laura Leal-Taix ́e, Ian Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark\n\nfor multi-object tracking. arXiv preprint arXiv:1603.00831, 2016. 6, 8\n\nJiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasidense similarity learning for multiple object tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 164–173, 2021. 7, 8\n\nShuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: A benchmark for detecting human in a crowd. arXiv preprint arXiv:1805.00123, 2018. 2, 6\n\nDaniel Stadler and J ̈urgen Beyerer. Modelling ambiguous assignments for multi-person tracking in crowds. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 133–142, 2022. 7\n\nPeize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple object tracking with transformer. arXiv preprint arXiv:2012.15460, 2020. 2, 4, 6, 7, 8\n\nPeize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai, Kris Kitani, and Ping Luo. DancearXiv preprint\n\ntracking in uniform appearance and diverse motion.\n\ntrack: Multi-object arXiv:2111.14690, 2021. 6, 8\n\nRamana Sundararaman, Cedric De Almeida Braga, Eric Marchand, and Julien Pettre. Tracking In Proceedings of the IEEE/CVF Conference on Computer\n\npedestrian heads in dense crowd. Vision and Pattern Recognition, pp. 3865–3875, 2021. 2\n\nMingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10781–10790, 2020. 6\n\nPavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10860–10869, 2021. 7\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2\n\nPaul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pp. 7942–7951, 2019. 2\n\nShuai Wang, Hao Sheng, Yang Zhang, Yubin Wu, and Zhang Xiong. A general recurrent tracking framework without real data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13219–13228, 2021a. 7\n\nYongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 13708–13715. IEEE, 2021b. 7\n\nYuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8741–8750, 2021c. 2, 3\n\nMark Weber, Jun Xie, Maxwell Collins, Yukun Zhu, Paul Voigtlaender, Hartwig Adam, Bradley Green, Andreas Geiger, Bastian Leibe, Daniel Cremers, et al. Step: Segmenting and tracking every pixel. arXiv preprint arXiv:2102.11859, 2021. 2\n\nJialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect In Proceedings of the IEEE/CVF conference on\n\nand segment: An online multi-object tracker. computer vision and pattern recognition, pp. 12352–12361, 2021. 8\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, and Cewu Lu. Pose flow: Efficient online\n\npose tracking. arXiv preprint arXiv:1802.00977, 2018. 2\n\nYihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus, and Xavier AlamedaPineda. Transcenter: Transformers with dense queries for multiple-object tracking. arXiv preprint arXiv:2103.15145, 2021. 7\n\nFan Yang, Xin Chang, Sakriani Sakti, Yang Wu, and Satoshi Nakamura. Remot: A model-agnostic\n\nrefinement for multiple object tracking. Image and Vision Computing, 106:104091, 2021. 7\n\nEn Yu, Zhuoling Li, Shoudong Han, and Hongwei Wang. Relationtrack: Relation-aware multiple\n\nobject tracking with decoupled representation. arXiv preprint arXiv:2105.04322, 2021. 7\n\nFangao Zeng, Bin Dong, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. arXiv preprint arXiv:2105.03247, 2021. 2, 4, 6, 7, 8\n\nYifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021a. 6, 7, 8\n\nYifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, 129(11):3069–3087, 2021b. 7, 8\n\nXingyi Zhou, Dequan Wang, and Philipp Kr ̈ahenb ̈uhl. Objects as points.\n\narXiv preprint\n\narXiv:1904.07850, 2019. 6\n\nXingyi Zhou, Vladlen Koltun, and Philipp Kr ̈ahenb ̈uhl. Tracking objects as points. In European\n\nConference on Computer Vision, pp. 474–490. Springer, 2020. 6, 8\n\nXingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp Kr ̈ahenb ̈uhl. Global tracking transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8771–8780, 2022. 2, 3, 5, 6, 7, 8\n\n12",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes an interesting 'Hierarchical Part-Whole Attention' for multi-object tracking. The proposed module is integrated with transformer network and achieves good performance (comparable or even better results than SOTA mot trackers). The overall training efficiency is also good, i.e., 4 hours on 4*v100 GPUs, while other Transformer based trackers may need days. This paper is well-written and organized, and I believe it will be a good baseline for future works to compare and in-depth development on this framework.\n\n# Strength And Weaknesses\n\nstrength: \n1. the idea of Hierarchical Part-Whole representation of target object in MOT seems interesting; \n2. the combination of Hierarchical Part-Whole Attention and Transformer works well on existing benchmark datasets; \n3. the paper is easy to follow and understand. \n\nweakness: \n1. the source code will be released or not is unclear. it is an interesting idea, but the implementation is also complicated. If the code is not available, maybe it is hard for other researchers to follow. \n2. the running efficiency is not real-time. i.e., not fast enough for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nthis paper is clearly written and the information is enough to reproduce the experiments.\n\n# Summary Of The Review\n\nthe idea is reasonable, \nthe framework is not very complicated for MOT. \nthe training time is acceptable, but the inference is fast; \nthis paper is well-written and easy to follow.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents a novel approach to multi-object tracking (MOT) through a framework called Hierarchical Part-Whole Attention (HiPWA). It introduces a three-level hierarchical representation of objects that includes target body parts, the whole target body, and the union area of targets with overlapping objects. By integrating these hierarchical representations with a spatio-temporal attention mechanism, the proposed method aims to improve tracking performance while being computationally efficient. Experimental results on benchmark datasets such as MOT17 and MOT20 indicate that HiPWA achieves superior tracking accuracy and efficiency compared to existing transformer-based methods, particularly in scenarios with limited temporal context.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative hierarchical representation that reduces the reliance on fine-grained annotations, making it more accessible for practical applications in object tracking. The methodological approach is well-structured, with clear steps outlined for detection, token generation, and global association. Additionally, the results demonstrate a significant performance improvement over state-of-the-art methods, particularly in terms of efficiency. However, a notable weakness is the performance drop in challenging scenarios involving heavy occlusions, which raises concerns about the robustness of the method under diverse conditions. Furthermore, while the ablation studies provide useful insights, additional experimentation with other datasets could strengthen the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with a logical flow of ideas and methodologies that facilitate understanding. The quality of the figures and tables is high, effectively summarizing the experimental results and supporting the claims made by the authors. The novelty of the hierarchical representation and its integration with an attention mechanism is commendable; however, the reproducibility could be improved by providing more detailed information on the training configurations and hyperparameter settings used in the experiments. While the methodology is described in sufficient detail, the lack of comprehensive code availability hampers reproducibility efforts.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to multi-object tracking through the HiPWA framework, demonstrating both efficiency and effectiveness in comparison to existing methods. While the proposed method shows significant potential, particularly in reducing the need for extensive annotations, concerns about robustness in challenging scenarios and reproducibility remain.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Hierarchical Part-Whole Attention (HiPWA), a novel approach for multi-object tracking (MOT) that leverages hierarchical representations to focus on body parts, whole bodies, and union areas to enhance object identification. The methodology utilizes transformers for processing these representations, enabling efficient online tracking without the need for extensive data annotation beyond bounding boxes. The findings demonstrate that HiPWA outperforms existing transformer-based methods on the MOT17 dataset in terms of Higher Order Tracking Accuracy (HOTA) and maintains competitive performance in challenging tracking scenarios such as in the MOT20 and DanceTrack datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative hierarchical representation that reduces reliance on detailed annotations and its computational efficiency, making it suitable for various hardware configurations. Additionally, HiPWA exhibits robustness in challenging scenarios, particularly in environments with heavy occlusion. However, the methodology shows limitations in scenarios involving long occlusions, where it struggles compared to methods that utilize longer temporal histories. Furthermore, while the association performance is commendable, the detection stage could benefit from integration with more advanced detection techniques to enhance overall performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, presenting the methodology and results clearly and concisely. The proposed hierarchical representation is novel and contributes significantly to the field of multi-object tracking. The empirical results are robust and well-supported through extensive experiments across multiple datasets, enhancing the reproducibility of the findings. However, additional details on implementation and hyperparameter tuning could further improve the reproducibility aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in multi-object tracking through the introduction of the HiPWA framework, which effectively leverages hierarchical representations for improved tracking performance. While it excels in association and robustness to occlusion, the method could be enhanced by addressing its limitations in detection performance and long-term occlusion scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach to multi-object tracking (MOT) through a hierarchical representation termed \"Hierarchical Part-Whole Attention\" (HiPWA). The methodology comprises three levels of representation: target body parts, the whole target body, and a union area that includes overlapping objects. By leveraging a spatio-temporal attention mechanism with transformers, HiPWA is designed to improve the discriminative power of object representations while maintaining computational efficiency. The experiments demonstrate that HiPWA achieves superior performance on the MOT17 dataset and comparable results on MOT20 and DanceTrack, emphasizing its effectiveness in real-world scenarios.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative hierarchical approach, which addresses the limitations of existing methods by providing a more informative representation without the need for complex annotations. The use of transformers for global association over multiple frames is a significant contribution that enhances tracking consistency. However, the paper also exhibits weaknesses, particularly in the evaluation of performance on the MOT20 dataset, where HiPWA underperformed relative to MeMOT due to frequent occlusions. Additionally, while the ablation studies demonstrate the benefits of the hierarchical structure, further analysis of the limitations and potential improvements could strengthen the argument for its robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and experimental results. The equations and concepts are explained with adequate detail, allowing for a good understanding of the proposed framework. The novelty of the hierarchical representation in the context of MOT is significant, providing a fresh perspective on the problem. However, reproducibility could be enhanced by providing more detailed descriptions of the experimental setups and hyperparameters used in the training process, as these are critical for others to replicate the results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in multi-object tracking through its hierarchical representation approach. While the methodology and results are largely impressive, the evaluation on certain datasets reveals areas needing improvement. Enhanced clarity in the experimental details would further bolster the contributions of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel hierarchical part-whole representation approach for multi-object tracking, termed HiPWA. This methodology emphasizes the visual specificity of objects by leveraging compositional, semantic, and contextual perspectives. The findings indicate that HiPWA achieves comparable or superior performance on benchmark datasets such as MOT17, MOT20, and DanceTrack while demonstrating enhanced computational efficiency through single-frame processing. However, the paper acknowledges limitations in its ability to handle complex scenarios involving occlusions and dynamic environments.\n\n# Strength And Weaknesses\nThe contributions of the paper are significant, particularly in the development of a hierarchical representation that enhances the understanding of object relationships and tracking efficiency. The computational efficiency of HiPWA is a notable strength, especially in contrast to methods requiring extensive temporal context. However, the reliance on single-frame representations raises concerns regarding the model's effectiveness in scenarios with significant motion or occlusion. Additionally, while the method shows promise, its performance may not generalize across all datasets due to challenges in handling complex tracking situations. The ablation studies provide valuable insights, although the limited testing scope may restrict the comprehensiveness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The quality of writing is high, making the concepts accessible for readers. The novelty of the hierarchical representation is evident, yet the reproducibility may be hampered by the simplistic association strategy employed. Future implementations may benefit from more sophisticated methods to enhance performance while maintaining the advantages of the proposed approach.\n\n# Summary Of The Review\nOverall, the paper presents a promising new methodology for multi-object tracking that balances efficiency with performance. While the hierarchical representation is innovative, the limitations regarding occlusion handling and the need for more sophisticated implementation strategies warrant further exploration in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach for multi-object tracking (MOT) through the Hierarchical Part-Whole Attention (HiPWA) framework, which employs a three-tiered visual representation. It introduces a hierarchical structure that consists of distinct body parts, the complete object, and contextual overlaps to enhance object association. The methodology includes a spatio-temporal attention mechanism that processes these hierarchical features effectively, allowing for computational efficiency and improved tracking performance. Experimental results on datasets such as MOT17 and MOT20 demonstrate that HiPWA achieves superior association metrics while maintaining lower computational costs compared to state-of-the-art methods.\n\n# Strength And Weaknesses\nThe key strengths of the paper lie in its innovative hierarchical framework and the integration of attention mechanisms, which significantly improve tracking performance without extensive computational requirements. The detailed ablation studies provide a solid basis for the effectiveness of each framework component, enhancing the credibility of the findings. However, a potential weakness is the reliance on the datasets used for validation; while results are promising, further testing on a wider variety of scenarios could strengthen the claims of generalizability. Additionally, the paper could benefit from a more in-depth discussion on the limitations of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results. The quality of writing is high, with technical details presented in an accessible manner. The novelty of the approach is significant, as it combines hierarchical representations with attention mechanisms in a way that is not previously explored in the context of MOT. The reproducibility of the results is supported by thorough descriptions of the experimental setup and data sources, although the inclusion of code or implementation details would further enhance this aspect.\n\n# Summary Of The Review\nIn summary, the paper presents a substantial advancement in multi-object tracking through the innovative HiPWA framework, which effectively combines hierarchical representations and attention mechanisms. The results are compelling, and the methodology demonstrates both technical and empirical novelty, establishing a new benchmark for future research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach to adversarial training, termed Hierarchical Adversarial Representation (HAR), which leverages a hierarchical representation of input data to enhance the robustness of neural networks against adversarial attacks. The methodology entails constructing a three-level hierarchy encompassing local features, global features, and contextual features, thus enabling the model to better differentiate between clean and adversarial examples. The findings demonstrate that HAR outperforms existing state-of-the-art methods in robustness while maintaining competitive performance with lower computational overhead.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative hierarchical representation framework, which addresses critical gaps in existing adversarial training literature. The integration of a spatio-temporal attention mechanism is particularly noteworthy as it allows the model to focus on essential features, enhancing robustness. However, weaknesses include the need for more diverse datasets and a broader range of adversarial attacks in the experimental evaluation. Additionally, the theoretical underpinnings of why hierarchical representations improve robustness compared to traditional methods could be more thoroughly explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, with clear explanations of the proposed methodology and its contributions. The quality of the experimental evaluation is commendable, although the lack of diversity in datasets may limit reproducibility and generalizability of the results. The novelty of the hierarchical approach is significant, presenting a new perspective in adversarial training that could stimulate further research in the field.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to adversarial training through hierarchical representations. Its contributions are significant, particularly in enhancing model robustness while maintaining efficiency, although there is room for improvement in the diversity of experimental setups and theoretical insights.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach for multi-object tracking called Hierarchical Part-Whole Attention (HiPWA). It introduces a three-level hierarchical representation (body parts, full body, union area) and incorporates a spatio-temporal attention mechanism using transformers. The authors claim that HiPWA significantly improves tracking performance while reducing computational demands. However, the empirical results indicate that the performance gains over existing methods are marginal, and the reported improvements may not be robust.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of a hierarchical representation that could be relevant for future research and the integration of transformers, which is a current trend in attention mechanisms. However, the weaknesses are notable: the performance improvements are often within the noise of experimental variation, and the claims of superior computational efficiency and global tracking capabilities are overstated. Moreover, the method's limitations in handling occlusions and its questionable generalizability further undermine its contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but the clarity is compromised by overly optimistic claims about the method's performance and novelty. The quality of the experimental design and data reporting raises concerns, as the selective reporting of metrics may obscure a true understanding of the method's effectiveness. Reproducibility is also a concern, given the vagueness surrounding the application of the hierarchical representation in diverse contexts.\n\n# Summary Of The Review\nWhile the paper presents an interesting framework in object tracking with HiPWA, the actual contributions to the field appear to be incremental rather than revolutionary. The claims of superior performance and efficiency are not convincingly supported by the experimental results, leading to a cautious interpretation of the proposed method's significance.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Hierarchical Part-Whole Attention (HiPWA), a novel approach for multi-object tracking that leverages hierarchical representations to enhance tracking performance while being computationally efficient. The methodology involves a global association strategy that processes detections and trajectories within single frames, allowing the model to differentiate objects based on compositional, semantic, and contextual perspectives. Key findings demonstrate that HiPWA outperforms existing transformer-based methods on several benchmark datasets, including MOT17, MOT20, and DanceTrack, achieving high scores in tracking metrics such as HOTA, MOTA, and IDF1.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative use of hierarchical representations, which significantly improve tracking performance with less reliance on fine-grained data annotation. The experimental results across multiple datasets are promising, showcasing HiPWA’s effectiveness in handling occlusions and maintaining high association capabilities. However, the paper also presents weaknesses, particularly in the DanceTrack results where the AssA score indicates potential limitations in association performance. Additionally, while the computational efficiency is highlighted, the paper could further elaborate on the trade-offs between model complexity and performance in different scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology and experimental findings. The results are presented with sufficient detail, and the use of tables facilitates comparison with existing methods. The novelty of the hierarchical approach is a significant contribution, although the paper could improve reproducibility by providing more detailed implementation specifics and hyperparameter settings. Overall, the clarity and quality are commendable, but the reproducibility aspects could be enhanced.\n\n# Summary Of The Review\nOverall, the HiPWA paper presents a compelling and innovative approach to multi-object tracking that demonstrates significant performance gains while maintaining computational efficiency. While the results are impressive, particularly in challenging conditions, there are areas for further exploration, especially regarding association performance in certain datasets.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel tracking approach called HiPWA (Hierarchical Part-Body-Union Association) that leverages a hierarchical representation of objects for improved tracking performance. By focusing on distinct body parts instead of traditional bounding boxes, the authors argue that their method reduces noise and enhances object association in complex environments. The methodology involves employing transformers for processing these hierarchical representations, claiming superior computational efficiency and effectiveness in tracking across various datasets, including MOT17, MOT20, and DanceTrack. The findings suggest that HiPWA outperforms or matches existing state-of-the-art methods, though the generalizability of the results remains unproven.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to object tracking by proposing a hierarchical representation that may improve feature extraction and reduce noise. The use of transformers is also a noteworthy aspect, particularly given their success in other domains. However, several weaknesses are evident. The assumption that hierarchical representations are inherently superior is not thoroughly substantiated, as existing simpler methods may suffice for many applications. The reliance on specific datasets raises questions about the robustness of the findings, and the computational efficiency claimed may come at the cost of performance. Additionally, the design of the feature discrimination loss and the implications of using short temporal contexts for tracking need further validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good; however, some assumptions and methodologies could benefit from more detailed explanations or justifications. The quality of the experimental design is adequate but limited by the chosen datasets, which may not capture the full spectrum of real-world tracking challenges. In terms of novelty, the hierarchical representation is a fresh perspective, but its significance is tempered by the lack of comprehensive comparisons with existing methods. Reproducibility is also a concern, as the generalization of results to other conditions and environments is not well established.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting method for object tracking that challenges conventional approaches, it suffers from several weaknesses in the assumptions made and the generalizability of results. The reliance on specific datasets and the need for further validation of its claims limit the impact of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Hierarchical Part-Whole Attention (HiPWA), a novel approach for multi-object tracking that leverages hierarchical representations to enhance efficiency and robustness. The methodology employs spatio-temporal attention mechanisms via transformers, which enables real-time processing with reduced computational overhead compared to existing methods. The authors demonstrate the effectiveness of HiPWA through extensive experiments on popular datasets such as MOT17, MOT20, and DanceTrack, revealing significant improvements in tracking performance and association accuracy over state-of-the-art techniques.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative use of hierarchical representations, which effectively capture the compositional, semantic, and contextual information of objects. This approach addresses the shortcomings of traditional bounding box representations and enhances the robustness of tracking across frames. The empirical evaluation is comprehensive, showcasing a clear advancement over existing methods. However, a notable weakness is the limited exploration of the computational complexity in practical scenarios, which may affect real-time applications in resource-constrained environments. Additionally, while the ablation studies are insightful, they could benefit from a more detailed discussion on the implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The figures and tables effectively illustrate key points, contributing to the overall clarity of the work. The quality of the writing is high, making it accessible to both experts and non-experts in the field. The novelty of the hierarchical attention mechanism is noteworthy, as it represents a significant departure from traditional methods. Reproducibility is supported through detailed descriptions of the methodology and datasets used, although sharing code and trained models would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in multi-object tracking through the introduction of HiPWA, which utilizes hierarchical representations and transformer-based attention mechanisms. While the contributions are substantial and well-supported by empirical evidence, further discussion on computational efficiency in real-world applications would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing the performance of machine learning models through the integration of advanced attention mechanisms. The authors propose a new framework that leverages these mechanisms to improve model interpretability and efficacy across multiple datasets. The experimental results demonstrate significant improvements in performance metrics compared to baseline models, highlighting the framework's potential to advance the current state of the art in the field.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear motivation and the innovative application of attention mechanisms, which are highly relevant in contemporary machine learning research. The comprehensive experimental setup, utilizing diverse datasets and metrics, adds credibility to the findings. However, the paper lacks thorough justifications for the selected benchmarks and could benefit from a more in-depth comparison with the latest state-of-the-art methods. Additionally, some theoretical aspects of the proposed methodology require further elaboration to strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing quality is generally good, but certain technical terms could be simplified to enhance accessibility for a wider audience. Figures and tables effectively communicate results, yet the inclusion of additional visualizations could further aid in understanding complex concepts. The paper is organized logically, though transitions between sections could be smoother. The novelty of the proposed method is promising, yet clearer differentiation from related works would amplify its significance. Reproducibility may be hindered by the lack of detailed discussions on the methodology and theoretical aspects.\n\n# Summary Of The Review\nThis paper introduces a promising framework that leverages advanced attention mechanisms to enhance machine learning model performance. While the contributions are significant, addressing the clarity of presentation, justifications for benchmarks, and theoretical grounding would strengthen the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel method for multi-object tracking titled Hierarchical Part-Whole Attention (HiPWA). The methodology emphasizes hierarchical representations of objects, differentiating between body parts, whole bodies, and their overlapping areas. By leveraging a spatio-temporal attention mechanism through transformers, HiPWA effectively tracks objects by focusing on specific body regions, reducing reliance on complete object views. The findings demonstrate that HiPWA not only matches the performance of existing state-of-the-art methods but does so with greater computational efficiency across various datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to object tracking through hierarchical representations, which enhances the specificity and accuracy of tracking. The use of transformers to implement a spatio-temporal attention mechanism is a significant contribution, allowing for real-time tracking with reduced computational demands. However, a potential weakness is that the paper could benefit from a more thorough comparison against a broader range of existing methods to establish the robustness of HiPWA comprehensively. Additionally, while the concept of hierarchical representations is well-articulated, the paper could provide deeper insights into the limitations of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly written, with a logical flow from the introduction to the conclusion. The quality of the methodology is high, and the results are presented in a clear manner. The novelty of the hierarchical part-whole representation is significant, as it introduces a fresh perspective in the domain of multi-object tracking. However, the reproducibility of the findings could be improved by providing more detailed descriptions of the experimental setup, datasets used, and evaluation metrics, allowing other researchers to replicate the study effectively.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of multi-object tracking by introducing a novel hierarchical representation approach that combines attention mechanisms with transformer architectures. While the methodology is innovative and the results are promising, the paper would benefit from more comprehensive comparisons and detailed experimental procedures to enhance reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Hierarchical Part-Whole Attention for Object Tracking\" introduces a novel hierarchical representation for multi-object tracking, which consists of three levels: target body parts, the whole body, and the union area of objects. The methodology employs a spatio-temporal attention mechanism based on transformers to facilitate global and online tracking, enabling parallel processing of information across multiple frames. The authors demonstrate the effectiveness of their approach through extensive experiments on several datasets, including MOT17, MOT20, and DanceTrack, showing significant improvements in robustness and computational efficiency compared to existing tracking methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative hierarchical representation, which leverages attention mechanisms to enhance object identification and tracking. The use of multi-level features allows for a more comprehensive visual descriptor, addressing issues related to traditional bounding box limitations. The empirical results demonstrate competitive performance across various metrics, particularly in association metrics, highlighting the method's robustness. However, a potential weakness is the reliance on transformer-based mechanisms, which, despite their advantages, may introduce computational overhead that could limit real-time application in certain contexts. Additionally, while the ablation study provides insights into the contribution of each hierarchical level, further exploration of the impact of varying training configurations could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its objectives, methodology, and experimental results. The clarity of explanations surrounding the hierarchical representation and attention mechanisms contributes positively to the reader’s understanding. The quality of the writing is high, with a logical flow that guides the reader through the research. In terms of novelty, the introduction of a hierarchical part-whole representation is a significant contribution to the field of object tracking, which is often limited by traditional methods. The reproducibility of the results is supported by the detailed description of datasets and evaluation metrics, although providing code or more detailed implementation guidance would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in multi-object tracking through the introduction of a hierarchical representation method that effectively balances complexity and computational efficiency. While there are minor concerns regarding the computational demands of transformer-based approaches, the empirical results demonstrate significant improvements over existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Object Tracking by Hierarchical Part-Whole Attention\" introduces a novel approach to multi-object tracking by leveraging a hierarchical part-whole representation of objects. The proposed method, HiPWA, employs an attention mechanism that allows for improved visual specificity in distinguishing objects. The methodology is structured into three main components: global association, hierarchical part-whole attention, and training/inference procedures. The experimental results demonstrate that HiPWA outperforms existing state-of-the-art tracking methods across several benchmark datasets, including MOT17, MOT20, and DanceTrack.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its well-defined problem statement and the innovative use of hierarchical part-whole representation, which addresses a critical gap in current object tracking approaches. The thorough literature review contextualizes the contribution effectively, grounding the proposed method in existing research. The experimental design is robust, with appropriate datasets and evaluation metrics that validate the effectiveness of HiPWA. However, a potential weakness is the limited diversity of the datasets used, which may affect the generalizability of the findings. Additionally, while the ablation studies are insightful, further exploration of edge cases could strengthen the argument for the method's applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to readers. The methodology is detailed, allowing for reproducibility; however, some implementation specifics could be elaborated to enhance clarity further. The novelty of the hierarchical attention mechanism is significant, providing a fresh perspective on multi-object tracking. Overall, the quality of writing and presentation is high, with clear figures and tables that effectively communicate the findings.\n\n# Summary Of The Review\nIn summary, this paper presents a significant advancement in multi-object tracking through the introduction of the HiPWA method, which effectively utilizes hierarchical representations and attention mechanisms. The empirical results demonstrate that the proposed approach is competitive with existing methods, although some concerns regarding dataset diversity and edge case evaluations remain.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents HiPWA (Hierarchical Part-Whole Attention), a novel methodology for multi-object tracking (MOT) that leverages hierarchical representations to enhance tracking accuracy and robustness. The authors propose a tri-level representation framework—comprising body parts, whole targets, and union areas—which is integrated with a spatio-temporal attention mechanism using transformer architecture. The empirical findings demonstrate that HiPWA achieves superior performance on standard MOT benchmarks (MOT17, MOT20, DanceTrack) compared to existing transformer-based methods, particularly in challenging conditions such as occlusion.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to hierarchical representations that address the limitations of traditional bounding box methods by reducing noise from background and overlapping objects. The methodology is well-structured, moving from detection to global association in a clear manner. However, a notable weakness is the performance drop in scenarios with significant occlusions, particularly in the MOT20 dataset, indicating that the model's temporal horizon may require further optimization. Additionally, while the empirical results are promising, a more extensive analysis of the computational costs associated with the hierarchical approach would enhance the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a coherent structure that facilitates understanding of the proposed methodology and its applications. The novel aspects of the hierarchical representation are clearly articulated, and the empirical evaluation is robust, providing convincing evidence of the model's effectiveness. However, details regarding hyperparameter settings and training procedures could be elaborated to ensure reproducibility. The references cited are relevant and provide a solid foundation for the proposed work, reinforcing its novelty within the context of existing literature.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in multi-object tracking through the HiPWA method, offering a compelling hierarchical approach that improves tracking performance in various contexts. Although the results are promising, particularly in noisy environments, the model's limitations under heavy occlusion warrant further investigation. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a new method called Hierarchical Part-Whole Attention (HiPWA) aimed at improving multi-object tracking (MOT) by leveraging hierarchical representations within a transformer architecture. The authors claim that their approach provides computational efficiency while enhancing tracking performance. However, the experimental results primarily focus on pedestrian tracking across a limited number of datasets, and the method does not demonstrate significant improvements over existing techniques, particularly in terms of widely accepted metrics like MOTA. \n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to integrate hierarchical representations into the transformer framework, a step that may contribute to future research in MOT. However, the main weaknesses are pronounced: the lack of novelty in the proposed method, as it largely reiterates existing concepts; exaggerated claims about computational efficiency; and insufficient experimental validation, which is limited both in scope and depth. The evaluation metrics used do not adequately capture true performance improvements, further undermining the conclusions drawn.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by vague implementation details, which hinder reproducibility and the ability to assess the method's true performance. The novelty is questionable, as the hierarchical representation does not convincingly offer advantages over traditional bounding box methods. The quality of the experiments is limited, lacking comprehensive ablation studies that would clarify component contributions to overall performance.\n\n# Summary Of The Review\nOverall, the paper proposes a method that lacks significant novelty and fails to deliver compelling empirical results. The claims of computational efficiency and performance improvements are not sufficiently substantiated, raising doubts about the method's applicability in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the Hierarchical Part-Whole Attention (HiPWA) method, a novel approach to multi-object tracking that employs a three-level hierarchical representation comprising body parts, whole bodies, and union areas. This innovative methodology leverages the spatio-temporal attention mechanism of transformers, allowing for robust and efficient global tracking in dynamic environments. The experimental results demonstrate that HiPWA outperforms several state-of-the-art transformer-based methods across multiple datasets, showcasing its computational efficiency, resilience to occlusions, and potential for real-world applications.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach and the effective use of hierarchical representations, which significantly enhance object distinction based on compositional and contextual information. The integration of transformer mechanisms further contributes to the robustness and efficiency of the method, allowing for real-time processing. Additionally, the computational efficiency and impressive training times make HiPWA accessible for a wide range of applications. However, the paper may benefit from a more detailed discussion of the limitations and potential drawbacks of the proposed approach, especially in scenarios that involve extreme occlusions or rapid object movements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, clearly articulating the methodology, experiments, and results. The quality of the writing is high, providing sufficient detail for reproducibility. The novelty of the hierarchical representation combined with transformer mechanisms is a significant contribution to the field, and the results are compelling. However, the reproducibility aspect could be strengthened by including more comprehensive implementation details and code availability to facilitate further research.\n\n# Summary Of The Review\nOverall, the Hierarchical Part-Whole Attention method represents a significant advancement in multi-object tracking, offering an efficient, robust, and versatile solution. The paper's clear articulation of methodology and findings, coupled with strong empirical results, establishes HiPWA as a leading approach in the field and encourages future research in this area.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework for multi-object tracking called Hierarchical Part-Whole Attention (HiPWA). It introduces a hierarchical representation of objects that enhances tracking accuracy by leveraging a spatio-temporal attention mechanism. The methodology integrates cognitive theories of visual processing, emphasizing the importance of compositional, semantic, and contextual perspectives in object representation. The findings suggest that HiPWA improves object identification and reduces mismatches by structuring representations into parts, whole bodies, and overlapping regions, thereby facilitating more efficient computational processes.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical grounding in cognitive models, which provide a robust framework for understanding object tracking. The hierarchical structure proposed is innovative and aligns with established cognitive theories, enhancing the potential for improved tracking accuracy. However, a notable weakness is the lack of comprehensive empirical validation; the paper could benefit from more extensive experiments to substantiate the claims made regarding the efficacy of HiPWA compared to existing methods. Additionally, while the theoretical implications are well-articulated, practical implementation details are somewhat sparse.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, making it accessible to readers familiar with multi-object tracking and cognitive theories. The quality of the writing is high, with a coherent presentation of ideas and arguments. The novelty of the approach is significant, particularly in its integration of various theoretical perspectives on object representation. However, reproducibility may be limited due to the insufficient detail in the methodology section, which could challenge other researchers aiming to implement the HiPWA framework.\n\n# Summary Of The Review\nOverall, the paper offers a compelling theoretical framework for multi-object tracking through its HiPWA approach, rooted in cognitive models. While it demonstrates strong potential for advancing the field, the empirical validation and methodological details require further development to enhance its practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel model called Hierarchical Part-Whole Attention (HiPWA) for multi-object tracking, which integrates hierarchical representations with transformer-based attention mechanisms. The methodology involves constructing hierarchical representations of objects, including body parts and their union areas, followed by a sophisticated feature extraction process using a pretrained ResNet-50 backbone. The findings demonstrate that HiPWA achieves competitive performance on standard benchmarks (MOT17, MOT20, DanceTrack), emphasizing computational efficiency and resource requirements.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to leveraging hierarchical representations and attention mechanisms, which enhances the model's ability to track multiple objects effectively. The detailed methodology, including the use of a sliding window for online inference and cross-attention for association, is well articulated. However, a notable weakness is the lack of explicit code availability, which may hinder reproducibility. Additionally, while the paper provides extensive implementation details, it could benefit from a more explicit discussion of the broader implications and contributions of the research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, with a clear structure that outlines the methodology and experimental setup. The quality of writing is good, though it tends to focus heavily on implementation specifics. The novelty of combining hierarchical representations with transformers for multi-object tracking is significant, although the paper does not sufficiently discuss the implications of these findings in the broader context of the field. Reproducibility is a concern due to the absence of a code repository, which poses challenges for other researchers wishing to validate the results or build upon the work.\n\n# Summary Of The Review\nOverall, the HiPWA model presents a promising advancement in multi-object tracking through its hierarchical attention mechanisms. While the methodology is robust and the results are competitive, the lack of code availability and a broader discussion of implications limit the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Hierarchical Part-Whole Attention (HiPWA), a novel approach for multi-object tracking that aims to improve computational efficiency while maintaining performance. The methodology centers on a hierarchical representation termed \"Part-Body-Union,\" which the authors claim enhances object distinction and tracking robustness. The experiments demonstrate HiPWA's performance on the DanceTrack dataset, where it reportedly achieves competitive results compared to state-of-the-art transformer-based methods. However, the evaluations rely heavily on HOTA scores and do not adequately address the potential trade-offs in robustness and accuracy under various conditions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to hierarchical representations within the tracking domain, which could provide a framework for future research. However, several weaknesses detract from its overall impact, including an overreliance on established concepts without sufficient acknowledgment, a lack of rigorous comparative analysis against existing models under similar conditions, and inadequate justification for claims of robustness and efficiency. The evaluation metrics used also raise concerns regarding their appropriateness for measuring true tracking performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the novelty of HiPWA is diminished by its apparent dependence on prior work and a lack of detailed comparisons. The experimental setup does not provide enough information on reproducibility, particularly concerning the computational environment and resource allocations, which limits its practical applicability. While the hierarchical approach is conceptually appealing, the lack of robust empirical validation and direct comparisons undermines its perceived originality.\n\n# Summary Of The Review\nIn summary, while the HiPWA method introduces interesting concepts for multi-object tracking, it suffers from a lack of rigorous comparative analysis and insufficient empirical validation. The claims made regarding its advantages over existing methods are not well-supported, leading to questions about its overall contribution to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to object tracking through a Hierarchical Part-Whole Attention (HiPWA) mechanism. The authors propose a methodology that leverages both local and global contextual information to enhance tracking robustness in real-time scenarios. Their findings indicate that HiPWA significantly improves tracking accuracy compared to existing methods, particularly in challenging environments with occlusions and rapid object movements.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to integrating part-whole relationships into the attention mechanism, which demonstrates a clear improvement in tracking performance. The experimental results are well-documented, showing a comprehensive evaluation against state-of-the-art methods. However, the paper suffers from several clarity issues, including inconsistent terminology and typographical errors that may hinder reader comprehension. Additionally, the lack of detailed information on training parameters limits reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the core ideas of the paper are promising, the clarity of presentation is undermined by several formatting inconsistencies and typographical errors. The novelty is strong, as the introduction of HiPWA is a unique contribution to the field. However, the paper's reproducibility is questionable due to the insufficient detail on experimental setup and training configurations, which are critical for replication of results.\n\n# Summary Of The Review\nIn summary, the paper introduces a promising approach to object tracking that leverages hierarchical attention mechanisms. Despite its contributions, the clarity of the writing and reproducibility of results need significant improvements. Overall, the paper has strong potential but requires revision to enhance its accessibility and rigor.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to multi-object tracking through the development of a hierarchical representation framework termed HiPWA (Hierarchical Part-Whole Attention for Tracking). The methodology emphasizes the integration of compositional, semantic, and contextual information to enhance tracking performance across multiple datasets. Key findings indicate that HiPWA achieves competitive results in terms of efficiency and accuracy, though the paper does not extensively explore applications beyond tracking, such as in object detection or instance segmentation.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to hierarchical representations and its demonstration of efficiency through ablation studies on various datasets. However, the paper has notable weaknesses, including a limited exploration of the method's applicability in complex environments (e.g., varying lighting or occlusions) and the lack of discussion on potential integrations with contextual information, such as temporal dynamics. Additionally, the reliance on bounding box annotations restricts its generalizability, and the implications of performance trade-offs under different computational constraints are not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally strong, with a well-structured presentation of ideas and methodology. The quality of the results is supported by empirical evidence; however, the novelty could be questioned given the narrow focus on multi-object tracking without consideration for broader applications. Reproducibility might be a concern as the paper does not provide sufficient details regarding dataset variations or the experimental setup that would allow for easy replication of results.\n\n# Summary Of The Review\nOverall, the paper introduces a promising method for multi-object tracking using hierarchical representations, but it fails to fully leverage its potential by restricting its focus to a narrow application area and not addressing several practical implications. While the findings are compelling, the lack of broader exploration and contextual integration limits the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces Hierarchical Part-Whole Attention (HiPWA), a novel approach to multi-object tracking (MOT) that leverages hierarchical representations to improve tracking accuracy. The methodology employs spatio-temporal attention mechanisms to create a combined representation of target body parts, whole bodies, and overlapping areas. Experimental results across several datasets (MOT17, MOT20, and DanceTrack) indicate that HiPWA achieves statistically significant improvements in performance metrics, particularly in HOTA, although specific statistical tests to validate these claims are not provided.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to object representation and its potential computational efficiency compared to existing transformer-based methods. The hierarchical representation model is well-grounded in cognitive theories, enhancing its theoretical appeal. However, the paper lacks rigorous statistical validation of the claims made regarding performance improvements, as specific tests for statistical significance are not detailed. This omission raises questions about the robustness of the results presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, making it accessible for readers familiar with the field. However, the lack of explicit statistical testing details detracts from the overall quality of the claims. While the novel approach to hierarchical representation is a significant contribution, the reproducibility of results may be hindered by insufficient methodological detail regarding statistical validation. More comprehensive experimental documentation would enhance the clarity and reproducibility of the findings.\n\n# Summary Of The Review\nOverall, the paper contributes a novel method for multi-object tracking that shows promise in improving performance through hierarchical representations. However, the lack of rigorous statistical validation undermines the strength of the claims made regarding the effectiveness of the proposed approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel method called Hierarchical Part-Whole Attention (HiPWA) aimed at improving object tracking, particularly in pedestrian tracking scenarios. The methodology incorporates a hierarchical representation to enhance attention mechanisms, with a specific focus on the relationships between parts and wholes of objects. While the findings indicate that HiPWA performs comparably to state-of-the-art methods, it does not consistently outperform them across all evaluation metrics, particularly in terms of Multiple Object Tracking Accuracy (MOTA).\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its introduction of the feature discrimination loss, which is a novel contribution that may positively influence tracking performance. Additionally, the method's computational efficiency relative to some transformer-based approaches is a notable advantage. However, significant weaknesses are evident, including its limited effectiveness in handling long-term occlusions, as demonstrated by subpar performance in datasets with heavy occlusions like MOT20. The model's reliance on bounding box annotations restricts its application to scenarios requiring more granular object tracking. Furthermore, the experimental focus solely on pedestrian tracking datasets limits the generalizability of results to different object classes or conditions, such as varying lighting or weather.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, but certain aspects could benefit from further elaboration, particularly regarding the implications of the feature discrimination loss and the potential for advanced query propagation techniques. While the novelty of the feature discrimination loss is acknowledged, the paper lacks a thorough exploration of how different configurations of this loss could impact performance. Reproducibility may be a concern due to the absence of exploration into biases present in the datasets used, which could affect overall validity.\n\n# Summary Of The Review\nOverall, the proposed HiPWA method contributes an interesting perspective to object tracking, particularly with its hierarchical approach and feature discrimination loss. However, its limitations in handling occlusions and the narrow focus on pedestrian tracking datasets raise questions about its broader applicability and effectiveness in more diverse tracking scenarios.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper \"Object Tracking by Hierarchical Part-Whole Attention\" proposes a novel approach to multi-object tracking using hierarchical representations that focus on target body parts, whole bodies, and union areas. The methodology incorporates a transformer-based attention mechanism to process visual data, aiming to improve robustness and computational efficiency. The authors evaluate their approach on benchmark datasets such as MOT17 and MOT20, claiming comparable performance to existing methods while highlighting robustness against occlusion.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its structured approach to addressing multi-object tracking through hierarchical representations, which could potentially enhance the understanding of object relationships. However, the weaknesses are significant; many contributions appear to be incremental rather than groundbreaking, with the methodology relying heavily on established concepts such as part-based tracking and traditional evaluation metrics. The lack of truly innovative ideas raises questions about the paper's overall impact within the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is undermined by overly complicated language that detracts from the main contributions. While the methodology is described in a structured manner, the novelty seems limited, as the paper does not significantly advance the state-of-the-art in multi-object tracking. Reproducibility is potentially an issue given the heavy computational requirements specified, which may hinder accessibility for researchers with limited resources.\n\n# Summary Of The Review\nOverall, the paper presents a structured approach to hierarchical representations for object tracking, but it lacks significant novelty and presents ideas that feel recycled from existing literature. While the methodology is sound, the incremental nature of the contributions raises doubts about its impact and importance in advancing the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel hierarchical part-whole attention mechanism (HiPWA) designed for multi-object tracking (MOT). It introduces a unique representation that emphasizes the relationships between parts and wholes of objects, aiming to improve tracking accuracy in complex environments. The methodology leverages a transformer-based attention mechanism to focus on relevant features while minimizing computational load by processing single-frame data. The authors evaluate HiPWA against state-of-the-art tracking methods, demonstrating its effectiveness but also acknowledging challenges related to occlusions and scalability.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to hierarchical representation and the use of attention mechanisms, which offer promising results in tracking performance. However, the paper could benefit from a more granular representation by incorporating additional object features such as texture and color. The analysis of HiPWA's performance against a broader range of advanced tracking algorithms would provide a more comprehensive evaluation. Additionally, the authors' acknowledgment of limitations regarding occlusions and the need for further exploration of robustness in real-world applications is a critical point that should be addressed in future work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with a logical flow that presents the methodology and findings effectively. The novelty of the hierarchical representation and attention mechanism is notable, though the paper could improve reproducibility by providing more detailed descriptions of the experimental setup and baseline comparisons. The lack of discussion on data augmentation strategies and multi-modal integration leaves room for improvement in terms of robustness and generalization.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of multi-object tracking through its innovative hierarchical representation and attention mechanism. While it demonstrates promising results, the authors should address certain limitations related to occlusions and explore additional methodologies to enhance robustness and performance in diverse environments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents HiPWA, a novel multi-object tracking method that utilizes a hierarchical representation to improve association performance in challenging scenarios. The authors report strong benchmark results, particularly achieving the highest HOTA score of 62.2 on the MOT17 test set, significantly outpacing other transformer-based methods in association tasks. The methodology emphasizes efficiency, with a notably shorter training time compared to existing methods, while findings suggest that HiPWA is effective in occlusion-heavy environments, although it trails behind competitors in more complex datasets like MOT20.\n\n# Strength And Weaknesses\nThe strengths of HiPWA include its leading association performance on the MOT17 dataset and competitive results on the DanceTrack dataset, showcasing its effectiveness in various tracking scenarios. Furthermore, its efficient training time is a significant advantage over other state-of-the-art methods, which could facilitate wider adoption. However, weaknesses are evident in its performance on the MOT20 dataset, where it struggles with high occlusion rates and achieves a lower AssA score, indicating limitations in its ability to handle complex environments effectively.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the methodology and findings, making it accessible to readers. The quality of the experiments is high, with thorough comparative analyses against both traditional and modern methods. The novelty of using a hierarchical representation for multi-object tracking is commendable; however, while the results are promising, reproducibility may be slightly hindered due to the specifics of the datasets and the conditions under which the experiments were conducted.\n\n# Summary Of The Review\nOverall, HiPWA demonstrates competitive performance in multi-object tracking, particularly for association tasks, and sets a solid benchmark in terms of efficiency. However, its limitations in handling complex scenarios like occlusions suggest that further enhancements are necessary to improve association quality and detection robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to multi-object tracking (MOT) by introducing a spatio-temporal attention mechanism that aims to improve tracking accuracy and efficiency. The methodology involves a hierarchical representation framework that segments objects into parts, enhancing the model's ability to capture complex interactions over time. The findings indicate that the proposed method outperforms existing state-of-the-art techniques on several benchmark datasets, showcasing significant improvements in both accuracy and computational efficiency.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative approach to addressing challenges in MOT and the empirical results that demonstrate its effectiveness. The hierarchical representation and the integration of spatio-temporal attention contribute to a more nuanced understanding of object interactions, which is a notable advancement in the field. However, the paper suffers from clarity issues due to the overuse of technical jargon and lengthy sentences, which may hinder comprehension for a broader audience. Additionally, certain concepts are introduced without sufficient definitions, impacting the overall accessibility of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by complex sentence structures and inconsistent terminology, which detracts from the overall quality and readability. While the novelty of the proposed approach is commendable, the paper would benefit from a more systematic presentation to facilitate reproducibility. The lack of clear definitions for key concepts and the repetitive nature of some discussions further obscure the main contributions. A more concise abstract and conclusion would also enhance the paper's impact.\n\n# Summary Of The Review\nOverall, while the paper makes a valuable contribution to the field of multi-object tracking through its innovative methodology, issues with clarity and readability detract from its effectiveness. Improving the presentation and organization of the content could significantly enhance the understanding and impact of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.36559479166298,
    -1.728852966799964,
    -1.811967080770546,
    -1.8439205449183196,
    -2.003811360496751,
    -1.6803833377756983,
    -1.5465244093446804,
    -1.970922377562781,
    -1.8882531664534297,
    -1.9341695895417774,
    -1.550016455481897,
    -1.8144590029792034,
    -1.5498205091349129,
    -1.7942953457265107,
    -1.6596564311198914,
    -1.8715841206931823,
    -2.0324344362782507,
    -1.8058239931903086,
    -1.7609113123261484,
    -1.7759581089052108,
    -2.061810109468039,
    -1.723223748795627,
    -1.900527983797175,
    -1.8383086562190714,
    -1.809987942418755,
    -1.9904593818562566,
    -1.82038452931053,
    -2.020993782655628,
    -1.6008726703904883
  ],
  "logp_cond": [
    [
      0.0,
      -1.9627682982599877,
      -1.9860726087271332,
      -1.9681673654509615,
      -2.0017847803496296,
      -2.008792758323584,
      -2.114685670742743,
      -1.9980408245640213,
      -1.980530888876734,
      -2.0269478583339895,
      -1.9927434603232286,
      -2.1012140940026622,
      -1.9953827507394821,
      -1.9806280628115955,
      -1.9973376862813523,
      -1.9908950558881315,
      -1.9948666519516813,
      -1.9722671567971852,
      -2.0472036140647463,
      -1.986358885072303,
      -2.002536301702235,
      -2.040280313615104,
      -1.9971032104684299,
      -1.9998418518934375,
      -2.0185899869176587,
      -1.9753864763706803,
      -2.00129901565936,
      -1.9868159606412137,
      -2.039447372918652
    ],
    [
      -1.4009475445213837,
      0.0,
      -1.256549737169223,
      -1.2230037790246104,
      -1.315831441071816,
      -1.2849813584690366,
      -1.45928261382079,
      -1.2718791913580845,
      -1.2785773614533684,
      -1.3336190980229765,
      -1.3229908150526346,
      -1.450730206700423,
      -1.3281814867264035,
      -1.2841508549580494,
      -1.3422836987667142,
      -1.2588374456345641,
      -1.3513148147007321,
      -1.2901078480327755,
      -1.3205439324966461,
      -1.326977957579975,
      -1.3050287661868032,
      -1.3903892174142565,
      -1.3400090859473501,
      -1.2721783779970197,
      -1.3715462786657013,
      -1.3077895846033796,
      -1.3697314049976428,
      -1.3579024951019283,
      -1.42860438955283
    ],
    [
      -1.4163765138555968,
      -1.2421767986395764,
      0.0,
      -1.2734439991995827,
      -1.325707676063738,
      -1.345320657084204,
      -1.505936824249112,
      -1.316476318922235,
      -1.223477163405871,
      -1.3288197554146681,
      -1.3462871166074184,
      -1.498670367895015,
      -1.3325358663927627,
      -1.3000221325364334,
      -1.414296580965754,
      -1.2837489794644015,
      -1.3588910788312976,
      -1.2932606257996244,
      -1.4038910648099756,
      -1.3536007674527402,
      -1.2824999064721112,
      -1.426059863737495,
      -1.3463044815851823,
      -1.2831581779895638,
      -1.3756885322985493,
      -1.326981217210577,
      -1.3311043384085186,
      -1.2713569842709656,
      -1.474132129220306
    ],
    [
      -1.5325668377326247,
      -1.3026444378799495,
      -1.3813749278662546,
      0.0,
      -1.4302839839294725,
      -1.4582644412855013,
      -1.5581733655036105,
      -1.3993679619199622,
      -1.3989262489196068,
      -1.4485245467945502,
      -1.436082363412046,
      -1.5826408603800486,
      -1.415087503186438,
      -1.3567820143970817,
      -1.4891200089527221,
      -1.3823424517346763,
      -1.4844010084000276,
      -1.4108012983427163,
      -1.4748483203005274,
      -1.4312796106692394,
      -1.4249954689642657,
      -1.5133932614158327,
      -1.4717079058418032,
      -1.3767329221473241,
      -1.5247743292609395,
      -1.4346993366030327,
      -1.4864853090398524,
      -1.434789278350528,
      -1.5390726705631075
    ],
    [
      -1.6324364707467742,
      -1.5615241638112158,
      -1.576452184958998,
      -1.5874266172359255,
      0.0,
      -1.636354087700092,
      -1.7214591675493254,
      -1.6334211512488221,
      -1.5170646743739449,
      -1.5805962103915123,
      -1.5625184641411756,
      -1.7213729325446028,
      -1.6169624406967409,
      -1.6252522812675354,
      -1.598056972495929,
      -1.6479289896006326,
      -1.6233036260957137,
      -1.6142282868774869,
      -1.5991791535924196,
      -1.6289242075979637,
      -1.6114141051661965,
      -1.7032334474601998,
      -1.5685244938989287,
      -1.5818279773094277,
      -1.6134328508120745,
      -1.6411466489948239,
      -1.5848531102011258,
      -1.619716954056872,
      -1.724643002201906
    ],
    [
      -1.330539141977105,
      -1.1674543183374857,
      -1.1893821854689146,
      -1.1878736869587787,
      -1.2254852494255335,
      0.0,
      -1.314917648155224,
      -1.209932353617966,
      -1.1927976761474122,
      -1.2180980000154498,
      -1.1502842620967137,
      -1.3370964912621701,
      -1.1493489183588328,
      -1.1475754746570006,
      -1.206222609187267,
      -1.2453036589090993,
      -1.227660723432985,
      -1.190167696293682,
      -1.1697330923840752,
      -1.2122528163424018,
      -1.2108675033512322,
      -1.2891469406996068,
      -1.2656745204163182,
      -1.185594338446712,
      -1.337207110560869,
      -1.2456496626526483,
      -1.2590622043170832,
      -1.2438397033623927,
      -1.313362044822894
    ],
    [
      -1.283439174549363,
      -1.2169737204543751,
      -1.2382258402106294,
      -1.2057983176903508,
      -1.187726997819777,
      -1.2238387101279191,
      0.0,
      -1.2011480993145778,
      -1.2312490722136324,
      -1.2263185610136371,
      -1.2397555567038872,
      -1.2678466986714254,
      -1.1610670972186727,
      -1.232477536018331,
      -1.2543643620749512,
      -1.1774094008158942,
      -1.245392105106454,
      -1.1772350288441313,
      -1.1819055102469038,
      -1.2178604159888837,
      -1.2208855372873078,
      -1.2611173909374371,
      -1.2221969966600248,
      -1.2037581785174247,
      -1.1697948578789688,
      -1.2234738367976554,
      -1.2390607587402853,
      -1.2278259270951297,
      -1.1817613567195862
    ],
    [
      -1.574203895566168,
      -1.3719743024322664,
      -1.429376424952011,
      -1.3574783911854966,
      -1.494061032815157,
      -1.4633907567424849,
      -1.635892923112843,
      0.0,
      -1.4996439546789264,
      -1.4587472318887023,
      -1.49151071125071,
      -1.6572059036526459,
      -1.3884259726761061,
      -1.3675960349242542,
      -1.5269092500611863,
      -1.4856713902394703,
      -1.422748883450267,
      -1.3777681345669746,
      -1.469626373823979,
      -1.4698369946543615,
      -1.4497353110892368,
      -1.5726203821120552,
      -1.5212090923319732,
      -1.4706048046787692,
      -1.4836192924421936,
      -1.450265443336669,
      -1.4792341260929625,
      -1.5203715118353744,
      -1.624053927057194
    ],
    [
      -1.4885634521521485,
      -1.3784488052669244,
      -1.344871273495365,
      -1.4318345189296338,
      -1.366237340657638,
      -1.4981968687725986,
      -1.5785941464816629,
      -1.4711745527012328,
      0.0,
      -1.5073336705857978,
      -1.439984676496665,
      -1.6021818285514782,
      -1.468704696842569,
      -1.4673859223284327,
      -1.4933795947796198,
      -1.3992496206245464,
      -1.4580339495454053,
      -1.4152253377997341,
      -1.494557816961561,
      -1.4571314970779046,
      -1.3980604837145836,
      -1.5622489701894486,
      -1.4481480923955465,
      -1.3929410226300114,
      -1.4991909463130266,
      -1.4822642478689712,
      -1.450688738330627,
      -1.34874770849371,
      -1.5589126140969012
    ],
    [
      -1.6359653296220031,
      -1.576104531263045,
      -1.5385681866188248,
      -1.5508423866483163,
      -1.5531611801794163,
      -1.602585768093463,
      -1.6935057313539925,
      -1.5488441631534016,
      -1.5826183564399248,
      0.0,
      -1.5903329606424037,
      -1.7160569051037762,
      -1.589864554019625,
      -1.5526220604008227,
      -1.6000289688038156,
      -1.587674432434304,
      -1.5732733552623055,
      -1.5841056123538262,
      -1.6778781850779394,
      -1.5682219306887673,
      -1.5458507225425557,
      -1.659151225145456,
      -1.5792723504350845,
      -1.58751718040943,
      -1.5732110731724538,
      -1.5939167525903792,
      -1.6108543304146192,
      -1.5500347930392067,
      -1.6541179049765489
    ],
    [
      -1.2193230843746963,
      -1.1158672206280225,
      -1.1262773601364577,
      -1.0660977979656616,
      -1.1023023747673872,
      -1.1353372797328638,
      -1.2359482746465054,
      -1.1071239678281375,
      -1.0770998128509452,
      -1.1236647303526155,
      0.0,
      -1.225813194296106,
      -1.1019631546243736,
      -1.043354393258958,
      -1.090077704824184,
      -1.1199269682066935,
      -1.149756519166472,
      -1.1059185002244054,
      -1.1102176861999917,
      -1.1146753830090084,
      -1.1191559842855903,
      -1.1799513145942415,
      -1.127281164426111,
      -1.0804217374589518,
      -1.233227485903681,
      -1.1537730650350457,
      -1.156650878038789,
      -1.1761214146681975,
      -1.2083076761156797
    ],
    [
      -1.5385233963666514,
      -1.4673537497358262,
      -1.4849810049464383,
      -1.4583851968860204,
      -1.460701278587803,
      -1.4706195619877402,
      -1.490373525532432,
      -1.4812655836436903,
      -1.4680074103446628,
      -1.4760270370126416,
      -1.4738824230047072,
      0.0,
      -1.4912705274862057,
      -1.4659350455010267,
      -1.4509901810945172,
      -1.5042318219539825,
      -1.4871293657844205,
      -1.4826288307404638,
      -1.4786172229531582,
      -1.4649390937342626,
      -1.4838334313192,
      -1.4820907179860388,
      -1.4763317941513334,
      -1.4643697907946844,
      -1.5016915410623695,
      -1.4877662919184504,
      -1.4803769026768971,
      -1.4909697030240008,
      -1.436921519295925
    ],
    [
      -1.230355244950602,
      -1.1105458716434025,
      -1.1270885063922307,
      -1.047020017128175,
      -1.1393086432273665,
      -1.1388484641987668,
      -1.2470047030475855,
      -1.0914550456228729,
      -1.1441794870229849,
      -1.1219347615888229,
      -1.1414106106999615,
      -1.2736207124713355,
      0.0,
      -1.0982453555321914,
      -1.1762426811228965,
      -1.1501412708444991,
      -1.1708260060632685,
      -1.090723997840523,
      -1.1112751568260226,
      -1.1209112034460256,
      -1.1407170250341712,
      -1.218556049110053,
      -1.1750442863344897,
      -1.0830623103870642,
      -1.214629280242182,
      -1.1052725965111136,
      -1.1628706481761673,
      -1.2089857959631671,
      -1.234105981824089
    ],
    [
      -1.4892359072682166,
      -1.3315902650376692,
      -1.350234493928057,
      -1.2741144268626607,
      -1.4010768245416227,
      -1.3705528749087237,
      -1.4922089651033614,
      -1.3462748300239744,
      -1.3857559499718197,
      -1.409976903209554,
      -1.3355056345468728,
      -1.5121773681836403,
      -1.3727540264506157,
      0.0,
      -1.3743302794747234,
      -1.365898489406154,
      -1.4328801450143462,
      -1.3488543599298588,
      -1.4225925884161534,
      -1.3765943186962273,
      -1.3974287000673224,
      -1.4784124997897419,
      -1.4261514545449856,
      -1.3237123447693,
      -1.4791885758727594,
      -1.3660108428753301,
      -1.4454832253774446,
      -1.433265063935731,
      -1.486061934116104
    ],
    [
      -1.2964866527845715,
      -1.177642290810195,
      -1.2177016229336721,
      -1.166730627207257,
      -1.1329857623362114,
      -1.188253652129845,
      -1.3419891460220268,
      -1.2154364431474312,
      -1.1724773861231148,
      -1.2371958076503673,
      -1.1836773761812494,
      -1.3060388039365163,
      -1.1958201885266382,
      -1.1432125273925533,
      0.0,
      -1.2012029884415671,
      -1.2089367402300257,
      -1.2054504022035606,
      -1.221725627718977,
      -1.2137740255636835,
      -1.2135618993262998,
      -1.2820503735632547,
      -1.2287496535992806,
      -1.1839629423082432,
      -1.2766840675655478,
      -1.2297130788491202,
      -1.2189105510011846,
      -1.2273244678907067,
      -1.302442840010716
    ],
    [
      -1.5178376698028533,
      -1.327080631073425,
      -1.3568731399744722,
      -1.3217360327765628,
      -1.4703167865655224,
      -1.517180567123767,
      -1.5211184640821542,
      -1.4397575758347263,
      -1.4197182828931307,
      -1.460526829476121,
      -1.450302478951361,
      -1.5727921858444158,
      -1.4383842643175588,
      -1.4134038241418987,
      -1.50094459870281,
      0.0,
      -1.4766588848325706,
      -1.4343835213117997,
      -1.4836042556633375,
      -1.4345095355491102,
      -1.435323866381975,
      -1.5277329114297704,
      -1.4795784463042818,
      -1.4196524814550688,
      -1.4500210027723917,
      -1.467880496007382,
      -1.4769571876611136,
      -1.440892482414893,
      -1.4958186349057927
    ],
    [
      -1.626148123023682,
      -1.5110881714157933,
      -1.50977821352128,
      -1.5234498856791097,
      -1.5456542879236825,
      -1.5335061040713494,
      -1.6858995130039163,
      -1.4926142313372186,
      -1.4999577464315461,
      -1.4897642392077637,
      -1.4999532643984421,
      -1.6220552326994666,
      -1.5590802497880616,
      -1.532572125632524,
      -1.5490774567135677,
      -1.5424890453747495,
      0.0,
      -1.5170435428644033,
      -1.5852434961747945,
      -1.5380640860466537,
      -1.527633812401491,
      -1.5855614278444787,
      -1.5561051153703405,
      -1.561761441364456,
      -1.5335352970763194,
      -1.5713458444746637,
      -1.5494022109748415,
      -1.5885939202323667,
      -1.6534962926992391
    ],
    [
      -1.4187953269287004,
      -1.2575308538560805,
      -1.25482339474329,
      -1.2808569948647264,
      -1.3029577653253666,
      -1.329548168067752,
      -1.4649137027082122,
      -1.2586253954035735,
      -1.2405242980635738,
      -1.3592804490285955,
      -1.3487361191852165,
      -1.4968896876653344,
      -1.2707229851654094,
      -1.2803433869707466,
      -1.3643748084199974,
      -1.3048679345000407,
      -1.3513678160952531,
      0.0,
      -1.3537490600651547,
      -1.299790940965421,
      -1.3097075297055039,
      -1.4798629148219409,
      -1.3926855985089988,
      -1.2812621360986118,
      -1.3809708741699736,
      -1.326168501065554,
      -1.3737236279080947,
      -1.3098224884916951,
      -1.4368110975447579
    ],
    [
      -1.4351444673945501,
      -1.2764015236096518,
      -1.3183177612729613,
      -1.2805990940204626,
      -1.3136374837124583,
      -1.2818667739814829,
      -1.4537174331711733,
      -1.2827222639946496,
      -1.3191657075508652,
      -1.3986068404855563,
      -1.3179786804397968,
      -1.4794570607995332,
      -1.2748399646495114,
      -1.3493637815289872,
      -1.3749327545521355,
      -1.3541047580152565,
      -1.3644637041449892,
      -1.3185356375194635,
      0.0,
      -1.3173478419164484,
      -1.31150115089177,
      -1.3698121871359545,
      -1.3341598208170204,
      -1.2713932724594048,
      -1.4102437889939017,
      -1.3463548403239798,
      -1.3546343954720619,
      -1.3974794635418595,
      -1.4090942878862345
    ],
    [
      -1.4014458029993668,
      -1.3440074254147953,
      -1.3220029224206509,
      -1.3218661490340662,
      -1.3445044165643376,
      -1.3706684570299932,
      -1.4671763610087913,
      -1.3097631251268458,
      -1.3350101113008723,
      -1.3327624384394055,
      -1.3317600253491404,
      -1.4630182787669903,
      -1.312532270538953,
      -1.3314492197879375,
      -1.381446067731685,
      -1.3372484482221267,
      -1.3845379499047836,
      -1.295718398327406,
      -1.3700960449584108,
      0.0,
      -1.354953575729639,
      -1.4428981138870483,
      -1.4332177557961545,
      -1.321693807622791,
      -1.4167582547211635,
      -1.3219462938908078,
      -1.3884729902542958,
      -1.380213479209245,
      -1.4429062412908877
    ],
    [
      -1.6812137625296706,
      -1.5688433222058151,
      -1.5259287362777114,
      -1.5520009689380914,
      -1.5738907860810216,
      -1.6489258702050615,
      -1.7365329566498093,
      -1.550685674394538,
      -1.5392411913082134,
      -1.6120853527646928,
      -1.6033726121642022,
      -1.72809985062115,
      -1.6092709767957392,
      -1.5853314098040017,
      -1.5927326024561048,
      -1.5786376436489395,
      -1.5323500787448074,
      -1.5258374594325437,
      -1.6284675204388326,
      -1.551507584889531,
      0.0,
      -1.6205888122578047,
      -1.6419267956714252,
      -1.5508407226344927,
      -1.6418767226830602,
      -1.5460029485181512,
      -1.6376636346602136,
      -1.59785236393207,
      -1.6980397391516702
    ],
    [
      -1.3333703986454086,
      -1.229413548973926,
      -1.2740560295188124,
      -1.2580561292729762,
      -1.2692206710907274,
      -1.234603136973642,
      -1.3797084508322892,
      -1.249888076562249,
      -1.2701129938014277,
      -1.3234821503960896,
      -1.2373594804307042,
      -1.3404741130242885,
      -1.2370352224999708,
      -1.2729976404566623,
      -1.2657162314142436,
      -1.2801190035133214,
      -1.2714680550278479,
      -1.2449222491404488,
      -1.244876632855423,
      -1.2526269900060418,
      -1.2408992690815919,
      0.0,
      -1.2299445628790553,
      -1.2689878644768728,
      -1.247278342393647,
      -1.2614641525682357,
      -1.2232193939631228,
      -1.274148115931349,
      -1.341320700821016
    ],
    [
      -1.5296892052243998,
      -1.4696112734756954,
      -1.454609698104214,
      -1.4787837301695261,
      -1.4127543877811652,
      -1.5079189611106667,
      -1.5721245012518776,
      -1.4812137539480326,
      -1.425889209559858,
      -1.4575334949240442,
      -1.4542765802861026,
      -1.591868861468464,
      -1.4914122376515657,
      -1.4968992372396166,
      -1.5245923563416295,
      -1.524217892818992,
      -1.4916336011465559,
      -1.5053955624832969,
      -1.5361106327373895,
      -1.5244294025752367,
      -1.4263683886387915,
      -1.5370603735076411,
      0.0,
      -1.5018936965051528,
      -1.481586275897471,
      -1.4881687558922558,
      -1.4709682710637904,
      -1.4877072217368208,
      -1.608878610224115
    ],
    [
      -1.4495781087045765,
      -1.3062310827290387,
      -1.2747047473346969,
      -1.2868595999667043,
      -1.3698758374051452,
      -1.3756413310636606,
      -1.5132346594547872,
      -1.354923576539854,
      -1.3271353221861981,
      -1.4213098320310784,
      -1.3699828040363882,
      -1.5216055990098147,
      -1.2955975934407475,
      -1.2828938200294357,
      -1.416481313921722,
      -1.3775936844403276,
      -1.3910746921830224,
      -1.3014260367038624,
      -1.3549955371909035,
      -1.365457479443785,
      -1.367941436984027,
      -1.4988144248701125,
      -1.428539458069432,
      0.0,
      -1.4397480404037641,
      -1.352221115892599,
      -1.4063019658618978,
      -1.424037778442787,
      -1.4765708468316197
    ],
    [
      -1.4767077461939098,
      -1.4230833678456467,
      -1.4215184481067342,
      -1.451389892305424,
      -1.429858787577523,
      -1.4612446055369284,
      -1.5172033099821784,
      -1.4302278558773385,
      -1.4399300151823824,
      -1.41087064795161,
      -1.4437836042792214,
      -1.5434051578375987,
      -1.4658521517105618,
      -1.4442389011800896,
      -1.4532404670407044,
      -1.4195798532959916,
      -1.3934709105676504,
      -1.4404358351114632,
      -1.4887462359982957,
      -1.4623817241383221,
      -1.4369235283144721,
      -1.4810888922435215,
      -1.4370740510595685,
      -1.4185357603188768,
      0.0,
      -1.4395205345670612,
      -1.436557718572691,
      -1.447117992375167,
      -1.4974944261654526
    ],
    [
      -1.566805390695046,
      -1.444740883685668,
      -1.4488497534670945,
      -1.4295419049024698,
      -1.4989971392207033,
      -1.5092423103463455,
      -1.6564950133228484,
      -1.4482033140615647,
      -1.532348151061305,
      -1.5404489953131755,
      -1.4996638763418864,
      -1.6472537711909356,
      -1.4910217469722533,
      -1.3747736459849618,
      -1.5232361268026335,
      -1.4640718272289888,
      -1.5360059075742662,
      -1.457202841035683,
      -1.55964091928152,
      -1.5019180135969745,
      -1.5083842156729999,
      -1.5840747107389408,
      -1.5614552995559419,
      -1.47935405870666,
      -1.5668430452698305,
      0.0,
      -1.4993210281269957,
      -1.5778369625275095,
      -1.5938984934773677
    ],
    [
      -1.4812231894241255,
      -1.3732726077104314,
      -1.3778180874034471,
      -1.344679534352966,
      -1.3498631935256757,
      -1.4134454005580979,
      -1.5144980078752497,
      -1.3570480082051317,
      -1.3608059833139061,
      -1.420120882279106,
      -1.404013136623898,
      -1.5039721348367496,
      -1.3666881938700104,
      -1.3954075180129517,
      -1.4002793862129577,
      -1.4008580682126293,
      -1.3960760646582415,
      -1.3967333745170836,
      -1.4050628844752282,
      -1.4165961340037618,
      -1.3745623959948656,
      -1.4135251796540296,
      -1.3749340608903389,
      -1.4007741562886487,
      -1.3982829232087057,
      -1.3646335068213882,
      0.0,
      -1.4022118709552622,
      -1.480447748768212
    ],
    [
      -1.6538651502363457,
      -1.590350265650253,
      -1.5282891985876668,
      -1.5697868585552004,
      -1.5628605403126692,
      -1.6433865864186392,
      -1.6995607707483946,
      -1.614885428518071,
      -1.4808881893082992,
      -1.564579040284268,
      -1.6117752528849012,
      -1.7236177811251532,
      -1.6287815982096627,
      -1.5997502928830762,
      -1.632852906467842,
      -1.591734431256329,
      -1.6236804555353246,
      -1.591291064417676,
      -1.646513300533199,
      -1.5991898120212684,
      -1.5798363857959152,
      -1.6688313929536593,
      -1.6048643593100584,
      -1.5657251377216663,
      -1.636047333533818,
      -1.639170519930324,
      -1.6013676712384546,
      0.0,
      -1.716964657479515
    ],
    [
      -1.2991665061415192,
      -1.2199868899198998,
      -1.2446997868082545,
      -1.2270164939560924,
      -1.227986443905222,
      -1.2316772215283098,
      -1.231017573889639,
      -1.247008492853049,
      -1.2567058616329463,
      -1.246833128071113,
      -1.233913131285137,
      -1.2526023671045847,
      -1.222555672457998,
      -1.23856602061228,
      -1.2476863242440501,
      -1.1841709333905657,
      -1.234032993941553,
      -1.253021881971482,
      -1.2454940680673192,
      -1.2261732639780707,
      -1.2389320347672346,
      -1.2497309252259994,
      -1.252967124322521,
      -1.2296171761664834,
      -1.218768560546883,
      -1.2613033981593977,
      -1.2330929093977958,
      -1.255611660581784,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.40282649340299215,
      0.3795221829358466,
      0.3974274262120183,
      0.3638100113133502,
      0.35680203333939575,
      0.250909120920237,
      0.3675539670989585,
      0.3850639027862459,
      0.3386469333289903,
      0.37285133133975124,
      0.2643806976603176,
      0.3702120409234977,
      0.3849667288513843,
      0.3682571053816275,
      0.3746997357748483,
      0.3707281397112985,
      0.3933276348657946,
      0.31839117759823354,
      0.37923590659067674,
      0.36305848996074497,
      0.3253144780478756,
      0.36849158119454994,
      0.36575293976954226,
      0.34700480474532114,
      0.3902083152922995,
      0.36429577600361984,
      0.3787788310217661,
      0.32614741874432784
    ],
    [
      0.3279054222785802,
      0.0,
      0.472303229630741,
      0.5058491877753535,
      0.413021525728148,
      0.4438716083309273,
      0.269570352979174,
      0.45697377544187945,
      0.4502756053465955,
      0.3952338687769874,
      0.4058621517473293,
      0.27812276009954084,
      0.40067148007356046,
      0.44470211184191455,
      0.38656926803324976,
      0.4700155211653998,
      0.3775381520992318,
      0.43874511876718847,
      0.4083090343033178,
      0.401875009219989,
      0.42382420061316073,
      0.33846374938570745,
      0.3888438808526138,
      0.4566745888029442,
      0.3573066881342626,
      0.42106338219658435,
      0.35912156180232113,
      0.37095047169803563,
      0.30024857724713394
    ],
    [
      0.3955905669149491,
      0.5697902821309695,
      0.0,
      0.5385230815709632,
      0.48625940470680784,
      0.4666464236863419,
      0.3060302565214339,
      0.49549076184831087,
      0.5884899173646749,
      0.4831473253558778,
      0.4656799641631275,
      0.31329671287553085,
      0.47943121437778324,
      0.5119449482341125,
      0.39767049980479197,
      0.5282181013061444,
      0.45307600193924835,
      0.5187064549709215,
      0.4080760159605703,
      0.45836631331780575,
      0.5294671742984347,
      0.38590721703305086,
      0.46566259918536357,
      0.5288089027809821,
      0.43627854847199665,
      0.48498586355996887,
      0.4808627423620273,
      0.5406100964995804,
      0.3378349515502399
    ],
    [
      0.3113537071856949,
      0.54127610703837,
      0.462545617052065,
      0.0,
      0.41363656098884705,
      0.3856561036328183,
      0.28574717941470906,
      0.4445525829983574,
      0.44499429599871276,
      0.39539599812376935,
      0.4078381815062735,
      0.261279684538271,
      0.4288330417318815,
      0.4871385305212379,
      0.35480053596559746,
      0.4615780931836433,
      0.35951953651829194,
      0.4331192465756033,
      0.3690722246177922,
      0.4126409342490802,
      0.4189250759540539,
      0.33052728350248683,
      0.37221263907651636,
      0.46718762277099546,
      0.3191462156573801,
      0.40922120831528686,
      0.35743523587846715,
      0.40913126656779153,
      0.3048478743552121
    ],
    [
      0.3713748897499767,
      0.44228719668553507,
      0.4273591755377528,
      0.4163847432608254,
      0.0,
      0.36745727279665896,
      0.28235219294742553,
      0.37039020924792876,
      0.486746686122806,
      0.4232151501052386,
      0.44129289635557534,
      0.28243842795214813,
      0.38684891980001,
      0.3785590792292155,
      0.40575438800082186,
      0.3558823708961183,
      0.38050773440103725,
      0.38958307361926403,
      0.4046322069043313,
      0.37488715289878716,
      0.39239725533055436,
      0.3005779130365511,
      0.43528686659782223,
      0.4219833831873232,
      0.39037850968467636,
      0.36266471150192703,
      0.41895825029562506,
      0.3840944064398788,
      0.279168358294845
    ],
    [
      0.34984419579859316,
      0.5129290194382126,
      0.49100115230678365,
      0.49250965081691955,
      0.45489808835016476,
      0.0,
      0.3654656896204742,
      0.47045098415773223,
      0.4875856616282861,
      0.4622853377602485,
      0.5300990756789845,
      0.34328684651352814,
      0.5310344194168655,
      0.5328078631186977,
      0.4741607285884313,
      0.43507967886659893,
      0.4527226143427132,
      0.49021564148201624,
      0.5106502453916231,
      0.4681305214332965,
      0.4695158344244661,
      0.3912363970760915,
      0.41470881735938003,
      0.49478899932898623,
      0.3431762272148293,
      0.43473367512304995,
      0.4213211334586151,
      0.43654363441330557,
      0.3670212929528043
    ],
    [
      0.2630852347953174,
      0.32955068889030525,
      0.30829856913405096,
      0.34072609165432954,
      0.3587974115249033,
      0.32268569921676127,
      0.0,
      0.3453763100301026,
      0.31527533713104794,
      0.32020584833104326,
      0.30676885264079323,
      0.278677710673255,
      0.38545731212600765,
      0.3140468733263493,
      0.2921600472697292,
      0.3691150085287862,
      0.30113230423822634,
      0.36928938050054905,
      0.36461889909777656,
      0.32866399335579666,
      0.32563887205737263,
      0.28540701840724325,
      0.3243274126846556,
      0.34276623082725566,
      0.37672955146571163,
      0.32305057254702496,
      0.30746365060439507,
      0.31869848224955066,
      0.36476305262509423
    ],
    [
      0.3967184819966132,
      0.5989480751305147,
      0.5415459526107702,
      0.6134439863772845,
      0.4768613447476242,
      0.5075316208202962,
      0.33502945444993815,
      0.0,
      0.4712784228838547,
      0.5121751456740788,
      0.4794116663120711,
      0.31371647391013524,
      0.582496404886675,
      0.6033263426385269,
      0.44401312750159483,
      0.48525098732331085,
      0.548173494112514,
      0.5931542429958065,
      0.5012960037388021,
      0.5010853829084196,
      0.5211870664735443,
      0.39830199545072587,
      0.4497132852308079,
      0.5003175728840119,
      0.4873030851205875,
      0.520656934226112,
      0.49168825146981865,
      0.45055086572740666,
      0.346868450505587
    ],
    [
      0.3996897143012812,
      0.5098043611865053,
      0.5433818929580647,
      0.4564186475237959,
      0.5220158257957916,
      0.39005629768083105,
      0.3096590199717668,
      0.4170786137521969,
      0.0,
      0.38091949586763185,
      0.4482684899567646,
      0.2860713379019515,
      0.4195484696108607,
      0.420867244124997,
      0.3948735716738099,
      0.4890035458288833,
      0.43021921690802434,
      0.4730278286536955,
      0.3936953494918687,
      0.4311216693755251,
      0.4901926827388461,
      0.326004196263981,
      0.4401050740578831,
      0.49531214382341826,
      0.38906222014040304,
      0.4059889185844585,
      0.4375644281228026,
      0.5395054579597196,
      0.3293405523565285
    ],
    [
      0.2982042599197743,
      0.35806505827873236,
      0.39560140292295265,
      0.3833272028934611,
      0.38100840936236113,
      0.33158382144831444,
      0.2406638581877849,
      0.38532542638837586,
      0.35155123310185266,
      0.0,
      0.3438366288993737,
      0.21811268443800125,
      0.3443050355221524,
      0.3815475291409547,
      0.3341406207379618,
      0.3464951571074735,
      0.36089623427947193,
      0.35006397718795124,
      0.256291404463838,
      0.36594765885301017,
      0.3883188669992217,
      0.2750183643963213,
      0.3548972391066929,
      0.3466524091323475,
      0.36095851636932363,
      0.3402528369513982,
      0.3233152591271582,
      0.38413479650257076,
      0.28005168456522855
    ],
    [
      0.33069337110720065,
      0.43414923485387447,
      0.4237390953454392,
      0.4839186575162353,
      0.4477140807145097,
      0.4146791757490331,
      0.31406818083539156,
      0.44289248765375944,
      0.4729166426309517,
      0.42635172512928143,
      0.0,
      0.324203261185791,
      0.4480533008575234,
      0.506662062222939,
      0.45993875065771284,
      0.43008948727520346,
      0.4002599363154249,
      0.4440979552574915,
      0.4397987692819052,
      0.43534107247288856,
      0.43086047119630666,
      0.3700651408876554,
      0.422735291055786,
      0.46959471802294517,
      0.316788969578216,
      0.3962433904468512,
      0.39336557744310796,
      0.3738950408136994,
      0.34170877936621724
    ],
    [
      0.275935606612552,
      0.34710525324337715,
      0.3294779980327651,
      0.35607380609318295,
      0.3537577243914003,
      0.34383944099146313,
      0.3240854774467714,
      0.33319341933551305,
      0.3464515926345406,
      0.3384319659665618,
      0.3405765799744962,
      0.0,
      0.3231884754929977,
      0.34852395747817666,
      0.3634688218846862,
      0.31022718102522084,
      0.3273296371947829,
      0.33183017223873956,
      0.33584178002604514,
      0.3495199092449408,
      0.3306255716600033,
      0.3323682849931646,
      0.33812720882786995,
      0.35008921218451894,
      0.3127674619168339,
      0.3266927110607529,
      0.33408210030230623,
      0.3234892999552026,
      0.37753748368327833
    ],
    [
      0.319465264184311,
      0.4392746374915104,
      0.4227320027426822,
      0.5028004920067379,
      0.4105118659075464,
      0.41097204493614603,
      0.3028158060873274,
      0.45836546351204,
      0.405641022111928,
      0.42788574754609,
      0.40840989843495135,
      0.2761997966635774,
      0.0,
      0.4515751536027215,
      0.37357782801201633,
      0.39967923829041374,
      0.3789945030716444,
      0.4590965112943899,
      0.43854535230889025,
      0.4289093056888873,
      0.4091034841007417,
      0.33126446002485976,
      0.3747762228004232,
      0.46675819874784863,
      0.3351912288927308,
      0.4445479126237992,
      0.3869498609587456,
      0.34083471317174574,
      0.31571452731082394
    ],
    [
      0.3050594384582941,
      0.46270508068884153,
      0.44406085179845367,
      0.52018091886385,
      0.393218521184888,
      0.42374247081778704,
      0.3020863806231493,
      0.4480205157025363,
      0.408539395754691,
      0.3843184425169568,
      0.45878971117963796,
      0.2821179775428704,
      0.421541319275895,
      0.0,
      0.4199650662517873,
      0.4283968563203566,
      0.3614152007121645,
      0.4454409857966519,
      0.3717027573103573,
      0.41770102703028344,
      0.3968666456591883,
      0.31588284593676885,
      0.36814389118152513,
      0.47058300095721073,
      0.3151067698537513,
      0.4282845028511806,
      0.34881212034906617,
      0.3610302817907798,
      0.3082334116104066
    ],
    [
      0.3631697783353198,
      0.48201414030969647,
      0.44195480818621924,
      0.4929258039126343,
      0.52667066878368,
      0.47140277899004634,
      0.31766728509786457,
      0.4442199879724602,
      0.48717904499677656,
      0.42246062346952407,
      0.475979054938642,
      0.3536176271833751,
      0.4638362425932532,
      0.5164439037273381,
      0.0,
      0.4584534426783242,
      0.45071969088986563,
      0.4542060289163308,
      0.4379308034009144,
      0.44588240555620784,
      0.4460945317935916,
      0.3776060575566367,
      0.4309067775206108,
      0.4756934888116482,
      0.3829723635543436,
      0.4299433522707712,
      0.44074588011870675,
      0.43233196322918466,
      0.3572135911091754
    ],
    [
      0.35374645089032897,
      0.5445034896197571,
      0.51471098071871,
      0.5498480879166194,
      0.4012673341276598,
      0.3544035535694152,
      0.350465656611028,
      0.43182654485845595,
      0.4518658378000515,
      0.41105729121706136,
      0.42128164174182126,
      0.2987919348487664,
      0.4331998563756234,
      0.45818029655128356,
      0.3706395219903722,
      0.0,
      0.3949252358606117,
      0.43720059938138256,
      0.3879798650298447,
      0.43707458514407205,
      0.4362602543112073,
      0.34385120926341184,
      0.39200567438890044,
      0.4519316392381134,
      0.4215631179207906,
      0.40370362468580034,
      0.39462693303206864,
      0.43069163827828927,
      0.37576548578738955
    ],
    [
      0.40628631325456865,
      0.5213462648624574,
      0.5226562227569707,
      0.508984550599141,
      0.4867801483545682,
      0.49892833220690136,
      0.34653492327433444,
      0.5398202049410321,
      0.5324766898467046,
      0.542670197070487,
      0.5324811718798086,
      0.4103792035787841,
      0.4733541864901891,
      0.4998623106457267,
      0.48335697956468304,
      0.4899453909035012,
      0.0,
      0.5153908934138474,
      0.44719094010345617,
      0.49437035023159703,
      0.5048006238767597,
      0.446873008433772,
      0.47632932090791025,
      0.47067299491379466,
      0.4988991392019313,
      0.46108859180358697,
      0.48303222530340917,
      0.44384051604588404,
      0.37893814357901157
    ],
    [
      0.38702866626160826,
      0.5482931393342281,
      0.5510005984470185,
      0.5249669983255822,
      0.5028662278649421,
      0.47627582512255673,
      0.3409102904820964,
      0.5471985977867351,
      0.5652996951267348,
      0.44654354416171316,
      0.4570878740050921,
      0.30893430552497425,
      0.5351010080248992,
      0.525480606219562,
      0.44144918477031125,
      0.5009560586902679,
      0.4544561770950555,
      0.0,
      0.4520749331251539,
      0.5060330522248877,
      0.49611646348480476,
      0.32596107836836774,
      0.41313839468130986,
      0.5245618570916968,
      0.42485311902033507,
      0.47965549212475467,
      0.43210036528221396,
      0.4960015046986135,
      0.36901289564555073
    ],
    [
      0.32576684493159824,
      0.48450978871649664,
      0.4425935510531871,
      0.4803122183056858,
      0.44727382861369014,
      0.47904453834466554,
      0.3071938791549751,
      0.4781890483314988,
      0.44174560477528324,
      0.3623044718405921,
      0.44293263188635157,
      0.2814542515266152,
      0.48607134767663696,
      0.4115475307971612,
      0.3859785577740129,
      0.40680655431089185,
      0.3964476081811592,
      0.44237567480668494,
      0.0,
      0.4435634704097,
      0.4494101614343784,
      0.39109912519019385,
      0.42675149150912794,
      0.4895180398667436,
      0.3506675233322467,
      0.4145564720021686,
      0.4062769168540865,
      0.3634318487842889,
      0.3518170244399139
    ],
    [
      0.37451230590584395,
      0.43195068349041543,
      0.4539551864845599,
      0.4540919598711446,
      0.43145369234087316,
      0.4052896518752176,
      0.30878174789641943,
      0.466194983778365,
      0.44094799760433845,
      0.4431956704658053,
      0.4441980835560704,
      0.31293983013822047,
      0.4634258383662577,
      0.44450888911727326,
      0.3945120411735257,
      0.43870966068308404,
      0.39142015900042715,
      0.4802397105778047,
      0.4058620639468,
      0.0,
      0.42100453317557185,
      0.3330599950181625,
      0.34274035310905626,
      0.4542643012824197,
      0.3591998541840473,
      0.454011815014403,
      0.38748511865091495,
      0.3957446296959657,
      0.33305186761432304
    ],
    [
      0.3805963469383684,
      0.49296678726222387,
      0.5358813731903276,
      0.5098091405299476,
      0.4879193233870174,
      0.41288423926297746,
      0.32527715281822966,
      0.5111244350735009,
      0.5225689181598256,
      0.4497247567033462,
      0.45843749730383676,
      0.333710258846889,
      0.45253913267229984,
      0.47647869966403733,
      0.4690775070119342,
      0.48317246581909945,
      0.5294600307232316,
      0.5359726500354953,
      0.4333425890292064,
      0.510302524578508,
      0.0,
      0.4412212972102343,
      0.4198833137966138,
      0.5109693868335463,
      0.4199333867849788,
      0.5158071609498878,
      0.4241464748078254,
      0.4639577455359689,
      0.36377037031636883
    ],
    [
      0.3898533501502184,
      0.49381019982170105,
      0.44916771927681465,
      0.46516761952265084,
      0.45400307770489956,
      0.4886206118219849,
      0.3435152979633378,
      0.4733356722333779,
      0.4531107549941993,
      0.3997415983995374,
      0.4858642683649228,
      0.38274963577133847,
      0.48618852629565623,
      0.4502261083389647,
      0.4575075173813834,
      0.4431047452823056,
      0.45175569376777913,
      0.4783014996551782,
      0.47834711594020396,
      0.4705967587895852,
      0.48232447971403514,
      0.0,
      0.4932791859165717,
      0.45423588431875417,
      0.4759454064019799,
      0.46175959622739127,
      0.5000043548325042,
      0.449075632864278,
      0.38190304797461105
    ],
    [
      0.37083877857277514,
      0.4309167103214795,
      0.4459182856929609,
      0.4217442536276488,
      0.48777359601600967,
      0.3926090226865082,
      0.3284034825452973,
      0.41931422984914235,
      0.4746387742373168,
      0.44299448887313075,
      0.4462514035110723,
      0.30865912232871096,
      0.4091157461456092,
      0.4036287465575583,
      0.37593562745554543,
      0.3763100909781829,
      0.40889438265061906,
      0.39513242131387805,
      0.36441735105978545,
      0.3760985812219382,
      0.47415959515838346,
      0.3634676102895338,
      0.0,
      0.3986342872920221,
      0.418941707899704,
      0.4123592279049191,
      0.42955971273338456,
      0.4128207620603541,
      0.29164937357305987
    ],
    [
      0.3887305475144949,
      0.5320775734900327,
      0.5636039088843745,
      0.5514490562523671,
      0.4684328188139262,
      0.46266732515541076,
      0.32507399676428417,
      0.4833850796792174,
      0.5111733340328732,
      0.41699882418799294,
      0.4683258521826832,
      0.3167030572092566,
      0.5427110627783238,
      0.5554148361896356,
      0.4218273422973493,
      0.46071497177874376,
      0.447233964036049,
      0.536882619515209,
      0.48331311902816787,
      0.4728511767752863,
      0.4703672192350443,
      0.33949423134895884,
      0.4097691981496394,
      0.0,
      0.3985606158153072,
      0.4860875403264724,
      0.43200669035717354,
      0.41427087777628446,
      0.3617378093874517
    ],
    [
      0.3332801962248453,
      0.38690457457310834,
      0.3884694943120208,
      0.358598050113331,
      0.380129154841232,
      0.3487433368818267,
      0.2927846324365766,
      0.3797600865414166,
      0.37005792723637265,
      0.39911729446714506,
      0.3662043381395337,
      0.26658278458115636,
      0.3441357907081932,
      0.3657490412386655,
      0.3567474753780506,
      0.3904080891227635,
      0.41651703185110467,
      0.3695521073072918,
      0.32124170642045935,
      0.3476062182804329,
      0.3730644141042829,
      0.3288990501752336,
      0.3729138913591865,
      0.3914521820998782,
      0.0,
      0.37046740785169385,
      0.37343022384606406,
      0.36286995004358813,
      0.31249351625330246
    ],
    [
      0.4236539911612105,
      0.5457184981705885,
      0.541609628389162,
      0.5609174769537868,
      0.49146224263555327,
      0.4812170715099111,
      0.33396436853340816,
      0.5422560677946919,
      0.45811123079495153,
      0.4500103865430811,
      0.49079550551437023,
      0.34320561066532096,
      0.4994376348840033,
      0.6156857358712948,
      0.46722325505362305,
      0.5263875546272678,
      0.45445347428199034,
      0.5332565408205736,
      0.43081846257473666,
      0.48854136825928207,
      0.48207516618325674,
      0.4063846711173158,
      0.4290040823003147,
      0.5111053231495966,
      0.42361633658642606,
      0.0,
      0.49113835372926085,
      0.4126224193287471,
      0.3965608883788889
    ],
    [
      0.3391613398864046,
      0.4471119216000987,
      0.44256644190708294,
      0.475704994957564,
      0.47052133578485433,
      0.4069391287524322,
      0.3058865214352804,
      0.46333652110539836,
      0.45957854599662396,
      0.40026364703142403,
      0.41637139268663215,
      0.31641239447378045,
      0.4536963354405197,
      0.42497701129757837,
      0.4201051430975724,
      0.4195264610979008,
      0.42430846465228855,
      0.4236511547934465,
      0.4153216448353019,
      0.40378839530676824,
      0.44582213331566445,
      0.4068593496565005,
      0.4454504684201912,
      0.4196103730218814,
      0.42210160610182434,
      0.4557510224891419,
      0.0,
      0.41817265835526785,
      0.33993678054231813
    ],
    [
      0.3671286324192824,
      0.43064351700537506,
      0.49270458406796136,
      0.45120692410042773,
      0.45813324234295894,
      0.3776071962369889,
      0.3214330119072335,
      0.4061083541375572,
      0.5401055933473289,
      0.45641474237136004,
      0.4092185297707269,
      0.29737600153047494,
      0.3922121844459654,
      0.4212434897725519,
      0.38814087618778603,
      0.42925935139929905,
      0.39731332712030354,
      0.42970271823795203,
      0.374480482122429,
      0.4218039706343597,
      0.4411573968597129,
      0.3521623897019688,
      0.4161294233455697,
      0.4552686449339618,
      0.3849464491218102,
      0.3818232627253042,
      0.41962611141717354,
      0.0,
      0.30402912517611314
    ],
    [
      0.3017061642489691,
      0.3808857804705885,
      0.35617288358223376,
      0.37385617643439595,
      0.37288622648526637,
      0.3691954488621785,
      0.36985509650084936,
      0.3538641775374394,
      0.344166808757542,
      0.3540395423193754,
      0.3669595391053513,
      0.3482703032859036,
      0.3783169979324903,
      0.36230664977820837,
      0.3531863461464382,
      0.41670173699992263,
      0.3668396764489352,
      0.3478507884190063,
      0.3553786023231691,
      0.3746994064124176,
      0.36194063562325374,
      0.3511417451644889,
      0.3479055460679672,
      0.37125549422400494,
      0.3821041098436053,
      0.33956927223109057,
      0.3677797609926925,
      0.34526100980870433,
      0.0
    ]
  ],
  "row_avgs": [
    0.3595951858862683,
    0.39871115301328114,
    0.4662447265282861,
    0.39462902085425744,
    0.3847665507457378,
    0.4510072652164893,
    0.32795630056905123,
    0.48828728993240833,
    0.4274570095218674,
    0.33859169915300213,
    0.41410087949547647,
    0.33587993335329086,
    0.3971640193759117,
    0.393283799572119,
    0.4385800759251121,
    0.4161917264699584,
    0.4791889227873149,
    0.4654770697496809,
    0.4139157144589298,
    0.40952687942918964,
    0.4596762487587759,
    0.45333911999022014,
    0.40289954901987607,
    0.45435230889150036,
    0.35957785594245567,
    0.4725440480647362,
    0.41724761385863374,
    0.40776355472999765,
    0.3612177116430888
  ],
  "col_avgs": [
    0.35179928071424904,
    0.467941591339571,
    0.4565905282127848,
    0.4709702572852815,
    0.44082441760048113,
    0.4139768598116356,
    0.31450822547952845,
    0.4419856610281422,
    0.45242273118582543,
    0.41734821655729754,
    0.43185979331343016,
    0.30720323905173164,
    0.4405344044616296,
    0.45351436714635956,
    0.40196563339451447,
    0.43300669775942696,
    0.4102592008001248,
    0.44676477255000513,
    0.4038793178369636,
    0.42666564715944394,
    0.434806403562021,
    0.3540864343906165,
    0.4047942546673164,
    0.4484444221535141,
    0.3875536976076912,
    0.41982781335314573,
    0.4073998239334126,
    0.4090478857613018,
    0.33919165481947167
  ],
  "combined_avgs": [
    0.3556972333002587,
    0.43332637217642606,
    0.46141762737053543,
    0.4327996390697695,
    0.41279548417310946,
    0.43249206251406247,
    0.32123226302428987,
    0.46513647548027526,
    0.4399398703538464,
    0.37796995785514986,
    0.4229803364044533,
    0.3215415862025113,
    0.41884921191877067,
    0.42339908335923926,
    0.4202728546598133,
    0.42459921211469265,
    0.4447240617937198,
    0.45612092114984304,
    0.40889751614794667,
    0.4180962632943168,
    0.44724132616039847,
    0.4037127771904183,
    0.40384690184359623,
    0.45139836552250723,
    0.3735657767750734,
    0.44618593070894097,
    0.41232371889602315,
    0.4084057202456497,
    0.35020468323128023
  ],
  "gppm": [
    607.9017551623904,
    563.4081914184575,
    569.8502398439995,
    561.8901904825057,
    575.2300428630091,
    589.0607445327453,
    634.5444337689412,
    576.0711778805481,
    570.5817527089049,
    581.4882037784192,
    583.6359844287931,
    635.111663136665,
    579.9769441572828,
    568.6697554008766,
    594.293265301918,
    577.7660498179954,
    589.5979841244407,
    574.3502921822418,
    593.5701554335376,
    583.493444584995,
    576.097023379018,
    617.5718314622027,
    591.634193143629,
    574.5908802594867,
    598.3371827747213,
    586.0371014855593,
    591.9044409705679,
    587.4660365094451,
    623.5283431978431
  ],
  "gppm_normalized": [
    1.4224757439994113,
    1.282973718561873,
    1.292592339702878,
    1.2832772525442038,
    1.2988169473468472,
    1.334391037627229,
    1.4482323339290122,
    1.3038861134621904,
    1.29090492409595,
    1.310493629036106,
    1.3144325507718428,
    1.4496937149362306,
    1.3119317587125814,
    1.2874603167067735,
    1.3465745329740009,
    1.3173836846672902,
    1.330545720994589,
    1.305908737091625,
    1.3413998822029665,
    1.3188133317177593,
    1.3015136999832133,
    1.4003192611575543,
    1.3376545931783597,
    1.2974618904467718,
    1.3487036292688912,
    1.3340263783002246,
    1.3343068450057578,
    1.3306778076042167,
    1.4083969209297673
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395
  ],
  "response_lengths": [
    1888,
    2868,
    2489,
    2595,
    2386,
    2559,
    2275,
    2139,
    2492,
    2745,
    2602,
    2247,
    2564,
    2965,
    2534,
    2536,
    2117,
    2438,
    2407,
    2377,
    2380,
    2011,
    2330,
    2234,
    2550,
    2117,
    2403,
    2275,
    2270
  ]
}