{
  "example_idx": 72,
  "reference": "Published as a conference paper at ICLR 2023\n\nCAUSALITY COMPENSATED ATTENTION FOR CONTEXTUAL BIASED VISUAL RECOGNITION\n\nRuyang Liu 1 1School of Electronic and Computer Engineering, Peking University 2ByteDance Inc {ruyang@stu,geli@ece,thomas@}.pku.edu.cn\n\nJingjia Huang 2 Thomas H. Li1 Ge Li (cid:66)1\n\nhuangjingjia@bytedance.com\n\nABSTRACT\n\nVisual attention does not always capture the essential object representation desired for robust predictions. Attention modules tend to underline not only the target object but also the common co-occurring context that the module thinks helpful in the training. The problem is rooted in the confounding effect of the context leading to incorrect causalities between objects and predictions, which is further exacerbated by visual attention. In this paper, to learn causal object features robust for contextual bias, we propose a novel attention module named Interventional Dual Attention (IDA) for visual recognition. Specifically, IDA adopts two attention layers with multiple sampling intervention, which compensates the attention against the confounder context. Note that our method is model-agnostic and thus can be implemented on various backbones. Extensive experiments show our model obtains significant improvements in classification and detection with lower computation. In particular, we achieve the state-of-the-art results in multi-label classification on MS-COCO and PASCAL-VOC.\n\n1\n\nINTRODUCTION\n\nThe last several years have witnessed the huge success of attention mechanisms in computer vision. The key insight behind different attention mechanisms (Wang et al., 2018; Hu et al., 2018; Woo et al., 2018; Chen et al., 2017; Zhu & Wu, 2021; Dosovitskiy et al., 2020; Liu et al., 2021) is the same, i.e., emphasizing key facts in inputs, while different aspects of information such as feature map and token query are considered. The impressive performances of these works show that attention is proficient in fitting training data and exploiting valuable features.\n\nHowever, whether the information deemed valuable by attention is always helpful in real application scenarios? In Fig.1(a), the Spatial Class-Aware Attention (SCA) considers the context dining table as the cause of spoon, which helps the model make correct predictions even if the stressed region is wrong, but it fails in cases where the object is absent in the familiar context. Fig.1(b) illustrates the problem in a quantitative view: we calculate the mIOU between the attention map (or the activation map for baseline) and the ground-truth mask. As is shown in the figure, although the model with attention gains some improvement measured by common evaluation metrics, the mIOU does not see progress, which means attention does not capture the more accurate regions of targets than the baseline. Moreover, Fig.1(c) illustrates the attention mechanism more easily attends to the wrong context regions when the training samples are insufficient. All above evidence reveals that attention mechanisms may not always improve the performance of the model by accessing meaningful factors and could be harmful, especially in the case of out-of-distribution scenarios.\n\nFortunately, the causality provides us with a theoretical perspective for this problem (Pearl et al., 2000; Neuberg, 2003). The familiar context of an object is a confounder (Yue et al., 2020; Zhang et al., 2020; Wang et al., 2020) confusing the causality between the object and its prediction. Even though the scene of the dining table is not the root cause for the spoon, the model is fooled into setting up a spurious correlation between them. On the other hand, the problem is tricky because context is naturally biased in the real world, and common datasets annotated by experts (e.g., MSCOCO (Lin et al., 2014)) also suffer from severe contextual bias. As is well known, networks equipped with attention can learn better representations of the datasets. However, the contextual bias in datasets can be also exacerbated when using the attention mechanism.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: (a) The attention maps of two examples in MS-COCO (Lin et al., 2014) using ResNet101 + SCA (our baseline attention). (b) The mAP and attention map mIOU of ResNet101 baseline, SCA, and the MS-SCA (our de-confounded model). (c) Attention map mIOU of the three models from head classes to tail classes.\n\nTo tackle the confounders in visual tasks, a common method is the causal intervention (Pearl et al., 2000; Neuberg, 2003). The interventions in most existing methods (Wang et al., 2021; Yang et al., 2021b; Yue et al., 2020; Zhang et al., 2020) share a maturity pipeline: defining the causality among given elements, implementing the confounders and ultimately implementing the backdoor adjustment (Pearl, 2014). Most of these methods, however, are difficult to implement on attention mechanism and migrate among different tasks. In this paper, we prove that a simple weighted multisampling operation on attention can be viewed as the intervention for the attention and confounding context. Based on that, we develop a novel causal attention module: Interventional Dual Attention (IDA). We first employ spatial class-aware attention (SCA) to extract the class-specific information in different positions of the feature map. Then, a multiple-sampling operation with Dot-Product Attention (DPA) re-weighting is implemented upon the SCA, which is essentially the causal intervention and builds a more robust attention map insensitive to the contextual bias. To receive a better trade-off between the performance and computation, we have two versions of IDA: the light one (pure two layers of attention) achieves huge improvements with limited parameter increment; the heavy one (the DPA is extended as a transformer) obtains the state-of-the-arts results with lower computation compared with the popular transformer-based models on the multi-label classification task. Furthermore, improvements in both classification and detection demonstrate the potential of our method to be applied in general visual tasks.\n\nOur main contributions can be summarized as follows:\n\n• We uncover that the attention mechanism may aggravate the existing contextual bias. Qualitative analysis from the viewpoint of causality as well as experimental results is presented to guarantee our arguments.\n\n• We propose an effective and model-agnostic attention module Interventional Dual Attention (IDA). Combining two different layers of attention and the multi-sampling intervention, the module is robust to the contextual bias, and can be extended to the popular transformer to obtain further enhancement.\n\n• We conduct extensive experiments to evaluate the superiority of our method. Rich quantitative and ablation results show that our method can bring about significant improvements on datasets MS-COCO (Lin et al., 2014) and PASCAL-VOC (Everingham et al., 2010) for various computer vision tasks with both CNN-based and transformer-based backbones.\n\n2 RELATED WORK\n\nAttention Mechanism. Attention mechanism (Itti et al., 1998; Rensink, 2000; Corbetta & Shulman, 2002) aims at imitating the perception system of humans, which adopts sequence signals and selects to focus on salient parts. Hence, no matter in what forms, attention is expected to bias the weight towards the most informative parts of an input signal, and the signal can be varied from a sequence, feature map, or token queries in different tasks.\n\n2\n\n(a)Two qualitative resultsprob of “fork”: 0.783prob of “fork”: 0.694(b)Comparison of mAPand mIOU(c)mIOUfrom head to tail classeshead classestail classesPublished as a conference paper at ICLR 2023\n\nOver the past several years, attention mechanism has won huge success in a wide range of tasks in computer vision. At the early stage, most of the works apply the attention to sequence-based models and tasks (Bluche, 2016; Stollenga et al., 2014). Then, here comes the age of attention is all your need (Vaswani et al., 2017), and many classic attention structures in computer vision arise such as SENET (Hu et al., 2018), Non-local (Wang et al., 2018) and CBAM (Woo et al., 2018). Finally is the popularity of self-attention (Dosovitskiy et al., 2020; Liu et al., 2021; Yuan et al., 2021) in these years, and it can be concluded as the operation among the query, key and value, which even totally replaces the CNN in pure visual tasks. However, as mentioned above, the usage of attention is usually not explicitly supervised. Consequently, the weight of attention is easily biased towards the bias in the dataset.\n\nCausalities in Computer Vision. Causality is one of the most important research areas in machine learning including causal discovery (Yehezkel Rohekar et al., 2021; Ng et al., 2021), causal structure learning (Kivva et al., 2021; Akbari et al., 2021) and causal inference (Zhang et al., 2021; Kaddour et al., 2021). Recent years have also witnessed growing applications of causalities in visual tasks such as long-tail classification (Tang et al., 2020; Zhu et al., 2022), few/zero-shot learning (Yue et al., 2020; 2021), and Visual Question Answering (Chen et al., 2020; Niu et al., 2021; Yang et al., 2021a). Significantly, many of these works are about conquering the contextual bias in their tasks. Yue et al. (2020) considered pre-training information as the culprit for the bias and debiased by implementing the backdoor adjustment. Niu et al. (2021) clarified the “good” and “bad” language context and eliminated the bias by pursuing counterfactual total causal effect.\n\nMost similar to our work, Yang et al. (2021b) and Wang et al. (2021) also visited the issue of attention aggravating the bias. Yang dealt with the confounding effect in vision-language tasks, and Wang resolved to build a more robust attention in OOD settings. However, our IDA is inherently different, especially with Wang et al. (2021) in two respects: 1) Different methods: we steer by the implementation of the confounder and other components when approximating the intervention, while Wang et al. (2021) focusing on the partition and annotation of each confounder explicitly. 2) Different applications: Wang et al. (2021) was aimed at OOD settings, which is a direct task for contextual debiasing, but lacks flexibility due to the requirement of pre-defining or clustering each context. In contrast, our model does not have as many limitations, and thus has the potential to be applied in more areas. Furthermore, Wang et al. (2021) has two defects: being evaluated in limited foregrounds and backgrounds (only animals in 10 contexts in NICO) and performing poorly when there are multiple objects. The two points are particularly attended to in our method because our model works well in full large-scale datasets of multi-label classification and detection.\n\n3 PRELIMINARIES: CAUSALITIES IN CONTEXTUAL BIAS\n\nIn this section, we first demonstrate a causal view of contextual bias in visual recognition. As is conveyed in Fig.2(a), we set up a simple Structural Causal Model (SCM), which clarifies the causality among the context (C), image content (X) and the prediction (Y ). In SCM, each link denotes causalities between two entities, e.g., X → Y means effect Y is generated from X. Although the appearance of SCM describing the contextual bias may be different in different tasks (Yue et al., 2020; Zhang et al., 2020; Yang et al., 2021b; Wang et al., 2021), their essence is depicted in Fig.2(a): an extra element (C) respectively points to a pair of the cause (X) and effect (Y ) where we want to build a mapping. Next, we will explore the rationale behind the SCM in detail.\n\nX → Y denotes the predictions depend on the content in images, which is the desired causal effect: to learn a network mapping an image to its prediction. The prediction is unbiased if the target in X is the only causal relation between X and Y . C → X means the context prior determines how an image is constructed by the contents. For example, if all spoons in a dataset appear on the dining table, the dining table may be regarded as a necessary context when picturing the image of spoon. C → Y exists because the contextual information has a considerable impact on the prediction, i.e., the object itself and its context both affect the recognition of it. X ← C → Y together gives rise to a confounding effect: the network will be fooled into building a spurious causal relation e.g., dining table is taken as the cause for the prediction of spoon. Fig.2(b) further illustrates the role of attention. The attention module can not identify the confounding effect, but focus on intensifying the causal effect X → Y even if this causal link is wrong, causing the deterioration of context bias.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: (a) The proposed causal graph for the causalities in contextual bias. (b) Two concrete causal examples. (c) The schematic diagram of causal intervention.\n\nThe sole way to eliminate the confounder is by causal intervention. The operation preserves the unavoidable context prediction C → Y and cuts off the bad link that an object relies on certain context. Common approaches to realizing intervention include RCT, frontdoor adjustment and backdoor adjustment (Pearl, 2014), while backdoor adjustment is the most frequently used in computer vision:\n\nP(Y |do(X)) =\n\n(cid:88)\n\nc\n\nP(Y |X, C = c)P(C = c),\n\n(1)\n\nwhere the do-operation denotes the causal intervention cutting off the edge C → X as illustrated in Fig.2(c). Free from the interference of confounding path, the network can always learn the unbiased causality between X and Y . Here, X can be extended as attention features. Consequently, the reinforcement from the attention mechanism can lead to a better prediction.\n\n4 METHODOLOGY\n\nIn this section, we present a novel framework that strengthens the robustness of attention on contextual bias. The overview of our model is illustrated in Fig.3. We first introduce our baseline attention: Spatial Class-Aware Attention (SCA), which can obtain class-specific representations but need guidance (Section 4.1). Then, to guarantee the attention module emphasizes the proper causality, we deduce and migrate the backdoor adjustment for the attention, where the intervention is approximated as the multiple sampling with re-weighting upon SCA (Section 4.2). Finally, we present the concrete implementation in our method, i.e., implementing three versions of multiple sampling on SCA (MS-SCA) and implementing the re-weighting as Dot-Product Attention (DPA) or the transformer (Section 4.3).\n\n4.1 CLASS-AWARE LAYER\n\nThe target of our Spatial Class-Aware Attention (SCA) is to bias the spatial representations toward where the objects are the most likely to show up for each category. Given an image, we can obtain its feature map X ∈ RH×W ×D from either a CNN-based or a transformer-based backbone, where D, H, W denotes the channel dimension, height, and width. Our purpose is to turn X into a categoryk=1 ∈ RK×D, where K is the number of the classes. For each aware representations E = {ek}K specific class k, its representation ek is computed from the weighted average of the spatial feature in X. Then, the feature for each category can be reformed according to its unique spatial information:\n\nek =\n\nH (cid:88)\n\nW (cid:88)\n\ni=1\n\nj=1\n\nP(Y = k|X = xi,j)xi,j,\n\n(2)\n\nwhere P(Y = k|X = xi,j) is obtained by feeding xi,j into a linear classifier f clf (.) followed with a softmax(.) regularization.\n\nWe adopt SCA as our baseline attention for two reasons: 1) SCA is useful for multi-object tasks. 2) SCA is easier to be affected by contextual bias (Zhu et al., 2017a; Ye et al., 2020; Zhao et al., 2021). Quite a few works adopt similar class-specific representations, due to the insight and interpretability that they empower the models to capture object-aware features in different areas of different images, which is significant for multi-instance tasks. However, pure SCA works badly because it needs guidance to capture causal locations. SCA is designed to underline crucial positions for each class, while the “crucial position” could be the familiar background due to the bias in the dataset. To tackle this problem, other works mainly employ complicated structures (e.g., GCN or Transformer) to further process the representations. By contrast, we argue that a simple intervention (Sec.4.2) is enough to inspire the potentiality of class-aware attention. In the appendix A.4, we show our framework also gains improvement on other classic attention structures.\n\n4\n\nCXYreinforced by attentionXYreinforced by attentionX : Image (fork)Y : Prediction (fork)√right causalityXYreinforced by attentionX : Image (scene of dining table without fork)Y : Prediction (fork)×wrong causalityCXYreinforced by attention(a)Proposed causal graph (b)Two examples of causal relation(c) Causal interventionPublished as a conference paper at ICLR 2023\n\nFigure 3: Overview of our proposed model. X could be either image feature from visual backbone or ROI feature from detection backbone. The model is composed of the baseline attention (SCA), the multiple sampling on SCA (MS-SCA), and the second attention layer (DPA or transformer).\n\n4.2 CAUSAL INTERVENTION\n\nP(Y |do(X = x)) calculates the probability of Y when all X turns to x, which is infeasible. Thereby, backdoor adjustment indeed uses realistic statistics (without do) to attain the effect equivalent to do-operation. However, it is still challenging that we are required to stratify and sample every possible c for rigorous backdoor adjustment in Eq. 1. In practice, it’s difficult to quantificat every possible context, and consequently the P(C = c) is not explicitly observable.\n\nThanks to the perspective of Inverse Probability Weighting (IPW) (Pearl, 2009), which further reforms the adjustment and simplifies the implementation, we can approximate the sampling on c via the sampling on the observed data, i.e., (k, x). Firstly, we rewrite Eq. 1 to get the equivalent formula:\n\nP(Y = k|do(X = x)) =\n\n=\n\n(cid:88)\n\nc\n\n(cid:88)\n\nc\n\nP(Y = k, X = x|C = c)P(C = c) P(X = x|C = c)\n\n.\n\nP(Y = k, X = x, C = c) P(X = x|C = c)\n\n,\n\n(3)\n\n(4)\n\nwhere 1/P(X = x|C = c) is the so-called inverse weight. Although it’s hard to sample c, in Eq. 4, there is only one (k, x) given one c, thereby, the number of c that Eq. 4 would encounter equals the number of samples (k, x) we observe. As a result, the observed P(Y, X, C) can be used to approximate P(Y = k|do(X = x)), i.e., the essence of IPW lies in “assign the Inverse Weight 1/P(X = x|C = c) to every observed P(Y, X, C), and act as though they were drawn from the post-intervention P(Y = k|do(X = x))” (Pearl, 2009). Hence, Eq. 4 can be further approximated as:\n\nP(Y = k|do(X = x)) ≈\n\nN (cid:88)\n\nn=1\n\nP(Y = k, X = xn, C = c) P(X = xn|C = c)\n\n,\n\n(5)\n\nwhich transforms the summation of C into the sampling of X, and N is sampling times. Here, C in the numerator can be omitted following the common practice of IPW. Then, we model terms in the summation as the sigmoid activated classification probability of the class-aware attention features :\n\nP(Y = k|do(X = x)) =\n\nN (cid:88)\n\nn=1\n\nP(Y = k|X = xn)P(X = xn) P(X = xn|C = c)\n\n=\n\nN (cid:88)\n\nn=1\n\nSigmoid(wken P(X = en\n\nk )P(en k ) k |C = c)\n\n(6)\n\nwhere wk is the classifier weight for class k and ek is the feature of class k in Sec. 4.1. Meanwhile, the denominator, i.e., the inverse weight, can be the Propensity Score (Austin, 2011) in the classification model following Rubin’s theory, where the normalization effect is divided into the treated\n\n5\n\nLogitsperson: 0.99car : 0.87cat : 0.06...backbone(CNN/Transformer)X∈R!×#×$Multiple SamplingTransformer Encoderoptionalperson............carcatBinary Classifier...e1e1e1e2e1ekE∈RK×DDot Product AttentionSpatial Class-Aware AttentionE1∈RK×D!...e1...e2eKe1...e2eKe1...e2eKE2ENMS-SCAInterventionPublished as a conference paper at ICLR 2023\n\nclass-specific group (∥wk∥2 ·∥en confounder (context) in our model is countable, thereby the sampled en ∥en\n\nk ∥2) and untreated class-agnostic group (γ ·∥en\n\nk ∥2 as 1. Finally, we compute the ultimate intervention effect by assembling them as follows:\n\nk ∥2). Moreover, the k is finite and we simplify the\n\nP(Y = k|do(X = x)) =\n\nN (cid:88)\n\nn=1\n\nSigmoid(wken k ) ∥wk∥2 + γ\n\nP(en\n\nk ),\n\n(7)\n\nwhere the implementations of en and P(en show that the SCA with multiple sampling intervention makes up a “1 + 1 > 2” effect.\n\nk ) are unfolded in the next section. In Sec.5.3, we will\n\n4.3 SAMPLING AND RE-WEIGHTING LAYER\n\nIn Eq. 7, the multiple sampling on ek is crucial to the intervention on the attention. Next, given a fixed feature dimension and sampling dimension (e.g., 2048 and 512), we describe several versions of multiple sampling on SCA (MS-SCA): 1) Random sampling: complete randomness is of no mean and unfriendly to backpropagation, hence, we assign random starting points and random intervals for each sample, and the interval for a single sample is fixed. 2) Multi-head (Vaswani et al., 2017): we equally divide the channel into N groups and take each group as a sample. 3) Channel-shuffle: considering the success of PixelShuffle (Shi et al., 2016; Liu et al., 2021), the shuffle on the channel may also be meaningful. In fact, the multi-head operation is a Channel-shuffle with interval 1. Moreover, channel-shuffle can boost the sampling times with different assigned intervals. Interestingly, we will show by experiments that our model is not sensitive to these choices, indicating that the generic multi-sampling behavior is the main reason for the observed improvements.\n\nk ). The simplest way is to assign P(en\n\nFinally is the implementation of P(en k ) as 1/N , which assumes a uniform prior of each sample. However, the status of each sample is unequal. Different positions in channel-wise dimension attend to different objects (Zhu et al., 2017b), hence, to achieve better class-aware features, it is necessary to bias the weight into the more crucial samples rather than the average. Another alternative is to introduce the learnable re-weight parameters, nonetheless, the learning capacity is limited and it is hard to scale up the model.\n\nIn consequence, we adopt another attention to operate the re-weighting. After the multiple sampling, we get sample-specific class-aware feature E ∈ RB×N ×K×D′ , where B denotes the batch size and D′ is sampling dimension. Then, we resize E into the sequence Es ∈ RN ×(B∗K)×D′ . Now, we can implement the scaled Dot-Product Attention (Vaswani et al., 2017) (DPA) to re-weight among different samples, for each class-aware representation:\n\nEs′\n\n= softmax(\n\n(WqEs)(WkEs)T √\nD′\n\n)(WvEs),\n\n(8)\n\nwhere Wq, Wk, Wv are the linear projection mapping the sequence into a common subspace for similarity measure. Thus far, we have explained our lightweight version of IDA. Our method increases almost no parameter except the shared KQV projection, and it outperforms all other models with comparable computation. Furthermore, to scale up the model, Eq. 8 can be naturally extended as the transformer, providing opportunities to trade-off between computation and performance:\n\nEs+1 = max(0, Es′\n\nW1 + b1)W2 + b2.\n\n(9)\n\nEq. 8 and Eq. 9 can continue iterating to build a stronger class-specific representation, which becomes our heavyweight version of IDA. Finally, the class-aware representations are fed into a binary classifier followed by a sigmoid activation, and the average logit of all samples is taken as the final logit for that class, where the classifier shares the same learnable weight with f clf (.) in Sec4.1.\n\n5 EXPERIMENTS\n\nIn this section, we conduct extensive experiments to demonstrate the superiority of the proposed method. We first introduce the general settings. Then, we present our quantitative results of multilabel classification and detection on three widely used datasets: MS-COCO (Lin et al., 2014), VOC2007, and VOC2012 (Everingham et al., 2010). Finally, we perform various ablation studies and analyses to show the effectiveness of different components in our model.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison of mAP (%) on the MS-COCO in multi-label classification. L and H mean the light version and the heavy version respectively, and WH denotes the resolution.\n\nAll\n\nTop3\n\nMethods\n\nResolutions mAP\n\nResNet-101 (He et al., 2016) ML-GCN (Chen et al., 2019c) MS-CMA (You et al., 2020) CSRA (Zhu & Wu, 2021) IDA-R101(L) IDA-R101(H) SSGRL (Chen et al., 2019b) C-Trans (Lanchantin et al., 2021) ADD-GCN (Ye et al., 2020) CCD (Liu et al., 2022) TDRG (Zhao et al., 2021) IDA-R101(L) IDA-R101(H) Swin-Base (Liu et al., 2021) Swin-Large (Liu et al., 2021) IDA-SwinB(H) IDA-SwinL(H)\n\n448 * 448 448 * 448 448 * 448 448 * 448 448 * 448 448 * 448 576 * 576 576 * 576 576 * 576 576 * 576 576 * 576 576 * 576 576 * 576 384 * 384 384 * 384 384 * 384 384 * 384 Table 2: Comparison of mAP (%) between our method and the baseline on the MS-COCO and VOC07 in object detection.\n\nCF1 OF1 CF1 OF1 76.0 76.3 76.3 78.0 77.1 78.4 76.5 77.9 77.3 78.5 77.4 78.7 76.2 76.8 77.6 79.9 77.9 80.1 77.9 80.2 80.4 78.1 78.1 79.8 78.2 80.4 79.9 82.2 80.8 83.6 80.4 83.7 81.1 84.7\n\n80.0 80.3 81.0 80.3 81.1 80.9 79.7 81.7 82.0 82.1 82.4 82.3 82.5 84.0 85.6 85.1 85.9\n\n81.5 83.0 83.8 83.5 84.3 84.8 83.8 85.1 85.2 85.3 86.0 85.5 86.3 88.4 89.3 89.3 90.3\n\n73.5 74.2 74.9 74.4 73.6 73.9 72.7 76.0 75.8 76.0 76.2 75.2 76.4 77.2 78.1 78.0 79.0\n\nMethods\n\nBaseline IDA (L) Baseline IDA (L) Baseline IDA (L)\n\nDetector\n\nFaster-RCNN-ROIAlign-R50 Faster-RCNN-ROIAlign-R50 RetinaNet-R50 RetinaNet-R50 DeTR-DC5-R101 DeTR-DC5-R101\n\nmAP@.5 58.1 59.3 52.5 55.8 64.7 65.7\n\nMS-COCO mAP@.75 40.5 42.0 36.6 38.6 47.7 49.2\n\nbbox-mAP 37.4 38.7 34.3 36.4 44.9 46.1\n\nVOC07 mAP@.5 80.5 83.8 77.8 80.5 -\n-\n\n5.1 EXPERIMENTS SETTINGS\n\nImplementation details. Unless otherwise stated, we use ResNet101 (He et al., 2016) pre-trained on ImageNet 1k (Deng et al., 2009) as our backbone. For the multiple-sampling module, we adopt channel-shuffle with start point = 0, intervals = 1 & 2, and sampling dimension = 512 (1/4 of the full dimension), and thus we have N = 8. γ is set as 1/32. For the heavy version of IDA, we only have 2 layers of transformer and do not implement the multi-head operation on dot-product attention. There is no extra data preprocessing besides the standard data augmentation (Ridnik et al., 2021; Chen et al., 2019c). The multi-label classification model and the detection model are both optimized by the Binary CrossEntropyLoss with sigmoid. We choose Adam as our optimizer with weight decay of 1e − 4 and (β1, β2) = (0.9, 0.9999). The learning rate is 1e − 4 for the batch size of 128 with a 1-cycle policy. All our codes were implemented in Pytorch (Paszke et al., 2017).\n\nEvaluation metrics. For multi-label classification, we adopt the mean average precision (mAP) as our main evaluation metric, and overall/per-class F1-measure (OF1/CF1) as supplements. Separate precision and recall are not employed because they are easily affected by the hyper-parameter. For detection, we employ the mAP@.5, mAP@.75 and mAP@[.5, .95] for MS-COCO, and mAP@.5 for Pascal VOC. In Sec. 5.3, we also evaluate our method on contextual bias benchmarks. The result of classification on Pascal VOC can be found in Appendix A.1.\n\n5.2 QUANTITATIVE RESULTS.\n\nMulti-label classification on MS-COCO. MS-COCO (Lin et al., 2014) is the most popular benchmark for object detection, segmentation and caption, and has also become widely used in multi-label recognition recently. It contains 122,218 images and covers 80 categories with 2.9 for each image on average. Considering the result of multi-label classification is highly related to the resolution and backbone, we perform experiments with different backbones and different resolutions to make the result more persuasive.\n\nAs is demonstrated in Table 1, for CNN backbone, we adopt ResNet101 pre-trained on ImageNet 1k with the resolutions of 448 × 448 and 576 × 576. The results of lightweight and heavyweight ver-\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Ablation studies on components of our method with different resolutions. DPA means the dot-product attention in the second layer and Trans means extending the DPA as a transformer.\n\nComponents\n\nmAP\n\nSCA\n\nMulti-Sampling\n\nDPA\n\nTrans\n\n✓\n\n✓\n\n✓ ✓\n\n✓ ✓\n✓ ✓\n✓\n\n✓ ✓\n✓\n\n✓\n\n448*448 81.4 81.9 82.1 83.6 82.2 84.3 84.8\n\n576*576 82.7 83.0 83.0 84.8 83.1 85.5 86.3\n\nsions are both present for fair comparisons. The first block shows that our light model has an obvious advantage over other methods with comparable computation (e.g., CSRA (Zhu & Wu, 2021)). For heavyweight models, other models universally adopt 3 or more layers of transformer or GNN (e.g., ADDGCN (Ye et al., 2020), C-Trans (Lanchantin et al., 2021) and TDRG (Zhao et al., 2021)), while ours only has two layers but outperforms all of them, confirming the trade-off between computation and performance in our method is more efficient. For transformer backbone, we use the recently popular Swin-Transformer (Liu et al., 2021) pre-trained on ImageNet 22k. The results in the last block convey our improvement. Based on such a strong backbone, it is very difficult to advance the performance, but our method still brings about considerable progress. In Appendix A.2, we further present the comparison of FLOPs and parameters among some of these methods.\n\nObject Detection on MS-COCO and Pascal VOC. Our framework receives the feature map, which can also adopt the ROI features in bounding box as input. Therefore, our method can be seamlessly applied to the detection, including RCNN-like, YOLO-like and DETR-like methods whose neck output the feature maps. To validate that our method can gain improvement in a wider range of visual tasks, we also conduct experiments on object detection on the two widely used datasets. We employ the Faster-RCNN (Ren et al., 2015), RetinaNet (Lin et al., 2017), and DETR (Carion et al., 2020) as our basic detector. The IDA is directly implemented on the RoI feature maps from the neck and the original bbox head is reserved as the residual path which is different from the classification. All the code is implemented in mmdetection (Chen et al., 2019a). The results are shown in Table2. We observe considerable improvements from the baseline on all detectors and datasets, revealing the generalization of IDA on other recognition tasks. Whereas, it must be admitted that the enhancement is not as significant as that in classification, because the role of IDA partly overlaps with the location module in the detection model. Hence, our method obtains greater improvements in the one-stage detection whose object regions are not as accurate as those in two-stage detection.\n\n5.3 ANALYSIS.\n\nEffects of different components in IDA. To evaluate the impact of our two layers of attention and the intervention, we split and reconstruct our model with different ablation components obeying the default setting. Results on MS-COCO with the resolutions of 448 and 576 are both given. For single multiple sampling, we adopt the channel-shuffle with the default setting and P(en k ) = 1/N . As is illustrated in Table3, pure multi-sampling and pure SCA both get very limited improvement due to their respective limitation. However, when combining them together, the results see significant improvements, which confirms our arguments that the attention fits the dataset but suffers from contextual bias, and the intervention mitigates the bias and activates the attention. All above tendencies are more obvious for the resolution of 576. We also implement the DPA on multiple samples without SCA, only to find the retrogress, indicating that the re-weighting in the second layer is meaningless if there is no attention information in the first layer. Then, the dot-product attention and transformer further enhance the result at the cost of computation. The improvement of heavyweight IDA on resolution 576 is more evident, meaning our heavyweight version has better model capacity. In Appendix 11, we will show our framework can gain improvement on various attention structures.\n\nResults on contextual bias benchmark. Contextual bias is widely seen in various visual tasks, meanwhile, there exist benchmarks particularly designed for contextual debiasing (or OOD setting). To directly evaluate the capacity of our method on contextual debiasing, we conduct experiments on two common benchmarks: ImageNet-A (Hendrycks et al., 2021) and MetaShift (Liang & Zou, 2022). For MetaShift, we adopt the standard domain generalization setting with the subset “Cat vs. Dog”. For both datasets, we take the setting without human-annotated context, because there is\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Different implementations of multiple sampling (above) and P(en\n\nSampling baseline SCA Random Sampling Multi-Head (2) Multi-Head (4) Multi-Head (8) Channel-shuffle Channel-shuffle Channel-shuffle Channel-shuffle\n\nk ) (below). P(en k ) -\nAverage Average Average Average Average Average Parameter DPA\n\nTable 5: Comparison among our method and other biasing method on MetaShift and ImageNet-A.\n\nMethods\n\nERM CaaM IDA(L)\n\nMetaShift: Cat vs Dog\n\nd=0.44 84.4 81.9 83.4\n\nd=0.71 60.5 63.8 64.2\n\nd=1.12 35.7 39.2 39.1\n\nd=1.43 24.0 36.1 33.6\n\nImageNet-A\n\n30.6 35.6 35.3\n\nTable 6: Comparsion between our method and CaaM on MS-COCO and contextual biased MS-COCO. OOD MS-COCO 50.0 53.7 58.6\n\nOriginal MS-COCO 81.4 81.0 84.3\n\nMethod Baseline CaaM IDA-L\n\nmAP 81.9 82.9 82.8 83.2 83.4 83.6 83.6 83.9 84.3\n\nno design for utilizing such partitions in our method. We compare our method with the empirical risk minimization (ERM) and IRM-based CaaM (Wang et al., 2021). As is conveyed in Table 5, on MetaShift, we achieve competitive results among the OOD methods. Specifically, ERM performs the best when the shifts are small; our method is best for moderate shifts; CaaM achieves the best when the the shifts are very large. CaaM obtains the best on ImageNet-A, while our methods have comparable result, and ERM performs much worse. It is also worth nothing that our method is more general for other tasks than the ERM-based and IRM-based methods.\n\nNonetheless, the benchmarks mentioned above are based on the single-label classification, where the context of single image is simple. While scenes in the wild are naturally multi-label, and the foreground of an object could be the context for other objects. Thereby, for fair comparison (because our method has specific design for multi-label scene), we sample and build a contextual biased subset from COCO to verify our effectiveness. Specifically, we first select six classes of worst performance (e.g., toothbrush) on the original test set. Then we rank the top5 co-exist classes with these target classes (e.g., toilet for toothbrush). Finally, we choose target classes arising in no frequently co-exist classes as positive samples, and the familiar context without target classes as negative samples. We sample half of the training set on MSCOCO in this way to form an OOD test set, and train on the rest training set. As illustrated by Table 6, our method gains a much more significant improvement, while CaaM achieves modest advancement on OOD MS-COCO and even regresses under normal setting. The results prove our effectiveness under context bias, especially when the instances are multiple, where the tranditional debiasing method does not do well.\n\nImplementation of multiple sampling and P(en k ). In Sec. 4.3, we have introduced three multiplesampling strategies: Random Sampling, Multi-Head, and Channel-Shuffle, as well as three implementations of P(en k ): Average, Parameter (learnable weight) and Dot Product Attention. For Random Sampling and Channel-Shuffle, we adopt sampling dimension = 512 and sampling number = 8 (intervals = 1 & 2 for Channel-Shuffle). For Multi-Head, the sampling number is fixed when the sampling dimension is chosen, thus we choose three sampling dimensions 1024, 512 and 256 with head numbers 2, 4 and 8 respectively. As is uncovered by Table 4, all sampling strategies gain considerable improvement on the original attention, while Random Sampling and Multi-Head with head numbers 2 have lower performance relatively. For the former, random sampling may miss some of the channels and the potential is not fully developed; for the latter, the sample number is too little and the intervention is incomplete. Anyhow, the improvements are not so sensitive to these choices when we have adequate intervention, which indicates the generic multi-sampling behavior is the most important. What’s more, the more sophisticated modeling of P(en k ) gets better results based on the multiple sampling, and our two layers of attention achieve the best.\n\n6 CONCLUSIONS\n\nIn this work, we propose an adaptive and effective Interventional Dual Attention (IDA) framework for visual recognition. We demonstrate that the attention mechanism may aggravate the contextual bias in visual tasks, which is blamed on the confounder in causal theory. Then, we define the intervention on the spatial class-aware attention (SCA) that guides the attention to reinforce the correct causal relation, where is intervention is implemented as multiple sampling with the dot-product attention re-weighting. Finally, extensive experiments on different datasets see improvement for both classification and detection, outperforming the state-of-the-arts on multi-label classification with less computation and better performance, meanwhile, a variety of ablation analyses demonstrate the effectiveness of different components in our method.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nAcknowledgements. This work was supported by Shenzhen Fundamental Research Program (GXWD20201231165807007-20200806163656003) and National Natural Science Foundation of China (No. 62172021).\n\nREFERENCES\n\nSina Akbari, Ehsan Mokhtarian, AmirEmad Ghassami, and Negar Kiyavash. Recursive causal structure learning in the presence of latent variables and selection bias. Advances in Neural Information Processing Systems, 34, 2021.\n\nPeter C Austin. An introduction to propensity score methods for reducing the effects of confounding\n\nin observational studies. Multivariate behavioral research, 46(3):399–424, 2011.\n\nTh ́eodore Bluche. Joint line segmentation and transcription for end-to-end handwritten paragraph\n\nrecognition. Advances in neural information processing systems, 29, 2016.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213–229. Springer, 2020.\n\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019a.\n\nLong Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5659–5667, 2017.\n\nLong Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang. Counterfactual In Proceedings of the IEEE/CVF\n\nsamples synthesizing for robust visual question answering. Conference on Computer Vision and Pattern Recognition, pp. 10800–10809, 2020.\n\nTianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and Liang Lin. Learning semantic-specific graph representation for multi-label image recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 522–531, 2019b.\n\nZhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo. Multi-label image recognition with graph convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5177–5186, 2019c.\n\nMaurizio Corbetta and Gordon L Shulman. Control of goal-directed and stimulus-driven attention\n\nin the brain. Nature reviews neuroscience, 3(3):201–215, 2002.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255. Ieee, 2009.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\n\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303–338, 2010.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15262–15271, 2021.\n\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks.\n\nIn Proceedings of the IEEE\n\nconference on computer vision and pattern recognition, pp. 7132–7141, 2018.\n\nLaurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on pattern analysis and machine intelligence, 20(11):1254– 1259, 1998.\n\nJean Kaddour, Yuchen Zhu, Qi Liu, Matt J Kusner, and Ricardo Silva. Causal effect inference for\n\nstructured treatments. Advances in Neural Information Processing Systems, 34, 2021.\n\nBohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Learning latent causal\n\ngraphs via mixture oracles. Advances in Neural Information Processing Systems, 34, 2021.\n\nJack Lanchantin, Tianlu Wang, Vicente Ordonez, and Yanjun Qi. General multi-label image clasIn Proceedings of the IEEE Conference on Computer Vision and\n\nsification with transformers. Pattern Recognition, pp. 16478–16488, 2021.\n\nQizhu Li, Anurag Arnab, and Philip HS Torr. Weakly-and semi-supervised panoptic segmentation.\n\nIn Proceedings of the European conference on computer vision (ECCV), pp. 102–118, 2018.\n\nWeixin Liang and James Zou. Metashift: A dataset of datasets for evaluating contextual distribution\n\nshifts and training conflicts. arXiv preprint arXiv:2202.06523, 2022.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European conference on computer vision (ECCV), pp. 740–755. Springer, 2014.\n\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ́ar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017.\n\nRuyang Liu, Hao Liu, Ge Li, Haodi Hou, TingHao Yu, and Tao Yang. Contextual debiasing for In Proceedings of the IEEE/CVF Conference on\n\nvisual recognition with causal mechanisms. Computer Vision and Pattern Recognition, pp. 12755–12765, June 2022.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\nLeland Gerson Neuberg. Causality: models, reasoning, and inference, by judea pearl, cambridge\n\nuniversity press, 2000. Econometric Theory, 19(4):675–685, 2003.\n\nIgnavier Ng, Yujia Zheng, Jiji Zhang, and Kun Zhang. Reliable causal discovery with improved exact search and weaker assumptions. Advances in Neural Information Processing Systems, 34, 2021.\n\nYulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: A cause-effect look at language bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12700–12710, 2021.\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nJudea Pearl. Causal inference in statistics: An overview. Statistics surveys, 3:96–146, 2009.\n\nJudea Pearl. Interpretation and identification of causal mediation. Psychological methods, 19(4):\n\n459, 2014.\n\nJudea Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress,\n\n19, 2000.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n\nRonald A Rensink. The dynamic representation of scenes. Visual cognition, 7(1-3):17–42, 2000.\n\nTal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 82–91, 2021.\n\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 618–626, 2017.\n\nWenzhe Shi, Jose Caballero, Ferenc Husz ́ar, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient In Proceedings of the IEEE conference on computer sub-pixel convolutional neural network. vision and pattern recognition, pp. 1874–1883, 2016.\n\nMarijn F Stollenga, Jonathan Masci, Faustino Gomez, and J ̈urgen Schmidhuber. Deep networks with internal selective attention through feedback connections. Advances in neural information processing systems, 27, 2014.\n\nKaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the good and removing the bad momentum causal effect. Advances in Neural Information Processing Systems, 33, 2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nMeng Wang, Changzhi Luo, Richang Hong, Jinhui Tang, and Jiashi Feng. Beyond object proposals: Random crop pooling for multi-label image recognition. IEEE Transactions on Image Processing, 25(12):5678–5688, 2016.\n\nTan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. Visual commonsense r-cnn.\n\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10760– 10770, 2020.\n\nTan Wang, Chang Zhou, Qianru Sun, and Hanwang Zhang. Causal attention for unbiased visual recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3091–3100, 2021.\n\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794–7803, 2018.\n\nYunchao Wei, Wei Xia, Min Lin, Junshi Huang, Bingbing Ni, Jian Dong, Yao Zhao, and Shuicheng Yan. Hcp: A flexible cnn framework for multi-label image classification. IEEE transactions on pattern analysis and machine intelligence, 38(9):1901–1907, 2015.\n\nSanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In Proceedings of the European conference on computer vision (ECCV), pp. 3–19, 2018.\n\nXu Yang, Chongyang Gao, Hanwang Zhang, and Jianfei Cai. Auto-parsing network for image captioning and visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2197–2207, 2021a.\n\nXu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai. Causal attention for vision-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9847–9857, 2021b.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, and Yu Qiao. Attention-driven dynamic graph convolutional network for multi-label image recognition. In Proceedings of the European conference on computer vision (ECCV), pp. 649–665. Springer, 2020.\n\nRaanan Yehezkel Rohekar, Shami Nisimov, Yaniv Gurwicz, and Gal Novik. Iterative causal discovery in the possible presence of latent confounders and selection bias. Advances in Neural Information Processing Systems, 34, 2021.\n\nRenchun You, Zhiyao Guo, Lei Cui, Xiang Long, Yingze Bao, and Shilei Wen. Cross-modality In Proceedings of the\n\nattention with semantic graph embedding for multi-label classification. AAAI Conference on Artificial Intelligence, volume 34, pp. 12709–12716, 2020.\n\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 558–567, 2021.\n\nZhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Interventional few-shot learning.\n\n33, 2020.\n\nZhongqi Yue, Tan Wang, Qianru Sun, Xian-Sheng Hua, and Hanwang Zhang. Counterfactual zeroshot and open-set visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15404–15414, 2021.\n\nDong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, and Qianru Sun. Causal intervention\n\nfor weakly-supervised semantic segmentation. 33, 2020.\n\nVicky Zhang, Chandler Squires, and Caroline Uhler. Matching a desired causal state via shift inter-\n\nventions. Advances in Neural Information Processing Systems, 34, 2021.\n\nJiawei Zhao, Ke Yan, Yifan Zhao, Xiaowei Guo, Feiyue Huang, and Jia Li. Transformer-based dual relation graph for multi-label image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 163–172, 2021.\n\nBeier Zhu, Yulei Niu, Xian-Sheng Hua, and Hanwang Zhang. Cross-domain empirical risk mini-\n\nmization for unbiased long-tailed classification. AAAI, 2022.\n\nFeng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang Wang. Learning spatial regularization with image-level supervisions for multi-label image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5513–5522, 2017a.\n\nKe Zhu and Jianxin Wu. Residual attention: A simple but effective method for multi-label recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 184– 193, 2021.\n\nXizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature flow for video recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2349–2358, 2017b.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Comparison of computation between our method and the state-of-the-arts on the MSCOCO. mAP, FLOPs, and parameters are presented.\n\nMethods\n\nParam.\n\nFLOPs(448)\n\nmAP(448)\n\nmAP(576)\n\nResNet101 CSRA (Zhu & Wu, 2021) C-Trans (Lanchantin et al., 2021) ADDGCN (Ye et al., 2020) CCD (Liu et al., 2022) TDRG (Zhao et al., 2021) IDA(L) IDA(H)\n\n44.7M 45.5M 45.0M 48.2M 48.3M 68.3M 45.6M 55.1M\n\n31.4 31.7 43.3 32.6 32.0 73.7 31.7 33.8\n\n81.5 83.5 -\n- 84.0 84.6 84.3 84.8\n\n82.7 -\n85.1 85.2 85.3 86.0 85.5 86.3\n\nTable 7: Comparison of mAP (%) between our method and the state-of-the-arts on the VOC07 in multi-label classification. mAP∗ means the result of pretraining on MS-COCO.\n\nMethods ResNet101 MLGCN(Chen et al., 2019c) ASL(Ridnik et al., 2021) CSRA(Zhu & Wu, 2021) ADDGCN(Ye et al., 2020) TDRG(Zhao et al., 2021) IDA-R101(L) IDA-R101(H)\n\nmAP\n\n92.9 94.0 94.6 94.7 93.6 95.0 94.5 95.0\n\nmAP∗ -\n- 95.8 96.0 96.0 -\n96.1 96.4\n\nTable 8: Comparison of mAP (%) between our method and the state-of-the-arts on the VOC12 in multi-label classification. All results are tested on official evaluation server.\n\nMethods ResNet101 HCP(Wei et al., 2015) RCP(Wang et al., 2016) SSGRL(Chen et al., 2019b) CSRA(Zhu & Wu, 2021) ADDGCN(Ye et al., 2020) IDA-R101(L) IDA-R101(H)\n\nmAP\n\n92.5 90.5 92.2 93.9 94.1 -\n94.6 95.0\n\nmAP∗ -\n- -\n94.8 95.2 95.5 95.9 96.3\n\nA APPENDIX\n\nIn Appendix, we will include more quantitative results, ablation analyses, and qualitative results.\n\nA.1 MULTI-LABEL CLASSIFICATION ON PASCAL VOC.\n\nSmaller but more delicate, PASCAL-VOC (Everingham et al., 2010) is another widely used dataset in detection and multi-label classification. VOC 2007 has 9,963 images in the dataset with 20 categories, and VOC 2012 has 22,531 images with the same categories. We implement both light and heavy versions of IDA obeying the default setting. Note that some works train the model from scratch (Chen et al., 2019c; Zhao et al., 2021), while others load the COCO-pretrained model (Ridnik et al., 2021; Ye et al., 2020). Hence, for fair comparisons, we implement both of them. Table7 reports the results on VOC 2007. It can be seen that IDA outperforms previous methods, even when some of those models have stronger backbones (e.g., ASL (Ridnik et al., 2021)) or resolutions (e.g., ADDGCN (Ye et al., 2020)). The results of VOC 2012 are conveyed in Table8. Different from VOC 2007, results on VOC 2012 are constrained to be tested on the official evaluation server to guarantee fairness. As is shown, our method also outperforms other state-of-the-art methods by a larger margin.\n\nA.2 THE FLOPS AND PARAMETERS.\n\nIn the main body, we claim that the proposed method can obtain performance improvement with low computations. To verify the argument, we choose the most competitive method recently. We compute the FLOPs and parameters of these methods (of our implementation) using ptflops, and compare them with the baseline and our model. The results are presented in Table 9. Among lightweight models, our model achieves the best performance with similar FLOPs and parameters. For heavyweight models (TDRG), our model achieves better results with half of the computation, indicating our method finds a better way to utilize the increment of parameters.\n\nA.3 DOES IDA ACHIEVE ACCURATE ATTENTION?\n\nIn Sec.1, we uncover that pure attention can not obtain precise attention map. To confirm it experimentally and to validate our method achieves a more robust attention map, we calculate the\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 10: The class-specific (C-S) and image-specific (I-S) attention mIOU (%) of different models. C-S mIOU\n\nI-S mIOU\n\nMethods\n\nbaseline SCA MS-SCA IDA\n\n16.8 17.0 19.2 20.5\n\n22.2 22.3 25.1 26.5\n\nTable 11: IDA on different attention structures. Results of mAP on MS-COCO are presented.\n\nVanilla +IDA(L)\n\nSENET 81.6 82.5\n\nCBAM 81.8 83.0\n\nNon-local 82.0 83.0\n\nGCNET 82.4 83.5\n\nSCA 81.9 84.3\n\nmIOU between the attention map and the ground truth segmentation mask in MS-COCO, which provides an exact evaluation of whether attention emphasizes the right position. In fact, computing Pseudo-Masks from CAM (Selvaraju et al., 2017) of the classification model is a common approach in Weakly-Supervised Semantic Segmentation (Li et al., 2018). For the baseline, we use CAM from the last layers before the classifier as mask, for bare SCA, we use the attention map, and for interventional SCA, we use the average (Multi-head sampling SCA with P(en k ) = 1/N ) and the weighted average (IDA) of the attention map from different heads. We compute both image-specific and class-specific mIOU to evaluate the model in different aspects. As is reported in Table 10, pure attention brings about very limited improvement in the attention IOU compared with the baseline, indicating the attention does not help the model find a better location of instances. While our full model outperforms the pure attention and the baseline in both metrics, which proves quantitatively that our method obtains a more accurate and robust attention map.\n\nA.4\n\nIDA FOR OTHER ATTENTION MODULES.\n\nRecent years have witnessed numerous attention structures for various visual tasks. Considering our method has the potential to be flexibly migrated to different attention modules, we also implement the IDA on four typical visual attention: Non-local, SENET, GCNET, and CBAM on COCO classification. As is shown in Table 11, IDA can give rise to improvement in different attention modules, though the SCA sees the best result and the most progress, because the SCA is the most applicable for the multi-target tasks, whereas it is also the most likely to be affected by the confounding context.\n\nFigure 4: The influence of head number in IDA. We use the IDA with multi-head sampling .\n\nFigure 5: Ablation studies on the number of transformer layers in IDA (H).\n\nA.5 EFFECTS OF SAMPLING NUMBER.\n\nA larger sampling dimension and more sampling numbers mean more fine-grained sampling and better results, however, the computation also rises up rapidly for self-attention when the sequence is longer. Hence, it is important to carefully choose a rational sampling number. In Fig.4, we study the impact of sampling numbers. For fair comparison, we adopt the multi-head sampling, and project different head dimensions (of different head numbers) into the same 512. As is shown, more sampling heads bring about more improvement, especially in the early stage, where one more sampling means a much more fine-grained approximation for the intervention. The improvement converges when head numbers become large, while the computation keeps rising. As a result, 4-8 heads achieve better trade-offs between performance and cost.\n\n15\n\n31.332.333.334.38282.58383.58484.51234681216FLOPs.head num.mAP (%)mAPFLOPs.8484.284.484.684.801234mAP ()%layer num.Published as a conference paper at ICLR 2023\n\nFigure 6: Visualization of baseline SCA and our IDA (L) using CAM, which comes from the spatial attention weight of each class. For our method, we adopt multi-head sampling and use the attention map in different sampling heads and their weighted average, whose weight comes from DPA in the second layer.\n\nA.6 DEEPER, BETTER?\n\nIn theory, the transformer extended from DPA can iterate for many rounds to get a larger model capacity. Hence, we show results on MS-COCO with different layers of transformer for our IDA (H). As is shown in Fig.5, the performance improves when we add the FFN and another layer of transformer for IDA (L), but sees invariability or drop when the layer continues to increase. The possible reason may lie in the function of the second layer that aims at re-weighting the class representations in different samples. Two layers of the transformer are enough to fit needed weights for different samples, and adding more transformer layers is of little use.\n\nA.7 QUALITATIVE RESULTS.\n\nTo demonstrate the effect of our method intuitively, we visualize the parameters via GradCAM (Selvaraju et al., 2017). For the baseline SCA, we use the spatial attention maps of each class, and for our method, we adopt the multi-head sampling of 4 heads and visualize the attention maps of SCA in different heads as well as their weighted average. The average weight comes from the result of softmax(QK T /(cid:112)D/N ) in the DPA of IDA (L). The result is illustrated in Fig.6. In general, the attention in different samples attends to different things: some for the target objects,\n\n16\n\nInputSCAHead1Head2Head3Head4Averagelabel: personweight: 0.65weight: 0.1weight: 0.15weight: 0.1label: carweight: 0.8weight: 0.1weight: 0.02weight: 0.08label: forkweight: 0.36weight: 0.05weight: 0.02weight: 0.57label: phoneweight: 0.12weight: 0.03weight: 0.55weight: 0.3label: handbagweight: 0.02weight: 0.2weight: 0.69weight: 0.09label: toothbrushweight: 0.39weight: 0.11weight: 0.42weight: 0.08Published as a conference paper at ICLR 2023\n\nsome for the whole background and some for the important context. Then, the weight from the second layer assembles the results, and the weight value is rational as shown. The comparisons between the baseline and our final result further reveal our effectiveness. For high-frequency classes (e.g., person and car), our method can locate the fine-grained or hard instances precisely. For the classes easily misled by the context (e.g., fork and phone), our attention still finds the target exactly and is not biased to the familiar context. Finally, for the rare or hard classes (e.g., toothbrush and handbag), our attention map is inevitably biased more to the context. Compared with the baseline, however, our method can still include the target in the active locations. In the future, we resolve to develop a more fine-grained method to achieve accurate activation maps for these hard instances. Moreover, the context is not always bad for the predictions, e.g., when the training and test set have the same bias distributions. Therefore, it may be better to develop a self-adapting method to judge if the task needs to mitigate contextual bias. As is shown in the visualization, different samples in our method indeed attend to different information, creating possibilities for the above assumptions, which will be studied in our future work.\n\n17",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes an attention mechanism that is more robust to misleading visual context. The motivation follows from the fact that current recognition models tend to mistake correlations (e.g. between background and objects) for causal factors, leading to mistaken predictions when e.g. observing the background without the foreground object.\n\nThe proposed attention mechanism comes in two variants: one that relies on two successive attention layers, and a more heavyweight version which involves a transformer-based encoder inserted between the aforementioned layer. These extra processing layers are combined with a form of sampling-based intervention, which should allow the model to ignore spurious context.\n\nThe method is evaluated on two multi-label classification tasks (MS-COCO and Pascal VOC), and compares favourably to competing methods. An ablation study shows how different components of the approach improve over the baseline.\n\n# Strength And Weaknesses\n\nI struggled to understand the approach and in particular to connect the stated motivation with the method being proposed as well as the results. The paper starts by describing an often observed problem, namely that common recognition methods struggle when the combination of foreground and context violates expectations. Based on the presentation, it is neither clear how the proposed method would address this, nor why the (small) improvements over the baseline indicate that the problem is indeed being successfully addressed. \n\nThe presentation of \"causalities in contextual bias\" in section 3 is confusing. The presented structural causal models (see Fig 2) only model the problem at a very abstract level. Its also for example not clear what is being proposed in (2a), if the directions of the edges make sense, why a misleading relationship between X & Y might be reinforced specifically by attention, and what it means to break the connection between the context and the image (2c) both conceptually and in practice. The connection between what is presented here and the method is very tenuous. How is the \"multiple-sampling operation with Dot-Product Attention (DPA) re-weighting\" \"essentially [a] causal intervention\"? What happens in the case when the context is necessary to correctly identify e.g. a small/low-resolution object? \n\nThe paper argues that an attention map that tracks more closely with the ground truth mask of the object is a sign of the method's success. Such results are presented e.g. in Fig 1b and in the Appendix (A.4, see final paragraph in section 5). These results show modest improvements and are also based on the faulty assumption that the context is to be ignored lest it be faulty, when in fact one would rather prefer a model that can take context into account when needed.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAs described above, the paper is not at all clear when it comes to connecting motivation to method and to results. The empirical results show modest improvements over the baselines, but these may very well be the result of additional model capacity rather than some form of causal reasoning as advertised.\n\n# Summary Of The Review\n\nThe paper presents a method that to me is only tenuously connected to the stated motivation. It is hard to connect the stated need for causal reasoning to overcome misleading context and the exact operations carried out by the proposed layer. As such, I believe this paper tells a story that is not quite supported by the method and the experiments. I am open to being corrected as this assessment may be the result of a misunderstanding.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents a novel approach to mitigating contextual bias in visual recognition tasks through a proposed attention mechanism called Interventional Dual Attention (IDA). This model-agnostic module employs dual attention layers and multiple sampling interventions to address the confounding effects of context on predictions. The authors demonstrate IDA's effectiveness across various datasets, achieving state-of-the-art results in multi-label classification on MS-COCO and PASCAL-VOC, while also maintaining computational efficiency.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of the issues posed by contextual bias in attention mechanisms and the innovative introduction of IDA as a solution. The extensive empirical evaluation across multiple datasets enhances the credibility of the proposed method, showcasing its robustness and efficiency. However, a potential weakness is the reliance on a heavy version of IDA, which may raise concerns about computational overhead in practical applications. Additionally, while the paper discusses causal frameworks, a deeper exploration of the implications of these interventions could strengthen the theoretical underpinning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and experimental results. The writing quality is high, with clear explanations of complex concepts. The novelty of the proposed IDA lies in its dual attention mechanism, which is well-conceived and presented. However, reproducibility could be enhanced by providing more detailed implementation specifics or code, especially given the model-agnostic nature of IDA.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in addressing contextual bias in visual recognition through the innovative Interventional Dual Attention mechanism. The empirical results support the effectiveness of the proposed method, although further details on implementation could improve reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper addresses the issue of contextual bias in visual recognition, which arises from attention mechanisms that can lead to spurious associations between objects and their contexts. The authors propose the Interventional Dual Attention (IDA) module, incorporating two attention layers and multiple sampling interventions to minimize the impact of confounding contexts. The IDA framework includes a Spatial Class-Aware Attention (SCA) baseline and a Dot-Product Attention (DPA) layer, demonstrating significant improvements in multi-label classification and object detection tasks across various datasets, including MS-COCO and PASCAL-VOC. The results indicate that IDA effectively mitigates contextual bias while achieving state-of-the-art performance.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing contextual bias through a causal framework, which is a novel contribution to the field. The versatility of the IDA module allows integration with various backbone models, enhancing its applicability across different visual recognition tasks. The comprehensive evaluation across multiple datasets and tasks robustly validates the proposed method. However, the complexity of implementing multiple sampling strategies may pose challenges to practical application. Additionally, the method still faces limitations in handling contextual bias, particularly in rare classes, and its effectiveness may be affected by the quality of training datasets. The computational costs associated with the heavy version of the model could also hinder its scalability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear presentation of the problem statement, methodology, and findings. The quality of experiments is high, supported by extensive evaluations and ablation studies. The novelty of the approach is significant, particularly in its framing of attention mechanisms within a causal context. Reproducibility is facilitated by detailed implementation descriptions, although the complexity of the approach may require additional guidance for practical replication.\n\n# Summary Of The Review\nOverall, this paper presents a novel and effective solution to the issue of contextual bias in visual recognition through the IDA module, demonstrating significant improvements across various tasks. While the approach shows great promise, challenges related to implementation complexity and contextual bias in rare classes remain.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents \"Causality Compensated Attention for Contextual Biased Visual Recognition,\" which introduces the Interventional Dual Attention (IDA) framework. This model-agnostic attention module compensates for contextual confounding through causal interventions, leveraging theoretical insights from causal inference literature. The authors demonstrate that IDA achieves state-of-the-art performance in multi-label classification tasks on the MS-COCO and PASCAL-VOC datasets, outperforming existing models while maintaining lower computational costs.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing contextual bias in visual recognition through a solid theoretical foundation in causality. The methodology for implementing causal interventions within attention mechanisms is well-articulated, and the experimental results are compelling, showing significant improvements across various tasks and architectures. However, the paper could benefit from a more detailed exploration of the limitations of the proposed approach, particularly concerning computational overhead and scalability with more complex datasets or tasks. Additionally, while the theoretical insights are robust, the practical implications in real-world applications could be more thoroughly discussed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a logical progression from theoretical foundations to methodology and experimental results. The clarity of the writing is commendable, making complex ideas accessible. The novelty of the approach, integrating causal inference with attention mechanisms, marks a significant contribution to the field. However, reproducibility could be improved by providing more detailed implementation specifics, including hyperparameter settings and code availability.\n\n# Summary Of The Review\nOverall, the paper provides a significant contribution to the intersection of attention mechanisms and causal inference, addressing a critical issue in visual recognition tasks. The proposed IDA framework is both innovative and effective, although further exploration of its limitations and reproducibility could enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel attention mechanism called Interventional Dual Attention (IDA), aimed at reducing contextual bias in visual recognition tasks. The authors argue that existing attention mechanisms often exacerbate bias, and IDA is designed to address this issue. The methodology includes extensive empirical validation on benchmark datasets such as MS-COCO and PASCAL-VOC, demonstrating significant performance improvements in classification and detection tasks. Notably, IDA is model-agnostic, allowing its implementation across various backbone architectures while maintaining computational efficiency with minimal parameter increments.\n\n# Strength And Weaknesses\nThe paper's strengths include its novel contribution to attention mechanisms, providing a solid theoretical foundation rooted in causal theory, and demonstrating empirical effectiveness through rigorous experiments. The model-agnostic nature of IDA offers versatility, making it applicable across different tasks. However, weaknesses include limited validation across diverse datasets, insufficient detail on performance across various backbones, and a lack of comprehensive comparisons regarding computational efficiency against state-of-the-art methods. Additionally, while ablation studies are informative, qualitative analysis and visualizations could enhance understanding of the model's behavior.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, effectively presenting the rationale behind the proposed IDA mechanism. The quality of the experiments is high, with thorough empirical validation, though some aspects, such as the theoretical framework and visualizations, could be more detailed. The novelty of the contribution is significant, addressing an important gap in current literature. However, reproducibility may be hindered by the lack of comprehensive details on the implementation and performance across various contexts.\n\n# Summary Of The Review\nOverall, the paper introduces an innovative attention mechanism that effectively addresses contextual bias, supported by solid empirical evidence and a strong theoretical foundation. While the contributions are noteworthy, further validation and more comprehensive analyses would enhance the robustness and applicability of the proposed method.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Causality Compensated Attention for Contextual Biased Visual Recognition\" by Ruyang Liu et al. addresses the shortcomings of conventional attention mechanisms in visual recognition systems, particularly their susceptibility to contextual biases that lead to incorrect predictions. The authors introduce a novel approach called Interventional Adaptive Attention (IAA), which enhances the attention mechanism by adapting attention weights based on contextual relevance to counteract these biases. The methodology combines a causal framework with an adaptive attention mechanism, demonstrating significant improvements in multi-label classification and object detection tasks across several benchmark datasets, notably MS-COCO and PASCAL-VOC.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing contextual bias in visual recognition through the introduction of the IAA mechanism, which effectively utilizes causal reasoning to enhance attention mechanisms. The empirical validation is robust, showcasing superior performance over baseline models in various tasks while maintaining lower computational costs. However, the paper could benefit from a more in-depth analysis of the limitations of the proposed method and potential scenarios where the IAA might underperform compared to traditional methods. Furthermore, while the extensive experiments are commendable, additional real-world application studies could further substantiate the practical implications of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the proposed methodology and its underlying causal framework. The writing quality is high, making the technical content accessible to a broad audience. The novelty of the IAA approach is significant, as it introduces a fresh perspective on integrating causality in attention mechanisms. Reproducibility is facilitated by the detailed descriptions of the experimental setup and the results, although providing access to the code and datasets used in the experiments would enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in visual recognition by proposing the Interventional Adaptive Attention mechanism, which effectively mitigates contextual biases in predictions. The empirical results strongly support the effectiveness of the approach, though the paper could be improved by addressing potential limitations and enhancing reproducibility through code availability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Adversarial Training with Causal Mechanisms for Robust Visual Recognition\" by Ruyang Liu et al. presents a novel approach to adversarial training in visual recognition tasks by integrating causal mechanisms to enhance model robustness. The authors propose a new framework called Interventional Adversarial Training (IAT), which seeks to mitigate the impact of contextual biases that often lead to poor model generalization when faced with adversarial examples. Through systematic methodology, IAT employs causal interventions during training, allowing models to focus on true causal relationships instead of spurious correlations. Extensive experiments conducted on benchmark datasets such as MS-COCO and PASCAL-VOC show that IAT not only improves robustness against adversarial attacks but also maintains competitive performance on standard tasks, achieving state-of-the-art results in multi-label classification.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of causal mechanisms into adversarial training, which addresses a significant gap in the existing literature. The empirical validation across multiple datasets is comprehensive, showcasing the effectiveness of IAT in real-world scenarios. Additionally, the clear and systematic methodology enhances the paper's contribution to the field. However, there are notable weaknesses, including the potential complexity involved in implementing causal interventions, which may hinder accessibility for practitioners. Furthermore, the authors do not thoroughly explore the limitations of their approach in specific contexts or datasets, which could provide a more balanced view of its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to the reader. The quality of writing is high, with a logical flow that aids in understanding the methodology and findings. The novelty of incorporating causal reasoning into adversarial training is significant and adds a fresh perspective to ongoing research in the field. However, the reproducibility may be affected by the complexity of the causal interventions proposed, necessitating further clarifications in implementation details to enhance practical applicability.\n\n# Summary Of The Review\nOverall, the paper presents a substantial advancement in adversarial training by effectively integrating causal mechanisms, backed by thorough empirical validation. While it offers impactful insights into enhancing model robustness, the complexity of implementation poses challenges for broader adoption in practice.\n\n# Correctness\n4/5 - The methodology and results appear sound, but further exploration of limitations could enhance the correctness assessment.\n\n# Technical Novelty And Significance\n5/5 - The integration of causal mechanisms into adversarial training represents a significant technical advancement in the field.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results are strong and support the proposed methodology, although additional context on limitations would enhance significance.",
    "# Summary Of The Paper\nThe paper titled \"Causality Compensated Attention for Contextual Biased Visual Recognition\" introduces the Interventional Dual Attention (IDA), an attention mechanism aimed at mitigating contextual bias in visual recognition tasks. The authors claim that current attention mechanisms exacerbate contextual bias and present IDA as a model-agnostic solution that enhances robustness in classification and detection tasks. The paper includes extensive experimental evaluations purportedly demonstrating significant improvements in performance metrics such as mean Average Precision on datasets like MS-COCO and PASCAL-VOC. However, the evidence supporting these claims is not sufficiently robust, as the improvements appear modest and inconsistent.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its ambitious goal of addressing contextual bias through a new attention mechanism. The proposed IDA framework is described in detail, and the inclusion of ablation studies is a positive aspect, allowing some insights into the method's components. However, the paper suffers from overstatements of its contributions, as the claimed improvements in performance may be inflated and lack rigorous comparison to baseline models. The methodology is complex, which may obscure the incremental nature of the advancements made. Furthermore, the limitations of the approach are not adequately discussed, particularly regarding its application in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas with a degree of clarity, although the complexity of the proposed framework may hinder comprehension for some readers. The quality of the writing is reasonably good, but the inflated claims regarding novelty and performance could mislead readers. The reproducibility of the results may be compromised due to insufficient detail in the experimental setup and a lack of transparency regarding the baseline comparisons. Overall, while the methodology is sound, the presentation of results raises concerns about the reliability of the findings.\n\n# Summary Of The Review\nIn summary, the paper introduces an innovative attention mechanism with the goal of reducing contextual bias in visual recognition tasks. However, the claims of significant improvements are overstated, and the actual contributions appear to be more incremental than revolutionary. A more critical analysis of the results is necessary to substantiate the authors' claims.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel model-agnostic attention mechanism called Interventional Dual Attention (IDA) designed to mitigate contextual bias in visual recognition tasks. The authors propose a two-layer attention framework that combines Spatial Class-Aware Attention (SCA) with a Dot-Product Attention (DPA) mechanism, employing multiple sampling interventions to enhance object representation. The experimental results demonstrate that IDA outperforms baseline models across various tasks, including multi-label classification and object detection on the MS-COCO dataset, achieving notable improvements in mean Average Precision (mAP) while also effectively addressing contextual bias as evidenced by performance on ImageNet-A and MetaShift benchmarks.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to combating contextual bias in visual recognition, making significant advancements in both methodology and empirical results. The dual attention mechanism and the introduction of multiple sampling interventions are well-justified and effectively implemented, leading to substantial performance gains. However, the paper could benefit from a more detailed discussion on the limitations of the proposed approach, particularly regarding its scalability to larger datasets or real-time applications. Additionally, while the results are promising, the reported metrics could be contextualized with more baseline comparisons to highlight the improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly outlines the methodology, experimental setup, and findings. The explanations of the IDA framework and its components are accessible, facilitating understanding. The empirical results are presented with appropriate statistical backing, including ablation studies, which bolster the claims made by the authors. However, reproducibility could be enhanced with further details on the implementation specifics and hyperparameter settings used in the experiments. Overall, the novelty of the approach is significant, but the clarity in how it could be replicated needs improvement.\n\n# Summary Of The Review\nThis paper introduces a compelling approach to reducing contextual bias in visual recognition tasks through the Interventional Dual Attention framework. The methodology is innovative and demonstrates strong empirical results; however, there are areas for improvement in discussing limitations and enhancing reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel Interventional Dual Attention (IDA) mechanism aimed at mitigating contextual bias in image classification tasks. The authors propose that IDA performs causal interventions through multiple sampling and re-weighting of contextual factors, thereby enhancing model performance across visual recognition challenges. The findings suggest that IDA improves accuracy on benchmark datasets like MS-COCO and PASCAL-VOC, although the methodology's generalizability and robustness to diverse contexts remain underexplored.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including a clear outline of the IDA mechanism and its intended purpose of addressing contextual bias, which is a pertinent issue in visual recognition. However, it also has notable weaknesses. The reliance on assumptions about causal structures and the uniformity of contextual factors raises questions about the method's applicability across different datasets and tasks. Additionally, the evaluation metrics used may not adequately capture the nuances of model performance in complex scenarios, and there is insufficient exploration of alternative methods that address similar challenges.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a logical flow and clear explanations of the proposed methodology. However, the novelty is somewhat limited by the lack of rigorous testing across various architectures and datasets, which hinders reproducibility. The authors' claims regarding the model-agnostic nature of IDA lack empirical support, and the potential for overfitting to historical datasets raises further concerns about the generalizability of their findings.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to addressing contextual bias in visual recognition through the IDA mechanism. However, several assumptions made by the authors and the limited exploration of alternative methods may undermine the robustness and generalizability of their findings. The work would benefit from a more thorough evaluation of its claims and a broader comparison with existing techniques.\n\n# Correctness\nRating: 3/5\n\n# Technical Novelty And Significance\nRating: 3/5\n\n# Empirical Novelty And Significance\nRating: 2/5",
    "# Summary Of The Paper\nThe paper introduces the Interventional Dual Attention (IDA) module, a novel approach designed to mitigate contextual bias in visual recognition tasks. The methodology is based on a Structural Causal Model (SCM) that distinguishes true correlations from spurious ones, thereby allowing for causal interventions in attention mechanisms. The findings demonstrate that IDA leads to significant improvements in both classification and detection tasks across several datasets, including MS-COCO and PASCAL-VOC, achieving state-of-the-art performance while maintaining computational efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing contextual bias through the lens of causality, which is a relatively underexplored area in attention mechanisms for visual tasks. The extensive experiments and ablation studies provide robust evidence for the effectiveness of IDA, showcasing clear performance gains over baseline models. However, a potential weakness is the lack of a comprehensive comparison with existing state-of-the-art methods that also address contextual bias, which could strengthen the claims of superiority for IDA. Additionally, the paper could benefit from a more detailed discussion on the computational efficiency of the IDA framework in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The logical flow from the introduction through to the experiments facilitates understanding, and the use of a causal framework for explaining contextual bias is both novel and insightful. The reproducibility of the results appears to be high, as the authors provide clear descriptions of the implementation details and experimental setups. However, the paper could improve by including code or supplementary materials to aid in reproducing the experiments.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of visual recognition by proposing the IDA module, which effectively addresses contextual bias through causal interventions. While the contributions are strong and well-supported by empirical evidence, a more extensive comparison with existing methods would further enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework aimed at enhancing model interpretability in deep learning. The authors propose a new method that utilizes a combination of attention mechanisms and local perturbation to provide meaningful insights into model decisions. The findings suggest that the proposed approach not only improves interpretability but also maintains competitive performance on benchmark datasets, thereby addressing a critical gap in existing interpretability techniques.\n\n# Strength And Weaknesses\n**Strengths:**\n- **Relevance:** The focus on model interpretability is particularly timely given the increasing deployment of ML models in critical applications where understanding decision-making is essential.\n- **Innovation:** The integration of attention mechanisms with local perturbation strategies presents a unique approach that stands out from traditional interpretability methods.\n- **Empirical Results:** The authors provide a robust set of experiments demonstrating the effectiveness of their method across multiple datasets, showcasing both interpretability and model performance.\n\n**Weaknesses:**\n- **Limited Comparison:** While the empirical results are compelling, the paper could benefit from a broader comparison with state-of-the-art methods for interpretability to fully establish its advantages.\n- **Scalability Concerns:** The paper does not sufficiently address the scalability of the proposed method when applied to larger models or datasets, which may limit practical applicability.\n- **Depth of Analysis:** The discussion around the implications of the interpretability findings could be expanded to provide deeper insights into the potential applications and limitations of the method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, making it accessible to readers with varying levels of expertise. The methodology is described clearly, though it could benefit from additional details regarding implementation steps to enhance reproducibility. The novelty of the approach is significant, combining established techniques in innovative ways; however, the empirical validation could be strengthened to highlight the method's effectiveness relative to existing solutions.\n\n# Summary Of The Review\nThis paper presents a promising approach to enhancing interpretability in deep learning models through a novel framework combining attention mechanisms and local perturbation. While the contributions are significant, the authors should address concerns regarding the scalability and provide a more comprehensive comparison with existing methods to solidify their claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel attention mechanism called Interventional Dual Attention (IDA) designed to address contextual bias in visual recognition tasks. Traditional attention models often misidentify object representations by incorrectly linking objects to their contexts, leading to spurious correlations. IDA employs two attention layers with multiple sampling interventions to reinforce correct causal relationships between objects and their contexts. The authors demonstrate that IDA is model-agnostic and applicable to various backbone architectures, achieving significant performance improvements in classification and detection tasks, including state-of-the-art results in multi-label classification on MS-COCO and PASCAL-VOC datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to mitigating contextual bias through a causal lens, which is a relevant and timely issue in visual recognition. The proposed IDA framework is well-justified, supported by extensive experimental results, and demonstrates its effectiveness across various tasks. However, a potential weakness is the lack of in-depth theoretical analysis regarding the causal mechanisms employed, which could enhance the understanding of the IDA’s underlying principles. Additionally, while the empirical results are promising, the paper could benefit from a more extensive comparison with existing methods beyond the highlighted state-of-the-art performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly articulates the problem of contextual bias in visual attention mechanisms. The methodology is described in a systematic manner, allowing readers to understand the proposed IDA framework and its implementation. The novelty of the approach is significant, as it takes a causal perspective in addressing a recognized challenge in the field. In terms of reproducibility, the paper provides adequate details on the experimental setup, although including more hyperparameter settings and implementation specifics would further facilitate replication efforts.\n\n# Summary Of The Review\nOverall, this paper presents a meaningful contribution to the field of visual recognition by addressing contextual bias through an innovative attention mechanism. The proposed IDA framework is both effective and applicable across various architectures, but it could benefit from deeper theoretical insights and broader empirical comparisons.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel attention mechanism called Interventional Dual Attention (IDA) aimed at mitigating contextual bias in visual recognition tasks. The authors argue that traditional attention mechanisms often fail to accurately capture essential object representations due to contextual influences. By leveraging a causal framework, IDA integrates two types of attention layers—Spatial Class-Aware Attention (SCA) and Dot-Product Attention (DPA)—and employs multiple sampling interventions to adjust attention weights. The results demonstrate that IDA significantly outperforms baseline models in multi-label classification and detection tasks on benchmark datasets such as MS-COCO and PASCAL-VOC, achieving state-of-the-art results with reduced computational overhead.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing contextual bias through a causal lens, which is a relatively underexplored area in the literature. The proposed IDA mechanism is model-agnostic, enabling its application across various architectures, which enhances its practical relevance. However, a potential weakness is the lack of extensive discussion on the limitations of the proposed method, particularly regarding its applicability to more complex scenarios or its performance on datasets beyond those tested. Additionally, while the results are promising, further validation on real-world applications could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe manuscript is well-structured and clearly articulates its contributions, methodology, and results. The novelty of the proposed IDA mechanism is evident, particularly in its integration of causality with attention mechanisms. However, while the experiments demonstrate significant improvements, the reproducibility of the results could be better supported by providing additional details on experimental setup and hyperparameter choices. Including code or supplementary material would facilitate further validation by the research community.\n\n# Summary Of The Review\nOverall, this paper presents a compelling contribution to the field of visual recognition by addressing contextual bias through a novel attention mechanism. The proposed IDA shows significant promise in improving classification and detection tasks, although further exploration of its limitations and broader applicability would enhance the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach called Interventional Dual Attention (IDA) aimed at mitigating contextual bias in visual recognition tasks. The authors detail the methodology behind the IDA module, which integrates attention mechanisms with causal inference principles to enhance multi-label classification and detection performance. Through extensive experiments on benchmark datasets such as MS-COCO and PASCAL-VOC, the findings indicate significant improvements in evaluation metrics, including mean Average Precision (mAP) and F1 scores, showcasing the effectiveness of the proposed method.\n\n# Strength And Weaknesses\nThe strengths of the paper include its thorough literature review, which contextualizes the importance of attention mechanisms in visual tasks, and the clear articulation of the problem related to contextual bias. The methodology is well-structured, providing a detailed breakdown of the IDA module. However, while the experimental results are promising, further exploration of potential limitations and broader applicability of the method could enhance the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable; it is well-organized, with clear definitions and appropriate technical language. The figures and tables effectively support the findings, enhancing understanding. The novelty of the IDA approach in addressing contextual bias is significant and well-articulated. Reproducibility is facilitated by providing comprehensive details on the experimental setup and evaluation metrics, though the authors could strengthen this aspect by including code or additional resources.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of computer vision by proposing a novel method to address contextual bias in visual recognition tasks. The research is well-executed and documented, although further exploration of limitations could enhance its robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces the Interventional Dual Attention (IDA) model, a novel attention mechanism designed to mitigate contextual bias in visual recognition tasks. The methodology employs a Structural Causal Model (SCM) to delineate causal relationships among context, image content, and predictions, addressing the confounding effects that can arise from contextual elements during training. The findings demonstrate that IDA outperforms existing attention mechanisms in multi-label classification and object detection tasks, showcasing significant improvements in mean Average Precision (mAP) metrics on benchmark datasets such as MS-COCO and PASCAL-VOC.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its rigorous causal analysis and the introduction of the IDA framework, which effectively combines spatial class-aware attention and multiple sampling interventions. This innovative approach addresses a critical gap in existing attention mechanisms by reducing misattribution of causal relationships. However, a potential weakness is the complexity of the model, particularly the heavyweight variant, which may limit its applicability in resource-constrained environments. Additionally, while the experimental results are compelling, more extensive comparisons with a broader range of models could strengthen the validation of IDA's efficacy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the motivations, methods, and results. The descriptions of the causal intervention framework and the IDA mechanism are detailed, allowing for a good understanding of the underlying principles. The quality of the experimentation appears robust, with thorough ablation studies supporting the claims made. The novelty of the approach is significant, as it introduces a causal perspective to the design of attention mechanisms. Reproducibility is facilitated through the clear exposition of methodologies, although providing access to code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of visual recognition by addressing contextual bias through a novel causal intervention framework. The IDA mechanism demonstrates impressive empirical performance across multiple benchmarks, although the complexity of the model may be a limiting factor for some applications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel attention module called Interventional Dual Attention (IDA), which aims to address limitations observed in current attention mechanisms, particularly concerning contextual bias. The authors propose a causal intervention strategy to improve the model's performance. However, the paper lacks a robust justification for the inadequacies of existing methods and does not convincingly demonstrate the unique advantages of IDA over previously established models. The findings suggest some performance improvements, but the significance and implications of these results remain inadequately explored.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to innovate within the attention mechanism framework with the introduction of IDA. However, the weaknesses are pronounced; the critique of existing methods is superficial and lacks depth. The methodology is convoluted and not clearly articulated, which may hinder reproducibility. Although the authors present ablation studies, these do not provide a comprehensive understanding of the components' contributions to IDA's performance. The paper also overstates its claims of being \"model-agnostic,\" and the discussion of contextual bias does not engage critically with the complexities of the issue.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the methodology section is notably lacking, which raises concerns about the paper's overall quality and reproducibility. The novelty of IDA is somewhat overshadowed by the authors' failure to robustly justify its necessity, and the empirical evidence presented does not convincingly establish its significance. While the paper attempts to position itself as a contribution to the field, the lack of engagement with existing literature and the limited scope of experimental validation undermine its impact.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to addressing contextual bias in attention mechanisms through IDA. However, the lack of depth in critique, clarity in methodology, and sufficient empirical validation significantly detracts from its contributions to the field.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces the Interventional Dual Attention (IDA) module, a novel approach to visual recognition that addresses the challenges of contextual bias in attention mechanisms. The IDA module employs a model-agnostic design, enabling integration across various backbone architectures, and demonstrates state-of-the-art performance on renowned datasets such as MS-COCO and PASCAL-VOC. Key methodologies include dual attention layers and multiple sampling interventions, enhancing the model's robustness and interpretability while reducing computational demands. Extensive experiments validate the effectiveness of IDA, showcasing its potential for diverse applications in computer vision.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative IDA module, which effectively addresses contextual bias, resulting in improved classification and detection performance. The model's versatility is commendable, allowing it to be applied across different architectures. The comprehensive experimental validation and ablation studies provide strong evidence of its superiority over traditional methods. However, the paper could benefit from further exploration of potential limitations or scenarios where IDA might underperform compared to existing models. Additionally, while the efficiency of IDA is highlighted, specific comparisons in various real-world scenarios could further strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The descriptions of the dual attention layers and multiple sampling interventions are sufficiently detailed, allowing for a good understanding of the innovations presented. The novelty of the IDA module is significant, as it introduces a new perspective on attention mechanisms in visual recognition. However, reproducibility could be enhanced with more detailed descriptions of the experimental setup, including hyperparameters and training protocols, to facilitate independent verification of results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in visual recognition through the introduction of the IDA module. Its innovative approach and strong empirical validation position it as a significant contribution to the field, with promising implications for future research and real-world applications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the concept of **contextual bias** in visual recognition, arguing that traditional attention mechanisms may inadvertently exacerbate this bias by obscuring the relationships between objects and their contexts. Utilizing a **Causal Theory Framework**, the authors emphasize the significance of understanding causal relationships to address the confounding effects of contextual bias, which may lead to spurious correlations in predictions. The proposed **Interventional Dual Attention (IDA)** framework serves as a theoretical intervention to refine attention mechanisms, employing a dual-layer attention structure designed to strengthen correct causal pathways while diminishing reliance on confounding contextual factors. The paper posits that this model-agnostic approach has broad applicability across various architectures and highlights the necessity for robust causal models in enhancing visual recognition systems.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative framing of attention mechanisms through a causal lens, providing a theoretical foundation that addresses a critical issue in visual recognition. The introduction of the IDA framework is a notable contribution, offering a potential solution to mitigate contextual bias in machine learning models. However, the paper's empirical results are somewhat limited, with the theoretical implications taking precedence over extensive experimental validation. This could raise questions about the practical applicability of the proposed methods and their effectiveness across diverse datasets and scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its arguments clearly, presenting complex concepts in a comprehensible manner. The quality of writing is high, and the theoretical insights are well-grounded in existing literature on causality. The novelty of the approach is significant, as it bridges the gap between causal inference and attention mechanisms in visual recognition. Nonetheless, without comprehensive empirical validation, the reproducibility of the findings may be uncertain, necessitating further investigations to confirm the effectiveness of the IDA framework in practice.\n\n# Summary Of The Review\nThis paper presents a compelling theoretical framework that addresses contextual bias in visual recognition through an innovative causal lens, contributing significantly to the discourse on causality in machine learning. While the IDA framework is a promising development, the limited empirical results may challenge its immediate practical application. Future research should focus on validating these theoretical insights through robust empirical studies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach called Interventional Dual Attention (IDA) aimed at addressing contextual bias in visual recognition tasks. The authors propose a model-agnostic framework that employs two attention layers along with multiple sampling interventions to enhance performance. Key findings indicate that IDA significantly outperforms existing methods, including the baseline Spatial Class-Aware Attention (SCA), in multi-label classification and object detection tasks across various datasets (MS-COCO and PASCAL-VOC). The paper also includes comprehensive ablation studies that highlight the effectiveness of the proposed components and strategies.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to mitigating contextual bias through causal interventions, which is a relevant issue in visual recognition. The methodology is well-structured, with extensive experiments validating the proposed model's effectiveness. However, a notable weakness is the reliance on specific sampling strategies that may not generalize well across all scenarios. Additionally, while the paper discusses potential future work, it lacks a detailed exploration of the limitations of the current approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-organized sections and clear explanations of the methodology and results. The quality of the experimental setup is high, leveraging established datasets and metrics, which enhances reproducibility. The novelty of the IDA framework is significant, as it introduces a fresh perspective on combining attention mechanisms with causal interventions to reduce contextual bias.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel solution to contextual bias in visual recognition through the Interventional Dual Attention framework. It is well-researched and demonstrates substantial improvements over existing methods, though it could benefit from a more thorough discussion of its limitations.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents the Interventional Dual Attention (IDA) method, claiming to enhance classification and detection tasks by addressing contextual bias through a novel attention mechanism. The authors assert that IDA provides state-of-the-art performance in multi-label classification on datasets like MS-COCO and PASCAL-VOC, while being model-agnostic. However, the paper primarily contrasts IDA against existing methods without thoroughly detailing its unique contributions or conducting extensive comparative analyses.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to tackle contextual bias in attention mechanisms, a relevant issue in the current research landscape. However, the paper falls short in several areas. The claims of novelty are undermined by a lack of convincing distinction from existing methods, such as CSRA and ADDGCN, which achieve similar results using more complex architectures. The discussion on causal interventions appears derivative, lacking innovative insights. Additionally, the evaluation of IDA's performance lacks depth, with vague ablation studies and superficial comparisons to other benchmarks, leaving questions about the robustness of its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hindered by inconsistencies in its narrative, particularly between the strong claims of superiority and the acknowledgment of future work needed to refine IDA. While the methodology is presented, the actual implementation details and reproducibility aspects are not sufficiently detailed, making it challenging for other researchers to replicate the results. The novelty of the approach is questionable given the existing literature on similar topics, and the authors need to provide clearer distinctions between IDA and prior work in the field.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to mitigating contextual bias through the IDA method, but it lacks sufficient evidence to substantiate its claims of novelty and effectiveness. The comparisons with existing methods are superficial, and the evaluation metrics need to be more rigorous to validate the authors' assertions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper proposes an Interventional Dual Attention (IDA) framework aimed at enhancing visual recognition in the presence of contextual bias. The authors introduce two versions of the IDA—light and heavyweight—both of which utilize self-attention mechanisms to mitigate confounding influences in visual data. Through extensive experiments on benchmark datasets, the paper demonstrates that IDA significantly improves performance metrics, notably mean Intersection over Union (mIOU), compared to existing methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to addressing contextual bias through causal interventions and the introduction of a dual attention framework. The empirical results are compelling, showcasing substantial improvements over baseline models, which suggests the effectiveness of the proposed methods. However, the paper lacks clarity in certain areas, such as the definitions of key terms like \"confounders\" and the distinction between the light and heavyweight versions of IDA. Additionally, some mathematical equations and figure references require more consistent formatting.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper exhibits a moderate level of clarity, with several sections requiring rephrasing for better comprehension. The quality of the experiments is commendable, but the reproducibility could be hindered by the insufficient detail in the methodology and variable definitions. The novelty of the proposed framework is noteworthy, yet the contributions could be highlighted more clearly throughout the paper.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of visual recognition by introducing an IDA framework that effectively incorporates causal interventions to counter contextual bias. While the empirical results are promising, enhancements in clarity and consistency in presentation are necessary for better understanding and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel Interventional Dual Attention (IDA) mechanism aimed at addressing contextual bias in visual recognition tasks. The proposed methodology leverages causal inference principles to improve the classification and detection performance of existing models. The findings demonstrate that IDA enhances model robustness against spurious correlations introduced by context, particularly in multi-label classification and object detection tasks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to contextual bias, which could contribute significantly to the field of visual recognition. The IDA mechanism shows promise in improving model performance. However, the paper's limitations include a narrow focus on contextual bias without exploring other biases, such as cultural or demographic influences, which could enhance its robustness. Additionally, the lack of discussion on the adaptability of IDA for real-time applications and its scalability to larger datasets limits its practicality. The absence of a comparative analysis with existing attention mechanisms and a thorough exploration of computational trade-offs further diminishes the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed methodology and findings. However, the novelty is somewhat constrained by the limited exploration of IDA's application across various tasks and its interaction with other debiasing techniques. The reproducibility of results may also be questioned due to the lack of extensive benchmarks against a broader range of state-of-the-art methods.\n\n# Summary Of The Review\nOverall, the paper introduces a promising mechanism to address contextual bias in visual recognition. While the IDA method shows potential for improving model performance, it lacks a comprehensive exploration of its broader applicability and practical implications, which could limit its overall significance in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework called Interventional Dual Attention (IDA) aimed at mitigating contextual bias in visual recognition tasks. The methodology employs a causal framework, utilizing Structural Causal Models (SCM) to analyze the relationships among context, image content, and predictions. The authors report significant improvements in performance metrics such as mean Intersection over Union (mIOU) and mean Average Precision (mAP) across multiple datasets, including MS-COCO and PASCAL-VOC. The results indicate that IDA outperforms traditional attention mechanisms and existing state-of-the-art methods, demonstrating its robustness and efficiency in counteracting contextual biases through a model-agnostic approach.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of a well-founded causal framework that provides a solid theoretical basis for addressing contextual bias in visual recognition. The use of quantitative evaluation metrics and extensive experiments on well-known datasets lends credibility to the findings. Additionally, the ablation studies effectively underscore the contributions of individual components within the IDA framework. However, weaknesses include a lack of explicit details regarding the statistical tests applied, which could enhance the transparency of significance claims. Furthermore, while the performance improvements are notable, additional insights into computational efficiency and parameter tuning may be beneficial for practitioners.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear in its presentation of the IDA methodology and results. The quality of the empirical evaluations appears robust, with careful consideration of statistical significance. The novelty of the approach, particularly its grounding in causal inference, is commendable and adds a fresh perspective to the field of visual recognition. However, reproducibility could be improved by providing more detailed descriptions of the statistical methods and experimental setups used in the evaluations.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to the field of visual recognition by introducing a causal approach to mitigate contextual biases. While the methodology and results are compelling, there are areas for improvement in terms of clarity regarding statistical procedures and practical implementation details.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces the Interventional Dual Attention (IDA) module aimed at addressing contextual bias in visual recognition tasks. The authors propose a causal intervention framework that leverages attention mechanisms to improve classification and detection performance. Key findings indicate that the IDA module enhances performance on multi-label classification tasks within the MS-COCO and PASCAL-VOC datasets. However, the methodology primarily focuses on specific scenarios without a comprehensive exploration of its generalizability across diverse contexts.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to mitigating contextual bias through the IDA module, utilizing a causal framework that adds a layer of interpretability to attention mechanisms. However, the paper exhibits significant weaknesses, including a limited exploration of the IDA's applicability across various visual recognition tasks beyond the tested datasets. Moreover, the reliance on certain assumptions about confounding variables raises concerns regarding the robustness of the model in real-world applications. The lack of analysis on hyperparameter impacts and the model's performance in differing training/testing distributions further limits the paper’s contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting the methodology and findings in a clear manner. However, the novelty is somewhat constrained due to a narrow focus on specific tasks and datasets. The reproducibility of results is questionable, as the experiments do not thoroughly assess the model's performance across diverse contexts and with varying hyperparameters, potentially limiting the practical application of the proposed method.\n\n# Summary Of The Review\nOverall, while the paper presents a promising approach to addressing contextual bias in visual recognition tasks through the IDA module, its contributions are undermined by limited generalizability and insufficient exploration of essential factors affecting model performance. The reliance on specific datasets and assumptions about confounding variables further restricts the applicability of the proposed method in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper proposes a novel approach to mitigate contextual bias in visual recognition tasks through a method called Interventional Dual Attention (IDA). The authors argue that traditional attention mechanisms not only highlight target objects but also incorporate irrelevant contextual information, leading to biased predictions. The methodology consists of a Spatial Class-Aware Attention (SCA) mechanism combined with causal interventions to refine attention focus. Experimental results claim state-of-the-art performance on benchmarks like MS-COCO and PASCAL-VOC, although such claims need to be interpreted with caution.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to integrate causality into attention mechanisms, which could theoretically improve model performance by addressing contextual bias—a well-known issue. However, the weaknesses are notable: the proposed solutions do not significantly diverge from existing methods, and the complexity introduced may not justify the marginal gains in performance. The reliance on familiar metrics (mAP, mIOU) and standard ablation studies undercuts the perceived novelty of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is adequately structured and presents its methodology in a clear manner, although the novelty of the contributions is overstated. While the authors provide sufficient details to facilitate reproducibility, the core concepts presented do not offer substantial advancements over prior work in the field. The paper's claims of novelty and significance appear more like a reiteration of established ideas rather than groundbreaking insights.\n\n# Summary Of The Review\nOverall, the paper provides a rehashing of known concepts in the field of attention mechanisms, presenting them under the guise of novelty with an added layer of complexity. While the integration of causality is noted, the contributions seem limited and do not significantly advance the state of the art in visual recognition. \n\n# Correctness\n4/5 - The methodology appears to be sound, and the results are replicable based on the metrics used, but the claims of novelty could be more substantiated.\n\n# Technical Novelty And Significance\n2/5 - The technical aspects of the proposed method do not significantly deviate from existing attention mechanisms, limiting its novelty.\n\n# Empirical Novelty And Significance\n2/5 - While the empirical results show improvements in performance, the benchmarks used and the claims made do not demonstrate substantial empirical novelty or significance in the context of existing literature.",
    "# Summary Of The Paper\nThe paper introduces the Interventional Dual Attention (IDA) module, which aims to address the issue of contextual bias in visual recognition tasks typically exacerbated by traditional attention mechanisms. The authors employ a weighted multi-sampling operation for causal intervention, demonstrating that IDA can significantly enhance performance in multi-label classification and detection tasks on standard benchmarks like MS-COCO and PASCAL-VOC. The methodology is presented as model-agnostic, allowing for potential integration into various neural network architectures, with findings suggesting improvements in attention maps and offering insights into visual interpretability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to mitigating confounding effects through causal interventions, which aligns with ongoing research trends aimed at improving the robustness of attention mechanisms. The model-agnostic characteristic of IDA is particularly appealing, suggesting versatility across different architectures. However, a potential weakness is the limited exploration of IDA's adaptability to less common datasets or low-resource environments, which raises questions about the generalizability of the findings. While ablation studies are included, further validation on diverse datasets could strengthen the claims regarding IDA's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and findings, contributing to its overall clarity and quality. The novelty of the IDA module is significant, as it addresses an important gap in current attention mechanisms. However, the reproducibility of the results may depend on the availability of implementation details and specific configurations used in the experiments. Future work should emphasize providing more comprehensive guidelines for replicating the experiments to enhance the reproducibility of the study.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to improving attention mechanisms through the Interventional Dual Attention module, demonstrating significant performance gains in visual tasks. While the methodology is promising and the results are compelling, the adaptability of IDA to diverse datasets and the need for further validation are areas that warrant additional exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Interventional Dual Attention (IDA) framework, which claims to achieve state-of-the-art results in multi-label classification on the MS-COCO and PASCAL-VOC datasets. The methodology involves a combination of Spatial Class-Aware Attention (SCA) and multiple sampling interventions, leading to significant improvements in performance. Key findings include a maximum mean Average Precision (mAP) of 84.8% at a resolution of 576x576, superior performance across various object detection frameworks, robust performance against contextual bias, and enhanced computational efficiency compared to existing methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its significant advancements in multi-label classification and object detection, as evidenced by comprehensive ablation studies and competitive performance metrics across multiple datasets. The method's robustness and generalizability across different visual recognition tasks are also notable. However, a potential weakness is the reliance on specific combinations of components (SCA and sampling interventions), which may limit its applicability in other contexts or tasks not explicitly addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, clearly articulating the contributions and findings with appropriate use of tables to present results. The methodology is described in sufficient detail to facilitate reproducibility, although further elaboration on the experimental setup and hyperparameter choices could enhance clarity. The novelty lies in the framework's integration of attention mechanisms and its empirical validation across diverse benchmarks, establishing a strong foundation for its claims.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in multi-label classification through the IDA framework, demonstrating superior performance and robustness across various datasets and conditions. While the contributions are substantial, the methodology's dependency on specific components may necessitate further exploration for broader applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing the efficiency of neural network architectures through a refined attention mechanism. The authors propose a \"Lightweight Attention\" model that reduces computational overhead while maintaining performance levels comparable to heavier models. The methodology involves a systematic evaluation of various attention configurations on standard benchmarks, leading to the conclusion that the Lightweight Attention model outperforms existing methods in both speed and accuracy.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to attention mechanisms, effectively addressing a crucial bottleneck in neural network efficiency. The empirical results demonstrate significant improvements, providing strong evidence for the proposed method's effectiveness. However, the paper suffers from a lack of clarity in its presentation, particularly in the methodology section, which could deter readers from fully grasping the contributions. Additionally, while the proposed model shows promise, the authors could provide more extensive comparisons with state-of-the-art methods to reinforce their claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by complex sentence structures and insufficient guidance through the methodology section. The quality of the figures and their captions could be improved to enhance understanding. In terms of novelty, the approach presents an interesting perspective on attention mechanisms, although similar concepts have been explored previously. Reproducibility is a concern, as the methodology lacks detailed explanations of the experimental setup and parameter choices, which are crucial for other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in attention mechanisms with its Lightweight Attention model, showcasing impressive empirical results. However, the lack of clarity and detailed methodological explanations limits its accessibility and reproducibility. A more structured presentation and thorough comparisons with existing methods would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.504910192776778,
    -1.6973395300473886,
    -1.7639300336165233,
    -1.693277956712604,
    -1.8685267487192039,
    -1.6073495945113578,
    -1.5854407754071975,
    -1.7784672753678363,
    -1.815538127425791,
    -1.8117644286081118,
    -1.5331917738640861,
    -1.5309171590572321,
    -1.6803047713995252,
    -1.6376147323148145,
    -1.708018154606652,
    -1.7196462781251662,
    -1.7770301627224796,
    -1.641722735131492,
    -1.7592508889293899,
    -1.5996489647650056,
    -1.9641948352668488,
    -1.794058554028699,
    -1.803439458840331,
    -1.6687573877872526,
    -1.7450447615338043,
    -1.9279916482727668,
    -1.796404402166083,
    -1.820017384396243,
    -1.6149670366426614
  ],
  "logp_cond": [
    [
      0.0,
      -2.3080734403901726,
      -2.295582617885789,
      -2.3381780634209117,
      -2.347928965376335,
      -2.3181492561894523,
      -2.36890928009059,
      -2.3412529279121563,
      -2.3031365214721857,
      -2.3305306286088054,
      -2.3333198266329784,
      -2.3797002791936386,
      -2.307297194042818,
      -2.3143422973176997,
      -2.33960418521938,
      -2.3032937533054927,
      -2.3692134953520902,
      -2.3317943213713312,
      -2.3405020629031146,
      -2.3399789595259697,
      -2.3446920248482472,
      -2.3359002283769437,
      -2.346957110198755,
      -2.356700079702795,
      -2.333485318130326,
      -2.3425496639825907,
      -2.3243009552973053,
      -2.332546594290494,
      -2.399413079718455
    ],
    [
      -1.2397075809521554,
      0.0,
      -1.1086049086590155,
      -1.1345889273951255,
      -1.1668782702637863,
      -1.2293695164640834,
      -1.3108969443908065,
      -1.1988966040977622,
      -1.1649045097379784,
      -1.2597817741904986,
      -1.172792373625218,
      -1.3620883495351388,
      -1.116653145753722,
      -1.1277979211152167,
      -1.204859888637513,
      -1.1337492566257301,
      -1.2673899616424524,
      -1.1750703877735382,
      -1.1780953912144936,
      -1.175623732847407,
      -1.1575473248523955,
      -1.2175733969846585,
      -1.2688717228361077,
      -1.2037998435805815,
      -1.2193867198238832,
      -1.2682873514684767,
      -1.152750951911131,
      -1.2051159855057345,
      -1.3622659046201209
    ],
    [
      -1.3848142277072903,
      -1.2716638458366005,
      0.0,
      -1.382172379192699,
      -1.3536300261477696,
      -1.4065118858820116,
      -1.4496818700440255,
      -1.3876497122660671,
      -1.2916001400513673,
      -1.39552310627885,
      -1.3207860530828366,
      -1.5239214857155343,
      -1.3035906266266488,
      -1.2291307607972122,
      -1.3710586594713485,
      -1.248943089642523,
      -1.4097336301930035,
      -1.3283147100076966,
      -1.353843721640885,
      -1.2463113665784986,
      -1.3541509377342706,
      -1.3726834606552807,
      -1.3834393906610838,
      -1.3892041395559818,
      -1.3284164702899657,
      -1.3879405175588058,
      -1.3110496819293078,
      -1.2990562554217087,
      -1.5164981215424245
    ],
    [
      -1.352124469058301,
      -1.1289019906713915,
      -1.2473706533566415,
      0.0,
      -1.1859376760635096,
      -1.1657124439949844,
      -1.3340389516969753,
      -1.2021176934548106,
      -1.2009984146744477,
      -1.2544037138330375,
      -1.2086270316413634,
      -1.3585962933775695,
      -1.160682614659532,
      -1.1855716535927714,
      -1.2394702331944738,
      -1.2313327046211866,
      -1.2969797145276538,
      -1.253550147344758,
      -1.210322326429336,
      -1.2499922264885426,
      -1.1955356197024243,
      -1.2767761722983613,
      -1.2553426948381254,
      -1.2805391241446793,
      -1.2086302209547475,
      -1.2880023941139427,
      -1.1840651567089884,
      -1.2567516836948882,
      -1.3452235862317898
    ],
    [
      -1.5055577933581437,
      -1.4374628585161544,
      -1.4499435108295764,
      -1.4458226476710134,
      0.0,
      -1.5164154066665503,
      -1.5581243520000214,
      -1.4165211449129054,
      -1.4538924350010687,
      -1.4840761234717559,
      -1.4302493172624466,
      -1.6062196673469176,
      -1.4160718046002518,
      -1.4271251827967393,
      -1.4418045530610988,
      -1.4361787547708347,
      -1.520854972820025,
      -1.4517615406276474,
      -1.4460491264649193,
      -1.4792734525251408,
      -1.399356158061105,
      -1.444460782570529,
      -1.4764037290016097,
      -1.4149640700309252,
      -1.4142602956658883,
      -1.5088384378061723,
      -1.4000815389115482,
      -1.5024344717941016,
      -1.5752342339857617
    ],
    [
      -1.2946178719290389,
      -1.1633056850481809,
      -1.1838469508002651,
      -1.1173399290886057,
      -1.2175714972635037,
      0.0,
      -1.2001697607254591,
      -1.1637427299948793,
      -1.1844584343915718,
      -1.2583142008729609,
      -1.1676646604678078,
      -1.2918259419889202,
      -1.1944708275536382,
      -1.1818561900517868,
      -1.1935782845042056,
      -1.1672396793956077,
      -1.264059705118136,
      -1.196393375700215,
      -1.1990088003397432,
      -1.2151665419369684,
      -1.2149098112069603,
      -1.2240496839804902,
      -1.2545463216561052,
      -1.2084324487361489,
      -1.2167849952610108,
      -1.2366100731990501,
      -1.2401868809455325,
      -1.2183135801410467,
      -1.2787155913404615
    ],
    [
      -1.2899978721055185,
      -1.1941817887334616,
      -1.2067689047280705,
      -1.2446028736979196,
      -1.2397734364725268,
      -1.1871772927521422,
      0.0,
      -1.2301272888612134,
      -1.2482811187082674,
      -1.243229550243213,
      -1.253572484611161,
      -1.3168619220139066,
      -1.2120729709095202,
      -1.2630070086629077,
      -1.2454217017811007,
      -1.2445495275384844,
      -1.2724812375369916,
      -1.2584866247785758,
      -1.2592883137011477,
      -1.2290559781970387,
      -1.2395367116925553,
      -1.2827815827120064,
      -1.275106648983321,
      -1.23504728792132,
      -1.232614884520907,
      -1.2256276453821844,
      -1.2379674049170406,
      -1.2325996383150777,
      -1.329000620040874
    ],
    [
      -1.4455515036762603,
      -1.3472568622797336,
      -1.3477992493496314,
      -1.282687996365396,
      -1.3365816103554784,
      -1.3396688929900804,
      -1.4272465724546386,
      0.0,
      -1.3332106001569326,
      -1.3603287244324198,
      -1.3690652103255154,
      -1.5214434552567022,
      -1.339921465862257,
      -1.3760732925253887,
      -1.356728845075508,
      -1.3591759853301522,
      -1.3608138055374834,
      -1.3610314219855095,
      -1.3771725008150906,
      -1.31464306202832,
      -1.3297493407684118,
      -1.3957036114039352,
      -1.3827863646483232,
      -1.3111415820084475,
      -1.3509504257246878,
      -1.3217378964514155,
      -1.3430786170495668,
      -1.3860530808095546,
      -1.511307323243094
    ],
    [
      -1.4295644336476854,
      -1.3527179061095473,
      -1.2233024924938025,
      -1.3988047330922853,
      -1.4076955721347708,
      -1.4480112805229979,
      -1.5157989543149981,
      -1.3735666946186746,
      0.0,
      -1.434232916921021,
      -1.4004439398694877,
      -1.4853714503900526,
      -1.3077615471373067,
      -1.2729509637432952,
      -1.383677087837105,
      -1.2925661860673707,
      -1.4488335743388385,
      -1.372275616937933,
      -1.388801013193446,
      -1.2639204481953377,
      -1.4141309313729524,
      -1.4180471764937363,
      -1.4023968694714064,
      -1.378750472445666,
      -1.4055011545820786,
      -1.408266416076544,
      -1.3685692932863829,
      -1.3251290304627052,
      -1.4927468327600988
    ],
    [
      -1.4624893396578094,
      -1.371032621942207,
      -1.4157523987831921,
      -1.4297804696752037,
      -1.3795087899635299,
      -1.4900291893171422,
      -1.478733779129587,
      -1.3365807026655803,
      -1.4344888503370843,
      0.0,
      -1.417152710184713,
      -1.5156074544320939,
      -1.347726301953167,
      -1.375876618776553,
      -1.4195479197752663,
      -1.4084517018791887,
      -1.359272177817119,
      -1.4127995253865415,
      -1.4159838026632552,
      -1.3874070597316193,
      -1.3484042048517395,
      -1.4725954854022283,
      -1.3624555873840674,
      -1.409496386331768,
      -1.3414049378313984,
      -1.4052753000715417,
      -1.3309035888965255,
      -1.4678295012449714,
      -1.5552926606796422
    ],
    [
      -1.1778746842390575,
      -1.0539356301762446,
      -1.0623112130985206,
      -1.0843347979411997,
      -1.091256361519539,
      -1.1292716317053118,
      -1.2059743161548064,
      -1.0906089766882827,
      -1.0840324977441909,
      -1.1213248431909575,
      0.0,
      -1.2320695617986404,
      -1.0624187748089085,
      -1.0719014194439223,
      -1.0683275073681555,
      -1.0171712246632358,
      -1.1426739375739925,
      -1.0828874481071349,
      -1.1341510689119698,
      -1.145271279584539,
      -1.0549805074391567,
      -1.1376898401185729,
      -1.1152077357982806,
      -1.0885326748946236,
      -1.068911080861016,
      -1.1307663135418098,
      -1.0487882434563427,
      -1.0920009990836708,
      -1.208273996157899
    ],
    [
      -1.2798300072847264,
      -1.2260320735773265,
      -1.2636520192878364,
      -1.2537806924788253,
      -1.2256373299060233,
      -1.2561437996041414,
      -1.2944182259006405,
      -1.264354437228687,
      -1.217700819999142,
      -1.2208343920789917,
      -1.2229140168253438,
      0.0,
      -1.228808634479073,
      -1.2504209664992827,
      -1.2446430901752261,
      -1.257357471051397,
      -1.2465383852546699,
      -1.2525737763347458,
      -1.2602320418003108,
      -1.2601457092747406,
      -1.2160955568896075,
      -1.267852253458979,
      -1.2318654957525492,
      -1.2757607000045075,
      -1.2241096452249562,
      -1.2929553336684028,
      -1.2384913396039565,
      -1.2743778044100667,
      -1.2215091569858532
    ],
    [
      -1.3022111050872238,
      -1.2010318817293753,
      -1.2111365516501633,
      -1.259446647295101,
      -1.24337449848817,
      -1.3327471305178622,
      -1.3582679375599844,
      -1.2646321408038628,
      -1.1976618759840285,
      -1.3152560589402587,
      -1.2801537867831863,
      -1.4353784152957492,
      0.0,
      -1.1424121911632708,
      -1.281594281938045,
      -1.2185142907690107,
      -1.3288288810377997,
      -1.2198422976685626,
      -1.2496922460324271,
      -1.2384706519155015,
      -1.2251743007304141,
      -1.3464537185262475,
      -1.30712362329561,
      -1.328627638535898,
      -1.3001903883812251,
      -1.3363719207128175,
      -1.2289161356427367,
      -1.2785205846456054,
      -1.3755312444646013
    ],
    [
      -1.2611906627044316,
      -1.0642927359439616,
      -1.075845324850213,
      -1.1832602519719952,
      -1.1992096334765532,
      -1.2289418002109838,
      -1.3543674743518772,
      -1.2215407504274962,
      -1.1186000099542952,
      -1.2494919495343546,
      -1.159454527174222,
      -1.3676320674219165,
      -1.0898825286041272,
      0.0,
      -1.1841898349643911,
      -1.1303815923753078,
      -1.2836056866068364,
      -1.1605814476535243,
      -1.243661230245519,
      -1.1571460354156824,
      -1.1792605130845546,
      -1.2886855825175783,
      -1.2287818925161669,
      -1.237356360512238,
      -1.2198386375155568,
      -1.1948064181662406,
      -1.1617190144275553,
      -1.2004992199195945,
      -1.3342488441269953
    ],
    [
      -1.30986991903572,
      -1.184452739846423,
      -1.2005748533214748,
      -1.25110386383694,
      -1.2325998979817707,
      -1.257184885366668,
      -1.3472556589584928,
      -1.2426804837629046,
      -1.253158913958484,
      -1.2716937589337736,
      -1.2144738178608754,
      -1.3626157983688891,
      -1.2087649685610895,
      -1.2118666152356605,
      0.0,
      -1.2308897312125109,
      -1.289232763690177,
      -1.2165114711616087,
      -1.2430032973478333,
      -1.2236876367328522,
      -1.2083345313638534,
      -1.2729713136952074,
      -1.285121594284623,
      -1.2448622410448467,
      -1.199817298846192,
      -1.2725632239112972,
      -1.187180914807941,
      -1.2705162351870132,
      -1.344290751910348
    ],
    [
      -1.3027652678450696,
      -1.2150872067470617,
      -1.182270594172355,
      -1.2820720434044992,
      -1.2860628082577203,
      -1.3065144014687506,
      -1.4266544963399537,
      -1.2626886593050386,
      -1.160956773467414,
      -1.3326090291709582,
      -1.2537446648535098,
      -1.4270546513278006,
      -1.2184727951339958,
      -1.2067491599494342,
      -1.2733299098399888,
      0.0,
      -1.3489881661600367,
      -1.261751833684846,
      -1.322057439683528,
      -1.2261455191998027,
      -1.2604927696284993,
      -1.296534243829053,
      -1.306244176125731,
      -1.2404365925008556,
      -1.3065576183619763,
      -1.2527479314955248,
      -1.2565119378024574,
      -1.1479738897446123,
      -1.4223701949454297
    ],
    [
      -1.4746622673252903,
      -1.3941363842325256,
      -1.3993822690138251,
      -1.378577331111345,
      -1.3986073628948301,
      -1.466587500394855,
      -1.4706114442657676,
      -1.3095139946847965,
      -1.4030660656545995,
      -1.36385381163338,
      -1.42338952221415,
      -1.5262094729177949,
      -1.4011860678797792,
      -1.4126612985140723,
      -1.4202753536631152,
      -1.4042254591392427,
      0.0,
      -1.3842867970025747,
      -1.4393497389842989,
      -1.3577894663412546,
      -1.3528395285006747,
      -1.4138029377691557,
      -1.427056523951704,
      -1.369608452821777,
      -1.4015714886639563,
      -1.4044367985154684,
      -1.337891412469976,
      -1.4141498718013044,
      -1.4904968229740505
    ],
    [
      -1.316376470102843,
      -1.122493535820241,
      -1.1537858760266706,
      -1.230922832158271,
      -1.2082880524588493,
      -1.2514299404808236,
      -1.335471727640757,
      -1.24116636643182,
      -1.2198785150146763,
      -1.274228241121648,
      -1.2008232231091354,
      -1.3598801859439917,
      -1.1477739615002833,
      -1.1708887976300912,
      -1.2405699551454161,
      -1.1816118753871567,
      -1.2521256892022326,
      0.0,
      -1.2578269125155126,
      -1.2322323388028598,
      -1.2018551002303057,
      -1.3010725610531695,
      -1.280160566036866,
      -1.2745663167826846,
      -1.2087138026535453,
      -1.3069547464958935,
      -1.1836815273258292,
      -1.212883389657156,
      -1.3276334338345526
    ],
    [
      -1.4557079991452155,
      -1.391814105848299,
      -1.3815448052749042,
      -1.4021689930922452,
      -1.4426517587532786,
      -1.437075566688173,
      -1.484404180878996,
      -1.3961018067230033,
      -1.4168721262596504,
      -1.4389483985527867,
      -1.425217514769085,
      -1.4873159435473207,
      -1.3610389890331247,
      -1.445465051778144,
      -1.4513063408489457,
      -1.4391372459550629,
      -1.4668263573610856,
      -1.4354669975324843,
      0.0,
      -1.439074085526385,
      -1.4399244993648266,
      -1.4176268927574707,
      -1.4211756717146886,
      -1.4253658973348695,
      -1.4149165518641957,
      -1.4554254182063762,
      -1.418268257289051,
      -1.4507800162091655,
      -1.5011650098707452
    ],
    [
      -1.2166803321019324,
      -1.1070375368023333,
      -0.9698258631710543,
      -1.1309521295628242,
      -1.1292102983366006,
      -1.1975320260401487,
      -1.2095981664793956,
      -1.0888813341233603,
      -1.0444823357999284,
      -1.1186473639620904,
      -1.1624633108719555,
      -1.302430475172516,
      -1.0283628089254548,
      -1.039920853589833,
      -1.0775609064189768,
      -1.0421207001620236,
      -1.1254507523404556,
      -1.128300761331985,
      -1.1630256482804406,
      0.0,
      -1.1219973633093985,
      -1.1792164618159267,
      -1.1584223360953312,
      -1.0921112833900808,
      -1.1128096395018412,
      -1.0798981180337872,
      -1.1141817251418502,
      -1.0898547484322967,
      -1.2752909519140267
    ],
    [
      -1.6212373715585695,
      -1.4892245801039388,
      -1.5233751674488762,
      -1.514651741459511,
      -1.4884438307518084,
      -1.5728857235811715,
      -1.6368590309983992,
      -1.4940477178158917,
      -1.564060816563583,
      -1.5431884731990082,
      -1.521468904452841,
      -1.6591859386180918,
      -1.462146422311357,
      -1.511655646104951,
      -1.5119851924659298,
      -1.5278117390014858,
      -1.5550871824959218,
      -1.5280568252616764,
      -1.577834315788657,
      -1.5729167524632677,
      0.0,
      -1.5763298775355477,
      -1.5476109419783106,
      -1.5632839967307035,
      -1.5164555623631581,
      -1.5447086225591082,
      -1.4972804607227432,
      -1.5563840306852803,
      -1.6295890728605402
    ],
    [
      -1.4098451620353916,
      -1.3193058319443192,
      -1.3076085057927433,
      -1.3147080828614692,
      -1.3015883105297206,
      -1.3400386011757373,
      -1.432407204978207,
      -1.345322316240243,
      -1.3361662973866164,
      -1.3497639184250392,
      -1.3458912808652348,
      -1.4741754957088866,
      -1.3291514115341467,
      -1.3254464405508124,
      -1.329161245601827,
      -1.3012831659848862,
      -1.3748393917582034,
      -1.3500279342689838,
      -1.3296096964778958,
      -1.3335896509879501,
      -1.3348869639216807,
      0.0,
      -1.33265405347927,
      -1.3336224797116232,
      -1.3172061649907214,
      -1.3758579223136327,
      -1.2985480543625163,
      -1.4109225389114048,
      -1.4132935077126505
    ],
    [
      -1.4623995309541122,
      -1.3413981134666702,
      -1.316863346591349,
      -1.4025936024582324,
      -1.3210615046601621,
      -1.3772908973299662,
      -1.4678832921286995,
      -1.3348920135335929,
      -1.3834430615628022,
      -1.3147414453277626,
      -1.3408481882467138,
      -1.4907053720289396,
      -1.3129023987824597,
      -1.344923068482822,
      -1.3330677002274078,
      -1.3522631334429818,
      -1.3919941358367636,
      -1.4107980758046925,
      -1.375296531039919,
      -1.3640183967494073,
      -1.3484221951971764,
      -1.3872540115541574,
      0.0,
      -1.365262495519978,
      -1.2445140114185684,
      -1.414638644586945,
      -1.3431030965270403,
      -1.4189238386871574,
      -1.4719369966034925
    ],
    [
      -1.3397087864588257,
      -1.2871727980582908,
      -1.2588643223816363,
      -1.327748379488829,
      -1.2674717528109658,
      -1.35685063497159,
      -1.3418105525709365,
      -1.2118180578265796,
      -1.3388175841904717,
      -1.3029255298379934,
      -1.2963557195916655,
      -1.4101440588762864,
      -1.318503159507672,
      -1.314482038114743,
      -1.3062005454548868,
      -1.2211788108909343,
      -1.2945254630023408,
      -1.3303752527225916,
      -1.3107274707037226,
      -1.246509922479431,
      -1.2760116446554257,
      -1.295146274703274,
      -1.290478481982083,
      0.0,
      -1.288411634698494,
      -1.2764301645518215,
      -1.2992831503110844,
      -1.350917158525215,
      -1.4171261841001008
    ],
    [
      -1.3730636786931203,
      -1.2611538209698168,
      -1.2083109168195292,
      -1.2891834989235986,
      -1.2724384970378055,
      -1.3423553484233894,
      -1.436449015930579,
      -1.2908141700187168,
      -1.3537988747886143,
      -1.236762679085227,
      -1.238897409913673,
      -1.4565119925397654,
      -1.2669187321264088,
      -1.2879426224577153,
      -1.2711019476837886,
      -1.2764974107593086,
      -1.37558758984358,
      -1.308787946374453,
      -1.3195554200128552,
      -1.2807976239167764,
      -1.3062158210635038,
      -1.3207617028268812,
      -1.2512550108440093,
      -1.3456349742529885,
      0.0,
      -1.3545724574409101,
      -1.2214995061944538,
      -1.3253289404381197,
      -1.4613779456386884
    ],
    [
      -1.6223691849273627,
      -1.568614544558062,
      -1.515397056457432,
      -1.5719397030855007,
      -1.6006181464914324,
      -1.6156643698700963,
      -1.5933802629983753,
      -1.5685938912827104,
      -1.5589586146398797,
      -1.5777419180893382,
      -1.5493944334150767,
      -1.6969643478872247,
      -1.5882409721666424,
      -1.5230008316021573,
      -1.5740820317143687,
      -1.5098174383458358,
      -1.5772221339315065,
      -1.5899401565690463,
      -1.5855742421249295,
      -1.4970611258283777,
      -1.5302432269059734,
      -1.609435650166001,
      -1.5887629784397277,
      -1.544569733735326,
      -1.567635090939867,
      0.0,
      -1.5544558071492705,
      -1.5233483276694697,
      -1.689927452989744
    ],
    [
      -1.4151122470892359,
      -1.2946119950946364,
      -1.3398376294897973,
      -1.3475215775682217,
      -1.2977296423667768,
      -1.4313765043164617,
      -1.481410172714683,
      -1.3488288315572579,
      -1.3452613979594763,
      -1.3419642025086458,
      -1.3472362157261322,
      -1.4970869609307662,
      -1.3404341207828558,
      -1.3501190315605116,
      -1.359947906064699,
      -1.3292140392188005,
      -1.368584377174892,
      -1.3273917155627954,
      -1.357644520030419,
      -1.352368559094547,
      -1.3051749214209543,
      -1.3934118112419984,
      -1.4154219203646297,
      -1.390379703067647,
      -1.3238213568032111,
      -1.4064842235970823,
      0.0,
      -1.3770890215217693,
      -1.4736876441606588
    ],
    [
      -1.4887918814486412,
      -1.3280497983802424,
      -1.275181948995053,
      -1.3695915238789,
      -1.3756813262455851,
      -1.4169368088711236,
      -1.4669202905453,
      -1.404210730547002,
      -1.2876719265342753,
      -1.427153367437513,
      -1.3908250672646654,
      -1.4883069026891025,
      -1.3530339892331666,
      -1.3047896056683361,
      -1.3894318739914862,
      -1.3022834516148407,
      -1.425763488464575,
      -1.3490207997205805,
      -1.4000929232840256,
      -1.3368589726780205,
      -1.3674261440250055,
      -1.4232503392438518,
      -1.4221233210430084,
      -1.407426443484179,
      -1.4134042004219087,
      -1.3712872973802506,
      -1.3858974647808542,
      0.0,
      -1.4820694843853737
    ],
    [
      -1.28331132789025,
      -1.2162681139925062,
      -1.242240365533259,
      -1.2292966006822312,
      -1.2111209746443434,
      -1.2388421888254413,
      -1.2915913343084628,
      -1.2390522196901317,
      -1.2048060964209437,
      -1.2417921514256622,
      -1.2283214619921523,
      -1.254925398527594,
      -1.2147547417620739,
      -1.2502121946949372,
      -1.2144375466744588,
      -1.238011316178578,
      -1.2590475803118635,
      -1.234176944474176,
      -1.2479194488486163,
      -1.2545277787831914,
      -1.171997381047386,
      -1.2100243590338418,
      -1.2277408197529034,
      -1.2405302750086469,
      -1.2014292820742662,
      -1.2622987075312693,
      -1.223096194005961,
      -1.2335728530324064,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.19683675238660525,
      0.20932757489098863,
      0.16673212935586612,
      0.15698122740044296,
      0.1867609365873255,
      0.13600091268618764,
      0.1636572648646215,
      0.20177367130459212,
      0.1743795641679724,
      0.1715903661437994,
      0.12520991358313927,
      0.19761299873395988,
      0.19056789545907815,
      0.165306007557398,
      0.20161643947128516,
      0.1356966974246876,
      0.1731158714054466,
      0.16440812987366327,
      0.16493123325080816,
      0.1602181679285306,
      0.16900996439983418,
      0.15795308257802265,
      0.1482101130739828,
      0.17142487464645173,
      0.16236052879418716,
      0.18060923747947255,
      0.1723635984862839,
      0.10549711305832288
    ],
    [
      0.4576319490952332,
      0.0,
      0.5887346213883731,
      0.5627506026522631,
      0.5304612597836023,
      0.46797001358330514,
      0.38644258565658207,
      0.49844292594962636,
      0.5324350203094101,
      0.43755775585688994,
      0.5245471564221706,
      0.3352511805122498,
      0.5806863842936665,
      0.5695416089321719,
      0.49247964140987555,
      0.5635902734216585,
      0.42994956840493614,
      0.5222691422738504,
      0.519244138832895,
      0.5217157971999815,
      0.539792205194993,
      0.4797661330627301,
      0.42846780721128086,
      0.4935396864668071,
      0.47795281022350533,
      0.42905217857891187,
      0.5445885781362576,
      0.49222354454165407,
      0.3350736254272677
    ],
    [
      0.379115805909233,
      0.4922661877799228,
      0.0,
      0.38175765442382437,
      0.4103000074687537,
      0.35741814773451175,
      0.31424816357249785,
      0.3762803213504562,
      0.4723298935651561,
      0.36840692733767333,
      0.44314398053368675,
      0.24000854790098902,
      0.4603394069898745,
      0.5347992728193112,
      0.3928713741451748,
      0.5149869439740002,
      0.3541964034235199,
      0.4356153236088267,
      0.4100863119756384,
      0.5176186670380247,
      0.4097790958822527,
      0.39124657296124266,
      0.38049064295543955,
      0.3747258940605416,
      0.43551356332655766,
      0.3759895160577176,
      0.4528803516872155,
      0.4648737781948147,
      0.24743191207409887
    ],
    [
      0.341153487654303,
      0.5643759660412124,
      0.4459073033559624,
      0.0,
      0.5073402806490943,
      0.5275655127176195,
      0.35923900501562867,
      0.4911602632577934,
      0.49227954203815627,
      0.43887424287956645,
      0.4846509250712405,
      0.33468166333503446,
      0.532595342053072,
      0.5077063031198326,
      0.45380772351813015,
      0.46194525209141735,
      0.39629824218495013,
      0.43972780936784606,
      0.48295563028326804,
      0.44328573022406137,
      0.49774233701017967,
      0.4165017844142427,
      0.4379352618744785,
      0.41273883256792465,
      0.48464773575785647,
      0.40527556259866127,
      0.5092128000036156,
      0.43652627301771574,
      0.3480543704808141
    ],
    [
      0.3629689553610602,
      0.43106389020304947,
      0.4185832378896275,
      0.42270410104819045,
      0.0,
      0.3521113420526536,
      0.31040239671918246,
      0.4520056038062985,
      0.4146343137181352,
      0.384450625247448,
      0.43827743145675724,
      0.2623070813722863,
      0.45245494411895204,
      0.4414015659224646,
      0.4267221956581051,
      0.4323479939483692,
      0.34767177589917897,
      0.4167652080915565,
      0.42247762225428453,
      0.3892532961940631,
      0.46917059065809896,
      0.424065966148675,
      0.3921230197175942,
      0.4535626786882787,
      0.4542664530533156,
      0.35968831091303155,
      0.4684452098076557,
      0.36609227692510227,
      0.2932925147334422
    ],
    [
      0.31273172258231896,
      0.44404390946317696,
      0.4235026437110927,
      0.49000966542275215,
      0.38977809724785417,
      0.0,
      0.4071798337858987,
      0.44360686451647857,
      0.422891160119786,
      0.349035393638397,
      0.43968493404355,
      0.31552365252243764,
      0.41287876695771963,
      0.42549340445957107,
      0.4137713100071523,
      0.44010991511575015,
      0.3432898893932219,
      0.41095621881114286,
      0.4083407941716146,
      0.39218305257438946,
      0.39243978330439755,
      0.38329991053086765,
      0.3528032728552526,
      0.398917145775209,
      0.39056459925034703,
      0.3707395213123077,
      0.3671627135658253,
      0.3890360143703111,
      0.3286340031708963
    ],
    [
      0.295442903301679,
      0.39125898667373593,
      0.378671870679127,
      0.34083790170927797,
      0.3456673389346707,
      0.3982634826550553,
      0.0,
      0.35531348654598416,
      0.3371596566989301,
      0.3422112251639846,
      0.3318682907960364,
      0.2685788533932909,
      0.37336780449767737,
      0.3224337667442898,
      0.34001907362609685,
      0.34089124786871317,
      0.3129595378702059,
      0.32695415062862176,
      0.32615246170604983,
      0.3563847972101588,
      0.3459040637146422,
      0.30265919269519115,
      0.3103341264238766,
      0.3503934874858776,
      0.35282589088629046,
      0.35981313002501314,
      0.3474733704901569,
      0.35284113709211984,
      0.2564401553663236
    ],
    [
      0.33291577169157605,
      0.4312104130881027,
      0.430668026018205,
      0.49577927900244023,
      0.44188566501235793,
      0.4387983823777559,
      0.35122070291319774,
      0.0,
      0.44525667521090373,
      0.4181385509354165,
      0.4094020650423209,
      0.2570238201111341,
      0.4385458095055794,
      0.40239398284244765,
      0.4217384302923284,
      0.4192912900376842,
      0.4176534698303529,
      0.4174358533823268,
      0.40129477455274576,
      0.4638242133395163,
      0.44871793459942455,
      0.38276366396390116,
      0.39568091071951317,
      0.4673256933593888,
      0.4275168496431485,
      0.45672937891642085,
      0.43538865831826956,
      0.39241419455828175,
      0.2671599521247423
    ],
    [
      0.38597369377810553,
      0.46282022131624356,
      0.5922356349319884,
      0.41673339433350565,
      0.4078425552910201,
      0.367526846902793,
      0.2997391731107928,
      0.44197143280711626,
      0.0,
      0.38130521050476984,
      0.41509418755630323,
      0.3301666770357383,
      0.5077765802884842,
      0.5425871636824957,
      0.43186103958868594,
      0.5229719413584202,
      0.3667045530869524,
      0.4432625104878578,
      0.42673711423234484,
      0.5516176792304532,
      0.4014071960528385,
      0.39749095093205455,
      0.4131412579543845,
      0.43678765498012484,
      0.4100369728437123,
      0.407271711349247,
      0.446968834139408,
      0.49040909696308566,
      0.3227912946656921
    ],
    [
      0.34927508895030246,
      0.4407318066659047,
      0.3960120298249197,
      0.38198395893290815,
      0.43225563864458194,
      0.3217352392909696,
      0.3330306494785249,
      0.47518372594253155,
      0.37727557827102753,
      0.0,
      0.3946117184233988,
      0.29615697417601794,
      0.4640381266549449,
      0.43588780983155884,
      0.39221650883284553,
      0.40331272672892315,
      0.45249225079099276,
      0.3989649032215703,
      0.39578062594485663,
      0.42435736887649256,
      0.4633602237563723,
      0.33916894320588353,
      0.44930884122404446,
      0.40226804227634383,
      0.47035949077671346,
      0.40648912853657015,
      0.48086083971158633,
      0.34393492736314046,
      0.2564717679284696
    ],
    [
      0.3553170896250286,
      0.47925614368784153,
      0.4708805607655655,
      0.4488569759228864,
      0.44193541234454714,
      0.40392014215877436,
      0.3272174577092797,
      0.4425827971758034,
      0.44915927611989526,
      0.41186693067312863,
      0.0,
      0.30112221206544576,
      0.47077299905517767,
      0.46129035442016386,
      0.4648642664959306,
      0.5160205492008503,
      0.3905178362900936,
      0.45030432575695123,
      0.3990407049521163,
      0.3879204942795471,
      0.4782112664249294,
      0.39550193374551323,
      0.41798403806580553,
      0.44465909896946254,
      0.46428069300307007,
      0.40242546032227633,
      0.4844035304077434,
      0.44119077478041535,
      0.3249177777061871
    ],
    [
      0.25108715177250573,
      0.30488508547990567,
      0.2672651397693957,
      0.27713646657840685,
      0.3052798291512089,
      0.2747733594530908,
      0.23649893315659165,
      0.2665627218285451,
      0.3132163390580902,
      0.31008276697824044,
      0.3080031422318883,
      0.0,
      0.3021085245781592,
      0.28049619255794944,
      0.286274068882006,
      0.2735596880058351,
      0.28437877380256227,
      0.2783433827224864,
      0.27068511725692135,
      0.27077144978249157,
      0.31482160216762467,
      0.26306490559825324,
      0.2990516633046829,
      0.25515645905272466,
      0.3068075138322759,
      0.23796182538882937,
      0.2924258194532756,
      0.2565393546471655,
      0.309408002071379
    ],
    [
      0.37809366631230135,
      0.4792728896701499,
      0.46916821974936185,
      0.42085812410442425,
      0.4369302729113551,
      0.34755764088166297,
      0.32203683383954074,
      0.4156726305956624,
      0.48264289541549665,
      0.3650487124592665,
      0.4001509846163389,
      0.244926356103776,
      0.0,
      0.5378925802362544,
      0.39871048946148013,
      0.4617904806305144,
      0.35147589036172544,
      0.4604624737309626,
      0.43061252536709804,
      0.4418341194840236,
      0.45513047066911105,
      0.33385105287327765,
      0.3731811481039151,
      0.35167713286362723,
      0.38011438301830003,
      0.34393285068670765,
      0.45138863575678845,
      0.4017841867539198,
      0.3047735269349239
    ],
    [
      0.3764240696103829,
      0.5733219963708529,
      0.5617694074646016,
      0.4543544803428192,
      0.4384050988382613,
      0.4086729321038307,
      0.28324725796293726,
      0.41607398188731826,
      0.5190147223605193,
      0.38812278278045986,
      0.47816020514059243,
      0.269982664892898,
      0.5477322037106873,
      0.0,
      0.45342489735042335,
      0.5072331399395067,
      0.3540090457079781,
      0.47703328466129014,
      0.39395350206929547,
      0.4804686968991321,
      0.4583542192302599,
      0.3489291497972362,
      0.4088328397986476,
      0.4002583718025765,
      0.4177760947992577,
      0.4428083141485739,
      0.47589571788725915,
      0.43711551239521995,
      0.3033658881878192
    ],
    [
      0.39814823557093204,
      0.5235654147602289,
      0.5074433012851771,
      0.4569142907697119,
      0.47541825662488124,
      0.4508332692399839,
      0.3607624956481592,
      0.46533767084374733,
      0.45485924064816796,
      0.4363243956728784,
      0.4935443367457766,
      0.34540235623776283,
      0.49925318604556246,
      0.49615153937099143,
      0.0,
      0.4771284233941411,
      0.41878539091647493,
      0.4915066834450432,
      0.46501485725881864,
      0.48433051787379977,
      0.4996836232427986,
      0.4350468409114445,
      0.42289656032202894,
      0.4631559135618053,
      0.50820085576046,
      0.4354549306953548,
      0.5208372397987109,
      0.4375019194196388,
      0.363727402696304
    ],
    [
      0.4168810102800966,
      0.5045590713781045,
      0.5373756839528112,
      0.43757423472066703,
      0.4335834698674459,
      0.4131318766564156,
      0.29299178178521257,
      0.45695761882012764,
      0.5586895046577522,
      0.387037248954208,
      0.46590161327165647,
      0.2925916267973656,
      0.5011734829911705,
      0.512897118175732,
      0.4463163682851774,
      0.0,
      0.37065811196512954,
      0.45789444444032035,
      0.39758883844163817,
      0.4935007589253635,
      0.4591535084966669,
      0.4231120342961132,
      0.41340210199943517,
      0.47920968562431066,
      0.41308865976319,
      0.46689834662964147,
      0.46313434032270884,
      0.5716723883805539,
      0.2972760831797365
    ],
    [
      0.30236789539718933,
      0.382893778489954,
      0.3776478937086545,
      0.39845283161113465,
      0.3784227998276495,
      0.31044266232762463,
      0.306418718456712,
      0.46751616803768314,
      0.37396409706788014,
      0.41317635108909956,
      0.35364064050832966,
      0.25082068980468475,
      0.3758440948427004,
      0.3643688642084073,
      0.3567548090593644,
      0.3728047035832369,
      0.0,
      0.39274336571990487,
      0.33768042373818075,
      0.41924069638122496,
      0.4241906342218049,
      0.36322722495332393,
      0.3499736387707757,
      0.4074217099007027,
      0.3754586740585233,
      0.37259336420701117,
      0.4391387502525037,
      0.36288029092117524,
      0.2865333397484291
    ],
    [
      0.32534626502864894,
      0.519229199311251,
      0.48793685910482143,
      0.410799902973221,
      0.43343468267264273,
      0.39029279465066846,
      0.30625100749073497,
      0.40055636869967204,
      0.42184422011681577,
      0.36749449400984413,
      0.44089951202235667,
      0.2818425491875003,
      0.4939487736312087,
      0.4708339375014008,
      0.4011527799860759,
      0.4601108597443353,
      0.3895970459292595,
      0.0,
      0.38389582261597943,
      0.40949039632863227,
      0.4398676349011863,
      0.34065017407832254,
      0.361562169094626,
      0.3671564183488074,
      0.4330089324779467,
      0.33476798863559853,
      0.4580412078056628,
      0.428839345474336,
      0.31408930129693946
    ],
    [
      0.3035428897841743,
      0.36743678308109096,
      0.37770608365448566,
      0.35708189583714467,
      0.3165991301761113,
      0.3221753222412169,
      0.2748467080503938,
      0.36314908220638653,
      0.34237876266973943,
      0.3203024903766032,
      0.33403337416030476,
      0.2719349453820692,
      0.39821189989626515,
      0.31378583715124586,
      0.30794454808044414,
      0.320113642974327,
      0.2924245315683043,
      0.32378389139690555,
      0.0,
      0.3201768034030048,
      0.3193263895645633,
      0.3416239961719192,
      0.33807521721470124,
      0.33388499159452034,
      0.3443343370651941,
      0.3038254707230137,
      0.3409826316403388,
      0.30847087272022433,
      0.25808587905864466
    ],
    [
      0.38296863266307324,
      0.4926114279626723,
      0.6298231015939513,
      0.4686968352021814,
      0.47043866642840504,
      0.4021169387248569,
      0.39005079828561007,
      0.5107676306416453,
      0.5551666289650772,
      0.4810016008029152,
      0.4371856538930501,
      0.29721848959248964,
      0.5712861558395508,
      0.5597281111751726,
      0.5220880583460288,
      0.557528264602982,
      0.47419821242455007,
      0.47134820343302053,
      0.436623316484565,
      0.0,
      0.47765160145560714,
      0.42043250294907897,
      0.44122662866967444,
      0.5075376813749248,
      0.4868393252631644,
      0.5197508467312184,
      0.4854672396231554,
      0.5097942163327089,
      0.32435801285097887
    ],
    [
      0.3429574637082793,
      0.47497025516291,
      0.44081966781797255,
      0.4495430938073377,
      0.4757510045150404,
      0.3913091116856773,
      0.32733580426844955,
      0.47014711745095705,
      0.4001340187032658,
      0.4210063620678406,
      0.4427259308140077,
      0.30500889664875697,
      0.5020484129554919,
      0.4525391891618977,
      0.452209642800919,
      0.436383096265363,
      0.409107652770927,
      0.4361380100051724,
      0.38636051947819183,
      0.3912780828035811,
      0.0,
      0.38786495773130114,
      0.4165838932885382,
      0.4009108385361453,
      0.44773927290369064,
      0.41948621270774056,
      0.46691437454410556,
      0.4078108045815685,
      0.33460576240630857
    ],
    [
      0.3842133919933075,
      0.4747527220843799,
      0.4864500482359557,
      0.47935047116722984,
      0.4924702434989785,
      0.4540199528529618,
      0.361651349050492,
      0.44873623778845606,
      0.45789225664208266,
      0.4442946356036599,
      0.4481672731634643,
      0.31988305831981245,
      0.46490714249455234,
      0.46861211347788667,
      0.46489730842687216,
      0.4927753880438128,
      0.4192191622704957,
      0.44403061975971525,
      0.46444885755080323,
      0.46046890304074894,
      0.45917159010701836,
      0.0,
      0.46140450054942916,
      0.4604360743170759,
      0.47685238903797766,
      0.4182006317150664,
      0.49551049966618277,
      0.3831360151172942,
      0.38076504631604857
    ],
    [
      0.34103992788621884,
      0.4620413453736609,
      0.486576112248982,
      0.4008458563820987,
      0.48237795418016893,
      0.42614856151036484,
      0.3355561667116316,
      0.4685474453067382,
      0.4199963972775289,
      0.4886980135125685,
      0.46259127059361727,
      0.3127340868113915,
      0.49053706005787134,
      0.45851639035750913,
      0.47037175861292324,
      0.4511763253973493,
      0.4114453230035675,
      0.39264138303563856,
      0.4281429278004121,
      0.4394210620909238,
      0.45501726364315465,
      0.4161854472861737,
      0.0,
      0.43817696332035316,
      0.5589254474217626,
      0.3888008142533861,
      0.4603363623132908,
      0.38451562015317364,
      0.33150246223683855
    ],
    [
      0.32904860132842684,
      0.3815845897289618,
      0.4098930654056163,
      0.3410090082984236,
      0.4012856349762868,
      0.3119067528156625,
      0.32694683521631607,
      0.456939329960673,
      0.3299398035967809,
      0.3658318579492592,
      0.37240166819558707,
      0.2586133289109662,
      0.35025422827958064,
      0.3542753496725095,
      0.36255684233236574,
      0.4475785768963183,
      0.37423192478491174,
      0.33838213506466097,
      0.35802991708352994,
      0.42224746530782165,
      0.39274574313182686,
      0.37361111308397854,
      0.37827890580516965,
      0.0,
      0.3803457530887586,
      0.3923272232354311,
      0.36947423747616814,
      0.3178402292620375,
      0.25163120368715175
    ],
    [
      0.371981082840684,
      0.4838909405639875,
      0.5367338447142751,
      0.45586126261020565,
      0.47260626449599874,
      0.40268941311041484,
      0.30859574560322534,
      0.4542305915150875,
      0.39124588674519,
      0.5082820824485772,
      0.5061473516201314,
      0.2885327689940389,
      0.47812602940739546,
      0.457102139076089,
      0.4739428138500157,
      0.4685473507744957,
      0.3694571716902242,
      0.43625681515935133,
      0.4254893415209491,
      0.46424713761702785,
      0.4388289404703005,
      0.42428305870692307,
      0.493789750689795,
      0.3994097872808158,
      0.0,
      0.39047230409289413,
      0.5235452553393505,
      0.4197158210956846,
      0.2836668158951159
    ],
    [
      0.3056224633454041,
      0.3593771037147049,
      0.41259459181533487,
      0.35605194518726613,
      0.3273735017813344,
      0.3123272784026705,
      0.3346113852743915,
      0.3593977569900564,
      0.3690330336328871,
      0.35024973018342864,
      0.37859721485769016,
      0.2310273003855421,
      0.3397506761061244,
      0.40499081667060954,
      0.35390961655839814,
      0.418174209926931,
      0.35076951434126036,
      0.3380514917037205,
      0.34241740614783733,
      0.4309305224443891,
      0.39774842136679345,
      0.3185559981067658,
      0.3392286698330391,
      0.3834219145374409,
      0.36035655733289973,
      0.0,
      0.37353584112349636,
      0.4046433206032971,
      0.23806419528302292
    ],
    [
      0.3812921550768471,
      0.5017924070714466,
      0.45656677267628565,
      0.44888282459786133,
      0.4986747597993062,
      0.3650278978496213,
      0.3149942294514001,
      0.44757557060882514,
      0.4511430042066067,
      0.4544401996574372,
      0.4491681864399508,
      0.2993174412353168,
      0.45597028138322715,
      0.4462853706055714,
      0.43645649610138393,
      0.46719036294728244,
      0.427820024991191,
      0.46901268660328754,
      0.43875988213566397,
      0.4440358430715361,
      0.49122948074512873,
      0.40299259092408457,
      0.38098248180145333,
      0.40602469909843597,
      0.4725830453628719,
      0.38992017856900074,
      0.0,
      0.41931538064431373,
      0.3227167580054242
    ],
    [
      0.3312255029476019,
      0.49196758601600066,
      0.54483543540119,
      0.450425860517343,
      0.4443360581506579,
      0.40308057552511944,
      0.353097093850943,
      0.4158066538492411,
      0.5323454578619677,
      0.3928640169587301,
      0.4291923171315777,
      0.33171048170714057,
      0.4669833951630764,
      0.5152277787279069,
      0.4305855104047569,
      0.5177339327814023,
      0.39425389593166815,
      0.4709965846756625,
      0.4199244611122175,
      0.4831584117182226,
      0.45259124037123755,
      0.39676704515239125,
      0.39789406335323463,
      0.41259094091206405,
      0.4066131839743343,
      0.4487300870159925,
      0.43411991961538887,
      0.0,
      0.3379479000108694
    ],
    [
      0.33165570875241146,
      0.39869892265015516,
      0.3727266711094024,
      0.38567043596043016,
      0.40384606199831796,
      0.3761248478172201,
      0.32337570233419854,
      0.3759148169525297,
      0.4101609402217177,
      0.37317488521699915,
      0.3866455746505091,
      0.3600416381150673,
      0.4002122948805875,
      0.36475484194772423,
      0.4005294899682026,
      0.37695572046408343,
      0.3559194563307979,
      0.3807900921684855,
      0.3670475877940451,
      0.36043925785946995,
      0.44296965559527535,
      0.40494267760881963,
      0.387226216889758,
      0.3744367616340145,
      0.41353775456839514,
      0.3526683291113921,
      0.39187084263670036,
      0.381394183610255,
      0.0
    ]
  ],
  "row_avgs": [
    0.16821972382117703,
    0.49079136410079105,
    0.40674002388396263,
    0.45193518509227415,
    0.4021182357716735,
    0.39495029259570424,
    0.33796862110296705,
    0.41100622897819583,
    0.42932973497873633,
    0.3991973190807999,
    0.42594361079015813,
    0.2820230456629461,
    0.40146325619971307,
    0.4312407313621655,
    0.45668675531288516,
    0.44158039332352683,
    0.36845068253192376,
    0.4026050229685176,
    0.325579943137262,
    0.4744251707968682,
    0.4174889089139799,
    0.4452399243675629,
    0.43081663388497493,
    0.3624718330205422,
    0.4331313488545801,
    0.35324330277345484,
    0.4264346789878843,
    0.4323930496727835,
    0.3804904060302487
  ],
  "col_avgs": [
    0.35094366365169016,
    0.44681127843486473,
    0.4542091215415759,
    0.4127734101239936,
    0.41970289902396335,
    0.3780250440682082,
    0.32085677596731116,
    0.42143193143571656,
    0.4260307856144128,
    0.39191639475454515,
    0.41571526091236055,
    0.2867006876833694,
    0.44747917876472565,
    0.43902004636815156,
    0.40763510962994864,
    0.4401381692710353,
    0.3712564769067899,
    0.4105281023129152,
    0.3965426290227008,
    0.4224690162303174,
    0.42804374585382204,
    0.3741648495103407,
    0.3857075968238784,
    0.400856952552296,
    0.4186418611835704,
    0.38551550521254274,
    0.43430760853579625,
    0.39910268137162463,
    0.29743864523579255
  ],
  "combined_avgs": [
    0.2595816937364336,
    0.4688013212678279,
    0.43047457271276923,
    0.4323542976081339,
    0.4109105673978184,
    0.38648766833195625,
    0.3294126985351391,
    0.4162190802069562,
    0.4276802602965746,
    0.3955568569176725,
    0.4208294358512593,
    0.28436186667315777,
    0.4244712174822194,
    0.43513038886515853,
    0.4321609324714169,
    0.440859281297281,
    0.36985357971935684,
    0.4065665626407164,
    0.36106128607998145,
    0.4484470935135928,
    0.42276632738390096,
    0.4097023869389518,
    0.4082621153544267,
    0.38166439278641906,
    0.42588660501907527,
    0.3693794039929988,
    0.43037114376184027,
    0.4157478655222041,
    0.3389645256330206
  ],
  "gppm": [
    591.9427374054328,
    592.203389524271,
    584.8162823982713,
    606.6724559905936,
    601.0499860716618,
    619.8909728411475,
    640.8075451509372,
    598.6010937866403,
    596.4272511586967,
    612.9047856019325,
    606.3909132585704,
    660.5395848616445,
    589.8868804628506,
    594.3289555903482,
    609.9932947574307,
    592.2230985810419,
    622.8293832708961,
    607.0036658402571,
    609.0622285520546,
    603.8458995816667,
    595.5305057614481,
    623.463280992656,
    617.9989537301122,
    610.0600842079723,
    603.2992181679522,
    610.3658778481267,
    594.3041275195875,
    610.5649247023072,
    658.8083400009612
  ],
  "gppm_normalized": [
    1.3689634362401844,
    1.299524753671513,
    1.2807326613044763,
    1.328926655987808,
    1.3153511531213407,
    1.3576882881214636,
    1.4079227502057756,
    1.309679569944304,
    1.3050713659193085,
    1.3427388523958714,
    1.327053138828789,
    1.4506953793767392,
    1.294233472205393,
    1.304462848646752,
    1.3339411237084053,
    1.2998935132850529,
    1.3641946368730993,
    1.328713197659231,
    1.3344338783433025,
    1.3173741281886813,
    1.297182581296628,
    1.361203174772413,
    1.3530481584440643,
    1.3266656915540047,
    1.3164438601338762,
    1.340829075514584,
    1.2979643689518727,
    1.3441891124405907,
    1.4407861424837287
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394,
    657,
    523,
    436,
    419,
    474,
    390,
    480,
    426,
    428,
    404,
    410,
    529,
    440,
    402,
    391,
    498,
    406,
    448,
    479,
    502,
    416,
    388,
    440,
    448,
    427,
    397,
    451,
    445,
    376,
    414,
    412,
    416,
    442,
    410,
    414,
    440,
    404,
    432,
    343,
    419,
    544,
    416,
    447,
    415,
    396,
    390,
    354,
    411,
    419,
    383,
    374,
    379,
    418,
    348,
    421,
    448,
    422,
    374,
    695,
    444,
    457,
    470,
    451,
    421,
    487,
    408,
    443,
    385,
    446,
    534,
    403,
    397,
    389,
    359,
    422,
    369,
    489,
    402,
    372,
    387,
    401,
    466,
    437,
    374,
    409,
    411,
    327,
    755,
    383,
    457,
    399,
    413,
    459,
    551,
    467,
    449,
    422,
    434,
    467,
    444,
    444,
    376,
    444,
    417,
    432,
    477,
    391,
    419,
    369,
    378,
    450,
    409,
    480,
    439,
    393,
    384
  ],
  "response_lengths": [
    3923,
    2175,
    2630,
    2345,
    2454,
    2714,
    3185,
    2611,
    2637,
    2311,
    2449,
    2764,
    2615,
    2560,
    2090,
    2540,
    2299,
    2532,
    2866,
    2181,
    2324,
    2105,
    2150,
    2577,
    2371,
    2641,
    2528,
    2234,
    2326
  ]
}