{
  "example_idx": 56,
  "reference": "Under review as a conference paper at ICLR 2023\n\nFACTOR LEARNING PORTFOLIO OPTIMIZATION INFORMED BY CONTINUOUS-TIME FINANCE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe study financial portfolio optimization in the presence of unknown and uncontrolled system variables referred to as stochastic factors. Existing work falls into two distinct categories: (i) reinforcement learning employs end-to-end policy learning with flexible factor representation, but does not precisely model the dynamics of asset prices or factors; (ii) continuous-time finance methods, in contrast, take advantage of explicitly modeled dynamics but pre-specify, rather than learn, factor representation. We propose FaLPO (factor learning portfolio optimization), a framework that interpolates between these two approaches. Specifically, FaLPO hinges on deep policy gradient to learn a performant investment policy that takes advantage of flexible representation for stochastic factors. Meanwhile, FaLPO also incorporates continuous-time finance models when modeling the dynamics. It uses the optimal policy functional form derived from such models and optimizes an objective that combines policy learning and model calibration. We prove the convergence of FaLPO and provide performance guarantees via a finite-sample bound. On both synthetic and real-world portfolio optimization tasks, we observe that FaLPO outperforms five leading methods. Finally, we show that FaLPO can be extended to other decision-making problems with stochastic factors.\n\n1\n\nINTRODUCTION\n\nPortfolio optimization studies how to allocate investments across multiple risky financial assets such as stocks and safe assets such as US government bonds. The investment target is often formulated as maximizing the expected utility of the investment portfolio’s value at a fixed time horizon, which conceptually maximizes profit while constraining risk (von Neumann & Morgenstern, 1947). With continuous-time stochastic models of stock prices, great advances in the expected utility maximization framework were made in Merton (1969) using stochastic optimal control (dynamic programming) methods. More realistic models incorporate factors like economic indices and proprietary trading signals (Merton et al., 1973; Fama & French, 2015; 1992), which (i) affect the dynamics of stock prices; (ii) stochastically evolve over time; (iii) are not affected by individual investment decisions. With greater data availability, it is natural to design and apply data-driven machine learning methods (Bengio, 1997; Dixon et al., 2020; De Prado, 2018) to handle factors for portfolio optimization. This work proposes a novel method—Factor Learning Portfolio Optimization (FaLPO)—which combines tools from both machine learning and continuous-time finance.\n\nPortfolio optimization with stochastic factors is challenging for three reasons. First, financial data is notoriously noisy and idiosyncratic (Goyal & Santa-Clara, 2003), causing complex purely data-driven methods to be unstable and prone to overfitting. Second, the relationship between the factors and their impact on stock prices can be extremely complicated and difficult to model ex ante. Third, many successful finance models are in continuous time and require interacting with the environment infinitely frequently. As a result, such models cannot be easily combined with machine learning methods, many of which are in discrete time.\n\nCurrent approaches to portfolio optimization broadly fall into two categories: reinforcement learning (RL) and continuous-time finance methods. Many RL solutions to portfolio optimization are built on deep deterministic policy gradient (Silver et al. 2014; Hambly et al. 2021, Section 4.3). Such methods parameterize the policy function as a neural network with strong representation power and learn the neural network by optimizing the corresponding portfolio performance. However, these approaches\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(as well as other model-free methods like Haarnoja et al. 2018) have high sample complexity and tend to overfit due to the high noise in the data. Other RL methods explicitly learn representation (Watter et al., 2015; Lee et al., 2020; Laskin et al., 2021) and leverage discrete-time models (Deisenroth & Rasmussen, 2011; Gu et al., 2016; Mhammedi et al., 2020; Janner et al., 2019; Nagabandi et al., 2018). Nonetheless, these methods are not informed by continuous-time finance models and, as our experiments suggest in Section 5, cannot benefit from structures inherent in the financial market.\n\nStochastic factor models can be used to mathematically derive optimal (or approximately optimal) investment policies (Kim & Omberg, 1996; Chacko & Viceira, 2005; Fouque et al., 2017; Avanesyan, 2021). To this end, one needs domain knowledge to pick and model the factors. Then, model calibration (a.k.a. model fitting, parameter estimation) is conducted by maximizing a calibration objective. With the calibrated model, the optimal investment policy can be derived analytically or numerically (Merton, 1992; Fleming & Soner, 2006). This procedure of calibration and optimization effectively constrains the ‘learning’ in the optimization step, and thus helps reduce overfitting to noisy data. However, this approach cannot capture the complicated factor effects in the data, because the factors may be complex and unlikely to be identified manually. Therefore, these methods may end up with oversimplified models and suffer from model bias with suboptimal performance.\n\nTo tackle these limitations, we propose factor learning portfolio optimization (FaLPO), a new method that interpolates between the two aforementioned solutions (Figure 1). FaLPO includes (i) a neural stochastic factor model to handle huge noise and complicated factor effects and (ii) a model-regularized policy learning method to combine continuous-time models with discrete-time policy learning methods. First, to reduce the sample complexity and avoid overfitting, FaLPO assumes factors and asset prices follow a parametric continuous-time finance model. To capture the complicated factor effects, FaLPO models the factors by a representation function φ parameterized by a neural network with minimal parametric constraints. Second, for policy learning, FaLPO incorporates two regularizations derived from continuous-time stochastic factor models: a policy functional form and model calibration. Specifically, we derive policy functional forms from the neural stochastic factor model using stochastic optimal control tools, and apply it to parameterize the candidate policy in FaLPO. The use of this form in the learning algorithm effectively acts as a regularizer. Then, model calibration and policy learning are conducted jointly, such that the learned policy is informed by continuous-time models.\n\nFigure 1: Demonstration of FaLPO\n\nTheoretically, we prove that the added continuous-time regularization leads to the optimal portfolio performance as the trading frequency increases. Empirically, we demonstrate the improved performance of the proposed method by both synthetic and real-world experiments. We review the related literature in Appendix A. We also discuss how FaLPO is extendable beyond portfolio optimization, and can be applied to other decision-making problems with stochastic factors in Appendix H.\n\n2 BACKGROUND\n\nIn this section, we first formulate the portfolio optimization problem. We then review two major solutions to this problem: deep deterministic policy gradient in reinforcement learning (RL) and stochastic factor models in continuous-time finance.\n\n2.1 PORTFOLIO OPTIMIZATION\n\nProblem Formulation Portfolio optimization seeks to derive a policy of asset allocation that yields high return while maintaining low risk for the investment. Formally, consider dS risky assets with prices St := [S1 ]⊤ and a risk-free money market account with, for simplicity, zero interest rate of return (like cash). We observe dY features (e.g. economic indices, market benchmarks) denoted as Yt. From Yt, we can derive dX factors denoted as Xt which (i) affect the dynamics of asset prices; (ii) evolve over time stochastically; (iii) are not affected by investment decisions. Given an initial investment capital (or wealth) z0 and the initial values for Yt and St as y0 and s0, we use a\n\nt , · · · SdS\n\nt , S2\n\nt\n\n2\n\nFaLPO Discrete Time v.s. Continuous TimeComplicatedFactor EffectsHuge NoiseChallengesModel-RegularizedPolicy Learning Reinforcement LearningExisting WorksOur SolutionNeural StochasticFactor Models Continuous-TimeFinance Under review as a conference paper at ICLR 2023\n\ndS × 1 vector πt to denote the fractions of wealth invested in the dS assets at time point t. Note that negative values are allowed in πt indicating short positions. At the terminal time T > 0, the target is to maximize the expectation of a given utility function E[U (Z π T )], where U : Z → R with Z ⊆ R is the utility function and Z π\n\nT denotes the terminal wealth under π.\n\nIntuitively, a utility function reflects the risk preference of an investor. It is an increasing function of wealth that is also concave: it changes significantly when the wealth is low but less so when the wealth is high (Figure 2). This work focuses on the power utility U (z; γ) := 1 1−γ z1−γ with Z = R+, γ > 0, and γ ̸= 1; and the exponential utility U (z; γ) := − exp(−γz) with Z = R and γ > 0. Here, γ is the investor’s risk aversion coefficient and is hand-picked (instead of tuned) by the user. A larger γ corresponds to more risk aversion, while a smaller γ corresponds to more risk tolerance. Beyond these two utilities, our method is also applicable to other utility functions and other objective functions for portfolio optimization (see Appendix B).\n\nFigure 2: Power & exponential utilities.\n\nγ\n\nDiscrete- and Continuous-Time Policies Discrete- and continuous-time policies are two major types of investment policies, differing on how frequently the portfolio is rebalanced. A discrete-time policy rebalances the portfolio fintely frequently, leading to a discrete-time dynamics for the wealth. Such policies are often considered in RL methods (Section 2.2). Continuous-time policies rebalance the investment infinitely frequently, leading to a continuous-time dynamics for the wealth. These policies are often found explicitly in continuous-time finance models (Section 2.3) 1.\n\n2.2 DEEP DETERMINISTIC POLICY GRADIENT\n\nWe review deep deterministic policy gradient (DDPG, Silver et al. 2014)—a quintessential example of RL methods. DDPG does not explicitly model the dynamics, but instead directly learns a discretetime policy for portfolio optimization. DDPG parameterizes the policy function as a deep neural network and conducts gradient-based policy learning. Denote by π(t, St, Zt, Yt; θD) the deep policy function with parameter θD. Without explicitly modeling the dynamics of St or Xt, DDPG directly maximizes the following performance objective to learn a policy:\n\nV (θD) with V (θD) := E[U (Z π(·;θD)\n\nT\n\n)],\n\nmax θD\n\n(1)\n\nwhere the expectation is over the terminal wealth Z π(·;θD) following the policy π(·; θD). A key step of DDPG is to compute the gradient of V (θD) to update θD. Following the procedure in Appendix C, this can be achieved by sampling the trajectories of St and Yt to approximate the expectation and thus the gradient of V (θD).\n\nT\n\nTypically, DDPG learns a discrete-time policy that rebalances the portfolio finitely frequently. To see how the policy rebalances the portfolio, we study its corresponding wealth process Z π(·;θD) that characterizes the changes in wealth over time. Let ∆t > 0 be the time interval (e.g. daily, weekly) to rebalance the portfolio and, for integer M > 0, let T := M ∆t be the fixed investment horizon (e.g. one or two months). At time m∆t with m ∈ {0, 1, 2, · · · , M − 1}, := πi(m∆t, Sm∆t, Zm∆t, Ym∆t; θD) as the fraction of current wealth invested define πi in the ith risky asset. Then, the wealth change at time m∆t is: Z π(·;θD) =\n\n(m+1)∆t − Z π(·;θD)\n\nm∆t\n\nm∆t\n\nt\n\n(cid:20)\n\nZ π(·;θD)\n\nm∆t\n\n(cid:80)dS\n\ni=1 πi\n\nm∆t\n\nSi\n\n(m+1)∆t−Si Si\n\nm∆t\n\nm∆t\n\n, where Z π(·;θD)\n\nm∆t πi\n\nm∆t\n\nSi\n\n(m+1)∆t−Si Si\n\nm∆t\n\nm∆t\n\nis the wealth change\n\n(cid:21)\n\ndue to the investment in the ith risky asset. Note that the number of shares invested in an asset (Z π(·;θD) ) does not change during (m∆t, (m + 1)∆t): the portfolio rebalances every ∆t time.\n\nm∆t\n\nm∆t\n\nπi Si\n\nm∆t\n\nRL methods like DDPG provide flexible representation for factors: the hidden layers of the neural network are considered as the representation learned for Yt, providing strong representation power.\n\n1Note that it is impossible to rebalance a portfolio infinitely frequently in practice. Thus, continuous-time\n\npolicies are more useful as analytical tools.\n\n3\n\n−20−100100.02.55.07.510.0WealthPower Utilityγ's0.20.40.60.81.21.41.61.8−2.0−1.5−1.0−0.50.00.02.55.07.510.0WealthExponential Utilityγ's0.511.522.533.544.55Under review as a conference paper at ICLR 2023\n\nNonetheless, there is not an explicit parametric model for the learned representation and asset prices. Consequently, such methods require lots of data and tend to overfit (Aboussalah, 2020).\n\n2.3 STOCHASTIC FACTOR MODELS\n\nWe review stochastic factor models in continuous-time finance. These models can explicitly formulate the dynamics and can also be used to mathematically derive the functional form of the optimal continuous-time investment policy. Stochastic factor models are described by stochastic differential equations (SDEs) (see Oksendal 2013 and Appendix D) to formulate the dynamics of asset prices St. Specifically, with a dX × 1 factor variable Xt, let Wt := [W 1 ]⊤ be a dW × 1 Brownian motion that characterizes random fluctuations. Then, St and Xt are assumed to follow\n\nt , · · · W dW\n\nt , W 2\n\nt\n\ndSi t\nSi t\n\n= f i\n\nS(Xt; θ∗\n\nS)dt +\n\ndW(cid:88)\n\nj=1\n\ngij S (Xt; θ∗\n\nS)dW j t ,\n\ni ∈ {1, 2, · · · , dS} ,\n\n(2)\n\ndXt = fX (Xt; θ∗\n\nS)dt + gX (Xt; θ∗\n\nS)⊤dWt.\n\nIn (2), fS : RdX → RdS , fX : RdX → RdX , gS : RdX → RdS ×dW , and gX : RdX → RdX ×dW are parametric functions pre-specified by domain knowledge. Further, fS and fX are often referred to as the drift, and gS and gX as the volatility of St and Xt respectively. Intuitively, SDEs formulate the change of a variable in an infinitesimal time step as the sum of a deterministic part (dt) and a stochastic part (dWt), and we use θ∗ S to parameterize the SDE. The factor Xt appears in both the drift and volatility of the asset prices, thus affecting the price transition. With the parametric functional forms in (2), we can use tools in stochastic optimal control to derive the functional form of the optimal continuous-time investment policy.\n\nContinuous-time policies change the investment in each asset at every time point. For a continuous- , with ̃Z0 = time investment policy ̃πt, the dynamics of wealth ̃Z ̃π z0, S0 = s0 and X0 = x0. Crucially, this is different from the discrete-time wealth process Z π t in in asset i now changes continuously over time, as opposed Section 2.2, as the number of shares to being rebalanced at finite intervals. This discrepancy creates obstacles to directly apply the results derived from stochastic factor models to discrete-time policy learning.\n\nt is defined as d ̃Z ̃π\n\n:= (cid:80)dS\n\n ̃Z ̃π t ̃πi Si t\n\ntdSi Si t\n\ni=1\n\n ̃Z ̃π\n\n ̃πi\n\nt\n\nt\n\nt\n\nt\n\nStochastic factor models can reduce sample complexity for portfolio optimization, since the assumed functional forms in (2) significantly constrain the solution space. However, a crucial step to apply stochastic factor models is to pick or even construct Xt from the observed Yt that perfectly follows a pre-specified model. This step often relies on domain knowledge and thus may end up with oversimplified models suffering from model bias and eventually leading to suboptimal performance.\n\n3 FACTOR LEARNING PORTFOLIO OPTIMIZATION\n\nWe propose factor learning portfolio optimization (FaLPO), a new decision-making framework that interpolates between DDPG and stochastic factor models. FaLPO has two components: (i) a neural stochastic factor model to handle huge noise and complicated factor effects and (ii) model-regularized policy learning to combine continuous-time models with discrete-time policy learning methods.\n\n3.1 NEURAL STOCHASTIC FACTOR MODELS\n\nWe describe neural stochastic factor models (NSFM) and discuss their benefits. On the one hand, a neural stochastic factor model assumes the existence of a representation function φ such that the factors of the problem can be directly learned from its features: Xt = φ(Yt; θ∗ φ). Here, φ is formulated as a neural network with parameter θ∗ φ (Figure 3). As a result, FaLPO avoids hand-picking factors from features as is the case in stochastic factor models (Section 2.3). The neural network representation has only a few parametric constraints and thus is able to capture complicated factor effects in the data. Furthermore, factors Xt and asset prices St are assumed to follow a stochastic factor model (e.g. (2) and (6)), which reduces the sample complexity and avoids overfitting.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.2 MODEL-REGULARIZED POLICY LEARNING\n\nt and the representation function φ(·; θ∗\n\nUnder the proposed neural stochastic factor model, we aim to learn the discrete-time optimal policy function π∗ φ). However, while the policy learning is for discrete-time policies, our proposed model is in continuous time. To bridge this gap, we incorporate two types of continuous-time model regularization into discrete-time policy learning: (i) the policy functional form (3) and (ii) the model calibration objective (4).\n\nPolicy Functional Form From our model, we apply the functional form of a continuous-time optimal policy into our discrete-time policy learning. Using tools in stochastic optimal control, we can derive the functional form of an optimal continuous-time policy: ̃π∗ ̃π), where the functional form of Π can be obtained in many existing stochastic factor models (Kim & Omberg, 1996; Chacko & Viceira, 2005; Avanesyan, 2021; Zariphopoulou, 2001; Wachter, 2002; Kraft, 2005), and θ∗ ̃π is an optimal parameter for Π. FaLPO uses the functional form of Π in policy learning and parameterize the candidate policy as\n\nt = Π(t, St, Zt, Xt; θ∗\n\nπ(t, St, Zt, Yt; θφ, θπ) := Π(t, St, Zt, φ(Yt; θφ); θπ),\n\n(3) where φ is the representation function for the factors in Section 3.1. As a result, Π constrains the policy space and acts as regularization. Importantly, although Π is derived for continuous-time policies, it can still provide guidance for discrete-time policy learning when ∆t is small. We rigorously prove the soundness of using (3) in Section 4.\n\nModel Calibration FaLPO also hinges on model calibration to regularize policy learning. Given the specific functional forms in (2), FaLPO conducts model calibration to estimate the parameters of the SDE. The calibration procedure can be summarized as maximizing a model calibration objective:\n\nFigure 3: Demonstration for neural stochastic factor models.\n\nAlgorithm 1 FaLPO\n\n1: Input: number of iterations N . 2: Initialize θφ and θπ . 3: for n ∈ [N ] do 4:\n\nParameterize the policy function according to (3). Estimate the policy gradient for H in (5) (Appendix C). Update θφ, θπ, and θS.\n\n5:\n\n6: 7: end for 8: Return π(·; θφ, θπ)\n\nmax θS\n\nL(θφ, θS).\n\n(4)\n\nIn practice, with discrete data, one may use likelihood (Phillips, 1972; Beskos & Roberts, 2005) or other likelihood-based objective functions (Bishwal, 2007; Ait-Sahalia & Kimmel, 2010) for L (see Appendix E for concrete examples).\n\nTo harness the information provided by model calibration in policy learning, FaLPO combines the model calibration objective L in (4) with the performance objective V in (1) and facilitates a joint optimization over the two. Note that naively combining L(θφ, θS) and V (θD) is not effective since the two in general do not share common parameters: the parameter of the policy network θD has no overlap with the SDE parameter θS or factor representation parameter θφ. However, by constraining the policy space to (3) in FaLPO, we can show that θφ is also part of the policy parameterization. Thus, V can be derived as V (θφ, θπ) := E[U (Z π(·;θφ,θπ) )]. In other words, θφ is shared in both V and L, and hence FaLPO can carry out a joint optimization over the two:\n\nT\n\nmax (θφ,θπ,θS )∈A\n\nH(θφ, θπ, θS), with H(θφ, θπ, θS) := (1 − λ)V (θφ, θπ) + λL(θφ, θS),\n\n(5)\n\nwhere the candidate policy follows the functional form of the optimal continuous-time policy (3) (see Algorithm 1), and A denotes the considered parameter set. The model calibration objective also acts as a model regularization, where λ ∈ (0, 1) is a hyperparameter determining its effect. In practice, we can optimize (5) by gradient-based methods, facilitating a easy and end-to-end learning procedure (see Appendix C for gradient estimation details).\n\n5\n\nFactor Representation Stochastic Factor Models Not SpecifiedTimeUnder review as a conference paper at ICLR 2023\n\n3.3 EXAMPLE OF FALPO\n\nIn portfolio optimization, one can use different types of stochastic factor models. FaLPO can be applied to many such types (Appendix F). In this section, we use the Kim–Omberg model (Kim & Omberg, 1996) as an example to illustrate FaLPO’s modeling and policy learning. Kim–Omberg is a standard model for portfolio optimization with stocahstic factors, which has been extensively studied empirically (Welch & Goyal, 2008; Muhle-Karbe et al., 2017). For modeling, FaLPO with Kim–Omberg model formulates the dynamics of asset prices and factors as\n\ndSi t\nSi t\n\n= X i\n\nt dt +\n\ndW(cid:88)\n\nj=1\n\nσij dW j\n\nt , dXt = μ(ω − Xt) dt + v dWt, and Xt = φ(Yt; θ∗\n\nφ),\n\n(6)\n\nwhere SDE parameters ω, σ, v, and μ are constant matrices or vectors.\n\nFor policy learning, we detail the policy functional form and model calibration. Under the KimOmberg model, we can derive the optimal policy functional form Π in (3). Specifically, for power utility Π(t, St, Zt, φ(Yt; θφ); θπ) = k1(t; θπ)φ(Yt; θφ) + k2(t; θπ), for exponential utility Π(t, St, Zt, φ(Yt; θφ); θπ) = k1(t; θπ)φ(Yt; θφ)/Zt + k2(t; θπ)/Zt, where k1(·; θπ) : [0, T ] → RdS ×dX and k2(·; θπ) : [0, T ] → RdS ×dX are two time dependent functions (Appendix F.2). We can derive the functional forms of k1 and k2 since the two are solutions to systems of ODEs related to algebraic Riccati equations (Appendix G). We can also directly use function approximators like neural networks or kernel methods for the two. For model calibration, we use a negative mean square loss with the derivation deferred to Appendix E: L(θφ, θS) :=\n\n(cid:80)dS\n\ni=1\n\n(cid:2) log(Si\n\nt+∆t) − log(Si\n\nt) − φi(Yt; θφ)∆t − θi\n\nS\n\n, where in this case θS is a dS × 1\n\n(cid:20)\n\n−E\n\n(cid:3)2(cid:21)\n\nvector. Therefore, to implement FaLPO, we can parameterize the candidate policy function using Π and optimize (5).\n\nNote that the methodology of FaLPO is also generally applicable to other decision-making problems besides portfolio optimization. In Appendix H, we use linear quadratic control with stochastic factors as an example to demonstrate the generality of FaLPO.\n\n4 THEORY\n\nWe theoretically analyze both the asymptotic and non-asymptotic characteristics of FaLPO.\n\n4.1 ASYMPTOTIC ANALYSIS\n\nFaLPO applies the policy functional form and model calibration derived from continuous-time models to discrete-time policy learning. We show that FaLPO can achieve the optimal performance asymptotically (i.e. with infinite data and perfect optimization). In the following, we describe the assumptions and results and provide the formal theorem in the end.\n\nWe provide an intuitive description on the assumptions, with the formal statements provided in Appendix I.1. First, we assume that the portfolio optimization problem satisfies some standard regularity conditions (Higham et al., 2002): the drift and volatility are locally Lipschitz continuous; meanwhile, the asset prices, the stochastic factors, and the wealth process under the optimal policy have bounded moments. Second, we assume that the utility function U (z) has linear growth on z ∈ Z. Note that some widely used cases like power utility with γ < 1 and exponential utility with lower-bounded wealth satisfy this assumption. Third, we consider only admissible policies with parameters in A and we assume that A covers the optimal continuous-time policy parameter. A policy is admissible if it is predictable and if the wealth process Z π t ∈ Z for any t ∈ [0, T ] almost surely. It is a common practice to only consider such admissible policies in portfolio optimization. The last two assumptions are artifacts of the current theoretical analysis; in practice FaLPO can achieve reasonable performance without enforcing them (see Section 5).\n\nWith the foregoing assumptions, we show that the performance of FaLPO can asymptotically converge to that of the best policy in discrete time. In detail, we define V ∗ ∆t := V (π∗) where π∗ is an optimal discrete-time admissible policy with time interval ∆t, i.e., V ∗ ∆t is the optimal performance obtained without constraining to the functional form (3) or leveraging model calibration like (5). Next, define\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nπ,∆t, θ∗\n\nφ,∆t, θ∗\n\nθ∗ ∆t := (θ∗ (3), such that V (θ∗ optimization. Then, Theorem 4.1 shows that the gap between V ∗ ∆t goes to zero.\n\nS,∆t) ∈ arg max(θφ,θπ,θS )∈A H(θφ, θπ, θS) with the policy functional form ∆t) is the performance that FaLPO can achieve with infinite data and perfect ∆t) converges to zero as\n\n∆t and V (θ∗\n\nTHEOREM 4.1 With assumptions in Appendix I.1, lim∆t→0\n\n(cid:0)V ∗\n\n∆t − V (θ∗\n\n∆t)(cid:1) = 0.\n\nTheorem 4.1 justifies the methodology of FaLPO under a small time interval. The proof is provided in Appendix I.2 and Appendix I.3.\n\n4.2 NON-ASYMPTOTIC ANALYSIS\n\nN\n\n(cid:80)N\n\nn=1 θn\n\nWe study the finite-sample performance of FaLPO. We describe the problem setup, major assumptions, and then provide the theorem. In each iteration, we collect B independent trajectories to estimate the gradient of H. Let θn be the estimate after the nth iteration, and N the total number of iterations. we analyze the average estimate ̄θ := instead of θN , which is a common technique for stochastic optimization analysis. Specifically, we aim to bound the expected difference between V ∗ ∆t and V ( ̄θ). Note that it is extremely challenging to theoretically analyze a non-convex stochastic optimization (5) without further specifications in problem setup and assumptions (Polyak, 1963; Bhandari & Russo, 2019; Jin et al., 2021; Ma, 2020; Wang et al., 2019). Therefore, we consider a projection-based variant of FaLPO, under which the optimization process is conducted in a bounded parameter set B ⊆ A. Furthermore, we assume that the objective function H is strongly concave in B with a local maximal point θ† P,∆t, θ† S,∆t). Similar local curvature assumptions are commonly used to analyze non-convex problems (Bach et al., 2017; Loh, 2017). With the above ∆t and V ( ̄θ) satisfies the following finite-sample setup and assumptions, the expected gap between V ∗ bound in Theorem 4.2.\n\n∆t := (θ†\n\nφ,∆t, θ†\n\nTHEOREM 4.2 With the aforementioned projection-based FaLPO algorithm and assumptions, both detailed in Appendix J.2, there exist positive constants C1, C2, C3, and C4 such that\n\nE[V ∗\n\n∆t − V ( ̄θ)] ≤\n\ne∆t 1 − λ\n\n+\n\nH(θ∗\n\n∆t) − H(θ†)\n\n1 − λ\n\n+\n\nC1 log(N ) N (1 − λ)\n\n+\n\nC1 log(N ) BN (1 − λ)\n\n(cid:2)(1 − λ)2C2 + λ2C3 + 2λ(1 − λ)C4\n\n(cid:3),\n\n(7)\n\nwhere λ ∈ [0, 1]. Also, e∆t is an error term not related to N or B but dependent on ∆t with lim∆t→0 e∆t = 0.\n\nTheorem 4.2 provides a non-asymptotic upperbound on the gap between the optimal performance V ∗ ∆t and the one achieved by FaLPO V ( ̄θ). We briefly comment on each term in the upperbound. e∆t 1−λ bounds the asymptotic performance gap caused by leveraging the continuous-time policy functional form constraint and model calibration, and we explain its connection to Theorem 4.1 in Appendix J.4. H(θ∗ controls the performance gap between the local optimal point θ† ∆t. The remaining terms characterize the performance gap between ̄θ and θ†\n\n∆t)−H(θ† 1−λ optimal point θ∗\n\n∆t and the global\n\n∆t)\n\n∆t.\n\nTheorem 4.2 has two implications. First, the bound in (7) is a rational function of λ. Accordingly, there exist situations where a λ ∈ (0, 1) provides a smaller upper bound than λ = 0, indicating the possibility that tuning λ can provide better performance (see experiments in Appendix J). Second, when λ = 1, the bound diverges to infinity. This makes sense since when λ = 1, H(θφ, θπ, θS) = L(θφ, θS) does not contain θπ: the algorithm does not learn the policy. We prove the theorem in Appendices J.1, J.3 and J.4.\n\n5 EXPERIMENTS\n\nBy incorporating continuous-time finance models into policy learning, FaLPO can deal with high data noise and complex factor effects. In this section, we demonstrate the improved performance of FaLPO against existing portfolio optimization methods, over synthetic and real-world experiments.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nMethods\n\nMMMC DDPG SLAC RichID CT-MB-RL FaLPO\n\nExplicit Factor Representation ✖\n✖ ✔\n✔ ✖\n✔\n\nContinuous-Time Model ✔\n✖ ✖\n✔ ✔\n✔\n\nDiscrete-Time Model ✖\n✖ ✔\n✖ ✖\n✖\n\nAnnual Volatility FaLPO DDPG SLAC RichID CT-MB-RL MMMC\n\n0.1\n\n0.3\n\n0.2 −0.465 ± 0.446 −1.35 ± 0.155 −2.737 ± 0.219 −5.495 ± 1.269 −3.30 ± 1.294 −1.650 ± 0.456 −6.160 ± 0.012 −5.50 ± 0.011 −0.750 ± 0.210 −6.325 ± 0.048 −5.65 ± 0.102 −3.350 ± 0.111 −2.850 ± 0.014 −6.160 ± 0.026 −5.35 ± 0.020 −4.723 ± 7.619 −5.602 ± 4.299 −6.124 ± 3.217\n\nTable 1: Competing methods and their characteristics.\n\nTable 2: Average terminal utility after tuning with standard deviation for synthetic data\n\n5.1 SYNTHETIC EXPERIMENTS\n\nMetrics We compare different methods using the average terminal utility since it is the ultimate goal in our portfolio optimization problem formulation and is commonly used in continuous-time finance models. There exist other statistics measuring the performance of portfolios (see Section B). These statistics are not equivalent or consistent with the utility, and thus we do not emphasize them.\n\nMethods We compare FaLPO with five competing methods representing various approaches in prior work (Table 1). (i) Merton Model with Model Calibration (MMMC): model calibration of a classic continuous-time finance baseline, which does not consider stochastic factors (Merton, 1969), combined with closed-from policy function. (ii) Deep Deterministic Policy Gradient (DDPG): a state-of-the-art model-free RL method with deterministic policy (Silver et al., 2014), which many empirical portfolio optimization methods build on. (iii) Stochastic latent actor critic (SLAC): a state-of-the-art representation learning RL method that explicitly learns representation of latent variables (Lee et al., 2020). (iv) Model-based RL with rich observations (RichID): a state-of-the-art model-based RL method with representation learning (Mhammedi et al., 2020). (v) Continuous-time model-based RL (CT-MB-RL): policy gradient optimizing the performance objective using the policy functional form derived from continuous-time models, but directly treating the features as the factors.\n\nFor policy gradient methods, we pick a deterministic policy approach like DDPG as, when compared to non-deterministic policy gradient alternatives, they are more suitable to portfolio optimization due to the continuous action space, high exploration cost, and high noise in financial data (Appendix A.2). For portfolio optimization, different variance reduction methods for policy gradient (Schulman et al., 2015; 2017; Xu et al., 2020) only provide minor performance improvements (Aboussalah, 2020). We hence do not report such results. For areas of RL with representation learning and model-based RL, we focus on those (SLAC and RichID) explicitly learning a representation of latent variables, since such methods are more closely related to FaLPO. There are other techniques like data augmentation, feature construction, adversarial training, and regularization that can improve the empirical performance of portfolio optimization (see survey in Hambly et al., 2021, Section 4.3). In this work, we focus on the central methodological task of policy learning, and most such techniques can be directly combined with our proposed method.\n\nProtocol We simulate environments with the Kim–Omberg model and implement the considered methods to compare their performance. Note that a key data generating parameter in portfolio optimization is the signal-to-noise ratio, which can be roughly characterized by the ratio between the scale of the drift and the scale of the volatility (see Appendix K.1 for detailed explanation). We test our method under different signal-to-noise ratios. To this end, we randomly generate stock drifts to around 10% (to mimic the real-world average return of stocks in SP500), vary the scale of volatility in {10%, 20%, 30%}, and simulate data following the procedure in Appendix K.2. Then, we apply the considered methods to maximize the terminal power and exponential utility with different γ’s. For each method, we tune the learning rate and other method-specific hyperparameters with early stopping (Appendix K.3). With each method-hyperparameter-environment combination, we repeat training, validation, and testing five times.\n\nResults For exponential utility maximization, Table 2 summarizes the average test utility after hyperparameter selection with 10 stocks, 10 features, and γ = 5. FaLPO outperforms all the competing methods in terms of average terminal utility. This performance gain may be explained by the factor representation learning informed by the continuous-time finance model, as other methods are incapable of doing so. Meanwhile, MMMC and CT-MB-RL underperform, which suggests the disadvantage of using oversimplified models. Compared with the more sophisticated RL methods like SLAC and RichID, the simple DDPG is fairly competitive. This is consistent\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nMethods FaLPO DDPG SLAC RichID\n\nEnergy\n\nMaterial\n\n−2.4 ± 1.9 −3.2 ± 1.0 −6.6 ± 1.2 −7.3 ± 1.5 −6.8 ± 0.2 −7.0 ± 1.5 −6.5 ± 0.1 −6.9 ± 1.4\n\nIndustrials −6.3 ± 2.3 −7.3 ± 2.1 −342.4 ± 886.8 −6.9 ± 0.4\n\nCT-MB-RL −4.2 ± 6.2 −5.4 ± 4.3 −11655 ± 32947.5\n\nMMMC\n\n−8.5 ± 7.6 −6.5 ± 1.7\n\n−11.0 ± 5.4\n\nMix −3.5 ± 1.5 −2.5 × 104 ± 3.3 × 108 −3.0 × 108 ± 4.3 × 1012 −8.1 ± 3.9 −5.7 ± 3.1 −7.5 ± 4.4\n\nTable 3: Average terminal utility for real-world data. Mix denotes a mix of stocks in the previous three sectors.\n\nFigure 4: FaLPO average return over portfolio terminal dates.\n\nwith the existing observation that more complicated RL solutions may not always be suitable for portfolio optimization due to the large noise and idiosyncrasy in the data. In the appendix, we provide additional experimental results with different problem dimensions (Appendix K.4.1), other γ values (Appendix K.4.2), and different utility functions (Appendix K.4.3). The results are consistent. Appendix K.4.4 studies a simplified case, where the optimal performance can be mathematically derived. FaLPO achieves a similar performance as the theoretically optimal one.\n\n5.2 REAL-WORLD STOCK TRADING\n\nIn this section, we present an application of FaLPO for real-world stock trading problems. Following the synthetic portfolio optimization setup, we study the six considered methods for 21-day (one month) stock trading in four different stock sectors using the daily stock price data from Yahoo finance between January 4, 2006 and April 1, 2022. For factors, we follow existing works (Aboussalah, 2020; De Prado, 2018; Dixon et al., 2020) and consider economic indexes, technical analysis indexes, and sector-specific features such as oil prices, gold prices, and related ETF prices, leading to around 30 factors for each sector. In each sector we select 10 stocks according to the availability and trading volume in the considered time range (Appendix L.1). The training, validation, and testing data are constructed using rolling windows (Appendix L.3). Table 3 reports the achieved average utility of each method under the selected hyperparamters. FaLPO achieves the highest average utility in all four sectors.\n\nNext, we conduct the training-tuning-testing procedure above with γ ∈ {5, 7, 9}, and report the returns of FaLPO in each quarter in Figure 4. Recall that a smaller γ corresponds to taking more risk. This is consistent with the observation in Figure 4 that the smaller the γ the bigger the return but the larger the fluctuations. Also, the return fluctuates and drops around late 2018 and early 2020. The former corresponds to the abrupt bear market at the end of 2018, and the latter is consistent with the time period that COVID-19 bursts. Under these two time periods, the financial market was especially noisy and unpredictable. We also implement sensitivity analysis on λ in Appendix L.4 and observe that a non-zero small λ works well in practice.\n\n6 EPILOGUE\n\nConclusion This work proposes FaLPO, a new decision-making framework for portfolio optimization with stochastic factors. By using continuous-time finance models to regularize policy learning, FaLPO is able to handle high noise and complex effects in financial data. We demonstrate FaLPO’s benefits both theoretically and empirically. We focus on policy learning and defer more advanced feature engineering methods to future work.\n\nLimitations FaLPO has two potential limitations. First, while we show the extension of FaLPO to problems beyond portfolio optimization, FaLPO is not applicable when there is no suitable parametric model to derive the optimal policy functional form. In such cases, the model-regularized policy learning of FaLPO cannot be implemented. Second, the performance of FaLPO still relies on good features (Yt) to generate factors (Xt). In the presence of unpredictable market events (like COVID19), or when the features do not contain any useful signals (like the Merton case in Appendix K.4.4), additional caution needs to be taken when applying FaLPO.\n\nReproducibility The assumptions and proof details are provided in Appendices I and J. The experiment implementation details are reported in Appendices K and L.\n\n9\n\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●0.00.20.40.60.8Q1 2018Q4 2018Q1 2020Q1 2022DateAverage Return●●●γ=5γ=7γ=9Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAmine Mohamed Aboussalah. What is the value of the cross-sectional approach to deep reinforcement\n\nlearning? Available at SSRN, 2020.\n\nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1–76, 2021.\n\nYacine A ̈ıt-Sahalia. Closed-form likelihood expansions for multivariate diffusions. The Annals of\n\nStatistics, 36(2):906–937, 2008.\n\nYacine Ait-Sahalia and Robert L Kimmel. Estimating affine multifactor term structure models using\n\nclosed-form likelihood expansions. Journal of Financial Economics, 98(1):113–144, 2010.\n\nLevon Avanesyan. Optimal investment in incomplete markets with multiple Brownian externalities.\n\nPhD thesis, Princeton University, 2021.\n\nLevon Avanesyan, Mykhaylo Shkolnikov, and Ronnie Sircar. Construction of forward performance processes in stochastic factor models and an extension of widder’s theorem. arXiv preprint arXiv:1805.04535, 2018.\n\nStephen H Bach, Bryan He, Alexander Ratner, and Christopher R ́e. Learning the structure of generative models without labeled data. In International Conference on Machine Learning, pp. 273–282. PMLR, 2017.\n\nKrzysztof Bartoszek, Sylvain Gl ́emin, Ingemar Kaj, and Martin Lascoux. Using the Ornstein– Uhlenbeck process to model the evolution of interacting populations. Journal of theoretical biology, 429:35–45, 2017.\n\nMaximilian Behr, Peter Benner, and Jan Heiland. Invariant Galerkin Ansatz spaces and DavisonMaki methods for the numerical solution of differential Riccati equations. arXiv preprint arXiv:1910.13362, 2019.\n\nYoshua Bengio. Using a financial training criterion rather than a prediction criterion. International\n\nJournal of Neural Systems, 8(04):433–443, 1997.\n\nAlexandros Beskos and Gareth O Roberts. Exact simulation of diffusions. The Annals of Applied\n\nProbability, 15(4):2422–2444, 2005.\n\nJalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv\n\npreprint arXiv:1906.01786, 2019.\n\nHomanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, and Sergey Levine. Information prioritization through empowerment in visual model-based rl. arXiv preprint arXiv:2204.08585, 2022.\n\nLukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.\n\ncom/. Software available from wandb.com.\n\nJaya PN Bishwal. Parameter estimation in stochastic differential equations. Springer, 2007.\n\nTomas Bj ̈ork. Arbitrage theory in continuous time. Oxford university press, 2009.\n\nSimone P Blomberg, Suren I Rathnayake, and Cheyenne M Moreau. Beyond brownian motion and the ornstein-uhlenbeck process: Stochastic diffusion models for the evolution of quantitative characters. The American Naturalist, 195(2):145–165, 2020.\n\nGeorge Chacko and Luis M Viceira. Dynamic consumption and portfolio choice with stochastic\n\nvolatility in incomplete markets. The Review of Financial Studies, 18(4):1369–1402, 2005.\n\nYevgen Chebotar, Karol Hausman, Marvin Zhang, Gaurav Sukhatme, Stefan Schaal, and Sergey Levine. Combining model-based and model-free updates for trajectory-centric reinforcement learning. In International conference on machine learning, pp. 703–711. PMLR, 2017a.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, and Sergey Levine. Path integral guided policy search. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 3381–3388. IEEE, 2017b.\n\nMarco Corazza and Francesco Bertoluzzo. Q-learning-based financial trading systems with applications. University Ca’Foscari of Venice, Dept. of Economics Working Paper Series No, 15, 2014.\n\nMarcos Lopez De Prado. Advances in financial machine learning. John Wiley & Sons, 2018.\n\nMarc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp. 465–472. Citeseer, 2011.\n\nMatthew F Dixon, Igor Halperin, and Paul Bilokon. Machine Learning in Finance. Springer, 2020.\n\nKenji Doya. Reinforcement learning in continuous time and space. Neural computation, 12(1):\n\n219–245, 2000.\n\nBenjamin Eysenbach, Alexander Khazatsky, Sergey Levine, and Ruslan Salakhutdinov. Mismatched no more: Joint model-policy optimization for model-based rl. arXiv preprint arXiv:2110.02758, 2021.\n\nEugene F Fama and Kenneth R French. The cross-section of expected stock. The Journal of Finance,\n\n47(2):427–465, 1992.\n\nEugene F Fama and Kenneth R French. A five-factor asset pricing model. Journal of financial\n\neconomics, 116(1):1–22, 2015.\n\nVicky Fasen. Statistical estimation of multivariate ornstein–uhlenbeck processes and applications to\n\nco-integration. Journal of Econometrics, 172(2):325–337, 2013.\n\nDamir Filipovi ́c and Eberhard Mayerhofer. Affine diffusion processes: theory and applications. In\n\nAdvanced financial modelling, pp. 125–164. De Gruyter, 2009.\n\nWendell Fleming and Raymond Rishel. Deterministic and stochastic optimal control. Springer, 1975.\n\nWendell H Fleming and Sanjoy K Mitter. Optimal control and nonlinear filtering for nondegenerate diffusion processes. Stochastics: An International Journal of Probability and Stochastic Processes, 8(1):63–77, 1982.\n\nWendell H Fleming and Halil Mete Soner. Controlled Markov processes and viscosity solutions,\n\nvolume 25. Springer Science & Business Media, 2006.\n\nJean-Pierre Fouque, Ronnie Sircar, and Thaleia Zariphopoulou. Portfolio optimization and stochastic\n\nvolatility asymptotics. Mathematical Finance, 27(3):704–745, 2017.\n\nMohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement\n\nlearning: A survey. Foundations and Trends® in Machine Learning, 8(5-6):359–483, 2015.\n\nAmit Goyal and Pedro Santa-Clara.\n\nIdiosyncratic risk matters! The journal of finance, 58(3):\n\n975–1007, 2003.\n\nShixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with model-based acceleration. In International conference on machine learning, pp. 2829–2838. PMLR, 2016.\n\nMao Guan and Xiao-Yang Liu. Explainable deep reinforcement learning for portfolio management: an empirical approach. In Proceedings of the Second ACM International Conference on AI in Finance, pp. 1–9, 2021.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pp. 2555–2565. PMLR, 2019.\n\nBen Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement learning in finance.\n\narXiv preprint arXiv:2112.04553, 2021.\n\nElad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for In Proceedings of the 24th Annual Conference on\n\nstochastic strongly-convex optimization. Learning Theory, pp. 421–436. JMLR Workshop and Conference Proceedings, 2011.\n\nFlorian Herzog, Gabriel Dondi, Hans P Geering, and Lorenz Schumann. Continuous-time multivariate strategic asset allocation. In Proceedings of the 11th Annual Meeting of the German Finance Association, Session 2B, pp. 1–34. Citeseer, 2004.\n\nDesmond J Higham, Xuerong Mao, and Andrew M Stuart. Strong convergence of euler-type methods for nonlinear stochastic differential equations. SIAM Journal on Numerical Analysis, 40(3): 1041–1063, 2002.\n\nVladim ́ır Hol`y and Petra Tomanov ́a. Estimation of ornstein-uhlenbeck process using ultra-highfrequency data with application to intraday pairs trading strategy. arXiv preprint arXiv:1811.09312, 2018.\n\nGuosheng Hu, Yuxin Hu, Kai Yang, Zehao Yu, Flood Sung, Zhihong Zhang, Fei Xie, Jianguo Liu, Neil Robertson, Timpathy Hospedales, et al. Deep stock representation learning: From candlestick charts to investment decisions. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 2706–2710. IEEE, 2018.\n\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based\n\npolicy optimization. Advances in Neural Information Processing Systems, 32, 2019.\n\nZhengyao Jiang, Dixing Xu, and Jinjun Liang. A deep reinforcement learning framework for the\n\nfinancial portfolio management problem. arXiv preprint arXiv:1706.10059, 2017.\n\nChi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points. Journal of the ACM (JACM), 68(2):1–29, 2021.\n\nHilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of\n\nstatistical mechanics: theory and experiment, 2005(11):P11011, 2005.\n\nHilbert Johan Kappen and Hans Christian Ruiz. Adaptive importance sampling for control and\n\ninference. Journal of Statistical Physics, 162(5):1244–1266, 2016.\n\nIoannis Karatzas and Steven Shreve. Brownian motion and stochastic calculus, volume 113. springer,\n\n1987.\n\nBelhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, pp. 1944–1974. PMLR, 2019.\n\nTaesup Kim, Sungjin Ahn, and Yoshua Bengio. Variational temporal abstraction. Advances in Neural\n\nInformation Processing Systems, 32, 2019.\n\nTong Suk Kim and Edward Omberg. Dynamic nonmyopic portfolio behavior. The Review of Financial\n\nStudies, 9(1):141–161, 1996.\n\nHolger Kraft. Optimal portfolios and heston’s stochastic volatility model: an explicit solution for\n\npower utility. Quantitative Finance, 5(3):303–313, 2005.\n\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAlex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. Advances in Neural Information Processing Systems, 33:741–752, 2020.\n\nXiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. Finrl: Deep reinforcement learning framework to automate trading in quantitative finance. In Proceedings of the Second ACM International Conference on AI in Finance, pp. 1–9, 2021.\n\nPo-Ling Loh. Statistical consistency and asymptotic normality for high-dimensional robust m-\n\nestimators. The Annals of Statistics, 45(2):866–896, 2017.\n\nTengyu Ma. Why do local methods solve nonconvex problems? Beyond the Worst-Case Analysis of\n\nAlgorithms, pp. 465, 2020.\n\nSidra Mehtab and Jaydip Sen. A robust predictive model for stock price prediction using deep\n\nlearning and natural language processing. Available at SSRN 3502624, 2019.\n\nRobert C Merton. Lifetime portfolio selection under uncertainty: The continuous-time case. The\n\nreview of Economics and Statistics, pp. 247–257, 1969.\n\nRobert C Merton. Continuous-time finance. Blackwell Cambridge, MA, 1992.\n\nRobert C Merton et al. An intertemporal capital asset pricing model. Econometrica, 41(5):867–887,\n\n1973.\n\nZakaria Mhammedi, Dylan J Foster, Max Simchowitz, Dipendra Misra, Wen Sun, Akshay Krishnamurthy, Alexander Rakhlin, and John Langford. Learning the linear quadratic regulator from nonlinear observations. Advances in Neural Information Processing Systems, 33:14532–14543, 2020.\n\nJohannes Muhle-Karbe, Max Reppen, and H Mete Soner. A primer on portfolio choice with small\n\ntransaction costs. Annual Review of Financial Economics, 9:301–331, 2017.\n\nR ́emi Munos. Policy gradient in continuous time. Journal of Machine Learning Research, 7:771–791,\n\n2006.\n\nAnusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 7559–7566. IEEE, 2018.\n\nAbhishek Nan, Anandh Perumal, and Osmar R Zaiane. Sentiment and knowledge based algorithmic trading with deep reinforcement learning. In International Conference on Database and Expert Systems Applications, pp. 167–180. Springer, 2022.\n\nArkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574– 1609, 2009.\n\nBernt Oksendal. Stochastic differential equations: an introduction with applications. Springer\n\nScience & Business Media, 2013.\n\nMatteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. StochasIn International Conference on Machine Learning, pp.\n\ntic variance-reduced policy gradient. 4026–4035. PMLR, 2018.\n\nPCB Phillips. The structural estimation of a stochastic differential equation system. Econometrica:\n\nJournal of the Econometric Society, pp. 1021–1041, 1972.\n\nPeter CB Phillips and Jun Yu. Maximum likelihood and gaussian estimation of continuous time\n\nmodels in finance. In Handbook of financial time series, pp. 497–530. Springer, 2009.\n\nBoris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational\n\nMathematics and Mathematical Physics, 3(4):864–878, 1963.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nKonrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learning by approximate inference. Proceedings of Robotics: Science and Systems VIII, 2012.\n\nA Max Reppen and H Mete Soner. Bias-variance trade-off and overlearning in dynamic decision\n\nproblems. arXiv preprint arXiv:2011.09349, 2020.\n\nRori V Rohlfs, Patrick Harrigan, and Rasmus Nielsen. Modeling gene expression evolution with an extended Ornstein–Uhlenbeck process accounting for within-species variation. Molecular biology and evolution, 31(1):201–211, 2014.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 1889–1897. PMLR, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nShai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated\n\nsub-gradient solver for svm. Mathematical programming, 127(1):3–30, 2011.\n\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pp. 387–395. PMLR, 2014.\n\nFreek Stulp and Olivier Sigaud. Path integral policy improvement with covariance matrix adaptation. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pp. 1547–1554, 2012.\n\nJingrui Sun and Jiongmin Yong. Stochastic Linear-Quadratic Optimal Control Theory: Open-Loop\n\nand Closed-Loop Solutions. Springer Nature, 2020.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nPeter Tankov. Financial modelling with jump processes. CRC press, 2003.\n\nEvangelos Theodorou, Jonas Buchli, and Stefan Schaal. A generalized path integral control approach to reinforcement learning. The Journal of Machine Learning Research, 11:3137–3181, 2010.\n\nBrandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Benchmarks\n\nfor data-driven offline model-based optimization. arXiv preprint arXiv:2202.08450, 2022.\n\nVladimir Vapnik. Principles of risk minimization for learning theory.\n\nIn Advances in neural\n\ninformation processing systems, pp. 831–838, 1992.\n\nOldrich Vasicek. An equilibrium characterization of the term structure. Journal of financial economics,\n\n5(2):177–188, 1977.\n\nJ. von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University\n\nPress, 1947.\n\nJessica A Wachter. Portfolio and consumption decisions under mean-reverting returns: An exact solution for complete markets. Journal of financial and quantitative analysis, 37(1):63–91, 2002.\n\nHaoran Wang, Thaleia Zariphopoulou, and Xunyu Zhou. Exploration versus exploitation in reinforce-\n\nment learning: a stochastic control approach. arXiv preprint arXiv:1812.01552, 2018.\n\nLingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global\n\noptimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.\n\nManuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. Advances in neural information processing systems, 28, 2015.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nIvo Welch and Amit Goyal. A comprehensive look at the empirical performance of equity premium\n\nprediction. The Review of Financial Studies, 21(4):1455–1508, 2008.\n\nZhuoran Xiong, Xiao-Yang Liu, Shan Zhong, Hongyang Yang, and Anwar Walid. Practical deep\n\nreinforcement learning approach for stock trading. arXiv preprint arXiv:1811.07522, 2018.\n\nPan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variancereduced policy gradient. In Uncertainty in Artificial Intelligence, pp. 541–551. PMLR, 2020.\n\nJiongmin Yong and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations,\n\nvolume 43. Springer Science & Business Media, 1999.\n\nPengqian Yu, Joon Sern Lee, Ilya Kulyatin, Zekun Shi, and Sakyasingha Dasgupta. Model-based deep reinforcement learning for dynamic portfolio optimization. arXiv preprint arXiv:1901.08740, 2019.\n\nThaleia Zariphopoulou. A solution approach to valuation with unhedgeable risks. Finance and\n\nstochastics, 5(1):61–82, 2001.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA RELATED LITERATURE\n\nIn this section, we discuss related literature.\n\nA.1 CONTINUOUS-TIME FINANCE MODELS\n\nExisting strategies for solving portfolio optimization using continuous-time finance models\n\ncan be loosely summarized as performing three steps:\n\n1. Choosing the model for the dynamics, i.e. the type of stochastic differential equation (SDE).\n\n2. Estimating the parameter of the selected model (which is also referred to as model fitting,\n\nmodel identification, or calibration).\n\n3. Solving for the optimal policy under the estimated model.\n\nThe third step leverages stochastic optimal control tools (Fleming & Rishel, 1975; Fleming & Mitter, 1982; Fleming & Soner, 2006; Yong & Zhou, 1999).\n\nFinding and estimating an appropriate model for stochastic optimal control requires significant domain knowledge. For instance, in finance, the modeler must specify both which features are relevant and how they affect stock prices (Fama & French, 1992). If not every relevant factor is correctly specified, optimal control can hardly lead to good performances. As a result, in stock trading, control methods would hand pick three to five economic indices as the factors and assume they follow a simple (often linear) SDE. But indeed trading can benefit from much richer datasets including related option prices, technical indicators, and interest rates (Aboussalah, 2020; De Prado, 2018; Dixon et al., 2020; Mehtab & Sen, 2019).\n\nFurther, even with a correctly specified model and factors, likelihood-based estimation for SDE control models can be very challenging (Phillips & Yu, 2009). As a result, methods like A ̈ıt-Sahalia (2008); Ait-Sahalia & Kimmel (2010) seek to replace the exact likelihood with other likelihood-based objective functions, while maintaining theoretical guarantees. However, the proposed objective function needs to be derived for each specific problem, and the derivation can be challenging. Other methods like Fasen (2013); Hol`y & Tomanov ́a (2018) rely on more specific parametric or lowdimensional setups. To alleviate these issues, our framework extends the existing continuous-time finance models by allowing for a flexible and generalized definition of stochastic factor dynamics. Further, we simultaneously conduct policy learning and model calibration in an RL manner, with a square-loss objective that avoids the calculation of an exact likelihood.\n\nA.2 REINFORCEMENT LEARNING\n\nRL aims to conduct the aforementioned three steps by (i) relying more on data (ii) in an end-to-end fashion. Methods like model-free RL assume no parametric forms on the dynamics, and directly learn the optimal policy while explicitly learning the model (step 1) and estimating the parameters (step 2).\n\nDiscrete-Time Model-Free RL There exist many discrete-time model-free RL methods (Sutton & Barto, 2018). In this category, deep deterministic policy gradient (DDPG) is the most relevant one, and is empirically most widely used for portfolio optimization (Hambly et al., 2021). The reason is twofold. First of all, DDPG is a policy-gradient based method, and thus can naturally handle continuous states and actions in portfolio optimization with simple procedures. Second, DDPG learns a deterministic policy instead of a stochastic one like Haarnoja et al. (2018). This characteristic is especially important in portfolio optimization where the policy learning goal is a deterministic policy since the cost of a stochastic policy is extremely expensive.\n\nContinuous-Time Model-Free RL Continuous-time model-free RL (Wang et al., 2018; Doya, 2000; Munos, 2006) aims to solve for a continuous-time policy. However, such methods do not use or assume any SDE structure, and thus struggle with the common open questions in model-free RL like poor stability and sample complexity. As one example, path integral methods stem from the\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\ntheoretical result that the value function of a type of continuous-time decision-making problems can be expressed in closed form as a Feyman-Kac path integral (Fleming & Rishel, 1975; Kappen, 2005). A series of control/RL methods follow the rationale of optimizing the policy to maximize such an integral. Specifically, Theodorou et al. (2010) propose an open-loop control strategy; Kappen & Ruiz (2016) builds RL with importance sampling; Chebotar et al. (2017a;b); Stulp & Sigaud (2012) combine path integral with other model-based or model-free RL methods. However, the core derivation only holds for decision-making satisfying Kappen & Ruiz (2016, Equation (1)), which is equivalent to assuming that the action does not affect the randomness in decision-making. Such an assumption is limiting, and does not hold for portfolio optimization, where how to allocate the wealth in order to minimize the risk is key to a successful policy.\n\nModel-based RL and RL with Representation Learning Model-based RL and RL with representation learning are two active research areas but without a clear general state-of-the-art (Bharadhwaj et al., 2022; Eysenbach et al., 2021; Trabucco et al., 2022; Janner et al., 2019; Deisenroth & Rasmussen, 2011; Nagabandi et al., 2018; Laskin et al., 2021; Lee et al., 2020; Watter et al., 2015; Chebotar et al., 2017a; Hafner et al., 2019; Kim et al., 2019). The closest to FaLPO are those that learn an explicit representation of a latent variable like Lee et al. (2020); Mhammedi et al. (2020). But such methods are unable to leverage continuous-time finance models for portfolio optimization.\n\nBayesian RL Our proposed framework is also related to Bayesian models (Ghavamzadeh et al., 2015; Rawlik et al., 2012), if we treat the learned representation of factors as the hidden variable. Strictly formulating an NSFM as a Bayesian model requires assumptions specifying the conditional distributions, and thus requires more domain knowledge. The optimization of Bayesian methods is also more challenging.\n\nRL for Stock Trading Various efforts have been made on applying RL to stock trading (Corazza & Bertoluzzo, 2014; Hambly et al., 2021; Nan et al., 2022; Xiong et al., 2018; Guan & Liu, 2021; Liu et al., 2021; Hu et al., 2018; Yu et al., 2019). However, these methods focus more on feature selection or empirical performance-improving techniques. Methodologically, they do not take advantage of continuous-time finance models.\n\nA.3 EMPIRICAL RISK MINIMIZATION\n\nAnother related area is Empirical Risk Minimization (ERM) (Vapnik, 1992). ERM studies the minimization of an objective function using the averages over training data to construct an empirical loss function. Recent work connected ERM with simulation-based and data-based offline decisionmaking methods (Reppen & Soner, 2020). More specifically, when the random input is observable and unaffected by actions, and a training set is available, the decision-making problem can be formulated as an ERM problem. As a result, the portfolio optimization may be reformulated as an ERM extension.\n\nB OTHER OBJECTIVE FUNCTIONS\n\nNote that the goal of portfolio optimization is to maximize the return while minimize or constrain the risk. In practice, one can use different objective functions for such a goal, like mean-variance objective (Hambly et al., 2021), Sharpe ratio, and so on. In this work, we consider utility maximization with power utility and exponential utility. The proposed method also works with other objective functions, as long as we can derive (part of) the optimal policy structure. Note that the selection among these objective functions is more a user-preference question.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nC GRADIENT ESTIMATES\n\nIn this section, we discuss the gradient estimation for both V and L. Assume that we collect B independent trajectories for St and Yt, denoted as\n\nD := {(s0,k, y0,k), (s∆t,k, y∆t,k), (s2∆t,k, y2∆t,k), · · · (sM ∆t,k, yM ∆t,k)}B\n\nk=1 .\n\nThen, the gradient estimate for V (θφ, θπ) is defined as\n\n ̄∇V (θφ, θπ) :=\n\n1 B\n\nB (cid:88)\n\nk=1\n\n ̃∇Vk(θφ, θπ) with ̃∇Vk(θφ, θπ) := ∇θφ,θπ U (zπ(·;θφ,θπ),∆t\n\nT,k\n\n).\n\nThe terminal wealth in the trajectory k under the policy where π(·; θπ) is denoted as zπ(·;θπ),∆t\n\nT,k\n\nwith\n\nzπ(·;θπ),∆t\n\nT,k\n\n:= z0 +\n\nM (cid:88)\n\nm=1\n\nzm−1\n\n(cid:20) dS(cid:88)\n\ni=1\n\nπi(m∆t, ym∆t; θφ, θπ)\n\nNext, we consider the gradient of L:\n\n ̄∇L(θφ, θS) :=\n\n1 B\n\nB (cid:88)\n\nk=1\n\n ̃∇Lk(θφ, θS).\n\n(m+1)∆t − si si si\n\nm∆t\n\nm∆t\n\n(cid:21) .\n\nSpecifically for likelihood and negative mean square loss, we have\n\n ̃∇LLikelihood,k(θφ, θS) :=\n\n1 M\n\nM −1 (cid:88)\n\nm=0\n\n∇θφ,θS log(P(s(m+1)∆t,k, φ(y(m+1)∆t,k; θφ)\n\n| sm∆t,k, φ(ym∆t,k; θφ); θS)),\n\n ̃∇LNM SL,k(θφ, θS) := −\n\n1 M\n\nM −1 (cid:88)\n\ndS(cid:88)\n\nm=0\n\ni=1\n\n(cid:18)\n\n∇θφ,θS\n\nlog(si\n\n(m+1)∆t,k) − log(si\n\nm∆t,k)\n\n(cid:20) (cid:90) (m+1)∆t\n\n− E\n\nm∆t\n\nf j S(Xs; θS)\n\n−\n\n1 2\n\ndS(cid:88)\n\ni=1\n\n(gij\n\n(cid:12) (cid:12) S (Xs; θS))2ds (cid:12) (cid:12)\n\nsm∆t,k, φ(ym∆t,k; θφ)\n\n.\n\n(cid:21)(cid:19)2\n\nAs a result, in each iteration, we collect B trajectories to estimate the gradient of H(θφ, θπ).\n\nD A PRIMER ON STOCHASTIC DIFFERENTIAL EQUATIONS (SDES)\n\nWe provide a general formulation of SDEs with two examples.\n\nD.1 FORMULATION OF SDES\n\nSDEs are a generalization of ordinary differential equations to dynamic systems influenced by random fluctuations. The structure of the randomness can in principle be quite general, such as with jump processes where the state evolution is no longer continuous (Tankov, 2003). Although our method can be generalized to all SDEs, we restrict ourselves to practical settings where the source of randomness is a Brownian motion.\n\nLet Wt be a multi-dimensional independent standard Brownian motion. For a random process St, an SDE is typically expressed using a differential form as\n\ndSt = f (St) dt + g(St) dWt, or St = S0 +\n\n(cid:90) t\n\n0\n\nf (St) dt +\n\n(cid:90) t\n\n0\n\ng(St) dWt,\n\n(8)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nwhere f (·) and g(·) are functions of St. The stochastic integral (cid:82) t 0 g(St)dWt is the accumulation of influence to the state due to the noise. We refer the reader to Karatzas & Shreve (1987) for details on the construction of stochastic integrals and SDE theory. Important here is that Equation (8) defines the transition of St in an infinitesimal time step. The drift coefficient f (St) characterizes the deterministic part of the change of St, and the diffusion coefficient g(St) models the randomness in the transition of St.\n\nD.2 EXAMPLES\n\nAs concrete examples, we discuss two families of SDEs widely used in finance, economics, and biology: Geometric Brownian motion (GBM) and Ornstein–Uhlenbeck (OU) processes (Merton et al., 1973; Vasicek, 1977; Bartoszek et al., 2017; Blomberg et al., 2020; Rohlfs et al., 2014). The OU structure appears in both applications below, and the financial application uses GBM as a base, but extends it with OU drift coefficients. The two types of SDEs are given by\n\nGBM:\n\ndSt St\n\n= μ dt + σ dWt,\n\nOU: dSt = μSt dt + σ dWt,\n\n(cid:110) dSit\n\n(cid:111)\n\nwhere dSt St the drift and diffusion coefficients.\n\n:=\n\nSit\n\ndenotes the component-wise division of St, and the matrices μ and σ define\n\nWe refer the interested reader to Fleming & Soner (2006) for more information on these topics. We now briefly formulate two classic stochastic optimal control models for decision-making with stochastic factors. The stochastic factors appear as the drift coefficients of other state variables and are themselves modeled as SDEs.\n\nD.3\n\nIT ˆO’S FORMULA\n\nItˆo’s Formula is a fundamental analytical tool for SDEs, and crucial for their analysis. We only provide a simple version here, which is sufficient for our analysis. A more general and rigorous statement with assumptions and proof of Itˆo’s formula and integral can be found in Karatzas & Shreve (1987, Theorem 3.3).\n\nLEMMA D.1 (Itˆo’s Formula) Consider a twice differentiable function G, and St following\n\ndSt = f (St) dt + g(St) dWt.\n\nThen, we have\n\ndG(t, St) =\n\n(cid:26) ∂G ∂t\n\n+\n\n(cid:18) ∂G ∂St\n\n(cid:19)⊤\n\nf (St) +\n\n1 2\n\nTr\n\n(cid:20)\n\ng(St)⊤ ∂2G\n\n∂S2 t\n\n(cid:21)(cid:27)\n\ng(St)\n\ndt +\n\n(cid:19)⊤\n\n(cid:18) ∂G ∂St\n\ng(St)dWt.\n\nLEMMA D.2 For a suitable bounded process St, the Itˆo integral (cid:82) t (cid:21)\n\n(cid:19)2(cid:35)\n\n(cid:34)(cid:18)(cid:90) t\n\n(cid:20)(cid:90) t\n\nE\n\nStdWt\n\n= 0,\n\nE\n\nStdWt\n\n0 StdWt satisfies:\n\n= E\n\n(cid:20)(cid:90) t\n\n0\n\n(cid:21)\n\nS2\n\nt dt\n\n.\n\nThe latter is also referred to as Itˆo’s isometry\n\n0\n\n0\n\nE MODEL CALIBRATION\n\nWe discuss two model calibration loss functions, log-likelihood and negative mean square loss.\n\nE.1 LOG-LIKELIHOOD\n\nWe can use log-likelihood as L for model calibration. The log-likelihood of SDEs is derived in a sequential manner. Specifically, for (2) the log-likelihood is derived as\n\nLLog−Likelihood(θφ, θS) := E[log(PθS (St+∆t, φ(Yt+∆t; θφ) | St, φ(Yt; θφ)))], (9) where PθS denotes the conditional likelihood according to (2) but with parameter θS instead of θ∗ S. Then, in specific models, one can derive PθS (St+∆t, φ(Yt+∆t) | St, φ(Yt); θ) (Phillips, 1972; Hol`y & Tomanov ́a, 2018; Beskos & Roberts, 2005) or the approximation of it (Ait-Sahalia & Kimmel, 2010).\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nE.2 NEGATIVE MEAN SQUARE LOSS\n\nFor the SDE system (2), one can also use a negative mean square loss (NMSL) as the calibration objective. To derive this loss function, we first drive the dynamics of log price by applying Itˆo’s Formula Lemma D.1 to (2):\n\nd log(Si\n\nt) = f i\n\nS(Xt; θS)dt −\n\n1 2\n\ndW(cid:88)\n\n(gij\n\nS (Xs; θS)])2dt +\n\nj=1\n\ndW(cid:88)\n\nj=1\n\nS (Xs; θS)]dW j gij t .\n\n(10)\n\nThen, combined with Lemma D.2, under proper assumptions of Lemma D.2, we pose expectation over both sides of the above equation, and derive\n\nE[d log(Si\n\nt)] = f i\n\nS(Xt; θS)dt −\n\n1 2\n\ndW(cid:88)\n\n(gij\n\nS (Xs; θS)])2dt.\n\nj=1\n\n(cid:80)dW\n\nIn words, the expectation of the log price change in an infinitesimal time is f i 1\n2 square loss between the log price change and the expected log price change:\n\nS(Xt; θS)dt − S (Xs; θS)])2dt. Therefore, one can estimate the parameter θS by minimizing the mean\n\nj=1(gij\n\nLNM SL(θS) := −E\n\n(cid:20) dS(cid:88)\n\n(cid:18)\n\ni=1\n\nlog(Si\n\nt+∆t) − log(Si\n\nt) − E\n\nf i\n\nS(Xs; θS)\n\n(cid:20) (cid:90) t+∆t\n\nt\n\n−\n\n1 2\n\ndW(cid:88)\n\nj=1\n\n(gij\n\n(cid:12) (cid:12) S (Xs; θS))2ds (cid:12) (cid:12)\n\n(cid:21)(cid:19)2(cid:21) .\n\nSt, Xt\n\n(11)\n\nIt can be easily proved that the true data generating SDE parameter satisfies θ∗ S ∈ arg max LNM SL(θS). Further, if we take Xt = φ(Yt; θφ) and parameterize the objective as LNM SL(θφ, θS) :=\n\n(cid:20)\n\n−E\n\n(cid:80)dS\n\ni=1\n\n(cid:2) log(Si\n\nt+∆t) − log(Si\n\nt) − φi(Yt; θφ)∆t − θi\n\nS\n\n, we can prove\n\n(cid:3)2(cid:21)\n\nφ, θ∗ θ∗\n\nS ∈ arg max LNM SL(θφ, θS).\n\nNote that in practice it can be very hard to calculate the expectation E(cid:2) (cid:82) t+∆t\n\nf i\n\nS (Xs; θS))2ds(cid:12)\n\n1 2\n\n(cid:80)dW\n\nj=1(gij pectation via E(cid:2)f i\n\n(cid:12)St, Xt\n\nS(XS; θS)∆t − 1\n\n2\n\nS(Xs; θS) − (cid:3). Therefore, when ∆t is small, we replace the conditional ex- (cid:3). Accordingly the calibration\n\nS (XS; θS))2∆t\n\nj=1(gij\n\n(cid:80)dW\n\nt\n\n(cid:12) (cid:12) St, Xt (cid:12) (cid:12)\n\nobjective is defined as\n\nLNM SL(θS) ≈ −E\n\n(cid:20) dS(cid:88)\n\n(cid:18)\n\ni=1\n\nlog(Si\n\nt+∆t) − log(Si\n\nt) − f i\n\nS(Xs; θS)∆t +\n\n1 2\n\ndW(cid:88)\n\nj=1\n\n(gij\n\nS (Xs; θS))2∆t\n\n(cid:21)(cid:19)2(cid:21)\n\n.\n\n(12)\n\nE.3 OTHER MODEL CALIBRATION OBJECTIVE\n\nAnother potential model calibration objective following the same rationale as (11) is\n\nLNM SL−X (θφ, θS) := −E\n\n(cid:20) dX(cid:88)\n\n(cid:18)\n\nφ(Yt+∆t; θφ)i − φ(Yt; θφ)i\n\ni=1\n\n(cid:20) (cid:90) t+∆t\n\n− E\n\nt\n\n(cid:12) (cid:12) X (φ(Ys; θφ)i; θS)ds f i (cid:12) (cid:12)\n\nφ(Yt; θφ)i\n\n(cid:21)(cid:19)2(cid:21) ,\n\nwhich is derived using the conditional expectation of Xt+∆ given Xt. However, the true datagenerating parameter (θ∗ S) is not a maximal point of LNM SL−X . To more clearly see this, this loss function encourages the representation function φ to take a constant output so that Xt is constant over time with f i X (Xs; θS) = 0, and LNM SL−X (θφ) = 0. We also try this loss in experiments, and it leads to poor validation and test performances.\n\nφ, θ∗\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nF APPLICATIONS OF FALPO TO DIFFERENT STOCHASTIC FACTOR MODELS\n\nIN CONTINUOUS-TIME FINANCE\n\nFaLPO can be used with many stochastic factor models in continuous-time finance models. In this section, we discuss the Merton model (Appendix F.1), Kim–Omberg (Appendix F.2) and EVE (Appendix F.3).\n\nF.1 MERTON MODEL\n\nMerton model (Merton, 1969) is a classic setup for portfolio optimization. It studies the allocation of capital across a set of financial assets in order to maximize profits and minimize risks.\n\nF.1.1 MODELING\n\nConsider p risky assets with prices St = {Si i=1 and an additional risk-free money market account with, for simplicity, zero interest rate of return (like cash). The Merton model does not include factors. The dynamics for asset prices is formulated as\n\nt}p\n\ndSi t\nSi t\n\n= μ dt + σdWt.\n\n(13)\n\nThe parameters μ, σ are p × p matrices, with σ denoting the volatility of assets. Further, we use ̃Z ̃π t\nto denote the wealth at time point t under the continuous-time policy ̃π. Under the famous and widely used self-financing assumption (Bj ̈ork, 2009), we have\n\nt\n\nd ̃Z ̃π ̃Z ̃π\n\nt\n\n= ̃πtμdt + ̃πtσdWt.\n\nAn investor’s goal is to maximize the expected utility of capital U (ZT ) at some future time point T :\n\nE ̃πt\n\n(cid:104)\n\nU ( ̃Z ̃π\n\n(cid:105) T )|Z0 = z, S0 = s\n\n.\n\nmax ̃πt\n\n(14)\n\nNegative values in the policy output are allowed, meaning the agent can short any asset.\n\nF.1.2 POLICY FUNCTIONAL FORM\n\nFor the Merton model, Π in (3) can be explicitly derived.\n\nLEMMA F.1 (Policy Functional Form for Merton Model) For a Merton model defined in (13), under common assumptions, the optimal policy for portfolio optimization (14), with power utility follows\n\nThe optimal policy for portfolio optimization (14) with exponential utility follows\n\n ̃π∗ = μ(σσ⊤)−1.\n\n ̃π∗ =\n\nμ(σσ⊤)−1 ̃Z ̃π∗\n\nt\n\n.\n\nProof. Lemma F.1 is a classic result in continuous-time finance, proposed in Merton (1969).\n\n□\n\nIn words, in a Merton model, the optimal policy is independent of time, stock prices, features, and factors. The optimal investment strategy is to keep a constant fraction or amount of wealth in each asset all along, depending on the choice of utility function. Let θπ be a dS × 1 parameter vector. According to Lemma F.1, in FaLPO, we parameterize the candidate policy function as\n\nπ(t, St, Zt, Yt; θφ, θπ) = Π(t, St, Zt, φ(Yt; θφ); θπ) = θπ\n\nfor power utility, and\n\nπ(t, St, Zt, Yt; θφ, θπ) = Π(t, St, Zt, φ(Yt; θφ); θπ) =\n\nθπ Zt\n\nfor exponential utility.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nF.1.3 MODEL CALIBRATION\n\nAccording to the Merton model formulation, there exist no factors affecting the evolution of stock prices. Therefore, we do not add the model calibration objective in FaLPO for Merton problem.\n\nF.2 KIM–OMBERG\n\nKim–Omberg model (Kim & Omberg, 1996) is a standard model for portfolio optimization with predictable asset returns, which has been discussed extensively in the empirical literature (Welch & Goyal, 2008; Muhle-Karbe et al., 2017).\n\nF.2.1 MODELING\n\nIn Kim–Omberg model, the stock dynamics are formulated as\n\ndSi t\nSi t\n\n= X i\n\nt dt +\n\ndW(cid:88)\n\nj=1\n\nσij dW j\n\nt\n\ndXt = μ(ω − Xt) dt + v dWt.\n\nThe portfolio optimization goal is formulated as\n\n ̃V ( ̃πt) with ̃V ( ̃πt) := E ̃πt[U ( ̃Z ̃π\n\nt )|X0 = x, Z0 = z, S0 = s].\n\n(15)\n\nmax ̃πt\n\nF.2.2 POLICY FUNCTIONAL FORM\n\nAn optimal policy function is derived in Lemma F.2\n\nLEMMA F.2 Under common assumptions in Kim & Omberg (1996); Herzog et al. (2004), an optimal policy functional form for (15) with power utility is derived as 1\n1 − γ\n\n(σσ⊤)−1(cid:2)Xt + σv⊤(k3(t)Xt + k2(t))(cid:3),\n\n ̃π∗ =\n\n+\n\n1 2\n\nwhere k2(t) and k3(t) satisfy dk1(t) dt dk2(t) dt dk3(t) dt\n\nTr (cid:8)v⊤(k2(t)k2(t)⊤ + k3(t))v(cid:9) + (μω)⊤k2(t) −\n\nγ 2(γ − 1)\n\n(k2(t)⊤vv⊤k2(t)) = 0,\n\n+ k3(t)vv⊤k2(t) − μ⊤k2(t) + k3(t)μω −\n\nγ γ − 1\n\n((σσ⊤)−1σvk2(t) + k3(t)vv⊤k2(t)) = 0,\n\n+ k3(t)vv⊤k3(t) − k3(t)μ − μ⊤k3(t) γ\nγ − 1\n\n−\n\n((σσ⊤)−1 + (σσ⊤)−1σv⊤k3(t) + k3(t)vσ⊤(σσ⊤)−1 + k3(t)vv⊤k3(t)) = 0,\n\nwith k1(T ) = 0, k2(T ) = 0, and k3(T ) = 0. Note that k1(t) is a scalar, k2(t) is a dS × 1 vector, and k3(t) is a dX × dX . And the ODE of k3(t) is the famous matrix Ricatti equation.\n\nSimilarly, an optimal policy functional form for (15) with exponential utility is derived as (cid:20)\n\n(cid:21)\n\n ̃π∗\n\nt = (σσ⊤)−1\n\nX(t) + σv⊤(k3(t)Xt + k2(t))\n\n,\n\n1 −γ2Zt\n\nwhere k2(t) and k3(t) satisfy 1\n2\n\n+ k3(t)vv⊤k2(t) − μk2(t) + k3(t)μω − ((σσ⊤)−1σvk2(t) + k3(t)vv⊤k2(t)) = 0,\n\n+\n\nTr (cid:8)v⊤(k2(t)k2(t)⊤ + k3(t))v(cid:9) + (μω)⊤k2(t) −\n\ndk1(t) dt dk2(t) dt dk3(t) dt − ((σσ⊤)−1 + (σσ⊤)−1σv⊤k3(t) + k3(t)vσ⊤(σσ⊤)−1 + k3(t)vv⊤k3(t)),\n\n+ k3(t)vv⊤k3(t) − k3(t)μ − μ⊤k3(t)\n\n1 2\n\n(k2(t)⊤vv⊤k2(t)) = 0,\n\nwith k1(T ) = 0, k2(T ) = 0, and k3(T ) = 0.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nF.2.3 MODEL CALIBRATION\n\nFollowing the derivation in Appendix E.2, the negative mean square loss for Kim–Omberg model can\n\nbe derived as L(θφ, θS) := −E\n\n(cid:80)dS\n\ni=1\n\n(cid:2) log(Si\n\nt+∆t) − log(Si\n\nt) − φi(Yt; θφ)∆t − θi\n\nS\n\n(cid:20)\n\n(cid:3)2(cid:21)\n\n, where in\n\nthis case θS is a dS × 1 vector.\n\nF.3 EVE MODEL WITH STOCHASTIC MARKOVIAN FACTORS\n\nWe take EVE model (Avanesyan et al., 2018) with stochastic Markovian factors as another example.\n\nF.3.1 MODELING\n\nWe first detail the modeling of EVE, which formulates the dynamics of asset prices by\n\ndSi t\nSi t\n\n= μi(Xt; θS)dt +\n\ndW(cid:88)\n\nj=1\n\nσji(Xt; θS)dW j t ,\n\ni = 1, 2, · · · , dS,\n\ndXt = (cid:0)M ⊤Xt + ω(cid:1)dt + κ(Xt; θS)⊤dBt, Bt = ρ⊤Wt + A⊤W ⊥ t .\n\n(16)\n\nt\n\nWe use Wt and W ⊥ to denote two sets of independent Brownian motions. ρ denotes a correlation matrix with components ρij ∈ [−1, 1]. Therefore, B is indeed another Brownian motion. We let μ, σ, and κ be parametric functions, with θS denoting all the parameters. The SDE parameters include all the parameter matrices and β’s. Again, with Zt as the wealth, we aim to maximize the power utility at the terminal time T > 0 as the performance objective:\n\n ̃V ( ̃πt) = E ̃π\n\n(cid:20) ( ̃Z ̃π\n\nT )1−γ 1 − γ\n\n(cid:21) .\n\nEVE model poses further assumptions on (16).\n\nASSUMPTION F.3 M has non-negative off-diagonal entries and ω ∈ [0, ∞)k. Further, we assume that there exist λ(x), Λ and L, and N such that μ(·), σ(·), κ(·) and ρ satisfy\n\nλ(x)⊤λ(x) = μ(x)⊤σ(x)−1(cid:0)σ(x)−1(cid:1)⊤ κ(x)⊤κ(x) = diag(L1x1, L2x2, · · · , Lkxk), with L1, L2, · · · , Lk ≥ 0 Γκ(x)⊤ρ⊤λ(x) = N ⊤x.\n\nμ(x) = Λ⊤x\n\n(17)\n\nThe conditions in Assumption F.3 are necessary for the process of Xt to be [0, ∞)k-valued and affine. Under these conditions, the SDE in (16) has a unique weak solution which is affine and takes values in [0, ∞)k (Filipovi ́c & Mayerhofer, 2009). Further, the EVE model requires the following two assumptions:\n\nASSUMPTION F.4 (Assumption 2.2 in Avanesyan et al. (2018)) The functions μ : RdX → RdS , σ : RdX → RdW ×dS are continuous. More over, the columns of ρ belong to the range of leftmultiplication by σ(x) for all x ∈ RdX .\n\nASSUMPTION F.5 (EVE Condition in Avanesyan et al. (2018)) For some p ∈ [0, 1],\n\nwhere I is the identity matrix. Note that when p = 1, ρ is a vector and thus we define p := ρ⊤ρ.\n\nρ⊤ρ = pI,\n\nF.3.2 CONCRETE EXAMPLE\n\nWe consider a more concrete example of EVE satisfying the formulation and assumptions in Appendix F.3.1. Specifically, we use Dμ, Dσ, and Dλ to denote diagonal matrices. Further, let D(x) denote the diagonal matrix whose diagonal is x. Also, we use x◦k for any k ∈ R to denote the component-wise power operation (Hadamard power).\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nThen, we define\n\nThen, we have\n\nFurther, we pose\n\nThen,\n\n2\n\nμ(x) := Dμx◦ 3 σ(x) := DσD(x) κ(x) := ρ−1DκD(x◦ 1\n\n2 ).\n\nλ(x) = D−1\n\nσ Dμx◦ 1 2 .\n\nρ⊤ρ = ρρ⊤ = I.\n\nλ(x) = (cid:0)σ(x)−1(cid:1)⊤ κ(x)⊤κ(x) = DκD(x◦ 1 Γκ(x)⊤ρ⊤λ(x) = ΓD(x◦ 1\n\nμ(x) = D−1 2 )(cid:0)ρ−1(cid:1)⊤\n\n2 )Dκ\n\nσ x◦ 1\n\n2 and λ(x)⊤λ(x) = D−2\n\nσ x,\n\nρ−1DκD(x◦ 1\n\n2 ) = D2\n\n(cid:0)ρ−1(cid:1)⊤\n\nρ⊤D−1\n\nσ x◦ 1\n\nκD(x), 2 = ΓD(x◦ 1\n\n2 )DκD−1\n\nσ x◦ 1\n\n2 = ΓDκD−1\n\nσ x.\n\nSuch a setup is shown satisfies (16).\n\nF.3.3 POLICY FUNCTIONAL FORM\n\nThe policy functional form of the EVE model with Markovian stochastic factors ca be derived as:\n\nLEMMA F.6 Under the assumptions in Appendix F.3.1 and Appendix F.3.2, the optimal policy function follows:\n\n(cid:18)\n\nσ(Xt)−1\n\nλ(Xt) + qρκ(Xt)k2(t)⊤\n\n(cid:19)\n\n,\n\nπ∗\n\nt =\n\nwith\n\n1 γ\n\ndki 2(t) dt\n\n+\n\n1 2\n\nLi(k2(t)i)2 +\n\ndX(cid:88)\n\n(M + N )ijk2(t)j +\n\nj=1\n\nΓ 2q\n\nΛi = 0,\n\ni = 1, 2, · · · , dX\n\ndk1(t) dt\n\n+ ω⊤k2(t) = 0.\n\nFor the specific example in F.3.2, in FaLPO, we parameterize the candidate policy as\n\nπ(t, Yt, Zt; θφ, θπ) = Π(t, Yt, Zt; θφ, θπ) = φ(Yt)\n\n1\n\n2 k(t; θπ).\n\nF.4 MODEL CALIBRATION\n\nFollowing the derivations in Section E.2, we can derive the calibration objective as:\n\nL(θS) := − min\n\nθS =(C1,C2)\n\nE\n\n(cid:13)∆ log(St) − C1φ(Yt; θφ)◦ 3\n\n2 ∆t − C2φ(Yt; θφ)◦2∆t\n\n(cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n.\n\nG SOLUTIONS OF RICCATI DIFFERENTIAL EQUATIONS\n\nAccording to the analysis in Section F, the optimal policy function is closely related to the solutions of Riccati differential equations, which also have closed-form solutions.\n\nSpecifically, with abuse of notation, let A, B and D be p × p matrices, and X(t) : [0, T ] → Rp×p as a function of t solving the following Riccati differential equation:\n\n= A⊤X(t) + X(t)A − X(t)BB⊤X(t) + D⊤D,\n\n∂dX(t) dt X(0) = X0.\n\n(18)\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFollowing the analysis and assumptions in (Behr et al., 2019), the unique symmetric positive stabilizing solution of X(t) follows:\n\nX(t) = X∞ − et ˆA⊤ (cid:0)X∞ − X0\n\n(cid:1)(cid:2)I − (XL − et ˆAXLet ˆA⊤\n\n)(X∞ − X0)(cid:3)−1\n\net ˆA,\n\nwhere\n\nwith ˆA := A − BB⊤X∞, and\n\nˆAXL + XL ˆA⊤ + BB⊤ = 0,\n\n0 = A⊤X∞ + X∞A + D⊤D.\n\nNote that with (18) we can further derive the policy functional forms without using neural networks to parameterize time-dependent fucntions.\n\nH EXTENSION TO LINEAR QUADRATIC CONTROL (LQC)\n\nThe methodology of FaLPO can also be applied to decision-making problems other than portfolio optimization. To implement FaLPO, one needs to first construct a neural stochastic factor model combining factor representation with a continuous-time model. Then, the policy learning is conducted while leveraging policy functional form and model calibration. As an example, we implement FaLPO to linear quadratic control (LQC), and detail modeling (Section H.1), policy functional form (Section H.2) and model calibration (Section H.3).\n\nH.1 MODELING\n\nWe consider the problem of LQC (Sun & Yong, 2020) but with stochastic factor Xt following an OU process. With slight abuse of notation, we use St to denote the sate variable in this section:\n\ndSt = (BSt + U At + Xt) dt +\n\nXt = μXt dt + v dWt,\n\ndW(cid:88)\n\nDjAt dW j t ,\n\nj=1\n\n(19)\n\nwhere B, U , μ, v, and Dj are redefined as matrices with appropriate dimensions. With At = π(·) following the policy π, we aim to solve\n\nmax π\n\nV (π) with V (π) := Eπ\n\n(cid:20) (cid:90) T\n\n0\n\n(cid:2)(QSt)⊤St + (RAt)⊤At\n\n(cid:3) dt + (GST )⊤ST\n\n(cid:21) ,\n\n(20)\n\nwith Q, R, and G as known matrices with appropriate dimensions, and T is terminal time. Further, we apply the modeling strategy in Section 3.1 and aim to learn the representation of stochastic factors from the available features Yt:\n\nXt = φ(Yt; θ∗\n\nφ).\n\nH.2 POLICY FUNCTIONAL FORM\n\nBy taking Ξt (the combination of St and Xt) as the state variables, we can reformulate the problem as a classic LQC problem:\n\ndΞt = BΞΞt + U ΞAt +\n\ndW(cid:88)\n\nj=1\n\n(DΞ\n\nj At + βΞ\n\nt )dW j t ,\n\nwith all the coefficients redefined. Then, under common assumptions in Sun & Yong (2020); Yong & Zhou (1999), it can be derived that the optimal policy satisfies:\n\nwhere\n\n ̃π∗(t, ξ) = ΛΞ(K Ξ(t))−1(U Ξ)\n\n⊤\n\nK Ξ(t)ξ,\n\nΛΞ(K Ξ(t)) = R +\n\ndW(cid:88)\n\nj=1\n\n(D⊤\n\nj K Ξ(t)Dj).\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nAlso, K Ξ(t) solves the differential Riccati equation:\n\nK Ξ(t) = − e(BΞ)⊤(T −t)GΞeBΞ(T −t)\n\n−\n\n(cid:90) T\n\nt\n\nwith\n\ne(BΞ)⊤(T −τ )K Ξ(τ )⊤U Ξ(cid:0)ΛΞ(K Ξ(τ ))⊤(cid:1)−1\n\n(U Ξ)⊤K Ξ(τ )e(BΞ)⊤(T −τ )dτ,\n\nK Ξ(T ) = 0.\n\nTherefore, we can formulate the candidate policy as\n\nπ(t, St, Yt; θφ, θπ) = Π(t, St, Yt; θφ, θπ) = k1(t; θπ)St + k2(t; θπ)φ(Yt; θφ).\n\n(21)\n\nH.3 MODEL CALIBRATION\n\nAccording to (19) and following the derivation strategy in Appendix E.2, we can derive the negative mean square loss for LQC as\n\nL(θφ, θS) := E(cid:2) ∥St+∆tSt − φ(Yt; θφ)∆t − C1St − C2At∥2\n\n2\n\n(cid:3),\n\n(22)\n\nwith θS = {C1, C2}.\n\nAs a summary, to apply FaLPO to LQC with stochastic factors, we parameterize candidate policies following (21) and maximize\n\nwith V in (20) and L in (22).\n\n(1 − λV (θφ, θπ)) + λL(θφ, θS),\n\nI EXTENDED RESULTS FOR THEOREM 4.1\n\nI.1 ASSUMPTIONS AND DEFINITIONS\n\nTo start with, we consider stochastic factor models such that the optimal feedback admissible policy admits a functional form as\n\n ̃π∗\n\nt = Π(t, Xt; θ∗\n\n ̃π).\n\nAlso, we consider the L function such that the true data generating parameters θ∗\n\nS, θ∗\n\nφ satisfy\n\nS, θ∗ θ∗\n\nφ ∈ arg max\n\nθφ,θS\n\nL(θφ, θS),\n\n(23)\n\nwhile other options for L can also be empirically used in our method.\n\nDEFINITION I.1 For a continuous-time policy ̃πt := Π(t, St, Zt, φ(Yt; θφ); θ ̃π), we define its value function as\n\n ̃V (θφ, θπ) := E[U (Z ̃π\n\nT )].\n\nAccordingly, we define the continuous-time version objective function as\n\n ̃H(θφ, θ ̃π, θS) := (1 − λ) ̃V (θφ, θ ̃π) + λL(θφ, θS).\n\nDEFINITION I.2 For t ∈ [m∆t, (m + 1)∆t), define ⌊t⌋ := m∆t and ⌊ ̃π∗\n\nt ⌋ := ̃π∗\n\n⌊t⌋.\n\nDEFINITION I.3 For the continuous-time optimal policy ̃π∗, we use ⌊ ̃π∗⌋ to denote the piece-wise constant version ̃π∗. We use ̃Z ⌊ ̃π∗⌋ to denote the wealth process when implementing the optimal continuous-time policy ̃π(·; θ∗\n\nt\n\nφ, θ∗\n\nπ) in the piece-wise constant manner. Specifically, dS(cid:88)\n\n( ̃π∗\n\nt\n\nd ̃Z ⌊ ̃π∗⌋\n\nt\n\n:= ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\n⌊t⌋)idSi Si\n\n⌊t⌋\n\ni=1\n\ndS(cid:88)\n\n(cid:26)\n\n=\n\ni=1\n\nΠi(⌊t⌋, X⌊t⌋; θ∗\n\n ̃π)f i\n\nS(Xt; θ∗ S)\n\nSi t\nSi\n\n⌊t⌋\n\n ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋ dt\n\n+ Πi(⌊t⌋, X⌊t⌋; θ∗ ̃π)\n\ngij S (Xt; θ∗ S)\n\nSi t\nSi\n\n⌊t⌋\n\ndW(cid:88)\n\nj=1\n\n26\n\n ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋ dW j\n\nt\n\n(cid:27)\n\n.\n\nUnder review as a conference paper at ICLR 2023\n\nFurther, remember ̃Z ̃π∗ the dynamics of continuous-time wealth process ̃Z ̃π\n\nis used to denote the continuous-time wealth process under the policy ̃π∗. By is derived as\n\nt in Section 2.3 the dynamics of ̃Z ̃π∗\n\nt\n\nt\n\nt\n\nd ̃Z ̃π∗ ̃Z ̃π∗\n\nt\n\n= Π(t, Xt; θ∗\n\n ̃π)⊤fS(Xt; θ∗\n\nS)dt + Π(t, Xt; θ∗\n\n ̃π)⊤gS(Xt; θ∗\n\nS)⊤dWt.\n\nASSUMPTION I.4 For each R > 0, if ∥x∥ ≤ R and t ≤ T , we assume that there exists a CR > 0 such that\n\n∥Π(t, x; θ∗\n\nπ)∥ ∨ ∥fS(x; θ∗\n\nS)∥ ∨ ∥gS(x; θ∗\n\nS)∥ ∨ ∥fX (x; θ∗\n\nS)∥ ∨ ∥gX (x; θ∗\n\nS)∥ ≤ CR,\n\nand Π(t, x; θ∗\n\nπ) is locally Lipschitz with Lipschitz constant CR.\n\nFor some p > 2, there exists a constant A such that |p(cid:3) ∨ E(cid:2) sup\n\nE(cid:2) sup\n\n| ̃Z ⌊ ̃π∗⌋\n\n|p(cid:3) ∨ E(cid:2) sup\n\n| ̃Z ̃π∗\n\n0≤t≤T\n\nt\n\n0≤t≤T\n\nt\n\n∥Xt∥p (cid:3) ∨ E(cid:2) sup\n\n0≤t≤T\n\n∥log(St)∥p (cid:3) ≤ A.\n\n0≤t≤T\n\nNote that Assumption I.4 requires the stochastic processes to have bounded high-order moments. For a specific model like Kim–Omberg, this is not guaranteed to hold for every initial value and SDE coefficient, but one can derive model-specific sufficient conditions for Assumption I.4. In practice when implementing the method, we calculate the empirical moments of wealth, factors and asset prices to approximately check whether Assumption I.4 holds.\n\nWith Assumption I.4, we define a stopping time:\n\nDEFINITION I.5 For R > 0, define a stopping time\n\n(cid:26)\n\nτR :=\n\ninf 0≤t≤T\n\n| | ̃Z ̃π∗\n\nt\n\n| ≥ R or ∥Xt∥ ≥ R or ∥log(St)∥ ≥ R or | ̃Z ⌊ ̃π∗⌋\n\nt\n\n(cid:27)\n\n| ≥ R\n\n.\n\nASSUMPTION I.6 The utility function U (z) has a linear bound on Z:\n\n|U (z)| ≤ CU (|z| + 1).\n\nNote that Assumption I.6 holds as long as the utility function is bounded at the smallest value in Z. Specifically for power utility, Assumption I.6 is equivalent to setting γ ∈ (0, 1).\n\nASSUMPTION I.7 There exists ∆t′ > 0 such that for any ∆t < ∆t′ , ⌊ ̃π∗⌋ is also an admissible policy.\n\nI.2 LEMMAS\n\nLEMMA I.8 Consider n non-negative constants c1, c2, · · · , cn. The following inequality is true:\n\nn (cid:88) (\n\ni=1\n\nci)2 ≤ n\n\nn (cid:88)\n\ni=1\n\nc2 i .\n\nProof. The proof follows the Cauchy-Schwartz inequality.\n\n□\n\nLEMMA I.9 (θ∗\n\nφ, θ∗\n\n ̃π, θ∗\n\nS) maximizes both ̃V (θφ, θ ̃π) and L(θφ, θS).\n\nProof. First of all, since θ ̃π∗ is defined to be the optimal parameter for the continuous-time policy π) maximizes ̃V . Then, by (23), and θ∗ □\nφ, θ∗ (θ∗\n\nφ is defined to be the true data generating parameter, (θ∗ S) maximize L.\n\nφ, θ∗\n\nLEMMA I.10 For any δ > 0,\n\nE[ sup\n\n0≤t≤T\n\n( ̃Z ⌊ ̃π∗⌋\n\nt\n\n− ̃Z ̃π∗\n\nt )2] ≤ E(cid:2) sup\n\n0≤t≤T\n\n( ̃Z ⌊ ̃π∗⌋\n\nt∧τR − ̃Z ̃π∗\n\nt∧τR\n\n)2(cid:3) +\n\n2p+1δA p\n\n+\n\n(p − 2)8A pδ2/(p−2)Rp\n\n.\n\nProof. Proof by applying Young’s inequality. See derivation in Higham et al. (2002, Equation (2.8)). □\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nLEMMA I.11 For any t ≤ τR, the difference between the coefficients of the dynamics of ̃Z ̃π∗ ̃Z ⌊ ̃π∗⌋\n\nare bounded by:\n\nt\n\nt\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ndS(cid:88)\n\ni=1\n\nΠ(t, Xt; θ∗\n\nπ)f i\n\nS(Xt; θ∗\n\nS) ̃Z ̃π∗\n\nt −\n\ndS(cid:88)\n\ni=1\n\nΠ(⌊t⌋, X⌊t⌋; θ∗\n\nπ)f i\n\nS(Xt; θ∗\n\nS) ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\nSi t\nSi\n\n⌊t⌋\n\n2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 5d2\n\n+ R2 (cid:13)\n\nS exp(2R)C 4 (cid:13) 2\n(cid:13)\n\n(cid:13)S⌊t⌋ − St\n\nR\n\n(cid:2) exp(2R)R2|t − ⌊t⌋|2 + exp(2R)R2(cid:12)\n\n(cid:12) (cid:12) + exp(2R) (cid:12)\n\n ̃Z ̃π∗\n\nt − ̃Z ⌊ ̃π∗⌋\n\nt\n\n2\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) + exp(2R) (cid:12)\n\n(cid:12) 2\n(cid:12)\n\n(cid:12)Xt − X⌊t⌋ ̃Z ⌊ ̃π∗⌋\n\nt\n\n− ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\n(cid:12) 2(cid:3), (cid:12) (cid:12)\n\nand\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ndS(cid:88)\n\ni=1\n\n(cid:20)\n\nΠ(⌊t⌋, X⌊t⌋; θ∗ π)\n\ndW(cid:88)\n\nj=1\n\n(cid:0)gij\n\nS (Xt; θ∗ S)\n\n ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\n(cid:1)\n\n(cid:21)\n\n−\n\nSi t\nSi\n\n⌊t⌋\n\ndS(cid:88)\n\ni=1\n\n(cid:20)\n\nΠ(t, Xt; θ∗ π)\n\ndW(cid:88)\n\nj=1\n\n≤5d2\n\nS exp(2R)C 4\n\nR\n\n(cid:20)\n\nexp(2R)R2|t − ⌊t⌋|2 + exp(2R)R2(cid:12)\n\n(cid:12)Xt − X⌊t⌋\n\n2\n\n(cid:12) (cid:12)\n\n(cid:0)gij\n\nS (Xt; θ∗\n\nS) ̃Z ̃π∗\n\nt\n\n ̃Z ̃π∗\n\nt − ̃Z ⌊ ̃π∗⌋\n\nt\n\n2\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) + exp(2R) (cid:12)\n\n ̃Z ⌊ ̃π∗⌋\n\nt\n\n− ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\n2(cid:21) .\n\n(cid:12) (cid:12) (cid:12)\n\n+ R2 (cid:13)\n\n(cid:13)S⌊t⌋ − St\n\n(cid:13) 2\n(cid:13)\n\n(cid:12) (cid:12) + exp(2R) (cid:12)\n\nProof.\n\nBy triangle inequality\n\nand\n\n(cid:1)\n\n2\n\n(cid:21)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n ̃πi\n\ntf i\n\nS(Xt; θ∗\n\nS) ̃Z ̃π∗\n\nt −\n\ndS(cid:88)\n\ni=1\n\n ̃πi\n\n⌊t⌋f i\n\nS(Xt; θ∗\n\nS) ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\nSi t\nSi\n\n⌊t⌋\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12)f i\n\nS(Xt; θ∗\n\nS)(cid:12) (cid:12)\n\n(cid:0)(cid:12) (cid:12) ̃πi (cid:12)\n\nt\n\n ̃Z ̃π∗\n\nt Si\n\n⌊t⌋ − ̃πi\n\n⌊t⌋\n\n ̃Z ̃π∗\n\nt Si\n\n⌊t⌋\n\n(cid:12) (cid:12) (cid:12)\n\ndS(cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) dS(cid:88)\n\ni=1\n\n≤\n\n1 Si\n\n⌊t⌋\n\ni=1 (cid:12) (cid:12) ̃πi (cid:12)\n\n+\n\n⌊t⌋\n\n ̃Z ̃π∗\n\nt Si\n\n⌊t⌋ − ̃πi\n\n⌊t⌋\n\n ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋ Si\n\n⌊t⌋\n\n(cid:12) (cid:12) (cid:12) +\n\n(cid:12) (cid:12) ̃πi (cid:12)\n\n⌊t⌋\n\n ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋ Si\n\n⌊t⌋ − ̃πi\n\n⌊t⌋\n\n ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋ Si\n\nt\n\n(cid:12) (cid:1). (cid:12) (cid:12)\n\nFor any t ≤ τR, we can further bound the right hand side by Assumption I.4\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ndS(cid:88)\n\ni=1\n\n ̃πi\n\ntf i\n\nS(Xt; θ∗\n\nS) ̃Z ̃π∗\n\nt −\n\ndS(cid:88)\n\n ̃πi\n\n⌊t⌋f i\n\nS(Xt; θ∗\n\nS) ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\ni=1\n\nSi t\nSi\n\n⌊t⌋\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤dS exp(R)C 2 + exp(R)(cid:0)(cid:12)\n\nR\n\n(cid:2) exp(R)R(cid:0)|t − ⌊t⌋| + (cid:13) (cid:12) t − ̃Z ⌊ ̃π∗⌋ (cid:12) (cid:12) +\n\n ̃Z ⌊ ̃π∗⌋\n\n(cid:12) (cid:12) (cid:12)\n\nt\n\nt\n\n ̃Z ̃π∗\n\n(cid:1)\n\n(cid:13)Xt − X⌊t⌋ − ̃Z ⌊ ̃π∗⌋\n\n(cid:13) (cid:13) (cid:1) + R (cid:13)\n\n(cid:12) (cid:12) (cid:12)\n\n⌊t⌋\n\n(cid:12) (cid:12)\n\n(cid:13)S⌊t⌋ − St\n\n(cid:13) (cid:13)\n\n(cid:3).\n\nThen, by Lemma I.8\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ndS(cid:88)\n\ni=1\n\n ̃πi\n\ntf i\n\nS(Xt; θ∗\n\nS) ̃Z ̃π∗\n\nt −\n\ndS(cid:88)\n\ni=1\n\n ̃πi\n\n⌊t⌋f i\n\nS(Xt; θ∗\n\nS) ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\n2\n\nSi t\nSi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ⌊t⌋ (cid:12) 2(cid:1) (cid:12) 2(cid:1) + R2 (cid:13)\n\n(cid:12)Xt − X⌊t⌋ (cid:12) (cid:12) (cid:12)\n\n⌊t⌋\n\n≤5d2\n\nS exp(2R)C 4\n\nR\n\n(cid:2) exp(2R)R2(cid:0)|t − ⌊t⌋|2 + (cid:12)\n\n+ exp(2R)(cid:0)(cid:12)\n\n(cid:12) (cid:12)\n\n=5d2\n\nS exp(2R)C 4\n\nR\n\n ̃Z ̃π∗\n\n2\n\n(cid:12) (cid:12) (cid:12)\n\nt − ̃Z ⌊ ̃π∗⌋ (cid:2) exp(2R)R2|t − ⌊t⌋|2 + exp(2R)R2(cid:12)\n\n− ̃Z ⌊ ̃π∗⌋\n\n(cid:12) ̃Z ⌊ ̃π∗⌋ (cid:12) (cid:12)\n\n+\n\nt\n\nt\n\n+ R2 (cid:13)\n\n(cid:13)S⌊t⌋ − St\n\n(cid:13) 2\n(cid:13)\n\n+ exp(2R)\n\n(cid:12) ̃Z ̃π∗ (cid:12) (cid:12)\n\nt − ̃Z ⌊ ̃π∗⌋\n\nt\n\n2\n\n(cid:12) (cid:12) (cid:12)\n\n28\n\n(cid:13)S⌊t⌋ − St\n\n2 (cid:3)\n\n(cid:13) (cid:13)\n\n(cid:12) (cid:12)Xt − X⌊t⌋ (cid:12) (cid:12) ̃Z ⌊ ̃π∗⌋ (cid:12) + exp(2R) (cid:12)\n\nt\n\n2\n\n− ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\n2(cid:3).\n\n(cid:12) (cid:12) (cid:12)\n\nUnder review as a conference paper at ICLR 2023\n\nSimilarly\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ndS(cid:88)\n\ni=1\n\n(cid:20)\n\n ̃πi\n\n⌊t⌋\n\ndW(cid:88)\n\nj=1\n\n(cid:0)gij\n\nS (Xt; θ∗ S)\n\nSi t\nSi\n\n⌊t⌋\n\n ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\n(cid:1)\n\n(cid:21)\n\n−\n\ndS(cid:88)\n\n(cid:20)\n\n ̃πi\n\nt\n\ndW(cid:88)\n\ni=1\n\nj=1\n\n(cid:0)gij\n\nS (Xt; θ∗\n\nS) ̃Z ̃π∗\n\nt\n\n(cid:1)\n\n2\n\n(cid:12) (cid:12) (cid:21) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 5d2\n\n+ R2 (cid:13)\n\nS exp(2R)C 4 (cid:13) 2\n(cid:13)\n\n(cid:13)S⌊t⌋ − St\n\nR\n\n(cid:2) exp(2R)R2|t − ⌊t⌋|2 + exp(2R)R2(cid:12)\n\n(cid:12) (cid:12) + exp(2R) (cid:12)\n\n ̃Z ̃π∗\n\nt − ̃Z ⌊ ̃π∗⌋\n\nt\n\n2\n\n(cid:12) (cid:12) (cid:12)\n\n+ exp(2R)\n\n2\n\n(cid:12) (cid:12)\n\n(cid:12)Xt − X⌊t⌋ ̃Z ⌊ ̃π∗⌋\n\nt\n\n(cid:12) (cid:12) (cid:12)\n\n− ̃Z ⌊ ̃π∗⌋\n\n⌊t⌋\n\n2(cid:3).\n\n(cid:12) (cid:12) (cid:12)\n\n□\n\nLEMMA I.12 With τR defined in Definition I.5, E (cid:13)\n\n(cid:13) 2\n(cid:13)Xt∧τR − X⌊t∧τR⌋ (cid:13) (cid:13) 2\n(cid:13) (cid:13) 2\n(cid:13) (cid:13)\n\n(cid:13)St∧τR − S⌊t∧τR⌋ (cid:13) ⌊t∧τR⌋ − ̃Z ⌊ ̃π∗⌋ ̃Z ⌊ ̃π∗⌋ (cid:13) (cid:13)\n\nt∧τR\n\nE (cid:13)\n\nE\n\n≤ 2C 2\n\nR(∆t2 + ∆t),\n\n≤ 2 exp(2R)C 2\n\nR(∆t2 + ∆t),\n\n≤ 2R2 exp(4R)C 4\n\nR(∆t2 + ∆t).\n\nProof. By the dynamics of Xt and Lemma I.8, we can derive\n\nE (cid:13)\n\n(cid:13)Xt∧τR − X⌊t∧τR⌋\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) where mt∧τR satisfies mt∧τR ∆t ≤ (t ∧ τR) < (mt∧τR + 1)∆t. Further, we apply Itˆo’s isometry with stopping time (Lemma D.2), and derive\n\nS)∥2 ds+2E\n\n∥fX (Xs; θ∗\n\ngX (Xs; θ∗\n\nS)⊤dWs\n\n≤ 2∆tE\n\nmt∧τR ∆t\n\nmt∧τR ∆t\n\n(cid:90) t∧τR\n\n(cid:90) t∧τR\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13)\n\n,\n\nE (cid:13)\n\n(cid:13)Xt∧τR − X⌊t∧τR⌋\n\n(cid:13) 2\n(cid:13)\n\n≤ 2∆tE\n\n(cid:90) t∧τR\n\nmt∧τR ∆t\n\n∥fX (Xs; θ∗\n\nS)∥2 ds + 2E\n\n(cid:90) t∧τR\n\nmt∧τR ∆t\n\n∥gX (Xs; θ∗\n\nS)∥2 ds.\n\nBy Assumption I.4, we derive E (cid:13)\n\n(cid:13)Xt∧τR − X⌊t∧τR⌋\n\n(cid:13) 2\n(cid:13)\n\n≤ 2C 2\n\nR(∆t2 + ∆t).\n\nSimilarly\n\nand\n\nE (cid:13)\n\n(cid:13)St∧τR − S⌊t∧τR⌋\n\n(cid:13) 2\n(cid:13)\n\n≤ 2 exp(2R)C 2\n\nR(∆t2 + ∆t),\n\nE\n\n(cid:13) ⌊t∧τR⌋ − ̃Z ⌊ ̃π∗⌋ ̃Z ⌊ ̃π∗⌋ (cid:13) (cid:13)\n\nt∧τR\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤ 2R2 exp(4R)C 4\n\nR(∆t2 + ∆t).\n\nLEMMA I.13 E(cid:2) sup\n\n0≤t≤τ\n\nProof.\n\n( ̃Z ̃π∗\n\nt∧tk\n\n− ̃Z ⌊ ̃π∗⌋\n\nt∧tk\n\n)2(cid:3) ≤10(T + 4)T d2\n\nSC 4\n\nR exp(4R)R2(cid:2)∆t2 + 2C 2\n\nR(∆t2 + ∆t)\n\n+ 2C 2\n\nR(∆t2 + ∆t) + 2 exp(4R)C 4\n\n+ 10(T + 4)d2\n\nSC 4\n\nR exp(4R)\n\n(cid:90) τ\n\n0\n\nR(∆t2 + ∆t)(cid:3) (cid:12) (cid:12) (cid:12) 0≤r≤s\n\n ̃Z ̃π∗\n\nr∧τR\n\nE sup\n\n− ̃Z ⌊ ̃π∗⌋\n\nr∧τR\n\nBy Cauchy–Schwarz inequality and the dynamics of ̃Z ̃π∗\n\nt\n\nand ̃Z ⌊ ̃π∗⌋\n\nt\n\n, for any τ ≤ T\n\nE(cid:2) sup\n\n0≤t≤τ\n\n( ̃Z ̃π∗\n\nt∧tk\n\n− ̃Z ⌊ ̃π∗⌋\n\nt∧tk\n\n)2(cid:3)\n\n(cid:20)\n\n≤2T E\n\n(cid:90) t∧τR\n\n0\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ndS(cid:88)\n\ni=1\n\nsup 0≤t≤τ\n\n ̃πi\n\nsf i\n\nS(Xs; θ∗\n\nS) ̃Z ̃π∗\n\ns −\n\ndS(cid:88)\n\ni=1\n\n ̃πi\n\n⌊s⌋f i\n\nS(Xs; θ∗\n\nS) ̃Z ⌊ ̃π∗⌋\n\n⌊s⌋\n\nSi s\nSi\n\n⌊t⌋\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:21)\n\nds\n\n(cid:20)\n\n+ 2E\n\nsup 0≤t≤τ\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ndW(cid:88)\n\n(cid:90) t∧τR\n\n(cid:20) dS(cid:88)\n\nj=1\n\n0\n\ni=1\n\n ̃πi\n\nsgij(Xs; θ∗\n\nS) ̃Z ̃π∗\n\ns − ̃πi\n\n⌊s⌋gij(X⌊s⌋; θ∗\n\nS)Si\n\ns/Si\n\n⌊s⌋\n\n(cid:21)\n\ndW j\n\ns\n\n29\n\n□\n\n2\n\n(cid:12) (cid:12) (cid:12)\n\nds.\n\n(cid:21) .\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nUnder review as a conference paper at ICLR 2023\n\nThen, by Doob’s martingale inequality\n\nE(cid:2) sup\n\n0≤t≤τ\n\n( ̃Z ̃π∗\n\nt∧tk\n\n− ̃Z ⌊ ̃π∗⌋\n\nt∧tk\n\n)2(cid:3)\n\n(cid:20)\n\n≤2T E\n\n(cid:90) t∧τR\n\n0\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ndS(cid:88)\n\ni=1\n\nsup 0≤t≤τ\n\n ̃πi\n\nsf i\n\nS(Xs; θ∗\n\nS) ̃Z ̃π∗\n\ns −\n\ndS(cid:88)\n\ni=1\n\n ̃πi\n\n⌊s⌋f i\n\nS(Xs; θ∗\n\nS) ̃Z ⌊ ̃π∗⌋\n\n⌊s⌋\n\nSi s\nSi\n\n⌊t⌋\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:21)\n\nds\n\n ̃πi\n\nsgij(Xs; θ∗\n\nS) ̃Z ̃π∗\n\ns − ̃πi\n\n⌊s⌋gij(X⌊s⌋; θ∗\n\nS)Si\n\ns/Si\n\n⌊s⌋\n\n(cid:21)\n\ndW j\n\ns\n\n(cid:19)2(cid:21) .\n\n(cid:20)(cid:18) dW(cid:88)\n\n+ 8T E\n\n(cid:90) t∧τR\n\n(cid:20) dS(cid:88)\n\nj=1\n\n0\n\ni=1\n\nNext, we apply Lemma I.11,\n\nE(cid:2) sup\n\n0≤t≤τ\n\n( ̃Z ̃π∗\n\nt∧tk\n\n− ̃Z ⌊ ̃π∗⌋\n\nt∧tk\n\n)2(cid:3)\n\n≤10(T + 4)d2\n\nSC 4\n\nR exp(2R)E\n\n(cid:0) exp(2R)R2|s − ⌊s⌋|2 + exp(2R)R2(cid:12)\n\n(cid:12)Xs − X⌊s⌋\n\n(cid:12) 2\n(cid:12)\n\n(cid:20) (cid:90) τ ∧τR\n\n0\n\n ̃Z ̃π∗\n\ns − ̃Z ⌊ ̃π∗⌋\n\ns\n\n2\n\n(cid:12) (cid:12) (cid:12)\n\n+ exp(2R)\n\n(cid:12) (cid:12) (cid:12)\n\n ̃Z ⌊ ̃π∗⌋\n\ns\n\n− ̃Z ⌊ ̃π∗⌋\n\n⌊s⌋\n\n(cid:21) 2(cid:1)ds .\n\n(cid:12) (cid:12) (cid:12)\n\n+ R2 (cid:13)\n\n(cid:13)S⌊s⌋ − Ss\n\n(cid:13) 2\n(cid:13)\n\n(cid:12) (cid:12) + exp(2R) (cid:12)\n\nTherefore, combined with I.12,\n\nE(cid:2) sup\n\n0≤t≤τ\n\n( ̃Z ̃π∗\n\nt∧tk\n\n− ̃Z ⌊ ̃π∗⌋\n\nt∧tk\n\n)2(cid:3) ≤10(T + 4)T d2\n\nSC 4\n\nR exp(4R)R2(cid:2)∆t2 + 2C 2\n\nR(∆t2 + ∆t)\n\n+ 2C 2\n\nR(∆t2 + ∆t) + 2 exp(4R)C 4\n\nR(∆t2 + ∆t)(cid:3) (cid:12) (cid:12) (cid:12) 0≤r≤s\n\n ̃Z ̃π∗\n\nr∧τR\n\nE sup\n\n− ̃Z ⌊ ̃π∗⌋\n\nr∧τR\n\n2\n\n(cid:12) (cid:12) (cid:12)\n\nds.\n\n□\n\n+ 10(T + 4)d2\n\nSC 4\n\nR exp(4R)\n\n(cid:90) τ\n\n0\n\nLEMMA I.14 With the definitions and assumptions in Section I.1,\n\nlim ∆t→0\n\nE[( ̃Z ⌊ ̃π∗⌋\n\nT − ̃Z ̃π∗\n\nT )2] = 0.\n\nProof.\n\nBy Lemma I.13, we apply the Gronwall inequality and obtain\n\nE(cid:2) sup\n\n0≤t≤T\n\n( ̃Z ̃π∗\n\nt∧tk\n\n− ̃Z ̃π∗\n\nt∧tk\n\n)2(cid:3)\n\n≤ 10(T + 4)T d2 + 2C 2\n\nR(∆t2 + ∆t) + 2 exp(4R)C 4\n\nSC 4\n\nR exp(4R)R2(cid:2)∆t2 + 2C 2\n\nR(∆t2 + ∆t) R(∆t2 + ∆t)(cid:3) exp(10(T + 4)d2\n\nSC 4\n\nR exp(4R)).\n\nThen, combined with Lemma I.10, for any δ > 0,\n\nE[ sup\n\n0≤t≤T\n\n( ̃Z ̃π∗\n\nt − ̃Z ⌊ ̃π∗⌋\n\nt\n\n)2]\n\nSC 4\n\nR exp(4R)R2(cid:2)∆t2 + 2C 2\n\nR(∆t2 + ∆t) R(∆t2 + ∆t)(cid:3) exp(10(T + 4)d2\n\nSC 4\n\nR exp(4R))\n\n≤ 10(T + 4)T d2 + 2C 2\n\nR(∆t2 + ∆t) + 2 exp(4R)C 4 (p − 2)8A pδ2/(p−2)Rp\n\n2p+1δA p\n\n+\n\n.\n\n+\n\nTherefore, E[sup0≤t≤T ( ̃Z ̃π∗\n\nt − ̃Z ⌊ ̃π∗⌋\n\nt\n\n)2] converges to 0 as ∆t goes to 0.\n\n□\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nI.3 PROOF\n\nFor ease of presentation, we define\n\nθ := (θφ, θπ, θS), θ∗ := (θ∗\n\nφ, θ∗\n\n ̃π, θ∗\n\nS) and θ∗\n\n∆t := (θ∗\n\nφ,∆t, θ∗\n\nπ,∆t, θ∗\n\nS,∆t).\n\nNote that every discrete-time admissible policy is a continuous-time admissible policy. Thus, the continuous-time admissible policy set includes the discrete-time admissible policy set. Therefore,\n\nIn other words, it is enough to bound ̃V (θ∗) − V (θ∗ ̃V and L simultaneously, leading to\n\n∆t) for the proof. By Lemma I.9, θ∗ maximizes\n\n ̃V (θ∗) ≥ V ∗\n\n∆t.\n\n ̃V (θ∗) − V (θ∗\n\n∆t) ≤\n\n ̃H(θ∗) − H(θ∗ 1 − λ\n\n∆t)\n\n.\n\nBy Assumption I.7, for ∆t ≤ ∆t′, θ∗ is also an admissible parameter θ∗ ∈ A, leading to H(θ∗) − H(θ∗\n\n∆t) ≤ 0. Further, for any δ > 0, by adding and subtracting equal terms,\n\n ̃V (θ∗) − V (θ∗\n\n∆t)\n\n≤\n\n≤\n\n1 1 − λ 1\n1 − λ\n\n[ ̃H(θ∗) − H(θ∗) + H(θ∗) − H(θ∗\n\n∆t)]\n\n(24)\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) ̃H(θ∗) − H(θ∗) (cid:12) (cid:12).\n\nNext, we focus on\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) ̃H(θ∗; δ) − H(θ∗; δ) (cid:12) (cid:12), which by definition has\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) ̃H(θ∗; δ) − H(θ∗; δ) (cid:12) (cid:12) (cid:12) = (1 − λ) (cid:12)\n\nE[U ( ̃Z ̃π∗\n\nT ; δ) − U ( ̃Z ⌊ ̃π∗⌋\n\nT\n\n(cid:12) (cid:12) (cid:12), ; δ)]\n\nwhere λL(θ∗) in both ̃H(θ∗; δ) and H(θ∗; δ) omit each other. By Lemma I.14, we have lim∆t→0 ̃Z ⌊ ̃π∗⌋ T . Since U (z; δ) is a continuous function, we implement the continuous mapping theorem and derive\n\nP−→ ̃Z ̃π∗\n\nT\n\n(cid:110) ̃Z ⌊ ̃π∗⌋\n\nBy assumption I.4, Assumption I.6, U ( ̃Z ⌊ ̃π∗⌋ with (25), we derive\n\nT\n\nT\n\nlim ∆t→0\n\nU ( ̃Z ⌊ ̃π∗⌋\n\nT\n\n; δ) P−→ U ( ̃Z ̃π∗\n\nT ; δ).\n\n(25)\n\n(cid:111)\n\n∆t<∆t′\n\nwith different finite ∆t is uniformly integrable. Then, following\n\n; δ) is also uniformly integrable since U (z; δ) has a linear bound. Combined\n\nlim ∆t→0\n\nE[U ( ̃Z ⌊ ̃π∗⌋\n\nT\n\n; δ)] −→ E[U ( ̃Z ̃π∗\n\nT ; δ)],\n\nwhich finishes the proof.\n\nJ EXTENDED RESULTS OF THEOREM 4.2\n\nIn this section, we study the non-asymptotic guarantees on the performance of FaLPO.\n\nJ.1 ANOTHER VERSION OF THEOREM 4.2\n\nDEFINITION J.1 For two random vectors v and w, we define the trace of the covariance matrix as\n\nVar(v) :=E[∥v∥2\n\n2 − ∥E[v]∥2\n\n2],\n\nFurther, we use Var θ(v) and Covθ(v, w) to denote the conditional version of the two given θ:\n\nCov(v, w) :=E[v⊤w] − E[v]⊤E[w].\n\nVarθ(v) :=E[∥v∥2\n\n2 − ∥E[v|θ]∥2\n\n2 |θ],\n\nCovθ(v, w) :=E[v⊤w|θ] − E[v|θ]⊤E[w|θ].\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nNote that it is challenging to theoretically analyze a non-convex stochastic optimization (5), while there are various ad-hoc procedures providing good empirical performances. To provide theoretical analysis, in this section, we study a projection-based version of FaLPO (Algorithm 2). Specifically, the learning/optimization process is conducted in a bounded parameter space B, under which we assume that the objective function is strongly concave regarding the parameters.\n\nAlgorithm 2 Projected FaLPO\n\n1: Input: Hyperparameter λ, learning rate η, number of iterations N , the strongly concave region\n\nB, and batch size B. 2: Output: θφ, θπ, and θS 3: Initialize neural networks with initial parameters (θφ, θπ, θS) ∈ B. 4: Parameterize the policy function by (3). 5: for n ∈ [N ] do 6: 7: 8: 9:\n\nCollect B trajectories. Estimate the gradients of H following the procedure in Appendix C with the parameter λ. Update θS and θR with learning rate η by gradients. Project the achieved update to B.\n\n10: end for 11: Return θφ, θπ, and θS.\n\nFor ease of presentation, we define\n\nθ∗ := (θ∗\n\nφ, θ∗\n\n ̃π, θ∗\n\nS), θ∗\n\n∆t := (θ∗\n\nφ,∆t, θ∗\n\nπ,∆t, θ∗\n\nS,∆t) and θ† := (θ†\n\nφ,∆t, θ†\n\nπ,∆t, θ†\n\nS,∆t).\n\nLet θn be the estimation after the nth iteration, and ̄θ := the average estimation. It is a common technique to consider the average estimation ̄θ instead of the final estimations θN for such analysis. Then, we provide a new version of Theorem 4.2.\n\nn=0 θn\n\nN\n\n(cid:80)N −1\n\nDEFINITION J.2 With the gradient estimations discussed in Appendix C, we define\n\n ̃∇Hk(θ) := (1 − λ) ̃∇Vk(θ) + λ ̃∇Lk(θ). THEOREM J.3 With assumptions in Section J.2, λ ∈ [0, 1), and η < 1 CL\n\nas the learning rate,\n\nE[V ∗\n\n∆t − V ( ̄θ)] ̃H(θ∗) − H(θ∗ 1 − λ\n\n≤\n\n∆t)\n\n+\n\nH(θ∗\n\n∆t) − H(θ†)\n\n1 − λ\n\n+\n\nCB log(N ) 2N (1 − λ)\n\n+\n\n1 2N B(1 − λ)\n\n(cid:20) N −1 (cid:88)\n\nE\n\nn=0\n\n1 n + 1\n\n[(1 − λ)2Var θn\n\n(cid:16) ̃∇Vk(θn)\n\n(cid:17)\n\n+ λ2Var θn\n\n(cid:16) ̃∇Lk(θn)\n\n(cid:17)\n\n(cid:21) + 2λ(1 − λ)Covθn ( ̃∇Vk(θn), ̃∇Lk(θn))] .\n\nAlso, there exits situations where a λ ∈ (0, 1) provides smaller value for (1 − λ)2Var\n\n(cid:16) ̃∇Hk(θn)\n\n(cid:17)\n\n+\n\n(cid:16) ̃∇Lk(θn)\n\n(cid:17)\n\nλ2Var cases where tuning λ may provide better performances.\n\n+ 2λ(1 − λ)Cov( ̃∇Hk(θn), ̃∇Rk(θn)) than λ = 0. In other words, there exist\n\nJ.2 ASSUMPTIONS\n\nASSUMPTION J.4 There exits a constant CB > 0 such that the parameter region B is a convex set and satisfies the following conditions\n\n1. In B ⊆ A, H(θφ,∆t, θπ,∆t, θS,∆t) is locally m-strongly concave with a local maximal point\n\n(θ†\n\nφ,∆t, θ†\n\nP,∆t, θ†\n\nS,∆t) ∈ B.\n\n2. For any θ ∈ B, ∥θ∥ ≤ CB .\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\n3. For any θ ∈ B, the expectation of the gradient estimation is bounded by\n\n(cid:20) (cid:80)B\n\nk=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nE\n\n ̃∇Hk(θ) B\n\n(cid:21)(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ CB.\n\nThese assumptions are widely used in existing analysis (Papini et al., 2018; Karimi et al., 2019; Agarwal et al., 2021; Bhandari & Russo, 2019; Wang et al., 2019; Xu et al., 2020).\n\nASSUMPTION J.5 At the nth iteration, we use the learning rate as ηn+1 = 1\n\nnm .\n\nNote that in practice we will tune the learning rate η as a hyperparameter, since we may not know m. However, it is a common practice to set the learning rate as in Assumption J.5 (Hazan & Kale, 2011; Nemirovski et al., 2009; Shalev-Shwartz et al., 2011)\n\nJ.3 TECHNICAL LEMMAS FOR THEOREM 4.2\n\nLEMMA J.6 With Assumption J.4 and J.5, we have\n\nH(θ†) − H( ̄θ) ≤\n\n1 2N\n\nN −1 (cid:88)\n\nn=0\n\n\n\n\n\n1 n + 1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n1 N\n\nN −1 (cid:88)\n\nn=0\n\n(cid:20) (cid:32)\n\n∇H(θn) −\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:33)⊤\n\n(θ† − θn)\n\n(cid:21) .\n\nProof. By the strong concavity of H in Assumption J.4,\n\n∇H(θn)(θ† − θn) ≥ H(θ†) − H(θn) +\n\nm 2\n\n(cid:13)θn − θ†(cid:13) (cid:13) 2\n(cid:13)\n\n.\n\n(26)\n\nFurther, since θn+1 is the projection of θn + ηn+1\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\nto B, the projection satisfies\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nθn + ηn+1\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n− θ†\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≥ (cid:13)\n\n(cid:13)θn+1 − θ†(cid:13) 2\n(cid:13)\n\n,\n\nwhich suggests\n\n(cid:13)θn − θ†(cid:13) (cid:13) 2\n(cid:13)\n\n− (cid:13)\n\n(cid:13)θn+1 − θ†(cid:13) 2\n(cid:13)\n\n≥ (cid:13)\n\n(cid:13)θn − θ†(cid:13) 2\n(cid:13)\n\n−\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nθn + ηn+1\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n− θ†\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n= −ηn+1\n\n= −η2\n\nn+1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n⊤\n\n ̃∇Hk(θn) B\n ̃∇Hk(θn) B\n\n(cid:0)2θn + ηn+1\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n− 2θ†(cid:1)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 2\n\n− 2ηn+1\n\n(cid:0)θn − θ†(cid:1)⊤\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n.\n\n(27)\n\nWe reorder (27) and derive\n\n−(cid:0)θn − θ†(cid:1)⊤\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n≤\n\n1 2ηn+1\n\n(cid:18)\n\n(cid:13)θn − θ†(cid:13) (cid:13) 2\n(cid:13)\n\n+ η2\n\nn+1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 2\n\n33\n\n− (cid:13)\n\n(cid:13)θn+1 − θ†(cid:13) (cid:13)\n\n2 (cid:19)\n\n.\n\nUnder review as a conference paper at ICLR 2023\n\nTaking the result back to (26):\n\n(cid:32)\n\nH(θ†) − H(θn) ≤\n\n∇H(θn) −\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:33)⊤\n\n(θ† − θn)\n\n+\n\n−\n\n(cid:32)\n\n\n\n(cid:13)θn − θ†(cid:13) (cid:13) 2\n(cid:13)\n\n\n\n− (cid:13)\n\n(cid:13)θn+1 − θ†(cid:13) 2\n(cid:13)\n\n+ η2\n\nn+1\n\n1 2ηn+1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\nm 2\n\n(cid:13)θn − θ†(cid:13) (cid:13) 2\n(cid:13)\n\n,\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=\n\n∇H(θn) −\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:33)⊤\n\n(θ† − θn) +\n\n1 2\n\n(η−1\n\nn+1 − m) (cid:13)\n\n(cid:13)θn − θ†(cid:13) 2\n(cid:13)\n\n−\n\n1 2ηn+1\n\n(cid:13)θn+1 − θ†(cid:13) (cid:13) 2\n(cid:13)\n\n+\n\n(cid:80)B\n\nk=1\n\n1 2\n\nηn+1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n ̃∇Hk(θn) B\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n.\n\nBy averaging over n with Assumption J.5 we get\n\nH(θ†) − H( ̄θ) ≤\n\nN −1 (cid:88)\n\n(H(θ†) − H(θn))/N\n\nn=0\n\n≤\n\n1 2N\n\n\n\n\n\nN −1 (cid:88)\n\nn=0\n\n1 n + 1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n1 N\n\nN −1 (cid:88)\n\nn=0\n\n(cid:20) (cid:32)\n\n∇H(θn) −\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:33)⊤\n\n(θ† − θn)\n\n(cid:21) ,\n\nwhere the first inequality is due to condition 1 of Assumption J.4.\n\nJ.4 PROOF OF THEOREM 4.2\n\n(28)\n\n□\n\nNote that every discrete-time admissible policy is a continuous-time admissible policy. Thus, the continuous-time admissible policy set includes the discrete-time admissible policy set. Therefore, ̃V (θ∗) ≥ V ∗ Therefore, it is enough to bound ̃V (θ∗) − V ( ̄θ) for the proof. By Lemma I.9, θ∗ maximizes ̃V and L simultaneously. Therefore,\n\n∆t.\n\n ̃V (θ∗) − V ( ̄θ) ≤\n\n ̃H(θ∗) − H( ̄θ) 1 − λ\n\n=\n\n ̃H(θ∗) − H(θ∗\n\n∆t) + H(θ∗\n\n∆t) − H(θ†) + H(θ†) − H( ̄θ) 1 − λ\n\n.\n\nThen, we use the convergence result for Algorithm 2 detailed by Lemma J.6 : \n\n2\n\nH(θ†) − H( ̄θ) ≤\n\n1 2N\n\nN −1 (cid:88)\n\nn=0\n\n1 n + 1\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n\n\n+\n\n1 N\n\nN −1 (cid:88)\n\n(cid:20) (cid:32)\n\n∇H(θn) −\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:33)⊤\n\n(θ† − θn)\n\n(cid:21) .\n\nn=0 Then, we take expectation on both sides\n\nE[H(θ†) − H( ̄θ)] ≤\n\n1 2N\n\nN −1 (cid:88)\n\nE\n\nn=0\n\n+\n\n1 N\n\nN −1 (cid:88)\n\nn=0\n\n\n\n\n\n1 n + 1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2\n\n\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:33)⊤\n\n(θ† − θn)\n\n(cid:21) .\n\n(cid:20) (cid:32)\n\nE\n\n∇H(θn) −\n\n34\n\n(29)\n\n(30)\n\n(31)\n\nUnder review as a conference paper at ICLR 2023\n\nFor the last component of (31), since ̃∇Hk(θn) is an unbiased gradient estimator:\n\n(cid:20) (cid:32)\n\nE\n\n∇H(θn) −\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:33)⊤\n\n(cid:21)\n\n(θ† − θn)\n\n(cid:20)\n\n(cid:20) (cid:32)\n\n= E\n\nE\n\n∇H(θn) −\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:33)⊤\n\n(cid:12) (cid:12) (θ† − θn) (cid:12) (cid:12)\n\nθn\n\n(cid:21)(cid:21)\n\n= 0.\n\nThen, for the first component in (31) \n\nE\n\n\n\n(cid:34)\n\n=E\n\nE\n\n1 n + 1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:20) 1\n\nn + 1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:21)(cid:35)\n\n|θn\n\n(cid:34)\n\nE\n\n(cid:34)\n\nE\n\n=\n\n1 n + 1\n\n≤\n\n1 n + 1\n\nVarθn (\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n) +\n\n(cid:20) (cid:80)B\n\nk=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nE\n\n ̃∇Hk(θn) B\n\n|θn\n\n2 (cid:35)\n\n(cid:21)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nVarθn (\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n(cid:35)\n\n) + C 2\n\nB\n\n,\n\nwhere the last inequality is due to condition 3 of Assumption J.4. Then, (31) can be further derived as\n\nE[H(θ†) − H( ̄θ)] ≤\n\n1 2N\n\n(cid:34)\n\nN −1 (cid:88)\n\nE\n\nn=0\n\n1 n + 1\n\nVarθn (\n\n(cid:80)B\n\nk=1\n\n ̃∇Hk(θn) B\n\n) +\n\n1 n + 1\n\n(cid:35)\n\nCB\n\n.\n\nAs a result,\n\nE[H(θ∗ =E[H(θ∗\n\n∆t) − H( ̄θ)] ∆t) − H(θ†) + H(θ†) − H( ̄θ)]\n\n≤H(θ∗\n\n∆t) − H(θ†) +\n\nCB log(N ) 2N η[(1 − λ)2Var θn\n\n(cid:16) ̃∇Vk(θn)\n\n(cid:17)\n\n+ λ2Var θn\n\n(cid:16) ̃∇Lk(θn)\n\n(cid:17)\n\n]\n\n+ E\n\n+ E\n\nN −1 (cid:88)\n\nn=0\n\nN −1 (cid:88)\n\nn=0\n\nBN (n + 1)\n\nη[2λ(1 − λ)Covθn ( ̃∇Vk(θn), ̃∇Lk(θn))] BN (n + 1)\n\n(cid:27)\n\n.\n\nTaking the results back to (29) we finish the proof.\n\nK EXTENDED RESULTS FOR SYNTHETIC EXPERIMENTS\n\nFor synthetic portfolio optimization, we provide details for drift and volatility (Appendix K.1), data generation (Appendix K.2), hyperparameter tuning (Appendix K.3), and extended experimental results (Appendix K.4). We consider 21-day trading, and generate 1000 trajectories with 21 observations for training, 1000 for validation, and 1000 for testing. To compare different methods, we calculate the average terminal utility as the metric.\n\nK.1 DRIFT AND VOLATILITY\n\nDrift and volatility are two important concepts characterising the strength of signal and noise in financial markets. To demonstrate this, for an asset price Si\n\nt and time interval ∆t, define return as:\n\nreturni\n\nt =\n\nSi\n\nt+∆t − Si Si t\n\nt\n\n.\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nThe returni t can be daily, monthly or yearly, depending on the length of ∆t. For a specific asset, S) in (2)) is approximately the expectation of the return, while volatility (gi(Xt; θ∗ drift (f i(Xt; θ∗ S)) is approximately the return’s standard deviation. Given multiple assets, drift (f (Xt; θ∗ S)) is a vector and volatility (g(Xt; θ∗ S)) is a matrix. When generating synthetic data (Appendix K.2), we fix the scale of drift and vary the scale of volatility, which is defined as the average value of each component.\n\nK.2 DATA GENERATION\n\nWe simulate data for St and Xt following SDE (6). To this end, drift and volatility are randomly picked while mimicking the historical stock price data, with an average annual return around as 0.1 and average annual volatility in {0.1, 0.2, 0.3}, leading to a daily return around 0.1 252 and a daily volatility around {0.1/252, 0.2/252, 0.3/252}. The true representation function is selected as a component-wise exponential operation. Then, we discritize the SDE following the explicit Euler method, and generate data accordingly (Beskos & Roberts, 2005).\n\nThe specific configurations for data generation is:\n\n• Define two scalars: Cd, and Cv determining the scale of drift and volatility:\n\nCd = 0.1/252, , and Cv ∈ {0.1/252, 0.2/252, 0.3/252} .\n\n• σ is selected as a random matrix, whose components follow a uniform distribution in\n\n[0.5Cv, 1.5Cv].\n\n• v is selected as a random matrix, whose components follow a uniform distribution in\n\n[−1.5CdCv, 1.5CdCv].\n\n• μ is selected as a diagonal matrix whose diagonal components follow a uniform distribution\n\nin [0.9, 1].\n\n• The initial values of X are randomly generated from a uniform distribution on [−2Cd, 2Cd].\n\n• The initial prices of assets are randomly generated from a uniform distribution in [20, 30].\n\nNote that the design makes sure that the simulated price has approximately a yearly return of 0.1 and yearly volatility in {0.1, 0.2, 0.3}. Table 4 reports the experimental setup parameters.\n\nExperiment Configurations\n\nValues\n\nTime Interval ∆t Terminal Time T Scale of Annual Drift Scale of Annual Volatility Number of Simulated Trajectories Utility Function Risk Aversion γ Number of Replications under Each Hyperparameter Compute Resources\n\n1 (Day) 21 0.1 {0.1, 0.2, 0.3} 1000 {P ower, Exponential} {0.1, 3, 5, 10} 5\nAWS ec2 m5ad.24xlarge\n\nTable 4: Setup for synthetic experiments\n\nK.3 EARLY STOPPING AND HYPERPARAMETER TUNING\n\nFor better performance, we conduct early stopping for all methods using the average validation utility with the patience as 5 steps. The considered hyperparameters include the learning rate, λ, and batch size. For each configuration, we conduct training for 5 times, and average the results. Then, we pick the configuration providing the best average validation utility, and test it on the test data and calculate the average test utility per trajectory. The tuning process is conducted using the software wandb (Biewald, 2020). Table 5 reports the hyperparameter values.\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nHyperparameters\n\nValues\n\nBatch Size λ\nLearning Rate\n\n{100, 50} {0, 05, 0.1, 0.9} {0.0005, 0.001, 0.01, 0.1}\n\nTable 5: Hyperparameters for synthetic experiments\n\nK.4 SYNTHETIC EXPERIMENT RESULTS\n\nTo gain a more holistic understanding of the performance of FaLPO in a variety of settings, we conduct experiments under different number of stocks to be traded (Appendix K.4.1), different risk preferences (Appendix K.4.2), and alternative utility functions (Appendix K.4.3). Finally, we also compare the performance of various methods under the Merton model as a sanity check (Appendix K.4.4).\n\nK.4.1 SYNTHETIC EXPERIMENT RESULTS WITH DIFFERENT DIMENSIONS\n\nTables 6 and 7 report the synthetic experiment results with the number of simulated stocks (dS) varying in {10, 15}. The performance is not strictly negatively correlated with the number of dimensions of the problem or the annual volatility in simulation. The reason is that the noise in the problem is indeed determined by the whole volatility matrix σ, which is randomly generated in the synthetic experiment (Appendix K.2). In other words, the dimension and average scale cannot fully characterize the extent of the noise in a synthetic task.\n\nMethods\n\nAnnual Volatility in Simulation FaLPO DDPG SLAC RichID CT-MB-RL MMMC\n\n0.1\n\n0.3\n\n0.2 −0.465 ± 0.446 −1.35 ± 0.155 −2.737 ± 0.219 −1.650 ± 0.456 −3.30 ± 1.294 −5.495 ± 1.269 −0.750 ± 0.210 −5.50 ± 0.011 −6.160 ± 0.012 −3.350 ± 0.111 −5.65 ± 0.102 −6.325 ± 0.048 −2.850 ± 0.014 −5.35 ± 0.020 −6.160 ± 0.026 −4.723 ± 7.619 −5.602 ± 4.299 −6.124 ± 3.217\n\nTable 6: Average terminal utility after tuning with standard deviation for synthetic data with dS = 10 and dW = 10.\n\nMethods\n\nAnnual Volatility in Simulation FaLPO DDPG SLAC RichID CT-MB-RL MMMC\n\n0.1\n\n0.3\n\n0.2 −2.463 ± 3.744 −1.021 ± 0.278 −2.243 ± 0.547 −3.976 ± 1.428 −1.443 ± 0.751 −5.205 ± 1.858 −4.749 ± 0.139 −6.129 ± 0.016 −6.526 ± 0.012 −4.973 ± 0.448 −6.321 ± 0.038 −6.641 ± 0.022 −3.074 ± 0.014 −5.714 ± 0.023 −6.363 ± 0.021 −5.388 ± 5.688 −6.465 ± 4.978 −7.155 ± 5.965\n\nTable 7: Average terminal utility after tuning with standard deviation for synthetic data with dS = 15 and dW = 15.\n\nK.4.2 SYNTHETIC EXPERIMENT RESULTS WITH DIFFERENT VALUES OF γ\n\nTables 8 and 9 report experimental results with dS = 10, dW = 10, and γ ∈ {3, 10} for an exponential utility. FaLPO outperforms the competing methods in most scenarios.\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nMethods\n\nAnnual Volatility in Simulation FaLPO DDPG SLAC RichID CT-MB-RL MMMC\n\n0.1\n\n0.2 −0.003 ± 0.0021 −0.0055 ± 0.0008 −0.0132 ± 0.0028 −0.003 ± 0.001 −0.0105 ± 0.006 −0.0205 ± 0.0034 −0.003 ± 0.0007 −0.0153 ± 0.0013 −0.0192 ± 0.0011 −0.012 ± 0.0005 −0.0188 ± 0.0002\n\n0.3\n\n−0.01 ± 0.0\n\n−0.0179 ± 0.0 −0.0162 ± 0.0212 −0.0194 ± 0.0135 −0.0210 ± 0.0102\n\n−0.0211 ± 0.0 −0.0206 ± 0.0\n\nTable 8: Average terminal utility after tuning with standard deviation for synthetic data with γ = 10.\n\nMethods\n\nAnnual Volatility in Simulation FaLPO DDPG SLAC RichID CT-MB-RL MMMC\n\n0.2\n\n0.3\n\n0.1 −4.575 ± 3.325 −23.113 ± 4.3472 −23.514 ± 11.7077 −44.629 ± 1.8797 −34.842 ± 0.4686\n\n−17.9358 ± 2.3349 −56.0405 ± 13.0502 −51.6559 ± 11.7981 −50.0399 ± 13.8451 −76.1371 ± 0.0355 −68.6816 ± 0.0254 −77.232 ± 0.1091 −69.481 ± 0.9413 −75.4364 ± 0.122 −65.41 ± 0.1331 −59.2338 ± 77.4511 −70.8667 ± 49.2500 −76.8619 ± 37.448\n\nTable 9: Average terminal utility after tuning with standard deviation for synthetic data with γ = 3.\n\nK.4.3 SYNTHETIC EXPERIMENT RESULTS WITH POWER UTILITY\n\nWe also conduct synthetic experiments maximizing the expected power utility for portfolio optimization. The results are summarized in Figure 5.\n\n(a) γ = 0.1\n\n(b) γ = 0.2\n\n(c) γ = 0.3\n\nFigure 5: Average Terminal Power Utility\n\nK.4.4 SYNTHETIC EXPERIMENT FOR THE MERTON CASE\n\nAs a sanity check, we study a Merton problem where the optimal performance can be mathematically derived, in order to compare the performance of FaLPO to the optimal one. We simulate data following a Merton model in Appendix F.1, with dS = 10, dW = 10. With an exponential utility function with γ = 5, according to Lemma F.1, the optimal policy can be derived as\n\n ̃π∗ =\n\nμ(σσ⊤)−1 Zt\n\n.\n\n(32)\n\nFurther, by taking (32) back into (14), we can derive the optimal expected terminal utility as\n\nmax ̃πt\n\nE ̃π[U (Z ̃π\n\nT )|z0] = −\n\ne−γz0 γ\n\ne− 1\n\n2 μ⊤(σσ⊤)−1μT ,\n\nwhich is the theoretically optimal performance. Then, to implement FaLPO, we generate fake features which are independent from the asset prices: the optimal policy is not dependent on these features. Ideally, FaLPO should be able to automatically ignore the fake features, and deliver performance similar to the theoretically optimal derivation. The results of FaLPO, MMMC, and the theoretically optimal derivation are reported in Figure 6. Note that FaLPO achieves slightly worse performance compared to the other two. The reason for the slight suboptimality of FaLPO in the Merton case is twofold: i. the expected terminal utility is derived for a continuous-time policy while FaLPO learns a\n\n38\n\n2425262728CT−MB−RLFaLPODDPGRichIDSLACMethodsAverage Utility30333639CT−MB−RLFaLPODDPGRichIDSLACMethodsAverage Utility50607080CT−MB−RLFaLPODDPGRichIDSLACMethodsAverage UtilityUnder review as a conference paper at ICLR 2023\n\ndiscrete-time policy with time interval ∆t; ii. FaLPO uses an over-complicated model with stochastic factors, while the true data generating process follows a Merton model without stochastic factors.\n\nFigure 6: Negative average terminal utility of FaLPO and MMMC, and negative expected optimal terminal utility. The smaller the better.\n\nL EXTENDED RESULTS OF REAL-WORLD STOCK TRADING\n\nL.1 PROTOCOL\n\nWe consider 21-day stock trading in four different stock sectors using the daily stock price data from Yahoo finance between January 4, 2006 and April 1, 2022. More specifically, we use the adjusted close price as the daily trading price. For factors, we consider economic indexes, technical analysis indexes (generated by python package TA), and sector-specific features such as oil prices, gold prices, and related ETF prices, leading to around 30 factors for each sector. In each sector we select 10 stocks according to the availability and trading volume in the considered time range. The considered sectors, stocks, and the features are provided in Table 10. We consider the same competing methods in Section 5.1 and compare the performance using the average achieved terminal utility over different trajectories as the metric. The larger the utility the better.\n\nSectors\n\nEnergy\n\nStocks\n\nFeatures for Factors\n\nAPA, COP, CVX, HAL, HES, MRO, OKE, OXY, VLO, WMB\n\nIndustrial\n\nBA, CAT, DE, EMR, ETN, GE, HON, LMT, LUV, PNR,\n\nMaterials\n\nAPD, AVY, BLL, DD, ECL, FMC, IFF, IP, NEM, VMC\n\nSP500 returns, MACD of stock prices, RSI of stock prices, oil prices, gasoline prices US Dollar/USDX - Index - Cash (DX-Y.NYB) SP500 returns, MACD of stock prices, RSI of stock prices, ETF prices including DIA, EXI, IYJ and VIS SP500 returns, MACD of stock prices, RSI of stock prices, gold prices, silver prices, ETF prices including IYM and VAW\n\nTable 10: Selected stocks and features\n\nL.2 EXTRA PENALTIES\n\nFor real-world experiments, we consider two extra penalty terms for better stability. The first penalty is the model calibration loss discussed in Appendix E.3. Given a trajectory with time interval ∆t, τ := {ti, sti, xti | i ∈ [m]}, it is defined as\n\n−λ1 min C,b\n\nm (cid:88)\n\ni=1\n\n(cid:13) (cid:13)φ(Xti+1) − Cφ(Xti) − b(cid:13) 2\n(cid:13)\n\n2 ,\n\nwhere C is a matrix and b a vector of proper dimensions. As discussed in Appendix E.3, this penalty encourages a simple representation function φ. The second penalty is the negative sample variance of the terminal wealth, with the parameter λ2 determining its strength. The intuition of this penalty is to further penalize the instability of the algorithm performance. The second penalty is implemented for all the competing methods except MMMC for a fair comparison.\n\n39\n\n4.64.85.05.2FaLPOMMMCOptimalMethodsNegative Average/Expected Terminal UtilityUnder review as a conference paper at ICLR 2023\n\nL.3 TRAIN-VALIDATION-TEST SPLIT BY SLIDING WINDOW\n\nWe detail the sliding window method for train-validation-test split for real-world portfolio optimization experiments (Figure 7). In financial markets, the dynamics under asset prices and factors vary over time, leading us to construct a sliding window on the dataset for training, validation and testing. Specifically, given a dataset of asset prices and observed features, we construct several windows of observations of equal length. We divide each window into three contiguous periods, the first used for training, the second for validation, and the third for testing. We refer to the length (in days) of the training period as the training size (same for validation and test periods).\n\nAfter constructing one window, we move the start time point by a fixed number of days (the window gap), and construct the second window. A given method is trained on the training set of each window separately, and then validated and tested on the corresponding validation and test sets. The final validation and test performances are calculated by averaging over each window. The experimental setup is summarized in Table 11. We report the considered hyperparameter values in Table 12.\n\nFigure 7: Demonstration of sliding window.\n\nExperiment Configurations\n\nValues\n\nTime Interval ∆t Terminal Time T Utility Function Risk Aversion γ Number of Replications under Each Hyperparameter Compute Resources\n\n1 (Day) 21 Exponential 5\n10 AWS ec2 m5ad.24xlarge\n\nTable 11: Setup for real-world experiments.\n\n40\n\nTimeTrainValidationTestWindow 1Window 2Window 3Window Gap...TrainValidationTestTrainValidationTestUnder review as a conference paper at ICLR 2023\n\nHyperparameters\n\nValues\n\nBatch Size λ\nLearning Rate Window Gap Train Size Validation Size Test Size\n\n{100, 200, 400} {0.1, 0.5, 0.9} {0.0005, 0.001, 0.01, 0.1} {63, 126} {1260} {63} {63}\n\nTable 12: Hyperparameters for real-world experiments\n\nL.4 SENSITIVITY ANALYSIS ON λ\n\nAccording to (5), the value of λ determines the weight of the FaLPO model calibration. In this section, we conduct sensitivity analysis of FaLPO on λ. Under the same protocol as the experiments in Section 5.2, we also report FaLPO with different values of λ when applied to different sectors. The results are reported in Figure 8. Compared to the case without model calibration (λ = 0), a small non-zero λ provides higher terminal utilities and lower variance. This observation justifies our method of incorporating model calibration into policy learning. Then, when λ gets bigger and close to one, the performance of FaLPO decays while the variance also gets smaller.\n\n(a) Oil\n\n(b) Material\n\n(c) Industrials\n\n(d) Mix\n\nFigure 8: Sensitivity analysis for λ\n\nM MORE INFORMATION FOR COMPETING METHODS\n\nHere we provide more information for the implemented competing methods. First of all, we focus on policy learning methods, without studying other performance improving techniques like data augmentation or feature engineering. (See Appendix A.2 for a review of such methods.) Such techniques can be easily applied to FaLPO. Further, for a thorough comparison, we summarize the existing policy-learning methods for portfolio optimization with the following four representatives. Note that, all the following methods take asset price data and features as the input for a fair comparison.\n\n• DDPG is implemented with the gradient estimation detailed in Appendix C and also discussed in Nan et al. (2022); Xiong et al. (2018); Jiang et al. (2017). This design makes sure that DDPG can leverage offline data without exploration.\n\n• SLAC (Lee et al., 2020) learns a representation of factors jointly with policy learning. But\n\nin this process, no parametric models are used.\n\n• RichID (Mhammedi et al., 2020) falls into the category of model-based policy learning like Yu et al. (2019). It first learns the representation of factors and then conduct policy learning. In this process, both steps take advantage of a parametric model. For better performance in portfolio optimization, we pick Kim-Omberg model as the used model, instead of the LQR model original proposed with this method.\n\n• CT-MB-RL is a policy gradient method optimizing the performance objective using the policy functional form derived from continuous-time models, but without factor representation learning.\n\n41\n\n−10−500.000.250.500.75λAverage Utility−6−4−20.000.250.500.75λAverage Utility−20−15−10−500.000.250.500.75λAverage Utility−10−500.000.250.500.75λAverage UtilityUnder review as a conference paper at ICLR 2023\n\nWe also implement MMMC as a representative of continuous-time finance methods. More complicated and advanced continuous-time finance methods are hard to implement for two reasons. First, to implement such methods, we need to estimate all the parameters of a multivariate SDE (like σ, v, μ and ω in Section 3.3). It is challenging since the derivation of likelihood requires solving multivariate stochastic integrals (Ait-Sahalia & Kimmel, 2010), Second, deriving explicit optimal policy functions is also difficult, which involves solving high-dimensional PDEs (like k2(t) and k3(t) in Lemma F.2). Further, such pure continuous-time models are expected to underperform, since they assume that the data exactly follow a parametric SDE and tend to underfit. This can also be seen by our comparison with CT-MB-RL (Section 5), which is a model-based RL method by relying on a Kim-Omberg model. As a result, we focus our empirical comparison to more competitive RL methods.\n\nNote that FaLPO circumvents the two aforementioned challenges. First, our model calibration does not aim to fit all the parameters in an SDE, but only those related to learning θφ and θπ. That is why our model calibration loss in Section 3.3 has such an easy-to-calculate form with the parameter θS as a simple vector. Second, FaLPO does not need a fully derived closed-form solution for the optimal policy. Like in Section 3.3, we use neural networks to parameterize K(t) and φ(), instead of fully deriving them like continuous-time finance methods. Being able to bridge this gap between continuous-time finance models and high multidimensional stock trading problems is one of our contributions.\n\nN EXPERIMENTS WITH TRANSACTION COSTS\n\nIn this section, we consider the case with transaction costs. Usually, the cost of borrowing a stock to short can vary but typically ranges from 0.3% to 3% per year. Therefore, we take 1% annual transaction cost for short selling an asset. (The fees are applied daily.) Under this setting, we replicate our real-world experiments for the oil sector, using the same protocol. After the tuning procedure in Appendix L.3, the achieved results are reported in Table 13. It should be noticed that the results are consistent with those without transaction costs.\n\nMethods\n\nAverage Utility\n\nFaLPO DDPG SLAC RichID CT-MB-RL\n\n−2.25 ± 1.649 −6.795 ± 0.8247 −7.115 ± 0.8872 −6.365 ± 0.5989 −5.57 ± 5.036\n\nTable 13: Average terminal utility in oil sector with transaction costs\n\nO EXPERIMENTS WITH DIFFERENT INITIAL WEALTH\n\nWe vary the initial wealth in {3000, 5000, 8000, 1000} for portfolio optimization using stocks in the oil sector following the same protocol as the experiments in Section 5.2. The results are summarized in Table 14. Specifically, FaLPO achieves superior performance to the competing methods with different initial wealth. Also, it should be noticed that all the methods achieve higher terminal utility given more initial wealth.\n\nInitial Wealth\n\n3000\n\n5000\n\n8000\n\n10000\n\nFaLPO DDPG SLAC RichID CT-MB-RL\n\n−21.08 ± 16.775 −909.5 ± 3443.846 −11865 ± 47260.664 −6.8 ± 0.2\n\n−2.4 ± 1.9 −6.6 ± 1.2 −0.34665 ± 0.01809 0.35465 ± 0.07427\n\n−0.243 ± 0.1209\n\n−0.03595 ± 0.008896 −0.046755 ± 0.00444 −0.04558 ± 0.00142\n\n−45.51 ± 8.368 −28.715 ± 18.303\n\n−6.5 ± 0.1 −0.33125 ± 0.009791 −0.045365 ± 0.0001446 −0.043055 ± 0.002061 −4.2 ± 6.2\n\n−0.30995 ± 0.1441\n\nTable 14: Average terminal utility in oil sector with different initial wealth\n\n42",
  "translations": [
    "# Summary Of The Paper\n\nThis work proposes Factor Learning Portfolio Optimization (FaLPO), which combines tools from both machine learning and continuous-time finance. It has three learnable parts for: (i) learning factors ($\\theta_\\phi$) (ii) learning trading policy ($\\theta_\\pi$) (iii) calibrating SDE ($\\theta_S$). The role of factor learning part (i) has an influence on both the objective of (ii) and (iii), therefore these three parts can be jointly optimised. On both synthetic and real-world portfolio optimization tasks, FaLPO outperforms some existing methods.\n\n# Strength And Weaknesses\n\n* Strength\n\n(i) The work is well written and motivated, compared to a policy network that observes pricing data only, the proposed method involves factors.\n\n(ii) The factors are not manually defined, instead they are learned, within the unified objective that considers both the evolving of factor and asset (through an SDE) and the RL-based loss (through DDPG).\n\n(iii) The comparison against several baselines showed some improvements.\n\n* Weakness\n\n(i) I think the experiment design has a major flaw. The key difference here is that the proposed method can access some external data -- factors, either in raw or derived. Therefore, to make a fair comparison, the authors should keep RL part (DDPG) the same, and try to feed (i) no factors (ii) \"off-the-shelf\" factors, e.g., Fama-French-Five (iii) \"off-line\" learned factors, e.g., by fitting SDE or training NN-based model separately. By doing so, we can identify the improvement is indeed credited to the joint learning of factors.\n\n(ii) Some details on backtesting (e.g., transaction cost, as short is allowed -- the cost of borrowing assets, etc.) are missing, and it's a concern why Table 3 (Mix) got some extreme values like 10^8.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work is clear and of the fair quality, but not reproducible because of the source of data and some technical details.\n\n# Summary Of The Review\n\nOverall, I think this paper address an important (and popular) problem in finance, but the current experiment design can not deliver a convincing result.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces FaLPO (Factor Learning Portfolio Optimization), a novel approach that combines deep reinforcement learning with continuous-time finance models to optimize financial portfolios. The main contributions include a convergence proof for FaLPO, finite-sample performance guarantees, and empirical evaluations demonstrating its superior performance against five leading methods in both synthetic and real-world stock trading scenarios. The methodology leverages neural stochastic factor models to learn factors from data and incorporates model regularization into policy learning, effectively managing the complexities and dynamics of financial markets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of machine learning with continuous-time finance, addressing significant challenges in portfolio optimization, such as noisy data and complex factor relationships. The thorough theoretical analysis and empirical validation bolster the claims of effectiveness and robustness. However, the paper's reliance on the quality of features for factor generation presents a limitation, as the approach may not perform well in scenarios lacking suitable parametric models. Additionally, while the empirical results are compelling, more extensive comparisons with additional baseline methods could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulately presents its contributions, methodology, and findings. The clarity of the theoretical derivations and experimental setups contributes to the overall quality of the work. The novelty of combining deep policy gradient methods with continuous-time finance is significant, offering fresh insights into portfolio optimization. The authors emphasize reproducibility through detailed methodological descriptions, although the availability of code or data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a robust and innovative framework for portfolio optimization that effectively combines machine learning with continuous-time finance. The theoretical and empirical contributions are substantial, although some limitations regarding feature dependency and the need for more extensive empirical comparisons are noted.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents FaLPO, a novel portfolio optimization framework that integrates reinforcement learning (RL) with continuous-time finance models to effectively manage portfolios amidst stochastic factors. It addresses the limitations of existing approaches, which either prioritize flexible factor representation at the expense of asset dynamics or rely on fixed factor representations that may oversimplify real-world scenarios. The methodology includes a Neural Stochastic Factor Model for capturing complex factor effects and a Model-Regularized Policy Learning mechanism that enables optimal policy derivation. Theoretical convergence and performance guarantees are established, and empirical results demonstrate that FaLPO outperforms five leading methods in both synthetic and real-world financial tasks.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach that merges RL with continuous-time finance, effectively overcoming limitations inherent in both methodologies. It also provides robust theoretical guarantees, enhancing its credibility. Empirically, the results indicate FaLPO's superior performance across various scenarios, suggesting practical relevance. However, the paper's weaknesses include dependency on high-quality features for effective performance and potential challenges in implementing the model in scenarios where suitable parametric representations are unavailable. Additionally, the complexity of integrating RL with continuous-time finance could hinder practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulately presents its contributions, making it accessible to readers familiar with portfolio optimization and reinforcement learning. The quality of the theoretical analysis is commendable, providing a solid foundation for the proposed framework. In terms of novelty, FaLPO represents a significant advancement in the field, merging distinct methodologies in a novel way. The reproducibility of the results may be hindered by the need for high-quality input features and the inherent complexity of the model, which could pose challenges for practitioners attempting to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of portfolio optimization by successfully combining reinforcement learning with continuous-time finance models. While the theoretical foundations and empirical results are impressive, the practical implementation may face challenges due to its reliance on feature quality and model complexity.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents \"Factor Learning Portfolio Optimization\" (FaLPO), an innovative framework that integrates reinforcement learning (RL) with continuous-time finance models to enhance portfolio optimization. FaLPO addresses the limitations of existing methods by simultaneously learning stochastic factors and modeling asset dynamics, which are traditionally challenging to capture accurately. The methodology involves deep policy gradient techniques for policy learning and incorporates a neural stochastic factor model (NSFM) to reduce sample complexity. Empirical results demonstrate that FaLPO outperforms five leading portfolio optimization methods in both synthetic experiments and real-world stock trading scenarios.\n\n# Strength And Weaknesses\nStrengths of the paper include its novel integration of RL with continuous-time finance models, which addresses key limitations in existing approaches. The theoretical contributions are robust, with proven convergence and finite-sample bounds that provide performance guarantees. The empirical results are compelling, showcasing FaLPO's effectiveness across various conditions. However, the paper does have weaknesses, notably its reliance on the assumption of suitable parametric models and high-quality feature representation, which may limit applicability in diverse scenarios. Additionally, while the experiments are comprehensive, further exploration into the robustness of the model against different market conditions would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex concepts accessible. The quality of the methodology and theoretical analysis is high, with thorough explanations of the frameworks employed. The novelty is significant, as FaLPO represents a meaningful advancement in the intersection of RL and finance. The authors provide sufficient details in the appendices to enable reproducibility, including assumptions, proofs, and experimental setups, enhancing the paper's overall transparency.\n\n# Summary Of The Review\nOverall, the paper presents a strong contribution to the field of portfolio optimization by effectively combining reinforcement learning with continuous-time finance models. The theoretical and empirical results support the proposed framework, although the applicability may be constrained by assumptions related to model parameters and feature quality. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FaLPO, a novel framework that integrates deep learning methodologies with continuous-time finance models for portfolio optimization. The authors present a joint optimization approach that simultaneously calibrates financial models and determines optimal investment policies. Empirical results demonstrate significant performance improvements over traditional methods in both synthetic and real-world scenarios; however, the robustness of these findings across varying market conditions remains a concern.\n\n# Strength And Weaknesses\nThe paper boasts several strengths, including the innovative framework that merges deep learning with finance, which could potentially enhance decision-making in complex environments. The authors provide a solid theoretical foundation for their approach, claiming convergence and performance guarantees under specific assumptions. Additionally, the framework's ability to handle noise and complexity in financial data is noteworthy, particularly in the context of high-dimensional decision-making problems.\n\nHowever, the paper also has several limitations. The complexity of the FaLPO framework may pose implementation challenges for practitioners unfamiliar with both deep learning and finance. The performance improvements reported may not generalize to all market conditions, especially during crises. Furthermore, the reliance on model calibration raises concerns about potential biases in policy decisions. The empirical results, while promising, lack comprehensive details about the datasets used, which could affect the reproducibility and robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its contributions clearly. The writing quality is high, with logical progression through the theoretical and empirical sections. However, the novelty of the approach, while significant, may be overshadowed by its complexity, which could hinder practical adoption. The lack of detailed information about the datasets and experimental conditions raises concerns about reproducibility, as it is crucial for validating the empirical findings presented.\n\n# Summary Of The Review\nThe paper presents a novel and promising framework, FaLPO, for portfolio optimization that integrates deep learning with continuous-time finance models. While the performance improvements are compelling, the framework's complexity and potential limitations in generalizability warrant careful consideration for practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to portfolio optimization called Factor Learning Portfolio Optimization (FaLPO). It integrates reinforcement learning techniques with continuous-time finance models to develop optimal investment policies while navigating the complexities of unknown stochastic factors that influence asset dynamics. Key contributions include a unique interpolation methodology that merges reinforcement learning with financial modeling, a policy learning framework with dynamic regularization, and theoretical performance guarantees. Empirical results demonstrate that FaLPO surpasses existing methods in both synthetic and real-world scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of reinforcement learning with continuous-time finance, providing a flexible framework that allows for the automatic learning of complex factor dynamics through neural stochastic representations. The introduction of a joint optimization paradigm enhances robustness and sample efficiency, addressing significant limitations in traditional models. However, a notable weakness is the potential reliance on accurate feature sets, which could limit the approach's applicability in highly volatile or unpredictable markets. Additionally, the paper could benefit from a more detailed exploration of the implications of its findings for real-world financial decision-making.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and experimental validation. The quality of the writing is high, making complex concepts accessible. The novelty of the approach is significant, as it bridges two traditionally separate fields—reinforcement learning and financial modeling. Reproducibility is supported by extensive experimental validation against multiple competing methodologies, though the authors could enhance reproducibility by providing additional details on the experimental setup and parameter tuning.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of financial portfolio optimization by introducing the FaLPO framework, which effectively combines reinforcement learning and continuous-time finance. The empirical results are compelling, demonstrating the framework's superior performance and flexibility. While the paper is well-written and presents a novel approach, attention to the limitations and potential real-world implications could further strengthen its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to adversarial training specifically designed for financial portfolio optimization, addressing the inherent uncertainties in asset markets. The authors introduce a framework that combines adversarial learning concepts with traditional optimization methodologies to enhance the robustness of investment strategies against market volatility and adversarial perturbations. Key contributions include the introduction of adversarial perturbations during training, the integration of continuous-time finance principles for realistic asset modeling, proven performance guarantees, and empirical validation through extensive experiments on both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of adversarial training with financial portfolio optimization, which is relatively unexplored in existing literature. The mathematical proofs provided for convergence and performance bounds lend credibility to the approach. However, the paper also presents weaknesses, such as potential computational inefficiencies due to the complexity of adversarial training, which may limit scalability for large datasets. Additionally, the reliance on the quality of adversarial examples raises concerns about the effectiveness of the training process if perturbation strategies are poorly designed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, making it accessible to readers familiar with both finance and machine learning. The quality of writing is high, with comprehensive explanations of the proposed techniques and their implications. The novelty of integrating adversarial training into financial contexts is significant, offering fresh insights into portfolio optimization. However, reproducibility may be an issue if the authors do not provide sufficient details on the adversarial example generation process and the specific datasets used in experiments.\n\n# Summary Of The Review\nOverall, this paper introduces an important advancement at the intersection of adversarial training and financial portfolio optimization, demonstrating enhanced resilience of investment strategies in uncertain environments. While the contributions are substantial and empirically supported, the potential computational challenges and reliance on adversarial example quality warrant further exploration in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FaLPO, a novel framework that integrates reinforcement learning with continuous-time finance models to enhance financial portfolio optimization. It claims to address significant challenges in portfolio management, such as data noise and complex factor relationships, by utilizing a neural stochastic factor model. The authors assert that FaLPO achieves optimal performance guarantees and demonstrates a substantial improvement over existing methods in both synthetic and real-world tasks, suggesting its broad applicability beyond finance.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative combination of techniques from reinforcement learning and continuous-time finance, which provides a fresh perspective on portfolio optimization. The theoretical contributions, particularly the claim of achieving asymptotic optimal performance and finite-sample bounds, represent significant advancements in the field. However, the paper seems to downplay potential limitations and challenges in practical application, which could impact the robustness of its claims. Additionally, while empirical results are compelling, the extent of generalizability to other domains beyond finance requires further validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings of FaLPO. The writing quality is high, with a logical flow that guides the reader through complex concepts. The novelty of integrating reinforcement learning with continuous-time finance is substantial, although the reproducibility of the results would benefit from more comprehensive details regarding the experimental setup and parameter choices.\n\n# Summary Of The Review\nFaLPO presents a promising advancement in financial portfolio optimization by merging reinforcement learning with continuous-time finance models. While the theoretical and empirical contributions are notable, the paper could improve on addressing practical limitations and enhancing reproducibility. Overall, it sets a high standard for future research in this domain.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to portfolio optimization called Factor Learning Portfolio Optimization (FaLPO), which integrates reinforcement learning (RL) with continuous-time finance models. The methodology employs a neural stochastic factor model to manage noise in financial data and incorporates model-regularized policy learning to enhance performance. The findings demonstrate that FaLPO significantly outperforms five leading portfolio optimization methods in both synthetic and real-world stock trading scenarios, showing improvements in terminal utility and resilience to transaction costs.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative combination of RL and continuous-time finance, which addresses common challenges such as high sample complexity and overfitting. The theoretical guarantees provided for convergence and finite-sample performance enhance the credibility of the approach. However, a notable weakness is the sensitivity of the performance to feature selection and model calibration, which could limit the method's robustness in unpredictable market conditions. Additionally, the paper may benefit from a more comprehensive exploration of the impact of different feature sets on performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly and is well-structured, making it easy to follow the proposed methodology and its implications. The quality of the experimental results is high, with thorough comparisons against established methods. In terms of novelty, FaLPO presents a significant advancement in portfolio optimization techniques by leveraging deep learning in a continuous-time framework. However, while the experiments are promising, reproducibility could be enhanced by providing more details about the datasets and experimental setup used.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new approach to portfolio optimization that effectively combines reinforcement learning and continuous-time models, yielding impressive empirical results. Despite some limitations regarding feature sensitivity and reproducibility, FaLPO represents a significant contribution to the field of financial modeling.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework, FaLPO (Financial Asset Learning for Portfolio Optimization), designed to optimize asset allocation through a combination of continuous-time financial models and discrete-time policy learning. It employs stochastic modeling to account for financial factors affecting asset prices and integrates regularization techniques to enhance learning robustness. The findings suggest promising performance in both synthetic and real-world data scenarios, although the empirical validation is limited by the representativeness of the data used.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to blending continuous and discrete frameworks, potentially improving portfolio optimization strategies. However, it has notable weaknesses including an overreliance on stochastic assumptions that may overlook significant market dynamics such as structural breaks. Additionally, the heavy dependence on model calibration raises concerns regarding the risk of overfitting, and the lack of rigorous empirical validation limits the generalizability of its findings across diverse market conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing is generally clear, but certain technical aspects, particularly regarding the hybrid methodology, could benefit from further elaboration. The novelty of combining continuous-time finance with discrete policy learning offers a fresh perspective, yet the paper falls short of providing sufficient reproducibility due to the subjective nature of hyperparameter tuning and the lack of comprehensive validation across various scenarios.\n\n# Summary Of The Review\nOverall, while the paper introduces an innovative framework for portfolio optimization with promising initial results, it is marred by significant assumptions and limitations in empirical validation that could hinder its practical applicability. Greater attention to model robustness and real-world conditions is necessary for the framework to be considered a viable solution in financial decision-making contexts.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents FaLPO (Factor Learning Portfolio Optimization), a novel framework for financial portfolio optimization that merges reinforcement learning with continuous-time finance models. FaLPO employs a neural stochastic factor model to capture the dynamics of financial data while addressing the challenges posed by noise and complex factor interdependencies. The methodology includes a model-regularized policy learning approach, which combines discrete-time reinforcement learning with continuous-time finance techniques, resulting in significant improvements in policy outcomes. Empirical results demonstrate that FaLPO outperforms five existing methods on both synthetic and real-world datasets, showcasing its robustness and adaptability.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative integration of reinforcement learning with continuous-time finance, which effectively addresses existing limitations in portfolio optimization methods. The rigorous theoretical analysis supports the proposed framework, providing convergence guarantees and finite-sample bounds for expected utility. However, the paper also has weaknesses, such as limited discussion on the assumptions underlying the model and the quality of input features, which may affect generalizability. Additionally, while the empirical results are compelling, more extensive testing across varied market conditions would further strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The quality of the writing is high, with a logical flow that guides the reader through complex concepts. The novelty of the approach is significant, as it introduces a unique combination of techniques in a well-defined context. However, the reproducibility of the results could be enhanced by providing more details on the implementation and datasets used, as well as potential hyperparameter settings.\n\n# Summary Of The Review\nOverall, the paper presents a strong contribution to the field of financial portfolio optimization by introducing FaLPO, which effectively combines reinforcement learning with continuous-time models. While the theoretical and empirical results are promising, further clarification on model assumptions and implementation details would enhance the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for addressing the challenges of model interpretability in deep learning. The authors propose a hybrid method that combines gradient-based techniques with rule-based systems to enhance the transparency of model predictions. Experimental results demonstrate that the proposed approach not only improves interpretability but also maintains competitive predictive performance across various datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of two established methodologies, which provides a fresh perspective on the interpretability of complex models. The empirical results are compelling, showing that the proposed technique can effectively bridge the gap between accuracy and interpretability, a significant challenge in the field. However, the paper lacks a thorough discussion of the limitations of the proposed framework, particularly in terms of its scalability and potential biases introduced by rule-based systems. Additionally, more detailed comparisons with existing interpretability methods could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a logical progression from problem formulation to methodology and results. Overall, the writing is clear, though some technical sections could benefit from further elaboration to enhance accessibility. The novelty of the approach is commendable, as it effectively combines different paradigms in a unique way. However, reproducibility is somewhat hampered by the lack of detailed descriptions of the experimental setup, including the datasets and evaluation metrics used.\n\n# Summary Of The Review\nThis paper offers a significant contribution to the field of model interpretability by presenting a novel hybrid framework that effectively combines gradient-based and rule-based methods. While the results are promising and the approach is innovative, improvements in clarity, detailed comparisons with existing methods, and transparency regarding limitations are necessary to enhance the overall impact of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces FaLPO (Factor Learning Portfolio Optimization), a novel approach that integrates deep reinforcement learning with continuous-time finance models to address the challenges of portfolio optimization in the presence of stochastic factors. The methodology combines deep policy gradient techniques for effective policy learning with a flexible factor representation derived from continuous-time finance models. The findings indicate that FaLPO outperforms existing methods on various portfolio optimization tasks, demonstrating its potential to enhance decision-making in financial contexts characterized by noisy data and complex relationships between factors and asset prices.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative combination of deep learning techniques and continuous-time finance, aiming to overcome the limitations of existing methods that either lack accurate dynamics modeling or flexible data representation. The empirical results provide a compelling case for FaLPO's effectiveness in portfolio optimization. However, the approach's reliance on suitable parametric models presents a notable weakness, as the quality of the features used to generate factors significantly impacts performance. Furthermore, the framework's adaptability to other decision-making problems, while promising, requires further exploration and validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The writing quality is high, with a logical flow that facilitates understanding of the complex concepts involved. The novelty of the approach, which bridges reinforcement learning and continuous-time finance, is significant and adds to the literature on portfolio optimization. However, the reproducibility of the results may depend on the accessibility of the implementation details and the specific datasets used, which should be transparently shared for broader validation.\n\n# Summary Of The Review\nOverall, the paper presents a well-conceived approach to portfolio optimization that effectively integrates machine learning with established financial modeling techniques. While the empirical results are promising, the reliance on specific parametric models and the need for robust feature engineering remain areas for improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents the FaLPO (Factor Learning Portfolio Optimization) framework, which integrates reinforcement learning (RL) with continuous-time finance models to address challenges in portfolio optimization involving stochastic factors. The methodology includes neural stochastic factor models to reduce noise and capture complex relationships, as well as model-regularized policy learning that combines both discrete and continuous-time frameworks. The authors provide theoretical guarantees for the convergence of their approach and demonstrate through empirical experiments that FaLPO significantly outperforms several existing methods in both synthetic and real-world stock trading scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative combination of RL and continuous-time finance models, which addresses the limitations of existing methods that either oversimplify factor dynamics or fail to leverage sophisticated modeling techniques. The theoretical contributions, including convergence proofs and performance guarantees, add robustness to the proposed framework. However, the paper's applicability is limited by the reliance on suitable parametric models and the quality of features used, which may not be universally available. Additionally, while the empirical results are promising, further exploration of the framework's limitations in diverse financial environments is warranted.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its problem statement, methodology, and findings. The quality of writing is high, facilitating comprehension of complex concepts. The novelty of the approach is significant, as it bridges gaps between RL and finance, although the underlying assumptions about factor dynamics may limit generalizability. Reproducibility is supported by detailed descriptions of methodologies and experiments, but the dependence on specific financial models may pose challenges for practitioners seeking to apply the framework in different contexts.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of portfolio optimization by effectively integrating RL with continuous-time finance models. While the theoretical and empirical results are robust, the framework's applicability may be constrained by model assumptions and feature quality. Further exploration of these limitations in future work is recommended.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to financial portfolio optimization called FaLPO, which integrates reinforcement learning (RL) with continuous-time finance models. The main contributions include the development of neural stochastic factor models for flexible factor representation, model-regularized policy learning, and the establishment of convergence proofs along with finite-sample performance guarantees. Empirical results indicate that FaLPO outperforms five leading methods in both synthetic and real-world stock trading scenarios, showcasing its effectiveness in maximizing expected utility while managing risk.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of RL with continuous-time finance, providing a comprehensive framework that addresses the complexities of financial data and factor relationships. The theoretical analysis is robust, featuring both asymptotic and non-asymptotic performance guarantees. However, the paper does have weaknesses, particularly regarding the generalizability of FaLPO, which may be limited when suitable parametric models are not available. Additionally, the reliance on high-quality input features for factor generation could constrain its applicability in diverse settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its objectives, methodology, and findings, making it easy to follow. The literature review adequately identifies gaps in existing research, highlighting the significance of the proposed approach. The methodological rigor is commendable, with clear explanations of the theoretical foundations and empirical validations. The empirical results are reproducible, as detailed methodology and experiments are included in the appendices, though the dependence on specific input features may affect reproducibility in different contexts.\n\n# Summary Of The Review\nOverall, the paper offers a significant advancement in portfolio optimization by effectively combining machine learning and finance. Its theoretical insights and empirical validations provide a strong foundation for future research, though attention should be given to its limitations regarding generalizability and feature dependence.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Factor Learning Portfolio Optimization (FaLPO) framework, which merges reinforcement learning (RL) techniques with continuous-time financial models for effective portfolio optimization under stochastic factors. The methodology utilizes deep policy gradient techniques and neural stochastic factor models to parameterize investment policies, addressing the complexities introduced by stochastic factors in financial data. Empirical findings demonstrate that FaLPO outperforms existing benchmarks, such as DDPG and SLAC, in both synthetic and real-world stock trading scenarios, with theoretical convergence and finite-sample performance guarantees supporting its efficacy.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative integration of deep learning with continuous-time finance, which is a significant advancement in portfolio optimization methodologies. The convergence proofs and performance bounds offer a solid theoretical foundation, enhancing the credibility of the proposed framework. However, the reliance on the existence of informative features for factor generation may limit the applicability of FaLPO in scenarios where such features are not readily available. Additionally, while the paper addresses overfitting concerns, further empirical validation in diverse market conditions could strengthen its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of writing is high, making complex concepts accessible to the reader. The novelty of the approach, especially in its combination of RL and financial modeling, is noteworthy. Reproducibility is facilitated by the clear exposition of methodologies and theoretical guarantees, although the implementation details could be expanded to aid practitioners who wish to apply FaLPO in real-world settings.\n\n# Summary Of The Review\nOverall, the paper provides a compelling contribution to the field of portfolio optimization by successfully integrating deep learning techniques with traditional financial models. The theoretical foundations and empirical validations bolster its significance, although further exploration of its limitations would enhance the discussion.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Factor Learning and Portfolio Optimization (FaLPO) framework, which integrates reinforcement learning with continuous-time finance models to optimize portfolio strategies. The authors claim that FaLPO outperforms five leading methods in terms of performance. However, the methodology heavily relies on a parametric continuous-time finance model, raising concerns about its applicability in real-world settings. The paper presents theoretical guarantees for convergence but lacks rigorous proof, and empirical results do not convincingly substantiate the claims of superiority over existing methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative attempt to combine reinforcement learning with continuous-time finance, which is a relevant area of research. However, significant weaknesses are present. The reliance on a restrictive parametric model may limit applicability, and the empirical results provided are not robust enough to support the authors' claims of outperforming leading methods. Furthermore, the lack of a thorough discussion on the limitations of continuous-time models and the implications of transaction costs undermines the practical utility of the proposed framework. Additionally, the sensitivity analysis fails to demonstrate the model's robustness across different scenarios, raising questions about its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate; while the main ideas are articulated, the complex integration of models may confuse readers unfamiliar with the underlying concepts. The quality of the writing is generally good, but the lack of rigorous proofs and comprehensive discussions on limitations detracts from the overall impression. In terms of novelty, while the integration of existing methods is noteworthy, it does not represent a groundbreaking advancement in portfolio optimization. Reproducibility may be an issue due to insufficient detail regarding the implementation and computational costs associated with the method.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to portfolio optimization through reinforcement learning and continuous-time finance models, but it suffers from several limitations that hinder its effectiveness and applicability. The empirical results lack sufficient rigor to support the claims made, and the reliance on a restrictive model and insufficient exploration of practical constraints further detracts from its contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents the FaLPO (Factor Learning Portfolio Optimization) framework, which integrates machine learning techniques with continuous-time finance models, offering a novel approach to portfolio optimization. The authors demonstrate the framework's robustness through empirical results, showing that FaLPO outperforms five leading methods in both synthetic and real-world scenarios. Additionally, the framework's flexibility allows it to address various decision-making problems involving stochastic factors, while the theoretical proof of convergence provides reliability for practical applications.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative framework that effectively combines machine learning with finance, resulting in superior performance in portfolio optimization tasks. The robust learning mechanism utilizing deep policy gradient methods enhances its real-world applicability. However, while the paper offers promising results, it may benefit from further exploration of the limitations of the framework in different market conditions or with varying types of data noise. Additionally, the paper could enhance its discussion on potential drawbacks or challenges in the implementation of FaLPO.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The quality of the writing is high, with coherent explanations and logical progression throughout the paper. The novelty of the FaLPO framework is significant, as it provides a fresh perspective on portfolio optimization. However, details regarding the reproducibility of the experiments could be more explicitly stated, especially concerning the datasets and implementation specifics used in the evaluation of the framework's performance.\n\n# Summary Of The Review\nOverall, the paper introduces a groundbreaking framework for portfolio optimization that combines advanced machine learning techniques with finance, demonstrating impressive empirical results. While the contributions are significant and the framework shows great promise for future applications, there are areas for improvement in discussing limitations and enhancing reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents Factor Learning Portfolio Optimization (FaLPO), a novel framework that combines reinforcement learning with continuous-time finance models to enhance portfolio optimization. The methodology employs stochastic factor models and neural stochastic factor models (NSFM) to derive optimal investment policies while ensuring theoretical constraints that align with established financial theories. Key findings include theoretical convergence guarantees and an assertion that the performance of the FaLPO method approaches that of the optimal policy as trading frequency increases, providing a strong theoretical foundation for its implementation in practical settings.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its robust theoretical framework that integrates advanced mathematical concepts with practical applications in portfolio optimization. The use of stochastic differential equations and the introduction of NSFM provide significant flexibility in modeling complex market behaviors. However, the paper's primary weakness is its limited empirical validation, as the focus remains heavily on theoretical constructs rather than practical demonstrations of the model's effectiveness in real-world scenarios. Additionally, while the theoretical results are compelling, the lack of extensive empirical evidence may hinder the immediate applicability of the framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical contributions, making it accessible to readers familiar with both reinforcement learning and financial optimization. The quality of the theoretical analysis is high, providing rigorous mathematical proofs and convergence results. In terms of novelty, the integration of neural networks into the stochastic model is a commendable advancement that enhances the flexibility of factor representation. However, reproducibility may be a concern due to the limited empirical examples and reliance on theoretical validation, which may not directly translate to practical application without further empirical studies.\n\n# Summary Of The Review\nOverall, FaLPO presents a theoretically sound approach to portfolio optimization, merging reinforcement learning with continuous-time finance. While the theoretical contributions are significant and well-articulated, the paper would benefit from more empirical validation to support its practical applicability and effectiveness in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces FaLPO (Factor Learning Portfolio Optimization), a novel framework that integrates neural stochastic factor models with continuous-time finance models to enhance portfolio optimization. The proposed methodology employs a neural network to parameterize the policy function and a representation function, allowing for model-regularized policy learning. The authors demonstrate that FaLPO effectively outperforms existing portfolio optimization methods (DDPG, SLAC, RichID, CT-MB-RL, MMMC) in both synthetic and real-world data experiments, with performance evaluated based on average terminal utility.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to combining machine learning with continuous-time finance models, which addresses a significant gap in the literature. The use of synthetic and real-world data to validate the framework enhances its credibility. However, a notable weakness is the reliance on the quality of features for factor generation, as this can limit the generalizability of the model. Additionally, the paper lacks explicit details on code availability, which may hinder reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, making it accessible to readers familiar with the domain. The technical quality appears high, with a solid foundation in both finance and machine learning principles. However, the lack of code availability is a drawback for reproducibility, despite the authors providing detailed algorithmic steps and hyperparameter settings.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to portfolio optimization that leverages neural networks and continuous-time finance models. While the empirical results are promising, concerns regarding feature quality and code availability may impact its broader applicability and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FaLPO, a framework that aims to integrate reinforcement learning (RL) with continuous-time finance models for portfolio optimization. The authors claim that FaLPO reduces overfitting through model calibration and can handle high noise and complex effects in financial data. They assert that their method outperforms five leading techniques, while also suggesting that it can be extended to other decision-making problems beyond finance. However, the paper lacks a detailed comparison with state-of-the-art approaches, raising questions about the robustness of its claims.\n\n# Strength And Weaknesses\nThe primary strength of FaLPO lies in its attempt to marry RL with continuous-time finance models, which could potentially offer a new perspective on portfolio optimization. However, the paper does not adequately address the high sample complexity issue prevalent in current RL methods, nor does it convincingly demonstrate its advantages over established techniques like DDPG and CT-MB-RL. Furthermore, while the authors provide theoretical guarantees, the empirical results and comparisons with existing models lack the necessary rigor. The broad claims about applicability to other decision-making problems are also not well-supported.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat undermined by vague empirical comparisons and a lack of statistical rigor, which diminishes the overall quality of the findings. In terms of novelty, while FaLPO presents a unique integration of concepts, it does not sufficiently differentiate itself from existing methodologies, making its contributions appear less significant. Reproducibility is not adequately addressed, as the empirical results are not detailed enough to allow for independent validation.\n\n# Summary Of The Review\nWhile FaLPO presents an intriguing framework for integrating RL and continuous-time finance models, it ultimately fails to provide convincing evidence of its superiority over existing approaches. The paper's contributions, while interesting, are overshadowed by a lack of robust comparisons and clarity in empirical results.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Factor Learning Portfolio Optimization Informed by Continuous-Time Finance Models\" presents a novel approach to portfolio optimization that integrates factor learning with continuous-time finance models. The authors propose a methodology that combines machine learning techniques with financial theories to enhance the decision-making process in asset allocation. The findings demonstrate improvements in portfolio performance compared to traditional optimization methods, highlighting the effectiveness of the proposed approach in managing financial risk and returns.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its innovative integration of factor learning with continuous-time models, which is a relatively unexplored area in portfolio optimization. Additionally, the empirical results substantiate the claims made, showcasing a significant performance boost in various market conditions. However, the paper suffers from several weaknesses, including inconsistent formatting, minor typographical errors, and a lack of clarity in some definitions and parameter notations, which could hinder understanding for readers unfamiliar with the underlying concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by various formatting issues and typographical errors, detracting from the overall quality. While the novel approach to combining factor learning and finance models is commendable, the lack of thorough explanation for certain concepts and parameters may challenge reproducibility. The empirical results are presented clearly, but further elaboration on the methodology and its implications would enhance comprehension.\n\n# Summary Of The Review\nOverall, the paper offers an intriguing and novel approach to portfolio optimization, but its clarity and presentation need significant improvement. Addressing the formatting inconsistencies and providing clearer definitions and explanations would greatly enhance the paper's accessibility and impact.\n\n# Correctness\n4/5 - The methodology and findings are generally correct, but minor typographical errors and unclear definitions could lead to misinterpretation.\n\n# Technical Novelty And Significance\n4/5 - The integration of factor learning with continuous-time finance models presents a novel contribution to the field, although further exploration of existing literature could enhance its significance.\n\n# Empirical Novelty And Significance\n3/5 - The empirical results are significant, but the contribution could be augmented with additional experiments and validation across diverse market scenarios to strengthen the claims made.",
    "# Summary Of The Paper\nThe paper presents the Factor-augmented Learning for Portfolio Optimization (FaLPO) framework, which integrates machine learning techniques to optimize portfolio management. The authors employ a neural network architecture to represent and utilize factors affecting asset prices, aiming to improve investment decisions in various market conditions. The findings suggest that FaLPO outperforms traditional portfolio optimization methods, but the authors acknowledge limitations in addressing external macroeconomic factors and transaction costs.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to portfolio optimization through the use of machine learning, which could enhance performance in diverse market scenarios. However, the paper has several weaknesses, including a lack of detailed discussion on transaction costs, the robustness of the model against market anomalies, and a limited exploration of the model's adaptability to real-time data. Additionally, a comparative analysis with more advanced methods could bolster the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear methodology and results. However, some sections could benefit from greater depth, particularly regarding the implications of the model's assumptions and limitations. The novelty of the approach is commendable, though the reproducibility may be impacted by the narrow scope of asset classes considered and the specific neural network architecture employed.\n\n# Summary Of The Review\nOverall, the paper introduces a promising machine learning approach to portfolio optimization, demonstrating potential advantages over traditional methods. Nevertheless, it could be strengthened by addressing its limitations and exploring broader applications and comparisons with existing methodologies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents FaLPO, a novel framework that integrates reinforcement learning with continuous-time finance models for portfolio optimization in the presence of stochastic factors. The main contributions include rigorous convergence proofs and finite-sample performance guarantees, which provide a theoretical foundation for the effectiveness of the FaLPO methodology. Through a combination of synthetic and real-world experiments, the authors demonstrate that FaLPO consistently outperforms five competing methods, suggesting its robustness across varying parameters, although explicit statistical validation measures are sparse.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its thorough theoretical underpinnings and the innovative combination of reinforcement learning with finance models. The convergence proofs and finite-sample analysis are particularly noteworthy, providing a clear rationale for the claimed performance improvements. However, the paper has weaknesses, particularly in its empirical validation. While the authors present strong results, the lack of explicit statistical significance testing (e.g., p-values, confidence intervals) raises questions about the robustness of the findings. Additionally, the reliance on standard deviations without further statistical tests leaves the conclusions somewhat unsupported.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-structured and clearly written, making it accessible to readers familiar with the subject matter. The quality of the theoretical contributions is high, though the empirical results would benefit from more rigorous statistical analysis to enhance replicability. The novelty of combining reinforcement learning with continuous-time finance is significant, although the empirical validation could be improved to ensure reproducibility of results across different settings.\n\n# Summary Of The Review\nIn summary, the paper presents a compelling framework for portfolio optimization that is underpinned by strong theoretical contributions. However, the lack of rigorous statistical validation in the empirical results somewhat diminishes the overall impact of the findings. Future work addressing these limitations could enhance the robustness and applicability of the proposed methodology.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces FaLPO (Financial Learning Policy Optimization), a novel framework designed for portfolio optimization in financial settings. It employs a continuous-time finance model to derive an optimal policy functional form, leveraging features extracted from market data. The main findings suggest that FaLPO provides theoretical guarantees for optimization under certain conditions, demonstrating promising results on selected synthetic and real-world datasets. However, the authors recognize several limitations regarding the model's applicability and robustness across diverse financial contexts.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to integrating continuous-time models into portfolio optimization, which can offer valuable insights for decision-making in finance. Additionally, the theoretical underpinnings provide a solid foundation for further exploration in this domain. However, the paper also has significant weaknesses, including limited applicability in scenarios lacking suitable parametric models, overfitting risks due to market noise, and insufficient guidance on hyperparameter selection. The complexity of the proposed model can also hinder practical implementation, and the focus on specific datasets raises concerns regarding generalizability to broader financial contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates the methodology and findings. Nevertheless, the reproducibility of the results is a concern, as the authors do not provide enough detail on implementation or parameter settings used in their experiments. While the technical novelty is notable, especially in the context of continuous-time models, the empirical novelty is less robust due to the reliance on limited datasets, which may not represent a comprehensive view of financial markets.\n\n# Summary Of The Review\nOverall, the paper presents a noteworthy contribution to portfolio optimization through FaLPO, emphasizing theoretical advancements. However, its limitations in applicability and reproducibility, along with generalizability issues, suggest that further work is needed to enhance its practical utility and empirical validation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Factor Learning Portfolio Optimization Informed by Continuous-Time Finance Models\" presents a framework called FaLPO, which integrates reinforcement learning with continuous-time finance models for portfolio optimization. The authors aim to address the challenges of noisy financial data while maximizing expected utility and managing risk. They provide theoretical guarantees for convergence and demonstrate the efficacy of their method through experiments on real-world financial data, claiming to outperform existing approaches.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to merge two established methodologies—reinforcement learning and continuous-time finance—into a single framework. However, this integration does not present significant novelty, as the concepts involved are well-known in the literature. The paper lacks depth in discussing the specific challenges associated with applying these methods to finance and seems to overlook the nuanced understanding required in real-world applications. The empirical results, while promising, do not sufficiently establish the uniqueness of the contribution, as the claimed improvements could be attributed to minor adjustments in existing techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is adequate, though the presentation of ideas often feels convoluted and overstated. The authors utilize standard terminologies and methods without sufficient context, which may confuse readers unfamiliar with the nuances of finance or reinforcement learning. The novelty of the paper is questionable, as it primarily rehashes existing concepts under the guise of a new framework. Reproducibility is not thoroughly addressed, and the experimental setup lacks sufficient detail to allow for independent validation of results.\n\n# Summary Of The Review\nIn summary, the paper attempts to contribute to the field of portfolio optimization by integrating established techniques from reinforcement learning and continuous-time finance. However, the lack of true novelty and insufficient depth in addressing key challenges significantly detracts from its overall impact. The authors' claims of advancement appear overstated, and while the empirical results are positive, they do not convincingly establish a meaningful innovation.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach, called FaLPO (Financial Portfolio Optimization), which integrates continuous-time finance models with deep reinforcement learning to optimize investment portfolios. The authors propose a methodology that leverages RL techniques to adaptively manage asset allocations based on market conditions. Key findings indicate that FaLPO outperforms traditional portfolio optimization methods in various simulated environments, demonstrating improved returns and risk management. However, the paper acknowledges limitations such as sensitivity to hyperparameters and the need for more robust data handling techniques.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of deep learning with financial modeling, providing a modern framework for portfolio optimization. The empirical results are promising, showcasing the potential of FaLPO in enhancing investment strategies. However, weaknesses include a lack of exploration into alternative methods for interpretability, limited hyperparameter tuning processes, and an absence of longitudinal analyses that could validate the model's robustness across different market conditions. The comparison with state-of-the-art methods could also be more comprehensive, potentially undermining the claimed advancements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, with a logical flow of information. The quality of the methodology is solid, but some sections could benefit from more detailed explanations, particularly regarding the implementation of hyperparameter tuning and the real-world applicability of the model. While the novelty of the approach is commendable, reproducibility may be hindered by the reliance on manual parameter adjustments rather than a systematic optimization strategy.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling framework for portfolio optimization that combines finance and deep reinforcement learning. While the contributions are significant, the work would benefit from addressing limitations in hyperparameter tuning, data handling, and comparisons with more recent methodologies. Enhancements in these areas could bolster the paper's impact and applicability in real-world trading scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel method called FaLPO (Flexible Asset and Liability Portfolio Optimization) aimed at enhancing portfolio optimization in both synthetic and real-world trading scenarios. The methodology combines continuous-time finance models with a flexible factor representation, allowing for effective utility maximization under various market conditions. The findings reveal that FaLPO consistently outperforms five competing methods (DDPG, SLAC, RichID, CT-MB-RL, and MMMC) in achieving higher terminal utility across diverse settings, demonstrating its robustness and adaptability in both synthetic environments and real-world stock trading.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to portfolio optimization and its strong empirical validation. FaLPO's performance metrics indicate superior utility maximization, particularly in high-volatility scenarios and varying risk aversion conditions, which underline its practicality for real-world applications. Additionally, the method shows low variance in results across multiple trials, highlighting its stability and reliability. However, while the paper thoroughly evaluates FaLPO against existing methods, it could benefit from more extensive discussion on potential limitations and a deeper analysis of the underlying theoretical framework that supports its methodology.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with thorough explanations of the methodology and experimental results. The quality of the writing is high, making complex concepts accessible. The novelty of FaLPO is evident in its unique combination of approaches to portfolio optimization, and the empirical results are reproducible as they are based on established benchmarks and comprehensive testing across different market conditions. However, further details on the implementation of the method would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of portfolio optimization through the introduction of FaLPO, which demonstrates superior performance in both synthetic and real-world contexts. The method's robustness and stability make it a valuable contribution, though further exploration of its limitations would strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a novel approach to reinforcement learning in continuous-time finance, introducing a method termed FaLPO (Financial Learning via Policy Optimization). The authors utilize a combination of stochastic modeling and reinforcement learning techniques to address challenges in model calibration and decision-making under uncertainty. The findings demonstrate that FaLPO outperforms existing benchmarks in terms of predictive accuracy and decision efficiency in financial applications.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of reinforcement learning with financial modeling, which addresses a significant gap in the literature. The empirical results provide compelling evidence of the method's effectiveness. However, the paper suffers from a lack of clarity in its presentation; the dense language and lengthy sentences may hinder reader comprehension. Additionally, the inconsistent use of terminology and technical jargon could alienate less experienced readers.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the contributions of the paper are noteworthy, the clarity of the writing needs improvement. The introduction lacks a coherent structure, and the technical jargon is not consistently defined. The methodology is sound, but the mathematical notation requires clearer definitions upon first use to enhance reproducibility. Overall, the quality of the writing detracts from the otherwise significant novelty of the research.\n\n# Summary Of The Review\nThis paper introduces a promising method for financial decision-making through reinforcement learning, but suffers from clarity and presentation issues. Improvements in writing style and structure would greatly enhance the paper's accessibility and impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.5150820157188742,
    -1.952049927208969,
    -1.8422678941692499,
    -1.7904691685455176,
    -1.7247470436563812,
    -1.7939303276491931,
    -1.7033410232555095,
    -1.7901792283814468,
    -1.809961738647622,
    -1.9786786800308773,
    -1.7420609355126864,
    -1.435780137090047,
    -1.7927652860609558,
    -1.7006910419211072,
    -1.950468568551368,
    -1.8009922682766706,
    -1.7534850727032572,
    -1.7764961263239796,
    -1.7851585228831894,
    -1.9466534682449474,
    -1.8678960544144485,
    -1.649557878400023,
    -1.7320112374096466,
    -1.6733324104813327,
    -1.8621401392337609,
    -1.8825470067840484,
    -1.7304808713194584,
    -1.8873621255511446,
    -1.8712341105361658
  ],
  "logp_cond": [
    [
      0.0,
      -2.2542432757375104,
      -2.2768946586102,
      -2.282376202733058,
      -2.25444465352295,
      -2.2764125507730575,
      -2.3388181954731717,
      -2.2648289953234424,
      -2.2804526736437727,
      -2.2809640204426906,
      -2.2571796661771923,
      -2.387645102202353,
      -2.250444651509961,
      -2.279377636364497,
      -2.2781004538617298,
      -2.258098928973714,
      -2.283926897207915,
      -2.265898529790769,
      -2.2714550933574627,
      -2.2416230362316,
      -2.2704289264292745,
      -2.3082989121870825,
      -2.3132696567162663,
      -2.28726775912201,
      -2.284501664051617,
      -2.2664192448185183,
      -2.292233646748903,
      -2.280013254548511,
      -2.3171380095733567
    ],
    [
      -1.482483848066654,
      0.0,
      -1.2702433067115795,
      -1.2434734576848332,
      -1.4365364087507875,
      -1.4012660547419482,
      -1.5220669221086025,
      -1.3544827379753117,
      -1.2956456556786258,
      -1.4653965110224811,
      -1.2678889312466344,
      -1.5903001437449422,
      -1.2263548603139205,
      -1.254787682807623,
      -1.0998218169487477,
      -1.2472533956794738,
      -1.4247135115065346,
      -1.3673905544540632,
      -1.4033070682876643,
      -1.263322919024354,
      -1.4254630125178418,
      -1.5283356899526506,
      -1.5343776813295948,
      -1.3713621317715186,
      -1.4294251705346077,
      -1.4240820303266604,
      -1.4524244358489413,
      -1.4581122241227586,
      -1.5090804614128102
    ],
    [
      -1.4410010140870109,
      -1.3061893462137764,
      0.0,
      -1.2861297683641602,
      -1.4646768208330212,
      -1.4095452656641707,
      -1.5209030593054085,
      -1.4123098601931483,
      -1.368582929079408,
      -1.4841365158836453,
      -1.3003755581584167,
      -1.5878644180702641,
      -1.348666379753827,
      -1.2430437636155585,
      -1.229922271124087,
      -1.3594964240380052,
      -1.3828383964938635,
      -1.4252039880898908,
      -1.3869392402503504,
      -1.3479346582951308,
      -1.4159266736911416,
      -1.5249209195279732,
      -1.5094501852508178,
      -1.3574487174661618,
      -1.4416217731429732,
      -1.451199958662136,
      -1.4839622583607457,
      -1.4969664341303992,
      -1.5394460139036927
    ],
    [
      -1.4101876867462646,
      -1.2144819278810859,
      -1.2210988867903259,
      0.0,
      -1.3810498329330345,
      -1.3310820550014781,
      -1.4274021588891856,
      -1.2989132365491691,
      -1.2745223472250748,
      -1.4120054813323664,
      -1.2751464145131022,
      -1.5219339943606898,
      -1.2836718747229645,
      -1.1510022551410966,
      -1.2202755087007318,
      -1.26683446768255,
      -1.2958654040352557,
      -1.3578220634369667,
      -1.3222498665476858,
      -1.2939699864817111,
      -1.3464733981781887,
      -1.4668066174700183,
      -1.4503755856734906,
      -1.321127307387811,
      -1.3828714767622174,
      -1.347405091709363,
      -1.4157639548900118,
      -1.4388842520251786,
      -1.4483046157530424
    ],
    [
      -1.3662719627840778,
      -1.3158125629587674,
      -1.2992558138990493,
      -1.3610794937522699,
      0.0,
      -1.339900593174707,
      -1.3915674220579146,
      -1.269072494177133,
      -1.322467199439128,
      -1.3371223730929849,
      -1.3069659089881944,
      -1.4749428549498558,
      -1.3331124336300628,
      -1.3318742326099855,
      -1.336145766057137,
      -1.3417959640862194,
      -1.3436092731794023,
      -1.281026212956043,
      -1.352803395005371,
      -1.3364964534324295,
      -1.301992069926853,
      -1.4286027799720304,
      -1.4157257408301993,
      -1.3463948787187239,
      -1.3280116624111427,
      -1.3474224214741815,
      -1.3796311174310714,
      -1.3638647288939698,
      -1.400903837202373
    ],
    [
      -1.4280341378366979,
      -1.3656035021369783,
      -1.3256480784109284,
      -1.3502160775445766,
      -1.3858939845832003,
      0.0,
      -1.4394852266871383,
      -1.33071069511291,
      -1.3192825596755418,
      -1.377090426964589,
      -1.3123241184330998,
      -1.4845707316834702,
      -1.3462839190021667,
      -1.3301225830790067,
      -1.3225158936296584,
      -1.3453511530301134,
      -1.3853836208670753,
      -1.350331575256989,
      -1.3781885353441747,
      -1.3494857477913746,
      -1.4148038925744297,
      -1.453443320054778,
      -1.4438438307001313,
      -1.385329290186154,
      -1.3757344752402607,
      -1.37491805279711,
      -1.4000223237303175,
      -1.3957638598617128,
      -1.424767525856519
    ],
    [
      -1.4135890336713948,
      -1.3835787245404327,
      -1.38240672566383,
      -1.3743866440796975,
      -1.3532732314853322,
      -1.3586262437099048,
      0.0,
      -1.3679527167975947,
      -1.3692003129276986,
      -1.4125260056767805,
      -1.3426833229570383,
      -1.4361707840267737,
      -1.3808263073814788,
      -1.3837222953815174,
      -1.3431897343137795,
      -1.3505342230652198,
      -1.3870312662115336,
      -1.3804630924190944,
      -1.4028437354744439,
      -1.3931836227834844,
      -1.4206147450108781,
      -1.4576114300397334,
      -1.443775290389008,
      -1.398487499095658,
      -1.4000965917713504,
      -1.385319025042206,
      -1.3858899004212988,
      -1.4212513351040543,
      -1.4387126818956735
    ],
    [
      -1.3853045801249444,
      -1.2203984911242465,
      -1.276435679057962,
      -1.2862174720164297,
      -1.3205932853049618,
      -1.3423748433912042,
      -1.4265014221320154,
      0.0,
      -1.3153958851533283,
      -1.4242204957849116,
      -1.2049906899525291,
      -1.4591254618634408,
      -1.3088143481806833,
      -1.2678872051730552,
      -1.2175318878180508,
      -1.2956417689098216,
      -1.349257598699537,
      -1.3305032747978223,
      -1.3279302045254864,
      -1.2821844031556076,
      -1.265655998567325,
      -1.450662393059943,
      -1.4425050132195427,
      -1.289714615092563,
      -1.330509547061456,
      -1.3142289634601625,
      -1.3918835012787512,
      -1.3509640724607148,
      -1.4084950440955464
    ],
    [
      -1.3915506267280913,
      -1.1987615561203995,
      -1.2040915905569207,
      -1.2049112298358045,
      -1.3083793347694346,
      -1.2760604818472767,
      -1.3965841900485543,
      -1.28325115104029,
      0.0,
      -1.3686106500052355,
      -1.167916848996971,
      -1.5195156354112354,
      -1.2989906545398489,
      -1.1857449672013338,
      -1.1838539336347504,
      -1.2524765297508675,
      -1.327115314015646,
      -1.2855499206810381,
      -1.2738989055053462,
      -1.1903569692375096,
      -1.2758330851332278,
      -1.4532986316029934,
      -1.4171342974533772,
      -1.3174339895312845,
      -1.3593723160724092,
      -1.3142657744234798,
      -1.3674878935028207,
      -1.3042432108799646,
      -1.4305935147260183
    ],
    [
      -1.6238252853590522,
      -1.5658152625663229,
      -1.5631682593315692,
      -1.5859741809769945,
      -1.5335720610903583,
      -1.5196940704556188,
      -1.628886252040535,
      -1.5543562626589384,
      -1.5771393320668954,
      0.0,
      -1.5452422349832413,
      -1.6484987078573978,
      -1.572414319073176,
      -1.5570559472253145,
      -1.5632509667383554,
      -1.5637822125733118,
      -1.5507109465234323,
      -1.5504399654972538,
      -1.5926651967121412,
      -1.583796364969795,
      -1.5431422653455997,
      -1.6345913246239352,
      -1.6026536512228857,
      -1.5703088467342807,
      -1.5167239141563567,
      -1.5751113902482738,
      -1.5759204753538814,
      -1.5744758632468543,
      -1.5416252227117848
    ],
    [
      -1.3339806701533359,
      -1.1344852806558647,
      -1.1054787862950928,
      -1.152303082719478,
      -1.2746789713228637,
      -1.2298392468731072,
      -1.3478693392830963,
      -1.1458379996142518,
      -1.1657396902088997,
      -1.3372630066355051,
      0.0,
      -1.428913376995198,
      -1.2339667069690181,
      -1.086419478743935,
      -1.082601525630881,
      -1.1861632907851283,
      -1.2583099786275262,
      -1.262639204398992,
      -1.2327140268797148,
      -1.1900891797662396,
      -1.2745581601267315,
      -1.388772993450584,
      -1.3783118226799196,
      -1.2380671655391686,
      -1.2889371120012703,
      -1.255281749699085,
      -1.3308422714891082,
      -1.2983318315774235,
      -1.364719964467186
    ],
    [
      -1.2001272207717713,
      -1.1242299220811394,
      -1.1396544360795047,
      -1.139060649739972,
      -1.1110252857099587,
      -1.1041887844862543,
      -1.1003301070282998,
      -1.1001618912294324,
      -1.1369172183447314,
      -1.0967036241706671,
      -1.1396866909627903,
      0.0,
      -1.097970306114365,
      -1.132032237898525,
      -1.1218613146836107,
      -1.1394972562935395,
      -1.1306300502574769,
      -1.141860812562704,
      -1.1386360516756129,
      -1.136055530719074,
      -1.1195526287416118,
      -1.1226087040043156,
      -1.1592708767592725,
      -1.1123270186652356,
      -1.0701392070210296,
      -1.104256914025233,
      -1.1078777860181945,
      -1.118365432882542,
      -1.1035042254128913
    ],
    [
      -1.343356762165008,
      -1.2043209427454042,
      -1.2610884567175538,
      -1.2715839793643273,
      -1.3264197575767942,
      -1.336085958890803,
      -1.4138508340109266,
      -1.3446982759592947,
      -1.3404158826481118,
      -1.386347888656283,
      -1.3032070620666298,
      -1.509640900865288,
      0.0,
      -1.2593620637340064,
      -1.2820391412322059,
      -1.2529721809987284,
      -1.3210490282812697,
      -1.3012184386868835,
      -1.3315663765854,
      -1.3005616839237222,
      -1.347315036985655,
      -1.435454871116089,
      -1.3988700427470921,
      -1.3250683915913786,
      -1.389714618686757,
      -1.3378681409867508,
      -1.3602545372483243,
      -1.3762197531523994,
      -1.3973530490130983
    ],
    [
      -1.335392054404555,
      -1.1181937612592325,
      -1.0851556161355933,
      -1.1276855755034882,
      -1.2509872731675507,
      -1.207874683509379,
      -1.3345116077182084,
      -1.2002373614866417,
      -1.2126488899785912,
      -1.3523006137820945,
      -1.1479286988725286,
      -1.4242868150821406,
      -1.1493654768473651,
      0.0,
      -1.0504724294966474,
      -1.1282785956418449,
      -1.2654315779010534,
      -1.2287124645698024,
      -1.2433821668728138,
      -1.2028103163344155,
      -1.2765867062144036,
      -1.3517217439950657,
      -1.362625950042685,
      -1.2314985923422632,
      -1.338602334132444,
      -1.2487091453627364,
      -1.3201400544489872,
      -1.2933221902356518,
      -1.389386264131445
    ],
    [
      -1.5672306593039307,
      -1.2251200069585382,
      -1.3132415071031938,
      -1.3169601936828665,
      -1.522988633253385,
      -1.473857362888626,
      -1.5751982535408775,
      -1.4114257914201147,
      -1.4149760591831715,
      -1.5533288637993308,
      -1.368375713642837,
      -1.7008329641288262,
      -1.4459858832358254,
      -1.3264313452722674,
      0.0,
      -1.3786431907137662,
      -1.5142099990781823,
      -1.4939609954290016,
      -1.504172032636759,
      -1.4344125294556613,
      -1.4968976192953298,
      -1.614051422419087,
      -1.5927641788021412,
      -1.4696385288822726,
      -1.5175683864502634,
      -1.47094352270489,
      -1.5368052568269999,
      -1.5282685228012773,
      -1.581430211191565
    ],
    [
      -1.4146076712915148,
      -1.2378042849716793,
      -1.3064516399742971,
      -1.2710996162301278,
      -1.3489069311180748,
      -1.3208861815582724,
      -1.4155050002711382,
      -1.3199722860793506,
      -1.334358931217303,
      -1.4019637627159824,
      -1.2911664596313785,
      -1.5295533493829347,
      -1.2733358357522893,
      -1.1874418771381128,
      -1.2263372578195895,
      0.0,
      -1.3343888914933686,
      -1.332671147411139,
      -1.3057948181768417,
      -1.2439411287485045,
      -1.3541039219562854,
      -1.4642354072163768,
      -1.415398910531796,
      -1.3244862019094619,
      -1.4020809609873512,
      -1.361944725421755,
      -1.3903944213844925,
      -1.3435661749309298,
      -1.4421626703846064
    ],
    [
      -1.4028322306667234,
      -1.3214148114903246,
      -1.3020847125999477,
      -1.3239227524537045,
      -1.3979200429464542,
      -1.4362550913890657,
      -1.4036366070287942,
      -1.374713852407477,
      -1.326638389858268,
      -1.431853229274287,
      -1.3262106273437557,
      -1.5164768787779976,
      -1.364532939784089,
      -1.3407542431202182,
      -1.3312285392067316,
      -1.3746481496236538,
      0.0,
      -1.332004295125601,
      -1.3285694380079889,
      -1.3457744272415624,
      -1.3197595695645759,
      -1.4775789251846332,
      -1.3934566404643502,
      -1.3601558032375252,
      -1.3846346162354897,
      -1.3245241607378548,
      -1.3849493080772985,
      -1.3890943657828956,
      -1.4856258138115128
    ],
    [
      -1.3495569834423171,
      -1.2345912724093353,
      -1.2796274820685558,
      -1.2714878405110641,
      -1.2950925045620396,
      -1.3285851733856244,
      -1.4136255915044182,
      -1.2982244695256397,
      -1.2880346099766213,
      -1.4318339162219427,
      -1.275659453270714,
      -1.5283969262845243,
      -1.2593947695093781,
      -1.2550853954571475,
      -1.276924560121674,
      -1.2604665790165694,
      -1.333539681338356,
      0.0,
      -1.339668629858356,
      -1.290262749319971,
      -1.314698344519809,
      -1.454987722563977,
      -1.4141288942555539,
      -1.3081172588790289,
      -1.3754321831857246,
      -1.3385671315202714,
      -1.3778485350115441,
      -1.3229899517914865,
      -1.4234825135677303
    ],
    [
      -1.4565606536380327,
      -1.3743146207699573,
      -1.311891568376321,
      -1.3745569511306872,
      -1.4456060364964924,
      -1.4132270154343811,
      -1.4782266836343825,
      -1.4071149424602425,
      -1.4041382513100364,
      -1.4684240621145166,
      -1.3806594230639975,
      -1.5584343625451367,
      -1.3975981098538988,
      -1.386964990744246,
      -1.3604861878819072,
      -1.3810293708642836,
      -1.3739293103162877,
      -1.4549333353894436,
      0.0,
      -1.404200977648651,
      -1.398435185355214,
      -1.4928120056532332,
      -1.4348880429631554,
      -1.3970088360582518,
      -1.4350496258814072,
      -1.406066439125102,
      -1.393147280392871,
      -1.4486336251714966,
      -1.4892277704741013
    ],
    [
      -1.4742052014206484,
      -1.2960289540902814,
      -1.3272945172415522,
      -1.391991511280185,
      -1.4369147801350142,
      -1.4640426526374715,
      -1.5785427138022166,
      -1.4436903001208294,
      -1.3161172121614275,
      -1.5006867774550603,
      -1.3280858047565653,
      -1.6371509575372822,
      -1.398724393579556,
      -1.3444469990917143,
      -1.2936957069992536,
      -1.3402232872668958,
      -1.4319485786814732,
      -1.420881493058587,
      -1.4383609841194993,
      0.0,
      -1.4034368558093853,
      -1.5493425752706582,
      -1.5373218440648675,
      -1.4309763028719884,
      -1.478340092991581,
      -1.4517515568076678,
      -1.4792964061698612,
      -1.2638282404921797,
      -1.5552753744293317
    ],
    [
      -1.4822113487407897,
      -1.412328715118251,
      -1.424322283213069,
      -1.454126860906659,
      -1.4209037243847145,
      -1.529545413156125,
      -1.5709435966067296,
      -1.403069512478415,
      -1.409813694876538,
      -1.5088674300296978,
      -1.447327242088737,
      -1.6151990449455513,
      -1.456374329531498,
      -1.4347106377375716,
      -1.434921930333656,
      -1.4498116513942427,
      -1.4210529768296158,
      -1.4464334257823317,
      -1.4798564431665573,
      -1.4027112431427258,
      0.0,
      -1.583953386784415,
      -1.552424273086763,
      -1.4446958176801679,
      -1.5127834577372448,
      -1.4380369520481395,
      -1.4956967498008862,
      -1.4200258188138606,
      -1.5188394833244157
    ],
    [
      -1.2645916012033742,
      -1.2863725285416572,
      -1.2962846067697362,
      -1.252836813098287,
      -1.2902911752615394,
      -1.2845849261744868,
      -1.2985174938058872,
      -1.2946457412842725,
      -1.255321787014472,
      -1.2681986725501349,
      -1.250443634480507,
      -1.3253468968944413,
      -1.2626136067221319,
      -1.2640172838956185,
      -1.2827368375320471,
      -1.2520714539961075,
      -1.2863390107205712,
      -1.298681256409166,
      -1.2762594866322448,
      -1.2726513311648804,
      -1.31737267755542,
      0.0,
      -1.2853170138280101,
      -1.2792055630731194,
      -1.2693561382291123,
      -1.2249467575992912,
      -1.249214593031713,
      -1.2899371212560127,
      -1.2634851497270916
    ],
    [
      -1.3237024759304403,
      -1.2886981643689515,
      -1.2615262201923456,
      -1.2731032129515536,
      -1.2718248266206018,
      -1.283274867603027,
      -1.3206517957670794,
      -1.2568356249091057,
      -1.2642616632045764,
      -1.2830119318324156,
      -1.2621258755319205,
      -1.3868599960135317,
      -1.2539330394998522,
      -1.2647079315948313,
      -1.2506024556490407,
      -1.2495797521122083,
      -1.2551676717640436,
      -1.2520289851879607,
      -1.27377019966487,
      -1.2423010676675437,
      -1.2925543981761014,
      -1.333039107372594,
      0.0,
      -1.2619234231004728,
      -1.3091765918648277,
      -1.2700803701075054,
      -1.255931030713203,
      -1.277631779026811,
      -1.3282228428374403
    ],
    [
      -1.2976594842525695,
      -1.2338618157371115,
      -1.2432696689402178,
      -1.2386149568670812,
      -1.2705015708990357,
      -1.312391077935116,
      -1.3659764045590939,
      -1.256205343353972,
      -1.276638407168974,
      -1.3042061906476168,
      -1.242852459056027,
      -1.415297411337275,
      -1.276387712476901,
      -1.274673524725062,
      -1.2108449828649541,
      -1.2740701198583322,
      -1.2632029617152813,
      -1.2680406750207516,
      -1.2915808016234176,
      -1.2755692865822439,
      -1.226056479856742,
      -1.3590817771183852,
      -1.3242285445093067,
      0.0,
      -1.2782222566328052,
      -1.2822206111241048,
      -1.2731771745832616,
      -1.2799043811641468,
      -1.3506042135355765
    ],
    [
      -1.5558391026293001,
      -1.4893094568599483,
      -1.4716253077783688,
      -1.5325709327928287,
      -1.4837705285153107,
      -1.5231659678464107,
      -1.5948920642170916,
      -1.4808945153186208,
      -1.5665461843693185,
      -1.5059779315967035,
      -1.5170644756708502,
      -1.59834104069041,
      -1.543559727032404,
      -1.5195781642776969,
      -1.4732460709956572,
      -1.523246023720676,
      -1.4912804183797594,
      -1.5354888905773514,
      -1.5265814179087769,
      -1.5191145749613901,
      -1.4888030341717746,
      -1.5556646509474064,
      -1.5891015758822449,
      -1.4916918478950565,
      0.0,
      -1.5116073208402883,
      -1.5516032301676466,
      -1.5132485921115473,
      -1.507352140392155
    ],
    [
      -1.5139566909292574,
      -1.4258757579142713,
      -1.4317819657015742,
      -1.4446511385938685,
      -1.4799182597318223,
      -1.477938133316511,
      -1.5209127216601708,
      -1.4355639701276022,
      -1.4361544733512426,
      -1.529453274784657,
      -1.3917751556885594,
      -1.6195587251813952,
      -1.440975511594452,
      -1.419964559203238,
      -1.4102234698914184,
      -1.441491995047352,
      -1.4362214068327666,
      -1.4340455943291583,
      -1.4603335614352455,
      -1.4611272390171592,
      -1.446336697500117,
      -1.5124805899185165,
      -1.5219016658194617,
      -1.4250510165511412,
      -1.5480591076024082,
      0.0,
      -1.477328907759837,
      -1.4877610723329517,
      -1.5421615560116977
    ],
    [
      -1.3782297180394698,
      -1.3092343919273197,
      -1.3422154455562132,
      -1.3367194285704649,
      -1.3020177013131766,
      -1.348330628502795,
      -1.3685534158375103,
      -1.3232195622393446,
      -1.358799472400842,
      -1.369201353569536,
      -1.3353002336318465,
      -1.411902469810458,
      -1.3265852836696945,
      -1.3339157519137061,
      -1.3049924121837946,
      -1.3261999625368805,
      -1.3457223758425885,
      -1.3306779500717418,
      -1.3352273796599792,
      -1.3380343162827684,
      -1.3127099130788937,
      -1.3980030754672839,
      -1.3671365982740142,
      -1.3170327546792824,
      -1.3178062854938335,
      -1.3099107855902792,
      0.0,
      -1.3354801600818567,
      -1.3611063987359617
    ],
    [
      -1.5508109564283419,
      -1.4697346521222319,
      -1.4657949353538846,
      -1.4934409285836874,
      -1.5041214356949928,
      -1.5135114740274256,
      -1.5813264454755203,
      -1.4733314334415109,
      -1.4632016149977662,
      -1.528219113555512,
      -1.4599040540418546,
      -1.590131987740114,
      -1.482483219820651,
      -1.4763566587784183,
      -1.444504564733844,
      -1.4345526108682545,
      -1.5022644336499582,
      -1.465463248680514,
      -1.504178824441578,
      -1.3008835067054043,
      -1.4285347742455088,
      -1.5695064020194711,
      -1.5552827170413608,
      -1.4633199334715716,
      -1.5225057304562393,
      -1.5069339392695107,
      -1.540220806770689,
      0.0,
      -1.5399175942830499
    ],
    [
      -1.5232351083017477,
      -1.450263921165733,
      -1.4265259937533152,
      -1.4575023411433947,
      -1.4213717846679113,
      -1.4284390099853168,
      -1.500940701876666,
      -1.4054152007964698,
      -1.4520583052752634,
      -1.4316314835909847,
      -1.430576147917524,
      -1.4981274902289134,
      -1.4247820243550506,
      -1.421512394793422,
      -1.3953724842919357,
      -1.4418492565417453,
      -1.480930831832232,
      -1.438150151668293,
      -1.4776910637697482,
      -1.4425394489988725,
      -1.4480912437190085,
      -1.4154837046231672,
      -1.4916831685868597,
      -1.4338518498444066,
      -1.4043787234275942,
      -1.4729349530071876,
      -1.4476204607466394,
      -1.422814440822458,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.26083873998136387,
      0.2381873571086741,
      0.23270581298581616,
      0.26063736219592437,
      0.23866946494581676,
      0.17626382024570253,
      0.25025302039543185,
      0.23462934207510155,
      0.2341179952761836,
      0.2579023495416819,
      0.12743691351652142,
      0.26463736420891326,
      0.23570437935437738,
      0.23698156185714447,
      0.25698308674516035,
      0.23115511851095905,
      0.24918348592810524,
      0.2436269223614116,
      0.27345897948727416,
      0.24465308928959972,
      0.20678310353179175,
      0.20181235900260797,
      0.22781425659686416,
      0.23058035166725732,
      0.24866277090035593,
      0.2228483689699714,
      0.23506876117036324,
      0.19794400614551755
    ],
    [
      0.4695660791423151,
      0.0,
      0.6818066204973896,
      0.7085764695241359,
      0.5155135184581816,
      0.5507838724670209,
      0.4299830051003666,
      0.5975671892336574,
      0.6564042715303433,
      0.48665341618648794,
      0.6841609959623347,
      0.36174978346402686,
      0.7256950668950486,
      0.6972622444013461,
      0.8522281102602214,
      0.7047965315294953,
      0.5273364157024345,
      0.5846593727549059,
      0.5487428589213048,
      0.6887270081846151,
      0.5265869146911273,
      0.4237142372563185,
      0.41767224587937424,
      0.5806877954374505,
      0.5226247566743614,
      0.5279678968823087,
      0.49962549136002776,
      0.49393770308621043,
      0.4429694657961589
    ],
    [
      0.401266880082239,
      0.5360785479554735,
      0.0,
      0.5561381258050897,
      0.37759107333622866,
      0.43272262850507914,
      0.3213648348638414,
      0.42995803397610155,
      0.4736849650898418,
      0.3581313782856046,
      0.5418923360108332,
      0.2544034760989857,
      0.4936015144154229,
      0.5992241305536914,
      0.6123456230451629,
      0.48277147013124466,
      0.4594294976753863,
      0.41706390607935906,
      0.45532865391889943,
      0.49433323587411904,
      0.4263412204781083,
      0.3173469746412767,
      0.3328177089184321,
      0.48481917670308805,
      0.4006461210262766,
      0.39106793550711383,
      0.35830563580850416,
      0.34530146003885065,
      0.3028218802655571
    ],
    [
      0.38028148179925303,
      0.5759872406644317,
      0.5693702817551918,
      0.0,
      0.4094193356124831,
      0.4593871135440395,
      0.363067009656332,
      0.4915559319963485,
      0.5159468213204428,
      0.37846368721315127,
      0.5153227540324155,
      0.26853517418482786,
      0.5067972938225531,
      0.6394669134044211,
      0.5701936598447859,
      0.5236347008629676,
      0.4946037645102619,
      0.432647105108551,
      0.4682193019978318,
      0.4964991820638065,
      0.44399577036732896,
      0.32366255107549935,
      0.34009358287202707,
      0.4693418611577067,
      0.4075976917833002,
      0.4430640768361547,
      0.3747052136555058,
      0.3515849165203391,
      0.34216455279247526
    ],
    [
      0.3584750808723034,
      0.40893448069761384,
      0.4254912297573319,
      0.3636675499041113,
      0.0,
      0.3848464504816742,
      0.3331796215984666,
      0.45567454947924824,
      0.40227984421725327,
      0.38762467056339633,
      0.4177811346681868,
      0.24980418870652543,
      0.3916346100263184,
      0.3928728110463957,
      0.38860127759924423,
      0.38295107957016183,
      0.38113777047697894,
      0.4437208307003382,
      0.37194364865101015,
      0.3882505902239517,
      0.4227549737295282,
      0.29614426368435076,
      0.3090213028261819,
      0.37835216493765733,
      0.39673538124523855,
      0.37732462218219975,
      0.34511592622530984,
      0.3608823147624114,
      0.3238432064540082
    ],
    [
      0.3658961898124953,
      0.4283268255122148,
      0.4682822492382648,
      0.4437142501046165,
      0.4080363430659928,
      0.0,
      0.35444510096205484,
      0.4632196325362832,
      0.4746477679736514,
      0.4168399006846042,
      0.4816062092160933,
      0.30935959596572293,
      0.44764640864702643,
      0.4638077445701865,
      0.47141443401953476,
      0.4485791746190797,
      0.4085467067821178,
      0.44359875239220403,
      0.4157417923050184,
      0.4444445798578185,
      0.3791264350747634,
      0.3404870075944151,
      0.3500864969490618,
      0.4086010374630391,
      0.4181958524089324,
      0.41901227485208303,
      0.3939080039188756,
      0.3981664677874803,
      0.36916280179267424
    ],
    [
      0.28975198958411474,
      0.3197622987150768,
      0.3209342975916796,
      0.32895437917581205,
      0.35006779177017733,
      0.3447147795456047,
      0.0,
      0.33538830645791484,
      0.3341407103278109,
      0.29081501757872896,
      0.3606577002984712,
      0.26717023922873584,
      0.3225147158740307,
      0.31961872787399215,
      0.36015128894173,
      0.35280680019028976,
      0.3163097570439759,
      0.32287793083641514,
      0.30049728778106566,
      0.3101574004720251,
      0.28272627824463137,
      0.24572959321577614,
      0.25956573286650153,
      0.30485352415985156,
      0.30324443148415914,
      0.31802199821330346,
      0.31745112283421073,
      0.2820896881514552,
      0.26462834135983604
    ],
    [
      0.40487464825650243,
      0.5697807372572004,
      0.5137435493234848,
      0.5039617563650172,
      0.46958594307648505,
      0.44780438499024267,
      0.36367780624943147,
      0.0,
      0.47478334322811855,
      0.3659587325965352,
      0.5851885384289177,
      0.331053766518006,
      0.4813648802007635,
      0.5222920232083916,
      0.5726473405633961,
      0.4945374594716252,
      0.44092162968190984,
      0.45967595358362456,
      0.46224902385596045,
      0.5079948252258393,
      0.5245232298141218,
      0.3395168353215039,
      0.3476742151619041,
      0.5004646132888839,
      0.45966968131999075,
      0.47595026492128434,
      0.3982957271026957,
      0.439215155920732,
      0.38168418428590045
    ],
    [
      0.41841111191953084,
      0.6112001825272226,
      0.6058701480907014,
      0.6050505088118177,
      0.5015824038781875,
      0.5339012568003454,
      0.41337754859906783,
      0.5267105876073321,
      0.0,
      0.4413510886423866,
      0.6420448896506512,
      0.29044610323638675,
      0.5109710841077733,
      0.6242167714462883,
      0.6261078050128717,
      0.5574852088967546,
      0.48284642463197613,
      0.524411817966584,
      0.5360628331422759,
      0.6196047694101126,
      0.5341286535143943,
      0.35666310704462867,
      0.39282744119424495,
      0.49252774911633757,
      0.45058942257521295,
      0.4956959642241423,
      0.44247384514480137,
      0.5057185277676575,
      0.37936822392160385
    ],
    [
      0.3548533946718251,
      0.41286341746455446,
      0.4155104206993081,
      0.3927044990538828,
      0.44510661894051906,
      0.45898460957525855,
      0.3497924279903424,
      0.4243224173719389,
      0.40153934796398194,
      0.0,
      0.433436445047636,
      0.3301799721734795,
      0.40626436095770124,
      0.42162273280556284,
      0.4154277132925219,
      0.4148964674575655,
      0.427967733507445,
      0.4282387145336235,
      0.3860134833187361,
      0.3948823150610823,
      0.4355364146852776,
      0.34408735540694213,
      0.3760250288079916,
      0.4083698332965966,
      0.46195476587452067,
      0.4035672897826035,
      0.40275820467699597,
      0.40420281678402303,
      0.4370534573190925
    ],
    [
      0.4080802653593505,
      0.6075756548568216,
      0.6365821492175936,
      0.5897578527932084,
      0.4673819641898227,
      0.5122216886395792,
      0.3941915962295901,
      0.5962229358984346,
      0.5763212453037867,
      0.40479792887718125,
      0.0,
      0.3131475585174883,
      0.5080942285436683,
      0.6556414567687514,
      0.6594594098818054,
      0.5558976447275581,
      0.4837509568851601,
      0.47942173111369435,
      0.5093469086329716,
      0.5519717557464467,
      0.46750277538595486,
      0.35328794206210246,
      0.3637491128327668,
      0.5039937699735177,
      0.4531238235114161,
      0.48677918581360147,
      0.4112186640235782,
      0.4437291039352629,
      0.37734097104550046
    ],
    [
      0.23565291631827567,
      0.31155021500890756,
      0.29612570101054225,
      0.2967194873500749,
      0.3247548513800882,
      0.3315913526037926,
      0.3354500300617471,
      0.3356182458606145,
      0.29886291874531556,
      0.3390765129193798,
      0.29609344612725663,
      0.0,
      0.33780983097568185,
      0.3037478991915219,
      0.3139188224064362,
      0.29628288079650744,
      0.30515008683257006,
      0.2939193245273428,
      0.29714408541443404,
      0.29972460637097287,
      0.31622750834843516,
      0.31317143308573137,
      0.2765092603307744,
      0.3234531184248113,
      0.3656409300690173,
      0.33152322306481397,
      0.3279023510718524,
      0.31741470420750484,
      0.33227591167715564
    ],
    [
      0.44940852389594776,
      0.5884443433155515,
      0.531676829343402,
      0.5211813066966284,
      0.46634552848416155,
      0.4566793271701528,
      0.37891445205002916,
      0.44806701010166106,
      0.4523494034128439,
      0.40641739740467275,
      0.4895582239943259,
      0.28312438519566774,
      0.0,
      0.5334032223269494,
      0.5107261448287499,
      0.5397931050622273,
      0.47171625777968607,
      0.4915468473740723,
      0.46119890947555575,
      0.4922036021372336,
      0.44545024907530073,
      0.3573104149448667,
      0.3938952433138636,
      0.4676968944695772,
      0.4030506673741987,
      0.454897145074205,
      0.4325107488126314,
      0.41654553290855634,
      0.39541223704785744
    ],
    [
      0.3652989875165522,
      0.5824972806618747,
      0.6155354257855139,
      0.573005466417619,
      0.44970376875355655,
      0.4928163584117282,
      0.3661794342028988,
      0.5004536804344655,
      0.488042151942516,
      0.34839042813901266,
      0.5527623430485786,
      0.2764042268389666,
      0.5513255650737421,
      0.0,
      0.6502186124244598,
      0.5724124462792624,
      0.4352594640200538,
      0.4719785773513048,
      0.45730887504829343,
      0.4978807255866917,
      0.4241043357067036,
      0.3489692979260415,
      0.33806509187842226,
      0.469192449578844,
      0.3620887077886632,
      0.45198189655837084,
      0.38055098747212,
      0.40736885168545545,
      0.31130477778966226
    ],
    [
      0.3832379092474374,
      0.7253485615928299,
      0.6372270614481743,
      0.6335083748685015,
      0.427479935297983,
      0.47661120566274207,
      0.3752703150104906,
      0.5390427771312534,
      0.5354925093681966,
      0.3971397047520373,
      0.5820928549085311,
      0.24963560442254185,
      0.5044826853155426,
      0.6240372232791007,
      0.0,
      0.5718253778376019,
      0.43625856947318575,
      0.45650757312236645,
      0.44629653591460916,
      0.5160560390957067,
      0.45357094925603825,
      0.33641714613228113,
      0.3577043897492269,
      0.48083003966909543,
      0.4329001821011047,
      0.47952504584647815,
      0.4136633117243682,
      0.42220004575009074,
      0.36903835735980306
    ],
    [
      0.3863845969851558,
      0.5631879833049913,
      0.49454062830237344,
      0.5298926520465428,
      0.4520853371585958,
      0.48010608671839816,
      0.38548726800553235,
      0.48101998219732,
      0.4666333370593676,
      0.39902850556068814,
      0.509825808645292,
      0.27143891889373584,
      0.5276564325243813,
      0.6135503911385578,
      0.5746550104570811,
      0.0,
      0.466603376783302,
      0.46832112086553157,
      0.49519745009982885,
      0.5570511395281661,
      0.4468883463203852,
      0.3367568610602938,
      0.3855933577448747,
      0.4765060663672087,
      0.39891130728931934,
      0.43904754285491565,
      0.41059784689217804,
      0.4574260933457408,
      0.35882959789206414
    ],
    [
      0.3506528420365338,
      0.43207026121293257,
      0.4514003601033094,
      0.42956232024955265,
      0.35556502975680293,
      0.31722998131419144,
      0.3498484656744629,
      0.37877122029578025,
      0.42684668284498906,
      0.3216318434289702,
      0.42727444535950143,
      0.2370081939252595,
      0.3889521329191681,
      0.41273082958303897,
      0.4222565334965256,
      0.37883692307960337,
      0.0,
      0.4214807775776561,
      0.4249156346952683,
      0.4077106454616948,
      0.4337255031386813,
      0.27590614751862397,
      0.3600284322389069,
      0.393329269465732,
      0.3688504564677675,
      0.4289609119654023,
      0.3685357646259586,
      0.3643907069203616,
      0.26785925889174433
    ],
    [
      0.42693914288166246,
      0.5419048539146443,
      0.49686864425542376,
      0.5050082858129155,
      0.48140362176193996,
      0.44791095293835514,
      0.36287053481956133,
      0.47827165679833983,
      0.48846151634735824,
      0.3446622101020369,
      0.5008366730532656,
      0.2480992000394553,
      0.5171013568146015,
      0.5214107308668321,
      0.4995715662023055,
      0.5160295473074101,
      0.4429564449856236,
      0.0,
      0.4368274964656236,
      0.4862333770040086,
      0.4617977818041705,
      0.32150840376000267,
      0.3623672320684257,
      0.4683788674449507,
      0.401063943138255,
      0.43792899480370817,
      0.39864759131243543,
      0.4535061745324931,
      0.3530136127562493
    ],
    [
      0.32859786924515677,
      0.4108439021132322,
      0.4732669545068684,
      0.4106015717525022,
      0.339552486386697,
      0.3719315074488083,
      0.30693183924880696,
      0.37804358042294695,
      0.381020271573153,
      0.31673446076867284,
      0.40449909981919197,
      0.22672416033805276,
      0.3875604130292907,
      0.3981935321389434,
      0.4246723350012822,
      0.40412915201890587,
      0.4112292125669017,
      0.33022518749374585,
      0.0,
      0.3809575452345384,
      0.3867233375279755,
      0.2923465172299562,
      0.350270479920034,
      0.38814968682493767,
      0.35010889700178227,
      0.37909208375808734,
      0.3920112424903184,
      0.3365248977116928,
      0.29593075240908817
    ],
    [
      0.4724482668242991,
      0.650624514154666,
      0.6193589510033952,
      0.5546619569647624,
      0.5097386881099333,
      0.482610815607476,
      0.3681107544427309,
      0.502963168124118,
      0.6305362560835199,
      0.44596669078988715,
      0.6185676634883821,
      0.3095025107076652,
      0.5479290746653915,
      0.6022064691532332,
      0.6529577612456938,
      0.6064301809780517,
      0.5147048895634743,
      0.5257719751863605,
      0.5082924841254481,
      0.0,
      0.5432166124355622,
      0.39731089297428923,
      0.40933162418007996,
      0.515677165372959,
      0.46831337525336636,
      0.4949019114372797,
      0.4673570620750862,
      0.6828252277527678,
      0.3913780938156157
    ],
    [
      0.38568470567365876,
      0.4555673392961974,
      0.4435737712013794,
      0.4137691935077894,
      0.44699233002973404,
      0.3383506412583235,
      0.29695245780771895,
      0.46482654193603357,
      0.45808235953791043,
      0.3590286243847507,
      0.4205688123257114,
      0.25269700946889717,
      0.4115217248829506,
      0.43318541667687693,
      0.4329741240807925,
      0.4180844030202058,
      0.44684307758483266,
      0.4214626286321168,
      0.3880396112478912,
      0.46518481127172273,
      0.0,
      0.2839426676300334,
      0.3154717813276855,
      0.42320023673428064,
      0.35511259667720374,
      0.429859102366309,
      0.3721993046135623,
      0.4478702356005879,
      0.34905657109003285
    ],
    [
      0.3849662771966489,
      0.3631853498583659,
      0.3532732716302869,
      0.3967210653017361,
      0.35926670313848375,
      0.3649729522255363,
      0.3510403845941359,
      0.35491213711575065,
      0.39423609138555116,
      0.38135920584988825,
      0.3991142439195161,
      0.3242109815055818,
      0.38694427167789125,
      0.3855405945044046,
      0.36682104086797596,
      0.3974864244039156,
      0.3632188676794519,
      0.350876621990857,
      0.37329839176777835,
      0.3769065472351427,
      0.3321852008446031,
      0.0,
      0.364240864572013,
      0.3703523153269037,
      0.3802017401709108,
      0.42461112080073193,
      0.4003432853683102,
      0.35962075714401043,
      0.3860727286729315
    ],
    [
      0.40830876147920625,
      0.4433130730406951,
      0.47048501721730096,
      0.458908024458093,
      0.46018641078904476,
      0.4487363698066196,
      0.4113594416425672,
      0.47517561250054086,
      0.46774957420507013,
      0.448999305577231,
      0.4698853618777261,
      0.34515124139611486,
      0.47807819790979433,
      0.4673033058148153,
      0.48140878176060586,
      0.4824314852974383,
      0.47684356564560293,
      0.4799822522216859,
      0.45824103774477654,
      0.4897101697421029,
      0.43945683923354517,
      0.39897213003705256,
      0.0,
      0.4700878143091738,
      0.42283464554481887,
      0.46193086730214117,
      0.4760802066964436,
      0.45437945838283555,
      0.4037883945722063
    ],
    [
      0.37567292622876325,
      0.43947059474422123,
      0.4300627415411149,
      0.4347174536142515,
      0.40283083958229704,
      0.36094133254621674,
      0.3073560059222389,
      0.4171270671273608,
      0.3966940033123587,
      0.36912621983371596,
      0.4304799514253057,
      0.2580349991440578,
      0.3969446980044318,
      0.39865888575627073,
      0.4624874276163786,
      0.39926229062300056,
      0.4101294487660514,
      0.40529173546058117,
      0.38175160885791515,
      0.39776312389908886,
      0.4472759306245908,
      0.31425063336294756,
      0.34910386597202603,
      0.0,
      0.39511015384852755,
      0.39111179935722795,
      0.4001552358980711,
      0.3934280293171859,
      0.3227281969457563
    ],
    [
      0.30630103660446073,
      0.37283068237381256,
      0.3905148314553921,
      0.3295692064409321,
      0.3783696107184502,
      0.3389741713873502,
      0.26724807501666925,
      0.38124562391514005,
      0.2955939548644424,
      0.3561622076370574,
      0.34507566356291064,
      0.26379909854335093,
      0.3185804122013569,
      0.342561974956064,
      0.3888940682381037,
      0.3388941155130849,
      0.37085972085400143,
      0.3266512486564095,
      0.335558721324984,
      0.34302556427237074,
      0.37333710506198625,
      0.3064754882863545,
      0.273038563351516,
      0.3704482913387044,
      0.0,
      0.3505328183934726,
      0.3105369090661143,
      0.3488915471222136,
      0.35478799884160583
    ],
    [
      0.36859031585479096,
      0.45667124886977706,
      0.45076504108247417,
      0.4378958681901799,
      0.40262874705222607,
      0.4046088734675375,
      0.36163428512387763,
      0.44698303665644623,
      0.4463925334328058,
      0.3530937319993914,
      0.49077185109548904,
      0.2629882816026532,
      0.4415714951895964,
      0.46258244758081046,
      0.47232353689262996,
      0.4410550117366965,
      0.44632559995128185,
      0.44850141245489006,
      0.4222134453488029,
      0.42141976776688916,
      0.43621030928393134,
      0.3700664168655319,
      0.36064534096458667,
      0.4574959902329072,
      0.33448789918164024,
      0.0,
      0.40521809902421135,
      0.39478593445109667,
      0.3403854507723507
    ],
    [
      0.35225115327998857,
      0.42124647939213866,
      0.3882654257632452,
      0.3937614427489935,
      0.42846317000628176,
      0.38215024281666343,
      0.36192745548194805,
      0.4072613090801138,
      0.3716813989186163,
      0.3612795177499224,
      0.3951806376876119,
      0.3185784015090003,
      0.40389558764976385,
      0.39656511940575223,
      0.4254884591356638,
      0.40428090878257783,
      0.3847584954768699,
      0.39980292124771655,
      0.3952534916594792,
      0.39244655503669,
      0.41777095824056465,
      0.3324777958521745,
      0.36334427304544414,
      0.413448116640176,
      0.4126745858256249,
      0.4205700857291792,
      0.0,
      0.3950007112376017,
      0.36937447258349665
    ],
    [
      0.3365511691228027,
      0.4176274734289127,
      0.42156719019725997,
      0.3939211969674572,
      0.3832406898561518,
      0.373850651523719,
      0.3060356800756243,
      0.4140306921096337,
      0.42416051055337833,
      0.3591430119956325,
      0.42745807150928994,
      0.29723013781103047,
      0.40487890573049357,
      0.4110054667727263,
      0.44285756081730066,
      0.4528095146828901,
      0.38509769190118637,
      0.42189887687063066,
      0.38318330110956667,
      0.5864786188457403,
      0.4588273513056358,
      0.31785572353167346,
      0.3320794085097838,
      0.42404219207957294,
      0.3648563950949053,
      0.3804281862816339,
      0.3471413187804555,
      0.0,
      0.3474445312680947
    ],
    [
      0.34799900223441815,
      0.42097018937043273,
      0.4447081167828506,
      0.4137317693927711,
      0.4498623258682546,
      0.44279510055084903,
      0.3702934086594998,
      0.4658189097396961,
      0.4191758052609025,
      0.43960262694518115,
      0.44065796261864176,
      0.3731066203072524,
      0.4464520861811152,
      0.4497217157427438,
      0.4758616262442301,
      0.4293848539944205,
      0.3903032787039338,
      0.43308395886787276,
      0.3935430467664176,
      0.4286946615372933,
      0.4231428668171573,
      0.45575040591299865,
      0.37955094194930616,
      0.43738226069175923,
      0.46685538710857166,
      0.39829915752897826,
      0.4236136497895264,
      0.44841966971370795,
      0.0
    ]
  ],
  "row_avgs": [
    0.23284071942842474,
    0.568142833474249,
    0.4305999437532075,
    0.4484146060876584,
    0.37639448483169274,
    0.4155321441466538,
    0.31162864749347774,
    0.4585389374720881,
    0.5043444813886174,
    0.40672008066146464,
    0.4918067957416647,
    0.3126182733636271,
    0.4549829983239491,
    0.45503929336862053,
    0.47369286733347565,
    0.4583293945014937,
    0.3784404133660151,
    0.44291358621400195,
    0.36631689199934175,
    0.517631965589839,
    0.3975036457094711,
    0.3730706941696184,
    0.45177811950733027,
    0.3888559714047126,
    0.33852709678565407,
    0.41208257043305363,
    0.3896142561422607,
    0.3934179113833279,
    0.4253136216171708
  ],
  "col_avgs": [
    0.375585840147346,
    0.4760250882602199,
    0.4744640809253546,
    0.4590131374037433,
    0.41869258673766735,
    0.41703229189118296,
    0.3486518949762763,
    0.44430374487499663,
    0.44237103349566526,
    0.37541414363365677,
    0.46859630240441963,
    0.2821793122592854,
    0.44681808565872894,
    0.47600482715435877,
    0.49155898700123685,
    0.458027437343418,
    0.421866565142022,
    0.42617152288923377,
    0.4198584586411842,
    0.45392043362975515,
    0.4259923907250037,
    0.3323896909623378,
    0.34509233494385944,
    0.43248201989652096,
    0.39957229105379805,
    0.4193684347585031,
    0.38906325426550425,
    0.40573198191816745,
    0.34884507269871573
  ],
  "combined_avgs": [
    0.30421327978788537,
    0.5220839608672344,
    0.45253201233928103,
    0.4537138717457008,
    0.39754353578468005,
    0.4162822180189184,
    0.330140271234877,
    0.4514213411735424,
    0.47335775744214137,
    0.39106711214756074,
    0.48020154907304213,
    0.2973987928114562,
    0.450900541991339,
    0.4655220602614897,
    0.48262592716735625,
    0.45817841592245584,
    0.4001534892540185,
    0.43454255455161783,
    0.393087675320263,
    0.4857761996097971,
    0.4117480182172374,
    0.3527301925659781,
    0.3984352272255949,
    0.41066899565061676,
    0.3690496939197261,
    0.41572550259577834,
    0.3893387552038825,
    0.39957494665074766,
    0.38707934715794323
  ],
  "gppm": [
    581.9648972405197,
    561.4970074430844,
    561.1223704278667,
    569.2402884351311,
    586.8772923246306,
    586.5111976778447,
    617.0419866625826,
    577.717767352209,
    577.4322320986561,
    604.3434252532696,
    566.2785612594676,
    650.0678113175518,
    575.0541553397541,
    563.5242695044839,
    554.8554339320804,
    570.625959661751,
    583.8728600325572,
    584.830197323421,
    585.6279963889389,
    571.4787948057507,
    582.6400470333565,
    623.6325085068584,
    621.6200824512065,
    583.1999719062605,
    593.8041770832449,
    584.3869140017205,
    600.2041850151421,
    590.0470819511141,
    619.0282000210076
  ],
  "gppm_normalized": [
    1.2938566779432423,
    1.3165052090413325,
    1.3234490039956817,
    1.332153958674769,
    1.3694038917496993,
    1.368702822992143,
    1.4425752206182842,
    1.3415584218610495,
    1.3471057299078664,
    1.406105183259362,
    1.317423612541427,
    1.5117940935644796,
    1.345273193309982,
    1.3162054621103654,
    1.2977319378454246,
    1.3284837547961454,
    1.3602698778401872,
    1.3646793929331937,
    1.36229834176714,
    1.3368095290895472,
    1.3531434071342354,
    1.44346970204871,
    1.4408707312369617,
    1.3570204925364115,
    1.3826608474290978,
    1.355642565441738,
    1.3952103840074812,
    1.3736908861872255,
    1.433107169485453
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399,
    1428,
    431,
    434,
    454,
    409,
    434,
    388,
    406,
    484,
    448,
    357,
    618,
    424,
    397,
    365,
    453,
    437,
    439,
    452,
    382,
    412,
    347,
    390,
    420,
    459,
    322,
    408,
    451,
    409,
    557,
    395,
    451,
    431,
    438,
    437,
    417,
    383,
    398,
    374,
    428,
    387,
    416,
    435,
    406,
    409,
    457,
    395,
    425,
    379,
    415,
    441,
    340,
    408,
    417,
    426,
    414,
    438,
    335
  ],
  "response_lengths": [
    2494,
    2433,
    2721,
    2557,
    2657,
    2671,
    2581,
    2261,
    2348,
    2205,
    2521,
    2244,
    2499,
    2595,
    2376,
    2419,
    2670,
    2357,
    2627,
    2079,
    2275,
    2694,
    1993,
    2438,
    2385,
    2481,
    2443,
    2496,
    1920
  ]
}