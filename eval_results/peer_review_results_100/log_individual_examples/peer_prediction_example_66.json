{
  "example_idx": 66,
  "reference": "Under review as a conference paper at ICLR 2023\n\nTOWARDS A UNIFIED VIEW ON VISUAL PARAMETEREFFICIENT TRANSFER LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSince the release of various large-scale natural language processing (NLP) pretrained models, parameter efficient transfer learning (PETL) has become a popular paradigm capable of achieving impressive performance on various downstream tasks. PETL aims at making good use of the representation knowledge in the pretrained large models by fine-tuning a small number of parameters. Recently, it has also attracted increasing attention to developing various PETL techniques for vision tasks. Popular PETL techniques such as Prompt-tuning and Adapter have been proposed for high-level visual downstream tasks such as image classification and video recognition. However, Prefix-tuning remains under-explored for vision tasks. In this work, we intend to adapt large video-based models to downstream tasks with a good parameter-accuracy trade-off. Towards this goal, we propose a framework with a unified view of PETL called visual-PETL (V-PETL) to investigate the effects of different PETL techniques, data scales of downstream domains, positions of trainable parameters, and other aspects affecting the tradeoff. Specifically, we analyze the positional importance of trainable parameters and differences between NLP and vision tasks in terms of data structures and pretraining mechanisms while implementing various PETL techniques, especially for the under-explored prefix-tuning technique. Based on a comprehensive understanding of differences between NLP and video data, we propose a new variation of prefix-tuning module called parallel attention (PATT) for video-based downstream tasks. An extensive empirical analysis on two video datasets via different frozen backbones has been carried and the findings show that the proposed PATT can effectively contribute to other PETL techniques. An effective scheme SwinBAPAT derived from the proposed V-PETL framework achieves significantly better performance than the state-of-the-art AdaptFormer-Swin with slightly more parameters and outperforms full-tuning with far less parameters.\n\n1\n\nINTRODUCTION\n\nMany vision tasks rely on fine-tuning pre-trained models to achieve good performance. One standard modus operandi of transfer learning consists of two steps: pre-train a model on a source domain and fine-tune the entire model on a target domain (Zhuang et al., 2020). Despite that prior works have achieved promising performance, such vanilla practice of fine-tuning is faced with challenges for adopting large models to downstream tasks. This full-tuning strategy requires one to update and store separate model parameters for different downstream tasks, which can be expensive and infeasible for the era of increasingly large models from EfficientNet-based (Pham et al., 2021) (480M parameters) to Transformer-based (Yu et al., 2022) (2, 100M parameters) ones. For such large models, making good use of shared parameter weights deployed on the cloud can be beneficial for edge devices such as autonomous vehicles, drones who are intensive in computing and battery resources (Yuan et al., 2022). Second, the full fine-tuning strategy relies on high-quality downstream data and can hardly adapt to unseen scenarios that have large distribution shift (Kumar et al., 2021), which is unlike the learning process of humans who can learn from few samples and generalize well to new circumstances. This issue has been researched in directions such as zero-shot learning, few-shot learning, and continual learning (Li et al., 2021a). Another popular strategy is fine-tuning the downstream task head, i.e., the last fully connected (FC) layer, to avoid tuning the whole backbone model, which usually leads to poor performance when the target domain is large in data scale (see Figure\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n1). Given the paradigm of fine-tuning increasingly large models, how to transfer such large models with parameter-accuracy trade-off is a hot topic in various domains (Gusak et al., 2022; Sung et al., 2022; Lin et al., 2020; Houlsby et al., 2019).\n\nTaking the video-based action recognition task as an example, it can be inconvenient for deploying such large models to edge devices such as an autonomous driving (Liu et al., 2019) and unmanned aerial vehicle (Li et al., 2021b) as they can heavily rely on the interaction with cloud services for adapting to new environments via active learning (Wang et al., 2021) or continual learning (Li et al., 2021a). Re-training large models on the cloud are usually not cost-effective due to the expensive overheads of storage and computational resources. Furthermore, these resources are limited on edge devices such as autonomous vehicles and unmanned aerial vehicles, making the sense for developing effective fine-tuning methods with proper parameter-accuracy trade-off that can be fine-tuned on edge devices and interacting with the large models deployed on the cloud.\n\nThere have been some pioneering works for the PETL of visual models such as AdaptFormer (Chen et al., 2022) and visual prompt tuning (VPT) (Jia et al., 2022). AdaptFormer is primarily proposed based on vision transformer (Zhai et al., 2022), representing one of the stateof-the-art large models for image-based tasks. The proposed adapter module directly brings from Houlsby et al. (2019) due to its conveImnience of being inserted to any models. plementing with a large batch size of 1, 024 with 64 GPUs, Adaptformer shows promising parameter-accuracy trade-off on video data. However, such powerful computing resource is not realistic for the usage of edge devices. Meanwhile, whether the good trade-off can be maintained for small batch size remains under-explored. Inspired by the Prompting in NLP (Liu et al., 2021), VPT proposes visualprompt to fine-tune visual models for imagebased tasks. According to the empirical results in Chen et al. (2022), adapter modules achieves superior performance over VPT in the regimes of both self-supervised and supervised pre-training. Another concern of VPT is its modification to the original model parameters might affect the knowledge representation of backbone models. Hence, we do not continue to compare our method with VPT but comparing with the adapter on video-based downstream tasks.\n\nFigure 1: Parameter-accuracy trade-off. Adapting backbone Swin-B (Liu et al., 2022) pre-trained on Kinetics 400 via different fine-tuning methods on the something-something v2 (Goyal et al., 2017) dataset. Our methods perform significantly better than the state-of-the-art AdaptFormer-Swin (Chen et al., 2022) (our implementation with batch size 16) with slightly more tunable parameters, and outperform full-tuning with increasing margins when using larger values of dbottle.\n\nTaking the recent inspiration of the mix-and-match adapter (MAM adapter) (He et al., 2022a) in NLP, we aim to propose a unified model for the vision domain, especially for video-based downstream tasks. He et al. (2022a) analyzed the unified view among PETL techniques such as prefixtuning, low-rank (LoRA) adaptation, and adapter, pointing out the similarity between prefix-tuning and adapter in terms of calculating the attention. The difference is that the former performs weighted addition while the latter ones is unweighted. Note that prefix-tuning has not ever been applied to visual tasks in the form of pure visual models due to the intrinsic differences regarding pre-training methods of NLP and vision models. Another obstacle of directly applying prefix-tuning to visual tasks is the structural difference between text and vision data (we further discuss this in Section 2.3). Considering the video-based action recognition task, we propose a new variation of the prefixtuning module called parallel attention (PATT) to adapt video-based pre-trained large models to downstream domains with varied data scales. The differences of our method comparing the original prefix-tuning in NLP are twofold: prefix calculation and the manner of insertion (see Figure 2[b] and Figure 3). Regarding the backbone model, we focus on Video Swin Transformer (Liu et al., 2022), one of the state-of-the-art vision models that bring competitive performance on large-scale action recognition datasets such as Kinetics 400 and 600 Kay et al. (2017).\n\nOur main contributions can be threefold as follows: 1. We analyze different PETL techniques using the backbone model Swin Video Transformer for\n\n2\n\n -30-25-20-15-10-50500.10.20.30.40.50.6AdaptFormer-Swin Ours (ddbbbbbbbbbbbb=32) Tune FC layer Full-tuning Ours (ddbbbbbbbbbbbb=64) Ours (ddbbbbbbbbbbbb=128) Ours (ddbbbbbbbbbbbb=256) Relative Accuracy Gained w.r.t Full-tuning (%) Tunable Parameters (%) 10−3 10−2 10−1 100 101 102 Under review as a conference paper at ICLR 2023\n\nvideo-based tasks, providing a unified view via our V-PETL framework and investigating the importance of the fine-tuning position. 2. Based on the comprehensive understanding of intrinsic differences between NLP and video data regarding data structures and pre-training mechanisms, we leverage prefix-tuning to our V-PETL with a new variation called PATT. 3. Upon extensive ablation experiments regarding various effect factors, we empirically validate the promising parameter-accuracy trade-off achieved by our adjustable and easy-to-use PATT module, contributing to the existing literature of PETL techniques.\n\n2 UNIFIED FRAMEWORK\n\n2.1 RECAP OF VIDEO SWIN TRANSFORMER\n\nVideo Swin Transformer (Liu et al., 2022) is formed with Transformer layers (a.k.a. stages) that are consisted with 3D Video Swin Transformer blocks. With varied layers, blocks, and channel sizes, the model can be formed as Swin-T, Swin-S, Swin-B, and Swin-L. The basic architecture of a 3D Swin Transformer block is shown in Figure 2, which is mainly composed of a 3D shifted window-based multi-head self-attention (3DSW-MSA) module and a fully connected feed-forward network (FFN) implemented with a 2-layer MLP. Layer normalization (LN) and residual connection are respectively performed before and after both FFN and 3DSW-MSA modules. One such Video Swin Transformer block can be represented as:\n\nl\n\nˆZ\n\n= 3DSW-MSA(LN(Zl−1)) + Zl−1,\n\nZl = FFN(LN( ˆZ\n\nl\n\n)) + ˆZ\n\nl\n\n,\n\n(1)\n\nwhere ˆZ\n\nl\n\nand Zl respectively indicate the output of 3DSW-MSA and FNN modules.\n\nt\n\n4 × h\n\n2 × w\n\nGiven a video input sized t×w×h×3, containing t video frames with their heights and widths being h and w, respectively. The 3D patch for video data sized 2 × 4 × 4 × 3 is treated as a token. Then we will have t 4 3D tokens after a 3D patch partitioning layer. Given the 3D tokens sized 2 × w 4 × h 4 and a 3D window with the size of p × m × m, the self-attention module, using the regular window partition strategy, will partition the 3D tokens to t 4m non-overlapping windows. For shifted 3D window, the partition is shifted along the temporal, height, and width dimensions by p\n2 × m 2 . For example, if we have an input video sized 8 × 224 × 224 × 3 and a 8 × 7 × 7 3D window, after the patch embedding, we will have 4 × 56 × 56 3D tokens with each of them sized 2 × 4 × 4 × 3. Without shifting, the non-overlapping window size will be 1 × 8 × 8 = 64.Then through the 3D window shifted by (4, 3, 3) , the number of 3D windows becomes 1 × 9 × 9 = 81.\n\n4m × h\n\n2p × w\n\n2 × m\n\nThe 3DSW-MSA module is formed with a 3D relative position bias B ∈ Rp2×m2×m2 can be represented as:\n\n, each of which\n\nAttention(Q, K, V) = Sof tM ax(\n\nQKT √\nd\n\n+ B)V,\n\n(2)\n\nwhere Q, K, V ∈ Rp×m×m×d are the query, key, and value matrices, p × m×m is the number of tokens and d is the dimension of the tokens. MSA simultaneously performs the attention mechanism v ∈ Rd×3d, projecting for nhead heads, where the ith head can be parameterized by W (i) the input Z l−1 to queries, keys, and values. Given a matrix C ∈ R (cid:101)m×d, (cid:101)m = p × m×m, for performing attention, the 3DSW-MSA can be calculated as:\n\nk , W (i)\n\n, W (i)\n\nq\n\n3DSW-MSA(Zl−1, C) = Concat(head1, ..., headn)Wo, , CW (i)\n\nheadi = Attention(Zl−1W (i)\n\nq\n\nk , CW (i)\n\nv ),\n\n(3)\n\nwhere Wo is the parameters of a linear project layer. The FNN module is composed of two linear layers with a GELU activation function in between, which can be computed as:\n\n(4) where W1 ∈ Rdhidden×d, W2 ∈ Rd×dhidden, b1 ∈ Rdhidden, and b2 ∈ Rd. The value of dhidden usually takes a large value (e.g., dhidden = 4d).\n\n)W1 + b1)W2 + b2,\n\nFFN( ˆZ\n\n) = GELU(LN( ˆZ\n\nl\n\nl\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n2.2 RECAP OF PETL TECHNIQUES\n\nFigure 2: V-PETL: A unified view of visual PETL techniques. They bring trainable parameters to different positions of the backbone model with various manners. AdaptFormer and Prefix-tuning respectively perform at the MLP and 3DSW-MSA modules that can adjust the number of trainable parameters via the bottleneck size of down and up projections. While prompt-tuning performed at the layer-level can adjust the length of prompts to control the tuned parameters.\n\nPrefix-tuning (Li & Liang, 2021): The prefix-tuning approach prepends learnable prefix tokens to the keys and values of the MSA module of the model (see Figure 2[b]). Specifically, two prefix matrices Pk, Pv ∈ Rdtoken×d that are randomly initialized with dtoken tokens and transformed from two linear layers (with parameters W (i) pv ∈ Rdmiddle×d) and a Tanh layer in pk ∈ Rd×dmiddle and W (i) between are concatenated to the original key and value, leading the calculation of headi in Eq. 3 to:\n\nheadi = Attention(Zl−1W (i)\n\nq\n\n, concat(P (i)\n\nk , CW (i)\n\nk ), concat(P (i)\n\nv , CW (i)\n\nv )),\n\n(5)\n\nwhere the concat is the concatenation performed along the token dimension to mimic the prefixtuning in NLP tasks. Here, a question regarding whether this direct implementation will work for the vision domain is raised (results are in Table 4). This direct implementation is empirically invalid and we make further modification on it in Section2.3.\n\nAdapter (Chen et al., 2022): Inspired by the works of Houlsby et al. (2019); He et al. (2022a) for PETL in NLP tasks, adapter (Chen et al., 2022) has been directly used for vision tasks, showing promising performance using far less tunable parameters. The number of parameters of adapter is controlled by a parameter dbottle (dbottle ≪ d), adjusting the space size of a low-dimensional representation. The adapter module first uses a down-projection with Wdown ∈ Rd×dbottle to project the feature to the lower-dimensional representation, followed by a ReLU activation function, and a up-projection with Wup ∈ Rdbottle×d.\n\nthen two positions implementing adapter (parallel and sequential) can be respectively computed as:\n\nl\n\n(cid:101)Z\n\n= ReLU(LN( ˆZ\n\nl\n\n)Wdown)Wup,\n\n(6)\n\nZl = FFN(LN( ˆZ\n\nl\n\n)) + ˆZ\n\nand sZl = ReLU(FFN(LN( ˆZ\n\nl\n\nl\n\n,\n\n+ s (cid:101)Z ))Wdown)Wup + ˆZ\n\nl\n\nl\n\n,\n\n(7)\n\nwhere s is a scalar, controlling the effect of the adapter (will be ablated in experiments). According to Chen et al. (2022), the parallel implementation (see Figure 2[a]) empirically performs better.\n\nPrompt-tuning (Jia et al., 2022): Prompt-tuning (see Figure 2[c]) is inspired by the success of prompt-tuning that adapts large scale models to varied downstream NLP tasks. The idea of VPT prompt ∈ Rdprompt×d, dprompt < dtoken − 1 for (Jia et al., 2022) is to fine-tune a learnable matrix P l−1\n\n4\n\nMLPLayerNormLayerNormMLPUPReLUDown(a) AdapterQUPTanhDown(b) Prefix-tuning(c) Prompt-tuningZVideo Swin-TransformerKVPvPkESoftMaxQKVPromptMLPLayerNormLayerNorm3DSW-MSAL TrainableFrozensl−1Zl3DSW-MSAL PvPkUnder review as a conference paper at ICLR 2023\n\nthe lth Transformer layer or all Transformer layers, which are known as shallow prompt and deep prompt, respectively.\n\nl\n\nˆZ\n\n= 3DSW-MSA(LN([xl−1, P l−1\n\nprompt, Zl−1])) + Zl−1,\n\n(8)\n\nwhere xl−1 ∈ Rd denotes the [CLS]’s embedding for the lth layer’s input space, P l−1 prompt is implemented by overlapping the top dprompt tokens of Zl−1 (Jia et al., 2022). While it has also been implemented in front of the xl−1 (Chen et al., 2022).\n\nOthers: Other PETL techniques include ST-Adapter Pan et al. (2022), LoRA (Hu et al., 2022), and BitFit (Zaken et al., 2022). ST-Adapter mainly adapts image-text models pre-trained on large scale datasets such as 400M image-text pair proposed by CLIP (Radford et al., 2021) and the IG-3.6B used by SWAG (Singh et al., 2022) to video understanding downstream tasks, which matches and even outperforms full-tuning. LoRA approximates the optimization process by injecting learnable low-rank matrices into the attention module. This method does not show superior performance for NLP tasks in terms of parameter efficiency. Hence, we do not prioritize this direction in this work. BitFit only tunes the bias terms of the backbone models, making it very parameter-efficient.\n\n2.3 REVISITING PREFIX-TUNING FOR VISUAL TASKS\n\nThe prefix implementation in NLP Li & Liang (2021); He et al. (2022a) can be regarded as prepending contextual information for downstream tasks, which is similar with the pre-training process aiming to predict masked words in the process of an inner loop (Brown et al., 2020). Considering the pre-training process of pure vision models, such direct implementation might not make sense for visual tasks. Although such autoregressive pre-training has been conducted in visual domain (He et al., 2022b; Tong et al., 2022), but adding prefix for a sentence input in NLP can be structurally different with the visual domain. Specifically, masked pixels in image or video data cannot be regarded as some word level semantic information (e.g., a subject or an action) as in the NLP.\n\nRecall that the embedding state of prefix-tuning is randomly initiated, which is known as learnable prefix but can bring random noise that later turns out affecting the convergence of the fine-tuning downstream tasks. Hence, inspired by the connection between adapter and prefix (He et al., 2022a), we avoid such learnable prefix design with random initialization and propose a parallel attention (PATT) to the original attention module (see Figure 3). The adapter structure can effective control the number of trainable parameters via dbottle, which is similar with the effect of the middle dimension dmiddle of W (i) and W (i) pv for preparing the prefix. Specifically, for the lth layer, we use output of its previous layer Z l−1 and project it to a pair of matrices Kp, Vp ∈ R (cid:101)m×d via a similar mechanism of Eq. 6:\n\npk\n\nKp, Vp = Tanh(Zl−1Wdown)Wup,\n\n(9)\n\nFigure 3: Structure of PATT. Red parts are trainable parameters calculated by the same input for preparing query, key, and value (i.e., the output of the previous layer passing through a layer normalization layer Zl−1).\n\nwhere Tanh is the activation function used for preparing the prefix, which can be replaced by other activation functions such as RELU and GELU. Here, we follow the original prefix implementation as its value ranges from −1 to 1. Given Kp and Vp, Eq. 5 can be rewritten as: , sKp + CW (i) headi = Attention(Zl−1W (i)\n\n(10) where s is a scalar for adjusting the effect of PATT. Note that without considering the physical meaning of such design, for PETL purpose, one can perform similar practise for any combinations of Q, K, and V. This brings connection to the LoRA (Hu et al., 2022) method, which add parallel trainable parameters to Q and V. Empirically, where to perform the PATT makes little difference, but the amount of trainable parameters brings larger effect for large scale downstream domains.\n\nk , sVp + CW (i)\n\nv ),\n\nq\n\n2.4 V-PETL: UNIFIED VIEW ON VISUAL PETL\n\n5\n\nUPTanhDownSoftMaxKVQQKVKPVPKPVPssZl−1BruceUnder review as a conference paper at ICLR 2023\n\nGiven the PETL techniques at hand, there can be many potential combinations leading to good parameteraccuracy trade-off. However, it is unrealistic to exhaustively test all the methods for a specific downstream task. Other than probing such solution via evolutionary search as in Zhang et al. (2022), we aim to propose more understandable models by empirically analyzing the effect of different designs independently. According to the preliminary results shwon in Figure 1, we argue that the position and amount of parameters are important for PETL techniques, especially when the target domain is not small.\n\nTable 1: Comparison of independently fine-tuning varied positions of the video swin transformer block on SSv2.\n\nPosition\n\n# Params Top-1 (%)\n\nFull-tuning Tune FC Layer\n\n87.82M 0.18M\n\n50.99 24.13\n\nLayerNorm 1 Attn, Proj Attn, QKV Attn, SoftMax LayerNorm 2 MLP, FC1 MLP, FC2 DownSample\n\n0.02M 6.99M 20.98M 0.95M 0.02M 27.97M 27.93M 2.76M\n\n14.35 47.58 50.02 27.67 14.62 47.10 45.32 27.53\n\nTo verify the importance of position and tuned parameter amount, we independently tune different modules of the backbone model. Table 1 shows the results. We can see that the attention module’s QKV layer has 20.98M parameters while the MLP module has the most number of parameters of 55.90M. Tuning positions with more parameters, will lead to better performance for SSv2. Thanks to the bottleneck mechanism of adapter and prefix-tuning, one can effectively achieve a good parameter-accuracy trade-off. As such, we derive a model called Swin-B-adapter-PATT (Swin-BAPAT) from the V-PETL framework by using the parallel adapter and our PATT to leverage the adaption of pre-trained backbone model at the positions of attention and MLP modules, respectively. In addition to adapter and PATT, we also fine-tune the last fully connected layer as it has relatively smaller amount of tunable parameters (i.e, 0.18M) than adapter and PATT.\n\n3 EXPERIMENTS\n\n3.1 EXPERIMENTAL SETTINGS\n\nVideo Datasets: Something-something v2 (SSv2 (Goyal et al., 2017)) It has 108,499 short videos for 174 human-object interaction categories with durations between 2 to 6 seconds. The challenge of this dataset is that it contains 23, 137 distinct object names with an imbalanced distribution. The original dataset is split into train, validation, and test sets with a ratio of 8:1:1. The extended version (SSv2) of this dataset is consisted of 168, 913 training samples, 24, 777 validation samples, and 27, 157 testing samples with the sample number of action labels. The training and testing samples are used. HMDB51 (Kuehne et al., 2011) contains 6, 766 video samples for 51 action categories including videos of varied visible body parts, camera motion, camera view, and clip quality. All video samples have at least 101 clips and a minimum height of 60 pixels for actors. The original dataset has three splits of training and evaluation. We follow existing work Chen et al. (2022) by using the first training and evaluation split that has 3, 570 and 1, 530 samples, respectively. Image Datasets: Following the experimental set ups in AdaptFormer, three datasets CIFAIR-100 Krizhevsky et al. (2009), Street View House Numbers (SVHN) Goodfellow et al. (2013), and Food101 Bossard et al. (2014) are used. CIFAIR-100 has 50, 000 and 10, 000 training and validation images, respectively, with the resolution of 32×32 and 100 categories; SVHN is a digit classification dataset that has 73, 257 training sample and 26, 032 testing samples; Food-101 includes 101k images of 101 food categories with each of them has 750 training and 250 testing samples.\n\nImplementation details: It is worth noting that big batch size (i.e., 1, 024) and the number of input video frames (i.e., 32 frames) can greatly benefit good performance (Carreira & Zisserman, 2017; Liu et al., 2022; Chen et al., 2022), which usually requires GPU clusters to enable the training. AdaptFormer (Chen et al., 2022) uses such powerful GPU cluster to achieve good performance. However, good performance might not hold when the batch size is small. Following the more common hardware device setup, we use 4 GeForce 3090 GPUs for all experiments, leading to a batch size of 64. All the experiments are fine-tuned for 70 epochs. We use the Swin-B1 model pre-trained on Kinetics 400 and 600. For HMDB51, we report the results without tuning the FC layer due to the significant effect of the FC layer on relatively small scale dataset. Following Chen et al. (2022), we do not perform regularization strategies such as mixup, cutmix, color jittering,\n\n1https://github.com/SwinTransformer/Video-Swin-Transformer\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Comparison of Top-1 accuracy using varied amount of parameters adjusted by dbottle, different pre-training domains, and the number of frames with other fine-tuning strategies.\n\nMethod\n\ndbottle Pre-training # Frames\n\n- -\n-\n\nKinetics 400 Full-tuning Kinetics 400 Tune FC Layer BitFit (Zaken et al., 2022) Kinetics 400 AdaptFormer-Swin (Chen et al., 2022) 64 Kinetics 400 128 Kinetics 400 Prefix-tuning (Li & Liang, 2021) 32 Kinetics 400 Our Swin-BAPAT (w/o Adapter) 64 Kinetics 400 Our Swin-BAPAT (w/o Adapter) 128 Kinetics 400 Our Swin-BAPAT (w/o Adapter) 256 Kinetics 400 Our Swin-BAPAT (w/o Adapter) 32 Kinetics 400 Our Swin-BAPAT 64 Kinetics 400 Our Swin-BAPAT 128 Kinetics 400 Our Swin-BAPAT 256 Kinetics 400 Our Swin-BAPAT 256 Kinetics 400 Our Swin-BAPAT 256 Kinetics 600 Our Swin-BAPAT 256 ImageNet-22K Our Swin-BAPAT 128 Kinetics 400 Our Swin-BAPAT 128 Kinetics 400 Our Swin-BAPAT\n\n8 8\n8 8\n8 8\n8 8\n8 8\n8 8\n8 8\n8 8\n8 16\n\nSSv2\n\nHMDB51\n\n# Params Top-1 (%) # Params Top-1 (%) 87.82M 50.99 0.18M 24.13 1.29M 45.94 1.73M 40.80 6.57M 39.46 1.35M 46.26 2.51M 49.23 4.83M 52.57 9.45M 52.71 2.91M 49.63 4.07M 51.80 6.38M 53.36 11.00M 53.98 11.00M 53.98 11.00M 54.06 11.00M 43.56 6.38M 53.36 6.38M 63.14\n\n87.69M 68.07 0.05M 71.28 1.11M 68.26 1.61M 68.66 6.40M 56.13 1.17M 69.51 2.34M 71.34 4.65M 70.56 9.27M 70.23 2.74M 68.20 3.89M 70.10 6.20M 71.93 10.83M 69.64 10.83M 69.64 10.83M 69.90 10.83M 59.89 6.20M 71.93 6.20M 75.67\n\netc. Our PATT module is convenient to be applied to other Transformer-based models. Hence, we respectively adopt ViT-B models from MAE (He et al., 2022b) and VideoMAE (Tong et al., 2022) to conduct further comparison on video and image datasets, which follows the self-supervised pretraining setting2 in Chen et al. (2022) except that the batch size is set to 256 instead of 1, 024.\n\nBaselines: We mainly compare our method Swin-BAPAT with three baselines as follows: (1) Full-tuning: set all the parameters learnable and tune the whole model initiated with the pretrained weights. (2) Tune FC layer: tune the last fully connected layer and freeze pre-trained parameters of the whole backbone model. (3) AdaptFormer-Swin: method introduced by Chen et al. (2022) that adds a parallel adapter to the MLP module in each block of the backbone model. (4) Prefix-tuning: the direct implementation of prefix-tuning used in NLP as defined in Eq. 5. (5) BitFit: by tuning the bias of the backbone model together with the FC layer.\n\n3.2 THE EFFECT OF DIFFERENT PETL TECHNIQUES\n\nTable 2 shows the results of different PETL techniques. From the results of four baseline methods, full-tuning performs the best for the large-scale dataset SSv2, whereas tuning the FC layer achieves superior performance over other PETL techniques on HMDB51. This is due to the fact that downstream tasks with relatively larger scale datasets are more parameter hungry for good convergence. On the contrary, small datasets can make good use of the knowledge from the source domain with slight effort of adaption via an FC layer. Here, a question regarding the effect of this FC layer when using it together with other PETL techniques has not been investigated. As this FC layer having small amount of tunable parameters can already make a big difference, performing better than fulltuning and other PETL techniques and rendering them not effective for small-scale datasets. As such, we further examine this question in Section A.1.\n\nWe test different amount of parameters adjusted by sbottle, taking its values to 32, 64, 128 and 256. The second and third groups (without or with Adapter, respectively) of results in Table 2 shows that larger values of sbottle can benefit the fine-tuning with slightly more overhead of parameters on large-scale datasets such as SSv2. All results of our Swin-BAPAT outperform the state-ofthe-art AdaptFormer-Swin with a big margin (using the smallest value sbottle = 32 can improve AdaptFormer-Swin by almost 25%). While without using Adapter, our method still outperforms baselines AdaptFormer-Swin and BitFit with roughly similar amount of parameters. When sbottle is larger than 64, our Swin-BAPAT starts to perform better than full-tuning on both datasets with proper parameter-accuracy trade-off, validating the effectiveness of our Swin-BAPAT for PETL.\n\n2https://github.com/ShoufaChen/AdaptFormer/blob/main/PRETRAIN.md\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Top-1 accuracy (%) using different scalar values on two datasets: SSv2 and HMDB51. The dbottle is set to 128; pretraining is based on Kinetics 400.\n\nScalar s\n\nSSv2 HMDB51\n\nFull-tuning Tune FC Layer AdaptFormer-Swin\n\ns = 0.2 s = 0.5 s = 0.8 s = 1.0\n\n50.99 24.13 40.80\n\n47.46 52.84 53.36 53.29\n\n71.28 68.07 68.66\n\n69.38 71.87 71.93 70.89\n\nFigure 4: Top-1 accuracy of different settings on SSv2 throughout training process. F: frame, S: scalar, B: dbottle, K: pre-training domain.\n\n3.3 THE EFFECT OF DIFFERENT PRE-TRAINING DOMAINS\n\nThe knowledge from the pre-trained model is learned from the source domain. We test two different models pre-trained on large-scale datasets: Kinetics 400, Kinetics 600, and ImageNet-22K. Findings show that both two models pre-trained on such large-scale datasets can benefit our proposed PETL strategy with the latter being slightly more significant (see the third group of comparison in Table 2). This is due to the fact that Kinectics 600 is larger than its 400 version and brings more knowledge to the pre-trained model, benefiting more downstream tasks. However, image-based pre-training cannot perform as good as video-based pre-training due to the larger domain gap.\n\n3.4 THE EFFECT OF DIFFERENT VIDEO INPUT SIZE\n\nWe also test whether our method is robust to increased number of input video frames. It is worth noting that larger number of input video frames usually can bring more spatial temporal information, benefiting data-driven models to learn more distinguishable features while keeping the model size remaining the same. The last group of comparisons in Table 2 shows that using double-sized video input (i.e., 16 frames) can greatly improve the performance of action recognition on both small and large-scale datasets. The improvements (increased 9.78% from 53.36% to 63.14% on SSv2, and 3.74% from 71.93% to 75.67% on HMDB51) are more significant than other factors such as dbottle and pre-training domain (around 1% to 2%). The top line in Figure 4 visualizes the significant effect of increasing the number of input video frames. These results suggest that our Swin-BAPAT can be promising for increased frames of video input.\n\n3.5 THE EFFECT OF DIFFERENT SCALE OF PATT\n\nRecall that the effect of our PATT on pretrained models can be adjusted by the variable s in Eq. 10. Table 3 shows that adopting the value of 0.8 can deliver consistent best performances on both datasets SSv2 and HMDB51 under our experimental setting. Smaller values of s will quantitatively reduce the effect of our PATT module on the knowledge transfer while large values will increase the effect of our PATT module. The good performance achieved via taking an effective scale of 0.8 indicates that our PATT module plays an important role in the knowledge transfer. However, even larger values over 0.8 can affect the importance of original knowledge thereof the pretrained model. Hence, proper valued scalar s is essential for balancing the role of PATT and\n\nTable 4: Ablation of different implementation positions of PATT defined in Eq. 10, e.g., Ours (K, V) indicates inserting PATT to the query and key of 3DSW-MSA modules. Pre-training on Kinetics 600. dbottle is set to 128; Scalar s is set to 0.8.\n\nMethod\n\nFull-tuning Concat (K, V) No Z l−1 (K, V) Ours (Q, K) Ours (K, V) Ours (Q, V) Ours (Q, K, V)\n\nSSv2\n\nHMDB51\n\n# Params Top-1 87.82M 50.99 6.38M 15.61 8.74M 51.06 6.38M 45.49 6.38M 53.38 6.38M 53.24 7.93M 53.23\n\n# Params Top-1 87.69M 68.07 6.20M 20.98 8.56M 67.41 6.20M 68.92 6.20M 71.41 6.20M 71.74 7.63M 69.57\n\n8\n\n Top-1 Accuracy (%) Number of Epoch 010203040506070147101316192225283134374043464952555861646770F8_S0.8_B256_K400F8_S0.8_B256_K600F8_S0.8_B128_K400F8_S0.8_B64_K400F8_S0.8_B32_K400F8_S0.5_B64_K400F8_S1.0_B64_K400F16_S0.8_B128_K400AdaptFormer-SwinTune FC LayerFull-tuningUnder review as a conference paper at ICLR 2023\n\npre-trained backbone model. Note this can be a learnable parameter upon specific implementation, here we empirically verified the effect of the scalar.\n\n3.6 THE EFFECT OF DIFFERENT METHODS YIELD FROM V-PETL\n\nWe have argued that, especially for relative large downstream datasets, the position and the amount of trainable parameters are important for parameter-efficient transfer learning in Section 2.4. The proposed Swin-BAPAT is one of instantiated models from the V-PETL framework regarding the insert position of our PATT. Other instantiations can be inserted into different positions such as query, key, and value of the attention module. We further instantiate other variations of our Swin-BAPAT by inserting PATT to different positions. Table 4 shows the results. Findings show that inserting to the value position of 3DSW-MSA can contribute more than inserting to other two positions. While inserting to query of key makes little difference for the performance. This is due to the fact that query and key make the calculation of the attention mask. Hence, inserting either one of them will lead to a similar effect. On one hand, these results, to some extent, justify the original design of prefix-tuning that bring learnable prefix to key and value of the attention module. On the other hand, it indicates that our claim regarding the unified view of PETL for visual tasks is reasonable. In Table 4, we also ablate the designs of PATT regarding concatenating Kp and Vp (i.e., Concat [K, V]), and using trainable parameters to generate Kp and Vp (i.e., No Z l−1 [K, V]).\n\n3.7 COMPARISON ON VARIED TASKS VIA SELF-SUPERVISED PRE-TRAINED MODELS\n\nTable 5 shows the comparison with AdaptFormer-64 (Chen et al., 2022) and VPT (Jia et al., 2022) on both image- and video-based downstream tasks. Our method ViT-BAPAT still shows promising parameter-accuracy trade-off via much smaller batch size, which is more convenient for reproduction on the general single server with 8 GPUs. The underperformance on SSv2 (better than full-tuning) can be due to the smaller batch size as SSv2 is much larger than other compared datasets and can be more relying on larger batch size. In real-world application scenarios, small dataset can be the more common case, which confirms our contributions.\n\nTable 5: Comparison of Top-1 accuracy via ViT-B models from MAE and VideoMAE pre-trained with self-supervised learning for image and video datasets, respectively.\n\nMethod\n\nImage SVHN 97.67\n\nCIFAR-100 85.90\n\nFull-tuning Tune FC Layer 94.02 (-3.65) 82.44 (-3.46) VPT (Jia et al., 2022) 0.08 (0.09%) 1.26 (1.46%) AdaptFormer-64 96.89 (-0.78) 85.90 (0.00) 2.13 (2.47%) 86.29 (+0.39) 97.18 (-0.49) Our ViT-BAPAT-32 Our ViT-BAPAT-64 3.02 (3.51%) 86.35 (+0.45) 97.18 (-0.49) Our ViT-BAPAT-128 4.79 (5.56%) 86.47 (+0.57) 97.28 (-0.39) Our ViT-BAPAT-256 8.33 (9.68%) 86.55 (+0.65) 97.24 (-0.43)\n\nAvg. Params (M) 86.04 (100%) 0.07 (0.08%) 69.83 (-16.07) 66.91 (-30.76) 69.74 (-20.35) 29.23 (-24.74) 49.84 (+3.43) 82.98 (-7.11) 43.73 (-10.24) 52.67 (+6.26) 59.02 (+5.05) 55.69 (+9.28) 87.61 (-2.48) 57.78 (+3.81) 57.18 (+10.77) 87.37 (-2.72) 57.55 (+3.58) 57.18 (+10.77) 87.53 (-2.56) 56.97 (+3.00) 57.70 (+11.29) 87.66 (-2.43) 56.53 (+2.56) 57.31 (+10.90) 87.68 (-2.41)\n\nHMDB51 46.41\n\nFood-101 90.09\n\nSSv2 53.97\n\nVideo\n\n4 CONCLUSION\n\nIn this paper, we introduced a V-PETL framework for exploiting good parameter-accuracy tradeoff around adapting video-based pre-trained large models to downstream tasks. Our Swin-BAPAT method derived from the V-PETL with a variation of prefix-tuning known as PATT can effectively bring good parameter-accuracy trade-off on downstream tasks. The proposed PATT can be easily plugged to the attention module of other transformer-like models. Meanwhile, the amount of trainable parameter can be easily adjusted by the parameter dbottle. With small amount overhead on trainable parameters, our method performs significantly better than state-of-the-art method AdapFormer-Swin and full-tuning on the datasets SSv2 and HMDB51 via small batch size, validating our contribution to the literature of PETL. In the future we will test our proposed model on more action recognition datasets surveyed in Sun et al. (2022) under more learning regimes such as zero/few-shot learning, active learning and continual learning with other pre-training methods such as visual-language models. We will also explore other backbone models, activation functions for PATT, and PETL techniques such as LoRA for visual tasks.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components with random forests. In European conference on computer vision, pp. 446–461. Springer, 2014.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nJoao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299–6308, 2017.\n\nShoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. arXiv preprint\n\nAdaptformer: Adapting vision transformers for scalable visual recognition. arXiv:2205.13535, 2022.\n\nIan J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit number recognition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082, 2013.\n\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The” something something” video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pp. 5842–5850, 2017.\n\nJulia Gusak, Daria Cherniuk, Alena Shilova, Alexandr Katrutsa, Daniel Bershatsky, Xunyi Zhao, Lionel Eyraud-Dubois, Oleh Shliazhko, Denis Dimitrov, Ivan Oseledets, and Olivier Beaumont. Survey on efficient training of large neural networks. In Lud De Raedt (ed.), Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pp. 5494–5501. International Joint Conferences on Artificial Intelligence Organization, 7 2022. doi: 10.24963/ ijcai.2022/769. URL https://doi.org/10.24963/ijcai.2022/769. Survey Track.\n\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=0RDcd5Axok.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, 2022b.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9.\n\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n\nSer-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.\n\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nHildegard Kuehne, Hueihan Jhuang, Est ́ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision, pp. 2556–2563. IEEE, 2011.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2021.\n\nTianjiao Li, Qiuhong Ke, Hossein Rahmani, Rui En Ho, Henghui Ding, and Jun Liu. Else-net: Elastic semantic network for continual action recognition from skeleton data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13434–13443, 2021a.\n\nTianjiao Li, Jun Liu, Wei Zhang, Yun Ni, Wenqian Wang, and Zhiheng Li. Uav-human: A large benchmark for human behavior understanding with unmanned aerial vehicles. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16266–16275, 2021b.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, 2021.\n\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 441–459, 2020.\n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.\n\nShaoshan Liu, Liangkai Liu, Jie Tang, Bo Yu, Yifan Wang, and Weisong Shi. Edge computing for autonomous driving: Opportunities and challenges. Proceedings of the IEEE, 107(8):1697–1716, 2019.\n\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3202–3211, 2022.\n\nJunting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-to-video transfer learning for action recognition. arXiv preprint arXiv:2206.13559, 2022.\n\nHieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11557–11568, 2021.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll ́ar, and Laurens van der Maaten. Revisiting In Proceedings of the IEEE/CVF weakly supervised pre-training of visual perception models. Conference on Computer Vision and Pattern Recognition, pp. 804–814, 2022.\n\nZehua Sun, Qiuhong Ke, Hossein Rahmani, Mohammed Bennamoun, Gang Wang, and Jun Liu. Human action recognition from various data modalities: A review. IEEE transactions on pattern analysis and machine intelligence, 2022.\n\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022.\n\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are dataefficient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602, 2022.\n\nXiaohan Wang, Linchao Zhu, Heng Wang, and Yi Yang. Interactive prototype learning for egocentric action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8168–8177, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui arXiv preprint\n\nCoca: Contrastive captioners are image-text foundation models.\n\nWu. arXiv:2205.01917, 2022.\n\nSha Yuan, Hanyu Zhao, Shuai Zhao, Jiahong Leng, Yangxiao Liang, Xiaozhi Wang, Jifan Yu, Xin Lv, Zhou Shao, Jiaao He, et al. A roadmap for big model. arXiv preprint arXiv:2203.14101, 2022.\n\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 1–9, 2022.\n\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104–12113, 2022.\n\nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu.\n\nNeural prompt search.\n\narXiv preprint\n\narXiv:2206.04673, 2022.\n\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1): 43–76, 2020.\n\nA APPENDIX\n\nA.1 THE EFFECT OF FC LAYER FOR SMALL SCALE DOWNSTREAM TASKS\n\nTable 6: Results of with or without tuning the FC layer on the small scale dataset HMDB51.\n\nMethod\n\ndbottle\n\nPre-training\n\n# Frames\n\nOur Swin-BAPAT Our Swin-BAPAT Our Swin-BAPAT Our Swin-BAPAT\n\nOur Swin-BAPAT Our Swin-BAPAT\n\nOur Swin-BAPAT Our Swin-BAPAT Our Swin-BAPAT\n\n32 64 128 256\n\n256 256\n\n128 128 128\n\nKinetics 400 Kinetics 400 Kinetics 400 Kinetics 400\n\nKinetics 400 Kinetics 600\n\nKinetics 400 Kinetics 400 Kinetics 400\n\n8 8\n8 8\n\n8 8\n\n8 16 32\n\nwith FC layer\n\nwithout FC layer\n\n# Params Top-1 (%)\n\n# Params Top-1 (%)\n\n2.79M 3.94M 6.25M 10.88M\n\n10.88M 10.88M\n\n6.25M 6.25M 6.25M\n\n65.97 67.28 66.75 67.67\n\n67.67 67.41\n\n66.75 70.56 74.82\n\n2.74M 3.89M 6.20M 10.83M\n\n10.83M 10.83M\n\n6.20M 6.20M 6.20M\n\n68.20 70.10 71.93 69.64\n\n69.64 69.90\n\n71.93 75.67 76.46\n\nFor the small dataset HMDB51, due to the good parameter-accuracy trade-off achieved by finetuning the FC layer only, adding the FC layer cannot bring extra improvement to our proposed method. Without sufficient taining data, full-tuning also cannot perform well (see results in Table 2). As such, small datasets do not need to rely on large models but can make use of large models with light transfer. Instead, without tuning the FC layer, our Swin-BAPAT can perform better than fine-tuning the FC layer with small amount of extra trainable parameters (see results in Table 6), validating the good parameter-accuracy trade-off of our method.\n\n12",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a new framework to adapt large video-based models to down-stream tasks with a parameter-accuracy trade-off. It analyzes different PETL techniques and investigates the importance of fine-tuning position of their methods. In order to better transfer prefix-tuning from NLP to vision task, it compares differences between NLP and video data regarding data structures and pre-training mechanisms.\n\n# Strength And Weaknesses\n\nStrengths:\n● The paper is well written and well structured.\n● The authors considers the situation of insufficient computing resources. Under the corresponding experimental settings, the experimental results demonstrate the superiority of the method proposed in paper.  \nWeakness:\n● As one of the baselines, Adaptformer can achieve excellent performance (59.02 on SSv2 and 55.69 on HMDB51) after larger batch size training.  Why does reducing the batch size has such a big impact on performance of Adaptformer? It is uncertain whether authors have explored the hyperparameters of Adaptformer under small batch size setting. In other words, authors can increase the batch size to compare the proposed method and Adaptformer.\n● The paper only shows the experimental results but does not elaborate on the reasons for the gains brought by the method.\nMinors:\n● There are some minor mistakes in the experimental table: first two lines in Table 1 appear to have numerical errors where \"0.11M\" is inconsistent with\"0.18M\" in Table 3, and \"Tune FC layer\" and \"Full-tuning\" should be swapped.\n\n# Summary Of The Review\n\nThe proposed method seems to achieve the best performance under the condition of limited computing resources. This paper should demonstrate the superiority of the method under fairer conditions or give sufficient reason analysis.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"Towards a Unified View on Visual Parameter-Efficient Transfer Learning\" presents a comprehensive framework for parameter-efficient transfer learning (PETL) tailored for video tasks. The authors propose a unified framework, termed visual-PETL (V-PETL), which includes an innovative prefix-tuning variant called parallel attention (PATT). Through empirical evaluations on several datasets, including Something-Something v2 and HMDB51, the authors demonstrate that their approach outperforms existing techniques such as AdaptFormer and full-tuning methods, achieving superior performance while optimizing the parameter-accuracy trade-off.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its systematic analysis of various PETL techniques and the introduction of PATT, which is informed by the specific challenges of video data. The empirical results are robust, showcasing significant improvements over existing methods. However, a potential weakness is the limited exploration of the theoretical underpinnings of why PATT performs better, which may leave readers seeking a deeper understanding of its mechanics. Additionally, while the paper includes a variety of datasets, a broader range of applications could enhance its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings, making it accessible to readers. The methodology is described in sufficient detail, allowing for reproducibility, particularly in the context of the experiments conducted. The novelty is evident in both the proposed framework and the PATT technique, with the authors providing a strong empirical basis for their claims. However, further clarification on the implementation details and the choice of hyperparameters could improve reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a valuable addition to the field of parameter-efficient transfer learning for video tasks, with a noteworthy framework and empirical results. While the contributions are strong, the paper could benefit from deeper theoretical insights and broader applications to enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to Parameter Efficient Transfer Learning (PETL) for video-based tasks, introducing a unified framework known as V-PETL. The authors specifically explore the prefix-tuning technique and propose a new variation called parallel attention (PATT) tailored for visual tasks. Through extensive empirical evaluations on video datasets (Something-something v2 and HMDB51) and image datasets (CIFAR-100, SVHN, Food101), the authors demonstrate that their method, Swin-BAPAT, outperforms existing PETL techniques in terms of parameter-accuracy trade-off, particularly excelling on larger datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative framework and comprehensive analysis of various PETL techniques, which provide valuable insights into their performance for vision tasks. The empirical results are robust, showcasing the effectiveness of Swin-BAPAT compared to established methods. However, the study has limitations, including potential scalability issues for users with less computational power and the restricted generalizability of findings across different datasets. Furthermore, the paper's baseline comparisons are somewhat limited, which could undermine the broader context of its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings, making it accessible to readers. The quality of the experimental design is commendable, with appropriate methodologies and thorough evaluations. The novelty of the proposed framework and techniques is significant, particularly in adapting PETL to video tasks. However, the reproducibility of results may be challenged due to the dependence on specific computational resources and the limited range of datasets used in the experiments.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of PETL by introducing a novel framework and empirical results that demonstrate its effectiveness for video tasks. Despite its strengths, the paper could benefit from broader baseline comparisons and a discussion on generalization across diverse datasets.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a unified framework for Parameter Efficient Transfer Learning (PETL) in visual tasks, specifically focusing on adapting large video-based models for downstream applications. It introduces the visual-PETL (V-PETL) framework, which includes a novel prefix-tuning variation termed parallel attention (PATT) and demonstrates its advantages through extensive empirical analysis on two video datasets: Something-something v2 and HMDB51. The proposed Swin-BAPAT model integrates PATT and an adapter design, showing improved parameter-accuracy trade-offs compared to other existing methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive analysis of various PETL techniques and the introduction of PATT, which effectively addresses the challenges of applying prefix-tuning to visual tasks. The empirical results indicate that Swin-BAPAT achieves superior performance, particularly in resource-constrained settings. However, the paper's limitations include a relatively narrow focus on specific datasets and tasks, which may affect the generalizability of the findings. Additionally, the methodology could benefit from a more detailed discussion on the implications of parameter scaling.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a solid methodology described in detail. The novel aspects of the proposed PATT and the V-PETL framework are presented in a manner that highlights their significance and utility. Reproducibility is bolstered by the thorough documentation of experimental settings and results, although providing more code or data access could enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper offers a significant contribution to the field of visual transfer learning by providing a unified framework that enhances parameter efficiency while maintaining performance. The innovative PATT method is a noteworthy advancement, although the scope of empirical evaluations could be broadened to ensure more comprehensive applicability across diverse scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a unified framework for parameter-efficient transfer learning (PETL) in visual tasks, referred to as V-PETL. The authors focus on prefix-tuning as a method for enhancing performance in vision tasks, addressing a gap in existing research. They conduct extensive empirical analysis using video datasets, demonstrating that their proposed Swin-BAPAT method achieves superior parameter-accuracy trade-offs compared to state-of-the-art techniques. The paper systematically analyzes various PETL techniques, the positioning of trainable parameters, and the impact of data scales, suggesting the potential for broader application of their methods across transformer-based models.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative framework, which provides a structured approach to PETL, and its focus on an under-explored technique—prefix-tuning for vision tasks. The extensive empirical validation reinforces the practical effectiveness of the proposed methods, particularly in demonstrating improved parameter-accuracy trade-offs, which is crucial for resource-constrained environments. However, the evaluation of novelty may be limited by the absence of comprehensive comparisons to existing frameworks, and the robustness of findings could be questioned due to the limited prior research on prefix-tuning. Additionally, the empirical results may benefit from a broader range of datasets and real-world applications to enhance generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that facilitates understanding of the framework and methodologies employed. The novelty of the proposed V-PETL framework is compelling, yet its significance may be undermined by a lack of thorough benchmarking against established methods. While the empirical results are presented in a clear manner, the reproducibility of the findings could be strengthened through more detailed descriptions of datasets and experimental setups.\n\n# Summary Of The Review\nOverall, this paper contributes valuable insights into parameter-efficient transfer learning for visual tasks and identifies practical challenges relevant to edge computing. However, the evaluation of its contributions may be limited due to a lack of comprehensive comparisons with existing frameworks and a need for broader empirical validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "## Summary Of The Paper\nThe paper presents a novel approach to visual parameter-efficient transfer learning (PETL) through a framework called the Visual Adaptation Framework (VAF). This framework incorporates an innovative adaptation method termed cross-modal attention (CMA), which enhances the performance of video-based models on various downstream tasks while optimizing the parameter-accuracy trade-off. Through extensive empirical evaluation across established video datasets such as Something-Something v2 and HMDB51, the authors demonstrate that CMA significantly outperforms existing methods, achieving superior performance with fewer trainable parameters.\n\n## Strength And Weaknesses\nThe main strength of the paper lies in its unified framework that synthesizes multiple PETL techniques, providing a comprehensive view of their impacts on transfer performance. The introduction of CMA is particularly noteworthy, as it effectively adapts the attention mechanism to suit video data's unique characteristics, creating a more robust model. However, the paper could benefit from a deeper exploration of CMA's scalability and potential integration with other PETL techniques, as its applicability to other visual tasks remains underexplored. Additionally, the empirical results, while strong, could be supported by more diverse datasets to further validate claims.\n\n## Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology and findings, making it accessible to readers in the field. The quality of the experiments is high, featuring detailed ablation studies and comprehensive comparisons with state-of-the-art methods. The novelty of the CMA approach positions the work uniquely within the domain of visual transfer learning. However, reproducibility could be enhanced by providing more granular implementation details and sharing code or datasets used in the experiments.\n\n## Summary Of The Review\nOverall, the paper makes a significant contribution to the field of visual parameter-efficient transfer learning by introducing a unified framework and an innovative adaptation method that enhances performance while reducing parameter complexity. The empirical results are compelling, though further exploration of scalability and broader applicability could strengthen the work's impact.\n\n## Correctness\n4/5\n\n## Technical Novelty And Significance\n5/5\n\n## Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces Adversarial Parameter-Efficient Training (APET), a novel approach to adversarial training aimed at enhancing model robustness against adversarial attacks while minimizing computational costs associated with full model retraining. It presents a unified framework for adversarial training techniques and explores less-studied methods, such as Adaptive Prompting and Layer Adapters, to improve the trade-off between parameters and accuracy. The authors validate their approach through extensive experiments on benchmark datasets, showing that APET achieves superior performance compared to existing adversarial training methods, particularly in resource-constrained scenarios.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to adversarial training, particularly the introduction of the Adversarial Parallel Attention (APA) technique, which enhances focus on adversarial features while maintaining efficiency. The empirical validation through rigorous experimentation reinforces the proposed framework's effectiveness. However, a notable weakness is the lack of clarity in some sections regarding the theoretical underpinnings of the methods, which could limit understanding for readers unfamiliar with the topic. Additionally, while comparisons with existing methods are made, a wider range of baseline techniques would provide a more comprehensive context for the reported results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with a clear presentation of methodologies and experimental results. However, some concepts, particularly those pertaining to the theoretical aspects of the proposed methods, require clearer explanations to enhance comprehension. The quality of the empirical results is high, demonstrating reproducibility potential, although details on the implementation of the proposed techniques could be elaborated further to facilitate easier replication by other researchers. The novelty of the proposed methods is significant, presenting fresh perspectives in the domain of adversarial training.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of adversarial training by proposing a parameter-efficient framework that balances robustness and efficiency. While the innovative aspects and empirical results are commendable, minor revisions for clarity and broader comparative analyses would strengthen the paper further.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel framework for visual parameter-efficient transfer learning (V-PETL) and presents a new technique called parallel attention (PATT) aimed at enhancing performance on vision tasks. It claims to provide a comprehensive analysis of existing parameter-efficient transfer learning (PETL) methods, positing that traditional fine-tuning approaches are outdated. The findings suggest only marginal improvements over existing methods, yet the authors frame these outcomes as significant advancements in the field.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ambition to redefine the landscape of PETL in visual tasks and its introduction of PATT as a potentially useful technique. However, the analysis presented is superficial, overlooking critical aspects of parameter efficiency and the complexities involved in model adaptation. Furthermore, the empirical validation is limited, and the performance gains are modest, raising concerns about the claims made regarding the revolutionary nature of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is compromised by exaggerated claims that may mislead readers regarding the significance of the proposed techniques. While the novelty of introducing PATT is acknowledged, its practical significance remains questionable due to limited empirical validation. Reproducibility is not adequately addressed, as the experimental setup lacks robustness and fails to explore the framework's performance across diverse datasets.\n\n# Summary Of The Review\nOverall, while the paper presents intriguing concepts in parameter-efficient transfer learning, it greatly exaggerates the significance of its contributions. The claims of revolutionary advancements do not align with the modest empirical outcomes, warranting a cautious approach from readers regarding its applicability and impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel framework for parameter-efficient transfer learning in visual tasks, termed visual-PETL (V-PETL). It emphasizes the prefix-tuning method, particularly through a new variant called parallel attention (PATT), aimed at enhancing video-based model adaptations. The authors present empirical results demonstrating that their proposed method, Swin-BAPAT, achieves significant improvements in accuracy over existing techniques like AdaptFormer-Swin, while utilizing fewer parameters. The findings suggest a promising trade-off between model performance and parameter efficiency.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its clear identification of a gap in the existing literature regarding parameter-efficient transfer learning, particularly for video tasks. The introduction of the V-PETL framework provides a structured approach to analyze various PETL techniques, which is valuable for the community. The empirical results are compelling, showcasing significant improvements in accuracy across several datasets. However, a notable weakness is the limited exploration of potential limitations or challenges posed by the proposed methods, such as the generalizability of results across different video contexts or datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a logical flow from the introduction through to the experimental results and conclusion. The methodology is clearly articulated, allowing for reproducibility, as implementation details and experimental setups are adequately described. The novelty of the proposed PATT method is evident, although the broader implications of its application in various scenarios require further discussion. Overall, the clarity and quality of the writing are commendable, enabling readers to grasp the significance of the findings easily.\n\n# Summary Of The Review\nThe paper presents a significant advancement in parameter-efficient transfer learning for video tasks through the introduction of the V-PETL framework and the PATT mechanism. While it demonstrates strong empirical results and clarity in presentation, it could benefit from a deeper exploration of its limitations and broader applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper investigates parameter-efficient transfer learning (PETL) methods in the context of both natural language processing (NLP) and computer vision. The authors propose that different pretraining mechanisms necessitate distinct PETL strategies and highlight the significance of tuning specific model parameters based on their positions within the architecture. They present empirical results demonstrating improvements in performance with their proposed methods on several tasks, particularly focusing on the constraints posed by edge devices.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its empirical validation of PETL strategies across diverse tasks, contributing to the discourse on efficient model tuning. However, the analysis exhibits several weaknesses, including a lack of critical examination of the assumptions regarding parameter efficiency and the generalization of findings across modalities. The paper tends to overstate the necessity of different approaches for NLP and vision without adequately addressing the fundamental similarities in pretraining. Additionally, the reliance on specific datasets raises concerns about the generalizability of the results, and the evaluation metrics used may overlook other important factors.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, although certain assumptions could be articulated more rigorously. The novelty of the contributions is evident, particularly in the exploration of parameter tuning strategies; however, the reproducibility may be hindered due to a lack of comprehensive details regarding the datasets and tasks used in the experiments. Furthermore, the paper does not provide a thorough analysis of alternative tuning strategies, which limits the understanding of the proposed methods' robustness.\n\n# Summary Of The Review\nOverall, the paper presents interesting contributions to the field of PETL but suffers from several critical assumptions that weaken the validity of its claims. While the empirical results are promising, the generalizability and applicability of the findings to broader contexts require further validation and exploration.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a novel unified framework called visual-PETL (V-PETL) aimed at enhancing parameter-efficient transfer learning (PETL) specifically for vision tasks, particularly when adapting large video models. The authors propose the PATT (parallel attention) module for prefix-tuning, which is integrated into the V-PETL framework to analyze the trade-offs between the number of trainable parameters and accuracy. Experimental results demonstrate that the proposed Swin-BAPAT model outperforms various baseline models across multiple datasets while utilizing fewer parameters, thereby validating the effectiveness of both the PATT module and the overall V-PETL framework.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of a significant problem in fine-tuning large pretrained models for vision tasks and the innovative approach of combining insights from both NLP and vision domains. The introduction of the PATT module is a noteworthy contribution that adds value to the framework. However, a potential weakness lies in the limited exploration of other PETL techniques, which might restrict the comprehensive understanding of the framework's applicability across a broader range of tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers familiar with the field. The methodology is robust, with a thorough experimental setup that enhances the quality of the findings. The novelty of the proposed PATT module and the V-PETL framework is evident, although the paper could improve its reproducibility by providing more detailed implementation guidelines or code availability.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of parameter-efficient transfer learning in vision tasks by introducing a coherent framework and a novel module that enhances performance while minimizing parameter usage. The findings are promising, although further exploration of PETL techniques could strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing transfer learning methodologies by proposing a unified framework that integrates various existing techniques. The authors introduce new components designed to improve both scalability and efficiency, and they validate their approach through empirical experiments on several benchmark datasets. The findings suggest that their method achieves superior performance compared to standard baselines, indicating its potential applicability across diverse tasks in the machine learning domain.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance:** The research addresses a crucial problem in the field, particularly regarding the optimization of transfer learning, which is highly relevant in contemporary machine learning applications.\n2. **Novelty:** The proposed framework introduces innovative ideas that add value to existing literature, showcasing a fresh perspective on established techniques.\n3. **Theoretical Foundation:** The authors provide a robust theoretical basis for their methodology, adequately motivating their approach and demonstrating an in-depth understanding of the subject.\n4. **Empirical Validation:** The empirical results are compelling, showing significant improvements over baseline methods, which supports the practical significance of the proposed approach.\n\n**Weaknesses:**\n1. **Clarity and Presentation:** Certain sections of the paper could benefit from improved clarity and organization, as some of the contributions may not be easily digestible for the reader.\n2. **Limited Scope of Experiments:** The experiments appear somewhat restricted in terms of the variety of datasets used, which may limit the generalizability of the findings.\n3. **Comparison with Existing Work:** The paper does not provide a sufficiently detailed comparison with related works, which could help clarify how the proposed approach fits within the broader landscape of transfer learning research.\n4. **Parameter Analysis:** The discussion surrounding parameter choices and their impact on performance is limited. A more thorough examination would enhance the understanding of the method's robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate, with several areas requiring more straightforward explanations and better organization to facilitate reader comprehension. The quality of the writing is adequate, but it could be improved in technical sections. The novelty of the approach is strong, as it proposes original ideas that contribute to the ongoing research in transfer learning. The reproducibility of the results would benefit from additional details regarding experimental setups and parameter choices.\n\n# Summary Of The Review\nThe paper makes a noteworthy contribution to the field of transfer learning by proposing a unified framework that enhances performance through innovative techniques. While the empirical results are promising, the paper would benefit from improved clarity, broader experimentation, and more in-depth comparisons with existing literature. Overall, the work is promising and worth considering for acceptance, pending revisions to address the highlighted issues.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a unified framework, termed visual-PETL (V-PETL), aimed at enhancing parameter-efficient transfer learning (PETL) for video-based tasks. It introduces a novel variation of prefix-tuning called parallel attention (PATT) which is particularly suited for visual applications. The proposed method demonstrates significant improvements in performance while using fewer parameters compared to existing state-of-the-art techniques. The research highlights the importance of managing parameter-accuracy trade-offs in adapting large models to specific downstream tasks, emphasizing the framework's potential for broader applications in the vision domain.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its comprehensive approach to unify various PETL techniques within the visual domain, addressing a clear gap in existing literature, particularly concerning prefix-tuning. The introduction of the PATT method marks a notable advancement, showing practical improvements in performance with reduced parameter counts. However, the paper could be seen as limited in its empirical evaluation, as it primarily focuses on video tasks without extensive testing across a wider variety of datasets or learning regimes. This narrow scope may restrict the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. The writing is accessible, making complex concepts understandable. The quality of the experiments appears solid, showcasing the effectiveness of the proposed methods. However, reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter choices. While the novelty of the proposed PATT method is evident, additional comparisons with a broader range of existing methods would strengthen the paper's contributions.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of visual parameter-efficient transfer learning by proposing a unified framework and introducing a novel tuning method. While the findings are promising, the empirical evaluation could be broadened to ensure the generalizability of the results.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a unified view on Parameter-Efficient Transfer Learning (PETL) specifically tailored for visual tasks, introducing a new variation of Prefix-tuning known as Parallel Attention (PATT). The authors propose a framework called V-PETL to analyze different PETL strategies and their performance implications. The empirical results showcase that the PATT method outperforms existing techniques, demonstrating significant enhancements in task performance by balancing parameter efficiency and accuracy across various datasets, including Something-something v2 (SSv2) and HMDB51.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive analysis of existing PETL techniques and the introduction of a novel approach (PATT) with empirical validation. The unified framework (V-PETL) provides a valuable structure for understanding and comparing different strategies in the context of visual tasks. However, the paper could be strengthened by providing more detailed discussions about the limitations of the proposed methods and a clearer exposition of the experimental results. While the findings are promising, more extensive testing across a wider range of datasets would enhance the robustness of the conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a logical flow from the introduction to the conclusion. The methodology is adequately described, allowing for reproducibility. However, some technical details, especially in the experimental settings, could benefit from further elaboration to ensure that other researchers can replicate the results reliably. The novelty of the PATT method and the V-PETL framework is evident, and the paper contributes a significant advancement in the field of parameter-efficient learning for vision tasks.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of Parameter-Efficient Transfer Learning in visual tasks through its introduction of the V-PETL framework and the PATT method. While the results are promising, further exploration of the limitations and a broader empirical evaluation would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Towards a Unified View on Visual Parameter Efficient Transfer Learning\" presents a novel framework, V-PETL, aimed at enhancing parameter-efficient transfer learning (PETL) techniques in vision tasks. The authors introduce a new method known as Parallel Attention (PATT), which serves as a variation of prefix-tuning tailored for video tasks. Through extensive empirical validation using the Swin Video Transformer as a backbone, the paper demonstrates significant performance improvements over existing methods across various datasets, including Something-something v2 (SSv2) and HMDB51.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its comprehensive analysis of various PETL techniques and the introduction of PATT, which fills a gap in existing literature by specifically addressing video tasks. The empirical results are robust, showcasing the advantages of the proposed method through ablation studies and performance comparisons against established baselines. However, a potential weakness lies in the limited exploration of other datasets and learning regimes, which may restrict the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear headings and a logical flow that enhances readability. The inclusion of figures and tables effectively supports the reported results. The technical language used is appropriate for the target audience, and the methodology is described in sufficient detail to facilitate reproducibility. The originality of the research is notable, as it addresses underexplored areas in visual PETL while providing empirical support for its findings.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of parameter-efficient transfer learning in vision tasks by introducing a novel framework and methodology. The empirical validation strengthens the claims made, although further exploration of different datasets could enhance its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper \"Towards a Unified View on Visual Parameter-Efficient Transfer Learning\" introduces a novel framework called visual-PETL (V-PETL), which aims to enhance Parameter Efficient Transfer Learning (PETL) techniques for visual tasks, particularly video-based ones. The authors propose a new variant of prefix-tuning called Parallel Attention (PATT) that addresses limitations in existing methods when applied to visual domains. Through empirical evaluations on various video datasets, the study demonstrates that PATT outperforms state-of-the-art methods like AdaptFormer-Swin while achieving superior parameter efficiency compared to traditional full-tuning approaches.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its comprehensive analysis of PETL methodologies and the introduction of PATT, which effectively adapts prefix-tuning for visual tasks. The empirical results are compelling, showcasing significant performance improvements and a favorable parameter-accuracy trade-off across multiple datasets. However, the paper could further strengthen its contribution by providing more detailed comparisons against a broader range of baseline methods and discussing the implications of the positional influence findings in greater depth.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of methodologies and results. The quality of the writing is high, and the mathematical formulations are presented clearly, aiding in the reader's understanding. The novelty of the proposed PATT approach is significant as it bridges a gap between NLP and visual tasks. However, while the empirical findings are promising, further details on the reproducibility of experiments, such as hyperparameter settings and computational resources, would enhance the paper's transparency.\n\n# Summary Of The Review\nOverall, this paper presents a substantial contribution to the field of parameter-efficient transfer learning in visual tasks, particularly through the introduction of the PATT method. The empirical results validate its effectiveness, making it a noteworthy advancement in the domain. However, additional clarity on experimental reproducibility could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework called V-PETL aimed at enhancing parameter-efficient transfer learning (PETL) techniques for vision tasks. The authors propose the use of prefix-tuning, supplemented by a parallel attention (PATT) module, to improve model performance on specific datasets. However, the paper lacks a compelling motivation for the necessity of this approach in comparison to established methods in natural language processing (NLP) and does not provide substantial empirical validation.\n\n# Strength And Weaknesses\nThe proposed V-PETL framework presents a new perspective on PETL for vision tasks; however, its contributions appear overstated. The rationale for adopting this framework is weak, as existing methods in NLP are already well-established. The authors' claim of prefix-tuning being \"under-explored\" in vision tasks is not convincingly supported, and the empirical results are limited in scope, relying heavily on a narrow set of datasets (SSv2 and HMDB51). Furthermore, the comparison against baseline methods is inadequate, which may lead to skewed interpretations of the results. While the PATT module is intended to offer significant improvements, it largely adapts existing concepts without substantial novelty.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by an insufficiently detailed discussion on the limitations of full-tuning and the implications of the proposed methods. The reproducibility of the empirical results is questionable due to the high computational demands required for the experiments, limiting accessibility. The novelty of the contributions is minimal, as incremental improvements are emphasized over groundbreaking advancements in the field of PETL for vision tasks.\n\n# Summary Of The Review\nOverall, the paper presents a framework that lacks strong justification or substantial novelty in the context of parameter-efficient transfer learning for vision tasks. The empirical analysis is limited and raises concerns about reproducibility and generalizability, overshadowing the potential contributions of the proposed methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel framework named Visual-PETL (V-PETL), which aims to unify various parameter-efficient transfer learning (PETL) techniques for visual tasks, particularly in video data. The authors introduce an innovative component called Parallel Attention (PATT), designed to enhance prefix-tuning methods, thereby achieving significant performance improvements while utilizing a minimal number of tunable parameters. Empirical evaluations demonstrate that the SwinBAPAT model, derived from the V-PETL framework, outperforms existing state-of-the-art methods and exhibits a strong parameter-accuracy trade-off.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach and comprehensive framework that effectively consolidates different PETL techniques. The introduction of the PATT module is particularly noteworthy, offering a new method for improving video-based tasks. Additionally, the robust empirical validation across multiple datasets highlights the versatility and applicability of the proposed methods. However, the paper could benefit from a more detailed discussion on the limitations of the V-PETL framework and the PATT module, as well as their scalability across diverse applications beyond video data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of the methodology and findings. The quality of the experiments is high, and the results are clearly articulated. The novelty is significant, particularly due to the introduction of the PATT module and its implications for the PETL landscape. Reproducibility is addressed through comprehensive empirical validation; however, providing additional implementation details and code availability would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper offers a substantial contribution to the field of visual parameter-efficient transfer learning by presenting a unified framework and an innovative technique, PATT. The empirical results are impressive and indicate a promising direction for future research and applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper introduces a theoretical framework for Visual Parameter-Efficient Transfer Learning (V-PETL), which seeks to optimize the performance of visual models through the strategic manipulation of trainable parameters. By analyzing the interactions and positional importance of different PETL techniques, such as adapter modules, prefix-tuning, and prompt-tuning, the authors highlight the need for tailored strategies that leverage the distinct characteristics of visual data. The findings suggest that thoughtful adjustments in parameter placement can enhance model accuracy while maintaining computational efficiency, especially relevant for deployment in resource-constrained environments.\n\n# Strength And Weaknesses\nThe paper presents significant contributions by providing a comprehensive theoretical analysis of PETL techniques within the visual domain, emphasizing the unique structural considerations that differentiate visual and NLP tasks. The comparative discussions and identification of the parameter-accuracy trade-off are particularly insightful, offering a structured approach to enhance model performance without excessive computational costs. However, the paper could benefit from more empirical validation to support the theoretical claims made, as well as clearer examples or case studies demonstrating the practical application of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its arguments clearly, although some sections may require additional elaboration to improve comprehension for readers less familiar with the PETL landscape. The quality of the theoretical discourse is high, effectively synthesizing existing knowledge while proposing novel insights into parameter efficiency in visual tasks. However, the reproducibility of the theoretical findings would benefit from explicit methodologies or frameworks that can be readily applied in practical settings, as the current focus leans heavily on theoretical exploration.\n\n# Summary Of The Review\nOverall, the paper provides a valuable theoretical framework for understanding Visual Parameter-Efficient Transfer Learning, highlighting the necessity for tailored strategies in the visual domain. While the insights are compelling and well-articulated, the lack of empirical validation and practical examples limits the immediate applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Towards a Unified View on Visual Parameter-Efficient Transfer Learning\" presents the Visual-PETL (V-PETL) framework aimed at investigating various Parameter-Efficient Transfer Learning (PETL) techniques for vision tasks, particularly in video-based models. The authors analyze techniques such as prompt-tuning, adapter methods, and a novel variation called Parallel Attention (PATT) using a Video Swin Transformer architecture. Through extensive experiments on datasets like Something-Something v2 and HMDB51, the authors demonstrate that their methods can achieve significant parameter-accuracy trade-offs, with Swin-BAPAT outperforming existing models while utilizing fewer parameters.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough investigation of multiple PETL techniques and their empirical performance across various datasets, showcasing the adaptability of the PATT module. The use of a well-established architecture like the Video Swin Transformer enhances the credibility of the results. However, the paper has some weaknesses, primarily in its limited exploration of broader theoretical implications and a lack of comprehensive discussions on the generalization of findings beyond the tested datasets. Furthermore, the presentation of results could benefit from clearer visualizations and comparisons to emphasize the advantages of V-PETL.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with clear implementation details and experimental settings, allowing for reproducibility. However, the clarity of some sections could be improved, particularly in articulating the significance of the PATT module and its flexibility in various configurations. The novelty is present in the proposed PATT technique; however, more context on its theoretical underpinnings would strengthen the paper's contribution to the field. The code availability enhances reproducibility, but the analysis could benefit from additional empirical validation across a broader range of tasks.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of Parameter-Efficient Transfer Learning in vision tasks, particularly through the introduction of the PATT module. While the empirical results are promising and the methodology is sound, the paper could be improved by offering deeper theoretical insights and enhancing clarity in presentation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the V-PETL framework aimed at providing a unified perspective on parameter-efficient transfer learning (PETL) for both NLP and vision tasks. The authors propose a novel parallel attention mechanism (PATT) and present empirical results that claim significant improvements over existing frameworks, particularly AdaptFormer and VPT. However, the paper lacks a thorough comparison with these established methods and does not sufficiently justify the claimed advantages of their approach.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its ambition to unify PETL techniques across different domains; however, it is marred by several weaknesses. The authors fail to adequately position their work within the existing literature, often overlooking significant advancements made by prior research, such as VPT and BitFit. The empirical claims made regarding the performance of their methods lack robust supporting evidence and could benefit from clearer experimental setups. Additionally, the paper does not sufficiently engage with theoretical foundations, which weakens the credibility of its findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the novelty of its contributions is overstated without sufficient context or comparison to existing works. The quality of the experimental results raises concerns due to a lack of rigorous baseline comparisons and a discussion of potential confounding factors. Reproducibility is not adequately addressed, as the implementation complexity of their method is not compared with other established techniques.\n\n# Summary Of The Review\nOverall, the paper presents a potentially valuable framework for parameter-efficient transfer learning but fails to provide a compelling argument for its novelty or superiority over existing methods. The lack of thorough literature engagement and empirical validation undermines its contributions. \n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"TOWARDS A UNIFIED VIEW ON VISUAL PARAMETER EFFICIENT TRANSFER LEARNING\" presents a comprehensive framework for understanding various approaches to parameter-efficient transfer learning (PETL) in visual tasks. The authors propose a unified model that consolidates different PETL methods, including fine-tuning, Prefix-tuning, and others, under a single theoretical umbrella. Utilizing both theoretical analysis and empirical evaluations across standard datasets, the findings demonstrate that the proposed unified view effectively enhances the performance of visual models while significantly reducing the number of trainable parameters.\n\n# Strength And Weaknesses\nThe strengths of the paper include its ambitious goal of unifying various PETL approaches, which could greatly aid the community in understanding and applying these methods. The theoretical insights provided are valuable, and the empirical results support the claims made. However, the paper could benefit from clearer definitions and consistent formatting throughout. Some sections contain typographical errors and inconsistencies that may hinder reader comprehension and detract from the overall professionalism of the presentation.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper's contributions are significant, the clarity is occasionally undermined by formatting issues, including inconsistent terminology and unclear figure legends. The quality of the empirical evaluations is commendable, but the reproducibility could be enhanced by providing more detailed descriptions of the methodologies and datasets used. The novelty of the proposed unified framework is noteworthy, as it addresses a critical gap in the literature regarding PETL techniques.\n\n# Summary Of The Review\nOverall, the paper represents a valuable addition to the field of parameter-efficient transfer learning by providing a unified framework that enhances understanding and application. Nevertheless, the paper requires improvements in clarity and consistency to fully realize its potential impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces the V-PETL (Video-based Parameter-Efficient Transfer Learning) framework aimed at enhancing video-based action recognition tasks. The authors propose novel methodologies for model adaptation and fine-tuning, demonstrating improvements in accuracy and efficiency. The empirical results suggest that the V-PETL framework outperforms existing parameter-efficient transfer learning techniques, particularly in specific controlled environments. However, the scope of the framework's applicability to other tasks and real-world scenarios is not fully explored.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to parameter-efficient transfer learning and its empirical validation, showcasing significant performance improvements over existing methods. However, the limitations include a narrow focus on video-based tasks without exploration of broader applications, insufficient comparison with a wider range of existing techniques, and a lack of discussion on real-time applications and scalability. Additionally, the robustness of the proposed methods to noise and adversarial attacks is not adequately addressed, which could undermine their reliability in practical settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a well-structured presentation of methodologies and results. However, the quality of the discussion is lacking, particularly in areas that would enhance understanding of the framework's limitations and potential improvements. The novelty of the proposed methodologies is evident, but the paper does not sufficiently explore their reproducibility across different datasets and tasks, nor does it provide detailed insights into hyperparameter tuning or alternative model configurations.\n\n# Summary Of The Review\nOverall, while the V-PETL framework presents an interesting advancement in parameter-efficient transfer learning for video-based action recognition, it falls short in exploring broader applicability and real-world relevance. The paper would benefit from a more comprehensive analysis of its limitations and a deeper examination of its robustness and interpretability in various contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates Parameter Efficient Transfer Learning (PETL) strategies, focusing on the proposed Swin-BAPAT model and its performance across various tasks such as video and image classification. The methodology includes a comprehensive experimental framework utilizing multiple datasets (e.g., Something-something v2, CIFAR-100) and a thorough analysis of the parameter-accuracy trade-off. The findings indicate significant performance improvements, with the Swin-BAPAT model achieving up to a 25% higher Top-1 accuracy compared to existing models like AdaptFormer-Swin, demonstrating the effectiveness of tuning specific parameters and the importance of dataset scale.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous statistical methodology and comprehensive experimental setup, which effectively highlight the differences between PETL techniques. The inclusion of ablation studies and a detailed examination of parameter tuning positions adds depth to the analysis, providing valuable insights into model performance. However, a notable weakness is the lack of explicit detail regarding the statistical tests used for significance validation, which could raise concerns about the robustness of the reported results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that outlines the methodology and findings. However, the clarity would benefit from more explicit descriptions of the statistical tests employed, making it easier for readers to assess the reproducibility of the results. The novelty lies in the proposed Swin-BAPAT model and its comparative analysis, contributing to the ongoing discourse on PETL strategies. While the methodology is solid, the reproducibility may be hindered by the insufficient detail on statistical validation.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of transfer learning, particularly in its exploration of parameter efficiency. While the findings are compelling and well-supported by empirical evidence, the lack of detailed statistical methodology could impact the perceived robustness of the results.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel framework for parameter-efficient transfer learning (PETL) specifically tailored for video data, leveraging prefix-tuning techniques. The proposed methodology aims to enhance performance on large-scale datasets, such as Kinetics 400 and 600, while maintaining a focus on efficiency in adapting pre-trained models. Key findings indicate significant improvements in task performance when using the proposed methods compared to traditional fine-tuning approaches.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to applying prefix-tuning to video tasks, which is relatively under-explored in the literature. However, several weaknesses are evident. The reliance on large datasets raises concerns about the applicability of the methods in smaller data scenarios. Additionally, the study does not provide a thorough comparison with other recent PETL techniques, limiting the contextual understanding of its contributions. There is also a lack of robustness analysis, particularly regarding how the methods perform under varying conditions, which could impact their real-world usability. \n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of its methodology and findings. However, the novelty is somewhat tempered by the limited exploration of prefix-tuning outside of video tasks and the absence of robust comparisons with existing techniques. Reproducibility may be hindered by the dependency on specific large datasets and batch sizes, which may not be accessible to all researchers. \n\n# Summary Of The Review\nOverall, the paper presents a promising approach to PETL in video tasks, but it is marred by limitations in dataset dependency, generalizability, and a lack of comprehensive comparisons with other methods. While the methodology shows potential, there are critical areas for improvement that could enhance its applicability and robustness.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Towards a Unified View on Visual Parameter-Efficient Transfer Learning\" presents a review and exploration of parameter-efficient transfer learning (PETL) methods, particularly in the context of visual tasks. The authors propose to unify existing approaches, including full-tuning, adapter modules, and prefix-tuning, in a detailed manner. They introduce the Video Swin Transformer as a novel architecture and conduct empirical analyses to demonstrate its effectiveness over existing methods, claiming improvements in performance metrics.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its comprehensive review of existing PETL techniques and the introduction of the Video Swin Transformer. However, these contributions are undermined by a lack of novelty, as many concepts discussed have been previously established in the literature. The paper appears to recycle well-known ideas without adding significant insights or advancements. Additionally, the authors' framing of common practices as innovative detracts from the perceived rigor of their claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the methodology is elaborately explained, the excessive detail may lead to confusion rather than clarity. The quality of writing is undermined by a tendency to overstate the novelty of the contributions. The paper's reproducibility is compromised by its failure to provide clear guidelines or sufficient detail for replicating its findings beyond the presented results. Overall, the clarity of the paper suffers due to convoluted language and inflated claims.\n\n# Summary Of The Review\nThis paper attempts to present a unified approach to visual parameter-efficient transfer learning but falls short of delivering original insights or significant advancements. The work feels like a reiteration of established methods, lacking both clarity and novelty.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel approach to parameter-efficient transfer learning (PETL) techniques for visual tasks, focusing on a method termed parallel attention (PATT) within the V-PETL framework. The authors empirically validate their method, demonstrating promising parameter-accuracy trade-offs while addressing the challenges of full-tuning large models, particularly in resource-constrained environments. The findings emphasize the impact of tuning positions and the potential of prefix-tuning, suggesting avenues for future research in zero-shot and continual learning.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to PETL, providing a valuable contribution to the field of transfer learning in vision tasks. The empirical validation strengthens its claims regarding the effectiveness of PATT and the framework's applicability to other transformer-based models. However, the paper could benefit from a more detailed comparative analysis with existing methods, particularly those that utilize adaptations of attention mechanisms. Additionally, the reliance on large batch sizes for training may limit the practical applicability of the proposed techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, making it accessible to a broad audience. The quality of the experiments is high, though the reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameters used. While the novelty of the proposed PATT method is apparent, further context on how it compares to existing approaches would strengthen the overall presentation.\n\n# Summary Of The Review\nOverall, the paper provides a compelling contribution to the field of parameter-efficient transfer learning for vision tasks, with a novel method and promising empirical results. However, the need for more comparative analysis and attention to training constraints slightly diminishes its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Swin-BAPAT, a novel method designed for video-based action recognition, evaluated against benchmark datasets including Kinetics 400, Kinetics 600, Something-Something v2 (SSv2), and HMDB51. The authors report a substantial improvement in performance over the existing model, AdaptFormer-Swin, achieving nearly 25% better accuracy on SSv2 with the smallest parameter setting. Key findings include a peak accuracy of 54.06% on SSv2 and competitive results on HMDB51, highlighting the method's efficiency in leveraging fewer parameters while maintaining performance. The paper also presents extensive ablation studies that demonstrate the significance of parameter tuning in attention modules compared to MLP layers, and the model's robust performance across varying input sizes.\n\n# Strength And Weaknesses\nStrengths of the paper include the significant performance improvements demonstrated on multiple datasets, particularly with a focus on parameter efficiency, which is crucial for real-world applications. The thorough ablation studies effectively elucidate the impact of different tuning strategies, providing valuable insights into model optimization. However, a potential weakness is the lack of a detailed comparison with a broader range of existing methods, which could further contextualize the contributions of Swin-BAPAT within the current literature.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions and findings. The methodology is described in sufficient detail, allowing for reproducibility, although additional specifics on implementation details could enhance clarity. The novelty of the approach is notable, particularly in its parameter-efficient design, although the concept of tuning attention modules is not entirely new. Overall, the quality of writing and presentation is high, making the findings accessible to the target audience.\n\n# Summary Of The Review\nOverall, the paper presents a compelling method for video-based action recognition that effectively balances performance and parameter efficiency. The empirical results are robust, and the insights from ablation studies add significant value to the work, although broader comparisons with existing methods could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"TOWARDS A UNIFIED VIEW ON VISUAL PARAMETER EFFICIENT TRANSFER LEARNING\" aims to consolidate various approaches in the field of Visual Parameter Efficient Transfer Learning (PETL). The authors propose a unified framework that integrates different PETL methods, providing theoretical insights and practical implications. The methodology includes a comprehensive literature review, the introduction of new concepts, and empirical evaluations across multiple benchmarks. The findings demonstrate that the proposed framework significantly enhances the performance of existing PETL techniques while maintaining computational efficiency.\n\n# Strength And Weaknesses\nThe main strengths of the paper are its comprehensive literature review, the introduction of a unified framework that bridges different PETL methodologies, and the empirical validation across various datasets, which showcases the applicability of the proposed approach. However, the paper exhibits weaknesses in clarity and accessibility due to complex jargon and lengthy sentences. Furthermore, the empirical results presentation could be improved for better readability, and there are minor issues with grammar and consistency that detract from the overall quality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by the heavy use of jargon, complex sentence structures, and instances of repetition which may alienate readers not deeply familiar with the topic. The quality of writing could benefit from a thorough proofreading and restructuring for improved flow and readability. In terms of novelty, the framework presented is significant as it attempts to unify disparate PETL methods, but the presentation of empirical results lacks clarity, making reproducibility challenging. The appendix, while containing relevant information, also lacks sufficient context for readers to understand its importance.\n\n# Summary Of The Review\nOverall, the paper presents a promising unified framework for PETL, with strengths in its theoretical contributions and empirical validation. However, significant improvements in clarity, organization, and presentation are needed to enhance accessibility and reader engagement.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.5931833204877717,
    -1.7343287364338127,
    -1.8227765145042922,
    -1.887741843162033,
    -2.0368068641775907,
    -1.7053064560961293,
    -1.7225066799289876,
    -2.0233144470594784,
    -1.8972711886956266,
    -1.9614612958508408,
    -1.8030455703349477,
    -1.605528098290983,
    -1.8438408392758463,
    -1.7017222244754542,
    -1.8578796265437338,
    -1.8501474476236721,
    -1.962395034799546,
    -1.7498069979923647,
    -1.823309014419115,
    -1.8282067236358528,
    -2.0066741491219746,
    -1.7291678620630913,
    -1.6153714176559535,
    -1.8645606584354781,
    -1.6436606453106315,
    -1.9657163701974854,
    -1.9027723814996318,
    -1.7746849297214438,
    -1.7979362829936072
  ],
  "logp_cond": [
    [
      0.0,
      -2.253092461442118,
      -2.2856405844192067,
      -2.262484292150913,
      -2.269174774944919,
      -2.276721053994755,
      -2.412932771298027,
      -2.362342508173878,
      -2.2947413675710457,
      -2.329860107669384,
      -2.308563356864124,
      -2.4008172601627815,
      -2.2908014504376184,
      -2.313259913747008,
      -2.301441775062951,
      -2.2601249504840335,
      -2.328802934310115,
      -2.307533392567897,
      -2.3044178018890897,
      -2.2776956747497166,
      -2.3260871890561483,
      -2.3829311949988923,
      -2.3483377388615287,
      -2.32926861111367,
      -2.3284789948602813,
      -2.3287390356575948,
      -2.2570349291706764,
      -2.340440723560044,
      -2.369881871280829
    ],
    [
      -1.357060560548734,
      0.0,
      -1.1266379404107074,
      -1.1528753507804597,
      -1.2147406790045148,
      -1.2522535210842853,
      -1.3734820377146848,
      -1.2767137802706772,
      -1.154259948670544,
      -1.339653839082905,
      -1.226306765262783,
      -1.3894671138918726,
      -1.1446400698382022,
      -1.153436317237584,
      -1.1220391124671636,
      -1.1061932368717262,
      -1.2576602013214202,
      -1.228204095003232,
      -1.2911149604910592,
      -1.1272933104302312,
      -1.2743586365950246,
      -1.2896628255717928,
      -1.3617307632218836,
      -1.2859844818834016,
      -1.3615374580236121,
      -1.2545907742079703,
      -1.1769795283495592,
      -1.328011381147035,
      -1.3307610434192645
    ],
    [
      -1.5168710905641125,
      -1.3139566235268154,
      0.0,
      -1.2575387320281688,
      -1.2777161196469333,
      -1.4378762243698908,
      -1.4718849304029877,
      -1.4472201352197833,
      -1.2909012915507045,
      -1.5024562146515446,
      -1.3257965834566814,
      -1.526078940737256,
      -1.3613325848999933,
      -1.3879218561162103,
      -1.3792744707734108,
      -1.3707374857238728,
      -1.416137156737683,
      -1.2847850223857857,
      -1.445294306673813,
      -1.2967706194056206,
      -1.4430130477118297,
      -1.4523227605916311,
      -1.4923777081494423,
      -1.3484483595572616,
      -1.4548508928297936,
      -1.4917316666459786,
      -1.3613308006886886,
      -1.4728993398967045,
      -1.4619099410681122
    ],
    [
      -1.507945949520054,
      -1.2969733719393244,
      -1.2239490222802845,
      0.0,
      -1.2464552339527366,
      -1.4359378264394234,
      -1.5657967191105175,
      -1.478475597326101,
      -1.2575094233865052,
      -1.549700806070135,
      -1.2878608215888436,
      -1.5888067369672774,
      -1.3130023630388683,
      -1.3403828072652695,
      -1.363737612034701,
      -1.3548403655508436,
      -1.4706675353720877,
      -1.3390945798009042,
      -1.4794741117453205,
      -1.2561466546766094,
      -1.4793538655217984,
      -1.5466662789167287,
      -1.565062869187829,
      -1.3803591852323374,
      -1.57958823047494,
      -1.5044971072674473,
      -1.411851877523847,
      -1.4680041016705012,
      -1.5701263950525173
    ],
    [
      -1.7319218201721227,
      -1.6244321571406046,
      -1.5163426475806971,
      -1.5007540711855485,
      0.0,
      -1.7068164438521571,
      -1.7881334099757704,
      -1.6747036390448604,
      -1.5642659998758812,
      -1.6978160669123135,
      -1.56136679433644,
      -1.773571849777386,
      -1.6083165971259885,
      -1.648941716145544,
      -1.6451923139498423,
      -1.6423321146576457,
      -1.655496712545931,
      -1.5519071844667942,
      -1.6771145072632612,
      -1.5587234730033714,
      -1.6713425757440796,
      -1.6938494177572587,
      -1.7117898548574468,
      -1.6219128128520064,
      -1.7109101714337809,
      -1.6553611793646816,
      -1.5902959476168625,
      -1.7289557343496122,
      -1.7293902837245456
    ],
    [
      -1.429535681312241,
      -1.2447945413759838,
      -1.25049746538885,
      -1.2663709186676584,
      -1.3118406259346225,
      0.0,
      -1.357021670147882,
      -1.3544265439470722,
      -1.3270598400978988,
      -1.3971468085738556,
      -1.3221285037168273,
      -1.46832172077378,
      -1.3226307431743318,
      -1.3206597129369064,
      -1.3042941907444547,
      -1.3140467275600642,
      -1.3543131536159196,
      -1.2479605111799268,
      -1.3505403936878302,
      -1.2495057744703848,
      -1.3536310218498848,
      -1.3129941669319174,
      -1.4033635203008974,
      -1.3253213149290737,
      -1.3602930378724918,
      -1.3799050807746571,
      -1.2936739598129636,
      -1.3872755794760856,
      -1.3547701945131787
    ],
    [
      -1.4971326978973407,
      -1.352238705234485,
      -1.330223591318174,
      -1.3510146462090733,
      -1.3174373034055245,
      -1.354501828714677,
      0.0,
      -1.3780052490225017,
      -1.359165860443744,
      -1.3977279619617755,
      -1.3848147599568927,
      -1.3991321409080233,
      -1.369225926741711,
      -1.3578359691175748,
      -1.3964155407466947,
      -1.386352126940412,
      -1.3818241847813237,
      -1.3319663593463456,
      -1.36678514628101,
      -1.3347315376643152,
      -1.361912281384816,
      -1.3543461648352433,
      -1.359913982811859,
      -1.3429565914669965,
      -1.3776713044578786,
      -1.397384961692483,
      -1.3256658878424108,
      -1.4117074465042285,
      -1.3537270873340226
    ],
    [
      -1.772112494855052,
      -1.5006066927293866,
      -1.5459803649448394,
      -1.485917728738046,
      -1.4811490539093173,
      -1.590917188660424,
      -1.6461987799532367,
      0.0,
      -1.4298330088020341,
      -1.5533199153351902,
      -1.4511138785440427,
      -1.6740675767178574,
      -1.482890720158064,
      -1.470104855936335,
      -1.4731909006755306,
      -1.508670584205778,
      -1.478407009396524,
      -1.5491133937612913,
      -1.5301748285170595,
      -1.4677517956635946,
      -1.4573049563451101,
      -1.6016448775829941,
      -1.4957700923230213,
      -1.6415237187908829,
      -1.608096155384563,
      -1.5493486831537964,
      -1.483243070124538,
      -1.611748673734003,
      -1.591651688868518
    ],
    [
      -1.5763712284345206,
      -1.2929049237686303,
      -1.256527670173858,
      -1.2481324085297154,
      -1.3156163173903115,
      -1.4607533186523731,
      -1.5293228563981307,
      -1.4009278776396439,
      0.0,
      -1.4937197216837321,
      -1.3232624583723653,
      -1.607190714902845,
      -1.339972698455954,
      -1.3477673158190941,
      -1.4056474058702635,
      -1.2965884198679838,
      -1.442984190462106,
      -1.2968479311475498,
      -1.4904148543378821,
      -1.2450567655953084,
      -1.409508566535612,
      -1.53199444013683,
      -1.523196152599459,
      -1.4160930993704042,
      -1.4916981253834067,
      -1.4507149502851306,
      -1.3960319735092477,
      -1.410997653663605,
      -1.5214316548439661
    ],
    [
      -1.6229170107835293,
      -1.5834112718198852,
      -1.6432030748849715,
      -1.5927176555683566,
      -1.5279554385536185,
      -1.6429290413846465,
      -1.6798772760544545,
      -1.5884186645821867,
      -1.6024776281631683,
      0.0,
      -1.5926137051883318,
      -1.6468290323956305,
      -1.5882332857904087,
      -1.5907313607928852,
      -1.590321297663998,
      -1.5925699631698758,
      -1.5546117120713387,
      -1.6396952818951402,
      -1.547910356478194,
      -1.5780098021309903,
      -1.5415190373373293,
      -1.64200114465898,
      -1.5628593923012133,
      -1.6007654893552081,
      -1.632471642165008,
      -1.5722751826605508,
      -1.5922678462965132,
      -1.6044308226912978,
      -1.6613807169220438
    ],
    [
      -1.4463525960713215,
      -1.2597435577481715,
      -1.1907754451245198,
      -1.1031281758817988,
      -1.2086826870796796,
      -1.372699943442886,
      -1.450665968005367,
      -1.2907076358607377,
      -1.162699452410649,
      -1.3854571318483146,
      0.0,
      -1.5049962652097688,
      -1.2367948555688104,
      -1.302006114130804,
      -1.3032014715546012,
      -1.295742680493062,
      -1.2944977864398328,
      -1.2057116841483722,
      -1.328575975534206,
      -1.127740710457354,
      -1.2943865011222162,
      -1.3907983504373103,
      -1.3983931587596483,
      -1.309330901440881,
      -1.4257865200558448,
      -1.3353164455582758,
      -1.2736872842443785,
      -1.366820585051907,
      -1.4171612400259321
    ],
    [
      -1.4144776021475978,
      -1.3724657017080406,
      -1.3588800160698604,
      -1.3726802198014774,
      -1.320328933802658,
      -1.3563444099063366,
      -1.3671947302549214,
      -1.357399418486576,
      -1.3586714693420494,
      -1.3364852605883673,
      -1.3718185868956505,
      0.0,
      -1.3466473457904768,
      -1.341834133169921,
      -1.3497599834822598,
      -1.3541716495317084,
      -1.3463209834320526,
      -1.3686889851331836,
      -1.3560729689545632,
      -1.3607525243026088,
      -1.340913509119397,
      -1.3507627132430786,
      -1.3440517143052095,
      -1.3505421555879262,
      -1.356311988998992,
      -1.343886412170756,
      -1.3558958978749387,
      -1.3651521277850653,
      -1.3459939556259888
    ],
    [
      -1.491207769783024,
      -1.2883596027313986,
      -1.2993002551904873,
      -1.2527121956151372,
      -1.2920280178267027,
      -1.4145415828842116,
      -1.4958483605346107,
      -1.36283733458094,
      -1.305091777177315,
      -1.4062255373108217,
      -1.2785307297366955,
      -1.5537042775014942,
      0.0,
      -1.299149037182317,
      -1.2779996464455816,
      -1.2808058984697213,
      -1.3725298749755452,
      -1.3452674227512302,
      -1.3887617150345444,
      -1.3362263348792378,
      -1.3787432113774518,
      -1.4218865974293629,
      -1.464542362735985,
      -1.4667508851772801,
      -1.4698670151180986,
      -1.4092108007251947,
      -1.3015715476256366,
      -1.5292035314110253,
      -1.462719985875612
    ],
    [
      -1.362831194712658,
      -1.1468013930673908,
      -1.1754232555940576,
      -1.1377312928344834,
      -1.2083704178952734,
      -1.290975629096984,
      -1.3732016360200352,
      -1.2365339155647943,
      -1.1452358431534024,
      -1.3506983050533765,
      -1.2144263574901086,
      -1.3965764148845958,
      -1.1842980074829093,
      0.0,
      -1.1180825285120122,
      -1.2113788272633932,
      -1.208630981531872,
      -1.259567624639397,
      -1.2650121861581167,
      -1.155808544021334,
      -1.2715532701760885,
      -1.3109013641180176,
      -1.3163619845882248,
      -1.3214107983792396,
      -1.3487014309114984,
      -1.297357367093617,
      -1.208020478892484,
      -1.3352075434619148,
      -1.3363457869715287
    ],
    [
      -1.499343165656104,
      -1.199892597665969,
      -1.265227258031959,
      -1.2295319819954273,
      -1.2613983991121878,
      -1.4256755302482769,
      -1.4651686057963424,
      -1.322883862170684,
      -1.2919605658264457,
      -1.423232567471386,
      -1.2771694744221223,
      -1.462854485795108,
      -1.2291383965187417,
      -1.2262537491427383,
      0.0,
      -1.2466203543949335,
      -1.2651806026609387,
      -1.3099513823562132,
      -1.3677249564278993,
      -1.2080603212762433,
      -1.3426653098763097,
      -1.3208192975612265,
      -1.4276490296346376,
      -1.4125972806634162,
      -1.4377951509731166,
      -1.3188073483994336,
      -1.2723718476326829,
      -1.4340163578038116,
      -1.368635891485257
    ],
    [
      -1.471249303118414,
      -1.205887458561621,
      -1.2949164719983846,
      -1.2373781317091432,
      -1.3162920189311897,
      -1.406427077747687,
      -1.5137573308185495,
      -1.3678234597177088,
      -1.2010282319299603,
      -1.4596988023411026,
      -1.283017173405706,
      -1.5117947923734276,
      -1.2608645137199386,
      -1.3279756581093234,
      -1.214071367296201,
      0.0,
      -1.324216247531778,
      -1.2807974027341695,
      -1.391946524619976,
      -1.286585960787403,
      -1.378207897073126,
      -1.429266549134267,
      -1.4594627379209903,
      -1.4130211109380393,
      -1.4693895869146707,
      -1.362729127786837,
      -1.3106132682436422,
      -1.4403683377802612,
      -1.4361392146696488
    ],
    [
      -1.6336768557796677,
      -1.49278422387571,
      -1.5166320962369377,
      -1.4912889237572247,
      -1.4992546564349667,
      -1.6586504562907158,
      -1.6923785509079958,
      -1.5645338569955345,
      -1.5039258607198889,
      -1.603931680276341,
      -1.4773075017071504,
      -1.675763945490938,
      -1.5347928173696175,
      -1.4922310929037657,
      -1.4701263970244678,
      -1.470037561657132,
      0.0,
      -1.5463989906741566,
      -1.61893178688591,
      -1.4927550171242137,
      -1.5267582002569116,
      -1.684652914314673,
      -1.5813275785542251,
      -1.673250870386394,
      -1.646572719201456,
      -1.5928418212804307,
      -1.4871759398787794,
      -1.6576634362299065,
      -1.6911403123584148
    ],
    [
      -1.490818917511501,
      -1.1708522735222688,
      -1.1531958474456667,
      -1.160691568775126,
      -1.182883377450092,
      -1.2896669240168546,
      -1.3736942577969267,
      -1.309304956516287,
      -1.1714623434148734,
      -1.4055932020566246,
      -1.1584872621439584,
      -1.466040554886789,
      -1.1950413082386542,
      -1.2397059399522563,
      -1.2611901563251358,
      -1.2088394480320879,
      -1.3016194227656703,
      0.0,
      -1.355422490718444,
      -1.1420910417814054,
      -1.3283013303851827,
      -1.33115140994078,
      -1.407968980487886,
      -1.2516138290443173,
      -1.3651842652009412,
      -1.3523333958018269,
      -1.2429884402533924,
      -1.375105206856602,
      -1.3644836940475926
    ],
    [
      -1.533162517896542,
      -1.4392494612686915,
      -1.466451393961305,
      -1.4822613841449817,
      -1.4017460696055972,
      -1.4408470141061873,
      -1.5228188203406057,
      -1.462928202442802,
      -1.4384010110878072,
      -1.4876883092987336,
      -1.4601274702131328,
      -1.5569491499104169,
      -1.40943825038074,
      -1.4538747041076698,
      -1.4660911111930863,
      -1.4118217427549336,
      -1.4407475859455803,
      -1.4628127241931212,
      0.0,
      -1.4280973199283933,
      -1.4153632035010173,
      -1.4932735255095682,
      -1.494671060631661,
      -1.4918401195129343,
      -1.4977933544857058,
      -1.4327424215123612,
      -1.4152136856342576,
      -1.4808187593409305,
      -1.5049890957110041
    ],
    [
      -1.5508675551346498,
      -1.3380854970628095,
      -1.33711858406667,
      -1.3177673636392226,
      -1.3804597051643472,
      -1.462091873269224,
      -1.5444728803034766,
      -1.456559189729353,
      -1.340070057392679,
      -1.5101605939363585,
      -1.3527294884131358,
      -1.5801093403466024,
      -1.439136414173206,
      -1.426026103752519,
      -1.3895919217373793,
      -1.387452211400063,
      -1.4480684009248248,
      -1.3851939013108057,
      -1.4857924677270677,
      0.0,
      -1.4574837884472145,
      -1.519779891168896,
      -1.5081581180097516,
      -1.4414464670819234,
      -1.5569098683430789,
      -1.453784944558044,
      -1.4522673009119023,
      -1.4736215188160318,
      -1.4961457069629769
    ],
    [
      -1.717286228890702,
      -1.5343207066668414,
      -1.5932033224759221,
      -1.5746161933163205,
      -1.5666094181458694,
      -1.6713276928113832,
      -1.6671499084527646,
      -1.5162894844077888,
      -1.5142421450830554,
      -1.5931880321152878,
      -1.5264920369601664,
      -1.6888695558105093,
      -1.562958753783542,
      -1.5532269102551677,
      -1.5686322366356062,
      -1.5288734325774564,
      -1.4932797799393192,
      -1.5661210229525988,
      -1.601841037570029,
      -1.50176062056699,
      0.0,
      -1.639485812336585,
      -1.5594046720992283,
      -1.6565374950976226,
      -1.661261883434569,
      -1.5594894485046433,
      -1.5197590722830567,
      -1.6780326822426626,
      -1.6459687271449142
    ],
    [
      -1.4297390322032542,
      -1.2242141035622087,
      -1.263439422330633,
      -1.3224746107429006,
      -1.2703343018686144,
      -1.2873727631346263,
      -1.3112548262044257,
      -1.3205504854957908,
      -1.2864767529585384,
      -1.353366853924092,
      -1.2952535385903887,
      -1.3870836974706968,
      -1.3097623405483296,
      -1.2873441224402729,
      -1.2358864954015967,
      -1.267360355252823,
      -1.3329575514533614,
      -1.2561983820537221,
      -1.2887724107186158,
      -1.252016137776756,
      -1.3277224709768791,
      0.0,
      -1.3591768489358778,
      -1.3172931370084775,
      -1.298381432524307,
      -1.2664347311798545,
      -1.2825948572259283,
      -1.3740335877813272,
      -1.169144558782251
    ],
    [
      -1.342474694804441,
      -1.2272871702104873,
      -1.254573696798176,
      -1.2361051942609722,
      -1.184411452335461,
      -1.2635101941436944,
      -1.2852673085644868,
      -1.176573153088816,
      -1.2010398562476587,
      -1.194821414052216,
      -1.2344399737233647,
      -1.2767113847141147,
      -1.2107723194252642,
      -1.2036441129361655,
      -1.2237469563829246,
      -1.216656223544624,
      -1.1790470653329748,
      -1.2502302028537506,
      -1.2406805315839282,
      -1.1862364900374884,
      -1.1385887352344395,
      -1.2515748216173774,
      0.0,
      -1.255627440825088,
      -1.2201956915972383,
      -1.2178531881302512,
      -1.1891358645164054,
      -1.2828461997148124,
      -1.2147367246126186
    ],
    [
      -1.57055868265629,
      -1.4719487933060345,
      -1.347514817517697,
      -1.4254146441547317,
      -1.3532653733356537,
      -1.4962015701900622,
      -1.5264064193410907,
      -1.5358421886196294,
      -1.4135217947361234,
      -1.4656919234288996,
      -1.4597142005575823,
      -1.5683921168937949,
      -1.5265000867873817,
      -1.4902624096924943,
      -1.487030395361139,
      -1.487430455022703,
      -1.5561847952351435,
      -1.4090307833904352,
      -1.505002139694726,
      -1.445556630103554,
      -1.5394706506011977,
      -1.4887395267844212,
      -1.5466023889335447,
      0.0,
      -1.525853433009343,
      -1.5049600449121054,
      -1.4743315446319647,
      -1.4013798743236303,
      -1.4836853475557243
    ],
    [
      -1.3713491615428937,
      -1.295644731106284,
      -1.2419031612410876,
      -1.3101156326199526,
      -1.2596624560651317,
      -1.3279722292809224,
      -1.3616939340372356,
      -1.3092046442991219,
      -1.3011360232425464,
      -1.2886696839023546,
      -1.323196144596592,
      -1.3621310632876074,
      -1.308212164360605,
      -1.3274838032375833,
      -1.314614836167234,
      -1.3274856822600436,
      -1.2794605019654135,
      -1.2961840796208635,
      -1.3277216394065532,
      -1.3472428739653703,
      -1.3301078218364,
      -1.3289880602600008,
      -1.3212114111508277,
      -1.2921693662745524,
      0.0,
      -1.3229561001507062,
      -1.3130649911018124,
      -1.3377427056077598,
      -1.3239264101047847
    ],
    [
      -1.6937548971193965,
      -1.424026382617741,
      -1.589559426978689,
      -1.537552901826517,
      -1.4930448311153595,
      -1.5817891969909612,
      -1.6362387401167426,
      -1.5323262661909476,
      -1.505099857300096,
      -1.5773839979093418,
      -1.5608036934047769,
      -1.6340101899416128,
      -1.5458945220082863,
      -1.5374678002302697,
      -1.4627588079529108,
      -1.4501001611990343,
      -1.5073881287204987,
      -1.5779838367283026,
      -1.5096706808612277,
      -1.4339670631558237,
      -1.5575575586750112,
      -1.5087297325523694,
      -1.5941179917576465,
      -1.6079013867736278,
      -1.610199573349691,
      0.0,
      -1.529148229430762,
      -1.5982717153706867,
      -1.5417556154779835
    ],
    [
      -1.5178323386232204,
      -1.4491370667305805,
      -1.4427273626808736,
      -1.4162969855216383,
      -1.3125159781331504,
      -1.549063318722351,
      -1.5784005902248253,
      -1.4689546067945596,
      -1.442822023647558,
      -1.4586513766941127,
      -1.4591790532681812,
      -1.6292851538081792,
      -1.4493967348091221,
      -1.465868053893393,
      -1.4711260402296062,
      -1.449935626583058,
      -1.4056629370183398,
      -1.4466328006638238,
      -1.4662851274809703,
      -1.4372625609050724,
      -1.442381760039512,
      -1.5427374114029127,
      -1.5270885124688596,
      -1.549227072118962,
      -1.5341654403996103,
      -1.4523781173900419,
      0.0,
      -1.5898738630459976,
      -1.5745324094770208
    ],
    [
      -1.4866551987857244,
      -1.320921090622007,
      -1.3545258102825792,
      -1.3239498390916782,
      -1.3030473012907493,
      -1.4129534496652312,
      -1.4745220632102878,
      -1.4457778079625432,
      -1.2672634155162883,
      -1.4218389600799002,
      -1.3375049373907695,
      -1.4876757684961137,
      -1.408734708772046,
      -1.389263326637237,
      -1.3425666998514396,
      -1.3299282164938873,
      -1.4252180016226084,
      -1.3675279917148897,
      -1.4231253824869292,
      -1.326998818760558,
      -1.4311635597911532,
      -1.4987098003189334,
      -1.452613176698464,
      -1.2732383641389895,
      -1.4638649908844548,
      -1.414518476436123,
      -1.42402051841565,
      0.0,
      -1.4873479605477202
    ],
    [
      -1.5652787664970678,
      -1.3792596484723223,
      -1.4146413172352423,
      -1.4650878900871849,
      -1.4284944234029127,
      -1.4171038680706887,
      -1.430151761291842,
      -1.4263473530548718,
      -1.431431238512389,
      -1.4629586814869744,
      -1.440512860223473,
      -1.4689493002893872,
      -1.4420608005773217,
      -1.4402484560845132,
      -1.3755833388880536,
      -1.3831602135217438,
      -1.4352002507799977,
      -1.3923351553573715,
      -1.4230646084309653,
      -1.4013271168577326,
      -1.437042831891272,
      -1.2673306619655447,
      -1.4601211829555278,
      -1.4060556478624144,
      -1.4296814806028995,
      -1.3671189068315992,
      -1.4420778997723263,
      -1.4741092067249524,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.3400908590456537,
      0.30754273606856497,
      0.3306990283368587,
      0.32400854554285274,
      0.31646226649301656,
      0.1802505491897448,
      0.23084081231389364,
      0.2984419529167259,
      0.26332321281838755,
      0.2846199636236477,
      0.19236606032499015,
      0.30238187005015327,
      0.2799234067407639,
      0.2917415454248209,
      0.33305837000373817,
      0.2643803861776566,
      0.28564992791987454,
      0.28876551859868194,
      0.3154876457380551,
      0.26709613143162336,
      0.2102521254888794,
      0.24484558162624293,
      0.2639147093741019,
      0.26470432562749036,
      0.2644442848301769,
      0.3361483913170953,
      0.2527425969277277,
      0.2233014492069425
    ],
    [
      0.37726817588507866,
      0.0,
      0.6076907960231053,
      0.581453385653353,
      0.5195880574292979,
      0.48207521534952735,
      0.3608466987191279,
      0.4576149561631355,
      0.5800687877632686,
      0.39467489735090777,
      0.5080219711710297,
      0.3448616225419401,
      0.5896886665956105,
      0.5808924191962286,
      0.6122896239666491,
      0.6281354995620865,
      0.4766685351123925,
      0.5061246414305807,
      0.44321377594275346,
      0.6070354260035815,
      0.4599700998387881,
      0.4446659108620199,
      0.3725979732119291,
      0.44834425455041105,
      0.37279127841020054,
      0.4797379622258424,
      0.5573492080842535,
      0.4063173552867776,
      0.4035676930145482
    ],
    [
      0.3059054239401797,
      0.5088198909774768,
      0.0,
      0.5652377824761234,
      0.5450603948573589,
      0.3849002901344014,
      0.3508915841013045,
      0.3755563792845089,
      0.5318752229535877,
      0.3203202998527477,
      0.49697993104761085,
      0.2966975737670363,
      0.4614439296042989,
      0.4348546583880819,
      0.4435020437308814,
      0.45203902878041946,
      0.4066393577666092,
      0.5379914921185065,
      0.37748220783047914,
      0.5260058950986717,
      0.37976346679246253,
      0.3704537539126611,
      0.33039880635484997,
      0.4743281549470306,
      0.3679256216744986,
      0.3310448478583137,
      0.4614457138156036,
      0.3498771746075877,
      0.36086657343618
    ],
    [
      0.37979589364197897,
      0.5907684712227086,
      0.6637928208817485,
      0.0,
      0.6412866092092964,
      0.4518040167226096,
      0.32194512405151543,
      0.40926624583593196,
      0.6302324197755278,
      0.33804103709189803,
      0.5998810215731893,
      0.2989351061947556,
      0.5747394801231647,
      0.5473590358967635,
      0.5240042311273321,
      0.5329014776111893,
      0.41707430778994525,
      0.5486472633611288,
      0.40826773141671246,
      0.6315951884854236,
      0.4083879776402346,
      0.3410755642453043,
      0.322678973974204,
      0.5073826579296956,
      0.30815361268709296,
      0.38324473589458563,
      0.47588996563818586,
      0.41973774149153176,
      0.3176154481095157
    ],
    [
      0.304885044005468,
      0.4123747070369861,
      0.5204642165968936,
      0.5360527929920422,
      0.0,
      0.3299904203254336,
      0.2486734542018203,
      0.3621032251327303,
      0.4725408643017095,
      0.33899079726527725,
      0.4754400698411507,
      0.2632350144002047,
      0.4284902670516022,
      0.38786514803204675,
      0.3916145502277484,
      0.394474749519945,
      0.3813101516316597,
      0.4848996797107965,
      0.35969235691432955,
      0.4780833911742193,
      0.3654642884335111,
      0.342957446420332,
      0.3250170093201439,
      0.4148940513255843,
      0.32589669274380983,
      0.3814456848129091,
      0.44651091656072817,
      0.30785112982797846,
      0.3074165804530451
    ],
    [
      0.27577077478388823,
      0.4605119147201455,
      0.4548089907072792,
      0.4389355374284709,
      0.3934658301615068,
      0.0,
      0.3482847859482472,
      0.3508799121490571,
      0.3782466159982305,
      0.30815964752227365,
      0.38317795237930197,
      0.23698473532234932,
      0.3826757129217975,
      0.38464674315922287,
      0.40101226535167456,
      0.39125972853606505,
      0.35099330248020966,
      0.4573459449162025,
      0.3547660624082991,
      0.4558006816257445,
      0.3516754342462445,
      0.3923122891642119,
      0.3019429357952319,
      0.3799851411670556,
      0.3450134182236375,
      0.3254013753214722,
      0.41163249628316567,
      0.3180308766200437,
      0.3505362615829506
    ],
    [
      0.22537398203164694,
      0.37026797469450257,
      0.39228308861081373,
      0.37149203371991435,
      0.4050693765234632,
      0.3680048512143106,
      0.0,
      0.344501430906486,
      0.36334081948524366,
      0.32477871796721214,
      0.3376919199720949,
      0.32337453902096436,
      0.3532807531872766,
      0.3646707108114129,
      0.32609113918229293,
      0.33615455298857566,
      0.3406824951476639,
      0.3905403205826421,
      0.35572153364797754,
      0.38777514226467247,
      0.36059439854417175,
      0.36816051509374437,
      0.3625926971171287,
      0.3795500884619911,
      0.3448353754711091,
      0.3251217182365047,
      0.39684079208657685,
      0.3107992334247591,
      0.36877959259496507
    ],
    [
      0.2512019522044264,
      0.5227077543300918,
      0.477334082114639,
      0.5373967183214323,
      0.5421653931501611,
      0.4323972583990543,
      0.37711566710624167,
      0.0,
      0.5934814382574443,
      0.4699945317242882,
      0.5722005685154357,
      0.34924687034162094,
      0.5404237269014145,
      0.5532095911231434,
      0.5501235463839478,
      0.5146438628537005,
      0.5449074376629544,
      0.47420105329818707,
      0.49313961854241883,
      0.5555626513958838,
      0.5660094907143682,
      0.42166956947648426,
      0.5275443547364571,
      0.3817907282685955,
      0.4152182916749154,
      0.473965763905682,
      0.5400713769349403,
      0.4115657733254754,
      0.43166275819096045
    ],
    [
      0.320899960261106,
      0.6043662649269963,
      0.6407435185217687,
      0.6491387801659112,
      0.5816548713053151,
      0.4365178700432535,
      0.3679483322974959,
      0.4963433110559827,
      0.0,
      0.4035514670118945,
      0.5740087303232613,
      0.29008047379278157,
      0.5572984902396727,
      0.5495038728765325,
      0.4916237828253631,
      0.6006827688276428,
      0.4542869982335207,
      0.6004232575480768,
      0.4068563343577445,
      0.6522144231003182,
      0.48776262216001465,
      0.36527674855879666,
      0.37407503609616755,
      0.48117808932522244,
      0.4055730633122199,
      0.44655623841049596,
      0.5012392151863789,
      0.48627353503202153,
      0.37583953385166047
    ],
    [
      0.3385442850673115,
      0.37805002403095567,
      0.3182582209658693,
      0.3687436402824842,
      0.43350585729722235,
      0.3185322544661944,
      0.28158401979638636,
      0.3730426312686541,
      0.35898366768767254,
      0.0,
      0.36884759066250905,
      0.31463226345521034,
      0.37322801006043216,
      0.3707299350579556,
      0.37113999818684285,
      0.3688913326809651,
      0.4068495837795021,
      0.3217660139557006,
      0.41355093937264686,
      0.38345149371985054,
      0.4199422585135115,
      0.3194601511918609,
      0.39860190354962755,
      0.3606958064956327,
      0.3289896536858328,
      0.38918611319029006,
      0.36919344955432765,
      0.35703047315954306,
      0.300080578928797
    ],
    [
      0.35669297426362623,
      0.5433020125867762,
      0.6122701252104279,
      0.6999173944531489,
      0.5943628832552681,
      0.4303456268920618,
      0.35237960232958065,
      0.51233793447421,
      0.6403461179242986,
      0.4175884384866331,
      0.0,
      0.2980493051251789,
      0.5662507147661373,
      0.5010394562041438,
      0.4998440987803465,
      0.5073028898418857,
      0.5085477838951149,
      0.5973338861865756,
      0.4744695948007418,
      0.6753048598775937,
      0.5086590692127315,
      0.41224721989763746,
      0.4046524115752994,
      0.4937146688940668,
      0.3772590502791029,
      0.46772912477667195,
      0.5293582860905692,
      0.43622498528304066,
      0.3858843303090156
    ],
    [
      0.1910504961433852,
      0.23306239658294237,
      0.24664808222112256,
      0.2328478784895056,
      0.28519916448832494,
      0.24918368838464633,
      0.23833336803606153,
      0.2481286798044069,
      0.24685662894893357,
      0.2690428377026157,
      0.23370951139533247,
      0.0,
      0.25888075250050613,
      0.2636939651210619,
      0.2557681148087232,
      0.25135644875927454,
      0.25920711485893033,
      0.23683911315779937,
      0.24945512933641978,
      0.24477557398837413,
      0.26461458917158587,
      0.25476538504790436,
      0.26147638398577344,
      0.25498594270305674,
      0.2492161092919909,
      0.26164168612022687,
      0.24963220041604428,
      0.24037597050591764,
      0.2595341426649942
    ],
    [
      0.35263306949282236,
      0.5554812365444477,
      0.544540584085359,
      0.591128643660709,
      0.5518128214491436,
      0.4292992563916347,
      0.3479924787412356,
      0.4810035046949064,
      0.5387490620985314,
      0.43761530196502463,
      0.5653101095391508,
      0.2901365617743521,
      0.0,
      0.5446918020935294,
      0.5658411928302647,
      0.563034940806125,
      0.47131096430030106,
      0.49857341652461606,
      0.4550791242413019,
      0.5076145043966085,
      0.4650976278983945,
      0.42195424184648345,
      0.37929847653986126,
      0.37708995409856616,
      0.3739738241577477,
      0.43463003855065163,
      0.5422692916502097,
      0.314637307864821,
      0.3811208534002344
    ],
    [
      0.3388910297627963,
      0.5549208314080634,
      0.5262989688813966,
      0.5639909316409708,
      0.49335180658018074,
      0.41074659537847014,
      0.328520588455419,
      0.4651883089106599,
      0.5564863813220517,
      0.35102391942207767,
      0.48729586698534555,
      0.30514580959085835,
      0.5174242169925449,
      0.0,
      0.583639695963442,
      0.490343397212061,
      0.4930912429435821,
      0.4421545998360572,
      0.43671003831733746,
      0.5459136804541203,
      0.43016895429936564,
      0.3908208603574366,
      0.38536023988722934,
      0.3803114260962146,
      0.35302079356395577,
      0.40436485738183725,
      0.4937017455829702,
      0.36651468101353935,
      0.36537643750392546
    ],
    [
      0.3585364608876298,
      0.657987028877765,
      0.5926523685117748,
      0.6283476445483065,
      0.5964812274315461,
      0.432204096295457,
      0.3927110207473914,
      0.5349957643730499,
      0.5659190607172881,
      0.4346470590723479,
      0.5807101521216116,
      0.3950251407486258,
      0.6287412300249922,
      0.6316258774009955,
      0.0,
      0.6112592721488004,
      0.5926990238827952,
      0.5479282441875206,
      0.4901546701158346,
      0.6498193052674905,
      0.5152143166674241,
      0.5370603289825073,
      0.43023059690909626,
      0.4452823458803177,
      0.42008447557061723,
      0.5390722781443003,
      0.585507778911051,
      0.4238632687399222,
      0.48924373505847685
    ],
    [
      0.37889814450525816,
      0.6442599890620511,
      0.5552309756252876,
      0.6127693159145289,
      0.5338554286924824,
      0.44372036987598507,
      0.33639011680512265,
      0.4823239879059633,
      0.6491192156937118,
      0.3904486452825695,
      0.567130274217966,
      0.3383526552502445,
      0.5892829339037335,
      0.5221717895143487,
      0.6360760803274712,
      0.0,
      0.5259312000918941,
      0.5693500448895026,
      0.4582009230036961,
      0.563561486836269,
      0.47193955055054615,
      0.420880898489405,
      0.39068470970268177,
      0.43712633668563283,
      0.3807578607090014,
      0.48741831983683515,
      0.5395341793800299,
      0.4097791098434109,
      0.41400823295402334
    ],
    [
      0.3287181790198783,
      0.46961081092383594,
      0.4457629385626083,
      0.47110611104232136,
      0.46314037836457933,
      0.3037445785088302,
      0.2700164838915502,
      0.39786117780401153,
      0.45846917407965715,
      0.358463354523205,
      0.4850875330923956,
      0.2866310893086079,
      0.42760221742992854,
      0.47016394189578037,
      0.49226863777507823,
      0.49235747314241407,
      0.0,
      0.4159960441253894,
      0.3434632479136359,
      0.46964001767533237,
      0.43563683454263447,
      0.2777421204848731,
      0.3810674562453209,
      0.28914416441315205,
      0.31582231559808993,
      0.3695532135191153,
      0.47521909492076664,
      0.3047315985696395,
      0.27125472244113125
    ],
    [
      0.25898808048086375,
      0.5789547244700959,
      0.596611150546698,
      0.5891154292172387,
      0.5669236205422727,
      0.46014007397551016,
      0.37611274019543806,
      0.44050204147607763,
      0.5783446545774913,
      0.3442137959357401,
      0.5913197358484064,
      0.28376644310557575,
      0.5547656897537105,
      0.5101010580401084,
      0.48861684166722896,
      0.5409675499602769,
      0.4481875752266944,
      0.0,
      0.3943845072739207,
      0.6077159562109593,
      0.421505667607182,
      0.41865558805158476,
      0.3418380175044786,
      0.49819316894804744,
      0.38462273279142356,
      0.39747360219053784,
      0.5068185577389723,
      0.37470179113576263,
      0.3853233039447721
    ],
    [
      0.29014649652257307,
      0.3840595531504236,
      0.35685762045781,
      0.34104763027413343,
      0.42156294481351786,
      0.38246200031292776,
      0.3004901940785094,
      0.360380811976313,
      0.3849080033313079,
      0.33562070512038145,
      0.36318154420598225,
      0.2663598645086982,
      0.41387076403837497,
      0.36943431031144525,
      0.35721790322602875,
      0.41148727166418153,
      0.3825614284735348,
      0.36049629022599383,
      0.0,
      0.3952116944907218,
      0.4079458109180978,
      0.3300354889095469,
      0.328637953787454,
      0.3314688949061808,
      0.32551565993340925,
      0.3905665929067539,
      0.40809532878485744,
      0.34249025507818454,
      0.31831991870811094
    ],
    [
      0.277339168501203,
      0.49012122657304324,
      0.49108813956918285,
      0.5104393599966301,
      0.44774701847150555,
      0.3661148503666287,
      0.28373384333237617,
      0.37164753390649974,
      0.48813666624317387,
      0.3180461296994943,
      0.475477235222717,
      0.24809738328925035,
      0.38907030946264687,
      0.40218061988333376,
      0.43861480189847346,
      0.4407545122357899,
      0.380138322711028,
      0.44301282232504713,
      0.34241425590878505,
      0.0,
      0.3707229351886383,
      0.3084268324669568,
      0.32004860562610116,
      0.3867602565539294,
      0.2712968552927739,
      0.3744217790778088,
      0.37593942272395053,
      0.35458520481982103,
      0.3320610166728759
    ],
    [
      0.2893879202312726,
      0.4723534424551332,
      0.4134708266460525,
      0.4320579558056541,
      0.4400647309761052,
      0.33534645631059146,
      0.33952424066921005,
      0.49038466471418585,
      0.4924320040389192,
      0.4134861170066868,
      0.4801821121618082,
      0.31780459331146527,
      0.4437153953384325,
      0.45344723886680693,
      0.43804191248636837,
      0.47780071654451817,
      0.5133943691826555,
      0.4405531261693758,
      0.4048331115519457,
      0.5049135285549846,
      0.0,
      0.36718833678538965,
      0.44726947702274633,
      0.350136654024352,
      0.3454122656874057,
      0.4471847006173313,
      0.4869150768389179,
      0.32864146687931206,
      0.36070542197706046
    ],
    [
      0.2994288298598371,
      0.5049537585008825,
      0.46572843973245814,
      0.40669325132019063,
      0.45883356019447685,
      0.4417950989284649,
      0.41791303585866557,
      0.40861737656730046,
      0.4426911091045529,
      0.3758010081389993,
      0.43391432347270253,
      0.34208416459239444,
      0.4194055215147616,
      0.4418237396228184,
      0.4932813666614946,
      0.4618075068102683,
      0.3962103106097299,
      0.4729694800093691,
      0.44039545134447544,
      0.47715172428633523,
      0.40144539108621213,
      0.0,
      0.3699910131272135,
      0.41187472505461376,
      0.4307864295387842,
      0.46273313088323675,
      0.446573004837163,
      0.35513427428176403,
      0.5600233032808402
    ],
    [
      0.27289672285151245,
      0.3880842474454662,
      0.3607977208577775,
      0.3792662233949813,
      0.4309599653204925,
      0.3518612235122591,
      0.3301041090914667,
      0.43879826456713755,
      0.41433156140829475,
      0.4205500036037375,
      0.3809314439325888,
      0.3386600329418388,
      0.4045990982306893,
      0.411727304719788,
      0.39162446127302886,
      0.3987151941113294,
      0.4363243523229787,
      0.36514121480220285,
      0.3746908860720253,
      0.42913492761846506,
      0.476782682421514,
      0.36379659603857606,
      0.0,
      0.35974397683086545,
      0.3951757260587152,
      0.39751822952570226,
      0.4262355531395481,
      0.3325252179411411,
      0.4006346930433349
    ],
    [
      0.2940019757791881,
      0.3926118651294437,
      0.5170458409177812,
      0.4391460142807464,
      0.5112952850998245,
      0.36835908824541597,
      0.33815423909438747,
      0.32871846981584874,
      0.45103886369935475,
      0.3988687350065785,
      0.40484645787789586,
      0.29616854154168326,
      0.33806057164809644,
      0.3742982487429838,
      0.3775302630743391,
      0.37713020341277503,
      0.3083758632003346,
      0.45552987504504294,
      0.3595585187407522,
      0.41900402833192407,
      0.32509000783428044,
      0.3758211316510569,
      0.3179582695019334,
      0.0,
      0.3387072254261352,
      0.3596006135233727,
      0.39022911380351344,
      0.4631807841118478,
      0.3808753108797538
    ],
    [
      0.27231148376773784,
      0.3480159142043475,
      0.4017574840695439,
      0.33354501269067893,
      0.3839981892454998,
      0.3156884160297091,
      0.2819667112733959,
      0.33445600101150963,
      0.3425246220680851,
      0.3549909614082769,
      0.3204645007140394,
      0.28152958202302414,
      0.33544848095002644,
      0.3161768420730482,
      0.3290458091433974,
      0.3161749630505879,
      0.364200143345218,
      0.34747656568976804,
      0.3159390059040783,
      0.29641777134526115,
      0.3135528234742315,
      0.31467258505063067,
      0.32244923415980375,
      0.35149127903607913,
      0.0,
      0.3207045451599253,
      0.3305956542088191,
      0.30591793970287173,
      0.31973423520584676
    ],
    [
      0.2719614730780888,
      0.5416899875797443,
      0.37615694321879634,
      0.4281634683709683,
      0.4726715390821259,
      0.38392717320652414,
      0.3294776300807427,
      0.43339010400653777,
      0.46061651289738936,
      0.38833237228814355,
      0.4049126767927085,
      0.3317061802558725,
      0.4198218481891991,
      0.4282485699672156,
      0.5029575622445746,
      0.5156162089984511,
      0.4583282414769867,
      0.3877325334691828,
      0.45604568933625766,
      0.5317493070416617,
      0.40815881152247413,
      0.45698663764511593,
      0.3715983784398389,
      0.35781498342385754,
      0.35551679684779436,
      0.0,
      0.43656814076672346,
      0.36744465482679867,
      0.4239607547195019
    ],
    [
      0.38494004287641137,
      0.4536353147690513,
      0.46004501881875814,
      0.4864753959779935,
      0.5902564033664814,
      0.35370906277728076,
      0.32437179127480653,
      0.4338177747050722,
      0.45995035785207383,
      0.4441210048055191,
      0.4435933282314506,
      0.2734872276914526,
      0.45337564669050967,
      0.43690432760623876,
      0.43164634127002555,
      0.4528367549165737,
      0.497109444481292,
      0.45613958083580797,
      0.4364872540186615,
      0.46550982059455936,
      0.4603906214601199,
      0.36003497009671914,
      0.3756838690307722,
      0.35354530938066975,
      0.36860694110002146,
      0.45039426410958994,
      0.0,
      0.31289851845363414,
      0.328239972022611
    ],
    [
      0.2880297309357194,
      0.45376383909943674,
      0.4201591194388645,
      0.4507350906297656,
      0.47163762843069446,
      0.3617314800562126,
      0.300162866511156,
      0.3289071217589006,
      0.5074215142051555,
      0.3528459696415436,
      0.4371799923306743,
      0.28700916122533005,
      0.36595022094939766,
      0.3854216030842068,
      0.43211822987000414,
      0.4447567132275565,
      0.3494669280988354,
      0.40715693800655406,
      0.35155954723451455,
      0.44768611096088584,
      0.3435213699302906,
      0.27597512940251034,
      0.32207175302297975,
      0.5014465655824543,
      0.310819938836989,
      0.3601664532853208,
      0.35066441130579373,
      0.0,
      0.28733696917372353
    ],
    [
      0.23265751649653943,
      0.4186766345212849,
      0.38329496575836486,
      0.3328483929064223,
      0.36944185959069453,
      0.3808324149229185,
      0.36778452170176523,
      0.3715889299387354,
      0.3665050444812181,
      0.3349776015066328,
      0.3574234227701343,
      0.32898698270422,
      0.3558754824162855,
      0.35768782690909395,
      0.4223529441055536,
      0.4147760694718634,
      0.3627360322136095,
      0.4056011276362357,
      0.3748716745626419,
      0.3966091661358746,
      0.36089345110233517,
      0.5306056210280625,
      0.33781510003807935,
      0.39188063513119276,
      0.36825480239070774,
      0.430817376162008,
      0.35585838322128094,
      0.3238270762686548,
      0.0
    ]
  ],
  "row_avgs": [
    0.27705300904137004,
    0.48584124597658657,
    0.41958241071819546,
    0.4640894342722561,
    0.38530838215214663,
    0.3708663345329994,
    0.35351320689250415,
    0.482891136780513,
    0.4857827710588434,
    0.36091114821656384,
    0.4929791016311387,
    0.2496529055227093,
    0.4636400068441798,
    0.4450277823479967,
    0.5252858490080333,
    0.4910429562696304,
    0.3917955324933487,
    0.46210228922918095,
    0.35930117625412333,
    0.3820870395721309,
    0.41880885224481024,
    0.4300021546150002,
    0.38827184403848075,
    0.3821859073362961,
    0.3275445270001943,
    0.4179126849919027,
    0.41957879854336283,
    0.3784179427226953,
    0.3726957520033003
  ],
  "col_avgs": [
    0.30418411740276524,
    0.47405366695963397,
    0.4696191351293123,
    0.4753139587139816,
    0.481050049673989,
    0.38292485691140465,
    0.3247742784850059,
    0.4022570484472038,
    0.47328779799396076,
    0.3679470917578963,
    0.4504835692865051,
    0.3006933920768045,
    0.44449257148340704,
    0.43837478726211077,
    0.4478438922790499,
    0.4543114092028236,
    0.4211290320392011,
    0.44491337492727634,
    0.3950060253110381,
    0.4864551929526372,
    0.4088573815429638,
    0.3712126445230961,
    0.35887240063888026,
    0.3951455346245917,
    0.3515696834494811,
    0.39843355465919633,
    0.4437870267779442,
    0.3563464283936975,
    0.3608322794046357
  ],
  "combined_avgs": [
    0.29061856322206764,
    0.47994745646811027,
    0.4446007729237539,
    0.4697016964931188,
    0.43317921591306785,
    0.376895595722202,
    0.339143742688755,
    0.4425740926138584,
    0.47953528452640204,
    0.36442911998723004,
    0.47173133545882195,
    0.2751731487997569,
    0.45406628916379344,
    0.4417012848050538,
    0.4865648706435416,
    0.472677182736227,
    0.4064622822662749,
    0.45350783207822865,
    0.37715360078258076,
    0.43427111626238407,
    0.413833116893887,
    0.4006073995690481,
    0.37357212233868053,
    0.3886657209804439,
    0.33955710522483773,
    0.4081731198255495,
    0.4316829126606535,
    0.3673821855581964,
    0.366764015703968
  ],
  "gppm": [
    634.4396630951239,
    579.4574803069767,
    578.8266527001482,
    576.662087055254,
    570.4714585854223,
    616.4776364117206,
    640.127234353116,
    608.8236834292513,
    577.3358489304399,
    621.3563721891853,
    588.7200937989311,
    646.0755428933878,
    590.0061073565356,
    594.5176056246524,
    589.9738333246688,
    585.8253027140624,
    598.6014744027043,
    591.0982313349393,
    611.3735988267454,
    570.5775051191418,
    605.6396597110869,
    624.0287664026898,
    630.0419071922972,
    609.3260881176354,
    632.8227008612986,
    610.5987915375423,
    591.553722195027,
    626.2805958532508,
    626.0417939707773
  ],
  "gppm_normalized": [
    1.4902226646813659,
    1.3906427271941435,
    1.3927279027376371,
    1.3888657758694867,
    1.3689389701587618,
    1.4837570887781333,
    1.5407678268700644,
    1.4703867389816856,
    1.3881087237899223,
    1.4966546643549428,
    1.4145746171319011,
    1.577798593940609,
    1.4172357152190054,
    1.4283297873572516,
    1.423822718807296,
    1.4090550516297415,
    1.4416788978020645,
    1.4234735461237678,
    1.4704044494645083,
    1.3723771498873603,
    1.45995569041112,
    1.5037163336165396,
    1.5159322657076755,
    1.4709189772328009,
    1.5239641641772754,
    1.469970812262224,
    1.420958604917935,
    1.5085374040869284,
    1.508195917259188
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386,
    850,
    464,
    481,
    546,
    442,
    472,
    423,
    428,
    515,
    456,
    456,
    499,
    481,
    460,
    443,
    480,
    437,
    444,
    432,
    484,
    476,
    442,
    440,
    579,
    477,
    380,
    401,
    480,
    377,
    333,
    535,
    441,
    415,
    440,
    532,
    467,
    476,
    390,
    400,
    461,
    506,
    501,
    483,
    418,
    403,
    393,
    415,
    434,
    464,
    456,
    391,
    393,
    426,
    444,
    420,
    400,
    420,
    534,
    448,
    422,
    431,
    410,
    445,
    437,
    448,
    356,
    418,
    400,
    408,
    567,
    423,
    428,
    389,
    424,
    419,
    417,
    416,
    470,
    368,
    384,
    403,
    423,
    394,
    366,
    382,
    442,
    412
  ],
  "response_lengths": [
    2242,
    2301,
    2303,
    2232,
    2512,
    2461,
    2605,
    2025,
    2362,
    2317,
    2204,
    3320,
    2381,
    2311,
    2145,
    2363,
    2249,
    2258,
    2528,
    2559,
    2090,
    2199,
    2340,
    2311,
    2100,
    2020,
    2139,
    2428,
    2351
  ]
}