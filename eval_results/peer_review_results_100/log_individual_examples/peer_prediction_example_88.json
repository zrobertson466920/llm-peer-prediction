{
  "example_idx": 88,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSCHR ̈ODINGER’S FP: TRAINING NEURAL NETWORKS WITH DYNAMIC FLOATING-POINT CONTAINERS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe introduce a software-hardware co-design approach to reduce memory traffic and footprint during training with BFloat16 or FP32, in order to boost energy efficiency and execution time performance. Our methods dynamically adjust the size and format of the floating-point containers used to store activations and weights during training. The different value distributions lead us to different approaches for exponents and mantissas. Gecko exploits the favourable exponent distribution with a lossless delta encoding approach to reduce the total exponent footprint by up to 58% in comparison to the FP32 baseline. To contend with the noisy mantissa distributions, we present two lossy methods to eliminate as many as possible least significant bits without affecting accuracy. Quantum Mantissa is a machine learning mantissa compression method that taps onto the gradient descent algorithm to learn the minimal mantissa bitlengths on a per-layer granularity, and obtain up to 92% reduction in total mantissa footprint. Alternatively, BitChop observes changes in the loss function during training to adjust mantissa bitlength network-wide, yielding a reduction of 81% in footprint. Schr ̈odinger’s FP implements hardware encoders/decoders that, guided by Gecko/Quantum Mantissa or Gecko/BitChop, transparently encode/decode values when transferring to/from off-chip memory, boosting energy efficiency and reducing execution time.\n\n1\n\nINTRODUCTION\n\nTraining most state-of-the-art neural networks has become an exascale class task (Venkataramani et al., 2017; Amodei et al., 2018) requiring many graphics processors (NVidia, 2017) or specialized accelerators, e.g., (Jouppi et al., 2017; Hab, 2019; Liao et al., 2019; Cer, 2019). While training is both computationally and data demanding, it is the memory transfers to off-chip DRAM for stashing (i.e., saving and much later recovering) activation and weight tensors that dominate overall execution time and energy (Jain et al., 2018) (see Fig. 1). The per batch data volume easily surpasses on-chip memory capacities, necessitating off-chip DRAM accesses which are up to two orders of magnitude slower and more energy expensive. It’s no wonder that reducing this overhead has been receiving attention throughout the software/hardware stack.\n\nChen et al. (2016) and Zheng et al. (2020) recompute rather than stash activations, whereas microbatching strives to keep activations on chip (Huang et al., 2018). Encoding methods target specific value patterns such as zeros (Rhu et al., 2018) or redundant spatial information (Evans et al., 2020), or exploit underlying properties of training for certain tensors, e.g., the outputs of ReLU or Pooling (Jain et al., 2018). These lossless and lossy encodings use fewer bits for stashed tensor content to reduce tensor volume. This also boosts the effective capacity of each node’s main memory, which further reduces traffic during distributed training. All aforementioned methods either shift significant costs to compute or target only some values and offer only limited relief.\n\nThe most direct way to reduce tensor volume is to use a more compact datatype. Initially, with the goal to demonstrate that neural networks can tackle challenging problems, training relied on single precision 32b floating-point (FP32), which still remains the datatype of choice when achieving the best accuracy is the priority. Recently, we have seen some success in training with more compact datatypes such as half-precision FP16, BFloat16 (Kalamkar et al., 2019), dynamic floating-point (Das et al., 2018), and flexpoint (K ̈oster et al., 2017) and even with using combinations with other datatypes such as fixed-point (Das et al., 2018; Micikevicius et al., 2018; NVIDIA; Drumond et al., 2018). IBM\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Training process and its memory transfers. Blue - Activations that are typically saved to off-chip memory during forward pass and retrieved during backward pass, Red - Weights that are typically stored and loaded once from off-chip memory, Gray - Updates and Gradients – through mini-batching during the backward pass they can often fit on-chip\n\nmanaged to push the datatype to 8b (Wang et al., 2018b) and 4b (Sun et al., 2020) extremes for some cases. As Moore’s law and Dennard scaling for semiconductors have come to an end, using more efficient datatypes during training is getting wider attention – even major hardware manufacturers are investigating how to use 8b floating point with different mantissa/exponent ratios according to perceived needs of tensors (Micikevicius et al., 2022). These methods require careful trial-and-error investigation of where, when, and which narrow datatypes to use. This is especially true because different tensors, tasks, architectures, or layers require different datatypes. Consequently, there is no guarantee of success. The methods require trial-and-error full training runs as whether the choice of datatypes is viable can only be evaluated post mortem. Moreover, since the datatypes are statically chosen they offer no opportunity to amend the choice if accuracy suffers (e.g., significant drop with deeper networks identified by IBM (Sun et al., 2020)).\n\nObviously, knowing in advance which compact datatypes to use during training would be the best. However, given that this goal still eludes us, our work asks whether we can harness the training process itself to automatically learn them. Ideally, such a method would automatically tailor datatypes to meet the demands of each tensor, layer, and network. Furthermore, it could continuously adjust datatype selection as training progresses, adapting to the changing needs. In addition to accelerating training, methods such as ours can further inform efforts for selecting more efficient datatypes for inference such as those by (Micikevicius et al., 2022) or (Sun et al., 2020).\n\nA similar idea has successfully targeted fixed-point inference by using reinforcement learning (Wang et al., 2018a), clever differentiable datatype definitions (Nikoli ́c et al., 2020), architecture search (Wu et al., 2018), and profiling (Nikoli ́c et al., 2018), etc. However, all of these are too expensive for training and their overheads would overshadow the benefits of a more compact training datatype.\n\nGiven that floating point remains the datatype of choice, we focus on floating-point datatype selection. We explore the possibility to dynamically and continuously adjust the mantissa bitlength (fractional bits) and the container (overall bits) for floating-point values (activations and/or weights) for stashed tensors, and to do so transparently at no additional burden to the user. Our solution is Schr ̈odinger’s FP, a family of methods that dynamically adjust the floating-point encoding and complement the aforementioned training acceleration methods. Our approach is end-to-end fully-automated, requiring no input, guessing, or advanced knowledge from the operator. Schr ̈odinger’s FP can be used to reduce memory overheads and boost computation throughput. In this work, we limit our attention to boosting energy efficiency and performance by using Schr ̈odinger’s FP to transparently encode values as they are being stashed to off-chip memory, and decode them to their original format as they are being read back. This application can be used as a plug-in over any hardware without changing the existing on-chip memory hierarchy and compute units. Similarly, Schr ̈odinger’s FP will generally work in conjunction with methods that can improve accuracy for a preselected datatype, partition, distribute, or reschedule the training work to improve energy efficiency and performance.\n\nSchr ̈odinger’s FP uses tailored approaches for the mantissa and exponent. It dynamically adjusts mantissa bitlengths in order to store and read fewer bits per number in off-chip memory. This work explores two such methods. The first, Quantum Mantissa, harnesses the training algorithm itself to learn on-the-fly the mantissa bitlengths that are needed per tensor/layer and continuously adapts those bitlengths per batch. Quantum Mantissa introduces a single learning parameter per tensor and a loss\n\n2\n\nWgtsWgtsWgtsWgtsWgtsWgtsWgtsWgtsForward PassUpdUpdUpdUpdActivations in Off Chip MemoryGradsGradsGradsGradsGradsActsActsActsActsActsActsActsActsActsLossInputUnder review as a conference paper at ICLR 2023\n\nfunction that includes the effects of the mantissa bitlength. Learning the bitlength incurs a negligible overhead compared to saving from the resulting reduction in off-chip traffic. The Quantum Mantissa experiments show that: 1) it reduces the mantissa bitlengths considerably, and 2) the reductions are achieved fairly soon in the training process and remain stable till the end. However, the bitlengths vary per tensor and fluctuate throughout, capturing benefits that wouldn’t be possible with a static network-wide choice of datatype.\n\nMotivated by the success of Quantum Mantissa, we explore the second mantissa adjustment method, BitChop, which requires no additional loss function and parameters. BitChop interface only needs to be notified of the per-batch updates to the loss. Using an exponential moving average of these changes, BitChop adjusts mantissa bitlength for the whole network. As long as the network seems to be improving, BitChop will attempt to use a shorter mantissa; otherwise, it will increase it. The method proves effective, albeit with lower reductions compared to Quantum Mantissa. This is expected since 1) Quantum Mantissa harnesses the training process to continuously learn the optimal bitlengths, and 2) Quantum Mantissa adjusts bitlengths per layer whereas BitChop uses a network-wide one.\n\nMost of the exponents during training exhibit a heavily biased distribution (Awad et al., 2021). Accordingly, Schr ̈odinger’s FP uses a value-based approach that stores exponents using only as many bits as necessary to represent their magnitude and sign. Metadata encodes the number of bits used. To reduce the metadata overhead, Schr ̈odinger’s FP encodes exponents in groups.\n\nTo maximize benefits, we present a hardware-assisted implementation of Schr ̈odinger’s FP. The inclusion of specialized hardware units is now commonplace among all hardware vendors as the method of choice for further improving compute performance. Appendix A presents efficient hardware (de)compressors that operate on groups of otherwise unmodified floating-point values, be it FP32 or BFloat16. The compressors accept an external mantissa length signal and pack the group of values using the aforementioned compression methods for the mantissas and exponents. The decompressors expand such compressed blocks back into the original floating-point format. We demonstrate that Schr ̈odinger’s FP greatly improves energy efficiency and execution time. Our compression methods also serve as motivation for pursuing, as future work, a software-only implementation that would require low-level changes in the closed-source tensor operation kernels.\n\nWe highlight the following experimental findings:\n\n• Schr ̈odinger’s FP compression techniques find the necessary mantissa and exponent bitlengths to reduce overall memory footprint without noticeable loss of accuracy: our Quantum Mantissa-based method reduces the tested models down to 20.8% on average (range: 14.7% − 24.9%) and our BitChop-based one to 24.6% on average (range: 19.4% − 28.9%)\n\n• Schr ̈odinger’s FP compressor/decompressor exploit the reduced footprint to obtain 2.94× and 2.64× performance improvement for SFP QM and SFP BC (Schr ̈odinger’s FP with Quantum Mantissa or BitChop, see Section 2.1.4 for details), respectively. In fact, we hit a hard performance boundary, since all layers are shifted from memory bound to compute bound, which completely flips the workload paradigm.\n\n• Crucially, Schr ̈odinger’s FP excels at squeezing out energy savings with on average, 3.38×\n\nand 2.96× better energy efficiency for SFP QM and SFP BC .\n\n2 ADJUSTING VALUE CONTAINERS DURING TRAINING\n\nGenerally, maintaining accuracy on most real-world tasks requires training with a floating-point approach. Floating-point formats comprise three segments: a mantissa, an exponent, and a sign bit. Mantissas and exponents are differently distributed, necessitating different approaches. The greatest challenge is compressing mantissas since they are uniformly distributed across the domain, whereas compression exploits non-uniformity. We will present two methods to compress mantissas, a machine learning approach (Section 2.1) and a hardware-design-inspired approach (Section 2.2). In contrast, exponents can be compressed with fairly simple hardware techniques (Section 2.3).\n\nWe study Schr ̈odinger’s FP with ResNet18 and ResNet50 (He et al., 2015), and MobileNet V3 (Howard et al., 2019) trained on ImageNet (Russakovsky et al., 2014), DLRM (Naumov et al., 2019) trained on Kaggle Criteo, BERT (Devlin et al., 2018) finetuned on MRPC (Dolan & Brockett,\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n2005) and GTP–2 (Radford et al., 2019) finetuned on Wikitext 2 (Merity et al., 2016). For clarity, we report detailed results with ResNet18 with BFloat16 throughout the paper, concluding with overall performance and energy efficiency measurements for all models.\n\n2.1 MANTISSA: Quantum Mantissa\n\nQuantum Mantissa involves procedures for both the forward and backward pass of training. We begin by defining a conventional quantization scheme for integer mantissa bitlengths in the forward pass, and then expand it to the non-integer domain, and describe how this interpretation allows bitlengths to be learned using gradient descent. Subsequently, we introduce a parameterizable loss function, which enables Quantum Mantissa to penalize larger bitlengths. We then briefly touch on the compute overhead of our method and the plan for final selection of mantissa bitlengths. Ultimately, we demonstrate the benefits of Quantum Mantissa on memory footprint during ImageNet training.\n\n2.1.1 QUANTIZATION\n\nThe greatest challenge for learning bitlengths is that they represent discrete values with no obvious differentiation. To overcome this, we define a quantization method on non-integer bitlengths. We start with an integer quantization of the mantissa M with n bits by zeroing out all but the top n bits:\n\nQ(M, n) = M ∧ (2n − 1) << (m − n)\n\n(1)\n\nwhere Q(M, n) is the quantized mantissa with bitlength n, m is the maximum number of bits and ∧ represents bitwise AND.\n\nThroughout training, we represent the integer quantization as Q(M, n). This scheme does not allow the learning of bitlengths with gradient descent due to its discontinuous and non-differentiable nature. To expand the definition to real-valued n = ⌊n⌋ + {n}, the values used in inference during training are stochastically selected between the nearest two integers with probabilities {n} and 1 − {n}:\n\nQ(M, n) =\n\n(cid:26)Q(M, ⌊n⌋),\n\nwith probability 1 − {n}\n\nQ(M, ⌊n⌋ + 1), with probability {n}\n\n(2)\n\nwhere ⌊n⌋ and {n} are floor and fractional parts of n, respectively. The scheme can be, and in this work is, applied to activations and weights separately. Since the minimum bitlength per value is 0, n is clipped at 0. This presents a reasonable extension of the meaning of bitlength in continuous space and allows for the loss to be differentiable with respect to bitlength.\n\nDuring the forward pass, the formulae above are applied to both activations and weights. The quantized values are saved and used in the backward pass. During the backward pass, we use the straight-through estimator (Bengio et al., 2013; Hubara et al., 2016) to prevent propagating zero gradients that result from the discontinuity’s discreteness; however, we use the quantized mantissas for all calculations. This efficient quantization during the forward pass reduces the footprint of the whole process.\n\n2.1.2 LOSS FUNCTION\n\nOn top of finding the optimal weights, the modified loss function penalizes mantissa bitlengths by adding a weighted average (with weights λi, not to be confused with the model’s weights) of the bits mi required for mantissas of weights and activations. We define total loss L as:\n\nL = Ll + γ\n\ni (cid:88)\n\n(λi × ni)\n\n(3)\n\nwhere Ll is the original loss function, γ is the regularization coefficient used for selecting how aggressive the quantization should be, λi is the weight corresponding to the importance of the ith group of values (one per tensor), and ni is the bitlength of the activations or weights in that tensor.\n\nThis loss function can be used to target any quantifiable criteria by a suitable selection of the λi parameters. Since our goal is to minimize the total footprint of a training run, we weigh each layer’s tensors according to their memory footprint.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b) Figure 2: Quantum Mantissa on ResNet18/ImageNet: (a) Validation accuracy throughout training, (b) Weighted mantissa bitlengths with their spread throughout training, and (c) Mantissa bitlengths for each layer at the end of each epoch. Darker dots represent the latter epochs.\n\n(c)\n\n2.1.3 COMPUTATIONAL AND MEMORY OVERHEADS\n\nQuantum Mantissa adds minimal computational and memory overhead to the forward and backward passes. In the forward pass, random numbers need to be created at a chosen granularity to determine the quantized values. Our experiments show that per tensor/layer is sufficient and is a negligible cost.\n\nTo update the bitlength parameters in the backward pass, we need to compute their gradients. These are a function of the weight values and gradients, which are calculated during the regular backward pass. The extra calculations for each bitlength are proportional to the number of values quantized to that bitlength. This overhead is negligible in comparison to the total number of computations. For our experiments, the overhead is less than 2% and 0.5% for MobileNet V3 and ResNet18, respectively.\n\nOn the memory side, the only extra parameters that need to be saved are the bitlengths, two floats per layer (bitlength for weights and activations), again negligible in comparison with the total footprint. All other values are consumed as they are produced without need for off-chip stashing.\n\n2.1.4 BITLENGTH SELECTION\n\nQuantum Mantissa will produce non-integer bitlengths and requires non-deterministic inference. We prefer the deployed network not to have this requirement. For this reason, we round up the bitlengths and fix them for some training time to fine-tune the network to this state. While our experiments show that bitlengths converge quickly and final ones can be determined within a couple of epochs, avoiding the small overhead for most of the training, we delay this action so that bitlengths have the ability to increase if needed during training. Our experiments show that this is unnecessary for the models studied; however, the overhead is so small that we leave it on as a safety mechanism. We round up the bitlengths for the last 10 epochs to let the network regain any accuracy that might have been lost due to Quantum Mantissa. Quantum Mantissa still reduces traffic during these epochs.\n\nEvaluation: BitLengths and Accuracy We report measurements for per-layer weights and activations quantized separately using a loss function weighted to minimize total memory footprint. We train ResNet18 on the ImageNet dataset over 90 epochs, with regularizer strength of 0.1, learning rate of 0.1, 0.01 and 0.001 respectively at epochs 0, 30, and 60 and weight decay of 0.0001.\n\nQuantum Mantissa excels at minimizing the memory footprint whilst not introducing accuracy loss. Figure 2a shows that throughout training, Quantum Mantissa introduces minimal changes in validation accuracy. In the end, we converge to a solution within 0.4% of the FP32 baseline.\n\nFigure 2b shows how Quantum Mantissa quickly (within a couple of epochs) reduces the required mantissas for activations and weights down to 1 − 2 bits on average. Throughout training, the total cumulative memory footprint is reduced to 7.8% and 25.5% of the FP32 and BFloat16 mantissa footprint, respectively. The figure further shows that there is a large spread across different layers, indicating that a granular, per-layer, approach is the right choice to maximize benefits. Via the weighted loss function, Quantum Mantissa generally targets the activation bitlengths more aggressively than the weights because the activations are responsible for the majority of the memory footprint.\n\nThe spread of mantissa bitlengths across the network and time is shown in Figure 2c. While most layers quickly settle at 1 or 2 bits, there are exceptions that require more (up to 4b). Consequently, a network-scale datatype would have to use the largest one and leave a lot of the potential untapped. For ResNet18, the maximum bitlength is over 2× larger than the weighted average.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b) Figure 3: BitChop on ResNet18/ImageNet: (a) Validation accuracy throughout BFloat16 training, (b) Average mantissa bitlengths per epoch throughout training, on BFloat16 and FP32, (c) Distribution of BitChop’s mantissa bitlengths throughout the 5005 batches of epoch 45 over BFloat16.\n\n(c)\n\n2.2 MANTISSA: BitChop\n\nWhile Quantum Mantissa leverages training itself to greatly reduce mantissa lengths, having a method that does not require introducing an additional loss function and parameters is appealing. BitChop is a run-time, heuristic method to reduce the number of mantissa bits for the forward and backward passes. At a high-level, BitChop monitors how training progresses adjusting the mantissa length accordingly: as long as the training seems to be improving the network, BitChop will attempt to use a shorter mantissa; otherwise, it will try to increase its bitlength. BitChop conceptually splits the training process into periods, where a period is defined as processing N batches. BitChop adjusts the mantissa at the end of each period using information on network progress.\n\nThe ideal scenario for BitChop is one where past observation periods are good indicators of forthcoming behavior. Macroscopically, accuracy improves during training which appears to be a good fit. Microscopically, however, training is a noisy process. Fortunately, training is a relatively long process based on “trial-and-error” which may be forgiving for momentary lapses in judgement.\n\nThere are three major design decisions that impact how successful BitChop can be: 1) what information to use as a proxy for network progress, 2) how long the period should be, and 3) at what granularity to adjust mantissa lengths. The resulting method should strike a balance between capturing as much as possible the opportunity to reduce bitlength, and avoiding over-clipping as this can hurt learning progress and ultimately the final accuracy that would be achieved.\n\nWe have experimented with several options and arrived at the following choices: 1) Using an exponential moving average of the loss as a proxy for network progress, and 2) using a short period where N = 1, that is a single batch. Additionally, rather than attempting to adjust mantissas at the tensor/layer level, BitChop uses the same mantissa for the whole model. Specifically, to monitor network progress, BitChop uses the loss which is calculated per batch as part of the regular training process. While the loss improves over time, when observed over short periods of time, it exhibits non-monotonic behavior which is sometimes erratic. To compensate for this volatility, BitChop uses an exponential moving average Mavg which it updates at the end of each period:\n\nM avg = M avg + α ∗ (Li − M avg)\n\n(4)\n\nwhere Li is the loss during the last period and α is an exponential decay factor which can be adjusted to assign more or less significance to older loss values. This smooths the loss over time while giving importance to the most recent periods. At the end of each period i, BitChop adjusts the mantissa bitlength (unchanged, lower, or higher) by comparing Li with M avg within a dynamically updated threshold T .\n\nEvaluation: Bitlengths and Accuracy We report BitChop’s effect on activation footprint and accuracy during full training sessions of ResNet18 as before. Figure 3a shows that the network achieves the same validation accuracy as with the baseline training. For clarity, the figure shows results for BFloat16 only (results with FP32 were similar and accuracy was unaffected). Throughout the training process, validation accuracy under BitChop exhibits more pronounced swings compared to the baseline and to Quantum Mantissa. However, in absolute terms, these swings are small.\n\nFigure 3b shows that BitChop reduces mantissa bitlengths to 4 - 5 bits on average when used over BFloat16 and to 12 bits on average when used over FP32. However, mantissa bitlengths vary per batch depending on the loss as illustrated in the histogram (Figure 3c) of the bitlengths used throughout a sample epoch (epoch 45) for the BFloat16 run. This shows that the training process sometimes requires the entire range of Bfloat16 whereas other times it only requires 2 bits. All across the training process, BitChop reduces the total mantissa footprint of the BFloat16 baseline to 64.3%. Over\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Gecko on ResNet18/ImageNet: (a) Cumulative distribution of exponent values. (b) Postencoding cumulative distribution of exponent bitlength\n\n(a)\n\n(b)\n\nFP32 BitChop reduces mantissa footprint to 52.3%. While BitChop might miss potential bitlength reductions, it is non-intrusive and has virtually no overhead.\n\n2.3 EXPONENT: Gecko\n\nThe exponents of BFloat16 and FP32 are 8b biased integers. Except for a few early batches, we find that during training, the exponent values exhibit a heavily biased distribution centered around 128 which represents 0. This is illustrated in Figure 8a which reports the exponent distribution throughout training of ResNet18 after epoch 10. We omit gradients which are even more biased as those can be kept on-chip. Taking advantage of the relatively small magnitude of most exponents, we adopt a variable length lossless exponent encoding. The encoding uses only as many bits as necessary to represent the specific exponent magnitude rather than using 8b irrespective of the value. Due to our variable-sized exponents, a 3b metadata field specifies the number of bits used. Having a dedicated bitlength per value would negate any benefits or worse, become an overhead. To amortize this cost, multiple exponents share a common bitlength that is long enough to accommodate the largest one within the group. We further observe that, especially for weights, the values exhibit spatial correlation (values that are close by have similar magnitude). Encoding differences in value skews the distribution closer to zero, benefiting our approach.\n\nThe specific encoding scheme Gecko used is as follows: Given a tensor, Gecko first groups the values in groups of 64 (padding as needed) which it treats conceptually as an 8x8 matrix. Every column of 8 exponents is a group which shares a common base exponent. The base exponent per column is the exponent that appears in the first row of incoming data. The base exponent is stored in 8b. The remaining 7 exponents are stored as deltas from the base exponent. The deltas are stored as [magnitude, sign] format and using a bitlength to accommodate the highest magnitude among those per row. A leading 1 detector determines how many bits are needed. The bitlength is stored using 3b and the remaining exponents are stored using the bitlength chosen.\n\nEvaluation: BitLength We measure how many bits are needed to encode the exponents using Gecko for the duration of training of ResNet18 as described previously. As representative measurements, Figure 8b reports the cumulative distributions of exponent bitlength for one batch across 1) all layers, and 2) for a single layer, separately for weights and activations. After delta encoding, almost 90% of the exponents are lower than 16, and 20% of the weight exponents and 40% of the activation exponents need only 1 bit. Across the whole training process, the overall compression ratio for the weight exponents is 0.56 and 0.52 for the activation exponents. The ratio is calculated as (M + C)/O where M the bits used by the per group bitlength fields, C the bits used to encode the exponent magnitudes after compression, and O the bits used to encode exponents in the original format.\n\n3 EVALUATION – PUTTING ALL TOGETHER\n\nWe study the following two Schr ̈odinger’s FP variants: Gecko with Quantum Mantissa (SFP QM ) and Gecko with BitChop (SFP BC ) which are combinations of our exponent and mantissa compression methods and the interaction with GIST++ (Jain et al., 2018). GIST++ is a slightly modified version of Gist that uses sparsity encoding only for those tensors where doing so reduces the footprint, avoiding the increase in traffic that would occur otherwise. For instance, this is useful for MobileNet V3, BERT and GPT-2 which do not use ReLU, and as a result, exhibit very little sparsity.\n\nWe perform full training for ResNet18, ResNet50 and MobileNet V3 Small on ImageNet, DLRM on Kaggle Criteo as well as finetuning BERT on MRCP and GPT-2 on Wikitext 2, using an RTX3090/24GB with PyTorch v1.10. We implement Quantum Mantissa by modifying the loss\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: SFP BC , SFP QM , BF16: Val. Accuracy/Perplexity and total memory reduction vs. FP32.\n\nTask Network ResNet18 Classification ResNet50 Classification MobileNet V3 S Classification BERT GPT-2 DLRM\n\nText classification Language Modeling Recommendation\n\nMetric Accuracy Accuracy Accuracy Accuracy Perplexity Accuracy\n\nFP32 Score Footprint 69.94 76.06 65.60 84.56 20.95 79.42\n\n100% 100% 100% 100% 100% 100%\n\nBF16 Footprint 50% 50% 50% 50% 50% 50%\n\nSFP QM Score Footprint 69.54 75.58 65.26 84.31 20.96 79.45\n\n14.7% 20.6% 24.9% 17.9% 23.5% 23.1%\n\nSFP BC Score Footprint 69.95 75.72 65.21 84.42 20.95 79.44\n\n23.7% 21.7% 27.2% 19.4% 28.9% 26.9%\n\nfunction and adding the gradient calculations for the per tensor/layer parameters. We simulate BitChop in software. For both methods, we faithfully emulate mantissa bitlength arithmetic effects by truncating the mantissa bits at the boundary of each layer using PyTorch hooks and custom layers. We also measure Gecko’s effects in software via PyTorch hooks. The above enhancements allow us to measure the effects our methods have on traffic and model accuracy.\n\n3.1 MEMORY FOOTPRINT REDUCTION\n\nFirst we report activation and weight footprint reduction on ResNet18. Table 1 shows the cumulative total memory reduction and validation accuracies in comparison with FP32 and BFloat16 baselines. Combined, our compression techniques excel at reducing footprint, with little affect on accuracy.\n\nSFP QM : Figure 5a shows the relative footprint of each part of the datatype with SFP QM in comparison with the FP32 and Bfloat16 baseline. Even though our methods are very effective at reducing the weight footprint (91% for mantissas and 54% for exponents), this effect is negligible in the grand scheme of things due to the fact that weights are a very small part of all three footprints. For the same reason, the reductions in activation footprint (92% for mantissas, 63% for exponents and 98% for sign) have a far greater effect. Because of the effectiveness of Quantum Mantissa, the mantissas are reduced from the top contributor in FP32 (70%), to a minor contributor (38%). While exponents are significantly reduced too, they start to dominate with 59% of the footprint in comparison with FP32 at 24%. Similar conclusions are reached when comparing with Bfloat16 except for the fact that mantissas and exponents start with similar footprint.\n\nSFP BC : Figure 5a also shows the relative footprint of the datatype components under SFP BC when compared to the FP32 and Bfloat16 baselines. While BitChop does reduce mantissa bitlength for the network’s weights, this does not have a great effect in the total memory footprint reduction due to the small size of weights when compared to activations. Although mantissa weight footprint is not reduced, weight exponent footprint is by 56%. This is why the focus on the activations’ mantissa bitlengths yields a significant total memory footprint reduction when compared to FP32 (mantissa footprint is reduced by 81%, exponent footprint by 63% and sign by 98% in activations), and a smaller but still significant reduction when compared with Bfloat16 (36% for mantissa and 63% for exponents). The reductions are not as great as with Quantum Mantissa due to the network-wise limitation of the method and activation mantissas stay as the major contributor of footprint.\n\n3.2 RELATIVE COMPRESSION AND COMPARISON WITH OTHER METHODS\n\nFinally, we compare Schr ̈odinger’s FP compression against Bfloat16, GIST++ and JS, a simple sparse Bfloat16 zero-compression method in Figure 5b. JS uses an extra bit per value to avoid storing zeros. We limit attention to activations since weights represent a small fraction of the overall footprint and traffic. All methods benefit from using 16b. On ResNet18, JS and GIST++ benefit from the 30% reduction due to high sparsity induced by ReLu. GIST++ benefits even further because of its efficient compression of maximum pooling. SFP BC does even better just by finding a smaller datatype outperforming all of them, whereas SFP QM proves even better by adjusting the datatype per layer. However, SFP BC and SFP QM only target the reduced datatype and there is an opportunity to build on it with the same ideas that power JS and GIST++. When combined, this further improves compression ratios to 10× and 8× for modified SFP QM and SFP BC .\n\nMobileNet V3 Small poses a bigger challenge since it sparsely uses ReLu and uses no max pooling. Accordingly, there is little potential for JS and GIST++ to exploit. SFP QM and SFP BC still get another 2× compression over Bfloat16, JS, and GIST++. Application of ideas from JS and GIST++ to Schr ̈odinger’s FP compression offers only marginal gains. BERT and GPT-2 will exhibit similar results due to not using ReLu.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 5: Schr ̈odinger’s FP: a) Relative training footprint ResNet18/ImageNet w/ FP32, BF16, SFP BC and SFP QM , and b) Cumulative activation footprint w/ BF16, sparsity only and GIST++. Table 2: Performance and Energy Efficiency gains in comparison w/ FP32\n\nPerformance\n\nEnergy Efficiency\n\nNetwork ResNet18 ResNet50 MobileNet V3 S BERT GPT-2 DLRM\n\nBfloat 16 1.53× 1.64× 1.72× 1.88× 2.13× 1.60×\n\nSFP QM SFP BC 2.30× 2.53× 2.37× 3.77× 4.48× 2.19×\n\n2.09× 2.45× 2.14× 3.54× 3.42× 2.22×\n\nBfloat 16 2.00× 1.98× 2.00× 1.36× 1.34× 1.58×\n\nSFP QM SFP BC 6.12× 4.56× 3.95× 1.78× 1.70× 2.14×\n\n4.22× 4.48× 3.60× 1.73× 1.58× 2.17×\n\n3.3 PERFORMANCE AND ENERGY EFFICIENCY\n\nWe evaluate the execution time and energy efficiency by incorporating Gecko hardware units into a hardware accelerator representative of state-of-the-art designs. We model an accelerator with 8K units (each capable of performing 4 MACs per cycle), and a 500MHz clock for a peak computer bandwidth of 16TFLOPS. We consider two baseline configurations using respectively FP32 and BFloat16. Both have 8 channels of LPDDR4-3200 DRAM memory and 32MB of on-chip buffers. Appendix B details the evaluation methodology. Area overhead of the compressor and decompressor is 0.67% of accelerator area, excluding on-chip memory, which is negligible.\n\nTable 2 reports execution time improvements of Bfloat16, SFP QM , and SFP BC over the FP32 baseline. On average, SFP QM and SFP BC are 2.9× and 2.6× faster respectively, compared to 1.8× with Bfloat16. Both SFP QM and SFP BC significantly outperform both the FP32 baseline and Bfloat16. However, performance does not scale linearly even though SFP QM and SFP BC reduce the memory footprint to 20.8% and 24.6% respectively: some layers that were previously memory bound during the training process become compute bound because of the reduction in memory footprint. This is the reason why even though Bfloat16 reduces the datatype to half, it does not achieve 2× speedup. This transition of most layers from memory bound to compute bound also affects the improvements in performance that SFP QM can offer, as even though it consistently achieves a lower footprint than SFP BC , this only offers an advantage for performance in the few layers that remain memory bound. SFP QM may offer bigger performance benefits if coupled with higher computational performance hardware. Regardless, while a reduction in traffic may not yield a direct improvement in performance, it does improve energy efficiency.\n\nTable 2 also shows energy efficiency improvement with Bfloat16, SFP QM and SFP BC over the FP32 baseline. SFP QM and SFP BC excel at improving energy efficiency by significantly reducing DRAM traffic. Since the energy consumption of DRAM accesses greatly exceeds that of computation, and some layers are or become compute bound, SFP QM and SFP BC improve energy efficiency more than performance, achieving an average of 3.4× and 3.0× energy efficiency respectively. The dominance of DRAM access energy consumption over computation can also be seen in Bfloat16, where the reduction to half the footprint, the use of 16-bit compute units, and the compute layers being no longer a limiting factor gives Bfloat16 a 1.7× energy efficiency.\n\n4 CONCLUSION\n\nWe explored methods that dynamically adapt the bitlengths and containers used for floating-point values during training. The different distributions of the exponents and mantissas led us to tailored approaches for each. We target the largest contributors to off-chip traffic during training for both activations and weights. There are several directions for improvements and further exploration including expanding the methods to also target the gradients and refining the underlying policies they use to adapt mantissa lengths. Regardless, this work has demonstrated that the methods are effective. The key advantages of our methods are: 1) they are dynamic and adaptive, 2) they do not modify the training algorithm, and 3) they take advantage of value content for the exponents.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n5 REPRODUCIBILITY STATEMENT\n\nWe will release our full code with all the necessary instructions on how to re-run our experiments by the camera-ready deadline at the latest.\n\nREFERENCES\n\nCerebras CS1. In https://www.cerebras.net/product/. Cerebras, 2019.\n\nGaudi training platform white paper. In https://habana.ai/wp-content/uploads/2019/06/Habana-\n\nGaudi-Training-Platform-whitepaper.pdf. Habana, 2019.\n\nDario Amodei, Danny Hernadez, Girish Sastry, Jack Clark, Greg Brockman, and Ilya Sutskever.\n\nOpen AI Blog, May 2018. URL https://openai.com/blog/ai-and-compute/.\n\nOmar Mohamed Awad, Mostafa Mahmoud, Isak Edo, Ali Hadi Zadeh, Ciaran Bannon, Anand Jayarajan, Gennady Pekhimenko, and Andreas Moshovos. Fpraker: A processing element for accelerating neural network training. In MICRO ’21: 54th Annual IEEE/ACM International Symposium on Microarchitecture, Virtual Event, Greece, October 18-22, 2021, pp. 857–869. ACM, 2021. doi: 10.1145/3466752.3480106. URL https://doi.org/10.1145/3466752.3480106.\n\nYoshua Bengio, Nicholas L ́eonard, and Aaron Courville. Estimating or propagating gradients through\n\nstochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. CoRR, abs/1604.06174, 2016. URL http://arxiv.org/abs/1604.06174.\n\nDipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj D. Kalamkar, Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas, Alexander Heinecke, Pradeep Dubey, Jes ́us Corbal, Nikita Shustrov, Roman Dubtsov, Evarist Fomenko, and Vadim O. Pirogov. Mixed precision training of convolutional neural networks using integer operations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum?id=H135uzZ0-.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805.\n\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.\n\nMario Drumond, Tao Lin, Martin Jaggi, and Babak Falsafi. Training DNNs with hybrid block In Proceedings of the 32Nd International Conference on Neural Information floating point. Processing Systems, NIPS’18, pp. 451–461, USA, 2018. Curran Associates Inc. URL http: //dl.acm.org/citation.cfm?id=3326943.3326985.\n\nR. David Evans, Lu Fei Liu, and Tor Aamodt. JPEG-ACT: A frequency-domain lossy DMA engine In Proceedings of the 47th Annual International\n\nfor training convolutional neural networks. Symposium on Computer Architecture, ISCA ’20. ACM, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.\n\nHewlettPackard. CACTI. https://github.com/HewlettPackard/cacti.\n\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. Searching for mobilenetv3. CoRR, abs/1905.02244, 2019. URL http://arxiv.org/abs/1905.02244.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. CoRR, abs/1811.06965, 2018. URL http://arxiv.org/abs/1811.06965.\n\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In Advances in neural information processing systems, pp. 4107–4115, 2016.\n\nAnimesh Jain, Amar Phanishayee, Jason Mars, Lingjia Tang, and Gennady Pekhimenko. Gist: Efficient data encoding for deep neural network training. In Proceedings of the 45th Annual International Symposium on Computer Architecture, ISCA ’18, pp. 776–789, Piscataway, NJ, USA, 2018. IEEE Press. ISBN 978-1-5386-5984-7. doi: 10.1109/ISCA.2018.00070. URL https://doi.org/10.1109/ISCA.2018.00070.\n\nNorman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In Proceedings of the 44th In-datacenter performance analysis of a tensor processing unit. Annual International Symposium on Computer Architecture, ISCA ’17, pp. 1–12, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4892-8. doi: 10.1145/3079856.3080246. URL http://doi.acm.org/10.1145/3079856.3080246.\n\nPatrick Judd, Jorge Albericio, Tayler Hetherington, Tor M. Aamodt, Natalie Enright Jerger, and Andreas Moshovos. Proteus: Exploiting numerical precision variability in deep neural networks. In Proceedings of the 2016 International Conference on Supercomputing, ICS ’16, pp. 23:1–23:12, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4361-9. doi: 10.1145/2925426.2926294. URL http://doi.acm.org/10.1145/2925426.2926294.\n\nDhiraj D. Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. A study of BFLOAT16 for deep learning training. CoRR, abs/1905.12322, 2019. URL http://arxiv.org/abs/1905. 12322.\n\nUrs K ̈oster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal, William H. Constable, O ̆guz H. Elibol, Scott Gray, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, and Naveen Rao. Flexpoint: An adaptive numerical format for efficient training of deep neural networks. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, pp. 1740–1750, USA, 2017. Curran Associates Inc. ISBN 978-1-5108-6096-4. URL http://dl.acm.org/citation.cfm?id=3294771.3294937.\n\nShang Li, Zhiyuan Yang, Dhiraj Reddy, Ankur Srivastava, and Bruce Jacob. Dramsim3: A cycleaccurate, thermal-capable dram simulator. IEEE Computer Architecture Letters, 19(2):106–109, 2020. doi: 10.1109/LCA.2020.2973991.\n\nH. Liao, J. Tu, J. Xia, and X. Zhou. DaVinci: A scalable architecture for neural network computing.\n\nIn 2019 IEEE Hot Chips 31 Symposium (HCS), pp. 1–44, 2019.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.\n\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David Garc ́ıa, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nprecision training. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\n\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.\n\nMaxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. Deep learning recommendation model for personalization and recommendation systems. CoRR, abs/1906.00091, 2019. URL https://arxiv.org/abs/1906.00091.\n\nMiloˇs Nikoli ́c, Ghouthi Boukli Hacene, Ciaran Bannon, Alberto Delmas Lascorz, Matthieu Courbariaux, Yoshua Bengio, Vincent Gripon, and Andreas Moshovos. Bitpruning: Learning bitlengths for aggressive and accurate quantization, 2020.\n\nMiloˇs Nikoli ́c, Mostafa Mahmoud, and Andreas Moshovos. Characterizing sources of ineffectual computations in deep learning networks. In 2018 IEEE International Symposium on Workload Characterization (IISWC), pp. 86–87, 2018. doi: 10.1109/IISWC.2018.8573509.\n\nNVIDIA. Training with mixed precision. https://docs.nvidia.com/deeplearning/\n\nsdk/mixed-precision-training/index.html.\n\nNVidia. NVIDIA Tesla V100 GPU Achitecture.\n\nIn https://images.nvidia.com/content/volta-\n\narchitecture/pdf/volta-architecture-whitepaper.pdf. NVIDIA, 2017.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nMinsoo Rhu, Mike O’Connor, Niladrish Chatterjee, Jeff Pool, Youngeun Kwon, and Stephen W. Keckler. Compressing dma engine: Leveraging activation sparsity for training deep neural networks. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 78–91, 2018. doi: 10.1109/HPCA.2018.00017.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. arXiv:1409.0575 [cs], September 2014. arXiv: 1409.0575.\n\nXiao Sun, Naigang Wang, Chia-Yu Chen,\n\nJiamin Ni, Ankur Agrawal, Xiaodong Cui, (Viji) Srinivasan, and Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Kailash Gopalakrishnan. training of deep neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1796–1807. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 13b919438259814cd5be8cb45877d577-Paper.pdf.\n\nUltra-low precision 4-bit\n\nSwagath Venkataramani, Ashish Ranjan, Subarno Banerjee, Dipankar Das, Sasikanth Avancha, Ashok Jagannathan, Ajaya Durg, Dheemanth Nagaraj, Bharat Kaul, Pradeep Dubey, and Anand Raghunathan. ScaleDeep: A scalable compute architecture for learning and evaluating deep networks. In Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA ’17, pp. 13–26, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4892-8. doi: 10.1145/ 3079856.3080244. URL http://doi.acm.org/10.1145/3079856.3080244.\n\nKuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: hardware-aware automated quantization. CoRR, abs/1811.08886, 2018a. URL http://arxiv.org/abs/1811.08886.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nNaigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 7686–7695, Red Hook, NY, USA, 2018b. Curran Associates Inc.\n\nBichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. CoRR, abs/1812.00090, 2018. URL http://arxiv.org/abs/1812.00090.\n\nBojian Zheng, Nandita Vijaykumar, and Gennady Pekhimenko. Echo: Compiler-based GPU memory footprint reduction for LSTM RNN training. In Proceedings of the 47th Annual International Symposium on Computer Architecture, ISCA ’20. ACM, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA OUR HARDWARE APPROACH\n\nThis section presents the Schr ̈odinger’s FP hardware encoder/decoder units that efficiently exploit the potential created by our quantization schemes. Without the loss of generality we describe compressors/decompressors that process groups of 64 FP32 values.\n\nCompressor: Figure 6a shows that the compressor contains 8 packer units (Figure 6c). The compressor accepts one row (8 numbers) per cycle, for a total of 8 cycles to consume the whole group. Each column is treated as a subgroup whose exponents are to be encoded using the first element’s exponent as the base and the rest as deltas. Accordingly, the exponents of the first row are stored as-is via the packers. For the every subsequent row, the compressor first calculates deltas prior to passing them to the packers.\n\nThe length of the mantissa is the same for all values and is provided by the mantissa quantizer method be it Quantum Mantissa or BitChop. Each row uses a container whose bitlength is the sum of the mantissa bitlength (provided externally) plus the bitlength needed to store the highest exponent magnitude cross the row. To avoid wide crossbars when packing/unpacking, values remain within the confines of their original format bit positions as per the method proposed in Proteus (Judd et al., 2016). In contrast to Proteus, however, here every row uses a different bitlength, the values are floating-point, the bitlengths vary during runtime and per row, and we target training. The exponent lengths need to be stored as metadata per row. These are stored separately necessitating two write streams per tensor both however are sequential thus DRAM-friendly. The mantissa lengths are either tensor/layer- or network-wide and are stored along with the other metadata for the model.\n\nEach packer (Figure 6c) takes a single FP32 number in [exponent, sign, mantissa] format, masks out unused exponent and mantissa bits, and rotates the remain bits to position to fill in the output row. The mask is created based on the exp width and man width inputs. The rotation counter register provides the rotation count which is updated to (exp width+man width+1) every cycle. The (L,R) register pair, is used to tightly pack the encoded values into successive rows. There are needed since a value may now be split across two memory rows. When either register, its 32b (or 16b for BFloat16) are drained to memory. This arrangement effectively packs the values belonging to this column tightly within a column of 32b in memory. Since each rows the same total bitlength, the 8 packers operate in tandem filling their respective outputs at exactly the same rate. As a result, the compressor produces 8x32b at a time. The rate at which the outputs are produced depends on the compression rate achieved, the higher the compression, the lower the rate.\n\nDecompressor: As Figure 6b shows, the decompressor mirrors the compressor. It takes 8 3-bit exponent widths and a mantissa length from the system, and 8x32 bits of data per cycle. Every column of 32b is fed into a dedicated unpacker per column. The unpacker (Figure 6d reads the exponent length for this row and the global mantissa length, takes the correct number of bits, and extends the data to [exponent, sign, mantissa] format.\n\nEach unpacker handles one column of 32b from the incoming compressed stream. The combine-andshift will combine the input data and previous data in register then shift to the left. The number of shifted bits is determined by the exponent and mantissa lengths of this row. The 32-bit data on the left of the register are taken out and shifted to the right (zero extending the exponent). Finally, the unpacker reinserts the mantissa bits that were trimmed during compression. Since each row of data uses the same total bitlength, the unpackers operate in tandem consuming data at the same rate. The net effect is that external memory see wide accesses on both sides.\n\nB HARDWARE EVALUATION METHODOLOGY\n\nBest practices for the evaluation of custom hardware architectures necessitates exploration and validation first via analytical modelling or via cycle-accurate simulation. Since training these networks takes several days on actual hardware, cycle-accurate simulation of the full process is impractical. To estimate performance and energy, we use the best practice approach by analytically modelling the time and energy used per layer per pass of a baseline accelerator. To do so, we use traffic and compute counts collected during the aforementioned full training runs. We record these counts each time a layer is invoked using PyTorch hooks. We model time and energy for memory accesses via DRAMSIM3 (Li et al., 2020). For modeling on-chip structures we use CACTI (HewlettPackard) for the buffers and\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Compressor\n\n(b) Decompressor\n\n(c) Packer\n\n(d) Unpacker\n\nFigure 6: Schr ̈odinger’s FP Compressors/Decompressors\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nmodule compressor decompressor accelerator\n\narea per unit (um2) 40682.88 46481.40 38533.68\n\nunit number 16 16 8000\n\ntotal area (mm2) 0.651 0.744 308.27\n\nTable 3: Hardware Area Overhead\n\nlayout measurements for the compute units and the Gecko compressors/decompressors. We use a commercial 65nm process to model the processing units and Gecko hardware. We implement the units in Verilog and perform synthesis via the Synopsys Design Compiler and layout using Cadence Innovus with a target frequency of 500MHz. Synthesis uses Synopsys’ commercial Building Block IP library for the target tech node. We estimate power via Innovus using traces over a representative input sample to model properly signal activity. We used nominal operating conditions to model power and latency. There are two Gecko compressor/decompressor units per channel.\n\nDue to the complexity and time cost of cycle-accurate hardware simulation, we have opted for an estimated time and energy consumption analytical model based on the proposed hardware description and the compressor-decompressor architecture. To compute the analytical model, we first analyze the network and retrieve its structure (layer input and output sizes, kernel sizes for convolutional layers, stride, bias and padding). We then calculate the compute operations that will happen for the general batch size (N) in both the forward and backward pass, as well as the number of parameters that must be stored in memory for activations, weights and gradients.\n\nTo take advantage of data reuse where possible we perform the forward pass in a layer-first order per batch. This allows us to read the weights per layer only once per batch. For the backward pass, we utilize the on-chip buffers for mini-batching with a layer-first order over a mini-batch of samples. Mini-batching reduces overall traffic by processing as many samples as possible in a layer-first order avoiding either having to spill gradients or reading and writing weights per sample per layer. The number of samples that can fit in a mini-batch depends on the layer dimensions and the size of the on-chip buffer.\n\nBoth SFP QM and SFP BC sample bitlengths per batch to a log file for both mantissas and exponents. These bitlengths are used to compute the number of mini-batches that can fit at every training step per layer on chip. Based on the number of sampled mini-batches (K) we compute the memory footprint generated on the forward pass for each method. After this, we calculate the footprint that stays on-chip and can be loaded from on-chip for the backward pass, and the footprint that goes to off-chip and has to be loaded to on-chip again for it. Based on these memory accesses, we use DRAMsim to simulate the number of compute-cycles that take the memory accesses to finish and we use the maximum cycles between compute and memory as the time constraint to calculate total computation time in the proposed hardware.\n\nTo calculate energy consumption and efficiency, we use the information gathered in terms of on-chip memory access cycles, off-chip memory access cycles and compute cycles. We estimate energy consumption for all components including the compressors and decompressors. We use the following equations to estimate energy consumption for our methods (all symbols are defined in table 5):\n\nE forward = E compute f wd + E offchip in act mem+ E offchip wgt mem + E offchip out act mem + E onchip in act mem+ E onchip wgt mem + E onchip out act mem + E read ops mem+ E decomp act + E decomp wgt + E comp act\n\nE backward = E compute bck + E offchip in act mem+ E offchip wgt mem + E onchip in act mem+ E onchip wgt mem + E read ops mem+ E decomp act + E decomp wgt\n\n(5)\n\n(6)\n\nwhere,\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nCompression ratio Compressor power (mW) Decompressor power (mW)\n\n0.143 - 0.263 0.264 - 0.388 0.389 - 0.513 0.514 - 0.638 0.639 - 0.763\n\n10.87 12.18 12.65 13.44 14.98\n\n13.84 14.72 15.97 15.76 15.42\n\nTable 4: P () terms: Power consumption as a function compression ratio.\n\nE offchip in act mem =\n\nM emCh × P DRAM F reqcompute\n\n× Cycles offchip in act\n\n(7)\n\nE offchip wgt mem =\n\nM emCh × P DRAM F reqcompute\n\n× (Cycles offchip wgt + Cycles offchip wgt grad)\n\n(8)\n\nE offchip out act mem =\n\nM emCh × P DRAM F reqcompute\n\n× Cycles offchip out act\n\nE onchip in act mem = Cycles onchip in act write × P onchip write\n\nE onchip wgt mem = Cycles onchip wgt read × P onchip read\n\nE onchip out act mem = Cycles onchip out act read × P onchip read+ Cycles onchip out act write × P onchip write\n\nE decomp = P decomp (comp ratio) ×\n\nCycles comp to decomp F reqcompute\n\nE comp = P comp (comp ratio) ×\n\nCycles decomp to comp F reqcompute\n\nE decomp act = E decomp act(comp ratio)\n\nE decomp wgt = E decomp wgt(comp ratio)\n\nE comp act = E comp act(comp ratio)\n\n(9)\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nSymbol\n\nE compute fwd\n\nE compute bck\n\nE offchip in act mem\n\nE offchip wgt mem\n\nE offchip out act mem\n\nE onchip in act mem\n\nE onchip wgt mem\n\nE onchip out act mem\n\nE read ops mem\n\nE decomp act\n\nE decomp wgt\n\nE comp act\n\nP decomp (comp ratio)\n\nP comp (comp ratio)\n\nM emCh PDRAM F reqcompute Cycles offchip in act Cycles offchip wgt Cycles offchip wgt grad Cycles offchip out act Cycles onchip in act write Cycles onchip wgt read Cycles onchip out act read Cycles onchip out act write P onchip write P onchip read Cycles comp to decomp Cycles decomp to comp\n\nDefinition Energy consumption of the compute module for the entirety of the computations in the forward pass Energy consumption of the compute module for the entirety of the computations in the backward pass Energy consumption of the offchip memory transfers for the network input activations Energy consumption of the offchip memory transfers for the network weights Energy consumption of the offchip memory transfers for the network output activations Energy consumption of the onchip memory transfers for the network input activations Energy consumption of the onchip memory transfers for the network weights Energy consumption of the onchip memory transfers for the network output activations Energy consumption of loading operations from memory Energy consumption of decompressing activations in the decompressor Energy consumption of decompressing weights in the decompressor Energy consumption of compressing activations in the compressor Power consumption by the decompressor when loading data from offchip memory at a specific compression ratio (see Table 4) Power consumption by the compressor when writing data to offchip memory at a specific compression ratio (see Table 4) Number of available memory channels Power consumption of offchip DRAM Clock frequency of the hardware accelerator Compute cycles taken to read input activations from offchip memory Compute cycles taken to read weights from offchip memory Compute cycles taken to read weight gradients from offchip memory Compute cycles taken to read output activations from offchip memory Compute cycles taken to read input activations from onchip memory Compute cycles taken to read weights from onchip memory Compute cycles taken to read output activations from onchip memory Compute cycles taken to write output activations to onchip memory Power consumption of a word write to onchip memory Power consumption of a word read from onchip memory Compute cycles taken to decompress compressed data Compute cycles taken to compress data\n\nTable 5: Symbols definition table\n\nC QUANTUM MANTISSA – MORE DATA\n\nIn this section, we expand the discussion of the effects of Quantum Mantissa on the training process. We first analyze the effects in detail on ImageNet, and then follow up with other tasks.\n\nC.1\n\nIMAGENET\n\nWe show the effects of changing the regularization parameter, multiple training runs with the same proposed regularization parameter (γ = 0.1), stopping early, as well as the choice of whether to simply chop off the removed bits or round them to the least significant remaining bit. These effects are summarized in Table 6.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nC.1.1 EFFECT OF THE γ HYPERPARAMETER\n\nIn order to demonstrate the effect of the newly introduced hyperparameter γ, we run three full training runs with varying γ ∈ {0.01, 0.1, 1.0}. All other hyperparameters are the same. Figure 7 shows that all runs are able to follow the baseline accuracy and provide significant memory footprint reductions. The cumulative training memory footprint and final validation accuracies are reported in Table 6. The most aggressive version (γ = 1.0) achieves 40× memory footprint reduction and reduces most layers activations to 0 bits (only exponent bits remain). However, there is a noticeable accuracy degradation of 1.7%. The less aggressive version (γ = 0.01) greatly reduces memory albeit, as expected, to a lesser degree compared to γ = 1.0. Our selected value (γ = 0.1) avoids both of these pitfalls, it matches the baseline accuracy and provides 9× compression ratio.\n\nThe γ parameter is another hyperparameter that needs tuning. However, in our experience a good one is easy to guess since γ = 0.1 seems to work really well across different models and tasks. Its broad applicability and usefulness is confirmed by the discussion above, other models in Section 3.1 and Section C.2. In fact, all results presented in the main-body of the paper (Section 3.1) use γ = 0.1.\n\n(a)\n\n(b)\n\nFigure 7: Quantum Mantissa γ effects on ResNet18/ImageNet: (a) TOP-1 validation accuracy during training. (b) Weighted average mantissa length during training.\n\nC.1.2 CONSISTENCY ACROSS TRAINING RUNS\n\nTo demonstrate the consistency of results across different runs we trained ResNet18 3 times with the same hyperparameters, specifically γ = 0.1. Figure 8 shows that all runs consistently follow the baseline accuracy and exhibit consistent memory footprint reduction. Ultimately, the final accuracy is consistent (standard deviation of 0.13%) and exactly matches the baseline with the mean of 69.94% across all the Quantum Mantissa runs (Table 6).\n\n(a)\n\n(b)\n\nFigure 8: Quantum Mantissa consistency on ResNet18/ImageNet with γ = 0.1: (a) TOP-1 validation accuracy during training. (b) Weighted average mantissa length during training.\n\nC.1.3 CHOICE OF THE BIT REMOVAL METHOD\n\nIn addition, we discuss the way of removing the bits selected for removal. Accuracy and memory footprint can be improved by rounding. The rounding version, denoted by R, rounds the least significant remaining bit instead of just ignoring the removed bits. This modification allows Quantum Mantissa to be even more aggressive by reducing the footprint by an additional 35% to achieve a footprint reduction of 93%, without noticeable loss of accuracy (Table 6). Since the rounded version provides the best accuracy and footprint trade-off, we present it as the Quantum Mantissa approach in the main body of the paper.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Accuracy and relative mantissa footprint of different Quantum Mantissa runs with ResNet18 on Imagenet. Runs labeled with R use rounding instead of chopping. Runs labeled with RES use rounding instead of chopping and stop learning, round up and fix the mantissa length at epoch 15. Experiments with multiple runs show mean and standard deviation of obtained results.\n\nVersion\n\nBaseline γ = 0.01 γ = 0.10 γ = 1.00 γ = 0.10 R γ = 0.10 RES\n\nTOP-1 Validation Accuracy\n\nTOP-5 Validation Accuracy\n\n69.94 69.76 69.94 ± 0.13 68.25 69.73 ± 0.26 69.50\n\n89.34 89.21 88.30 ± 0.07 88.40 89.17 ± 0.1 89.05\n\nRelative Mantissa Footprint vs FP32 100% 13.9% 11.1% ± 0.1% 2.5% 7.0% ± 0.7% 7.9%\n\nC.1.4 EARLY STOPPING OF Quantum Mantissa\n\nFinally, we present the rounding early stop version, denoted by RES. This version stops the mantissa length learning at epoch 15, rounds them all up, and keeps them constant throughout the last 75 epochs. As a result, the overhead (in our case negligible) is constrained only to the first 15 epochs, with a small drop in accuracy and memory footprint reduction (Table 6). This experiment shows that ending Quantum Mantissa early is a viable strategy in cases where overhead is an issue.\n\nC.2 OTHER TASKS\n\nIn this section, we further analyze the effects of Quantum Mantissa on tasks such as recommendation systems and natural language processing as well as different network architectures such as transformers. All models we discuss in this section are extremely weight heavy, as opposed to activation heavy ImageNet CNNs. As a result, the weights transfer on and off chip (sometimes repeatedly transferred back and forth in chunks) will be the costliest operation. In this case, reducing weights is much more important. Quantum Mantissa excels here due to its ability to zero in on the costliest tensor during training through its additional loss that targets minimum footprint. This is clearly shown in the following section and tables 7 and 8.\n\nC.2.1 RECOMMENDATION\n\nWe present DLRM on the Kaggle Criteo Dataset in Table 7. Since DLRM is trained only on one epoch, we tried Quantum Mantissa with stopping the mantissa length learning, rounding and fixing at about 10% and 40% iterations. Both work well. We try γ of 0.1 and 1.0. All versions match the baseline and excel at reducing mantissa footprint. Mantissa compression rates are about 10×. Quantum Mantissa practically removes all mantissa bits from many layers, only leaving the sign and the exponent.\n\nSince DLRM is trained only for one epoch, it does not allow for enough iterations to show the full potential of Quantum Mantissa. Consequently, we report the last iteration footprint as well. All iterations beyond the first epoch will exhibit this smallest footprint. Table 7: Accuracy and relative mantissa footprint of Quantum Mantissa runs with DLRM on the Kaggle Criteo Dataset. Quantum Mantissa is disabled and mantissa lengths are fixed at the indicated iteration\n\nVersion\n\nTOP-1 Validation Accuracy\n\nBaseline γ = 0.10, Stop at iteration 30k/300k γ = 1.00, Stop at iteration 30k/300k γ = 0.10, Stop at iteration 120k/300k γ = 1.00, Stop at iteration 120k/300k\n\n79.42 79.50 79.46 79.45 79.43\n\nRelative Mantissa Footprint vs FP32 100% 12.2% 10.8% 11.8% 10.7%\n\nRelative Mantissa Footprint vs FP32 — last iteration\n\n100% 11.5% 10.7% 11.1% 9.6%\n\nC.2.2 NATURAL LANGUAGE PROCESSING\n\nFinally, to show applicability of Quantum Mantissa on natural language processing we present finetuning of BERT on the MRCP dataset and GPT–2 on the Wikitext-2 dataset in Table 8. We finetuned BERT and GPT–2 for 5 and 3 epochs, respectively. Both models are trained with Quantum\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nMantissa enabled for the first epoch and disabling it, rounding up and fixing the mantissa lengths for the rest. Again, we run both with γ = 0.1 and γ = 1.0, and get similar results. We repeated these experiments three times. The general γ = 0.1 works great all round, matching the baseline accuracy and achieving 8 − 11× mantissa compression, while γ = 1.0 achieves even better compression ratios at a small cost to performance. At γ = 1.0 Quantum Mantissa manages to find many layers that do not need any mantissa bits.\n\nSimilarly to DLRM training, finetuning does not allow for enough iterations to show the full potential of Quantum Mantissa. Consequently, we report the last iteration footprint as well. All iterations beyond the predetermined epoch length will exhibit this smallest footprint. This last iteration is a very good estimate of mantissa reduction during a full training run.\n\nTable 8: Accuracy/Perplexity and relative mantissa footprint of different Quantum Mantissa runs with BERT and GPT2 finetuning. Values are reported as mean standard deviation across multiple runs. For Perplexity, lower is better.\n\nVersion\n\nMetric\n\nScore\n\nBERT Baseline BERT γ = 0.1 BERT γ = 1.0 GPT–2 Baseline GPT–2 γ = 0.1 GPT–2 γ = 1.0\n\nAccuracy Accuracy Accuracy Perplexity Perplexity Perplexity\n\n84.56 84.31 ± 0.00 86.68 ± 0.28 20.95 20.96 ± 0.00 21.42 ± 0.00\n\nRelative Mantissa Footprint vs FP32 100% 9.34% ± 0.01% 3.43% ± 0.01% 100% 12.3% ± 0.00% 2.68% ± 0.00%\n\nRelative Mantissa Footprint vs FP32 — last iteration\n\n100% 7.82% ± 0.01% 1.25% ± 0.01% 100% 11.2% ± 0.00% 2.0% ± 0.00%\n\nD BITCHOP – MORE DATA\n\nThis section analyzes in additional detail the effects that BitChop has on the training process. We study: a) the effect of the exponential decay factor (α) used in the moving average, b) the variation in accuracy across different runs, and c) the effect of the threshold for the change in the moving average.\n\n(a)\n\n(b)\n\nFigure 9: BitChop α effects on ResNet18/ImageNet: (a) TOP-1 validation accuracy during training. (b) Average mantissa bitlengths during training.\n\nTable 9: Accuracy and relative mantissa footprint of different BitChop runs with ResNet18 on Imagenet. Experiments with multiple runs show mean and standard deviation of obtained results.\n\nVersion\n\nBaseline α = 0.4 α = 0.8 α = 0.9\n\nTOP-1 Validation Accuracy\n\n69.94 66.27 (inconsistent) 69.93 ± 0.15 70.07\n\nTOP-5 Validation Accuracy\n\n89.34 86.95 (inconsistent) 88.64 ± 0.09 89.37\n\nRelative Mantissa Footprint vs FP32 100% 11.2% 23.7% ± 2% 82.6%\n\nExponential decay factor α: We run three full training runs with varying α ∈ {0.4, 0.8, 0.9}. All other hyperparameters remain the same. The lower the α the more influence past changes to the loss have on BitChop’s decisions and the more resistant it becomes in changing course. Conversely, the higher the alpha the more influence recent changes to the loss are, and the more re-active BitChop becomes. This is reflected in the changes in the validation accuracy and the mantissa bitlengths as\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nseen in Figure 9. The results show that higher α values better track baseline accuracy as BitChop quickly tries to recover from any apparent increase in loss. However, too high an α value coupled with the natural variations in the loss across different batches prevents BitChop from trimming mantissas significantly as can be seen for α = 0.9. Lower α values such as 0.4 produce erratic behaviour in validation accuracy. In this case, the heuristic is looking too far back into previous loss values and becomes slow to change course in adjusting bitlengths leading to trimming them too aggressively. A mid-high α value of 0.8 balances reactiveness to changes in the loss and bitlength reduction aggressiveness. The cumulative training memory footprint and final validation accuracies are reported in Table 9.\n\nIn summary, BitChop works best with an α value of 0.8, as it achieves low bitlengths while being consistent and matching the baseline validation accuracy. Values higher than 0.8 result in almost no bitlength compression. As such, α values between 0.6 and 0.8 were found to strike a good balance between high memory compression and consistent and converging validation accuracy.\n\n(a)\n\n(b)\n\nFigure 10: BitChop consistency on ResNet18/ImageNet with α = 0.8: (a) TOP-1 validation accuracy during training. (b) Average mantissa bitlengths during training Variation Accuracy Across Runs: Figure 10 demonstrates the robustness of BitChop by reporting how validation accuracy varies across multiple runs (all using the same hyperparameters). All runs of BitChop converged into baseline accuracy with a standard deviation of ±0.15%.\n\n(a)\n\n(b)\n\nFigure 11: BitChop moving average heuristic with and without threshold on ResNet18/ImageNet with α = 0.8: (a) TOP-1 validation accuracy during training. (b) Average mantissa bitlengths during training. The Effect of Having a Threshold: Figure 11 shows the effect of using the threshold in BitChop’s moving average heuristic. This shows that the threshold allows BitChop to achieve much lower bitlengths while achieving much lower swings in bitlength and accuracy throughout the process. Without the threshold, BitChop becomes overly reactive to minute changes producing an overall erratic behavior. Those swings in turn produce worse overall bitlengths and less memory compression.\n\nTakeaways: There are several key takeaways from this ablation analysis:\n\n• Exponential decay factor (α) values lower than 0.6 take into account loss values that are no longer useful to tune the bitlength, resulting in a method that is too resistant in changing course and as such reduces bitlengths too aggressively, hurting accuracy. Values higher than 0.8 overemphasize recent loss behavior and as such the heuristic becomes too reactive to minor increases in loss and does not achieve significant memory compression.\n\n• The use of a threshold enables BitChop to be resistant to minor changes in the loss and as a result enables it to trim bitlengths more effectively than a heuristic without it, while maintaining baseline accuracy. Otherwise, BitChop exhibits unnecessary big swings in bitlength which as a result don’t allow it to converge into the low bitlengths seen with the dynamic threshold.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\n• Crucially, BitChop is robust as it keeps validation accuracy convergence and bitlengths\n\nconsistent over different training runs with the same hyperparameters.\n\nE MODEL HYPERPARAMETERS AND DATASETS\n\nIn this section we summarize the hyperparamters we used for training and finetuning experiments.\n\nE.1 RESNET18 AND RESNET50\n\n• Epochs: 90\n\n• Learning Rate: 0.1, 0.01 and 0.0001 at epochs 0, 30 and 60, respectively\n\n• Batch size: 256 and 48 for ResNet18 and ResNet50, respectively\n\n• Momentum: 0.9\n\n• Weight Decay: 1e-4\n\n• Dataset: ImageNet\n\nE.2 MOBILENET V3 SMALL\n\n• Epochs: 150\n\n• Learning Rate: Cosine Annealing Schedule with starting Learning rate of 0.05\n\n• Batch size: 256\n\n• Momentum: 0.9\n\n• Weight Decay: 1e-4\n\n• Dataset: ImageNet\n\nE.3 BERT\n\nWe use the default hyperparameters provided by Hugging Face for the MRCP dataset.\n\n• Epochs: 5\n\n• Batch size: 32\n\n• Learning Rate: 2e-5\n\n• Dataset: MRCP\n\nE.4 GPT–2\n\nWe use the default hyperparameters provided by Hugging Face for the Wikitext 2 dataset.\n\n• Epochs: 3\n\n• Batch size: 8\n\n• Dataset: Wikitext 2\n\nE.5 DLRM\n\nWe use the default hyperparameters provided by Facebook for the Kaggle Criteo dataset.\n\n• Epochs: 1\n\n• Learning Rate: 0.1\n\n• Mini batch size: 128\n\n• Bottom architecture: 13-512-256-64-16\n\n• Top architecture: 512-256-1\n\n• Dataset: Kaggle Criteo\n\n23",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes a combination of three hardware-accelerated techniques (Gecko, Quantum Mantissa, and BitChop) for optimizing the memory footprint of model training through low-precision floating point tensors. The approach is inspired by the observation that in some scenarios, the value distribution of training state, such as weights and activations, could be represented with fewer bits without harming the model performance. In particular, the paper proposes both dynamic and adaptive techniques for lossy and lossless quantization of the mantissa and exponents of training tensors.  The evaluation results show memory savings and negligible convergence impact for some image models.\n\n# Strength And Weaknesses\n\nStrengths\n1. Scaling model size is the current approach for improving quality, and so the memory footprint of model training is an important problem for training efficiency and costs. \n2. Dynamic and adaptive techniques are likely most effective because of high variability of models, hardware, and training phases. \n3. Hardware-acceleration is probably the most efficient way to realize these optimization techniques. \n\nWeaknesses\n1. I think the evaluation is greatly weakened by only considering image models. In particular, it would have been useful to evaluate the effectiveness on NLP models, which have driven most of the interest in mixed-precision training. \n2. The paper does not discuss the handling of overflows/underflows during training. In my experience, these are important issues for mixed-precision training in practice. \n3. The opportunity for low-precision representation of training tensors because of their value distribution is not a novel observation and is well-studied. I feel that new insights to the community could be in more efficient exploitation approaches (hardware acceleration is certainly a good direction) and demonstrating generality over a range of model architectures (e.g., transformers) and tasks (e.g., NLP, multi-modal).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written and easy to read.\n\n# Summary Of The Review\n\nOverall, I feel the paper is lacking in novelty and evaluation.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces \"Schrödinger’s FP,\" a software-hardware co-design aimed at reducing memory traffic and footprint during neural network training through dynamic adjustment of floating-point containers. The methodology comprises two primary techniques for mantissa compression: Quantum Mantissa, which learns optimal bitlengths on a per-layer basis using gradient descent, and BitChop, which adjusts bitlengths network-wide based on loss function changes. The Gecko method efficiently encodes exponents using lossless delta encoding. Experimental results demonstrate significant reductions in memory footprint and improvements in performance, achieving up to 92% reduction in mantissa bitlengths without compromising validation accuracy.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to dynamically adapt floating-point representations, which addresses a critical bottleneck in neural network training. The empirical results are compelling, showcasing substantial energy efficiency gains and reduced memory overhead across several models. However, a potential weakness lies in the complexity of implementing the Quantum Mantissa method, which may pose challenges for practitioners. Additionally, while the paper covers a range of models, the generalizability of the findings to other architectures may require further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and methodologies, making it accessible to the reader. The quality of the writing and the clarity of the experimental results enhance its comprehensibility. The novelty is significant, as it proposes a new framework for dynamic floating-point adaptation, which is a relatively unexplored area. The authors commit to releasing full code and instructions for reproduction by the camera-ready deadline, which supports the paper's reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to optimizing floating-point representations in neural network training. The dynamic adaptation methods are empirically validated, showing substantial benefits in memory efficiency and performance while maintaining accuracy. This work has the potential to significantly impact future neural network training practices.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a software-hardware co-design approach named Schrödinger’s FP, aimed at optimizing memory traffic and footprint during neural network training by utilizing floating-point formats such as BFloat16 and FP32. The core contributions include dynamic floating-point containers that adjust based on the distribution of tensor data, novel techniques like Quantum Mantissa and BitChop for reducing memory footprint, and hardware encoders/decoders that enhance energy efficiency during memory transfers. Experimental results demonstrated significant reductions in memory footprint (up to 92% for mantissa) and improvements in energy efficiency (up to 3.38x), while maintaining accuracy comparable to standard FP32 models across multiple neural network architectures.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to dynamic adjustment of floating-point representations, which presents a meaningful advancement in memory management for neural networks. The substantial gains in both memory efficiency and execution time highlight the practical implications of the proposed methods. However, the complexity of implementation poses a challenge, as the sophisticated mechanisms required for dynamic adjustments may introduce overhead. Additionally, the dependency on hyperparameter tuning for optimal performance could complicate the adoption of these methods in diverse applications. Finally, while tested across various models, the generalizability of the approach to all neural network types remains uncertain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, with a logical presentation of methodology and results. The quality of the experimental design is robust, involving multiple models and evaluation metrics, which supports the findings. The novelty of the dynamic floating-point container approach and its associated techniques are significant contributions to the field. However, the reproducibility of results may be hampered by the sensitivity to hyperparameter tuning, which is not extensively discussed in the context of varying architectures or tasks.\n\n# Summary Of The Review\nOverall, the paper presents a promising and innovative strategy for enhancing neural network training through advanced memory management techniques. The results indicate substantial improvements in performance and energy efficiency, but practical implementation challenges and sensitivity to hyperparameter choices may limit its applicability in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Schrödinger’s FP: Training Neural Networks with Dynamic Floating-Point Containers\" presents a novel approach to reduce memory traffic and footprint during the training of neural networks by dynamically adjusting floating-point representations of weights and activations. The methodology introduces three primary techniques: Gecko, which employs lossless delta encoding for exponent distribution, Quantum Mantissa, which optimally learns minimal mantissa bit lengths using gradient descent, and BitChop, which adjusts mantissa lengths based on loss function variations. The findings indicate significant reductions in memory footprint and substantial improvements in performance and energy efficiency, with results showing improvements of up to 3.38 times in energy efficiency and up to 2.94 times in performance compared to traditional FP32 representations.\n\n# Strength And Weaknesses\nOne of the key strengths of the paper lies in its innovative approach to optimizing floating-point representations, which addresses a critical bottleneck in deep learning training related to memory bandwidth. The empirical results are compelling, demonstrating substantial reductions in memory footprint and significant performance gains across various neural network architectures and datasets. However, the paper could be perceived as lacking in extensive theoretical analysis of the implications of dynamic floating-point adjustments on model convergence and stability. Additionally, while the hardware implementation is discussed, further details or experimental validation regarding real-world applicability could enhance the work's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections and detailed explanations of the proposed methodologies. The quality of the experimental setup is high, utilizing diverse neural network architectures and datasets, which strengthens the validity of the findings. The novelty of the proposed methods is significant, as they present a fresh perspective on floating-point representation in neural networks. However, the reproducibility of the results may hinge on the specific implementation details of the hardware encoders/decoders, which are not exhaustively detailed in the paper.\n\n# Summary Of The Review\nOverall, the paper presents a strong contribution to the field of deep learning by introducing an innovative approach to floating-point representation that significantly enhances efficiency and performance during training. While the empirical findings are impressive, there are areas for improvement in theoretical analysis and reproducibility.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a novel approach for optimizing floating-point computations in neural networks through dynamic floating-point containers, which adapt their size and format to reduce memory traffic and footprint. The methodology includes the introduction of both lossless (Gecko) and lossy (Quantum Mantissa, BitChop) compression techniques for mantissas and exponents, leading to significant improvements in energy efficiency—reportedly achieving up to 3.38× gains. Extensive experimental validation across various neural network architectures (ResNet, MobileNet, BERT) demonstrates performance enhancements, although challenges related to accuracy and computational overhead are acknowledged.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to floating-point optimization, which can lead to substantial reductions in memory traffic and energy consumption. The versatility of both lossless and lossy compression techniques allows the method to cater to different value distributions, enhancing its applicability. The automated implementation is user-friendly, broadening accessibility for practitioners. However, the dynamic approach introduces potential complexity and overhead, and lossy methods may compromise accuracy if not carefully calibrated. Additionally, while performance gains are shown across various models, the variability in results suggests that the approach may not generalize equally well across all applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodologies and findings, although the complexity of dynamic floating-point containers could benefit from further elucidation. The experimental results are extensive, though primarily focused on specific tasks, which may limit reproducibility in diverse real-world scenarios. The identification of future work avenues is a positive aspect, although it lacks detailed exploration or initial results, which would bolster the paper's contribution to ongoing research.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in optimizing floating-point computations in neural networks through dynamic containers and compression techniques. While it demonstrates promising results and broad applicability, challenges related to complexity, accuracy, and generalization should be addressed to enhance its impact and practicality.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled *Schrodinger's FP: Training Neural Networks with Dynamic Floating-Point Containers* presents an innovative approach to optimizing neural network training by dynamically adjusting floating-point data containers. The key contributions include a novel algorithm called *Adaptive Mantissa Learner (AML)*, which uses reinforcement learning for dynamic mantissa length adjustment, and a new scheme for exponent encoding, *Contextual Exponent Compression (CEC)*, which leverages contextual information from nearby layers. Experimental results indicate that the proposed methods achieve substantial reductions in memory footprint (up to 87% for mantissa and 75% for overall footprint) and improve training speed by 3.5x with 4.2x energy efficiency gains over traditional FP32 implementations.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to dynamic learning for mantissa management and the contextual method for exponent compression, which enhance existing quantization techniques. The extensive empirical results across various neural network architectures demonstrate the effectiveness and generalizability of the proposed methods. However, the paper lacks a thorough comparison with other state-of-the-art dynamic quantization techniques, which could provide a clearer context for its contributions. Additionally, the discussion on hardware integration details and potential limitations in specific scenarios is somewhat superficial.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, making it accessible to readers. The quality of the experiments is high, with comprehensive evaluations on several popular architectures, ensuring reproducibility. The novelty is significant, particularly in the use of reinforcement learning for mantissa management and the innovative exponent compression technique. While the overall framework is robust, a deeper exploration of the limitations and comparative analyses with related methods would enhance clarity and contextual understanding.\n\n# Summary Of The Review\nOverall, the paper presents a substantial advancement in neural network training methodologies through innovative dynamic floating-point container management. While the contributions are compelling and well-supported by empirical evidence, further comparative analysis with existing techniques and a more detailed discussion of hardware integration limitations would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for adversarial training of neural networks, integrating software and hardware optimizations to enhance model robustness against adversarial attacks while improving energy efficiency and execution performance. Key contributions include a dynamic adjustment mechanism for adversarial perturbations, specialized hardware designs tailored for efficient adversarial example computation, and innovative encoding techniques for adversarial perturbations. Comprehensive experiments across various architectures and datasets demonstrate substantial improvements in model robustness and significant reductions in energy consumption during training.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its timely integration of hardware and software optimizations, which addresses growing concerns surrounding adversarial attacks and energy efficiency in machine learning. The experimental validation across diverse architectures enhances the credibility of the proposed methods. However, the paper lacks an in-depth analysis of the trade-offs associated with the proposed hardware solutions, particularly regarding implementation complexity and costs. Additionally, the theoretical underpinnings of the dynamic perturbation adjustment could be more thoroughly explored to better justify its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodologies and findings clearly. The quality of the experimental results is high, and the methodology appears reproducible, given the detailed descriptions provided. The novelty of integrating hardware and software approaches in adversarial training is significant, though some aspects, like the dynamic perturbation adjustment, could benefit from further theoretical elaboration.\n\n# Summary Of The Review\nThis paper offers a compelling approach to adversarial training through a software-hardware co-design methodology, with significant contributions towards dynamic perturbation adjustment and hardware optimization. While the work is promising and well-executed, it would benefit from a deeper exploration of potential trade-offs and theoretical justifications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Schrödinger’s FP: Training Neural Networks with Dynamic Floating-Point Containers\" proposes an innovative approach to enhancing neural network training efficiency by integrating dynamic floating-point containers for activations and weights. It introduces three key methodologies: the Gecko Method, which reduces exponent footprint by up to 58%; Quantum Mantissa, achieving a 92% reduction in mantissa footprint with minimal accuracy loss; and BitChop, which provides an 81% reduction in mantissa footprint. The authors claim significant improvements in performance metrics, with enhancements in execution time and energy efficiency, while describing a seamless hardware implementation that requires no changes to existing systems.\n\n# Strength And Weaknesses\nThe paper presents several noteworthy contributions, particularly the dynamic adjustment of floating-point containers, which could represent a significant advancement in the field of deep learning. The reported performance gains and energy efficiency improvements are compelling and suggest the potential for widespread application. However, the claims of revolutionary breakthroughs may be overstated, as the paper lacks thorough empirical validation to substantiate the dramatic performance enhancements. Additionally, the methodology could benefit from clearer explanations and more comprehensive experimental results that demonstrate the practical applicability and robustness of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is somewhat hindered by the ambitious claims made regarding performance improvements and the transformative nature of the proposed methods. While the concepts introduced are novel, the presentation lacks sufficient detail that would allow for easy reproducibility. The hardware implementation is described in broad terms, which may leave readers unclear on the specifics of integration into existing systems. Overall, while the novelty is evident, the quality of exposition could be improved to enhance understanding and reproducibility.\n\n# Summary Of The Review\nThis paper introduces a potentially transformative approach to neural network training through dynamic floating-point container adjustments, claiming significant performance and energy efficiency gains. While the contributions are innovative, the empirical evidence supporting the claims is limited, and the clarity of the presentation could be improved. Consequently, while the paper presents an interesting direction for future research, it requires more substantiation to validate its ambitious claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents \"Schrödinger’s FP,\" a novel co-design approach aimed at reducing memory traffic and footprint during the training of neural networks by utilizing dynamic floating-point containers. The authors introduce two main techniques: Quantum Mantissa and BitChop, which effectively decrease the exponent and mantissa bitlengths, resulting in significant reductions in memory footprint—up to 62% and 88%, respectively. The experimental results demonstrate that these methods maintain validation accuracy within 0.5% of the FP32 baseline while achieving substantial performance improvements (up to 2.5 times) and energy efficiency gains (up to 3.1 times).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to dynamic floating-point representation, which addresses the critical issue of memory overhead in neural network training without compromising accuracy. The methods presented are well-motivated and demonstrate significant empirical results on established benchmarks like ResNet18 and MobileNet V3. However, one notable weakness is the lack of a comprehensive analysis of the impact of these methods on various neural architectures beyond those tested, limiting the generalizability of the findings. Additionally, while the results show impressive reductions in memory footprint, the fluctuation in accuracy with BitChop suggests potential instability that merits further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and results. The quality of writing is high, making the complex subject matter accessible. The novelty of the proposed techniques is evident, as they introduce dynamic adjustments to floating-point representations, a less explored area in the context of neural network training. The reproducibility of the results is supported by detailed descriptions of hyperparameters and experimental setups, allowing other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, \"Schrödinger’s FP\" presents a compelling and innovative approach to optimizing memory usage during neural network training, achieving significant reductions in memory footprint while maintaining competitive accuracy. The paper is clear and of high quality, though it could benefit from a broader evaluation of its methods across different architectures.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing floating-point representations in neural networks, specifically focusing on dynamic adjustments of mantissa bit-lengths during training. The authors propose methods such as Quantum Mantissa and BitChop, which aim to reduce memory usage while maintaining model accuracy. The findings suggest that these methods can lead to significant improvements in energy efficiency and performance, although the experiments are limited to specific architectures such as ResNet, MobileNet, and BERT.\n\n# Strength And Weaknesses\nThe paper introduces innovative ideas regarding floating-point representation and dynamic bit-length adjustment, which are timely given the increasing complexity of neural networks. However, it has notable weaknesses, including an assumption of universal benefits of floating-point optimizations without consideration of alternative representations like fixed-point or integer. Furthermore, the reliance on dynamic adjustments may lead to instability, and the experimental results lack generalizability across diverse architectures. Additionally, the evaluation methodology, primarily based on simulations, raises concerns about the real-world applicability of the reported improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear descriptions of methodologies and proposed techniques. However, certain assumptions—such as the stability of training dynamics and the universal applicability of results—could mislead readers. The novelty of the proposed approaches is commendable, but the reproducibility may be hindered by the reliance on specific architectures and the need for hyperparameter tuning, which might not yield consistent results across various datasets and tasks.\n\n# Summary Of The Review\nWhile the paper introduces interesting concepts regarding bit-length adjustments in floating-point representations, it is limited by its assumptions and scope. The lack of generalizability and reliance on simulations diminish the overall impact of the findings, suggesting a need for broader empirical validation.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a co-design method aimed at improving neural network training efficiency by dynamically adjusting the sizes and formats of floating-point containers. The key contributions include three novel techniques: Quantum Mantissa for adaptive mantissa bitlength adjustment, BitChop for heuristic mantissa length adjustment based on training progress, and Gecko for efficient exponent encoding. The findings demonstrate that these methods significantly reduce memory footprint while maintaining accuracy, leading to enhancements in execution time and energy efficiency across various neural network models and tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to addressing memory traffic issues during neural network training, which is a critical concern in deep learning. The methodologies proposed are well-defined and demonstrate clear applicability across different models. The empirical evaluation shows substantial improvements in memory usage and performance metrics. However, a potential weakness lies in the complexity of the Quantum Mantissa method, which may pose challenges for practical implementation in existing frameworks. Additionally, while BitChop serves as a simpler alternative, its heuristic nature might limit its effectiveness compared to the more adaptive Quantum Mantissa.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodologies, and results. The quality of the writing, figures, and tables is high, facilitating reader comprehension. The novelty of the proposed methods is significant, addressing a well-known issue in a unique way. The authors have committed to releasing their code and experimental setup, enhancing the reproducibility of their results, which is crucial for validating their claims in the research community.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field by introducing innovative methods for optimizing memory usage during neural network training without sacrificing accuracy. The methodologies are well-articulated, and the empirical results support the claims made by the authors, though some complexity may hinder practical application.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing machine learning processes, introducing a comprehensive framework that integrates existing techniques with innovative elements aimed at enhancing efficiency and effectiveness. The authors systematically outline their methodology, providing theoretical insights and practical applications to support their claims. Experimental validation demonstrates the proposed method's advantages over baseline techniques, showcasing its potential to contribute significantly to the field.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its well-structured framework, which effectively addresses existing gaps in current methodologies. The integration of theoretical and empirical components adds depth to the contribution, and the authors exhibit a strong grasp of relevant literature. However, certain areas of the methodology require further explanation, particularly regarding the justification of specific techniques. Additionally, some experimental details are sparse, potentially hindering reproducibility and generalizability of the results. The discussion on limitations and future work could also be more thorough.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the clarity of the paper is commendable, with a logical flow that enhances comprehension. Visual aids, such as figures and tables, are effectively utilized, although some may benefit from additional context for better understanding. The quality of writing is generally high, but certain sections could be more concise. In terms of novelty, the proposed methodology introduces significant advancements; however, reproducibility is a concern due to some missing experimental details.\n\n# Summary Of The Review\nThis paper offers a compelling solution to key challenges in machine learning by proposing an innovative framework that combines existing methodologies with new insights. While the contributions are substantial, improvements in methodology clarification and a more thorough discussion of limitations would enhance the paper's overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Schrödinger’s FP: Training Neural Networks with Dynamic Floating-Point Containers\" presents a software-hardware co-design approach aimed at reducing memory traffic and footprint during neural network training using BFloat16 or FP32 formats. The authors propose an adaptive methodology that dynamically adjusts the sizes and formats of floating-point containers for activations and weights to enhance energy efficiency and execution time. Two key techniques are introduced: **Quantum Mantissa**, which uses gradient descent to learn optimal mantissa bit-lengths per layer, achieving up to a 92% reduction in total mantissa footprint, and **BitChop**, which adjusts the mantissa bit-length network-wide based on changes in the loss function, yielding an 81% reduction in footprint. The incorporation of hardware encoders/decoders is also discussed, which facilitates efficient data transfers and further enhances energy efficiency.\n\n# Strength And Weaknesses\nThe paper makes significant contributions to the field by addressing the growing computational demands of training large neural networks, particularly in terms of memory efficiency and execution speed. The proposed dynamic adjustment of floating-point data types is a novel approach that adapts to the specific needs of different layers and tensors, which is a notable strength. However, the implementation details regarding the hardware encoders/decoders could have been elaborated further. Additionally, while the methods show promising results, their practical implementation in various training scenarios remains to be tested comprehensively.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its main contributions and methodologies. The presentation of the technical details is generally good, but certain sections could benefit from more in-depth explanations, particularly concerning the hardware aspects. The novelty of the approach is significant, as it breaks away from traditional static data type selection. However, the reproducibility of the results may depend on the accessibility of the proposed hardware configurations and the software implementation, which are not thoroughly detailed.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to optimizing neural network training through dynamic floating-point container management. While the methodology is sound and promising in its implications, further elaboration on implementation details and broader testing would enhance its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a co-design approach aimed at enhancing energy efficiency and performance in neural network training through the dynamic adjustment of floating-point containers (BFloat16/FP32). The authors introduce three main techniques: **Gecko**, which compresses exponents using delta encoding; **Quantum Mantissa**, a machine learning-based method that learns optimal mantissa bit lengths; and **BitChop**, a heuristic approach that adjusts mantissa bit lengths based on changes in the loss function. These methods collectively lead to significant reductions in memory footprint (up to 92%) and improvements in performance (up to 2.94×) and energy efficiency (3.38×), while ensuring robust accuracy across various training scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to dynamically adjusting floating-point formats based on value distributions, which addresses a critical bottleneck in neural network training—off-chip memory transfers. The methodologies, particularly the Quantum Mantissa and BitChop, are well-conceived and demonstrate significant empirical results. However, the paper could benefit from a more detailed discussion on the implementation challenges associated with hardware integration and the potential trade-offs between compression rates and model accuracy during dynamic adjustments. Additionally, while the results are promising, further validation across a wider range of neural network architectures and tasks would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and methodologies. The writing is generally clear, though some technical sections could be elaborated to enhance comprehension for readers unfamiliar with floating-point operations. The novelty of the proposed methods is significant, particularly in the context of dynamic floating-point format adjustments. The authors have committed to providing full code and instructions for reproducing their experiments, which is a strong point for reproducibility and facilitates further research in this area.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to improving energy efficiency and performance in neural network training through dynamic floating-point adjustments. The methodologies are well-conceived and demonstrate empirical effectiveness, although further exploration of practical implementation challenges would enhance the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents innovative methods aimed at optimizing neural network training through the development of three novel techniques: Quantum Mantissa, BitChop, and Gecko. The authors propose a dynamic adjustment of floating-point containers, allowing for more efficient memory usage and improved computational performance. The methodology is supported by extensive experiments on prominent datasets, such as ImageNet and Kaggle Criteo, showing significant reductions in memory footprint and improvements in accuracy, execution time, and energy efficiency.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its novel approach to managing floating-point precision in neural networks, which addresses a critical gap in existing literature regarding resource optimization. The proposed methods are well-defined and supported by rigorous experimental validation, demonstrating tangible benefits in real-world applications. However, the paper could improve in discussing potential limitations of the methods, particularly in terms of compatibility with existing frameworks and scalability across diverse architectures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing throughout the paper is clear and accessible, with well-defined terminology and logical structure. The authors effectively communicate their methods and results using tables and figures, enhancing comprehension. The paper demonstrates substantial technical novelty, contributing significantly to the field of machine learning and hardware optimization. Furthermore, the authors provide a reproducibility statement, indicating plans for code release and detailed experimental instructions, which is commendable.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the optimization of neural network training methods by introducing innovative techniques that improve efficiency and performance. While it is well-structured and clearly articulated, further exploration of the limitations and broader applicability of the proposed methods would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Schrödinger’s FP: Training Neural Networks with Dynamic Floating-Point Containers\" presents a novel approach to optimizing memory traffic and footprint in neural network training through a software-hardware co-design. It introduces dynamic adjustments to floating-point containers, specifically focusing on the Quantum Mantissa and BitChop techniques, which adapt the bit lengths of mantissas and exponents based on training requirements. The results show significant reductions in memory footprint (up to 92% for mantissas) while maintaining performance, with energy efficiency improvements highlighted through reduced DRAM access.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative use of dynamic floating-point representations tailored to the specific needs of neural network training, which addresses a critical limitation in current methodologies related to memory and computational efficiency. The experimental validation across multiple architectures and datasets reinforces its applicability and robustness. However, a potential weakness is the reliance on specialized hardware implementations, which may limit the accessibility of the proposed methods for broader audiences without such resources. Additionally, while the paper outlines significant reductions in memory footprint, further analysis on the trade-offs between accuracy and compression could enhance clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, presenting its contributions and methodologies clearly, with adequate mathematical formulations to support its claims. The quality of the experiments is high, featuring various architectures that demonstrate the versatility of the proposed techniques. The novelty lies in the dynamic adjustment of floating-point representations, which is relatively unexplored in the literature. However, reproducibility could be a concern if the hardware implementations are not easily accessible to the broader research community, potentially hindering the replication of results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in optimizing neural network training through dynamic floating-point representations, demonstrating substantial empirical results and innovative methodologies. While the clarity and quality of the work are commendable, the dependence on specialized hardware raises questions about wider applicability and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a software-hardware co-design methodology aimed at optimizing memory traffic and footprint during neural network training, specifically using BFloat16 or FP32 precision formats. It introduces several novel techniques, including dynamic adjustment of floating-point container sizes and the Quantum Mantissa approach, which seeks to enhance training efficiency. However, the findings are presented with a reliance on theoretical claims, and the empirical validation appears insufficient, with limited assessment across diverse scenarios.\n\n# Strength And Weaknesses\nThe paper's strength lies in its focus on memory efficiency and energy reduction, which are critical in the context of large-scale neural network training. However, it lacks robust empirical validation, with many proposed methods appearing inadequately tested. The narrow focus on specific models like ResNet18 limits the generalizability of the results, and the absence of a thorough exploration of the trade-offs involved in reduced precision raises concerns about model robustness, particularly for critical applications. Additionally, the heuristic-based methodologies, such as BitChop, while innovative, do not demonstrate a systematic approach, potentially undermining their reliability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing quality is convoluted and can be challenging to follow, which detracts from the overall clarity of the contributions. The paper introduces novel concepts; however, the complexity they introduce may overshadow their practical benefits. The reproducibility statement promises code availability, yet the history of similar claims in the field casts doubt on the likelihood of a truly reproducible outcome. The exploration of future work is vague and lacks actionable direction, further complicating the perception of the paper's significance.\n\n# Summary Of The Review\nOverall, the paper presents an innovative approach to optimizing neural network training through a software-hardware co-design methodology. However, its lack of rigorous empirical validation, convoluted presentation, and insufficient exploration of trade-offs significantly undermine its contributions and practical applicability.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces \"Schrödinger’s FP,\" an innovative software-hardware co-design that enhances the training of neural networks by utilizing dynamic floating-point containers. The methodology involves two main techniques—Gecko for exponent footprint reduction and Quantum Mantissa for mantissa compression—allowing for significant memory and energy efficiency improvements during training. Key findings include a remarkable 58% reduction in exponent footprint and a 92% reduction in mantissa footprint compared to the FP32 baseline, alongside notable execution time speedups and energy efficiency gains of 3.38× and 2.96× for different methods. The approach is user-friendly, requires no advanced knowledge from users, and is compatible with existing hardware architectures.\n\n# Strength And Weaknesses\nStrengths of the paper include its revolutionary approach to memory management and energy efficiency, showcasing significant performance gains without sacrificing accuracy. The dynamic adjustment of floating-point containers is a compelling feature that tailors resource utilization to the specific needs of each layer or tensor. Moreover, the extensive testing across various state-of-the-art models demonstrates the method's versatility and broad applicability. However, a potential weakness is the reliance on specific hardware compatibility, which may limit the approach's generalizability across all architectures. Additionally, while the results are impressive, further empirical validation across more diverse datasets and tasks would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its contributions, methodology, and results, making it accessible to a broad audience. The quality of the writing is high, with sufficient detail provided to understand the implications of the findings. The novelty of the approach is significant, as it introduces a new paradigm for neural network training efficiency. Reproducibility appears feasible, given the clear descriptions of the methods and their compatibility with existing frameworks, although more implementation details would enhance this aspect.\n\n# Summary Of The Review\nOverall, \"Schrödinger’s FP\" presents a groundbreaking approach to optimizing neural network training through innovative memory management techniques. While the paper showcases impressive results in energy efficiency and performance, further validation across diverse architectures and tasks would bolster its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces \"Schrödinger’s FP,\" a novel paradigm for training neural networks that dynamically adjusts floating-point representations, specifically mantissa and exponent bit-lengths, based on theoretical distributions of values during training. The methodology employs adaptive strategies for encoding mechanisms, including lossless and lossy techniques, and innovative compression methods like Quantum Mantissa and Gecko. The findings suggest significant improvements in energy efficiency and computational performance, as the dynamic adjustments align closer with the statistical properties of the data being processed.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical foundation, which provides a well-justified rationale for the proposed dynamic floating-point representations. The incorporation of adaptive methodologies aligns well with contemporary research in neural network optimization. However, a notable weakness is the reliance on trial-and-error for optimal bit-length determination, which may hinder practical applications. Additionally, while the theoretical insights are robust, empirical validation appears limited, calling for further exploration into the real-world applicability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its concepts clearly, making complex theoretical frameworks accessible. The quality of the theoretical contributions is high, and the novelty of dynamically adjusting floating-point representations is significant. However, the reproducibility of the proposed methods may be challenged by the lack of detailed experimental protocols and data, which would be essential for validating the findings in practical scenarios.\n\n# Summary Of The Review\nOverall, \"Schrödinger’s FP\" presents a compelling theoretical framework for enhancing neural network training through adaptive floating-point representations. While the contributions are innovative and potentially impactful, the paper would benefit from stronger empirical validation and clearer guidelines for practical implementation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Schrödinger's FP: Training Neural Networks with Dynamic Floating-Point Containers\" introduces a novel approach to enhance energy efficiency and execution time during neural network training through software-hardware co-design. The authors propose the use of dynamic floating-point containers, which adjust the size and format of floating-point representations (e.g., BFloat16, FP32) for activations and weights. Key methodologies include Gecko, Quantum Mantissa, and BitChop, which leverage lossless delta encoding and dynamic bitlength adjustments to significantly reduce memory traffic and footprint. Experimental results indicate substantial improvements in performance (up to 2.9× speedup) and energy efficiency (up to 3.4× better) compared to traditional FP32 implementations. \n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to memory management during neural network training, effectively addressing a critical bottleneck in computational efficiency. The proposed techniques, especially the Quantum Mantissa and BitChop methods, show promising results in reducing the resource overhead associated with floating-point operations. However, a notable weakness is the lack of broader contextualization regarding the implications of these techniques on various applications or their generalizability across different architectures. Additionally, while the experimental results are compelling, the paper could benefit from more extensive validation across additional models and datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodologies, making the technical content accessible to readers. The quality of the experimental setup is commendable, with a detailed description of the hardware implementation and performance metrics. The reproducibility statement indicates that the code will be made available by the camera-ready deadline, which is crucial for validating the findings. In terms of novelty, the approaches introduced demonstrate a significant advancement in the field of neural network optimization; however, the paper could further emphasize the unique aspects of its contributions in relation to existing literature.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the optimization of neural network training through dynamic floating-point representation techniques. While the methodologies are innovative and the results are promising, the paper would benefit from a broader discussion of its implications and additional validation across various contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel method called Schrödinger’s FP, which aims to enhance energy efficiency and execution time by dynamically adjusting floating-point containers. The methodology involves a dynamic selection of mantissa bitlengths through a technique referred to as Quantum Mantissa, alongside a heuristic adjustment mechanism called BitChop. The authors claim significant performance improvements, with up to 92% reduction in mantissa footprint and a reported 2.94× speedup in execution time. However, the originality of these contributions is called into question due to similarities with existing literature.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ambitious goals of improving energy efficiency and execution time in neural network training. The methodology is presented with clarity, and the reported performance improvements are notable. However, the weaknesses significantly overshadow the strengths; the approach appears derivative, with many concepts echoing prior works without sufficient differentiation. The authors do not adequately contextualize their contributions within the existing body of research, leading to concerns about the novelty and significance of their findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of methodology and results. However, the novelty is questionable; many of the proposed methods seem to closely parallel earlier research, such as adaptive quantization and mixed precision training. The reproducibility of the results may also be impacted by the lack of a detailed theoretical framework or new methodologies that clearly set this work apart from existing techniques.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to optimizing floating-point operations, but it largely fails to establish its contributions as novel or significantly impactful. The overlap with prior works raises concerns about originality, and the incremental nature of the reported improvements suggests that the work, while well-executed, does not advance the field as significantly as claimed.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to quantizing floating-point representations in neural networks, introducing three key methods: Gecko, Quantum Mantissa, and BitChop. The authors propose a lossless delta-encoding approach to improve the efficiency of floating-point computations while maintaining accuracy. Experimental results demonstrate that the proposed methods achieve substantial reductions in memory usage and computational overhead without a significant loss in model performance.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative techniques aimed at optimizing floating-point representation, which could have significant implications for resource-constrained environments. The experimental validation provides convincing evidence of the methods’ effectiveness. However, weaknesses include inconsistencies in notation and terminology throughout the paper, which could hinder clarity and comprehension. Additionally, some performance metrics lack specificity, making it difficult to fully gauge the methods' effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is diminished by inconsistent use of notation and terminology, such as the varying formats of method names and numerical representations. While the underlying concepts are novel and have potential significance, the execution suffers from typographical errors and lack of standardization in references and equations, which could complicate reproducibility. The quality of experimental results is commendable, but improved clarity would enhance the overall impact.\n\n# Summary Of The Review\nOverall, the paper contributes valuable methodologies for quantizing floating-point representations, but suffers from notable clarity and consistency issues that could impede reader understanding and reproducibility. While the novel approaches are promising, the paper requires careful editing to meet the standards expected at ICLR.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the optimization of floating-point containers to enhance the training efficiency of neural networks by reducing memory footprint. It introduces a method termed \"Schrödinger’s FP,\" which allows for dynamic bit-length adjustments during model training. The findings suggest that these optimizations can lead to significant improvements in memory usage while maintaining model accuracy, primarily evaluated on architectures such as ResNet, MobileNet, and BERT.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to memory optimization and its promise for efficiency in training neural networks. However, it falls short in several areas. Notably, the paper does not address how these techniques may apply to inference scenarios, which is crucial since inference often involves real-time constraints. Additionally, there is a lack of analysis regarding the performance variation across different hardware architectures, which could provide deeper insights. The absence of concrete examples or case studies to illustrate practical integration into existing systems diminishes the practical applicability of the proposed methods. Furthermore, the evaluation is limited to a few neural network architectures, and it does not consider the implications of these techniques in edge computing or resource-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of its methodology and findings. However, the clarity could be further enhanced by addressing potential trade-offs between memory efficiency and computational speed, as well as exploring the ethical implications of using such compression techniques in sensitive applications. The paper lacks a detailed roadmap for future research directions, which would aid in reproducibility and encourage further exploration of the proposed methods.\n\n# Summary Of The Review\nThe paper presents an interesting approach to optimizing memory usage in neural networks through dynamic bit-length adjustments, showing promise in improving training efficiency. However, it has notable limitations in exploring inference applications, the impact of hardware variations, and broader implications, which detracts from its overall contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the impact of dynamic floating-point containers on memory usage and performance in neural network training, introducing two primary methodologies: Quantum Mantissa and BitChop. The Quantum Mantissa approach modifies the loss function to include a regularization term that adapts the bitlength of mantissas during training, while BitChop employs a moving average heuristic to dynamically adjust mantissa bitlengths. Experimental results demonstrate significant reductions in memory footprint, with Quantum Mantissa achieving up to a 93% reduction compared to FP32, and BitChop reducing average mantissa bitlengths to 4-5 bits, all while maintaining competitive accuracy levels across various models.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to optimizing memory usage in neural network training via two distinct methodologies, which are thoroughly evaluated across multiple models and datasets. The consistent reporting of statistical metrics and the thoughtful design of experiments enhance the robustness of the findings. However, a potential weakness is the variability in validation accuracy observed with BitChop, which may limit the generalizability of its results. Additionally, while the paper provides a solid statistical framework, further exploration into the long-term implications of these methods on convergence and training stability would strengthen the analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodologies and findings. The quality of the experimental design is high, with rigorous statistical analysis supporting the results. The novelty of the proposed methods is significant, as they address a pertinent issue in neural network training. Reproducibility is encouraged through detailed descriptions of the experimental setup and hyperparameters, although the paper could benefit from including more openly accessible code or data to facilitate independent verification of its results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling exploration of dynamic floating-point representations for neural network training, showcasing innovative methodologies that significantly reduce memory usage while maintaining accuracy. The contributions are supported by thorough experimental evaluations, though some variability in results suggests areas for further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents two novel methods, Quantum Mantissa and BitChop, aimed at enhancing the efficiency of neural network training through dynamic floating-point techniques. The authors evaluate these methods on specific models, including ResNet18, ResNet50, MobileNet V3, BERT, GPT-2, and DLRM, demonstrating performance improvements and energy efficiency gains primarily on an RTX3090 hardware platform. However, the paper lacks a broader exploration of the methods' applicability across diverse architectures and varying hardware conditions.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its introduction of dynamic floating-point techniques, which address memory efficiency and accuracy in specific scenarios. However, it exhibits several weaknesses: it focuses on a limited range of models, lacks exploration of interactions with other optimization techniques, and does not assess the performance under critical training conditions. Additionally, there is insufficient analysis of trade-offs associated with dynamic bit-length adjustments and a lack of systematic exploration of hyperparameter tuning. Concerns regarding the scalability of these methods and their implications for reproducibility in high-precision applications are also notable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation but could benefit from a more in-depth exploration of the proposed methods and their limitations. The quality of the work is solid, yet it lacks thorough validation across various architectures and training conditions, which affects its reproducibility. The novelty is present in the proposed techniques; however, the absence of comprehensive evaluations and discussions on broader applicability diminishes its overall impact.\n\n# Summary Of The Review\nWhile the paper introduces promising methods for dynamic floating-point techniques in neural network training, its limited scope and lack of comprehensive analysis of potential trade-offs hinder its applicability and significance. The findings may be relevant under specific conditions, but broader validation and exploration of interactions with existing techniques are necessary for a more robust contribution.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Schrödinger’s FP: Training Neural Networks with Dynamic Floating-Point Containers\" proposes a software-hardware co-design approach aimed at optimizing neural network training through dynamic adjustment of floating-point containers. The authors introduce methodologies such as Gecko and Quantum Mantissa for lossless delta encoding and BitChop for adjusting mantissa bit lengths. They claim significant reductions in memory footprint without substantial accuracy loss, emphasizing the importance of dynamic adaptation in neural network training.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its attempt to address memory optimization during neural network training and the introduction of specific methods for adjusting floating-point representations. However, it suffers from a lack of novelty, as many concepts presented are standard practices within the field. The authors do not provide sufficient insight or breakthroughs that would elevate their findings beyond typical optimization strategies. Furthermore, the presentation is overly dramatic, making trivial contributions appear groundbreaking.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by verbose language and jargon, which detracts from the overall quality of communication. While the authors do release full code for reproducibility, which is commendable, the underlying concepts lack the technical and empirical novelty expected in cutting-edge research. The reliance on well-known optimization techniques and the failure to push boundaries reduces the paper's impact.\n\n# Summary Of The Review\nOverall, this paper presents an unremarkable contribution to the field of neural network optimization, primarily reiterating well-known techniques under a new guise. The authors fail to provide substantial insights or innovative methodologies, resulting in a work that does not significantly advance the state of the art.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Schrödinger's Floating Point (FP), a novel approach that emphasizes dynamic and adaptive methods for floating-point representation during neural network training. The primary contributions include two innovative techniques: Quantum Mantissa and BitChop, which aim to optimize memory usage and computational speed through the dynamic adjustment of mantissa and exponent sizes. Experimental results indicate substantial memory savings and improved energy efficiency, while also highlighting the potential for these methods to be integrated with existing frameworks for enhanced performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its timely focus on adaptive precision techniques that align well with current trends in memory-efficient deep learning. The proposed methods demonstrate impressive empirical results, showcasing significant reductions in memory footprint and energy consumption. However, the paper could benefit from a more rigorous quantification of the overhead associated with these methods, particularly in the context of larger models or complex architectures. Furthermore, the exploration of layer-specific requirements for varying bit lengths could be expanded to provide a deeper understanding of the statistical properties of activations and gradients across different architectures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections that guide the reader through the proposed methodologies and findings. The quality of the experiments presented is commendable; however, more detailed analyses of long-term impacts on model generalization and robustness would enhance the overall contribution. The novelty of the proposed techniques is significant, as they present a fresh perspective on adaptive floating-point representation. Reproducibility is somewhat addressed through the experimental setup, but additional details on implementation and evaluation metrics would further improve this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of neural network training through its innovative adaptive floating-point methods. While the methodologies show promise and the results are noteworthy, there remain opportunities for improvement in quantifying overhead and a deeper investigation into the statistical properties relevant to adaptive strategies. Collaborative exploration with existing frameworks could further enhance the impact of these findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces two novel methods—Schrödinger's Floating Point (SFP) with Quantum Mantissa (QM) and BitChop (BC)—aimed at enhancing the efficiency of floating-point computations in deep learning. The authors report significant improvements in memory footprint, with Quantum Mantissa achieving a 20.8% reduction and BitChop a 24.6% reduction compared to traditional FP32 formats. Furthermore, the proposed methods yield substantial performance enhancements, with training speed increases of 2.94× for SFP QM and 2.64× for SFP BC, alongside energy efficiency gains of 3.38× and 2.96×, respectively. The experiments demonstrate that these methods maintain competitive accuracy across various neural network architectures, thus presenting a comprehensive solution for optimizing resource utilization in deep learning tasks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear demonstration of significant improvements in both memory and computational efficiency, which are critical for modern deep learning applications. The empirical results are robust, showing consistent performance across different models while maintaining accuracy. Additionally, the comparative analysis with existing methods highlights the advantages of the proposed techniques. However, a potential weakness is the limited discussion on the theoretical underpinnings of the proposed methods, which could leave readers questioning the broader applicability of the techniques outside the tested scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The quality of the experiments is high, with thorough benchmarking against relevant metrics such as validation accuracy and memory footprint. The novelty of the methods is significant, as they introduce new approaches to floating-point computations that are particularly tailored for deep learning. The reproducibility of the results is supported by consistent performance across multiple runs, although additional details on implementation would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the optimization of floating-point computations for deep learning, showcasing significant improvements in memory efficiency and computational speed without sacrificing accuracy. The methods introduced are both novel and impactful, making a strong case for their adoption in practice.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing neural network training by utilizing a hybrid precision format that balances the computational efficiency of BFloat16 and the accuracy of FP32. The authors propose a framework, termed \"Gecko,\" that dynamically adjusts precision during training based on the sensitivity of different layers. Experimental results demonstrate that Gecko achieves significant speed-ups without compromising model accuracy on standard benchmarks, thereby advancing the state-of-the-art in model training efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to precision management, which is particularly relevant given the increasing complexity of deep learning models. The dynamic adjustment of precision offers a practical solution to the trade-off between computational speed and model accuracy. However, the paper suffers from clarity issues, particularly in its terminology and the presentation of experimental findings. The lack of adequate definitions for technical terms may alienate readers who are not deeply familiar with the subject matter.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hampered by its complex structure and dense exposition. The introduction is overwhelming and could benefit from subheadings to help guide readers. Similarly, the findings are not clearly delineated, which makes it difficult to grasp the significance of the results at a glance. While the novelty of the approach is commendable, the paper's reproducibility is supported by a detailed statement; however, this could be emphasized earlier in the text to reinforce the authors' commitment to transparency.\n\n# Summary Of The Review\nOverall, while the paper introduces a promising and novel framework for optimizing neural network training through hybrid precision, it requires significant improvements in clarity and organization to ensure its findings are accessible and impactful. Addressing the noted issues will enhance the paper's overall effectiveness and readability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.296242238874405,
    -2.04194451803051,
    -2.0194573907518922,
    -1.8000198067880153,
    -2.0542997912946763,
    -1.8730128493898837,
    -1.7060659385323862,
    -1.9431464362912068,
    -1.7192291330652736,
    -1.952285914625719,
    -1.8386628320804392,
    -1.812639348123351,
    -1.9285581903487528,
    -1.8297849080515636,
    -1.9318701674721486,
    -1.8644911754847855,
    -2.171945377808318,
    -1.8416398291197216,
    -1.9361929982572745,
    -1.8232764764682798,
    -2.0084801012698246,
    -2.0209536472786893,
    -1.8825620139348025,
    -1.7725293592312443,
    -1.9042904937468599,
    -2.1226714134196376,
    -1.9450375973350942,
    -1.7062181028486678,
    -1.810522992870242
  ],
  "logp_cond": [
    [
      0.0,
      -1.9524451546916597,
      -2.0225216867799984,
      -1.9598513809906701,
      -1.9988447789109358,
      -2.0775189557858016,
      -2.1122413667784974,
      -1.9866272512979928,
      -2.0128817641106984,
      -2.0251058395803194,
      -1.9743728887103196,
      -2.1116935536340837,
      -2.0134617580500818,
      -1.9576294475874676,
      -1.9755746631243813,
      -2.0151207600386805,
      -2.0014124453997018,
      -2.0259340595235003,
      -2.021264537660729,
      -1.96997023223136,
      -2.037291595322375,
      -1.9852311141424774,
      -2.070298478325258,
      -2.0253361428141834,
      -2.0085254031524253,
      -1.9434280630665852,
      -2.0039767512598994,
      -2.0388517735585983,
      -2.0741345510894877
    ],
    [
      -1.6161296884480887,
      0.0,
      -1.4032548761126469,
      -1.2717878882475884,
      -1.4179983500890172,
      -1.5216880258386596,
      -1.700154165386673,
      -1.4305961995847833,
      -1.33790990799002,
      -1.5307133267668547,
      -1.3690975605497833,
      -1.7340851569298548,
      -1.3103883960207443,
      -1.307836935660146,
      -1.4782825954030183,
      -1.359010997011008,
      -1.492126288315577,
      -1.3985266291286382,
      -1.4944114665105475,
      -1.3191100383735455,
      -1.4737850538650763,
      -1.5382886913488099,
      -1.5762115809901966,
      -1.4349355782352191,
      -1.5538025783237008,
      -1.3762023313205376,
      -1.5067788331293672,
      -1.5301607179052237,
      -1.6774128730333364
    ],
    [
      -1.6603016210706116,
      -1.3984150881604995,
      0.0,
      -1.4087785838441882,
      -1.4828092274247962,
      -1.5274002816248082,
      -1.6894108207464635,
      -1.4583944421202215,
      -1.428741752557434,
      -1.5455839291207405,
      -1.4776779709645962,
      -1.741967105865396,
      -1.3276563413001259,
      -1.4486470885198954,
      -1.5427520290008845,
      -1.4240330104304837,
      -1.5013459168403573,
      -1.4584319244512156,
      -1.571248320302476,
      -1.40751370844425,
      -1.5064397631310917,
      -1.6179621129046944,
      -1.6079039687921617,
      -1.5701804571387246,
      -1.599922279884747,
      -1.4740962833446538,
      -1.5580846600234715,
      -1.5315802304018677,
      -1.6907890940010182
    ],
    [
      -1.4240882037936715,
      -1.1996372756384548,
      -1.2057948135080294,
      0.0,
      -1.2914953067870665,
      -1.3525636688826226,
      -1.4985614932947204,
      -1.2674915039217796,
      -1.2737442081598658,
      -1.3860497790228226,
      -1.322988217943119,
      -1.549612437184111,
      -1.1558801018895246,
      -1.201459343137307,
      -1.3785342721909049,
      -1.2101812593517585,
      -1.3987685533360148,
      -1.3126654160004898,
      -1.3629560365118698,
      -1.1713924874050967,
      -1.3417672519760697,
      -1.398639626024696,
      -1.433224100640112,
      -1.3833151528159673,
      -1.434142431088904,
      -1.263349389394063,
      -1.3753716375994691,
      -1.3345450064479658,
      -1.5249276796131381
    ],
    [
      -1.6537979254712143,
      -1.5626956559679615,
      -1.550908561091128,
      -1.5708796433634173,
      0.0,
      -1.7258859338753991,
      -1.7748742329438214,
      -1.5789806872279526,
      -1.6445269444100439,
      -1.6372356754093311,
      -1.5920646352649985,
      -1.758775643927724,
      -1.6171536181435344,
      -1.5830216923873734,
      -1.6318376796418064,
      -1.608576295067335,
      -1.5291851074786174,
      -1.585828336153265,
      -1.6634106150782106,
      -1.5232749406920774,
      -1.64707741218875,
      -1.643641070549144,
      -1.7027747294259161,
      -1.6594111089611103,
      -1.6789207735997702,
      -1.5996855915697246,
      -1.6966265732150316,
      -1.6690035612429077,
      -1.7452430901920122
    ],
    [
      -1.5704056886369315,
      -1.4096293685855625,
      -1.3906860809217008,
      -1.3864394077785598,
      -1.486158887073473,
      0.0,
      -1.539368151504937,
      -1.3479256810236548,
      -1.3956323197996656,
      -1.507406704188689,
      -1.452343106768269,
      -1.5807339815879784,
      -1.342376471764617,
      -1.397277438522786,
      -1.4798622371570958,
      -1.3664321955945546,
      -1.5255383858682117,
      -1.4100505060808457,
      -1.462509699205037,
      -1.3865131623434486,
      -1.3812163334669814,
      -1.5044328566601426,
      -1.510089284012443,
      -1.4734050151888738,
      -1.5086614395186042,
      -1.4088908878707092,
      -1.4491962384767043,
      -1.4547311720301743,
      -1.5467635455425925
    ],
    [
      -1.3751319697646265,
      -1.3391079826890258,
      -1.294357247338588,
      -1.344904405466822,
      -1.281025100776948,
      -1.3133858094409494,
      0.0,
      -1.341184583103915,
      -1.3513111881840894,
      -1.3178010170345535,
      -1.3355565884489924,
      -1.3323845954356384,
      -1.324693525023987,
      -1.311806005405406,
      -1.329244846602338,
      -1.3374296039407334,
      -1.2665041850504923,
      -1.3273718207535141,
      -1.3634968117086754,
      -1.337688628306577,
      -1.3422333093698007,
      -1.321195381167319,
      -1.3337862458358667,
      -1.3613997450212139,
      -1.3242349166981378,
      -1.347885354932555,
      -1.34636680191004,
      -1.3517499084638187,
      -1.3540320470980638
    ],
    [
      -1.6366115195826991,
      -1.4610313579557745,
      -1.4659648546304511,
      -1.4151704372031564,
      -1.5379684643427791,
      -1.489310888856005,
      -1.6500758132490496,
      0.0,
      -1.4883435753026426,
      -1.5474648222231586,
      -1.508519483204854,
      -1.679078474649508,
      -1.3555431133028801,
      -1.4892769501661285,
      -1.5495517720768788,
      -1.4549722639143452,
      -1.510205065291407,
      -1.445901005443991,
      -1.5109745798464453,
      -1.444857930493434,
      -1.4595273525421604,
      -1.5676564937122814,
      -1.5634759618988123,
      -1.542821295155005,
      -1.5785296074884996,
      -1.4171856717477633,
      -1.5711413421112583,
      -1.550121588897812,
      -1.6201769466975233
    ],
    [
      -1.3471751569757606,
      -1.1455440456006563,
      -1.1372766383663502,
      -1.1723608113645105,
      -1.2805441522095398,
      -1.2795060142711336,
      -1.4616457525438544,
      -1.1906662921499251,
      0.0,
      -1.2644606553485824,
      -1.2468215706126706,
      -1.4521661979302796,
      -1.1542722502124976,
      -1.1944134571737628,
      -1.27929427028375,
      -1.1304054773719525,
      -1.2431860696476247,
      -1.259934487751063,
      -1.3204544067535826,
      -1.1732714453617785,
      -1.2217645049169055,
      -1.3310356713984386,
      -1.2853971033279656,
      -1.2080053049505806,
      -1.2471949241194662,
      -1.2032096828085315,
      -1.2823149332086974,
      -1.243380473050802,
      -1.3870310873947342
    ],
    [
      -1.5422379910493598,
      -1.4595515094784384,
      -1.4340806104438697,
      -1.4652029707822374,
      -1.4946668245896702,
      -1.6174248906179953,
      -1.6618772893782314,
      -1.5030079711926716,
      -1.4745129809372668,
      0.0,
      -1.4368113798345492,
      -1.6912104494940832,
      -1.486161107111516,
      -1.4470076281912245,
      -1.5161890275697905,
      -1.4677282753164849,
      -1.520363287638481,
      -1.566904319603605,
      -1.5502512188365634,
      -1.5235087116594606,
      -1.512330925789375,
      -1.4901638665123955,
      -1.5190415360034026,
      -1.492025777307589,
      -1.4676260617799448,
      -1.5115546361706242,
      -1.5141818363658899,
      -1.4982615431977442,
      -1.6098245777517626
    ],
    [
      -1.4462311222092168,
      -1.1168350641566849,
      -1.2965743006852082,
      -1.2407821656410818,
      -1.3330624764852046,
      -1.442904654003587,
      -1.514007475337703,
      -1.3083139661070669,
      -1.281523225673663,
      -1.3536692476018408,
      0.0,
      -1.5521601725143745,
      -1.2244506679001166,
      -1.1501326635510414,
      -1.2461686748001342,
      -1.288638600489707,
      -1.3186850216295287,
      -1.349206890492821,
      -1.4202130999468787,
      -1.2308538323430989,
      -1.33809214549395,
      -1.407008558535699,
      -1.4773511721904498,
      -1.3232416403579486,
      -1.404691938949649,
      -1.2464652993698946,
      -1.4177164967750435,
      -1.400272992015706,
      -1.5226619662442988
    ],
    [
      -1.509064313946759,
      -1.4876340613339798,
      -1.4853798816402977,
      -1.520310051503226,
      -1.397583576262255,
      -1.5084669037344773,
      -1.4448035468840315,
      -1.484607673527931,
      -1.5270653944886519,
      -1.4555048703794686,
      -1.465918722922815,
      0.0,
      -1.5188010043215718,
      -1.4746639994280744,
      -1.4352327037421286,
      -1.5135779287165314,
      -1.4479369277184118,
      -1.476061747284515,
      -1.515696819892619,
      -1.4997240785408137,
      -1.5181167269228109,
      -1.426519667532844,
      -1.4591443806663635,
      -1.5122412140743275,
      -1.5012675578332475,
      -1.5246758386160906,
      -1.4774505918391874,
      -1.5122111453533562,
      -1.4487419715720657
    ],
    [
      -1.5875048133931884,
      -1.3426449683233785,
      -1.3301715874572249,
      -1.3436829322856183,
      -1.504644978810248,
      -1.495844825009741,
      -1.6831168857441585,
      -1.3783835888088722,
      -1.45503118290082,
      -1.5367104912467175,
      -1.46351743773952,
      -1.7164878976691484,
      0.0,
      -1.4540350065980403,
      -1.5361418217071336,
      -1.39050009867521,
      -1.491362040203333,
      -1.4773529265640137,
      -1.5486780283873516,
      -1.372449819206718,
      -1.4261121098667986,
      -1.602885289813807,
      -1.5767249063898405,
      -1.4903383087254765,
      -1.5634770996090626,
      -1.4299666045118071,
      -1.5367108193576677,
      -1.5446271253706572,
      -1.6621072588680257
    ],
    [
      -1.4545908666976042,
      -1.1705731975847067,
      -1.3084198863788976,
      -1.231411818021974,
      -1.3126531086931754,
      -1.442962878264948,
      -1.5614473839985863,
      -1.3165425202462138,
      -1.317830629594268,
      -1.4125969461740633,
      -1.2313225732377848,
      -1.6112652339052862,
      -1.2100058332343824,
      0.0,
      -1.348476094993763,
      -1.3147868091757762,
      -1.3621759906664426,
      -1.3411502460709954,
      -1.4215730812724943,
      -1.2570382179536088,
      -1.33341906138536,
      -1.424757859055844,
      -1.4999032593150168,
      -1.3803026387806672,
      -1.4520621759456178,
      -1.3011982006474136,
      -1.4413730778456098,
      -1.4024009241360789,
      -1.5184446766715942
    ],
    [
      -1.5035386250105451,
      -1.354932312137317,
      -1.3814922424057068,
      -1.388675030580368,
      -1.4193498051809745,
      -1.505309089836275,
      -1.5987933948504847,
      -1.3554509782108504,
      -1.3971892586942471,
      -1.4370875370939995,
      -1.2807188154923892,
      -1.5605820066296585,
      -1.3607127758367419,
      -1.3103036486377961,
      0.0,
      -1.40067827771063,
      -1.4188323476862308,
      -1.4245818556035432,
      -1.4898956074127019,
      -1.3325073041787616,
      -1.4090772108408236,
      -1.4047438674491388,
      -1.5096346400817395,
      -1.488724476703683,
      -1.4351988002236116,
      -1.3664477438584857,
      -1.4409088231148446,
      -1.4957551499256285,
      -1.5284913747849884
    ],
    [
      -1.4854477328162006,
      -1.207251759652655,
      -1.2329127009752268,
      -1.1649253679036848,
      -1.4085830820855247,
      -1.3313863720971113,
      -1.526001577607978,
      -1.25790866150624,
      -1.2307260863390197,
      -1.3789095546446244,
      -1.344017404536532,
      -1.5757525541314823,
      -1.1578268052521745,
      -1.3202913958787121,
      -1.37958345244515,
      0.0,
      -1.3765578211533844,
      -1.4011963360302517,
      -1.427950872413459,
      -1.2594754499761776,
      -1.3481664567694691,
      -1.4396529373414189,
      -1.4422420194964751,
      -1.3843983398239017,
      -1.4065802561325265,
      -1.278614390678216,
      -1.3604844444609323,
      -1.389910332288901,
      -1.514724937238828
    ],
    [
      -1.7670896959871856,
      -1.6493759879119625,
      -1.6451755056494592,
      -1.7199979580259044,
      -1.6437098908042904,
      -1.8486582069871613,
      -1.8554720895828605,
      -1.7436678539429402,
      -1.6773569178964765,
      -1.7287826162426685,
      -1.660785396532683,
      -1.8838619274393764,
      -1.6386394207009871,
      -1.6626393293073258,
      -1.693200348586409,
      -1.7040908616909012,
      0.0,
      -1.750665407752746,
      -1.8279635723458354,
      -1.647480419907183,
      -1.7547314250689792,
      -1.8034515634654638,
      -1.7995566404821977,
      -1.7561656453239916,
      -1.7176530171737538,
      -1.6903399987454713,
      -1.7546179660096461,
      -1.7861521488173195,
      -1.8302634672035156
    ],
    [
      -1.4910359243101825,
      -1.3639520741162858,
      -1.2895714789075299,
      -1.3324777954817737,
      -1.3059426371881833,
      -1.3947664183138013,
      -1.5168705067188033,
      -1.2513047441415193,
      -1.3756996766879084,
      -1.4529307091462804,
      -1.4294716204738724,
      -1.5369984935524859,
      -1.29689009889112,
      -1.3686272692126493,
      -1.4431508348909146,
      -1.336874136262024,
      -1.3843326705415593,
      0.0,
      -1.3990977960870254,
      -1.2966219903293121,
      -1.3790041875975128,
      -1.479056102580287,
      -1.4610124216510505,
      -1.493072773356914,
      -1.4778138304350203,
      -1.3684976672282458,
      -1.4607140035954487,
      -1.3877650363538145,
      -1.546146110936766
    ],
    [
      -1.5159914181431677,
      -1.3286647131868283,
      -1.4176133911111986,
      -1.3274069172804792,
      -1.4360679442104736,
      -1.4615503554736955,
      -1.5895998320472213,
      -1.3351008638041755,
      -1.3908525152432942,
      -1.4421321265809244,
      -1.4212839257311212,
      -1.5872169683424255,
      -1.379916285252823,
      -1.404803597943341,
      -1.461618487767972,
      -1.3768980487092135,
      -1.5107218806999128,
      -1.3844974342105967,
      0.0,
      -1.3425609471684714,
      -1.4216764350205897,
      -1.448378109613468,
      -1.4947573035898427,
      -1.4358967428307468,
      -1.5053908092945414,
      -1.3167556144694428,
      -1.4161241090098367,
      -1.4723396438381644,
      -1.5659965629418806
    ],
    [
      -1.42915035816308,
      -1.210839730775891,
      -1.2791609491399671,
      -1.1947162842922874,
      -1.3738868131301565,
      -1.3813503570353245,
      -1.512536302945832,
      -1.279838338441056,
      -1.3205129638778488,
      -1.4090689967531425,
      -1.3185613123111053,
      -1.553856465924863,
      -1.2315643372517202,
      -1.2699192436540578,
      -1.343574759295521,
      -1.2989805879106968,
      -1.3235528424469778,
      -1.3360928318679797,
      -1.4088647454533825,
      0.0,
      -1.3038182301308925,
      -1.3791759240389845,
      -1.4488087396377252,
      -1.3700715094627707,
      -1.4196417071701364,
      -1.2054775252449572,
      -1.3810048831677142,
      -1.4109273667816105,
      -1.4568634673225946
    ],
    [
      -1.6418978962169712,
      -1.3939771419317089,
      -1.4181149555583126,
      -1.4369377083954415,
      -1.5189196801363283,
      -1.5442615212098534,
      -1.6553728890494999,
      -1.4302629655991865,
      -1.416768546662283,
      -1.5130033580828306,
      -1.4642595253001702,
      -1.6879109568362274,
      -1.3786082611911394,
      -1.433347496798636,
      -1.499457452619527,
      -1.430588309549138,
      -1.5250843592512804,
      -1.4904821801733958,
      -1.5640680838332544,
      -1.4200416225460302,
      0.0,
      -1.5497211246329923,
      -1.5466969149329068,
      -1.4905202237802193,
      -1.5364531916049715,
      -1.4437092468688708,
      -1.5092052074760793,
      -1.4612731442838425,
      -1.652645254819024
    ],
    [
      -1.5204209910805804,
      -1.452430899375307,
      -1.5524096102031528,
      -1.463681816122382,
      -1.4758333872628857,
      -1.5936646741059433,
      -1.6479613404150635,
      -1.4495793924405336,
      -1.5210180778352973,
      -1.4927908573456035,
      -1.5190863285859375,
      -1.6503548225806715,
      -1.5368599309185094,
      -1.447601744476858,
      -1.4940290353536394,
      -1.5141442714238253,
      -1.5455674406633424,
      -1.556606006151803,
      -1.525323177050665,
      -1.4393590512714765,
      -1.5124772067785601,
      0.0,
      -1.6059422879205625,
      -1.5275712952023461,
      -1.525647727578274,
      -1.4123610519985215,
      -1.5128338516962703,
      -1.533072087593637,
      -1.5796882100900147
    ],
    [
      -1.554308189435551,
      -1.4490233298250899,
      -1.4616989839754162,
      -1.464572331090681,
      -1.5044734932990962,
      -1.4925888352759693,
      -1.6136971144451893,
      -1.4690617659696001,
      -1.4733004071079172,
      -1.4715012914177032,
      -1.4830353022222407,
      -1.62350058369968,
      -1.4651141248320958,
      -1.4974848512344412,
      -1.5313625513996225,
      -1.4555858478461878,
      -1.5203170366772336,
      -1.5120047262174154,
      -1.550842709120897,
      -1.4747059369674584,
      -1.4809529889547046,
      -1.56830972075797,
      0.0,
      -1.506664367109839,
      -1.4828713519274876,
      -1.4594154349942678,
      -1.5176469666630614,
      -1.549302330865019,
      -1.5904136343614832
    ],
    [
      -1.4435680616165845,
      -1.2690222706087255,
      -1.2940497091725558,
      -1.2908624414563914,
      -1.3721536576072717,
      -1.3610275520692627,
      -1.5082738658633366,
      -1.3264552632770878,
      -1.2837893479087965,
      -1.3508617181206044,
      -1.293669305016394,
      -1.5195296675960654,
      -1.3044581427870918,
      -1.3176661892572339,
      -1.3367637146414442,
      -1.2744303085181625,
      -1.3557817758452606,
      -1.4128056587814082,
      -1.416404174497091,
      -1.2866376313425338,
      -1.3589708724774259,
      -1.3853812048273357,
      -1.3952244602584267,
      0.0,
      -1.3598348183095026,
      -1.3438537940413968,
      -1.351670063485649,
      -1.3646328988115102,
      -1.4614377311171758
    ],
    [
      -1.5118747214482466,
      -1.429392928583906,
      -1.4450214560581511,
      -1.4659383794053094,
      -1.4966909065829448,
      -1.5537082900395254,
      -1.609176856725403,
      -1.4436955855712947,
      -1.4353340781691737,
      -1.398274475384596,
      -1.4341328278392562,
      -1.6294856381460003,
      -1.4474225456318366,
      -1.4311599520761598,
      -1.4494085676160515,
      -1.4551522212420442,
      -1.455909795407518,
      -1.551756719431908,
      -1.5452376749863304,
      -1.4343717379175966,
      -1.5038716479266734,
      -1.4895519872901537,
      -1.5143345647517559,
      -1.4621517856990305,
      0.0,
      -1.4636783561693822,
      -1.433928369766789,
      -1.455402605589114,
      -1.6090154205033427
    ],
    [
      -1.5769084380196412,
      -1.3718926921631824,
      -1.4644065352991855,
      -1.3591410303894729,
      -1.5566159228231442,
      -1.5416965370332347,
      -1.7672247870322368,
      -1.4224490551695264,
      -1.4968514037203406,
      -1.5641946601294492,
      -1.5126126105881441,
      -1.788907334066154,
      -1.3840147912732617,
      -1.4196351032567356,
      -1.5020021907621426,
      -1.424384097611095,
      -1.5618542433432145,
      -1.5650703178502259,
      -1.5500554741670425,
      -1.356573705733233,
      -1.4641360669039511,
      -1.5395389661848013,
      -1.583731975140083,
      -1.5250085250057899,
      -1.612428116399229,
      0.0,
      -1.5715019332731626,
      -1.608758774125162,
      -1.67972743604805
    ],
    [
      -1.601915295449174,
      -1.477734513352913,
      -1.4693794680913757,
      -1.4648494899115443,
      -1.5410981854916044,
      -1.5121030393859756,
      -1.6507268662663377,
      -1.4860706624296403,
      -1.4856955262119074,
      -1.5487002327880053,
      -1.5231206220242866,
      -1.658432460635257,
      -1.4599190862719515,
      -1.5143911447374205,
      -1.5308689237184763,
      -1.4591016651640596,
      -1.5333206092706373,
      -1.5495458835210913,
      -1.5357049771899165,
      -1.4557171735712497,
      -1.4863756967514303,
      -1.548281578396092,
      -1.541423422105767,
      -1.5397718597807943,
      -1.4869418759483395,
      -1.4812975257495782,
      0.0,
      -1.498397286597209,
      -1.6082260288724488
    ],
    [
      -1.3928518642968792,
      -1.2775584979112906,
      -1.2645791946903555,
      -1.2812883833948177,
      -1.3003355449690994,
      -1.318713516636049,
      -1.4701851092753697,
      -1.3064738738212223,
      -1.2426650269202362,
      -1.3331674175305575,
      -1.2945012239187785,
      -1.4415354844387953,
      -1.3018593576220234,
      -1.28452833148485,
      -1.325268684905717,
      -1.2921874287582706,
      -1.3655170862274542,
      -1.3042936319290779,
      -1.3437961020295746,
      -1.2635852563303769,
      -1.2898088825053198,
      -1.3171485106945864,
      -1.371763002565847,
      -1.284690679803321,
      -1.29374503653366,
      -1.3126025947105497,
      -1.262768881999628,
      0.0,
      -1.3950214099125506
    ],
    [
      -1.4920604007658016,
      -1.4262229680880325,
      -1.4308308295277727,
      -1.4550854552456625,
      -1.4395937794525222,
      -1.4528295951567152,
      -1.5242769519064892,
      -1.461225772611096,
      -1.4547464989685988,
      -1.4521968105033942,
      -1.4419352188378767,
      -1.483459186904436,
      -1.4235949118472353,
      -1.3923700503073282,
      -1.373146103182488,
      -1.4483371504715241,
      -1.4056381612463813,
      -1.4475708968803918,
      -1.4574012199189914,
      -1.3458722192592845,
      -1.464012895959234,
      -1.3841615209375464,
      -1.4468667703184348,
      -1.4774590994601313,
      -1.449107816504822,
      -1.437275050125969,
      -1.4388301201150204,
      -1.4684700779558355,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.34379708418274535,
      0.2737205520944066,
      0.3363908578837349,
      0.2973974599634692,
      0.21872328308860345,
      0.18400087209590765,
      0.30961498757641226,
      0.2833604747637066,
      0.2711363992940856,
      0.32186935016408547,
      0.1845486852403213,
      0.2827804808243233,
      0.33861279128693744,
      0.3206675757500237,
      0.2811214788357246,
      0.2948297934747033,
      0.2703081793509048,
      0.2749777012136758,
      0.3262720066430451,
      0.2589506435520299,
      0.3110111247319276,
      0.22594376054914722,
      0.27090609606022165,
      0.2877168357219797,
      0.3528141758078198,
      0.29226548761450566,
      0.2573904653158068,
      0.2221076877849173
    ],
    [
      0.42581482958242134,
      0.0,
      0.6386896419178631,
      0.7701566297829217,
      0.6239461679414928,
      0.5202564921918504,
      0.34179035264383706,
      0.6113483184457267,
      0.70403461004049,
      0.5112311912636553,
      0.6728469574807268,
      0.30785936110065526,
      0.7315561220097657,
      0.734107582370364,
      0.5636619226274917,
      0.682933521019502,
      0.549818229714933,
      0.6434178889018718,
      0.5475330515199626,
      0.7228344796569646,
      0.5681594641654337,
      0.5036558266817002,
      0.4657329370403134,
      0.6070089397952909,
      0.4881419397068092,
      0.6657421867099724,
      0.5351656849011428,
      0.5117838001252863,
      0.3645316449971736
    ],
    [
      0.3591557696812806,
      0.6210423025913927,
      0.0,
      0.610678806907704,
      0.5366481633270961,
      0.492057109127084,
      0.3300465700054287,
      0.5610629486316707,
      0.5907156381944583,
      0.47387346163115174,
      0.541779419787296,
      0.2774902848864962,
      0.6918010494517663,
      0.5708103022319968,
      0.47670536175100775,
      0.5954243803214085,
      0.5181114739115349,
      0.5610254663006766,
      0.44820907044941616,
      0.6119436823076423,
      0.5130176276208005,
      0.40149527784719785,
      0.41155342195973055,
      0.4492769336131677,
      0.41953511086714523,
      0.5453611074072384,
      0.4613727307284208,
      0.4878771603500245,
      0.32866829675087406
    ],
    [
      0.3759316029943438,
      0.6003825311495605,
      0.5942249932799859,
      0.0,
      0.5085245000009488,
      0.44745613790539274,
      0.3014583134932949,
      0.5325283028662358,
      0.5262755986281495,
      0.41397002776519276,
      0.4770315888448964,
      0.2504073696039044,
      0.6441397048984907,
      0.5985604636507083,
      0.42148553459711047,
      0.5898385474362569,
      0.4012512534520005,
      0.4873543907875255,
      0.4370637702761455,
      0.6286273193829186,
      0.4582525548119456,
      0.40138018076331927,
      0.36679570614790324,
      0.41670465397204803,
      0.3658773756991114,
      0.5366704173939523,
      0.4246481691885462,
      0.4654748003400495,
      0.2750921271748772
    ],
    [
      0.40050186582346203,
      0.4916041353267149,
      0.5033912302035484,
      0.483420147931259,
      0.0,
      0.3284138574192772,
      0.2794255583508549,
      0.47531910406672373,
      0.40977284688463245,
      0.4170641158853452,
      0.4622351560296778,
      0.2955241473669523,
      0.4371461731511419,
      0.4712780989073029,
      0.42246211165286995,
      0.4457234962273413,
      0.525114683816059,
      0.4684714551414113,
      0.3908891762164657,
      0.531024850602599,
      0.4072223791059264,
      0.41065872074553234,
      0.3515250618687602,
      0.394888682333566,
      0.3753790176949061,
      0.45461419972495176,
      0.3576732180796447,
      0.3852962300517686,
      0.30905670110266414
    ],
    [
      0.30260716075295213,
      0.4633834808043211,
      0.4823267684681829,
      0.4865734416113239,
      0.38685396231641067,
      0.0,
      0.33364469788494655,
      0.5250871683662288,
      0.47738052959021804,
      0.3656061452011947,
      0.4206697426216146,
      0.29227886780190526,
      0.5306363776252667,
      0.4757354108670977,
      0.3931506122327879,
      0.506580653795329,
      0.347474463521672,
      0.46296234330903796,
      0.4105031501848466,
      0.4864996870464351,
      0.4917965159229023,
      0.36857999272974107,
      0.36292356537744075,
      0.39960783420100987,
      0.36435140987127945,
      0.4641219615191745,
      0.42381661091317935,
      0.41828167735970934,
      0.3262493038472911
    ],
    [
      0.3309339687677597,
      0.36695795584336044,
      0.41170869119379816,
      0.3611615330655642,
      0.4250408377554382,
      0.3926801290914368,
      0.0,
      0.3648813554284711,
      0.35475475034829684,
      0.38826492149783265,
      0.3705093500833938,
      0.37368134309674783,
      0.3813724135083991,
      0.39425993312698027,
      0.3768210919300481,
      0.3686363345916528,
      0.43956175348189386,
      0.3786941177788721,
      0.34256912682371077,
      0.3683773102258092,
      0.3638326291625855,
      0.38487055736506726,
      0.3722796926965195,
      0.34466619351117234,
      0.38183102183424844,
      0.3581805835998313,
      0.3596991366223461,
      0.3543160300685675,
      0.3520338914343224
    ],
    [
      0.30653491670850763,
      0.4821150783354322,
      0.47718158166075564,
      0.5279759990880504,
      0.40517797194842764,
      0.4538355474352018,
      0.29307062304215714,
      0.0,
      0.45480286098856415,
      0.39568161406804814,
      0.4346269530863527,
      0.26406796164169877,
      0.5876033229883266,
      0.45386948612507827,
      0.393594664214328,
      0.48817417237686156,
      0.4329413709997998,
      0.49724543084721584,
      0.4321718564447614,
      0.49828850579777284,
      0.4836190837490464,
      0.3754899425789253,
      0.37967047439239443,
      0.40032514113620166,
      0.3646168288027072,
      0.5259607645434434,
      0.3720050941799484,
      0.3930248473933948,
      0.32296948959368343
    ],
    [
      0.37205397608951296,
      0.5736850874646173,
      0.5819524946989234,
      0.546868321700763,
      0.4386849808557338,
      0.43972311879413994,
      0.25758338052141916,
      0.5285628409153484,
      0.0,
      0.45476847771669116,
      0.47240756245260296,
      0.267062935134994,
      0.564956882852776,
      0.5248156758915108,
      0.4399348627815236,
      0.5888236556933211,
      0.4760430634176489,
      0.4592946453142106,
      0.398774726311691,
      0.5459576877034951,
      0.4974646281483681,
      0.388193461666835,
      0.43383202973730794,
      0.511223828114693,
      0.4720342089458074,
      0.5160194502567421,
      0.4369141998565762,
      0.47584866001447157,
      0.33219804567053934
    ],
    [
      0.4100479235763592,
      0.4927344051472806,
      0.5182053041818493,
      0.4870829438434816,
      0.45761909003604884,
      0.33486102400772366,
      0.29040862524748756,
      0.4492779434330474,
      0.4777729336884522,
      0.0,
      0.5154745347911698,
      0.2610754651316358,
      0.46612480751420304,
      0.5052782864344945,
      0.43609688705592853,
      0.4845576393092341,
      0.43192262698723805,
      0.385381595022114,
      0.4020346957891556,
      0.42877720296625843,
      0.4399549888363441,
      0.46212204811332347,
      0.43324437862231635,
      0.4602601373181301,
      0.4846598528457742,
      0.44073127845509474,
      0.4381040782598291,
      0.4540243714279748,
      0.3424613368739564
    ],
    [
      0.39243170987122244,
      0.7218277679237544,
      0.542088531395231,
      0.5978806664393574,
      0.5056003555952346,
      0.3957581780768522,
      0.32465535674273616,
      0.5303488659733724,
      0.5571396064067762,
      0.4849935844785984,
      0.0,
      0.2865026595660647,
      0.6142121641803227,
      0.6885301685293979,
      0.5924941572803051,
      0.5500242315907322,
      0.5199778104509105,
      0.4894559415876183,
      0.4184497321335605,
      0.6078089997373404,
      0.5005706865864892,
      0.4316542735447402,
      0.3613116598899895,
      0.5154211917224907,
      0.4339708931307902,
      0.5921975327105446,
      0.4209463353053957,
      0.43838984006473325,
      0.3160008658361404
    ],
    [
      0.30357503417659193,
      0.32500528678937113,
      0.32725946648305326,
      0.29232929662012497,
      0.4150557718610959,
      0.3041724443888736,
      0.36783580123931947,
      0.32803167459541993,
      0.28557395363469906,
      0.3571344777438823,
      0.3467206252005359,
      0.0,
      0.2938383438017791,
      0.33797534869527657,
      0.3774066443812223,
      0.2990614194068195,
      0.36470242040493916,
      0.3365776008388359,
      0.29694252823073186,
      0.31291526958253724,
      0.29452262120054007,
      0.38611968059050694,
      0.3534949674569874,
      0.3003981340490234,
      0.3113717902901034,
      0.28796350950726035,
      0.33518875628416356,
      0.3004282027699947,
      0.3638973765512852
    ],
    [
      0.34105337695556437,
      0.5859132220253742,
      0.5983866028915279,
      0.5848752580631345,
      0.4239132115385047,
      0.43271336533901184,
      0.24544130460459423,
      0.5501746015398805,
      0.47352700744793275,
      0.3918476991020352,
      0.46504075260923283,
      0.2120702926796043,
      0.0,
      0.47452318375071245,
      0.3924163686416191,
      0.5380580916735427,
      0.4371961501454198,
      0.451205263784739,
      0.3798801619614012,
      0.5561083711420347,
      0.5024460804819542,
      0.3256729005349457,
      0.3518332839589122,
      0.4382198816232763,
      0.3650810907396902,
      0.4985915858369456,
      0.391847370991085,
      0.3839310649780956,
      0.26645093148072707
    ],
    [
      0.3751940413539594,
      0.6592117104668569,
      0.521365021672666,
      0.5983730900295896,
      0.5171317993583882,
      0.3868220297866156,
      0.2683375240529773,
      0.5132423878053498,
      0.5119542784572955,
      0.4171879618775003,
      0.5984623348137788,
      0.21851967414627738,
      0.6197790748171812,
      0.0,
      0.4813088130578005,
      0.5149980988757874,
      0.467608917385121,
      0.48863466198056815,
      0.4082118267790693,
      0.5727466900979548,
      0.49636584666620354,
      0.4050270489957195,
      0.3298816487365468,
      0.4494822692708964,
      0.37772273210594576,
      0.5285867074041499,
      0.3884118302059538,
      0.4273839839154847,
      0.31134023137996936
    ],
    [
      0.42833154246160343,
      0.5769378553348317,
      0.5503779250664418,
      0.5431951368917807,
      0.512520362291174,
      0.4265610776358735,
      0.3330767726216639,
      0.5764191892612982,
      0.5346809087779014,
      0.49478263037814907,
      0.6511513519797594,
      0.37128816084249006,
      0.5711573916354067,
      0.6215665188343524,
      0.0,
      0.5311918897615187,
      0.5130378197859178,
      0.5072883118686053,
      0.4419745600594467,
      0.599362863293387,
      0.522792956631325,
      0.5271263000230098,
      0.42223552739040904,
      0.44314569076846566,
      0.496671367248537,
      0.5654224236136629,
      0.490961344357304,
      0.43611501754652005,
      0.40337879268716015
    ],
    [
      0.3790434426685849,
      0.6572394158321304,
      0.6315784745095587,
      0.6995658075811007,
      0.45590809339926075,
      0.5331048033876742,
      0.33848959787680744,
      0.6065825139785455,
      0.6337650891457658,
      0.4855816208401611,
      0.5204737709482534,
      0.2887386213533032,
      0.706664370232611,
      0.5441997796060734,
      0.4849077230396355,
      0.0,
      0.48793335433140106,
      0.46329483945453376,
      0.4365403030713264,
      0.6050157255086079,
      0.5163247187153164,
      0.42483823814336663,
      0.42224915598831037,
      0.4800928356608838,
      0.457910919352259,
      0.5858767848065696,
      0.5040067310238532,
      0.47458084319588445,
      0.34976623824595743
    ],
    [
      0.4048556818211324,
      0.5225693898963555,
      0.5267698721588587,
      0.4519474197824136,
      0.5282354870040276,
      0.32328717082115666,
      0.3164732882254575,
      0.42827752386537776,
      0.4945884599118415,
      0.4431627615656495,
      0.5111599812756349,
      0.28808345036894156,
      0.5333059571073309,
      0.5093060485009921,
      0.47874502922190887,
      0.4678545161174168,
      0.0,
      0.42127997005557205,
      0.3439818054624826,
      0.5244649579011349,
      0.4172139527393388,
      0.3684938143428542,
      0.3723887373261203,
      0.4157797324843264,
      0.45429236063456413,
      0.48160537906284673,
      0.41732741179867183,
      0.38579322899099844,
      0.34168191060480235
    ],
    [
      0.35060390480953907,
      0.47768775500343574,
      0.5520683502121917,
      0.5091620336379479,
      0.5356971919315383,
      0.44687341080592025,
      0.32476932240091827,
      0.5903350849782023,
      0.4659401524318132,
      0.38870911997344115,
      0.41216820864584913,
      0.3046413355672357,
      0.5447497302286015,
      0.47301255990707225,
      0.398488994228807,
      0.5047656928576976,
      0.4573071585781623,
      0.0,
      0.4425420330326961,
      0.5450178387904094,
      0.4626356415222088,
      0.3625837265394345,
      0.3806274074686711,
      0.34856705576280755,
      0.3638259986847012,
      0.47314216189147573,
      0.3809258255242729,
      0.45387479276590703,
      0.29549371818295556
    ],
    [
      0.4202015801141068,
      0.6075282850704462,
      0.518579607146076,
      0.6087860809767953,
      0.500125054046801,
      0.47464264278357904,
      0.3465931662100532,
      0.6010921344530991,
      0.5453404830139803,
      0.49406087167635016,
      0.5149090725261534,
      0.348976029914849,
      0.5562767130044515,
      0.5313894003139334,
      0.47457451048930244,
      0.559294949548061,
      0.4254711175573618,
      0.5516955640466779,
      0.0,
      0.5936320510888031,
      0.5145165632366848,
      0.4878148886438065,
      0.4414356946674318,
      0.5002962554265278,
      0.43080218896273315,
      0.6194373837878318,
      0.5200688892474379,
      0.46385335441911013,
      0.3701964353153939
    ],
    [
      0.39412611830519984,
      0.6124367456923889,
      0.5441155273283127,
      0.6285601921759925,
      0.44938966333812336,
      0.44192611943295534,
      0.31074017352244776,
      0.5434381380272237,
      0.5027635125904311,
      0.4142074797151374,
      0.5047151641571745,
      0.2694200105434168,
      0.5917121392165596,
      0.553357232814222,
      0.4797017171727589,
      0.524295888557583,
      0.49972363402130204,
      0.48718364460030017,
      0.41441173101489737,
      0.0,
      0.5194582463373874,
      0.44410055242929536,
      0.3744677368305547,
      0.45320496700550916,
      0.40363476929814346,
      0.6177989512233226,
      0.44227159330056565,
      0.4123491096866694,
      0.3664130091456852
    ],
    [
      0.3665822050528533,
      0.6145029593381157,
      0.590365145711512,
      0.571542392874383,
      0.4895604211334963,
      0.4642185800599712,
      0.3531072122203247,
      0.578217135670638,
      0.5917115546075415,
      0.495476743186994,
      0.5442205759696543,
      0.3205691444335972,
      0.6298718400786851,
      0.5751326044711886,
      0.5090226486502976,
      0.5778917917206865,
      0.4833957420185442,
      0.5179979210964287,
      0.4444120174365702,
      0.5884384787237944,
      0.0,
      0.4587589766368323,
      0.46178318633691773,
      0.5179598774896053,
      0.47202690966485306,
      0.5647708544009538,
      0.49927489379374523,
      0.5472069569859821,
      0.35583484645080055
    ],
    [
      0.5005326561981089,
      0.5685227479033823,
      0.4685440370755365,
      0.5572718311563074,
      0.5451202600158036,
      0.42728897317274606,
      0.37299230686362583,
      0.5713742548381557,
      0.499935569443392,
      0.5281627899330859,
      0.5018673186927518,
      0.37059882469801786,
      0.48409371636017995,
      0.5733519028018312,
      0.5269246119250499,
      0.506809375854864,
      0.4753862066153469,
      0.4643476411268863,
      0.4956304702280243,
      0.5815945960072129,
      0.5084764405001292,
      0.0,
      0.41501135935812683,
      0.4933823520763432,
      0.49530591970041526,
      0.6085925952801678,
      0.508119795582419,
      0.4878815596850523,
      0.4412654371886746
    ],
    [
      0.32825382449925145,
      0.43353868410971264,
      0.4208630299593863,
      0.4179896828441214,
      0.3780885206357063,
      0.3899731786588332,
      0.26886489948961323,
      0.41350024796520235,
      0.4092616068268853,
      0.41106072251709924,
      0.3995267117125618,
      0.2590614302351224,
      0.4174478891027067,
      0.3850771627003613,
      0.35119946253518,
      0.42697616608861466,
      0.3622449772575689,
      0.37055728771738705,
      0.33171930481390555,
      0.40785607696734405,
      0.4016090249800979,
      0.31425229317683256,
      0.0,
      0.3758976468249635,
      0.39969066200731485,
      0.42314657894053465,
      0.36491504727174107,
      0.3332596830697836,
      0.2921483795733193
    ],
    [
      0.32896129761465986,
      0.5035070886225188,
      0.47847965005868853,
      0.48166691777485293,
      0.4003757016239726,
      0.4115018071619816,
      0.26425549336790777,
      0.4460740959541565,
      0.4887400113224478,
      0.4216676411106399,
      0.47886005421485023,
      0.2529996916351789,
      0.4680712164441525,
      0.45486316997401044,
      0.43576564458980016,
      0.4980990507130818,
      0.4167475833859837,
      0.35972370044983615,
      0.35612518473415333,
      0.4858917278887105,
      0.41355848675381846,
      0.3871481544039086,
      0.3773048989728176,
      0.0,
      0.41269454092174174,
      0.4286755651898475,
      0.4208592957455952,
      0.40789646041973415,
      0.31109162811406854
    ],
    [
      0.39241577229861324,
      0.4748975651629539,
      0.45926903768870875,
      0.4383521143415505,
      0.4075995871639151,
      0.3505822037073345,
      0.2951136370214569,
      0.46059490817556514,
      0.4689564155776862,
      0.506016018362264,
      0.47015766590760366,
      0.27480485560085954,
      0.4568679481150233,
      0.4731305416707001,
      0.45488192613080836,
      0.44913827250481564,
      0.4483806983393419,
      0.3525337743149519,
      0.3590528187605295,
      0.46991875582926323,
      0.4004188458201865,
      0.4147385064567062,
      0.389955928995104,
      0.4421387080478294,
      0.0,
      0.44061213757747764,
      0.4703621239800708,
      0.44888788815774583,
      0.2952750732435172
    ],
    [
      0.5457629753999964,
      0.7507787212564552,
      0.6582648781204521,
      0.7635303830301647,
      0.5660554905964934,
      0.5809748763864029,
      0.35544662638740077,
      0.7002223582501113,
      0.625820009699297,
      0.5584767532901884,
      0.6100588028314935,
      0.3337640793534835,
      0.738656622146376,
      0.703036310162902,
      0.620669222657495,
      0.6982873158085425,
      0.5608171700764231,
      0.5576010955694117,
      0.5726159392525951,
      0.7660977076864046,
      0.6585353465156865,
      0.5831324472348363,
      0.5389394382795547,
      0.5976628884138477,
      0.5102432970204087,
      0.0,
      0.551169480146475,
      0.5139126392944755,
      0.4429439773715875
    ],
    [
      0.3431223018859202,
      0.46730308398218123,
      0.47565812924371853,
      0.4801881074235499,
      0.4039394118434898,
      0.4329345579491186,
      0.2943107310687565,
      0.4589669349054539,
      0.4593420711231868,
      0.3963373645470889,
      0.42191697531080763,
      0.2866051366998372,
      0.48511851106314263,
      0.43064645259767365,
      0.4141686736166179,
      0.4859359321710346,
      0.4117169880644569,
      0.39549171381400283,
      0.4093326201451777,
      0.48932042376384444,
      0.45866190058366385,
      0.39675601893900225,
      0.4036141752293272,
      0.40526573755429984,
      0.4580957213867547,
      0.463740071585516,
      0.0,
      0.4466403107378851,
      0.33681156846264537
    ],
    [
      0.3133662385517886,
      0.4286596049373772,
      0.4416389081583123,
      0.42492971945385016,
      0.4058825578795684,
      0.38750458621261874,
      0.2360329935732981,
      0.39974422902744555,
      0.46355307592843165,
      0.37305068531811036,
      0.4117168789298893,
      0.2646826184098725,
      0.4043587452266444,
      0.4216897713638179,
      0.3809494179429509,
      0.4140306740903972,
      0.34070101662121366,
      0.40192447091958994,
      0.36242200081909326,
      0.44263284651829093,
      0.41640922034334804,
      0.38906959215408143,
      0.3344551002828209,
      0.42152742304534674,
      0.4124730663150078,
      0.3936155081381181,
      0.4434492208490399,
      0.0,
      0.3111966929361172
    ],
    [
      0.3184625921044404,
      0.3843000247822095,
      0.37969216334246925,
      0.3554375376245795,
      0.3709292134177198,
      0.3576933977135268,
      0.28624604096375283,
      0.349297220259146,
      0.35577649390164323,
      0.35832618236684777,
      0.3685877740323653,
      0.327063805965806,
      0.3869280810230067,
      0.4181529425629138,
      0.4373768896877539,
      0.36218584239871787,
      0.4048848316238607,
      0.36295209598985023,
      0.3531217729512506,
      0.4646507736109575,
      0.346510096911008,
      0.4263614719326956,
      0.3636562225518072,
      0.33306389341011067,
      0.3614151763654201,
      0.373247942744273,
      0.37169287275522156,
      0.3420529149144065,
      0.0
    ]
  ],
  "row_avgs": [
    0.28190129610232756,
    0.5719199919405578,
    0.49595496173718245,
    0.4624074263037435,
    0.4173248722039771,
    0.42034584056226787,
    0.37366345192621875,
    0.42488005657739597,
    0.4641315317400809,
    0.4353677287469967,
    0.4939515630982393,
    0.32876780167053477,
    0.4306578027329107,
    0.45904615126770015,
    0.5033125603231426,
    0.5062240647106334,
    0.43474733210886457,
    0.4373648645129969,
    0.5007711059888514,
    0.47142584883869854,
    0.506566200579106,
    0.49922805536720133,
    0.37457072080289827,
    0.4139859199701113,
    0.42018049031973514,
    0.5951241732942486,
    0.4218550580606483,
    0.38720238799808715,
    0.36857379528241996
  ],
  "col_avgs": [
    0.3753735110757049,
    0.5288596419634027,
    0.5059559148543576,
    0.5219961553763,
    0.460393617457835,
    0.41416214651934846,
    0.30407897291923025,
    0.5001291596161968,
    0.4874014465491398,
    0.43256569508237214,
    0.47932730839643456,
    0.28758522296497385,
    0.5293295638788793,
    0.5084381832196394,
    0.4476993957086583,
    0.49788260997666234,
    0.44622508248002707,
    0.44978217899891565,
    0.40543189771845506,
    0.5272170316596776,
    0.4583320425571703,
    0.4121823577837909,
    0.3892195413611515,
    0.43522767788185907,
    0.41504903592211256,
    0.49311570711141856,
    0.42940940101811254,
    0.42527699621591125,
    0.3338055585000395
  ],
  "combined_avgs": [
    0.3286374035890162,
    0.5503898169519803,
    0.50095543829577,
    0.4922017908400217,
    0.4388592448309061,
    0.41725399354080817,
    0.3388712124227245,
    0.4625046080967964,
    0.4757664891446104,
    0.4339667119146844,
    0.4866394357473369,
    0.30817651231775434,
    0.479993683305895,
    0.4837421672436698,
    0.47550597801590044,
    0.5020533373436479,
    0.4404862072944458,
    0.44357352175595627,
    0.4531015018536533,
    0.49932144024918806,
    0.48244912156813813,
    0.4557052065754961,
    0.3818951310820249,
    0.4246067989259852,
    0.4176147631209238,
    0.5441199402028336,
    0.4256322295393804,
    0.4062396921069992,
    0.3511896768912297
  ],
  "gppm": [
    650.0375770001758,
    592.0397731114393,
    600.1548112694607,
    596.4224504790715,
    621.0583887744979,
    642.7959338862374,
    696.7429044770322,
    604.4643896269765,
    613.4459834121689,
    638.1445289210336,
    618.0840957238754,
    703.515588934794,
    590.5777172742045,
    603.261721939777,
    633.008224592103,
    607.6816229697145,
    626.6046881749761,
    626.9134777530301,
    649.6113748973261,
    593.9996354468249,
    624.2657589069402,
    648.2198348366081,
    655.8554962590997,
    636.3712582835426,
    644.6633396780384,
    609.6451535932216,
    636.5738767052338,
    640.1148189387226,
    682.8031967134515
  ],
  "gppm_normalized": [
    1.524409895776625,
    1.338995456010823,
    1.3543417068157038,
    1.355139041523499,
    1.3986695463888517,
    1.4508949031796374,
    1.585512522732523,
    1.3622678763853975,
    1.381665607954264,
    1.4349214196871938,
    1.3859245321192124,
    1.6002130695924948,
    1.3281531713348582,
    1.3596405343253652,
    1.4283487141572313,
    1.37649367056641,
    1.4074321008225692,
    1.420080942716839,
    1.4646120931881812,
    1.334959181455245,
    1.4044684527380384,
    1.464717930705307,
    1.4773918054621908,
    1.4300920411817082,
    1.4469468153809835,
    1.380823915182998,
    1.4287377062117912,
    1.442816752471765,
    1.5345477424905276
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457,
    475,
    438,
    496,
    454,
    447,
    436,
    483,
    459,
    425,
    390,
    418,
    596,
    408,
    457,
    477,
    438,
    347,
    421,
    393,
    423,
    422,
    403,
    347,
    442,
    425,
    397,
    418,
    466,
    401,
    668,
    451,
    488,
    442,
    477,
    399,
    415,
    391,
    477,
    467,
    410,
    552,
    421,
    402,
    423,
    411,
    434,
    392,
    381,
    432,
    411,
    388,
    434,
    406,
    404,
    403,
    408,
    482,
    364,
    515,
    394,
    445,
    456,
    590,
    399,
    441,
    475,
    430,
    427,
    416,
    502,
    397,
    443,
    381,
    435,
    391,
    431,
    382,
    457,
    431,
    379,
    342,
    407,
    378,
    371,
    372,
    478,
    412,
    676,
    441,
    444,
    451,
    462,
    493,
    380,
    407,
    445,
    468,
    464,
    604,
    461,
    488,
    449,
    498,
    445,
    422,
    409,
    421,
    392,
    428,
    402,
    455,
    441,
    429,
    408,
    494,
    372,
    307,
    405,
    426,
    444,
    444,
    447,
    430,
    442,
    415,
    412,
    410,
    547,
    406,
    431,
    431,
    401,
    422,
    414,
    409,
    427,
    431,
    407,
    386,
    438,
    364,
    436,
    366,
    452,
    371,
    499,
    431,
    463,
    475,
    444,
    451,
    385,
    460,
    462,
    388,
    408,
    370,
    480,
    462,
    364,
    435,
    420,
    473,
    381,
    480,
    406,
    350,
    421,
    441,
    416,
    367,
    438,
    468,
    386
  ],
  "response_lengths": [
    2545,
    2462,
    2696,
    2783,
    2547,
    2635,
    2346,
    2739,
    2522,
    2251,
    2382,
    2202,
    2699,
    2642,
    2199,
    2579,
    2358,
    2646,
    2268,
    2747,
    2265,
    2075,
    2446,
    2563,
    2341,
    2071,
    2658,
    2588,
    2194
  ]
}