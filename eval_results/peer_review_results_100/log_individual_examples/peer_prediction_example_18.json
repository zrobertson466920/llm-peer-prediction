{
  "example_idx": 18,
  "reference": "Under review as a conference paper at ICLR 2023\n\nVARIATIONAL IMBALANCED REGRESSION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nExisting regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point’s representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation’s variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets show that our VIR can outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation.\n\n1\n\nINTRODUCTION\n\nDeep regression models are currently the state of the art in making predictions in a continuous label space and have a wide range of successful applications in computer vision (Yin et al., 2021), natural language processing (Jiang et al., 2020), etc. However, these models fail however when the label distribution in training data is imbalanced. For example, in visual age estimation (Moschoglou et al., 2017), where a model infers the age of a person given her visual appearance, models are typically trained on imbalanced datasets with overwhelmingly more images of younger adults, leading to poor regression accuracy for images of children or elderly people (Yang et al., 2021). Such unreliability in imbalanced regression settings motivates the need for both improving performance for the minority in the presence of imbalanced data and, more importantly, providing reasonable uncertainty estimation to inform practitioners on how reliable the predictions are (especially for the minority where accuracy is lower).\n\nExisting methods for deep imbalanced regression (DIR) only focus on improving the accuracy of deep regression models by smoothing the label distribution and reweighting data with different labels (Yang et al., 2021). On the other hand, methods that provide uncertainty estimation for deep regression models operates under the balance-data assumption and therefore do not work well in the imbalanced setting (Amini et al., 2020; Mi et al., 2022; Charpentier et al., 2022).\n\nTo simultaneously cover these two desiderata, we propose a probabilistic deep imbalanced regression model, dubbed variational imbalanced regression (VIR). Different from typical variational autoencoders assuming I.I.D. representations (a data point’s representation is not directly affected by other data points), our VIR assumes Neighboring and Identically Distributed (N.I.D.) and borrows data with similar regression labels to compute the latent representation’s variational distribution. Specifically, VIR first encodes a data point into a probabilistic representation and then mix it with neighboring representations (i.e., representations from data with similar regression labels) to produce its final probabilistic representation; VIR is therefore particularly useful for minority data as it can borrow probabilistic representations from data with similar labels (and naturally weigh them using our probabilistic model) to counteract data sparsity. Furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions by the importance weight computed from the smoothed label distribution to impose probabilistic reweighting on the imbalanced data. This allows the negative log likelihood to naturally put more focus on the minority data, thereby balancing the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\naccuracy for data with different regression labels. Our VIR framework is compatible with any deep regression models and can be trained end to end.\n\nWe summarize our contributions as below:\n\n1. While previous work has studied imbalanced regression and uncertainty estimation separately, none of them has considered uncertainty estimation in the imbalanced setting. We identify the problem of probabilistic deep imbalanced regression as well as two desiderata, balanced accuracy and uncertainty estimation, for the problem.\n\n2. We propose VIR to simultaneously cover these two desiderata and achieve state-of-the-art\n\nperformance compared to existing methods.\n\n3. As a byproduct, we also provide strong baselines for benchmarking high-quality uncertainty\n\nestimation and promising prediction performance on imbalanced datasets.\n\n2 RELATED WORK\n\nVariational Autoencoder. Variational autoencoder (VAE) (Kingma & Welling, 2014) is an unsupervised learning model that aims to infer probabilistic representations from data. However, as shown in Figure 1, VAE typically assumes I.I.D. representations, where a data point’s representation is not directly affected by other data points. In contrast, our VIR borrows data with similar regression labels to compute the latent representation’s variational distribution.\n\nImbalanced Regression. Imbalanced regression is underexplored in the machine learning community. Most existing methods for imbalanced regression are direct extensions of the SMOTE algorithm (Chawla et al., 2002), a commonly used algorithm for imbalanced classification, where data from the minority classes is over-sampled. These algorithms usually synthesize augmented data for the minority regression labels by either interpolating both inputs and labels (Torgo et al., 2013) or adding Gaussian noise (Branco et al., 2017; 2018).\n\nFigure 1: Comparison on inference networks between typical VAE (Kingma & Welling, 2014) and our VIR. In VAE (left), a data point’s latent representation (i.e. z) is affected only by itself, while in VIR (right), neighbors participate to modulate the final representation.\n\nSuch algorithms fail to the distance in continuous label space and fall short in handling highdimensional data (e.g., images and text). Recently, DIR (Yang et al., 2021) addresses these issues by applying kernel density estimation to smooth and reweight data on the continuous label distribution, achieving state-of-the-art performance. However, DIR only focuses on improving the accuracy, especially for the data with minority labels, and therefore does not provide uncertainty estimation, which is crucial to assess the predictions’ reliability. Ren et al. (2022) focuses on re-balancing the mean squared error (MSE) loss for imbalanced regression, and Gong et al. (2022) introduces ranking similarity for improving deep imbalanced regression. In contrast, our VIR provides a principled probabilistic approach to simultaneously achieve these two desiderata, not only improving upon DIR in terms of performance but also producing reasonable uncertainty estimation as a much-needed byproduct to assess model reliability. There is also related work on imbalanced classification (Deng et al., 2021), which is related to our work but focusing on classification rather than regression.\n\nUncertainty Estimation in Regression. There has been renewed interest in uncertainty estimation in the context of deep regression models (Kendall & Gal, 2017; Kuleshov et al., 2018; Song et al., 2019; Zelikman et al., 2020; Amini et al., 2020; Mi et al., 2022; van Amersfoort et al., 2021; Liu et al., 2020; Gal & Ghahramani, 2016; Stadler et al., 2021; Snoek et al., 2019; Heiss et al., 2022). Most existing methods either directly predict the variance of the output distribution as the estimated uncertainty (Kendall & Gal, 2017; Zhang et al., 2019; Amini et al., 2020) or rely on post-hoc confidence interval calibration (Kuleshov et al., 2018; Song et al., 2019; Zelikman et al., 2020). Meanwhile, Posterior Networks methods Charpentier et al. (2020; 2022); Stadler et al. (2021) consider conjugate distribution, pseudo-count interpretations, posterior updates, and variational losses for fast and high-quality uncertainty estimation. Closest to our work is Deep Evidential Regression (DER) (Amini et al., 2020), which attempts to estimate both aleatoric and epistemic uncertainty (Kendall & Gal, 2017; Hüllermeier & Waegeman, 2019) on regression tasks by training\n\n2\n\nNxzφNxNzφUnder review as a conference paper at ICLR 2023\n\nthe neural networks to directly infer the parameters of the evidential distribution, thereby producing uncertainty measures. While Posterior Networks Charpentier et al. (2020; 2022) are designed for general classification/regression tasks and achieve promising performance, they do not explicitly consider imbalance in regression tasks, which is the focus of this paper. DER (Amini et al., 2020) is designed for the data-rich regime and therefore fails to reasonably estimate the uncertainty if the data is imbalanced; for data with minority labels, DER (Amini et al., 2020) tends produce unstable distribution parameters, leading to poor uncertainty estimation (as shown in Sec. 4). In contrast, our proposed VIR explicitly handles data imbalance in the continuous label space to avoid such instability; VIR does so by modulating both the representations and the output conjugate distribution parameters according to the imbalanced label distribution, allowing training/inference to proceed as if the data is balance and leading to better performance as well as uncertainty estimation (as shown in Sec. 4).\n\n3 METHOD\n\nIn this section we introduce the problem setting, provide an overview of our VIR, and then describe details on each of VIR’s key components.\n\n3.1 PROBLEM SETTINGS\n\nAssuming an imbalanced dataset in continuous space {xi, yi}N i=1 where N is the total number of data points, xi ∈ Rd is the input, and yi ∈ Y ⊂ R is the corresponding label from a continuous label space Y. In practice, Y is partitioned into B equal-interval bins [y(0), y(1)), [y(2), y(2)), ..., [y(B−1), y(B)), with slight notation overload. To directly compare with baselines, we use the same grouping index for target value b ∈ B as in (Yang et al., 2021). We denote representations as zi, and use ((cid:101)zμ i , (cid:101)zΣ i ) = qφ(z|xi; θ) to denote the probabilistic representations for input xi generated by a probabilistic encoder parameterized by θ. Similarly we use ((cid:98)yi, (cid:98)si) to denote the mean and variance of the predictive distribution generated by a probabilistic predictor pθ(yi|z). Furthermore, we denote ̄z as the mean of representation zi in each bins (i.e., letting ̄z = 1 i=1 zi in a bin with Nb data points). Nb\n\n(cid:80)Nb\n\n3.2 METHOD OVERVIEW\n\nIn order to achieve both desiderata in probabilistic deep imbalanced regression (i.e., performance improvement and uncertainty estimation), our proposed variational imbalanced regression (VIR) operates on both the encoder qφ(zi|{xi}N\n\ni=1) and the predictor pθ(yi|zi).\n\nFigure 2: Overview of our VIR method. Left: The inference model infers the latent representations given input x’s in the neighborhood. Right: The generative model reconstructs the input and predicts the label distribution (including the associated uncertainty) given the latent representation.\n\nTypical VAE (Kingma & Welling, 2014) lower-bounds input xi’s marginal likelihood; in contrast, VIR lower-bounds the marginal likelihood of input xi and labels yi:\n\nlog pθ(xi, yi) = DKL\n\n(cid:0)qφ(zi|{xi}N\n\ni=1)||pθ(zi|xi, yi)(cid:1) + L(θ, φ; xi, yi).\n\nNote that our variational distribution qφ(zi|{xi}N task is to predict yi and (2) conditions on all (neighboring) inputs {xi}N second term L(θ, φ; xi, yi) is VIR’s evidence lower bound (ELBO), which is defined as:\n\ni=1) (1) does not conditions on labels yi, since the i=1 rather than just xi. The\n\nL(θ, φ; xi, yi) = Eq\n\n(cid:124)\n\n(cid:2) log pθ(xi|zi)(cid:3) (cid:123)(cid:122) (cid:125) LD i\n\n+ Eq (cid:124)\n\n(cid:2) log pθ(yi|zi)(cid:3) (cid:123)(cid:122) (cid:125) LP i\n\n(cid:124)\n\n− DKL(qφ(zi|{xi}N\n\ni=1)||pθ(zi)) (cid:125)\n\n.\n\n(1)\n\n(cid:123)(cid:122) LKL i\n\nwhere the pθ(zi) is the standard Gaussian prior N (0, I), following typical VAE (Kingma & Welling, 2014), and the expectation is taken over qφ(zi|{xi}N i=1), which infers zi by borrowing data with similar regression labels to produce the balanced probabilistic representations, which is beneficial especially for the minority (see Sec. 3.3 for details).\n\n3\n\nNxzyθNxzφNUnder review as a conference paper at ICLR 2023\n\nDifferent from typical regression models which produce only point estimates for yi, our VIR’s predictor, pθ(yi|zi), directly produces the parameters of the entire NIG distribution for yi and further imposes probabilistic reweighting on the imbalanced data, thereby producing balanced predictive distributions (more details in Sec. 3.4).\n\n3.3 CONSTRUCTING q(zi|{xi}N\n\ni=1)\n\nTo cover both desiderata, one needs to (1) produce balanced representations to improve performance for the data with minority labels and (2) produce probabilistic representations to naturally obtain reasonable uncertainty estimation for each model prediction. To learn such balanced probabilistic representations, we construct the encoder of our VIR (i.e., qφ(zi|{xi}N i=1)) by (1) first encoding a data point into a probabilistic representation, (2) computing probabilistic statistics from neighboring representations (i.e., representations from data with similar regression labels), and (3) producing the final representations via probabilistic whitening and recoloring using the obtained statistics.\n\nProbabilistic Representations. We first encode each data point into a probabilistic representation. Note that this is in contrast to existing work (Yang et al., 2021) that uses deterministic representations. We assume that each encoding zi is a Gaussian distribution with parameters {zμ i }, which are generated from the last layer in the deep neural network.\n\ni , zΣ\n\nFrom I.I.D. to Neighboring and Identically Distributed (N.I.D.). Typical VAE (Kingma & Welling, 2014) is an unsupervised learning model that aims to learn a variational representation from latent space to reconstruct the original inputs under the I.I.D. assumption; that is, in VAE, the latent value (i.e., zi) is generated from its own input xi. This I.I.D. assumption works well for data with majority labels, but significantly harms performance for data with minority labels. To address this problem, we replace the I.I.D. assumption with the N.I.D. assumption; specifically, VIR’s variational latent representations still follow Gaussian distributions (i.e., N (zμ i ), but these distributions will be first calibrated using data with neighboring labels. For a data point (xi, yi) where yi is in the b’th bin, i.e., i=1) ≜ N (zi; (cid:101)zμ yi ∈ [y(b−1), y(b)), we compute q(zi|{xi}N i = I(xi), b , Σμ b , ΣΣ\n\n(2) (3)\n\ni , (cid:101)zΣ\n\ni , zΣ\n\ni ) as\n\nMean and Covariance of Initial zi: zμ Statistics of Bin b’s Statistics: μμ Smoothed Statistics of Bin b’s Statistics: (cid:101)μμ Mean and Covariance of Final zi: (cid:101)zμ\n\ni , zΣ b , μΣ b , (cid:101)μΣ i , (cid:101)zΣ\n\nΣ\n\nb = A({zμ b = S({μμ b , μΣ i , zΣ\n\ni , zΣ b , μΣ b , Σμ\n\ni }N i=1), b , Σμ b , ΣΣ\n\ni , μμ\n\nμ b , (cid:101)Σ b , (cid:101)Σ i = F(zμ\n\nb , ΣΣ b , (cid:101)μμ\n\nb }B b=1), b , (cid:101)μΣ\n\nb , (cid:101)Σ\n\n(4)\n\nΣ b ),\n\nμ b , (cid:101)Σ\n\nwhere the details of functions I(·), A(·), S(·), and F(·) are described below.\n\nFunction I(·): From Deterministic to Probabilistic Statistics. Different from deterministic statistics in (Yang et al., 2021), our VIR’s encoder uses probabilistic statistics (i.e., statistics of statistics). Specifically, VIR treats zi as a distribution with the mean and covariance (zμ i ) = I(xi) rather than a deterministic vector. As a result, all the deterministic statistics, μb, Σb, (cid:101)μb, and (cid:101)Σb are replaced by distributions with the means and covariances, (μμ b ), and\n\nb ), (Σμ\n\ni , zΣ\n\nb , ΣΣ\n\nb , μΣ\n\nb ), ((cid:101)μμ\n\nb , (cid:101)μΣ\n\n( (cid:101)Σ\n\nb , (cid:101)Σ\n\nb ), respectively (more details in the following three paragraphs on A(·), S(·), and F(·)).\n\nμ\n\nΣ\n\nFunction A(·): Statistics of the current Bin b’s Statistics. As part of our probabilistic overall statistics, the probabilistic overall mean becomes a distribution with the mean (letting μb = ̄z) and covariance (assuming diagonal covariance):\n\nμμ\n\nb = E[ ̄z] = 1\n\nNb\n\n(cid:88)Nb i=1\n\nzμ i ,\n\nμΣ\n\nb = V[ ̄z] = 1\n\nN 2 b\n\n(cid:88)Nb i=1\n\nzΣ i .\n\nSimilarly, our probabilistic overall covariance becomes a matrix-variate distribution (Gupta & Nagar, 2018) with the mean:\n\nΣμ\n\nb = 1\n\nNb\n\n(cid:88)Nb i=1\n\n(zi − ̄z)2 = 1 Nb\n\n(cid:88)Nb i=1\n\n(cid:104)\n\ni + (zμ zΣ\n\ni )2 −\n\n(cid:16)\n\n[μΣ\n\nb ]i + ([μμ\n\nb ]i)2(cid:17)(cid:105)\n\n,\n\nb and V[ ̄z] = μΣ\n\nsince E[ ̄z] = μμ b , involves computing the fourth-order moments, which is computationally prohibitive. Therefore in practice, we directly set ΣΣ b to zero for simplicity; empirically we observe that such simplified treatment already achieves promising performance improvement upon the state of the art.\n\nb . Note that the covariance of Σb, i.e., ΣΣ\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFunction S(·): Neighboring Data and Smoothed Statistics. Next, we can borrow data with neighboring labels (from neighboring label bins) to compute the smoothed statistics of the current bin b by applying a symmetric kernel k(·, ·) (e.g., Gaussian, Laplacian, and Triangular kernels). Specifically, the probabilistic smoothed mean and covariance are (assuming diagonal covariance):\n\n(cid:101)μμ\n\nb =\n\n(cid:88)\n\nb′ ∈B\n\nk(yb, yb′)μμ\n\nb′, (cid:101)μΣ\n\nb =\n\n(cid:88)\n\nb′ ∈B\n\nk2(yb, yb′)μΣ\n\nb′, (cid:101)Σ\n\nb =\n\nμ\n\n(cid:88)\n\nb′ ∈B\n\nk(yb, yb′)Σb′.\n\nFunction F(·): Probabilistic Whitening and Recoloring. We develop a probabilistic version of the whitening and re-coloring procedure (Sun et al., 2016) used in (Yang et al., 2021). Specifically, we produce the final probabilistic representation {(cid:101)zμ i , (cid:101)zΣ\n\ni } for each data point as:\n\n(cid:114)\n\nμ\n\nb\n\n(cid:102)Σ Σμ\n\nb\n\nb ) ·\n\ni − μμ\n\ni = (zμ (cid:101)zμ\n\ni = (zΣ (cid:101)zΣ\n\n+ (cid:101)μμ b , Inspired by (Yang et al., 2021), we keep updating the probabilistic overall statistics, {μμ b , Σb}, and the probabilistic smoothed statistics, {(cid:101)μμ b }, cross different epochs. The probabilistic representation {(cid:101)zμ i } are then re-parameterized (Kingma & Welling, 2014) into the final representation zi, and passed into the final layer (discussed in Sec. 3.4) to generate the prediction and uncertainty estimation. Note that the computation of statistics from multiple x’s is only needed during training. During testing, VIR directly uses these statistics and therefore does not need to re-compute them.\n\n+ (cid:101)μΣ b .\n\ni + μΣ\n\nb , (cid:101)μΣ\n\ni , (cid:101)zΣ\n\nb , μΣ\n\nb ) ·\n\n(5)\n\nb\n\nb\n\n(cid:114)\n\nμ\n\n(cid:102)Σ Σμ\n\n3.4 CONSTRUCTING p(yi|zi)\n\nOur VIR’s predictor p(yi|zi) ≜ N (yi; (cid:98)yi, (cid:98)si) predicts both the mean and variance for yi by first predicting the NIG distribution and then marginalizing out the latent variables. It is motivated by the following observations on label distribution smoothing (LDS) in (Yang et al., 2021) and deep evidental regression (DER) in (Amini et al., 2020), as well as intuitions on effective counts in conjugate distributions.\n\nLDS’s Limitations in Our Probabilistic Imbalanced Regression Setting. The motivation of LDS (Yang et al., 2021) is that the empirical label distribution can not reflect the real label distribution in an imbalanced dataset with a continuous label space; consequently, reweighting methods for imbalanced regression fail due to these inaccurate label densities. By applying a smoothing kernel on the empirical label distribution, LDS tries to recover the effective label distribution, with which reweighting methods can obtain ‘better’ weights to improve imbalanced regression. However, in our probabilistic imbalanced regression, one needs to consider both (1) the performance for the data with minority labels and (2) uncertainty estimation for each model. However, LDS only focuses on improving the accuracy, especially for the data with minority labels, and therefore does not provide uncertainty estimation, which is crucial to assess the predictions’ reliability.\n\nDER’s limitations in Our Probabilistic Imbalanced Regression Setting. In DER (Amini et al., 2020), the predicted labels with their correspond uncertainties are produced by the representation of the posterior parameters in Normal Inverse Gamma (NIG) distribution N IG(γ, ν, α, β), while the model is trained via minimizing the negative log-likelihood (NLL) of a Student-t distribution:\n\nLDER\n\ni\n\n= 1\n\n2 log( π\n\nν ) + (α + 1\n\n2 ) log((yi − γ)2ν + Ω) − α log(Ω) + log( Γ(α)\n\nΓ(α+\n\n1 2 )\n\n),\n\n(6)\n\nwhere Ω = 2β(1 + ν). It is therefore nontrivial to properly incorporate a reweighting mechanism into the NLL. One straightforward approach is to directly reweight LDER for different data points (xi, yi). However, this contradicts the formulation of NIG and often leads to poor performance, as we verify in Sec. 4.\n\ni\n\nIntuition of Pseudo-Counts for VIR. To properly incorporate different reweighting methods, our VIR relies on the intuition of pseudo-counts (pseudo-observations) in conjugate distributions (Bishop, 2006). Assuming Gaussian likelihood, the conjugate distributions would be an NIG distribution (Bishop, 2006), i.e., (μ, Σ) ∼ N IG(γ, ν, α, β), which means: μ ∼ N (γ, Σ/ν), Σ ∼ Γ−1(α, β),\n\nwhere Γ−1(α, β) N IG(γ0, ν0, α0, β0), the posterior distribution of the NIG after observing n real data points are:\n\nis an inverse gamma distribution. With a NIG prior distribution\n\nγn = γ0ν0+nΨ\n\nνn\n\n,\n\nνn = ν0 + n, αn = α0 + n 2 ,\n\n5\n\nβn = β0 + 1\n\n2 (γ2\n\n0 ν0) + Φ,\n\n(7)\n\nUnder review as a conference paper at ICLR 2023\n\n2 ((cid:80)\n\ni x2\n\ni − γ2\n\nwhere Ψ = ̄x and Φ = 1 nνn). Here ν0 and α0 can be interpreted as virtual observations, i.e., pseudo-counts or pseudo-observations that contribute to the posterior distribution. Overall, the mean of posterior distribution above can be interpreted as an estimation from (2α0 + n) observations, with 2α0 virtual observations and n real observations. Similarly, the variance can be interpreted an estimation from (ν + n) observations. This intuition is crucial in developing the predictor of our VIR.\n\nFrom Pseudo-Counts to Balanced Predictive Distributions. Based on the intuition above, we construct our predictor (i.e., p(yi|zi)) by (1) generating the parameters in the posterior distribution of NIG, (2) computing re-weighted parameters by imposing the importance weights obtained from LDS, and (3) producing the final prediction with corresponding uncertainty estimation.\n\nBased on Eqn. 7, we feed the final representation {zi}N linear layer to output the intermediate parameters ni, Ψi, Φi for data point (xi, yi):\n\ni=1 generated from the Sec. 3.3 (Eqn. 5) into a\n\nni, Ψi, Φi = G(zi),\n\nzi ∼ q(zi|{xi}N\n\ni=1) = N (zi; (cid:101)zμ\n\ni , (cid:101)zΣ i )\n\n1 We then apply the importance weights (cid:80) 2 calculated from the smoothed label distribution to the pseudo-count ni to produce the re-weighted parameters of posterior distribution of NIG. Along with the pre-defined prior parameters (γ0, ν0, α0, β0), we are able to compute the parameters of posterior distribution N IG(γi, νi, αi, βi) for (xi, yi):\n\nb′ ∈B k(yb, yb′)(cid:1)−\n\nγ∗\n\ni =\n\nγ0ν0+(cid:0) (cid:80)\n\nb\n\n′\n\n∈B\n\nk(yb,yb′ )(cid:1)−\n\nν∗ n\n\n1 2 ·niΨi\n\n,\n\ni = α0 + (cid:0) (cid:88) α∗\n\nk(yb, yb′)(cid:1)−\n\n1\n\n2 · ni 2 ,\n\nb′ ∈B\n\ni = ν0 + (cid:0) (cid:88) ν∗\n\nk(yb, yb′)(cid:1)−\n\n1 2 · ni,\n\nb′ ∈B\n\nβ∗\n\ni = β0 + 1\n\n2 (γ2\n\n0 ν0) + Φi.\n\nBased on the NIG posterior distribution, we can then compute final prediction and uncertainty estimation as\n\n(cid:98)yi = γ∗\n\ni , (cid:98)si =\n\nβ∗ i (α∗\n\ni\n\ni −1) .\n\nν∗\n\nWe use an objective function similar to Eqn. 6, but with different definitions of (γ, ν, α, β), to optimize our VIR model: (cid:2) 1 2 log( π\n\ni ) + log( Γ(α∗\n\n2 ) log((yi − γ∗\n\nn + Ω) − α∗\n\ni log(ω∗\n\ni = E\n\n) + (α∗\n\ni )2ν∗\n\ni + 1\n\n(8)\n\n)(cid:3),\n\nLP\n\nqφ(zi|{xi}N\n\ni=1)\n\nν∗ i\n\ni ) 1\ni + 2\n\n)\n\nΓ(α∗\n\ni = 2β∗\n\ni (1 + ν∗\n\nwhere ω∗ 2020), we use an additional regularization term to achieve better accuracy1: LR\n\ni ). Note that LP\n\ni = (ν + 2α) · |yi − (cid:98)yi|. together constitute the objective function for learning the predictor p(yi|zi).\n\nLP\n\ni and LR\n\ni\n\ni\n\nis part of the ELBO in Eqn. 1. Similar to (Amini et al.,\n\n3.5 FINAL OBJECTIVE FUNCTION\n\nPutting together Sec. 3.3 and Sec. 3.4, our final objective function (to minimize) for VIR is:\n\nLVIR =\n\n(cid:88)N\n\ni=1\n\nLVIR\n\ni\n\n, LVIR\n\ni\n\n= λLR\n\ni − L(θ, φ; xi, yi) = λLR\n\ni − LP\n\ni − LD\n\ni + LKL\n\ni\n\n,\n\nwhere L(θ, φ; xi, yi) = LP is the ELBO in Eqn. 1. λ adjusts the importance of the additional regularizer and the ELBO, and thus lead to a better result both on accuracy and uncertainty estimation.\n\ni − LKL\n\ni + LD\n\ni\n\n3.6 DISCUSSION ON I.I.D. AND N.I.D. ASSUMPTIONS\n\nGeneralization Error, Bias, and Variance. We could analyze the generalization error of our VIR by bounding the generalization with the sum of three terms: (a) the bias of our estimator, (2) the variance of our estimator, (3) model complexity. Essentially VIR uses the N.I.D. assumption increases our estimator’s bias, but significantly reduces its variance in the imbalanced setting. Since the model complexity is kept the same (using the same backbone neural network) as the baselines, N.I.D. will lead to a lower generalization error (see more discussion in Sec. A of the Appendix).\n\n1Note that in DER, the total evidence Φ has a value 2ν + α, but to the best of our knowledge, it would be\n\nmore reasonable to use ν + 2α as the total evidence for an NIG distribution (Bishop, 2006).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4 RESULTS\n\nDatasets. In this work, we evaluate our methods in terms of prediction accuracy and uncertainty estimation on two imbalanced datasets2, AgeDB (Moschoglou et al., 2017), IMDB-WIKI (Rothe et al., 2018). We follow the preprocessing procedures in DIR (Yang et al., 2021). Details for label density distributions and levels of imbalance are discussed in DIR (Yang et al., 2021).\n\nAgeDB-DIR: We use AgeDB-DIR constructed in DIR (Yang et al., 2021), which contains 12.2K images for training and 2.1K images for validation and testing. The maximum age in this dataset is 101 and the minimum age is 0, and the number of images per bin varies between 1 and 353.\n\nIMDB-WIKI-DIR: We use IMDB-WIKI-DIR constructed in DIR (Yang et al., 2021), which contains 191.5K training images and 11.0K validation and testing images. The maximum age is 186 and minimum age is 0; the maximum bin density is 7149, and minimum bin density is 1.\n\nSTS-B-DIR: We use STS-B-DIR constructed in DIR (Yang et al., 2021), which contains 5.2K pairs of training sentences and 1.0K pairs for validation and testing. This dataset is a collection of sentence pairs generated from news headlines, video captions, etc. Each pair is annotated by multiple annotators with a similarity score between 0 and 5.\n\nBaselines. We use ResNet-50 (He et al., 2016) as our backbone network, and we describe the baselines below.\n\nVanilla: We use the term VANILLA to denote a plain model without adding any approaches.\n\nSynthetic-Sample-Based Methods: Various existing imbalanced regression methods are also included as baselines; these include SMOTER (Torgo et al., 2013) and SMOGN (Branco et al., 2017). Furthermore, following DIR (Yang et al., 2021), in IMDB-WIKI-DIR, we also include another two methods: MIXUP (Zhang et al., 2018) and M-MIXUP (Verma et al., 2019).\n\nCost-Sensitive Reweighting: As shown in DIR (Yang et al., 2021), the square-root weighting variant 1\n(SQINV) baseline (i.e. (cid:0) (cid:80) 2 ) always outperforms Vanilla. Therefore, for simplicity and fair comparison, all our experiments (for both baselines and VIR) use SQINV weighting. To use SQINV in VIR, one simply needs to use the symmetric kernel k(·, ·) described in Sec. 3.3. To use SQINV in DER, we replace the final layer in DIR (Yang et al., 2021) with the DER layer (Amini et al., 2020) to produce the predictive distributions.\n\nb′ ∈B k(yb, yb′)(cid:1)−\n\nEvaluation Metrics - Accuracy. We follow the evaluation metrics in (Yang et al., 2021) to evaluate the accuracy of our proposed methods; these include Mean Absolute Error (MAE), Mean Squared Error (MSE), and Geometric Mean (GM). The formulas for these metrics are as follows:\n\nMAE = 1\n\nN\n\n(cid:88)N\n\ni=1\n\n|yi − (cid:98)yi|, MSE = 1\n\nN\n\n(cid:88)N\n\ni=1\n\n(yi − (cid:98)yi)2, GM =\n\n(cid:104) (cid:89)N\n\ni=1\n\n(cid:105) |yi − (cid:98)yi|\n\n1\n\nN .\n\nEvaluation Metrics - Uncertainty Estimation. We use typical evaluation metrics for uncertainty estimation in regression problems to evaluate our produced uncertainty estimation; these include Negative Log Likelihood (NLL), Area Under Sparsification Error (AUSE). Eqn. 8 shows the formula for NLL, and more details regarding to AUSE can be found in (Ilg et al., 2018).\n\nEvaluation Process. Following (Liu et al., 2019; Yang et al., 2021), for a data sample xi with its label yi which falls into the target bins bi, we divide the label space into three disjoint subsets: many-shot region {bi ∈ B | yi ∈ bi & |yi| > 100}, medium-shot region {bi ∈ B | yi ∈ bi & 20 ≤ |yi| ≤ 100}, and few-shot region {bi ∈ B | yi ∈ bi & |yi| < 20}, where | · | denotes the cardinality of the set. We report results on the overall test set and these subsets with the accuracy metrics discussed above.\n\nImplementation Details. We use ResNet-50 (He et al., 2016) for all experiments in AgeDB-DIR and IMDB-WIKI-DIR. We use the Adam optimizer (Kingma & Ba, 2015) to train all models for 100 epochs, with same learning rate and decay by 0.1 and the 60-th and 90-th epoch, respectively. In order to determine the optimal batch size for training, we try different batch sizes and achieve the same\n\n2Among the five datasets proposed in (Yang et al., 2021), only four of them are publicly available. In this\n\npaper we use the largest (IMDB-WIKI) and the smallest (AgeDB) among the four to evaluate our method.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nMetrics\n\nShot\n\nTable 1: Evaluation results of accuracy on AgeDB-DIR.\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany\n\nMed.\n\nFew\n\nMany\n\nMed.\n\nFew\n\nMany\n\nMed.\n\nFew\n\nVANILLA (Yang et al., 2021) DEEP ENSEMBLE (Lakshminarayanan et al., 2017) SMOTER (Torgo et al., 2013) SMOGN (Branco et al., 2017) SQINV (Yang et al., 2021) DER (Amini et al., 2020) FDS (Yang et al., 2021) LDS (Yang et al., 2021) LDS + FDS (Yang et al., 2021) FDS + RANKSIM (Gong et al., 2022) LDS + FDS + RANKSIM (Gong et al., 2022) LDS + FDS + DER (Yang et al., 2021; Amini et al., 2020) VIR (OURS)\n\nOURS VS. VANILLA OURS VS. SQINV OURS VS. DER OURS VS. LDS + FDS (SOTA IN DIR)\n\n101.28 100.94 114.34 117.29 104.76 106.81 109.78 102.22 102.16 83.51 84.96 112.62 86.89\n\n+14.39 +17.87 +19.92 +15.27\n\n78.40 79.30 93.35 101.36 92.67 91.32 93.99 83.62 86.99 71.99 74.27 94.21 77.69\n\n+0.71 +14.98 +13.63 +9.30\n\n131.17 129.95 129.89 133.86 127.04 122.45 124.96 128.73 128.04 99.14 93.64 140.03 96.55\n\n+34.62 +30.49 +25.90 +31.49\n\nAll\n\n7.79 7.73 8.16 8.26 7.92 8.11 8.12 7.67 7.82 7.02 7.03 8.18 7.14\n\n256.32 249.18 244.57 232.90 205.16 209.76 216.97 204.64 199.18 149.05 161.92 210.72 145.76\n\n6.70 6.62 7.39 7.64 7.42 7.36 7.52 6.98 7.19 6.49 6.54 7.44 6.67\n\n9.42 9.37 8.65 9.01 8.80 9.03 8.68 8.86 9.08 7.84 7.68 9.52 7.70\n\nAll\n\n5.18 4.87 5.21 5.36 5.03 5.31 5.13 4.85 5.01 4.53 4.45 5.30 4.58\n\n4.53 4.37 4.65 4.90 4.81 4.65 4.80 4.39 4.56 4.13 4.07 4.75 4.27\n\n6.75 6.50 5.69 6.19 5.72 6.48 5.97 5.80 6.10 5.37 5.23 6.74 5.09\n\n13.98 13.90 12.28 12.09 11.46 12.69 12.25 10.89 11.24 9.68 9.92 11.45 9.52\n\n+4.46 +1.94 +3.17 +1.72\n\n11.54 11.35 8.49 8.44 8.23 10.52 8.85 7.45 7.02 6.89 6.35 7.68 6.31\n\n+5.23 +1.92 +4.21 +0.71\n\n+110.56 +59.40 +64.00 +53.42\n\n+0.65 +0.78 +0.97 +0.68\n\n+0.03 +0.75 +0.69 +0.52\n\n+1.72 +1.10 +1.33 +1.38\n\n+0.60 +0.45 +0.73 +0.43\n\n+0.26 +0.54 +0.38 +0.29\n\n+1.66 +0.63 +1.39 +1.01\n\nconclusion as the DIR paper, i.e., the optimal batch size is 256 when other hyperparameters are fixed. Therefore, we stick to the batch size of 256 through out the experiments in the paper. Meanwhile, we use the same hyperparameters as in DIR (Yang et al., 2021).\n\nWe use PyTorch to implement our method. For fair comparison, we implemented a PyTorch version for the official TensorFlow implementation of DER(Amini et al., 2020). To make sure we can obtain the reasonable uncertainty estimations, we restrict the range for α to [1.5, ∞) instead of [1.0, ∞) in DER. Besides, in the activation function SoftPlus, we set the hyperparameter beta to 0.1. As discussed in Sec. 3.4, we implement a layer which produces the parameters n, Ψ, Ω. We assign 2 as the minimum number for n, and use the same hyperparameter settings for activation function for DER layer.\n\nTo search for a combination hyperparameters of prior distribution {γ0, ν0, α0, β0} for NIG, we combine grid search method and random search method (Bergstra & Bengio, 2012) to select the best hyperparameters. We first intuitively assign a value and a proper range with some step sizes which correspond to the hyperparameters, then, we apply grid search to search for the best combination for the hyperparameters on prior distributions. After locating a smaller range for each hyperparameters, we use random search to search for better combinations, if it exists. In the end, we find our best hyperparameter combinations for NIG prior distributions.\n\n4.1 RESULTS FOR IMBALANCED REGRESSION ACCURACY\n\nWe report the accuracy of different methods in Table 1 and Table 2 for AgeDB-DIR and IMDB-WIKIDIR, respectively3. In both tables, we can conclude that our methods outperform the baselines in their categories. For ablation studies, see Table 5 and Table 6 of the Appendix. Note that to ensure fair and solid comparison, we re-run the DIR methods based on our machine and software settings4.\n\nOverall Performance. As shown in the last category (i.e., last four rows) of both tables, our proposed method’s best variants compare favorably against the state of the art including DIR variants (Yang et al., 2021) and DER (Amini et al., 2020), especially on the imbalanced data samples (i.e., in the few-shot columns). This verifies the effectiveness of our methods in terms of overall performance.\n\n4.2 RESULTS FOR IMBALANCED REGRESSION UNCERTAINTY ESTIMATION\n\nDifferent from DIR (Yang et al., 2021) which only focuses on accuracy, we create a new benchmark for uncertainty estimation in imbalanced regression. Table 3 and Table 4 show the results on uncertainty estimation for two datasets AgeDB-DIR and IMDB-WIKI-DIR, respectively. Note that most baselines from Table 1 and Table 2 are deterministic methods (as opposed to probabilistic\n\n3Results for STS-B-DIR are reported in Table 7, Table 8, and Table 9 of the Appendix. 4We find that due to differences in PyTorch, GPU, and CUDA versions, as well as numbers of GPUs used for parallel training, the results in DIR may vary. Furthermore, the randomness in multiple workers in the Dataloader also affect the performance.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Evaluation results of accuracy on IMDB-WIKI-DIR.\n\nMetrics\n\nShot\n\nVANILLA (Yang et al., 2021) MIXUP (Zhang et al., 2018) M-MIXUP (Verma et al., 2019) SMOTER (Torgo et al., 2013) SMOGN (Branco et al., 2017) SQINV (Yang et al., 2021) DER (Amini et al., 2020) FDS (Yang et al., 2021) LDS (Yang et al., 2021) LDS + FDS (Yang et al., 2021) LDS + FDS + DER (Yang et al., 2021; Amini et al., 2020) VIR (OURS)\n\nOURS VS. VANILLA OURS VS. SQINV OURS VS. DER OURS VS. LDS + FDS (SOTA IN DIR)\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany\n\nMed.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\n135.48 141.11 137.45 138.75 136.09 134.36 133.81 131.93 133.93 136.72 120.86 119.60\n\n+15.88 +14.76 +14.21 +17.12\n\n107.01 109.13 108.33 111.55 109.15 111.23 107.51 107.76 109.70 112.76 97.75 99.25\n\n+7.76 +11.98 +8.26 +13.51\n\n352.02 389.95 363.72 346.09 339.09 308.63 332.90 311.29 320.26 322.50 297.64 298.85\n\n+53.17 +9.78 +34.05 +23.65\n\n973.73 1037.98 957.53 935.89 944.20 834.08 916.18 880.32 830.81 811.83 873.10 809.34\n\n+164.39 +24.74 +106.84 +2.49\n\n7.99 8.22 8.22 8.14 8.03 7.87 7.85 7.80 7.91 8.08 7.24 7.23\n\n7.18 7.29 7.39 7.42 7.30 7.24 7.18 7.20 7.30 7.47 6.64 6.66\n\n+0.76 +0.64 +0.62 +0.85\n\n+0.52 +0.58 +0.52 +0.81\n\n14.88 16.23 15.24 14.15 14.02 12.44 13.35 12.64 13.02 13.21 11.87 11.90\n\n+2.98 +0.54 +1.45 +1.31\n\n26.72 28.11 26.70 25.28 25.93 22.76 24.12 23.20 22.41 22.54 23.44 21.78\n\n+4.94 +0.98 +2.34 +0.76\n\n4.51 4.68 4.80 4.64 4.63 4.47 4.47 4.39 4.48 4.66 3.93 3.90\n\n4.12 4.22 4.39 4.30 4.30 4.22 4.18 4.16 4.22 4.39 3.69 3.68\n\n+0.61 +0.57 +0.57 +0.76\n\n+0.44 +0.54 +0.50 +0.71\n\n10.46 12.28 10.85 9.05 8.74 7.25 8.18 7.04 7.72 8.01 6.64 6.51\n\n+3.95 +0.74 +1.67 +1.50\n\n21.40 23.55 21.86 19.46 20.12 15.10 15.18 13.42 13.75 14.33 16.00 13.34\n\n+8.06 +1.76 +1.84 +0.99\n\nmethods like ours) and cannot provide uncertainty estimation; therefore they are not applicable here. To show the superiority of our VIR model, we create a strongest baseline by concatenating the DIR variants (LDS + FDS) with the DER (Amini et al., 2020).\n\nMetrics\n\nTable 3: Uncertainty estimation results on AgeDB-DIR.\n\nResults show that VIR outperform the baselines in all few-shot metrics. In some categories, VIR may not perform better in the overall, many-shot and median shot metrics, but the gap tends to be minimal. Note that our proposed methods mainly focus on the imbalanced setting, therefore we also focus on the few-shot metrics. Lastly, comparing our model variant with the best performance against the baseline (DER), we can conclude that our methods successfully improve uncertainty estimation in the probabilistic imbalanced regression setting.\n\n5.311 DEEP ENSEMBLE (Lakshminarayanan et al., 2017) DER (Amini et al., 2020) 3.936 LDS + FDS + DER (Yang et al., 2021; Amini et al., 2020) 3.794 3.703 VIR (OURS)\n\n+0.064 +0.071 +0.060 +0.225 +0.153 +0.026 +0.007 +0.036\n\n0.626 0.449 0.260 0.474\n\n0.466 0.468 0.392 0.319\n\n0.483 0.500 0.617 0.413\n\n8.523 4.421 4.214 4.196\n\n0.541 0.590 0.463 0.437\n\n4.031 3.768 3.699 3.598\n\n6.726 3.865 3.969 3.805\n\nAll Many Med.\n\nAll Many Med.\n\nOURS VS. DER\n\nAUSE ↓\n\nNLL ↓\n\nShot\n\nFew\n\nFew\n\nTable 4: Uncertainty estimation results on IMDB-WIKI-DIR.\n\nWe also observe that the improvements of the uncertainty estimation on IMDB-WIKI are larger than those on Age-DB. We suspect that this because IMDB-WIKI contains much more training, validating and testing data, therefore enjoying more stable uncertainty estimation improvements brought by VIR compared to those in Age-DB.\n\nDER (Amini et al., 2020) 3.850 LDS + FDS + DER (Yang et al., 2021; Amini et al., 2020) 3.683 3.652 VIR (OURS)\n\nAll Many Med.\n\n4.997 4.391 4.419\n\n6.638 5.697 5.560\n\n3.699 3.602 3.568\n\nOURS VS. DER\n\nMetrics\n\nNLL ↓\n\nShot\n\nFew\n\nAUSE ↓\n\nAll Many Med.\n\nFew\n\n0.813 0.784 0.622\n\n0.802 0.670 0.645\n\n0.650 0.455 0.511\n\n0.541 0.483 0.374\n\n+0.198 +0.131 +0.578 +1.078 +0.191 +0.157 +0.202 +0.167\n\n5 CONCLUSION\n\nWe identify the problem of probabilistic deep imbalanced regression, which aims to both improve accuracy and obtain reasonable uncertainty estimation in imbalanced regression. We propose VIR, which can use any deep regression models as backbone networks. VIR borrows data with similar regression labels to produce the probabilistic representations and modulates the conjugate distributions to impose probabilistic reweighting on imbalanced data. Furthermore, we create new benchmarks for uncertainty estimation on imbalanced regression. Experiments show that our methods outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation. Future work may include (1) improving VIR by better approximating variance of the variances in probability distributions, and (2) developing novel approaches that can achieve stable performance even on imbalanced data with limited sample size, and (3) exploring techniques such as mixture density networks (Bishop, 1994) to enable multi-modality in the latent distribution, thereby further improving the performance.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach.\n\nLearn. Res., 13:281–305, 2012.\n\nChristopher M. Bishop. Mixture density networks. Technical report, 1994.\n\nChristopher M Bishop. Pattern recognition and machine learning. springer, 2006.\n\nPaula Branco, Luís Torgo, and Rita P. Ribeiro. SMOGN: a pre-processing approach for imbalanced regression. In First International Workshop on Learning with Imbalanced Domains: Theory and Applications, LIDTA@PKDD/ECML 2017, 22 September 2017, Skopje, Macedonia, volume 74 of Proceedings of Machine Learning Research, pp. 36–50. PMLR, 2017.\n\nPaula Branco, Luís Torgo, and Rita P. Ribeiro. REBAGG: resampled bagging for imbalanced regression. In Second International Workshop on Learning with Imbalanced Domains: Theory and Applications, LIDTA@ECML/PKDD 2018, Dublin, Ireland, September 10, 2018, volume 94 of Proceedings of Machine Learning Research, pp. 67–81. PMLR, 2018.\n\nBertrand Charpentier, Daniel Zügner, and Stephan Günnemann. Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nBertrand Charpentier, Oliver Borchert, Daniel Zügner, Simon Geisler, and Stephan Günnemann. Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\n\nNitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321–357, 2002.\n\nZongyong Deng, Hao Liu, Yaoxing Wang, Chenyang Wang, Zekuan Yu, and Xuehong Sun. PML: progressive margin loss for long-tailed age classification. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1050–1059. JMLR.org, 2016.\n\nYu Gong, Greg Mori, and Frederick Tung. RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 7634–7649. PMLR, 2022.\n\nArjun K Gupta and Daya K Nagar. Matrix variate distributions, volume 104. CRC Press, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778. IEEE Computer Society, 2016.\n\nJakob Heiss, Jakob Weissteiner, Hanna S. Wutte, Sven Seuken, and Josef Teichmann. NOMU: Neural Optimization-based Model Uncertainty. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 8708–8758. PMLR, 2022.\n\nEyke Hüllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning:\n\nA tutorial introduction. CoRR, abs/1910.09457, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nEddy Ilg, Özgün cCiccek, Silvio Galesso, Aaron Klein, Osama Makansi, Frank Hutter, and Thomas Brox. Uncertainty estimates and multi-hypotheses networks for optical flow. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VII, volume 11211 of Lecture Notes in Computer Science, pp. 677–693. Springer, 2018.\n\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 2177–2190. Association for Computational Linguistics, 2020.\n\nAlex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer\n\nvision? arXiv preprint arXiv:1703.04977, 2017.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.\n\nVolodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using calibrated regression. In International Conference on Machine Learning, pp. 2796–2804. PMLR, 2018.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6402–6413, 2017.\n\nJeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness. In Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.\n\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-scale long-tailed recognition in an open world. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 2537–2546. Computer Vision Foundation / IEEE, 2019.\n\nLu Mi, Hao Wang, Yonglong Tian, and Nir Shavit. Training-free uncertainty estimation for neural\n\nnetworks. In AAAI, 2022.\n\nStylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: The first manually collected, in-the-wild age database. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 1997–2005, 2017.\n\nJiawei Ren, Mingyuan Zhang, Cunjun Yu, and Ziwei Liu. Balanced MSE for Imbalanced Visual Regression. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 7916–7925. IEEE, 2022.\n\nRasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a\n\nsingle image without facial landmarks. Int. J. Comput. Vis., 126(2-4):144–157, 2018.\n\nJasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian Nowozin, D. Sculley, Joshua V. Dillon, Jie Ren, and Zachary Nado. Can you trust your model’s uncertainty? Evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 13969–13980, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nHao Song, Tom Diethe, Meelis Kull, and Peter Flach. Distribution calibration for regression. In\n\nInternational Conference on Machine Learning, pp. 5897–5906. PMLR, 2019.\n\nMaximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Zügner, and Stephan Günnemann. Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 18033–18048, 2021.\n\nBaochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In Dale Schuurmans and Michael P. Wellman (eds.), Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pp. 2058–2065. AAAI Press, 2016.\n\nLuís Torgo, Rita P. Ribeiro, Bernhard Pfahringer, and Paula Branco. SMOTE for regression. In Luís Correia, Luís Paulo Reis, and José Cascalho (eds.), Progress in Artificial Intelligence - 16th Portuguese Conference on Artificial Intelligence, EPIA 2013, Angra do Heroísmo, Azores, Portugal, September 9-12, 2013. Proceedings, volume 8154 of Lecture Notes in Computer Science, pp. 378–389. Springer, 2013.\n\nJoost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. Improving Deterministic Uncertainty Estimation in Deep Learning for Classification and Regression. CoRR, abs/2102.11409, 2021.\n\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 6438–6447. PMLR, 2019.\n\nYuzhe Yang, Kaiwen Zha, Ying-Cong Chen, Hao Wang, and Dina Katabi. Delving into deep imbalanced regression. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 11842–11851. PMLR, 2021.\n\nWei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from a single image. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 204–213. Computer Vision Foundation / IEEE, 2021.\n\nEric Zelikman, Christopher Healy, Sharon Zhou, and Anand Avati. Crude: calibrating regression\n\nuncertainty distributions empirically. arXiv preprint arXiv:2005.12496, 2020.\n\nHongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n\nZizhao Zhang, Adriana Romero, Matthew J Muckley, Pascal Vincent, Lin Yang, and Michal Drozdzal. Reducing uncertainty in undersampled mri reconstruction with active acquisition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2049–2058, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA DISCUSSION ON I.I.D. AND N.I.D. ASSUMPTIONS\n\nGeneralization Error, Bias, and Variance. We could analyze the generalization error of our VIR by bounding the generalization with the sum of three terms: (a) the bias of our estimator, (2) the variance of our estimator, (3) model complexity. Essentially VIR uses the N.I.D. assumption increases our estimator’s bias, but significantly reduces its variance in the imbalanced setting. Since the model complexity is kept the same (using the same backbone neural network) as the baselines, N.I.D. will lead to a lower generalization error.\n\nVariance of Estimators in Imbalanced Settings. In the imbalanced setting, one typically use inverse weighting to produced an unbiased estimator (i.e., making the first term of the aforementioned bound zero). However, for data with extremely low density, its inverse would be extremely large, therefore leading to a very large variance for the estimator. Our VIR replaces I.I.D. with N.I.D. to “smooth out” such singularity, and therefore significantly lowers the variance of the estimator (i.e., making the second term of the aforementioned bound smaller), and ultimately lowers the generalization error.\n\nB ADDITIONAL EXPERIMENT RESULTS\n\nB.1 ABLATION STUDY ON VIR\n\nIn this section, we include ablation studies to verify that our VIR can outperform its counterparts in DIR (i.e., smoothing on the latent space) and DER (i.e., NIG distribution layers).\n\nMetrics\n\nTable 5: Ablation study on AgeDB-DIR in terms of accuracy.\n\nAblation Study on q(zi|{xi}N i=1). To verify the effectiveness of VIR’s encoder q(zi|{xi}N i=1), we replace VIR’s predictor p(yi|zi) with a linear layer (as in DIR). Table 5 shows that compared to its counterpart, FDS (Yang et al., 2021), our encoderonly VIR still leads to a considerable improvements even without generating the NIG distribution, therefore verifying the effectiveness of our VIR’s q(zi|{xi}N\n\nDER (Amini et al., 2020) PREDICTOR-ONLY VIR (OURS)\n\nFDS (Yang et al., 2021) ENCODER-ONLY VIR (OURS)\n\nAll Many Med.\n\n209.76 203.76\n\n216.97 157.92\n\n124.96 121.78\n\n109.78 95.99\n\n106.81 88.96\n\n122.45 95.85\n\nMany Med.\n\n91.32 74.79\n\n93.99 81.89\n\n12.25 10.03\n\n12.69 11.63\n\n8.68 8.72\n\n9.03 7.76\n\n8.12 7.57\n\n8.11 7.28\n\n7.52 6.97\n\n7.36 6.68\n\nMAE ↓\n\nMSE ↓\n\nShot\n\nFew\n\nFew\n\nAll\n\ni=1).\n\nTable 6: Ablation study on AgeDB-DIR in terms of uncertainty estimation.\n\nAblation Study on p(yi|zi). To verify the effectiveness of VIR’s predictor p(yi|zi), we replace VIR’s encoder q(zi|{xi}N i=1) with a simple deterministic encoder as in DER (Amini et al., 2020). Table 5 and Table 6 show that compared to DER, the counterpart of VIR’s predictor, our VIR’s predictor still outperforms than DER, demonstrating its effectiveness; this verifies our claim (Sec. 3.4) that directly reweighting DER breaks NIG and leads to poor performance.\n\nDER Amini et al. (2020) PREDICTOR-ONLY VIR (OURS)\n\nMany Med.\n\nMany Med.\n\n0.449 0.387\n\n3.865 3.854\n\n3.936 3.887\n\n0.590 0.443\n\n0.468 0.390\n\n3.768 3.755\n\n0.500 0.407\n\n4.421 4.394\n\nAUSE ↓\n\nMetrics\n\nNLL ↓\n\nShot\n\nFew\n\nFew\n\nAll\n\nAll\n\nB.2 RESULT ON STS-B-DIR DATASET\n\nIn this section, we report the accuracy and uncertainty evaluation on STS-B-DIR (more details for the dataset is in DIR (Yang et al., 2021)). From Table 7, Table 8, and Table 9 below, we can conclude\n\nTable 7: Evaluation results of accuracy on STS-B-DIR.\n\nMetrics\n\nShot\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nINV DIR (YANG ET AL., 2021) DIR + DER (YANG ET AL., 2021; AMINI ET AL., 2020) VIR (OURS)\n\n1.031 1.000 1.007 0.895\n\n0.930 0.912 0.880 0.799\n\n1.426 1.368 1.535 1.309\n\n1.152 1.055 1.086 0.919\n\n0.825 0.812 0.812 0.760\n\n0.783 0.772 0.757 0.718\n\n1.004 0.989 1.046 0.960\n\n0.850 0.809 0.842 0.732\n\n0.567 0.560 0.558 0.509\n\n0.537 0.535 0.518 0.493\n\n0.744 0.739 0.765 0.669\n\n0.535 0.477 0.574 0.377\n\nthat our model also outperforms all baselines in terms of both accuracy metrics and uncertainty estimation metrics in this NLP dataset; this verifies the superiority of our model for NLP datasets.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Evaluation results of accuracy on STS-B-DIR.\n\nMetrics\n\nShot\n\nPearson ↑\n\nSpearman ↑\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nINV DIR (YANG ET AL., 2021) DIR + DER (YANG ET AL., 2021; AMINI ET AL., 2020) VIR (OURS)\n\n0.718 0.732 0.729 0.765\n\n0.701 0.711 0.714 0.740\n\n0.612 0.646 0.635 0.663\n\n0.705 0.742 0.731 0.770\n\n0.723 0.731 0.730 0.770\n\n0.678 0.672 0.680 0.713\n\n0.530 0.519 0.526 0.534\n\n0.685 0.739 0.699 0.770\n\nTable 9: Uncertainty estimation results on STS-B-DIR.\n\nMetrics\n\nShot\n\nNLL ↓\n\nAUSE ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nDIR + DER (YANG ET AL., 2021; AMINI ET AL., 2020) VIR (OURS)\n\n2.561 1.996\n\n2.514 1.810\n\n2.880 2.754\n\n2.358 2.152\n\n0.672 0.591\n\n0.581 0.575\n\n0.609 0.602\n\n0.615 0.510\n\nB.3 DIFFERENCE BETWEEN DIR’S AND OUR REPRODUCED RESULTS\n\nTo reproduce the results on AgeDB, we use exactly the same settings as in DIR’s code (Yang et al., 2021) (i.e., by directly running their code on our machines without modifying hyperparameters). for each model in DIR we report, we use five different random seeds to produce five results. We then report the performance by taking the average of them. Table 10 and Table 11 show the example for SQINV and LDS+FDS on AgeDB-DIR. From the table we can see that under our hardware and\n\nTable 10: Results of running SQINV for 5 different random seeds on AgeDB.\n\nMetrics\n\nShot\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nSQINV 1 SQINV 2 SQINV 3 SQINV 4 SQINV 5 SQINV AVG SQINV STD SQINV RESULTS FROM (YANG ET AL., 2021)\n\n107.02 111.55 114.33 106.24 104.73 108.77 12.89 105.14\n\n90.71 93.43 96.83 91.81 90.24 92.60 5.67 87.21\n\n131.5 141.03 134.56 120.26 127.33 130.94 48.46 127.66\n\n193.39 209.17 223.86 203.78 208.05 207.65 96.71 212.30\n\n8.04 8.12 8.21 7.94 7.99 8.06 0.01 7.81\n\n7.40 7.47 7.59 7.39 7.47 7.46 0.01 7.16\n\n9.01 9.17 9.01 8.58 8.98 8.95 0.04 8.80\n\n11.33 11.58 11.81 11.39 11.49 11.52 0.03 11.20\n\n5.15 5.21 5.17 5.06 5.07 5.13 0.01 4.99\n\n4.73 4.85 4.74 4.74 4.79 4.77 0.01 4.57\n\n8.81 5.75 5.85 5.41 5.68 6.30 1.60 5.73\n\n8.22 8.25 8.27 7.66 7.98 8.08 0.05 7.77\n\nsoftware environments, the SQINV model and LDS+FDS model (SOTA in DIR) could not perform as well as it is reported in DIR Yang et al. (2021), therefore for fair comparison, we use our replicated performance rather than theirs.\n\nB.4 ABLATION STUDY ON λ\n\nIn this section, we include ablation studies on the λ in our objective function. For λ ∈ {10.0, 1.0, 0.1, 0.01, 0.001}, we run our VIR model on the AgeDB dataset. Table 12 shows the results. We can conclude that when λ = 0.1, our model achieves the best performance.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Results of running LDS+FDS for 5 different random seeds on AgeDB.\n\nMetrics\n\nShot\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nLDS+FDS 1 LDS+FDS 2 LDS+FDS 3 LDS+FDS 4 LDS+FDS 5 LDS+FDS AVG LDS+FDS STD LDS+FDS RESULTS FROM (YANG ET AL., 2021)\n\n104.33 104.59 110.17 102.68 105.77 105.51 6.41 99.46\n\n88.67 94.63 95.97 98.20 91.07 93.71 11.70 84.10\n\n128.99 125.60 123.24 126.41 127.00 126.25 3.52 112.20\n\n194.06 200.14 208.11 201.16 185.85 197.86 55.97 209.27\n\n7.87 7.98 8.07 8.02 7.93 7.97 0.01 7.55\n\n7.26 7.44 7.54 7.50 7.35 7.42 0.01 7.01\n\n8.97 8.77 8.71 8.82 8.80 8.81 0.01 8.24\n\n10.88 11.16 11.41 11.34 10.96 11.15 0.04 10.79\n\n5.02 5.00 5.09 5.08 5.07 5.05 0.01 4.72\n\n4.60 4.71 4.73 4.63 4.74 4.68 0.03 4.36\n\n5.87 5.62 5.71 5.74 5.52 5.69 0.01 5.45\n\n7.51 7.81 7.48 7.56 7.73 7.62 0.02 6.79\n\nTable 12: Ablation study on λ for VIR on AgeDB-DIR\n\nMetrics\n\nShot\n\nλ = 10.0 λ = 1.0 λ = 0.1 λ = 0.01 λ = 0.001\n\nMSE ↓\n\nMAE ↓\n\nNLL ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\n104.31 104.10 86.28 86.86 87.25\n\n91.01 87.28 76.87 76.58 74.13\n\n116.43 128.26 101.57 99.95 104.78\n\n196.35 196.12 132.90 147.82 162.64\n\n7.88 7.83 7.19 7.12 7.13\n\n7.38 7.21 6.75 6.69 6.64\n\n8.42 8.81 7.97 7.72 7.92\n\n11.13 10.89 9.19 9.59 9.63\n\n3.827 3.848 3.785 3.887 3.980\n\n3.733 3.738 3.694 3.797 3.868\n\n4.140 4.041 3.963 4.007 4.161\n\n4.407 4.356 4.151 4.401 4.546\n\n15",
  "translations": [
    "# Summary Of The Paper\n\nThe paper looks at uncertainty estimation for regression on imbalanced datasets. It proposes a new method called VIR which combines variational inference, smoothed statistics, and conjugate distribution parametrization. In the experiments, VIR is evaluated on 2 imbalanced datasets on accuracy and calibration metrics.\n\n# Strength And Weaknesses\n\nPros:\n\n1. The task of uncertainty estimation for regression on imbalanced dataset is important.\n2. The idea of combining smoothed statistics to fix imbalanced dataset and produce uncertainty estimates via parametrizing conjugate distributions is interesting.\n3. The method achieves great improvement on 2 datasets.\n\n##########################################################################\n\nCons:\n\n- Related works: The paper misses many important related works for uncertainty estimation for regression like [1, 2, 3, 4, 5, 6, 7] but also many others. This includes very common baselines like Dropout [6] and Ensemble [1] which work for regression. The paper also does not discuss GP-based methods [2, 3] which can be used for regression. Further, the paper does not discuss Posterior Networks methods [4, 5, 7]. More specifically, [4, 5] shares many similarities with VIR. They use conjugate distributions, pseudo-count interpretations, posterior updates, and variational losses which are key parts of VIR. NatPN is also designed to work for regression. Action suggestion: discuss all of these methods in the related work section.\n- Desiderata: The two desiderata are sometimes not clear. It feels that the two desiderata are high quality *uncertainty estimation* and high performance on *imbalanced dataset*. However the paper mentions that the desiderata are “performance improvement and uncertainty estimation”. The paper also phrases the identification of the desiderata of imbalanced dataset and uncertainty estimation as a contribution.  However people already looked at imbalanced datasets and uncertainty estimation separately in previous works. Finally, the paper does not discuss existing desiderata for uncertainty estimation [8, 7, 9]. Action suggestion: I would recommend to change the phrasing of this contribution and have a discussion on existing desiderata in uncertainty estimation.\n- Loss: I felt that the description of the loss was sometimes confusing. I would be interested in the derivation fo the loss. This could also be provided in the appendix. In eq.(1), what is p_\\theta(z_i) ? Does this ELBO loss assume any prior ? What is the importance of the regularization loss in the total loss ? Action suggestion: Beyond the answer to these questions, it would be intresting to show the important of the regularization term in an experiment.\n- Clarity: The paper is sometimes hard to read. E.g. the paper mentions multiple times “see below” which leaves unclear where to exactly find the missing information. Action suggestion: make explicit reference when pointing to further details.\n- Bins: It is unclear what is the sensitivity of the method to the number of bins. It was also unclear to me when readin sec. 3.1. what data are used to build the mean representation from the bins. Action suggestion: I would recommend to clarify the bins construction since it is an essential part of the method. I would also be interested in an experiment on the number of bins.\n- Experiences: The experiments do not look very extensive to me. They consider only two datasets and a single backbone network. They do not compare to DropOut, Ensemble, GP, or NatPN which would be appropriate baselines. They do not look at common uncertainty estimation metrics like OOD detection scores. They also do not provide error bars which are key to assess the significance of the results. Action suggestion: I would recommend to add at least one dataset (e.g. depth estimation) with another backbone architecture. I would also recommend to add at least Ensemble which is a common and powerful baseline and NatPN which shares many similarities with VIR. I would also recommend to add error bars.\n\nI am happy to improve my score if a majority of the above points are addresses (e.g. with the action suggestions).\n\n[1] Simple and scalable predictive uncertainty estimation using deep ensembles, NeurIPS 2017\n\n[2] On feature collapse and deep kernel learning for single forward pass uncertainty.\n\n[3] Simple and principled uncertainty estimation with deterministic deep learning via distanceawareness, NeurIPS 2020.\n\n[4] Posterior Network: uncertainty estimation without ood samples via density-based pseudo-counts. NeurIPS 2020\n\n[5] Natural Posterior Network: deep bayesian uncertainty for exponential family distributions, ICLR 2022\n\n[6] Dropout as a bayesian approximation: representing model uncertainty in deep learning, ICML 2016\n\n[7] Graph Posterior Network: bayesian predictive uncertainty for node classification. NeurIPS 2021.\n\n[8] Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift, NeurIPS 2019.\n\n[9] NOMU: Neural Optimization-based Model Uncertainty. ICML 2022\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is quite clear even if it is sometimes hard to read with quite dense mathematical notations (e.g. eq. 2, 3, 4) and some unclear cross references (i.e. \"see below\" statements). The paper combines existing techniques (smoothed statistics and conjugate distribution parametrization)  to solve the important problem of prediction on imbalanced datasets. However, it misses many important related works which would allow to more correctly estimate the novelty of the paper for the reader.\n\n# Summary Of The Review\n\nOverall, I vote for strong reject. The task is important and well motivated and the . My major concerns are about the related work, and the experiences (see cons beow). Hopefully the authors can address my concern in the rebuttal period.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents \"Variational Imbalanced Regression\" (VIR), a novel probabilistic deep learning model designed to improve both accuracy and uncertainty estimation in imbalanced regression scenarios. Unlike traditional variational autoencoders that assume independent and identically distributed (I.I.D.) representations, VIR employs a Neighboring and Identically Distributed (N.I.D.) approach. The model predicts full normal-inverse-gamma distributions rather than point estimates, enhancing uncertainty estimations. Experimental results demonstrate that VIR outperforms existing state-of-the-art models across various datasets and metrics, particularly in scenarios with imbalanced data.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to addressing the dual challenges of accuracy and uncertainty estimation in imbalanced datasets, which is a significant gap in current methodologies. The use of the N.I.D. assumption is both novel and effective, allowing for better representation of the data. Furthermore, the comprehensive evaluation on multiple datasets and the inclusion of ablation studies lend credence to the model's robustness. However, the paper could benefit from a deeper exploration of the computational efficiency of VIR, as well as a discussion on potential limitations when scaling to larger datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The explanations of the model's components, particularly the encoder and predictor, are lucid and easy to follow. The quality of the writing is high, with appropriate use of figures and tables to illustrate results. The novelty of the approach is significant, particularly in the context of imbalanced regression, although the reproducibility could be improved by providing more detailed descriptions of experimental setups and hyperparameter settings.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of imbalanced regression by introducing the VIR model, which effectively combines accuracy and uncertainty estimation. While the methodology is innovative and results are promising, a more thorough exploration of computational aspects and clearer reproducibility measures would enhance the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Variational Imbalanced Regression (VIR), a novel regression model designed to address the challenges posed by imbalanced label distributions in regression tasks. By moving from traditional independent and identically distributed (I.I.D.) assumptions to neighboring and identically distributed (N.I.D.) assumptions, VIR leverages probabilistic representations of data. It predicts entire normal-inverse-gamma distributions instead of point estimates, thus improving both accuracy and uncertainty estimation. The model incorporates a probabilistic reweighting mechanism to better balance contributions from imbalanced datasets. Experimental results demonstrate that VIR consistently outperforms state-of-the-art regression methods in terms of both accuracy and uncertainty estimation across multiple imbalanced datasets, including AgeDB-DIR, IMDB-WIKI-DIR, and STS-B-DIR.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its novel approach to handling imbalanced data by focusing on neighboring data points, which provides a fresh perspective on regression modeling. The dual emphasis on improving both accuracy and uncertainty estimation is significant for practical applications, particularly in fields where understanding model confidence is essential. Furthermore, the comprehensive evaluation across various datasets and strong baseline comparisons substantiate the model's effectiveness. However, the paper also presents several weaknesses, including the potential computational complexity of the probabilistic model, sensitivity to hyperparameters, and challenges related to generalization beyond the datasets used. Additionally, the dependence on neighboring data may limit the model's performance in contexts where such data is sparse.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents the methodology, experimental design, and results. The writing is precise, making it easy to follow the paper's contributions and findings. The novelty of the approach is significant, particularly the shift to N.I.D. assumptions and the focus on uncertainty estimation. However, the reproducibility of results may be affected by hyperparameter sensitivity, and the computational demands could hinder practical implementation in resource-limited scenarios. The implementation details provided are adequate, but further clarity on hyperparameter tuning guidelines would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a strong contribution to the field of regression modeling for imbalanced datasets with its innovative approach and comprehensive evaluation. While the proposed model demonstrates significant improvements in both accuracy and uncertainty estimates, concerns regarding complexity and generalization need to be addressed in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Variational Imbalanced Regression (VIR) model, addressing the challenges of regression tasks with imbalanced label distributions, particularly focusing on accuracy and uncertainty estimation. VIR innovatively predicts entire normal-inverse-gamma (NIG) distributions instead of point estimates and employs a unique method of probabilistic reweighting through conjugate distributions. Through extensive experiments on datasets like AgeDB, IMDB-WIKI, and STS-B, the authors demonstrate that VIR significantly outperforms existing methods in both accuracy and uncertainty estimation, particularly in few-shot scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its dual focus on improving both accuracy and uncertainty estimation in imbalanced regression contexts, filling a notable gap in the literature. The methodological innovations, such as the transition from I.I.D. to N.I.D. representations and the introduction of a comprehensive objective function combining different losses, are well-articulated and offer substantial improvements over previous approaches. However, a potential weakness is the reliance on specific datasets, which may limit the generalizability of the findings. Additionally, while the paper discusses the model's theoretical underpinnings, a more detailed analysis of the computational complexity and scalability of the VIR model could enhance its practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers familiar with regression and uncertainty estimation techniques. The quality of the experiments is high, with appropriate baseline comparisons and a clear explanation of evaluation metrics. The novelty of the approach is significant, as it effectively combines insights from variational inference with considerations for imbalanced data. Reproducibility is supported by the mention of using PyTorch for implementation and explicit details on hyperparameter tuning, though providing a public repository of the code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of regression analysis under imbalanced conditions by introducing the VAR model, which effectively integrates accuracy and uncertainty estimation. The methodology is innovative, and the empirical results substantiate its claims. However, more extensive validation across diverse datasets and clearer insights into computational efficiency would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel model called Variational Imbalanced Regression (VIR), which aims to improve both accuracy and uncertainty estimation in the context of imbalanced regression tasks. The methodology incorporates a probabilistic framework to generate outputs that reflect uncertainty, addressing a significant gap in existing literature. The findings demonstrate that VIR outperforms state-of-the-art approaches on various real-world datasets, showcasing enhanced performance in terms of both predictive accuracy and uncertainty quantification.\n\n# Strength And Weaknesses\nThe primary strength of this work is its innovative approach to tackling the challenges of imbalanced regression, specifically through the dual focus on accuracy and uncertainty estimation. The experimental results are comprehensive, illustrating the model's superiority over existing methods. However, limitations include the model's reliance on the assumption of Neighboring and Identically Distributed (N.I.D.) data, which may restrict its applicability. Additionally, while the uncertainty estimation is a valuable contribution, the paper lacks extensive validation across diverse datasets, potentially undermining trust in its practical utility. The theoretical framework, while well-articulated, may be too complex for practitioners, and the evaluation metrics, though robust, could benefit from a deeper discussion concerning their relevance to real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that facilitates understanding. The theoretical contributions are significant and provide a solid foundation for the proposed model. However, the complexity of the theoretical explanations may pose challenges for practitioners unfamiliar with probabilistic modeling. The reproducibility of the results is supported by the comprehensive experimental design, although a broader inclusion of recent methods in the benchmarking process would enhance the evaluation's relevance.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of imbalanced regression through the introduction of the Variational Imbalanced Regression model. While the contributions are noteworthy, certain limitations regarding generalizability and practical applicability warrant consideration. The work lays a promising foundation for future research in this underexplored area.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Variational Imbalanced Regression (VIR), a new approach aimed at addressing challenges associated with imbalanced data in regression tasks. VIR seeks to improve performance for minority classes while providing robust uncertainty estimates. The methodology employs a Neighboring and Identically Distributed (N.I.D.) approach, leveraging similar regression labels to enhance probabilistic representations. Experimental results demonstrate that VIR outperforms state-of-the-art methods in both accuracy and uncertainty estimation across benchmark datasets, such as AgeDB and IMDB-WIKI.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by identifying a novel problem space that unifies accuracy and uncertainty estimation in imbalanced regression. The methodological innovation of using N.I.D. data points presents a meaningful departure from traditional I.I.D. assumptions, enabling improved performance, especially for minority samples. The comprehensive uncertainty estimation is a notable strength, providing more nuanced predictions. However, the paper could benefit from clearer terminology regarding new concepts like N.I.D., and further validation through more detailed ablation studies is recommended. Additionally, broader examples of real-world applications could enhance the practical relevance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, but the introduction of specific terms like N.I.D. may require additional context for clarity. The quality of the methodology is sound, and the novelty is evident in the dual focus on both accuracy and uncertainty in imbalanced datasets. The reproducibility of the findings is supported by extensive experimental validation on benchmark datasets, although more details on implementation and hyperparameter settings could further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new approach to imbalanced regression through VIR, demonstrating significant improvements in both accuracy and uncertainty estimation. While the methodology is innovative and well-executed, enhancing clarity around terminology and providing additional empirical validation would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces Variational Adversarial Training (VAT), a novel framework designed to enhance the robustness of deep learning models against adversarial attacks while integrating uncertainty estimation into the training process. The methodology involves encoding input data into a latent space using a variational autoencoder, from which adversarial examples are generated through probabilistic sampling. The findings demonstrate that VAT achieves state-of-the-art performance on benchmark datasets, such as CIFAR-10 and MNIST, leading to improved adversarial robustness and accurate uncertainty estimates.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative integration of variational methods with adversarial training, which not only addresses the critical issue of robustness but also incorporates uncertainty estimation—an aspect often overlooked in traditional approaches. The theoretical insights provided lend credibility to the proposed framework, and the extensive empirical validation showcases VAT's effectiveness across various datasets and attack scenarios. However, a notable weakness is the potential increase in computational complexity, which could limit the applicability of VAT in resource-constrained environments. Additionally, the paper could benefit from a more thorough exploration of real-world applications and scalability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The quality of both the theoretical and empirical components is high, with sufficient detail to allow for reproducibility of results. The novelty of combining variational methods with adversarial training is significant, marking a meaningful advancement in the field.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in adversarial training through the introduction of Variational Adversarial Training, effectively combining robustness with uncertainty estimation. While the complexity of the method poses some challenges for practical implementation, its strong theoretical foundation and empirical results warrant acceptance for presentation at ICLR.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents the Variational Imbalanced Regression (VIR) model, which aims to address the issues of accuracy and uncertainty estimation in regression tasks affected by imbalanced label distributions. The authors claim that VIR introduces a novel approach by assuming data points are dependent on neighboring points, which they argue is a significant departure from existing methods. The experimental results purportedly show that VIR outperforms all baseline models, achieving unprecedented accuracy and reliability in uncertainty estimation.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its identification of a critical problem in imbalanced regression that has been largely overlooked, and the introduction of VIR as a potential solution. However, the claims regarding the model's groundbreaking performance appear exaggerated, as the paper does not sufficiently substantiate its superiority over existing methods. The discussion of related work is dismissive of prior contributions, which could mislead readers about the existing landscape of regression modeling. Additionally, while the methodology introduces an innovative learning mechanism, the generalizability of the model remains uncertain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's presentation could benefit from improved clarity, particularly in the methodology section, where the novelty of the learning mechanism is described but not thoroughly explained. The quality of the writing is generally acceptable, but the overstatements about the model's impact detract from its perceived rigor. Reproducibility is a concern due to the lack of detailed descriptions of experimental setups and benchmarks, which are crucial for validating the reported results.\n\n# Summary Of The Review\nOverall, the paper introduces the VIR model as a potentially transformative approach to imbalanced regression; however, the claims of superiority and groundbreaking performance are overstated and lack sufficient empirical backing. While the identification of the dual issues of accuracy and uncertainty is commendable, the dismissal of prior work and the insufficient clarity in methodology raise concerns about the validity of the findings.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents Variational Imbalanced Regression (VIR), a novel approach designed to improve performance in regression tasks on imbalanced datasets while also providing reliable uncertainty estimates. The methodology involves a Neighboring and Identically Distributed (N.I.D.) framework that enhances probabilistic representation by leveraging similar regression labels to counteract data sparsity. Experimental results demonstrate that VIR outperforms several baseline methods, including Vanilla, SMOTER, and DER, in terms of both prediction accuracy and uncertainty estimation across multiple datasets, specifically AgeDB and IMDB-WIKI.\n\n# Strength And Weaknesses\nStrengths of the paper include its dual focus on accuracy and uncertainty estimation in imbalanced regression contexts, which is often overlooked in existing literature. The introduction of the N.I.D. approach is a significant methodological contribution that addresses common challenges in dealing with imbalanced datasets. However, a potential weakness lies in the adjustments made to baseline results, which might question the objectivity of the comparisons. The paper could benefit from a more detailed discussion on the implications of using the modified baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and provides clear explanations of the methodology and experimental results. The quality of the figures and tables is high, aiding in the understanding of performance metrics. However, while the novel approach is articulated, the reproducibility of results could be improved by providing more detailed information on the implementation specifics and hyperparameter settings utilized during experiments.\n\n# Summary Of The Review\nOverall, this paper presents a compelling contribution to the field of imbalanced regression by addressing both accuracy and uncertainty through the innovative VIR framework. While the results are promising, the modifications to baseline performance metrics warrant further scrutiny to ensure robust comparisons.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to handling imbalanced regression tasks through a framework that incorporates Neighborhood-Dependent (N.I.D.) representations and probabilistic reweighting based on smoothed label distributions. The authors argue that their method enhances uncertainty estimation and overall model performance compared to state-of-the-art techniques. Empirical results suggest that their approach improves the accuracy of predictions on minority labels, although concerns about the generalizability of these findings across diverse datasets remain.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative framework that attempts to address the common issue of imbalanced data in regression tasks, proposing N.I.D. representations as a potential solution. However, the paper has several weaknesses, including an overreliance on specific assumptions about data homogeneity and the effectiveness of probabilistic reweighting. Furthermore, the evaluation metrics employed are limited, and the exploration of alternative methods for imbalanced regression is insufficient, potentially leading to a narrow perspective on the problem.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates its contributions; however, certain assumptions, such as the N.I.D. approach's universal applicability and the robustness of results across datasets, are not thoroughly justified. The novelty of the proposed methodology is significant yet could benefit from a more comprehensive exploration of existing methods. Reproducibility is a concern, as the paper does not adequately address the generalization of results across different datasets and the sensitivity of the model to hyperparameter tuning.\n\n# Summary Of The Review\nOverall, the paper presents a promising methodological advancement for dealing with imbalanced regression problems, but it suffers from several limitations regarding assumptions, evaluative metrics, and the exploration of alternative approaches. While the framework shows potential, its practical application may be hindered by issues of robustness and generalizability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents Variational Imbalanced Regression (VIR), a novel probabilistic deep learning model designed to enhance both accuracy and uncertainty estimation in imbalanced regression tasks. The authors introduce a framework that operates under a Neighboring and Identically Distributed (N.I.D.) assumption, allowing the model to leverage neighboring data points with similar regression labels. VIR predicts full distributions rather than mere point estimates, addressing the shortcomings of traditional regression models that fail to adequately handle imbalanced label distributions. Experimental results on benchmark datasets, including AgeDB and IMDB-WIKI, demonstrate that VIR outperforms state-of-the-art models in both accuracy and uncertainty estimation, thus establishing new benchmarks in this field.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing the dual challenges of accuracy and uncertainty in imbalanced regression tasks. The N.I.D. assumption is a thoughtful contribution that enhances the model's ability to consider local data distributions, thereby improving predictions for minority classes. Additionally, the empirical results provide strong evidence of the model's effectiveness. However, a potential weakness is the reliance on specific datasets for validation; further testing on a wider array of datasets could strengthen the claims. Furthermore, while the paper discusses future work on variance approximations and multi-modality, it lacks detailed exploration of potential limitations or challenges inherent in these areas.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The writing quality is high, with technical terms appropriately defined and contextualized within the existing literature. The novelty of the approach is significant, particularly in the integration of uncertainty estimation with imbalanced regression. However, the paper could benefit from additional details on the implementation of the model to enhance reproducibility; including more comprehensive algorithms and hyperparameter settings would aid future research efforts looking to replicate or extend this work.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of imbalanced regression through the introduction of the VIR model, which successfully addresses both accuracy and uncertainty estimation. While the findings are compelling and well-supported by empirical evidence, further exploration of model limitations and enhanced reproducibility details would improve the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at enhancing the robustness of machine learning models under adversarial conditions. The authors propose a hybrid model that integrates elements from robust optimization and ensemble learning to improve model performance in the presence of noise and adversarial attacks. Through extensive experimentation, the authors demonstrate that their approach outperforms existing state-of-the-art methods on several benchmark datasets, highlighting its effectiveness in real-world scenarios where adversarial threats are prevalent.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Framework:** The combination of robust optimization and ensemble learning is a fresh perspective that could significantly enhance the stability of machine learning models.\n2. **Strong Theoretical Foundation:** The paper articulates a clear theoretical basis for the proposed method, which aids in understanding how the framework can mitigate adversarial effects.\n3. **Comprehensive Evaluation:** The authors conduct a thorough evaluation across multiple datasets, providing compelling evidence of the method's superiority over traditional approaches.\n\n**Weaknesses:**\n1. **Limited Scope of Experiments:** While the results are promising, the experiments are primarily based on a narrow set of datasets, which may limit the generalizability of the findings.\n2. **Lack of Baseline Comparisons:** The paper does not sufficiently compare the proposed method to a broad range of baseline techniques, making it difficult to assess the true advancements made.\n3. **Unclear Hyperparameter Sensitivity:** The sensitivity of the model to hyperparameter choices is not adequately discussed, which is crucial for practitioners looking to apply the method in varied contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, with a logical flow that facilitates understanding. The novelty of the proposed hybrid framework is a significant contribution to the field, though the reproducibility could be enhanced by providing more details on experimental setups and hyperparameter configurations. Overall, the quality of the writing and presentation is high, making it accessible to both experts and those new to the topic.\n\n# Summary Of The Review\nThis paper offers a noteworthy contribution to enhancing model robustness against adversarial attacks through a novel hybrid approach. While the theoretical insights and empirical results are strong, the paper would benefit from broader experimental validation and clearer comparative analyses to solidify its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Variational Imbalanced Regression (VIR), a novel framework designed to improve accuracy and uncertainty estimation in regression tasks with imbalanced label distributions. Unlike traditional models that treat data points independently, VIR modifies latent representations by leveraging similar regression labels, allowing for a more comprehensive understanding of data distributions. The paper demonstrates that VIR predicts full normal-inverse-gamma distributions and utilizes probabilistic reweighting, significantly outperforming existing state-of-the-art models in imbalanced regression tasks according to experimental results.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing both accuracy and uncertainty in imbalanced regression scenarios, filling a notable gap in the existing literature. The methodology is well-conceived, as it reframes the problem through a probabilistic lens, which is particularly beneficial for handling imbalanced datasets. However, a potential weakness is the need for clarity regarding the computational complexity and efficiency of the proposed method compared to traditional models, which may affect its practical applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clear and well-structured, effectively communicating the motivation, methodology, and findings. The quality of writing is high, although additional details regarding the implementation and specific experimental setups could enhance reproducibility. The novelty of the approach is significant, as it integrates uncertainty estimation into the framework of imbalanced regression, which has been largely overlooked in prior research.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to imbalanced regression that effectively integrates uncertainty estimation, marking a significant advancement in the field. While the methodology shows great promise, further clarification of computational aspects could strengthen its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel approach called Variational Imbalanced Regression (VIR), designed to address the challenges posed by imbalanced datasets in regression tasks. The methodology involves predicting entire normal-inverse-gamma distributions rather than point estimates, thus enabling improved accuracy and uncertainty estimation for minority data. The model is built on the assumption of Neighboring and Identically Distributed (N.I.D.) representations and incorporates probabilistic reweighting through conjugate distributions. Empirical evaluations on real-world datasets, such as AgeDB and IMDB-WIKI, demonstrate that VIR outperforms existing state-of-the-art models in both accuracy and uncertainty estimation metrics.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to combining regression accuracy with uncertainty estimation, as well as its clear identification of a significant gap in existing methodologies. The use of N.I.D. representations is a notable advancement over traditional I.I.D. assumptions commonly found in similar models. However, a weakness lies in the potential complexity of the model's architecture, which may hinder reproducibility and understanding for practitioners unfamiliar with variational methods. Additionally, the paper could benefit from a deeper discussion of the computational efficiency and scalability of the VIR model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making a compelling case for the proposed VIR model. The quality of the writing is high, with clear definitions and explanations of key concepts. The novelty of the approach is significant, particularly in the context of imbalanced regression. However, the reproducibility of the results could be enhanced by providing more details on the implementation and hyperparameter settings used in the experiments. The inclusion of code or supplementary materials would also aid in this regard.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to addressing the limitations of traditional regression models in imbalanced datasets. While the contributions are significant and the methodology is sound, there are areas for improvement concerning clarity in implementation details and potential computational complexities.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Variational Imbalanced Regression\" addresses the challenges posed by imbalanced label distributions in regression tasks. It introduces a novel model, Variational Imbalanced Regression (VIR), which leverages variational inference to improve both accuracy for minority data and uncertainty estimation. Key contributions include the identification of probabilistic deep imbalanced regression issues, the proposal of the VIR model, and the introduction of Neighboring and Identically Distributed (N.I.D.) representations. The authors demonstrate the effectiveness of their method through extensive experiments on datasets including AgeDB, IMDB-WIKI, and STS-B, reporting superior performance compared to existing regression and uncertainty estimation techniques.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to combining accuracy and uncertainty estimation in the context of imbalanced regression, which is a critical issue in many real-world applications. The detailed methodology and clear experimental setup provide a solid foundation for the proposed model. However, a notable weakness is the lack of discussion regarding the limitations of the VIR model, which could provide a more balanced perspective on its applicability. Additionally, while the results are promising, further validation on more diverse datasets may strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulate, making it easy to follow the proposed methodology and findings. The use of figures and tables effectively supports the text, enhancing clarity. The novelty of the approach is significant, particularly in its integration of uncertainty estimation with regression under imbalanced conditions. The authors provide sufficient details regarding implementation and evaluation, ensuring reproducibility of their results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of regression in the presence of imbalanced data. The proposed VIR model is innovative and shows potential based on the empirical results provided. However, addressing the limitations of the model and validating its performance on a wider range of datasets would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Variational Imbalanced Regression (VIR), a novel probabilistic architecture aimed at overcoming limitations in predictive accuracy and uncertainty quantification in regression tasks characterized by imbalanced label distributions. VIR operates under the Neighboring and Identically Distributed (N.I.D.) paradigm, in contrast to conventional variational autoencoders (VAEs), which rely on independent and identically distributed (I.I.D.) assumptions. By leveraging normal-inverse-gamma (NIG) distributions for probabilistic output and integrating neighboring data statistics, VIR effectively addresses the shortcomings of existing imbalanced regression methods. Empirical evaluations on datasets such as AgeDB-DIR and IMDB-WIKI-DIR demonstrate that VIR significantly outperforms state-of-the-art methodologies in both accuracy and uncertainty estimation.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing imbalanced regression through a probabilistic framework that enhances both predictive accuracy and uncertainty quantification. The introduction of the N.I.D. paradigm represents a significant advancement over traditional methods, allowing for better handling of minority class data. However, a potential weakness is the complexity of the model, which may limit its applicability in real-time scenarios where computational efficiency is critical. Additionally, while the empirical results are promising, further validation across a broader range of datasets could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings. The novelty of the approach is evident, particularly in the transition from I.I.D. to N.I.D. assumptions, which is a significant contribution to the field. The methodology is described in sufficient detail, facilitating reproducibility; however, the model's complexity may pose challenges for practitioners seeking to implement it. Overall, the quality of writing and presentation is high, making the concepts accessible to readers with a background in the subject.\n\n# Summary Of The Review\nThe paper presents a compelling advancement in imbalanced regression through the introduction of Variational Imbalanced Regression (VIR), which effectively combines predictive accuracy and uncertainty estimation. While the methodology is innovative and the results are robust, its complexity may hinder real-world applicability. Overall, the contributions are significant, representing a valuable addition to the literature on regression methodologies.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new model called Variational Imbalanced Regression (VIR), which aims to improve both accuracy and uncertainty estimation in regression tasks. The methodology relies on the assumption of Neighboring and Identically Distributed (N.I.D.) data and incorporates complex components such as probabilistic representations and Normal Inverse Gaussian (NIG) distributions. The authors claim that VIR outperforms state-of-the-art models, although the empirical results are based on selective comparisons with limited justification regarding the choice of datasets and hyperparameters.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to address the dual challenge of accuracy and uncertainty estimation in regression. However, the reliance on N.I.D. data raises concerns about the applicability of the model in real-world scenarios, where such assumptions may not hold. The complexity introduced by new components may lead to overfitting, particularly in datasets with limited samples. Furthermore, the lack of a clear breakdown of performance improvements creates ambiguity about the significance of the findings, while the selective nature of the comparisons undermines the robustness of the claims. Additionally, the paper's heavy reliance on empirical results without sufficient theoretical justification raises concerns about the reproducibility and validity of the proposed methodology.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's writing is overly technical and convoluted, which may hinder readers from fully grasping the contributions and implications of the research. The novelty of the proposed approach is somewhat diluted by its complexity and the lack of a thorough discussion on the limitations of competing baselines. The reproducibility of the findings is questionable due to insufficient justification of hyperparameters and dataset selection, which may limit the practical applicability of the model in diverse settings.\n\n# Summary Of The Review\nOverall, while the proposed Variational Imbalanced Regression (VIR) model presents an innovative approach to addressing regression tasks, it suffers from significant methodological and clarity issues. The reliance on strong assumptions and the complexity of the approach may hinder its practical applicability and generalizability, raising concerns about the validity of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Variational Imbalanced Regression (VIR), a novel approach that addresses the challenge of imbalanced regression in machine learning. The key contributions include a probabilistic deep learning model that merges accuracy and uncertainty estimation, alongside an innovative architecture that employs the Neighboring and Identically Distributed (N.I.D.) assumption to enhance representation learning for minority data. Extensive experiments demonstrate that VIR outperforms existing imbalanced regression models, achieving significant accuracy improvements and providing robust uncertainty measures, particularly in few-shot scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its dual focus on accuracy and uncertainty estimation, filling a notable gap in the literature. The model's state-of-the-art performance on real-world datasets, such as AgeDB and IMDB-WIKI, demonstrates its effectiveness, making it a substantial contribution to the field. However, the paper could benefit from a more detailed discussion of the limitations of the N.I.D. assumption and its implications in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, making it accessible to a broad audience. The novelty of combining deep learning with probabilistic modeling for imbalanced regression is significant, and the benchmarks provided for uncertainty estimation pave the way for future research. While the methodology appears reproducible, the paper could enhance reproducibility by providing more details on the experimental setup and datasets used.\n\n# Summary Of The Review\nOverall, the paper presents a groundbreaking approach to imbalanced regression through the VIR model, demonstrating its superiority in accuracy and uncertainty estimation. While the contributions are substantial and the methodology is innovative, further exploration of the N.I.D. assumption and detailed experimental setups would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Variational Imbalanced Regression (VIR), a novel probabilistic framework designed to address the challenges of imbalanced regression settings. It shifts from traditional independent and identically distributed (I.I.D.) assumptions to a Neighboring and Identically Distributed (N.I.D.) approach, which leverages neighboring data points to enhance model robustness. Key contributions include the development of a methodology for predicting normal-inverse-gamma (NIG) distributions, effectively quantifying uncertainty in predictions, and a thorough analysis of the bias-variance trade-off in imbalanced contexts, demonstrating improved generalization capabilities over existing methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative theoretical contributions and the practical implications of employing a probabilistic framework for imbalanced regression. The introduction of N.I.D. assumptions provides a fresh perspective on data representation by borrowing strength from neighboring observations. However, the paper could benefit from more extensive empirical validation to substantiate its theoretical claims, particularly regarding the effectiveness of the proposed model compared to existing approaches in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical concepts clearly, making complex ideas accessible. The quality of the writing is high, with appropriate use of technical language and formalism. The novelty of the approach is significant, as it challenges traditional assumptions in regression modeling. However, reproducibility may be hindered by a lack of detailed experimental setups and datasets used for validation, which should be addressed to facilitate further research and application of the VIR framework.\n\n# Summary Of The Review\nOverall, this paper presents a compelling theoretical framework for addressing imbalanced regression through the VIR model, offering valuable insights into uncertainty quantification and generalization error analysis. While the theoretical contributions are noteworthy, the empirical validation is somewhat lacking, which could limit the practical applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents Variational Imbalanced Regression (VIR), a novel probabilistic deep learning model designed to enhance performance in imbalanced regression tasks while providing reliable uncertainty estimation. The methodology involves an encoder that generates probabilistic representations of data points using Gaussian distributions and a predictor that outputs parameters of a normal-inverse-gamma distribution, incorporating probabilistic reweighting based on smoothed label distributions. The authors validate VIR on three datasets—AgeDB-DIR, IMDB-WIKI-DIR, and STS-B-DIR—demonstrating significant improvements in accuracy and uncertainty estimation compared to baseline methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of the challenges associated with existing deep regression models on imbalanced datasets and the innovative use of neighboring data points to enhance representation for minority classes. The framework's compatibility with various deep regression models is also a notable advantage, allowing for flexible implementation. However, a weakness lies in the limited discussion of broader implications and the theoretical underpinnings of the proposed model, which could enrich the contribution. Additionally, while empirical results are strong, the paper lacks extensive exploration of the model's performance in scenarios with highly limited samples, which could be a practical consideration in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and findings clearly. The quality of writing is high, with well-defined terms and concepts that facilitate understanding. The novelty of the approach is evident in the integration of probabilistic techniques within the regression framework, although similar ideas have been explored in related domains. The reproducibility of the results is supported by the availability of code in PyTorch and the detailed description of experimental setups, including hyperparameter optimization methods.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of imbalanced regression through the introduction of the VIR framework, which improves accuracy and uncertainty estimation. While the empirical results are compelling, the paper could benefit from a more comprehensive theoretical discussion and exploration of edge cases in data availability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Variational Imbalanced Regression (VIR) model, which claims to enhance performance in imbalanced regression tasks by integrating uncertainty estimation and addressing Neighboring and Identically Distributed (N.I.D.) data. The authors assert that VIR outperforms existing methods such as DIR and DER while providing a new benchmark for uncertainty estimation in this domain. However, the claims are supported by experimental results that lack comprehensive contextual information, making it difficult to assess the actual improvements offered by VIR compared to other models.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambitious goal of combining accuracy with uncertainty estimation in regression tasks, as well as its novel approach to modeling imbalanced data. However, the weaknesses are pronounced; the authors do not sufficiently engage with existing literature, particularly regarding the strengths of competing models like DIR and DER. Additionally, the comparisons made with traditional methods appear biased, and claims of establishing new benchmarks are inflated due to a lack of thorough evaluation against previous work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hindered by its failure to adequately contextualize its contributions within the broader literature, leading to potential misinterpretations of its novelty. The methodology is presented as novel, yet it lacks critical engagement with prior work exploring similar probabilistic frameworks. The reproducibility of the results is questionable, as the experimental conditions under which the comparisons were made are not well-defined, limiting the ability to replicate the findings.\n\n# Summary Of The Review\nOverall, while the VIR model presents an interesting approach to imbalanced regression with a focus on uncertainty estimation, it suffers from a lack of engagement with existing literature and inflated claims regarding its contributions. The experimental validation is limited in scope, making it challenging to fully assess the model's effectiveness across diverse datasets.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "**Under review as a conference paper at ICLR 2023**\n\n# Summary Of The Paper\nThe paper presents a novel approach to imbalanced regression through a method termed Variational Imbalanced Regression (VIR). The authors propose a probabilistic deep learning framework that incorporates a normal-inverse-gamma distribution to model the output uncertainty and address the challenges associated with imbalanced datasets, particularly for minority classes. The methodology involves the computation of importance weights derived from a smoothed label distribution and the optimization of negative log likelihood (NLL) for improved regression accuracy. Experimental results demonstrate that VIR outperforms existing state-of-the-art methods on several real-world datasets, including AgeDB-DIR.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to handling imbalanced data, highlighting a significant gap in the current literature. The methodology is well-founded in probabilistic principles, and the use of variational inference adds depth to the analysis. However, weaknesses include a lack of clarity in some sections, particularly regarding the definitions and implications of key concepts like \"latent representation\" and \"importance weights.\" Additionally, while the empirical results are promising, the paper could benefit from a more comprehensive discussion on the datasets used and potential limitations of the VIR approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper’s clarity is somewhat hindered by inconsistent terminology and notation. For instance, the introduction of acronyms such as \"N.I.D.\" and terms like \"probabilistic deep imbalanced regression model\" could be streamlined for better readability. The quality of the writing is generally good but requires attention to detail regarding formatting and consistency. The novelty of the proposed method is significant, particularly in its application to regression tasks, though the reproducibility of results may be impacted by the lack of detail in the experimental setup.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of regression under imbalanced conditions, with a novel approach that shows strong empirical performance. However, improvements in clarity and detail are necessary to enhance the paper's overall impact and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel regression model, referred to as VIR, aimed at addressing the challenges posed by imbalanced datasets and enhancing uncertainty estimation. The authors utilize a probabilistic framework to develop their model, presenting experimental results that demonstrate its efficacy across select datasets. However, the paper lacks an exploration of broader applications, such as multi-task learning or transfer learning, and does not adequately address practical implications for critical fields like healthcare or autonomous systems.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its introduction of the VIR model, which offers a new perspective on uncertainty estimation in imbalanced regression contexts. However, the paper has notable weaknesses, including a limited discussion on the model's applicability to other domains, a lack of comparative analysis with existing uncertainty quantification methods, and insufficient exploration of the model's computational efficiency. Additionally, the authors do not thoroughly address the limitations and assumptions of their model, which could impact its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, although certain sections could benefit from more detailed explanations, particularly regarding the implications of the VIR model in real-world applications. The novelty of the approach is commendable, but the lack of comprehensive comparisons and discussions around its limitations raises concerns about its reproducibility and practical deployment. Future work suggestions are present but lack specificity, which diminishes the paper's forward-looking significance.\n\n# Summary Of The Review\nOverall, while the paper presents a novel approach to addressing imbalanced regression and uncertainty estimation, it falls short in exploring the broader implications and applications of its findings. The absence of thorough discussions on limitations and comparisons with existing models limits the impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Variational Imbalanced Regression (VIR), a novel model designed to address the challenges of imbalanced regression tasks by effectively estimating both accuracy and uncertainty. The methodology leverages probabilistic modeling, specifically variational inference, to compute latent representations as Gaussian distributions rather than point estimates. Key contributions include the formulation of the Evidence Lower Bound (ELBO) for optimization, the use of Negative Log Likelihood (NLL) for uncertainty estimation, and a regularization term to balance accuracy and uncertainty. Empirical results demonstrate that VIR outperforms existing state-of-the-art models across several datasets while providing robust uncertainty estimates.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to handling imbalanced datasets through the N.I.D. assumption, which allows for more nuanced probabilistic representations. The use of variational inference and the incorporation of uncertainty into the regression framework are significant advancements. However, the paper's weaknesses lie in its lack of rigorous statistical validation for its claims; it does not sufficiently detail the statistical tests employed to confirm the significance of performance improvements. Additionally, the proposed methods would benefit from clearer exposition regarding the hyperparameter tuning process and its impact on model performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-structured sections that outline the methodology and results. However, some areas could be improved by providing more detailed explanations of the statistical tests used to validate the results. The quality of the writing is acceptable, but the reproducibility of the experiments is compromised due to the absence of detailed descriptions of the datasets and the experimental setup. The novelty of the approach is notable, particularly in the context of imbalanced regression, yet further elucidation of the implications of the findings would enhance its impact.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of imbalanced regression through the introduction of a novel probabilistic model. While it demonstrates significant improvements in accuracy and uncertainty estimation, the lack of comprehensive statistical validation and reproducibility raises concerns about the robustness of its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel Variational Inference Regression (VIR) model aimed at addressing uncertainty estimation in regression tasks, particularly in scenarios involving imbalanced datasets. The authors propose a methodology that leverages the Neighboring and Identically Distributed (N.I.D.) assumption to facilitate uncertainty quantification. Empirical results demonstrate some improvements over existing models in terms of predictive accuracy and uncertainty estimation. However, the paper does not sufficiently explore the scalability, computational efficiency, or robustness of the model under various challenging conditions.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to uncertainty estimation in regression, which is a critical aspect of machine learning applications. The use of the N.I.D. assumption provides a unique angle for tackling regression tasks with imbalanced data. However, the paper exhibits several weaknesses: it lacks a thorough comparative analysis against a broader set of existing models and does not address scalability issues or computational efficiency. Additionally, the exploration of the model's robustness, particularly in highly imbalanced datasets and under data sparsity, is insufficient. There is also a notable absence of discussion regarding the impact of model hyperparameters, kernel choices for smoothing, and the influence of outliers on the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's writing is generally clear, but certain sections would benefit from more in-depth discussions regarding methodological choices and their implications. The novelty of the approach is commendable, yet it is somewhat undermined by the lack of comprehensive empirical validation across varied scenarios. The reproducibility of the results may be compromised due to the limited detail on experimental setups and hyperparameter settings, which are crucial for replicating the findings.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting approach to uncertainty estimation in regression tasks, its contributions are overshadowed by significant limitations regarding scalability, robustness, and empirical validation. The lack of thorough analysis and discussion in key areas raises concerns about the practical applicability of the proposed VIR model.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper addresses the issue of imbalanced regression through a proposed method termed \"Variational Imbalanced Regression\" (VIR). The authors claim to enhance the performance of minority class predictions by introducing a probabilistic deep learning framework. Their methodology includes a novel distinction between Identically and Neighboring Distributed (I.I.D. and N.I.D.) data, along with a process they refer to as \"probabilistic whitening and recoloring.\" The experimental results claim to outperform existing state-of-the-art methods using standard metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Geometric Mean (GM).\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its attempt to address a well-known problem in machine learning—imbalanced datasets—by proposing a probabilistic framework. However, the weaknesses are significant: the paper lacks originality, as many of the concepts discussed are not new and have been explored in previous literature. The criticism of existing methods appears superficial, lacking depth in analysis. Furthermore, the experimental comparisons may not be robust, as they seem to selectively highlight favorable results without adequately addressing limitations or providing comprehensive benchmarks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear but suffers from a lack of depth and substantive novelty. While the writing is coherent, it often resorts to jargon and terms that may obscure rather than clarify the proposed methods. The reproducibility of the results is questionable due to the potential cherry-picking of experimental comparisons and the absence of thorough validation against a comprehensive set of benchmarks. Overall, the quality of the contributions does not meet the expectations set by the claims made in the introduction.\n\n# Summary Of The Review\nIn summary, the paper attempts to tackle the challenge of imbalanced regression with a probabilistic approach. However, it lacks originality and depth, presenting established concepts under new terminology without providing significant empirical advancements or insights. The overall contribution to the field appears limited, and the methodology does not offer the novelty that the authors seem to claim.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the Variational Imbalanced Regression (VIR) framework, which effectively combines uncertainty estimation with regression performance in the context of imbalanced datasets. The methodology includes the application of a normal-inverse-gamma distribution for output prediction and a probabilistic reweighting mechanism to address class imbalance. Empirical evaluations on AgeDB and IMDB-WIKI datasets demonstrate that VIR surpasses existing methods, particularly in few-shot scenarios, while also providing promising results in terms of uncertainty estimation metrics such as negative log-likelihood (NLL) and average uncertainty score (AUSE).\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to integrating uncertainty into regression tasks, addressing a critical gap in handling imbalanced datasets. The use of probabilistic reweighting is a notable contribution that enhances the model's robustness. However, the paper could benefit from exploring alternative probabilistic modeling techniques, such as mixture density networks or Gaussian processes, which might provide greater flexibility. Additionally, the evaluation could be strengthened by testing on a broader array of datasets to assess the generalizability of the VIR framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, presenting its contributions and findings in a coherent manner. The quality of the experiments is solid, although the paper lacks a detailed exploration of various hyperparameter settings and model architectures, which may affect reproducibility. The novelty of the proposed framework is commendable, but there is room for improvement in the depth of the empirical evaluations and the exploration of additional uncertainty metrics for a more robust assessment.\n\n# Summary Of The Review\nThe Variational Imbalanced Regression framework presents a significant advancement in handling uncertainty in regression tasks involving imbalanced datasets. While the paper demonstrates promising results and introduces valuable methodologies, further exploration of alternative modeling techniques and broader empirical evaluations would enhance its impact and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel approach named Variational Imbalanced Regression (VIR) aimed at addressing challenges in imbalanced regression tasks. The methodology involves leveraging variational inference to enhance prediction accuracy while effectively managing uncertainty. The findings demonstrate that VIR consistently outperforms state-of-the-art models across multiple datasets, including AgeDB-DIR and IMDB-WIKI-DIR, achieving lower Mean Squared Error (MSE) and better uncertainty metrics. Notably, VIR excels in few-shot scenarios, showcasing its robustness and significant improvements over existing methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive benchmarking against various state-of-the-art models, demonstrating clear advancements in both accuracy and uncertainty estimation. The extensive evaluation across multiple datasets adds credibility to the findings, while the ablation studies further validate the model's robustness. However, a potential weakness is the lack of detailed theoretical analysis of the proposed methodology, which could enhance understanding of the underlying principles governing its performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, experimental setup, and results. The quality of the writing is high, making complex concepts accessible. The novelty of the approach is significant, particularly in its application to imbalanced regression tasks. While the empirical results are compelling, the reproducibility could be improved by providing more detailed descriptions of the experimental setup and hyperparameter tuning processes.\n\n# Summary Of The Review\nOverall, the paper presents a substantial contribution to the field of imbalanced regression through the introduction of the VIR model, which demonstrates superior performance in both accuracy and uncertainty management. The extensive evaluation across multiple datasets reinforces its relevance, though further theoretical insights would strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to imbalanced regression and uncertainty estimation through the application of normal-inverse-gamma distributions and probabilistic reweighting techniques. The authors propose a methodology that aims to enhance the performance of regression models in scenarios where data is skewed, offering both theoretical foundations and empirical validations. Key findings suggest that the proposed method outperforms existing techniques in various benchmarks, particularly in terms of predictive accuracy and uncertainty quantification.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to addressing the challenges of imbalanced data in regression tasks, as well as its thorough empirical evaluations that demonstrate its effectiveness. However, there are notable weaknesses, including the dense and complex writing style that may obscure key points, as well as inconsistent formatting and clarity issues throughout the text. The introduction and conclusion sections could benefit from a more concise presentation of findings to enhance reader comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the technical content of the paper is robust, the clarity suffers due to overly complex sentence structures and the frequent use of jargon without sufficient explanation. The quality of writing could be improved by simplifying language and streamlining sections for better navigability. The novelty of the proposed method is significant, as it introduces new probabilistic techniques to regression, but the lack of clear transitions and repetitive phrasing impacts the overall readability. Reproducibility is addressed through empirical tests, but clearer guidelines and consistent formatting would bolster this aspect further.\n\n# Summary Of The Review\nOverall, the paper introduces a significant contribution to the field of imbalanced regression and uncertainty estimation, yet it is hindered by clarity and consistency issues. Improving the writing quality and structure would greatly enhance the accessibility and impact of the findings presented.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.059313638586521,
    -1.6300918267080153,
    -1.7828587144996493,
    -1.7134500925073208,
    -1.7842414188419462,
    -1.9339147372871677,
    -1.5600224517973456,
    -1.87186868806868,
    -1.9523808913752976,
    -1.816960622348432,
    -1.6555973289112054,
    -1.3847948337456055,
    -1.8150868675637388,
    -1.704853948823806,
    -1.6995722711301184,
    -1.6006140112274916,
    -1.7103411032534575,
    -1.6954539317134034,
    -1.7494591621331637,
    -1.8438146899726797,
    -2.002487506684939,
    -1.7069063886949571,
    -1.770744896585252,
    -1.6951591581849568,
    -1.692435102402599,
    -1.8666201823362172,
    -1.7526580232438735,
    -1.6402086954985198,
    -1.7320162290660757
  ],
  "logp_cond": [
    [
      0.0,
      -1.9519239357791007,
      -1.9675430103662197,
      -1.9592108534325416,
      -1.9707940369761494,
      -1.9714096373980103,
      -1.9896544149708366,
      -1.9701656671759709,
      -1.9647507731417466,
      -1.980709475664699,
      -1.9680298925741802,
      -1.9936271773869025,
      -1.9733823131760448,
      -1.9675341242949562,
      -1.9714307153533424,
      -1.97311369427871,
      -1.9632822934360976,
      -1.9778399728653588,
      -1.983321158423545,
      -1.9494807485094676,
      -1.9732419281806546,
      -1.9545013942403269,
      -1.9532843033860514,
      -1.9726059449384061,
      -1.9604681435412135,
      -1.9881836257059013,
      -1.9678588929605652,
      -1.966149982455397,
      -1.9762543556162693
    ],
    [
      -1.2809531968845003,
      0.0,
      -1.1227574908657831,
      -1.1213838779912015,
      -1.1612619556352646,
      -1.1470462364298257,
      -1.3042077584671319,
      -1.2249260422735087,
      -1.1230907930332736,
      -1.2579407898078774,
      -1.0767407786062848,
      -1.3431573249367446,
      -1.1539570092085807,
      -1.049779841911771,
      -1.1501879252900742,
      -1.128101171273342,
      -1.1688332694894612,
      -1.1571411013301671,
      -1.1294685545107181,
      -1.1762629108728062,
      -1.1730284516058906,
      -1.245394381114032,
      -1.2762921234665572,
      -1.1441113655545048,
      -1.2028721437652317,
      -1.2392160019102627,
      -1.2171203310594727,
      -1.2546037534274372,
      -1.3241985197969994
    ],
    [
      -1.5247323243900195,
      -1.3583981088262924,
      0.0,
      -1.2955531720838782,
      -1.3886807978807894,
      -1.3419470457645875,
      -1.5031496804172806,
      -1.4610570856546208,
      -1.3674848860280042,
      -1.430130888000979,
      -1.3643649028015008,
      -1.5455311658423323,
      -1.3883128024155575,
      -1.3001413010796723,
      -1.3346268200999445,
      -1.32192695988286,
      -1.4123936072513266,
      -1.404872108076116,
      -1.3915815792375097,
      -1.331908564850356,
      -1.4273829477311641,
      -1.455925384656409,
      -1.4837209916189418,
      -1.4400148681788267,
      -1.4686004402766495,
      -1.460519478119332,
      -1.414264568711634,
      -1.4537658426670668,
      -1.529235375373658
    ],
    [
      -1.3988410906926292,
      -1.244551649119126,
      -1.1968274155361753,
      0.0,
      -1.3188637713153333,
      -1.2500512503558654,
      -1.4155748978802505,
      -1.351446489693618,
      -1.2902317525164881,
      -1.3512502692764388,
      -1.2584941668115186,
      -1.4405489501078077,
      -1.2672413830601281,
      -1.1377358422034238,
      -1.193708244241191,
      -1.2758966337997681,
      -1.35222571951754,
      -1.266804800766515,
      -1.2890566252118199,
      -1.25418650158421,
      -1.3247213415327546,
      -1.3706842114819553,
      -1.3802369382685336,
      -1.3024727135032421,
      -1.3962931572763198,
      -1.3868742407520145,
      -1.2525962108418955,
      -1.3460092669070487,
      -1.3864819878475043
    ],
    [
      -1.4899195975780368,
      -1.325537400567243,
      -1.3544500150321919,
      -1.4176839456868433,
      0.0,
      -1.3195374387473717,
      -1.4399018576214733,
      -1.4429412448060626,
      -1.3615093428763025,
      -1.4150347884306012,
      -1.3548201282724053,
      -1.4980211849128746,
      -1.4238974449146284,
      -1.3467098328885638,
      -1.3523010478567903,
      -1.398338904796084,
      -1.2976538919568177,
      -1.3431411576200747,
      -1.414059859929725,
      -1.4530341488134975,
      -1.374596054599088,
      -1.4722597446419172,
      -1.4500360904460752,
      -1.3772548069972022,
      -1.368108437084288,
      -1.4037933749081466,
      -1.4467461287431813,
      -1.4602054675574487,
      -1.4939767223460152
    ],
    [
      -1.581211911019416,
      -1.4566332560245174,
      -1.4055824851087129,
      -1.4533960502271077,
      -1.4911899737692196,
      0.0,
      -1.6162579350293886,
      -1.5156689835982706,
      -1.354094746699329,
      -1.552858262803068,
      -1.376917207827405,
      -1.6571278839094705,
      -1.5064054647045328,
      -1.378344772907125,
      -1.4339317586832594,
      -1.4191797753436761,
      -1.4645311702565398,
      -1.4366969521690203,
      -1.4942488845306796,
      -1.5002577901098546,
      -1.4759177931328133,
      -1.5286060705072635,
      -1.5954626141713018,
      -1.4919894981768413,
      -1.528904368370137,
      -1.4783417886407175,
      -1.5129271897240957,
      -1.5446718692380823,
      -1.6308300244345306
    ],
    [
      -1.257408030261182,
      -1.1854836890979354,
      -1.1975814373110578,
      -1.2329657421949023,
      -1.1748230422318813,
      -1.1979586959660422,
      0.0,
      -1.21587436146102,
      -1.2006138684484626,
      -1.206079491940731,
      -1.2028363689814974,
      -1.193936731217973,
      -1.208451432656979,
      -1.234942307928668,
      -1.2006098103699754,
      -1.228565100289976,
      -1.19817964819485,
      -1.2197300522412113,
      -1.2398266065620416,
      -1.1858959968532878,
      -1.2028193226763475,
      -1.19535792947924,
      -1.2338437786135295,
      -1.215944218512127,
      -1.2040042668999424,
      -1.2213764428089682,
      -1.2214558356674963,
      -1.205974003551548,
      -1.2413829673382557
    ],
    [
      -1.6271621957983697,
      -1.446452599367839,
      -1.4248078254593275,
      -1.4357291359613038,
      -1.4481412518054133,
      -1.4514403642894114,
      -1.5701285379061654,
      0.0,
      -1.428946737762784,
      -1.5371289059576851,
      -1.4131506829070521,
      -1.6014097616187744,
      -1.4038446782559095,
      -1.5170800306536936,
      -1.4581664747893346,
      -1.4341700054501967,
      -1.4789747641339093,
      -1.463281679940928,
      -1.520252310602654,
      -1.5058764491494194,
      -1.4633361047787252,
      -1.544108790726686,
      -1.5384985165128646,
      -1.465078661962141,
      -1.537343770888066,
      -1.5886471870431464,
      -1.5321462671488844,
      -1.5196207230672691,
      -1.5579660732288318
    ],
    [
      -1.5830143147193758,
      -1.4636487216918885,
      -1.4045777372054404,
      -1.439772081645627,
      -1.5034658656928592,
      -1.2965522352186964,
      -1.6173918588065586,
      -1.5110876271107656,
      0.0,
      -1.557028072379014,
      -1.400965359764498,
      -1.6696500382788486,
      -1.5284280682834879,
      -1.4255586594635232,
      -1.4284873646639065,
      -1.469591152111015,
      -1.516358371635895,
      -1.463571330849132,
      -1.5067597358403482,
      -1.4640450014897934,
      -1.4873740751302962,
      -1.4961969299041726,
      -1.6083772517520598,
      -1.5116269165289322,
      -1.5770400982910022,
      -1.532474133493969,
      -1.5060766456412271,
      -1.517944067395751,
      -1.5998332212887083
    ],
    [
      -1.5319568163426232,
      -1.4055570440981657,
      -1.3736996006478313,
      -1.394815503288955,
      -1.4334102374187963,
      -1.4005254713332682,
      -1.5232604543381298,
      -1.4435759644680135,
      -1.4209911537070257,
      0.0,
      -1.3801954767291043,
      -1.5216257751900577,
      -1.4057046978259236,
      -1.345856069241534,
      -1.4107075459516412,
      -1.3887036828192545,
      -1.3620127400177882,
      -1.4257936675510194,
      -1.4374824396324049,
      -1.4039518417282177,
      -1.418797313085153,
      -1.4632745658736812,
      -1.4877114986124786,
      -1.4123471678381654,
      -1.3977797113804489,
      -1.453940039786648,
      -1.4964063817804552,
      -1.4924620841623828,
      -1.5330115970505844
    ],
    [
      -1.4032195523896904,
      -1.175075129068909,
      -1.2188942839856463,
      -1.2356019463450039,
      -1.2474324465884552,
      -1.1382319139347903,
      -1.3993064319951218,
      -1.3160706540054385,
      -1.1709634070849524,
      -1.309292257419807,
      0.0,
      -1.404689189971912,
      -1.2323046500443022,
      -1.1806655095853273,
      -1.1789179287102824,
      -1.2250678285096432,
      -1.2536410135216158,
      -1.197991355009873,
      -1.270373940908563,
      -1.2709437948222708,
      -1.2130414353871053,
      -1.3395034888645048,
      -1.3814287262336133,
      -1.2569985224889604,
      -1.3046281709220975,
      -1.330540416397193,
      -1.3000668310998682,
      -1.313682211827999,
      -1.411977356013538
    ],
    [
      -1.1057522037536984,
      -1.084235051590818,
      -1.1207174545349927,
      -1.131347911309476,
      -1.0988031964935716,
      -1.120784823172951,
      -1.1124950437039423,
      -1.128657216043058,
      -1.0976965398050635,
      -1.061531363677898,
      -1.0848407929398665,
      0.0,
      -1.120139559592352,
      -1.1255302581564193,
      -1.1071074197940467,
      -1.121567096786347,
      -1.0936496857713975,
      -1.1296847664132315,
      -1.118990107392035,
      -1.0983158813330398,
      -1.1110452023198467,
      -1.1178365678006927,
      -1.0995965223040436,
      -1.0880822911939083,
      -1.0889971527131002,
      -1.11721913301902,
      -1.1109276153410137,
      -1.089088506543221,
      -1.1052933450756912
    ],
    [
      -1.4556545701517252,
      -1.2332102631905697,
      -1.2627543319126486,
      -1.2519365911398332,
      -1.3600717514195975,
      -1.2698898243831906,
      -1.4592734569106578,
      -1.313635349694512,
      -1.30134654692374,
      -1.4180237972858931,
      -1.2018485249671753,
      -1.4944989551373085,
      0.0,
      -1.2543301685070078,
      -1.349360494937246,
      -1.308190135935766,
      -1.3861788438269775,
      -1.3545245566450732,
      -1.3228043089581154,
      -1.323971911798953,
      -1.3430377260870245,
      -1.4071757455245795,
      -1.4148647240812944,
      -1.3141289532722193,
      -1.4398137526195653,
      -1.4303042096429799,
      -1.3037655082226265,
      -1.3783237548432834,
      -1.453201280330616
    ],
    [
      -1.3642964589602296,
      -1.0433180327207923,
      -1.074831054586192,
      -1.0546797470324938,
      -1.211838025883686,
      -1.1124800412091032,
      -1.4569678050326897,
      -1.2908050843934282,
      -1.1956793629726574,
      -1.2777219997708766,
      -1.1205459042219779,
      -1.4358440821618297,
      -1.2042029196271136,
      0.0,
      -1.1413077023001712,
      -1.1438399080259345,
      -1.240060005017601,
      -1.177963364609675,
      -1.2132452935848028,
      -1.2112680494357937,
      -1.216619419922729,
      -1.2872009786481402,
      -1.3276814442005767,
      -1.2358525288755668,
      -1.3005797305388103,
      -1.2575518386184394,
      -1.2215133534836577,
      -1.300245644969185,
      -1.3459010034914782
    ],
    [
      -1.3749127827936118,
      -1.1753990596165478,
      -1.1480716097506825,
      -1.1049966361014507,
      -1.1924833524032523,
      -1.1442681291619685,
      -1.3526850200831613,
      -1.2534404761023703,
      -1.1924947030735826,
      -1.2736000066262785,
      -1.136858417860061,
      -1.3989988916029448,
      -1.2415833350925807,
      -1.155795051058889,
      0.0,
      -1.1739289207715908,
      -1.1739023575755523,
      -1.1593069476849933,
      -1.2677042528473559,
      -1.2218985620460576,
      -1.1666143709698933,
      -1.2600680246138247,
      -1.3393127834764007,
      -1.2399556957186662,
      -1.2550221013809344,
      -1.2516486153356978,
      -1.2717237962038421,
      -1.297568945035485,
      -1.3548005282037063
    ],
    [
      -1.3478575398632864,
      -1.1204495463118913,
      -1.0727640878925262,
      -1.157559417151871,
      -1.1937784394046433,
      -1.1217431631802055,
      -1.3352492039713049,
      -1.2558458704695072,
      -1.1453515324220507,
      -1.2496882629626904,
      -1.1040438906385839,
      -1.3300067996299332,
      -1.2170447247359042,
      -1.1111113966579729,
      -1.1331550135427013,
      0.0,
      -1.1953875050803997,
      -1.1637664794067863,
      -1.1697814344754585,
      -1.1761969775069907,
      -1.1991409597234595,
      -1.218919539044044,
      -1.3309390877127487,
      -1.214732559257621,
      -1.2511501602020867,
      -1.275198461701625,
      -1.2238738261469004,
      -1.1961373223243514,
      -1.3090781636277526
    ],
    [
      -1.4522271678025085,
      -1.2776053129716702,
      -1.2915560155695192,
      -1.3779513711096376,
      -1.251937742877121,
      -1.2756391477886002,
      -1.4401006030792394,
      -1.3943022713424071,
      -1.3219875576546165,
      -1.3909075779808762,
      -1.2890482738738362,
      -1.4772451095894603,
      -1.3768869746186951,
      -1.321821270165282,
      -1.3239389146041265,
      -1.3170423691672042,
      0.0,
      -1.31004925162384,
      -1.3843517093452649,
      -1.366768293603592,
      -1.334040555971401,
      -1.4457311379715168,
      -1.4314831521518185,
      -1.310207269444745,
      -1.3207698490911859,
      -1.3790627668058955,
      -1.3944729926800052,
      -1.3814436828360779,
      -1.4590748925728434
    ],
    [
      -1.3622047785194271,
      -1.1454066524711028,
      -1.1854203569168387,
      -1.1265065346078742,
      -1.1554833018208572,
      -1.1026449492337767,
      -1.3875230909495293,
      -1.2775697998793636,
      -1.171098938955914,
      -1.2725696482845987,
      -1.1060998126999628,
      -1.4045348723710336,
      -1.2584374933402818,
      -1.1023495392767844,
      -1.1042377683090165,
      -1.1590810437026644,
      -1.204522855023386,
      0.0,
      -1.2107609242225572,
      -1.2441462856160805,
      -1.1274932130617696,
      -1.3236722786556494,
      -1.322520672052284,
      -1.1980748982173337,
      -1.2371081499914252,
      -1.232674386122986,
      -1.2253831604945506,
      -1.2394423762565994,
      -1.3874696974033491
    ],
    [
      -1.4305829635779808,
      -1.2311057054928618,
      -1.2968150484340046,
      -1.3457847103042333,
      -1.3525999996021245,
      -1.3112231721863135,
      -1.4442655790379393,
      -1.3547349515765237,
      -1.349947400817429,
      -1.389322729972434,
      -1.3396402275176287,
      -1.4687593442465805,
      -1.3562017944335225,
      -1.3152712304704814,
      -1.3507367862792665,
      -1.2921740679606544,
      -1.334152093200387,
      -1.323996564350019,
      0.0,
      -1.360149938973403,
      -1.3065871115748375,
      -1.3977542737154622,
      -1.412402768473768,
      -1.3260706598418652,
      -1.34492888370921,
      -1.3604257480867323,
      -1.3834292824594245,
      -1.428165958359715,
      -1.4716335225658388
    ],
    [
      -1.4955709196355391,
      -1.4315732111598465,
      -1.3316903344087683,
      -1.3782463651612678,
      -1.4658280171589533,
      -1.4143783400909011,
      -1.5394349020674611,
      -1.4874748924672754,
      -1.3633062361385997,
      -1.4444804369159328,
      -1.3706643560212768,
      -1.5671526391505144,
      -1.4382614848142616,
      -1.3710464793698172,
      -1.3906271367591692,
      -1.419262425815215,
      -1.5074818992167938,
      -1.4284352511112164,
      -1.4978218742921117,
      0.0,
      -1.4764740055502472,
      -1.390327003873065,
      -1.5203096867567611,
      -1.460343988892496,
      -1.5455673602872162,
      -1.5084171655731307,
      -1.403809689472455,
      -1.4187178783327539,
      -1.490589036082374
    ],
    [
      -1.686660395279055,
      -1.4526277560364225,
      -1.425276535329844,
      -1.526717693562671,
      -1.4299436615914127,
      -1.4426463776077545,
      -1.6451488937121206,
      -1.5522347506749974,
      -1.495024909084159,
      -1.5517574390347324,
      -1.4281940041467411,
      -1.697395259781872,
      -1.5249152985401453,
      -1.4500805816550573,
      -1.4771273158405374,
      -1.4519512687044278,
      -1.4624034655261091,
      -1.4502186997999444,
      -1.5031026245036903,
      -1.5251710182563454,
      0.0,
      -1.5976293891399702,
      -1.5675029837574712,
      -1.4844220655642073,
      -1.5263502930931026,
      -1.492209404408882,
      -1.5841267221211224,
      -1.565674649319182,
      -1.6371105960541876
    ],
    [
      -1.4318403174453394,
      -1.3234318665973577,
      -1.3152116820173299,
      -1.3430977922701866,
      -1.36085706260226,
      -1.316503145314218,
      -1.4206351697882775,
      -1.3754416053926326,
      -1.3108830806972225,
      -1.343065850431052,
      -1.3155414077268397,
      -1.4537512963149155,
      -1.3566506616306828,
      -1.265183071745195,
      -1.2946175966840672,
      -1.3241976356725103,
      -1.3713489543087298,
      -1.328803188738592,
      -1.3733541634322797,
      -1.272948998225097,
      -1.3700061155926757,
      0.0,
      -1.393892244020716,
      -1.2944820772075685,
      -1.4053606282274032,
      -1.3637219201941586,
      -1.340397850341628,
      -1.3341640696033048,
      -1.329469768518363
    ],
    [
      -1.3829802644437972,
      -1.2917921032971789,
      -1.2987307410391322,
      -1.3186257829692671,
      -1.2968880216878442,
      -1.3321309646048627,
      -1.4040519249800034,
      -1.3197312104874057,
      -1.3019865946295686,
      -1.2824668840545324,
      -1.244402763225903,
      -1.4268729246708343,
      -1.3082586192384726,
      -1.3116957441232353,
      -1.28943665904302,
      -1.3348071456843176,
      -1.2821711709500851,
      -1.321160446009497,
      -1.3296866274565418,
      -1.3288743989959344,
      -1.253562762626755,
      -1.3739819035651737,
      0.0,
      -1.2816827557944155,
      -1.242248473858046,
      -1.3528538215928023,
      -1.3130961661192948,
      -1.3376005363674488,
      -1.3361691193738197
    ],
    [
      -1.377656495941367,
      -1.2572268036665935,
      -1.268623816526364,
      -1.3259740737814292,
      -1.2379635195589322,
      -1.2692957426643539,
      -1.375222738369782,
      -1.3021876557403775,
      -1.2795525967547516,
      -1.2991804313169724,
      -1.2572725171887928,
      -1.3938080664573358,
      -1.301715642377506,
      -1.2860264380987052,
      -1.3036537029199189,
      -1.28460067674904,
      -1.2364608285872816,
      -1.2752311770705145,
      -1.2876268288799522,
      -1.3378904440717005,
      -1.2247405089844745,
      -1.304821352712535,
      -1.3275723932744479,
      0.0,
      -1.2540535030056748,
      -1.3197691837235614,
      -1.3420788459244233,
      -1.2918617965523118,
      -1.3829513487979583
    ],
    [
      -1.4032744532698134,
      -1.2941767068391552,
      -1.308232673363589,
      -1.3726525159512062,
      -1.2827311842748828,
      -1.30832371659888,
      -1.4056218866087236,
      -1.4133986811182753,
      -1.3627323277469277,
      -1.302598088621861,
      -1.2979083657369639,
      -1.4475059599161129,
      -1.3676368385662103,
      -1.3071589182582088,
      -1.3384846540584066,
      -1.3301331340109641,
      -1.2580418063561924,
      -1.319364186234988,
      -1.3322125296932348,
      -1.3856227353872086,
      -1.3393215734575956,
      -1.4639634231877188,
      -1.3498672616972902,
      -1.3290477020601645,
      0.0,
      -1.3592373488354024,
      -1.3645358050098717,
      -1.3949944820574869,
      -1.4607101470699206
    ],
    [
      -1.620048294248217,
      -1.424237878143971,
      -1.4698179796600022,
      -1.4777215092324656,
      -1.4443988851643401,
      -1.396089021018085,
      -1.5527277663907177,
      -1.5329818038785676,
      -1.4861592253515807,
      -1.4759954828986808,
      -1.4563980202957185,
      -1.5927080118193768,
      -1.5134834736038683,
      -1.428377956786654,
      -1.4038730102763683,
      -1.4330526772609546,
      -1.482728183804017,
      -1.4071376963828623,
      -1.4535098113191058,
      -1.4747012781942868,
      -1.4396250966763493,
      -1.477383594469832,
      -1.5336682390636383,
      -1.4788943902368439,
      -1.4941432430905395,
      0.0,
      -1.5095260662384802,
      -1.5084175992793116,
      -1.598099620520182
    ],
    [
      -1.4151644639658965,
      -1.348310068071947,
      -1.291860396188678,
      -1.276356315792632,
      -1.3807058690347171,
      -1.3467636675998191,
      -1.4374751927416416,
      -1.4667888959062336,
      -1.344407510089724,
      -1.4149067251613625,
      -1.3321477044608618,
      -1.4614707553684052,
      -1.350720228868985,
      -1.2653895981858878,
      -1.3565677930402273,
      -1.3483598628420275,
      -1.4097968125384197,
      -1.362318941604973,
      -1.390163388110379,
      -1.3263576628223084,
      -1.4049469548405615,
      -1.4127902798003886,
      -1.4153304946117407,
      -1.3691363056182035,
      -1.3942792301028337,
      -1.4407626610394444,
      0.0,
      -1.3404986240748358,
      -1.4603547703020312
    ],
    [
      -1.2974792591540902,
      -1.1881865211299951,
      -1.1704869246944152,
      -1.1495070012589437,
      -1.2096512504127148,
      -1.2100597899759624,
      -1.2839323138824106,
      -1.2306973079273804,
      -1.1863283063300278,
      -1.2514340056318385,
      -1.1703473024374236,
      -1.2774824641812912,
      -1.2484395824227765,
      -1.1575542863940893,
      -1.1931881455324898,
      -1.1808358026632344,
      -1.219279235404864,
      -1.1160050382032716,
      -1.241357698628841,
      -1.1236420168442653,
      -1.1783509689698233,
      -1.2205664437449668,
      -1.2438390022308834,
      -1.1438940254955032,
      -1.2449202544663323,
      -1.2868489475227372,
      -1.1857718663122192,
      0.0,
      -1.3032454656849088
    ],
    [
      -1.4546260329310776,
      -1.368552095522967,
      -1.3791005316138027,
      -1.381824423480607,
      -1.3875837275260638,
      -1.3982178404781946,
      -1.437717607814166,
      -1.368605047304579,
      -1.3747081644692734,
      -1.3851971413461013,
      -1.41643909829939,
      -1.4307536130744944,
      -1.3588768096962103,
      -1.3614356544609385,
      -1.4117765804859161,
      -1.3673426258579915,
      -1.3534780156994068,
      -1.4109632941612742,
      -1.3841714260560511,
      -1.325251800773141,
      -1.381393753978721,
      -1.3588229878410303,
      -1.3504277438394945,
      -1.3875578733041465,
      -1.3932777389557056,
      -1.4290312766390425,
      -1.3372577525578264,
      -1.4348002151245767,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.10738970280742022,
      0.09177062822030124,
      0.1001027851539793,
      0.08851960161037153,
      0.0879040011885106,
      0.06965922361568433,
      0.08914797141055009,
      0.09456286544477432,
      0.07860416292182193,
      0.0912837460123408,
      0.0656864611996184,
      0.0859313254104761,
      0.09177951429156472,
      0.08788292323317859,
      0.08619994430781097,
      0.09603134515042333,
      0.0814736657211621,
      0.07599248016297588,
      0.10983289007705332,
      0.08607171040586636,
      0.10481224434619407,
      0.10602933520046953,
      0.08670769364811481,
      0.09884549504530749,
      0.0711300128806196,
      0.09145474562595579,
      0.09316365613112398,
      0.08305928297025167
    ],
    [
      0.34913862982351507,
      0.0,
      0.5073343358422322,
      0.5087079487168138,
      0.46882987107275076,
      0.4830455902781896,
      0.32588406824088345,
      0.4051657844345067,
      0.5070010336747417,
      0.37215103690013795,
      0.5533510481017305,
      0.2869345017712708,
      0.47613481749943465,
      0.5803119847962444,
      0.47990390141794115,
      0.5019906554346734,
      0.46125855721855413,
      0.4729507253778482,
      0.5006232721972972,
      0.4538289158352091,
      0.4570633751021247,
      0.38469744559398333,
      0.35379970324145815,
      0.48598046115351057,
      0.4272196829427837,
      0.3908758247977526,
      0.41297149564854263,
      0.3754880732805781,
      0.3058933069110159
    ],
    [
      0.2581263901096298,
      0.42446060567335686,
      0.0,
      0.48730554241577106,
      0.3941779166188599,
      0.4409116687350618,
      0.2797090340823687,
      0.3218016288450285,
      0.41537382847164506,
      0.3527278264986702,
      0.4184938116981485,
      0.23732754865731698,
      0.39454591208409173,
      0.482717413419977,
      0.4482318943997048,
      0.46093175461678926,
      0.3704651072483227,
      0.3779866064235333,
      0.3912771352621396,
      0.45095014964929336,
      0.35547576676848514,
      0.3269333298432402,
      0.2991377228807075,
      0.34284384632082254,
      0.3142582742229998,
      0.32233923638031725,
      0.3685941457880153,
      0.3290928718325825,
      0.2536233391259912
    ],
    [
      0.31460900181469165,
      0.46889844338819486,
      0.5166226769711455,
      0.0,
      0.39458632119198755,
      0.46339884215145544,
      0.29787519462707035,
      0.3620036028137028,
      0.4232183399908327,
      0.36219982323088207,
      0.45495592569580223,
      0.27290114239951313,
      0.4462087094471927,
      0.575714250303897,
      0.5197418482661298,
      0.4375534587075527,
      0.36122437298978083,
      0.4466452917408059,
      0.424393467295501,
      0.45926359092311086,
      0.3887287509745663,
      0.34276588102536554,
      0.3332131542387873,
      0.4109773790040787,
      0.3171569352310011,
      0.32657585175530635,
      0.4608538816654253,
      0.36744082560027214,
      0.3269681046598165
    ],
    [
      0.2943218212639094,
      0.4587040182747033,
      0.42979140380975434,
      0.3665574731551029,
      0.0,
      0.4647039800945745,
      0.34433956122047293,
      0.34130017403588364,
      0.42273207596564366,
      0.36920663041134505,
      0.4294212905695409,
      0.2862202339290716,
      0.36034397392731776,
      0.4375315859533824,
      0.4319403709851559,
      0.3859025140458623,
      0.4865875268851285,
      0.4411002612218715,
      0.37018155891222126,
      0.33120727002844874,
      0.40964536424285813,
      0.311981674200029,
      0.334205328395871,
      0.40698661184474405,
      0.4161329817576582,
      0.3804480439337996,
      0.3374952900987649,
      0.32403595128449747,
      0.290264696495931
    ],
    [
      0.3527028262677516,
      0.47728148126265024,
      0.5283322521784548,
      0.48051868706005996,
      0.4427247635179481,
      0.0,
      0.31765680225777904,
      0.41824575368889705,
      0.5798199905878387,
      0.3810564744840996,
      0.5569975294597627,
      0.27678685337769715,
      0.4275092725826348,
      0.5555699643800427,
      0.4999829786039083,
      0.5147349619434916,
      0.4693835670306279,
      0.4972177851181474,
      0.43966585275648806,
      0.43365694717731307,
      0.4579969441543543,
      0.4053086667799042,
      0.3384521231158659,
      0.44192523911032633,
      0.40501036891703057,
      0.45557294864645015,
      0.420987547563072,
      0.38924286804908537,
      0.30308471285263705
    ],
    [
      0.30261442153616347,
      0.3745387626994101,
      0.36244101448628774,
      0.32705670960244326,
      0.3851994095654643,
      0.36206375583130335,
      0.0,
      0.34414809033632565,
      0.3594085833488829,
      0.3539429598566146,
      0.3571860828158482,
      0.3660857205793726,
      0.3515710191403665,
      0.32508014386867745,
      0.35941264142737017,
      0.33145735150736955,
      0.36184280360249566,
      0.3402923995561342,
      0.320195845235304,
      0.3741264549440577,
      0.35720312912099805,
      0.36466452231810553,
      0.3261786731838161,
      0.3440782332852186,
      0.3560181848974031,
      0.3386460089883774,
      0.3385666161298493,
      0.3540484482457975,
      0.31863948445908985
    ],
    [
      0.24470649227031016,
      0.4254160887008409,
      0.4470608626093524,
      0.4361395521073761,
      0.4237274362632666,
      0.42042832377926853,
      0.30174015016251454,
      0.0,
      0.4429219503058959,
      0.33473978211099475,
      0.45871800516162775,
      0.27045892644990555,
      0.46802400981277037,
      0.35478865741498633,
      0.4137022132793453,
      0.4376986826184832,
      0.39289392393477063,
      0.40858700812775184,
      0.3516163774660259,
      0.36599223891926047,
      0.4085325832899547,
      0.32775989734199396,
      0.3333701715558153,
      0.4067900261065389,
      0.33452491718061395,
      0.28322150102553345,
      0.33972242091979554,
      0.35224796500141076,
      0.31390261483984805
    ],
    [
      0.36936657665592176,
      0.48873216968340905,
      0.5478031541698571,
      0.5126088097296706,
      0.44891502568243835,
      0.6558286561566011,
      0.334989032568739,
      0.44129326426453197,
      0.0,
      0.3953528189962836,
      0.5514155316107996,
      0.28273085309644896,
      0.4239528230918097,
      0.5268222319117744,
      0.5238935267113911,
      0.4827897392642826,
      0.43602251973940254,
      0.48880956052616553,
      0.4456211555349494,
      0.48833588988550414,
      0.4650068162450014,
      0.45618396147112494,
      0.34400363962323777,
      0.44075397484636536,
      0.37534079308429535,
      0.4199067578813285,
      0.44630424573407046,
      0.43443682397954664,
      0.35254767008658927
    ],
    [
      0.2850038060058089,
      0.4114035782502663,
      0.4432610217006008,
      0.42214511905947694,
      0.3835503849296358,
      0.4164351510151638,
      0.2937001680103022,
      0.3733846578804185,
      0.39596946864140636,
      0.0,
      0.4367651456193278,
      0.2953348471583743,
      0.4112559245225085,
      0.471104553106898,
      0.40625307639679087,
      0.42825693952917754,
      0.45494788233064387,
      0.3911669547974126,
      0.3794781827160272,
      0.4130087806202143,
      0.39816330926327903,
      0.3536860564747508,
      0.3292491237359534,
      0.40461345451026665,
      0.41918091096798316,
      0.36302058256178404,
      0.3205542405679769,
      0.32449853818604923,
      0.2839490252978476
    ],
    [
      0.25237777652151494,
      0.4805221998422964,
      0.4367030449255591,
      0.4199953825662015,
      0.40816488232275017,
      0.5173654149764151,
      0.25629089691608353,
      0.3395266749057668,
      0.484633921826253,
      0.34630507149139844,
      0.0,
      0.2509081389392933,
      0.42329267886690314,
      0.474931819325878,
      0.4766794002009229,
      0.4305295004015621,
      0.40195631538958954,
      0.4576059739013323,
      0.38522338800264233,
      0.38465353408893455,
      0.4425558935241001,
      0.3160938400467006,
      0.274168602677592,
      0.398598806422245,
      0.3509691579891079,
      0.32505691251401236,
      0.35553049781133717,
      0.34191511708320643,
      0.24361997289766735
    ],
    [
      0.2790426299919071,
      0.3005597821547876,
      0.2640773792106128,
      0.25344692243612954,
      0.2859916372520339,
      0.2640100105726546,
      0.27229979004166327,
      0.2561376177025476,
      0.287098293940542,
      0.3232634700677075,
      0.29995404080573906,
      0.0,
      0.2646552741532535,
      0.25926457558918625,
      0.2776874139515588,
      0.2632277369592586,
      0.291145147974208,
      0.25511006733237407,
      0.26580472635357055,
      0.2864789524125657,
      0.2737496314257588,
      0.2669582659449128,
      0.28519831144156194,
      0.2967125425516972,
      0.29579768103250537,
      0.26757570072658554,
      0.27386721840459183,
      0.29570632720238454,
      0.2795014886699143
    ],
    [
      0.35943229741201366,
      0.5818766043731691,
      0.5523325356510902,
      0.5631502764239056,
      0.45501511614414136,
      0.5451970431805482,
      0.35581341065308103,
      0.5014515178692269,
      0.5137403206399989,
      0.3970630702778457,
      0.6132383425965635,
      0.32058791242643037,
      0.0,
      0.560756699056731,
      0.46572637262649286,
      0.5068967316279729,
      0.4289080237367613,
      0.4605623109186656,
      0.4922825586056234,
      0.4911149557647858,
      0.4720491414767143,
      0.40791112203915936,
      0.40022214348244445,
      0.5009579142915195,
      0.3752731149441735,
      0.38478265792075894,
      0.5113213593411123,
      0.4367631127204554,
      0.3618855872331228
    ],
    [
      0.34055748986357637,
      0.6615359161030137,
      0.6300228942376138,
      0.6501742017913121,
      0.49301592294011987,
      0.5923739076147028,
      0.24788614379111618,
      0.4140488644303777,
      0.5091745858511485,
      0.4271319490529293,
      0.5843080446018281,
      0.26900986666197624,
      0.5006510291966924,
      0.0,
      0.5635462465236347,
      0.5610140407978714,
      0.464793943806205,
      0.5268905842141309,
      0.4916086552390031,
      0.49358589938801223,
      0.488234528901077,
      0.4176529701756657,
      0.37717250462322927,
      0.4690014199482391,
      0.40427421828499566,
      0.4473021102053665,
      0.48334059534014817,
      0.4046083038546209,
      0.3589529453323277
    ],
    [
      0.3246594883365066,
      0.5241732115135707,
      0.551500661379436,
      0.5945756350286677,
      0.5070889187268661,
      0.5553041419681499,
      0.34688725104695717,
      0.44613179502774813,
      0.5070775680565358,
      0.42597226450383996,
      0.5627138532700575,
      0.30057337952717367,
      0.45798893603753776,
      0.5437772200712294,
      0.0,
      0.5256433503585276,
      0.5256699135545662,
      0.5402653234451251,
      0.43186801828276256,
      0.4776737090840608,
      0.5329579001602252,
      0.4395042465162937,
      0.36025948765371774,
      0.4596165754114523,
      0.44455016974918404,
      0.4479236557944206,
      0.4278484749262763,
      0.4020033260946334,
      0.3447717429264121
    ],
    [
      0.2527564713642052,
      0.4801644649156003,
      0.5278499233349654,
      0.44305459407562053,
      0.40683557182284824,
      0.4788708480472861,
      0.2653648072561867,
      0.34476814075798434,
      0.4552624788054409,
      0.35092574826480116,
      0.4965701205889077,
      0.2706072115975584,
      0.3835692864915874,
      0.4895026145695187,
      0.46745899768479027,
      0.0,
      0.4052265061470919,
      0.43684753182070524,
      0.4308325767520331,
      0.4244170337205009,
      0.4014730515040321,
      0.38169447218344765,
      0.26967492351474287,
      0.38588145196987056,
      0.3494638510254049,
      0.3254155495258666,
      0.37674018508059115,
      0.40447668890314015,
      0.29153584759973894
    ],
    [
      0.25811393545094896,
      0.4327357902817872,
      0.4187850876839383,
      0.33238973214381984,
      0.45840336037633644,
      0.43470195546485724,
      0.2702405001742181,
      0.3160388319110503,
      0.38835354559884094,
      0.3194335252725813,
      0.4212928293796212,
      0.23309599366399714,
      0.3334541286347623,
      0.3885198330881754,
      0.38640218864933096,
      0.39329873408625327,
      0.0,
      0.40029185162961745,
      0.3259893939081926,
      0.34357280964986536,
      0.3763005472820564,
      0.2646099652819407,
      0.2788579511016389,
      0.40013383380871237,
      0.38957125416227156,
      0.331278336447562,
      0.3158681105734522,
      0.32889742041737957,
      0.25126621068061405
    ],
    [
      0.3332491531939763,
      0.5500472792423006,
      0.5100335747965647,
      0.5689473971055292,
      0.5399706298925462,
      0.5928089824796268,
      0.3079308407638741,
      0.41788413183403983,
      0.5243549927574893,
      0.4228842834288047,
      0.5893541190134406,
      0.29091905934236983,
      0.43701643837312165,
      0.593104392436619,
      0.5912161634043869,
      0.536372888010739,
      0.49093107669001745,
      0.0,
      0.4846930074908462,
      0.4513076460973229,
      0.5679607186516338,
      0.371781653057754,
      0.37293325966111945,
      0.4973790334960697,
      0.45834578172197826,
      0.46277954559041734,
      0.47007077121885277,
      0.45601155545680405,
      0.3079842343100543
    ],
    [
      0.3188761985551829,
      0.5183534566403019,
      0.452644113699159,
      0.40367445182893036,
      0.39685916253103914,
      0.4382359899468502,
      0.3051935830952244,
      0.39472421055664,
      0.3995117613157346,
      0.36013643216072966,
      0.409818934615535,
      0.28069981788658316,
      0.39325736769964115,
      0.4341879316626822,
      0.39872237585389714,
      0.4572850941725093,
      0.4153070689327767,
      0.42546259778314477,
      0.0,
      0.3893092231597606,
      0.4428720505583261,
      0.3517048884177014,
      0.33705639365939555,
      0.42338850229129843,
      0.4045302784239537,
      0.3890334140464313,
      0.3660298796737391,
      0.3212932037734486,
      0.27782563956732487
    ],
    [
      0.3482437703371406,
      0.41224147881283324,
      0.5121243555639114,
      0.46556832481141197,
      0.37798667281372644,
      0.4294363498817786,
      0.3043797879052186,
      0.35633979750540434,
      0.48050845383408003,
      0.39933425305674697,
      0.47315033395140293,
      0.27666205082216533,
      0.4055532051584181,
      0.47276821060286256,
      0.45318755321351056,
      0.42455226415746483,
      0.33633279075588596,
      0.41537943886146333,
      0.34599281568056806,
      0.0,
      0.3673406844224325,
      0.45348768609961465,
      0.3235050032159186,
      0.38347070108018366,
      0.2982473296854635,
      0.33539752439954906,
      0.44000500050022473,
      0.42509681163992585,
      0.3532256538903058
    ],
    [
      0.3158271114058837,
      0.5498597506485163,
      0.5772109713550948,
      0.47576981312226785,
      0.5725438450935261,
      0.5598411290771843,
      0.3573386129728182,
      0.4502527560099414,
      0.5074625976007798,
      0.4507300676502064,
      0.5742935025381977,
      0.3050922469030668,
      0.4775722081447935,
      0.5524069250298815,
      0.5253601908444014,
      0.550536237980511,
      0.5400840411588297,
      0.5522688068849944,
      0.49938488218124855,
      0.4773164884285934,
      0.0,
      0.4048581175449686,
      0.4349845229274676,
      0.5180654411207315,
      0.47613721359183625,
      0.5102781022760567,
      0.4183607845638164,
      0.4368128573657568,
      0.3653769106307512
    ],
    [
      0.2750660712496178,
      0.38347452209759947,
      0.3916947066776273,
      0.3638085964247706,
      0.3460493260926971,
      0.3904032433807392,
      0.2862712189066796,
      0.33146478330232454,
      0.39602330799773466,
      0.36384053826390517,
      0.39136498096811745,
      0.25315509238004164,
      0.3502557270642743,
      0.4417233169497621,
      0.41228879201088997,
      0.3827087530224469,
      0.3355574343862273,
      0.3781031999563651,
      0.33355222526267747,
      0.43395739046986015,
      0.33690027310228143,
      0.0,
      0.31301414467424116,
      0.4124243114873887,
      0.301545760467554,
      0.34318446850079853,
      0.3665085383533291,
      0.37274231909165234,
      0.3774366201765942
    ],
    [
      0.38776463214145473,
      0.47895279328807305,
      0.4720141555461197,
      0.4521191136159848,
      0.4738568748974077,
      0.4386139319803892,
      0.3666929716052485,
      0.4510136860978462,
      0.46875830195568335,
      0.48827801253071956,
      0.526342133359349,
      0.34387197191441765,
      0.4624862773467793,
      0.4590491524620166,
      0.48130823754223195,
      0.43593775090093434,
      0.48857372563516677,
      0.4495844505757549,
      0.44105826912871016,
      0.4418704975893175,
      0.517182133958497,
      0.3967629930200782,
      0.0,
      0.48906214079083643,
      0.528496422727206,
      0.4178910749924496,
      0.4576487304659571,
      0.4331443602178031,
      0.43457577721143226
    ],
    [
      0.3175026622435897,
      0.4379323545183633,
      0.42653534165859286,
      0.3691850844035276,
      0.4571956386260245,
      0.4258634155206029,
      0.31993641981517484,
      0.39297150244457923,
      0.41560656143020513,
      0.3959787268679844,
      0.4378866409961639,
      0.301351091727621,
      0.39344351580745074,
      0.40913272008625157,
      0.3915054552650379,
      0.41055848143591667,
      0.4586983295976752,
      0.4199279811144423,
      0.4075323293050046,
      0.3572687141132562,
      0.47041864920048226,
      0.3903378054724218,
      0.3675867649105089,
      0.0,
      0.441105655179282,
      0.3753899744613953,
      0.3530803122605335,
      0.40329736163264496,
      0.31220780938699844
    ],
    [
      0.2891606491327856,
      0.3982583955634438,
      0.38420242903901003,
      0.3197825864513928,
      0.4097039181277162,
      0.38411138580371906,
      0.2868132157938754,
      0.2790364212843237,
      0.3297027746556713,
      0.3898370137807379,
      0.3945267366656351,
      0.24492914248648612,
      0.32479826383638866,
      0.38527618414439013,
      0.3539504483441924,
      0.36230196839163487,
      0.43439329604640653,
      0.3730709161676109,
      0.3602225727093642,
      0.3068123670153904,
      0.35311352894500336,
      0.2284716792148802,
      0.34256784070530877,
      0.36338740034243444,
      0.0,
      0.3331977535671966,
      0.3278992973927273,
      0.2974406203451121,
      0.23172495533267834
    ],
    [
      0.24657188808800012,
      0.4423823041922461,
      0.396802202676215,
      0.3888986731037516,
      0.42222129717187706,
      0.4705311613181322,
      0.31389241594549944,
      0.33363837845764954,
      0.38046095698463644,
      0.3906246994375364,
      0.41022216204049866,
      0.27391217051684036,
      0.3531367087323489,
      0.43824222554956327,
      0.46274717205984883,
      0.43356750507526254,
      0.38389199853220024,
      0.45948248595335484,
      0.41311037101711134,
      0.3919189041419304,
      0.4269950856598679,
      0.38923658786638526,
      0.33295194327257893,
      0.3877257920993733,
      0.3724769392456777,
      0.0,
      0.357094116097737,
      0.35820258305690555,
      0.2685205618160351
    ],
    [
      0.3374935592779771,
      0.40434795517192645,
      0.46079762705519545,
      0.47630170745124145,
      0.3719521542091564,
      0.4058943556440544,
      0.31518283050223195,
      0.2858691273376399,
      0.4082505131541496,
      0.33775129808251103,
      0.4205103187830117,
      0.29118726787546834,
      0.4019377943748885,
      0.48726842505798573,
      0.3960902302036462,
      0.4042981604018461,
      0.34286121070545383,
      0.3903390816389005,
      0.36249463513349456,
      0.42630036042156516,
      0.347711068403312,
      0.3398677434434849,
      0.3373275286321329,
      0.38352171762567,
      0.3583787931410398,
      0.31189536220442915,
      0.0,
      0.41215939916903777,
      0.29230325294184234
    ],
    [
      0.34272943634442954,
      0.45202217436852465,
      0.4697217708041046,
      0.4907016942395761,
      0.43055744508580496,
      0.43014890552255736,
      0.3562763816161092,
      0.40951138757113936,
      0.45388038916849194,
      0.3887746898666813,
      0.4698613930610962,
      0.3627262313172286,
      0.39176911307574325,
      0.48265440910443047,
      0.44702054996603,
      0.4593728928352854,
      0.4209294600936557,
      0.5242036572952482,
      0.39885099686967873,
      0.5165666786542544,
      0.4618577265286965,
      0.41964225175355296,
      0.3963696932676364,
      0.49631467000301654,
      0.3952884410321875,
      0.3533597479757826,
      0.4544368291863006,
      0.0,
      0.33696322981361093
    ],
    [
      0.27739019613499805,
      0.3634641335431086,
      0.3529156974522729,
      0.3501918055854687,
      0.3444325015400118,
      0.333798388587881,
      0.29429862125190964,
      0.36341118176149667,
      0.3573080645968023,
      0.3468190877199744,
      0.3155771307666857,
      0.3012626159915812,
      0.3731394193698654,
      0.37058057460513716,
      0.3202396485801595,
      0.3646736032080842,
      0.3785382133666688,
      0.32105293490480147,
      0.3478448030100245,
      0.4067644282929346,
      0.35062247508735456,
      0.3731932412250454,
      0.3815884852265812,
      0.3444583557619292,
      0.33873849011037005,
      0.30298495242703316,
      0.39475847650824925,
      0.297216013941499,
      0.0
    ]
  ],
  "row_avgs": [
    0.0893224790783543,
    0.43887628740377593,
    0.3685650110026025,
    0.40273910957513814,
    0.3808317738194123,
    0.43812236296158286,
    0.3484538382345909,
    0.37497974209843754,
    0.4492774293654123,
    0.3824764601377266,
    0.3812919577277594,
    0.278011522725079,
    0.46486829476551816,
    0.4739953493846763,
    0.4646064363732834,
    0.39275860533301665,
    0.3497106306001366,
    0.4713668789113678,
    0.38949978651814077,
    0.3953399393807005,
    0.4798580762518615,
    0.35944713081136415,
    0.45296109191063794,
    0.39497990355291906,
    0.33888192004591133,
    0.38212354607532373,
    0.37536762421583186,
    0.42901829451503054,
    0.3452594121627831
  ],
  "col_avgs": [
    0.30812162081405037,
    0.44606176492898986,
    0.452228065097681,
    0.42938852212893586,
    0.4138598430756912,
    0.4457939475070806,
    0.299805104815678,
    0.3635968476599133,
    0.4252920545214957,
    0.3687523470424461,
    0.45355620481238507,
    0.2789649410928175,
    0.3919091128529661,
    0.45159169745856237,
    0.43007438613020993,
    0.42751041770712755,
    0.40837343223712624,
    0.419024266178888,
    0.389906841159696,
    0.40646759716258485,
    0.4076493870842621,
    0.356877257453525,
    0.33153851713642146,
    0.4064913403690233,
    0.3731028248843309,
    0.35773084330097793,
    0.3817112074085874,
    0.3632672644127627,
    0.30434323850415873
  ],
  "combined_avgs": [
    0.19872204994620235,
    0.4424690261663829,
    0.41039653805014176,
    0.416063815852037,
    0.39734580844755174,
    0.44195815523433174,
    0.3241294715251345,
    0.3692882948791754,
    0.437284741943454,
    0.37561440359008635,
    0.41742408127007224,
    0.2784882319089482,
    0.4283887038092421,
    0.46279352342161933,
    0.4473404112517467,
    0.4101345115200721,
    0.3790420314186314,
    0.44519557254512787,
    0.3897033138389184,
    0.40090376827164265,
    0.4437537316680618,
    0.35816219413244454,
    0.3922498045235297,
    0.4007356219609712,
    0.3559923724651211,
    0.36992719468815083,
    0.3785394158122096,
    0.39614277946389664,
    0.32480132533347095
  ],
  "gppm": [
    618.1100549842912,
    623.0412004583017,
    615.2219551570904,
    627.687663607594,
    635.6388978044661,
    620.7522454204856,
    690.4807818031708,
    657.5395562612058,
    630.2256706258056,
    656.1523890451601,
    618.6339910065009,
    698.6591884777326,
    647.5900794548961,
    620.0104850650347,
    629.868835285072,
    630.333979286326,
    638.3166204959037,
    637.2153306091151,
    647.9303910032387,
    636.553376940419,
    637.5460868404687,
    659.9765813705181,
    674.1127971990527,
    639.7718861213617,
    654.6255442331502,
    658.6053481524433,
    651.2960063295843,
    661.8909312846354,
    686.2950670593775
  ],
  "gppm_normalized": [
    1.4455726596240228,
    1.4160779432517554,
    1.3971581591308844,
    1.4307169141215594,
    1.435533109205824,
    1.4052724786098787,
    1.5687149148494843,
    1.4884349420193967,
    1.4336308576337455,
    1.4894697704647333,
    1.3999158875985842,
    1.5820263011685316,
    1.4716175539559913,
    1.4083881434349468,
    1.4317419455753304,
    1.430243948740099,
    1.443414191984083,
    1.4429508349522069,
    1.4718272571109683,
    1.4466619599470814,
    1.4416728891154804,
    1.502023303412774,
    1.528860430264401,
    1.4462668302377615,
    1.4847005801764674,
    1.4948829969873099,
    1.4713755527205692,
    1.5014391342645226,
    1.5573941592602494
  ],
  "token_counts": [
    548,
    467,
    449,
    492,
    391,
    412,
    438,
    397,
    480,
    439,
    411,
    388,
    456,
    452,
    464,
    438,
    403,
    409,
    446,
    444,
    401,
    457,
    413,
    395,
    434,
    435,
    372,
    423,
    413,
    819,
    418,
    459,
    508,
    546,
    421,
    391,
    416,
    442,
    444,
    460,
    526,
    464,
    390,
    413,
    420,
    381,
    431,
    456,
    441,
    417,
    437,
    387,
    449,
    412,
    382,
    431,
    441,
    393,
    1833,
    487,
    412,
    448,
    441,
    422,
    437,
    449,
    434,
    443,
    455,
    399,
    423,
    469,
    417,
    417,
    413,
    427,
    462,
    486,
    420,
    386,
    363,
    387,
    417,
    398,
    419,
    415,
    390,
    1274,
    448,
    513,
    484,
    438,
    412,
    402,
    417,
    391,
    410,
    485,
    491,
    375,
    442,
    430,
    492,
    454,
    385,
    413,
    450,
    397,
    458,
    377,
    455,
    438,
    450,
    412,
    383,
    390
  ],
  "response_lengths": [
    6180,
    2478,
    2968,
    2740,
    2568,
    2373,
    2337,
    2360,
    2192,
    2299,
    2813,
    2736,
    2218,
    2495,
    2420,
    2765,
    2530,
    2162,
    2384,
    2583,
    2247,
    2493,
    2196,
    2631,
    2495,
    2413,
    2373,
    2189,
    2247
  ]
}