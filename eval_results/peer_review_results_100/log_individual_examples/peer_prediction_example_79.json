{
  "example_idx": 79,
  "reference": "Under review as a conference paper at ICLR 2023\n\nTOWARD RELIABLE NEURAL SPECIFICATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHaving reliable specifications is an unavoidable challenge in achieving verifiable correctness, robustness, and interpretability of AI systems. Existing specifications for neural networks are in the paradigm of data as specification. That is, the local neighborhood centering around a reference input is considered to be correct (or robust). However, our empirical study shows that such a specification is extremely overfitted since usually no data points from the testing set lie in the certified region of the reference input, making them impractical for real-world applications. We propose a new family of specifications called neural representation as specification, which uses the intrinsic information of neural networks — neural activation patterns (NAP), rather than input data to specify the correctness and/or robustness of neural network predictions. We present a simple statistical approach to mining dominant neural activation patterns. We analyze NAPs from a statistical point of view and find that a single NAP can cover a large number of training and testing data points whereas ad hoc data-as-specification only covers the given reference data point. To show the effectiveness of discovered NAPs, we formally verify several important properties, such as various types of misclassifications will never happen for a given NAP, and there is no-ambiguity between different NAPs. We show that by using NAP, we can verify the prediction of the entire input space, while still recalling 84% of the data. Thus, we argue that using NAPs is a more reliable and extensible specification for neural network verification.\n\n1\n\nINTRODUCTION\n\nThe advances in deep neural networks (DNNs) have brought a wide societal impact in many domains such as transportation, healthcare, finance, e-commerce, and education. This growing societal-scale impact has also raised some risks and concerns about errors in AI software, their susceptibility to cyber-attacks, and AI system safety (Dietterich & Horvitz, 2015). Therefore, the challenge of verification and validation of AI systems, as well as, achieving trustworthy AI (Wing, 2021), has attracted much attention of the research community. Existing works approach this challenge by building on formal methods – a field of computer science and engineering that involves verifying properties of systems using rigorous mathematical specifications and proofs (Wing, 1990). Having a formal specification — a precise, mathematical statement of what AI system is supposed to do is critical for formal verification. Most works (Katz et al., 2017; 2019; Huang et al., 2017; 2020; Wang et al., 2021) use the specification of adversarial robustness for classification tasks that states that the NN correctly classifies an image as a given adversarial label under perturbations with a specific norm (usually l∞). Generally speaking, existing works use a paradigm of data as specification — the robutness of local neighborhoods of reference data points with ground-truth labels is the only specification of correct behaviors. However, from a learning perspective, this would lead to overfitted specification, since only local neighborhoods of reference inputs get certified.\n\nAs a concrete example, Figure 1 illustrates the fundamental limitation of such overfitted specifications. Specifically, a testing input like the one shown in Fig. 1a can never be verified even if all local neighborhoods of all training images have been certified using the L∞ norm. This is because adversarial examples like Fig. 1c fall into a much closer region compared to testing inputs (e.g., Fig. 1a), as a result, the truly verifiable region for a given reference input like Fig. 1b can only be smaller. All neural network verification approaches following such data-as-specification paradigm inherit this fundamental limitation regardless of their underlying verification techniques. In order to avoid such a limitation, a new paradigm for specifying what is correct or wrong is necessary.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) A testing image from MNIST, classified as 1\n\ntraining (b) The closest image in MNIST, whose L∞ distance to Fig. 1a is 0.5294\n\n(c) An adversarial example misclassified as 8, whose L∞ distance to Fig. 1b is 0.2\n\n(d) A testing image, on which our verified NAP (for digit 1) disagrees with the ground truth (i.e., 7)\n\nFigure 1: The limitation of “data-as-specification”: First three images show that a test input can be much further away (in L∞) from its closest train input compared to adversarial examples (the upper bound of a verifiable local region). The last image shows that even data itself can be imperfect.\n\nThe intrinsic challenge is that manually giving a proper specification on the input space is no easier than directly programming a solution to the machine learning problem itself. We envision that a promising way to address this challenge is developing specifications directly on top of, instead of being agnostic to, the learned model.\n\nWe propose a new family of specifications, neural representation as specification, where neural activation patterns form specifications. The key observation is that inputs from the same class often share a dominant neural activation pattern (NAP) – a carefully chosen sub-sets of neurons that are expected to be activated (or not activated) for the majority of inputs. Although two inputs are distant in a certain norm in the input space, the neural activations exhibited when the same prediction is made are very close. For instance, we can find a single dominant NAP that is shared by nearly all training and testing images (including Fig. 1a and Fig. 1b) in the same class but not the adversarial example like Fig. 1c. We can further formally verify that all possible inputs following this particular dominant NAP can never be misclassified. Specifications based on NAP enable successful verification of a broad region of inputs, which would not be possible if the data-as-specification paradigm were used. For the MNIST dataset, we find that a verifiable dominant NAP mined from the training images could cover up to 84% testing images, a significant improvement in contrast to 0% when using neighborhoods of training images as the specification. To our best knowledge, this is the first time that a significant fraction of unseen testing images have been formally verified. Interestingly, the verified dominant NAP also enables us to “double check” whether ground truth labels given by human beings are indeed reliable. Fig. 1d shows such an example on which our verified NAP disagrees with the ground truth. We have a tunable parameter to specialize a dominant NAP to avoid accepting such a potentially controversial region of inputs when necessary.\n\nThis unique advantage of using NAPs as specification is enabled by the intrinsic information (or neural representation) embedded in the neural network model. Furthermore, such information is a simple byproduct of a prediction and can be collected easily and efficiently. Besides serving as reliable specifications for neural networks, we foresee other important applications of NAPs. For instance, verified NAPs may serve as proofs of correctness or certificates for predictions. We hope our initial findings shared in this paper would inspire new interesting applications. We summarize our contribution as follows:\n\n• We propose a new family of formal specifications for neural networks, neural representa-\n\ntion as specification, which use activation patterns (NAPs) as specifications.\n\n• We propose a simple yet effective method to mine dominant NAPs from neural networks\n\nand training dataset.\n\n• We show that NAPs can be easily checked by out-of-the-box neural network verification tools used in VNNCOMP (2021) – the latest neural network verification competition, such as Marabou.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n• We conduct thorough experimental evaluations from both statistical and formal verification perspectives. Particularly, we show that a single dominant NAP is sufficient for certifying a significant fraction of unseen testing images.\n\n2 BACKGROUND\n\n2.1 NEURAL NETWORKS FOR CLASSIFICATION TASKS\n\nIn this paper, we focus on feed-forward neural networks for classification. A feed-forward neural network N of L layers is a set {(W i, bi) | 1 ≤ i ≤ L}, where W i and bi are the weight matrix and the bias for layer i, respectively. The neural network N defined a function FN : Rd0 → RdL (d0 and dL represent the input and output dimension, respectively), defined as FN (x) = zL(x), wherez0(x) = x, zi(x) = W iσ(zi−1(x))+bi and σ is the activation function. Neurons are indexed linearly by v0, v1, · · · . In this paper we focus only on the ReLU activation function, i.e., σ(x) = max(x, 0) element-wise, but the idea and techniques can be generalized for different activation functions and architectures as well. The ith element of the prediction vector FN (x)[i] represents the score or likelihood for the ith label, and the one with the highest score (arg maxi FN (x)[i]) is often considered as the predicted label of the network N . We denote this output label as ON (x). When the context is clear, we omit the subscript N for simplicity.\n\n2.2 ADVERSARIAL ATTACKS AGAINST NEURAL NETWORKS AND THE ROBUSTNESS\n\nVERIFICATION PROBLEM\n\nGiven a neural network N , the aim of adversarial attacks is to find a perturbation δ of an input x, such that x and x + δ are “similar” according to some domain knowledge, yet O(x) ̸= O(x + δ). In this paper, we use the common formulation of “similarity” in the field: two inputs are similar if the L∞ norm of δ is small. Under this formulation, finding an adversarial example can be defined as solving the following optimization problem:1\n\nmin||δ||∞ s.t O(x) ̸= O(x + δ)\n\nIn practice, it is very hard to formally define “similar”: should an image and a crop of it “similar”? Should two sentences differ by one synonym the same? We refer curious readers to the survey (Xu et al., 2020) for a comprehensive review of different formulations.\n\nOne natural defense against adversarial attacks, called robustness verification, is to prove that min ||δ||∞ must be greater than some user-specified threshold ε. Formally, given that O(x) = i, we verify\n\n(1) where B(x, ε) is a L∞ norm-ball of radius ε centered at x: B(x, ε) = {x′ | ||x − x′||∞ ≤ ε}. If Eq. (1) holds, we say that x is ε-robust.\n\n∀x′ ∈ B(x, ε) · ∀j ̸= i · F (x′)[i] − F (x′)[j] > 0\n\n2.3 MARABOU\n\nFor this paper, we use Marabou (Katz et al., 2019), a dedicated state-of-the-art NN verifier. Marabou extends the Simplex (Nelder & Mead, 1965) algorithm for solving linear programming with special mechanisms to handle non-linear activation functions. Internally, Marabou encodes both the verification problem and the adversarial attacks as a system of linear constraints (the weighted sum and the properties) and non-linear constraints (the activation functions). Same as Simplex, at each iteration, Marabou tries to fix a variable so that it doesn’t violate its constraints. While in Simplex, a violation can only happen due to a variable becoming out-of-bound, in Marabou a violation can also happen when a variable doesn’t satisfy its activation constraints.\n\nBy focusing only on neural networks with piecewise-linear activation functions, Marabou makes two insights: first, only a small subset of the activation nodes are relevant to the property under consideration. Hence, Marabou treats the non-linear constraints lazily and reduces the number of expensive\n\n1While there are alternative formulations of adversarial robustness (see Xu et al. (2020)), in this paper, we\n\nuse adversarial attacks as a black box, thus, stating one formulation is sufficient.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ncase-splits, making it much faster than traditional SMT solvers. Second, Marabou repeatedly refines each variable’s lower and upper bound, hoping that many piecewise-linear constraints can be turned into linear (phase-fixed), reducing further the need for case splitting. Altogether, Marabou achieves state-of-the-art performance on a wide range of benchmarks (VNNCOMP, 2021).\n\n3 NEURAL ACTIVATION PATTERNS\n\nIn this section, we discuss in detail neural activation patterns (NAP), what we consider as dominant NAPs and how to relax them, and what interesting properties of neural activation patterns can be checked using Marabou.\n\n3.1 NAPS, DOMINANT NAPS, RELAXED DOMINANT NAPS\n\nIn our setting (Section 2), the output of each neuron is passed to the ReLU function before going to neurons of the next layer, i.e., zi(x) = W iσ(zi−1(x)) + bi. We abstract each neuron into two states: activated (if its output is positive) and deactivated (if its output is non-positive). Clearly, for any given input, each neuron can be either activated or deactivated, but not both.\n\nDefinition 3.1 (Neural Activation Pattern). A Neural Activation Pattern (NAP) of a neural network N is a pair PN := (A, D), where A and D are two disjoint set of neurons.\n\nWe say that an input x follows a NAP PN if after computing FN (x), the neurons in A are all activated, and neurons in D are all deactivated. We denote this as PN (x) = T rue. Note that A and D are not required to contain all activated and deactivated neurons, thus for a given N an input x can follow multiple NAPs. Our intuition is that for some NAPs, all inputs following them should be classified correctly. We call such NAPs the dominant NAPs.\n\nDefinition 3.2 (Dominant NAP with respect to a label l). Given a label l and a neural network N , we define a dominant NAP with respect to l – denoted P l\n\nN as\n\nP l\n\nN := (Al, Dl) s.t ∀x · O(x) = l ⇐⇒ P l\n\nN (x) = T rue\n\nNote that x is quantified over all the input space, not restricted to a L∞ ball or a train or test set. The dominant NAP is a certificate for the prediction: any input that follows P l N (x) must be classified as l by N . However, dominant NAPs are very hard to find (as hard as verifying the robustness property for the whole input space), or may not exist at all. Thus, we wish to relax the definition of dominant NAPs: First, instead of quantifying x over the input space, we limit x to a dataset. Second, we break the strong ⇐⇒ condition, keeping only the condition O(x) = l =⇒ P l N (x) = T rue. Finally, we relax the implication even further, by allowing a neuron to be added to the NAP even when it is not activated/deactivated in all inputs of the same label. We call this relaxed version the δ-relaxed dominant NAP.\n\nDefinition 3.3 (δ-relaxed dominant NAP). Let S be the dataset, and Sl be the set of data labeled as l. Clearly ∀l, Sl ⊆ S and (cid:83) l Sl = S. We introduce a relaxing factor δ ∈ [0, 1]. A δ-relaxed dominant NAP for the label l – denoted δ.P l N – is defined through construction (mining)\n\n1. Initialize two counters ak and dk for each neuron vk.\n\n2. ∀x ∈ Sl, compute FN (x), let ak += 1 if vk is activated; let dk += 1 if vk is deactivated.\n\n3. Al ← {vk | ak\n\n|Sl| ≥ 1 − δ} , Dl ← {vk | dk\n\n|Sl| ≥ 1 − δ}\n\n4. δ.P l\n\nN ← (Al, Dl)\n\nIntuitively, for each neuron, we ask: how often is this neuron activated/deactivated upon seeing inputs from a label l. δ controls how precise we want to be: should we add a neuron to our NAP only if it is activated in all the inputs labeled l, or 95% of the inputs is enough? Thus, as δ increases, we expect the size of NAPs also to increase. Choosing the right δ has a huge impact on how useful the relaxed dominant NAPs can be. Table 1 gives us some intuition why. For MNIST, the table shows how many test images from a label l follow δ.P l, together with how many test images from other labels that also follow the same δ.P l. For example, there are 980 images in the test set with\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: The number of the test images in MNIST that follow a given δ.NAP. For a label i, i represents images with labels other than i yet follow δ.NAPi. The leftmost column is the values of δ. The top row indicates how many images in the test set are of a label.\n\n0 (980)\n\n0\n\n20 1\n0 0\n\n0\n\n967 775 376 111\n\n1 (1135)\n\n2 (1032)\n\n3 (1010)\n\n1\n\n1124 959 456 126\n\n1\n\n8 0\n0 0\n\n2\n\n997 792 261 43\n\n2\n\n22 4\n1 0\n\n3\n\n980 787 320 92\n\n3\n\n13 2\n0 0\n\n4 (982)\n\n4\n\n25 3\n0 0\n\n4\n\n959 766 259 76\n\n5 (892)\n\n5\n\n32 1\n0 0\n\n5\n\n874 677 226 24\n\n6 (958)\n\n6\n\n26 4\n0 0\n\n6\n\n937 726 200 45\n\n7 (1028)\n\n8 (974)\n\n9 (1009)\n\n7\n\n1003 809 357 144\n\n7\n\n28 2\n0 0\n\n8\n\n941 696 192 44\n\n8\n\n22 3\n0 0\n\n9\n\n967 828 277 73\n\n9\n\n12 4\n0 0\n\n0.00 0.01 0.05 0.10\n\nlabel 0 (second column). Among them, 967 images follow 0.P 0. In addition to that, there are 20 images from the other 9 labels that also follow 0.P 0. With the increase of δ, we can see that in both cases, both numbers decrease, suggesting that it is harder for an image to follow 0.P 0 without being classified as 0 (the NAP is more precise), at the cost of having many images classified as 0 fail to follow 0.P 0 (the NAP recalls worse). Essentially, δ controls the precision-recall trade-off for the NAP. Note that a small increase in δ, such as 0.01, can already have a huge effect: a neuron may be activated in 99% of the input, but the collection of all said neurons is only activated together in 79% of the input (775 out of 980), as can be seen with label 0 in Table 1.\n\nFor the rest of the paper, if not explicitly stated, we refer to the δ-relaxed dominant NAPs when talking about NAPs. When the context is clear, we drop any of the δ, N and l from the notation for clarity.\n\n3.2 NEURAL NETWORK PROPERTIES WITH NEURAL ACTIVATION PATTERNS\n\nWe expect that NAPs can serve as the key component in more reliable specifications of neural networks. As the first study on this topic, we introduce here two important ones.\n\nThe non-ambiguity property of NAPs We want our NAPs to give us some confidence about the predicted label of an input, thus a crucial sanity check is to verify that no input can follow two different NAPs of two different labels. Formally, we verify:\n\n∀x · ∀i, j(i ̸= j) · P i(x) = T rue =⇒ P j(x) = F alse\n\n(2)\n\n(cid:84) Dj Note that this property is trivial if either Ai N is non-empty: a single input N\ncannot activate and deactivate the same neuron. If that’s not the case, we can encode and verify the property using Marabou.\n\nN or Aj\n\n(cid:84) Di\n\nN\n\nNAP-augmented robustness property As seen in Figure 1 and Section 4.1, in many cases the maximum verifiable B(x, ε) does not contain any other input in the dataset. Our key insight is that by adding NAP constraints, we change the problem from verifying a “perfect” but small L∞ ball to an “imperfect” but much bigger L∞ ball, which may cover more of the input space. We explain this insight in more detail in Section 3.3. Concretely, we formalize our new robustness verification problem as follows: given a neural network N and a mined P i\n\nN , we check\n\n∀x′ ∈ B+(x, ε, P i\n\nN ) · ∀j ̸= i · F (x′)[i] − F (x′)[j] > 0\n\nin which O(x) = i and\n\nB+(x, ε, P i\n\nN ) = {x′ | ||x − x′||∞ ≤ ε, P i\n\nN (x′) = T rue}\n\nWorking with NAPs using Marabou NAPs and NAP properties can be encoded using Marabou with little to no changes to Marabou itself. To force a neuron to be activated or deactivated, we add a constraint for its output. To improve performance, we infer ReLU’s phases implied by the NAPs, and change the corresponding constraints2. For example, given a ReLU vi = max(vk, 0), to enforce vk to be activated, we remove the constraint from Marabou and add two new ones:vi = vk, and vk ≥ 0.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: The frequency of each ReLU and the dominant NAPs for each label. Activated and deactivated neurons are denoted by + and −, respectively, and ∗ denotes an arbitrary neuron state.\n\nLabel\n\n0 (Green)\n\n1 (Red)\n\nNeuron states (+, −, −, +, −, +) (+, +, −, +, −, +) (+, +, −, −, +, −) (−, +, −, −, +, −) (+, +, −, −, +, +)\n\n#samples 8\n2 7\n2 1\n\nDominant NAP\n\n(+, ∗, −, +, −, +)\n\n(∗, +, −, +, −, ∗)\n\n(a) Linear regions in different colors are determined by weights and biases of the neural network. Points colored either red or green constitute the training set.\n\n(b) NAPs are more flexible than L∞ norm-balls (boxes) in terms of covering verifiable regions.\n\nFigure 2: Visualization of linear regions and NAPs as specifications compared to L∞ norm-balls.\n\n3.3 CASE STUDY: VISUALIZING NAPS OF A SIMPLE NEURAL NETWORK\n\nWe show the advantages of NAPs as specifications using a simple example of a three-layer feedforward neural network that predicts a class of 20 points located on a 2D plane. We trained a neural network consisting of six neurons that achieves 100% accuracy in the prediction task. The resulting linear regions as well as the training data are illustrated in Fig. 2a. Table 2 summarizes the frequency of states of each neuron based on the result of passing all input data through the network, and NAPs for labels 0 and 1.\n\nFig. 2b visualizes NAPs for labels 0 and 1, and the unspecified region which provides no guarantees on data that fall into it. The green dot is so close to the boundary between P 0 and the unspecified region that some L∞ norm-balls (boxes) such as the one drawn in the dashed line may contain an adversarial example from the unspecified region. Thus, what we could verify ends up being a small box within P 0. However, using P 0 as a specification allows us to verify a much more flexible region than just boxes, as suggested by the NAP-augmented robustness property in Section 3.2. This idea generalizes beyond the simple 2D case, and we will illustrate its effectiveness further with a critical evaluation in Section 4.2.\n\n4 EVALUATION\n\nIn this section, we validate our observation about the distance between inputs, as well as evaluate our NAPs and NAP properties.\n\n2Marabou has a similar optimization, but the user cannot control when or if it is applied.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) The distribution of L2-norms between any two images from the same label. Images of digit (label) 1 are much similar than that of other digits.\n\n(b) The distribution of L∞-norms between any two images from the same label. The red line is drawing at 0.05 – the largest ε used in VNNCOMP (2021).\n\nFigure 3: Distances between any two images from the same label (class) are quite significant under different metrics of norm.\n\n4.1 TEST DATA ARE FAR AWAY FROM MAXIMUM VERIFIED BOUNDS\n\nWe argue that the maximum verifiable bounds are much smaller than the distance between real data, as illustrated in Figure 1. To show that this is not a special case, we plot the distribution of distances 3 and L∞ norm between any pair of images with the same label from the MNIST dataset, as in L2 shown in Figure 3. We obverse that the smallest L∞ distance for each label is greater than 0.2, much larger than 0.05, the largest verifiable perturbation ε used in VNNCOMP (2021).\n\nThis strongly verifies our motivation that the data as specification paradigm is not sufficient to verify data in real-life scenarios. The differences between data of each class are clearly larger than the differences allowed in some certain specifications using L∞ norm-balls.\n\n4.2 THE NAP-AUGMENTED ROBUSTNESS PROPERTY\n\nWe conduct two experiments to demonstrate the NAP-augmented robustness property. Both experiments use the MNIST dataset and a 4-layers fully connected network used in VNNCOMP (2021), with 256 neurons for each layer. All checks are run with a 10-minute time limit, on an AMD Ryzen 5565U laptop with 16GBs of RAM. The results are shown in Table 3 and Table 4. For each label l ∈ [0, 9], ‘Y’ and ‘N’ indicate that the network is robust for l (i.e., no adversarial example) or not, respectively. ‘T/o’ means the verification of robustness timed out.\n\nThe first experiment circles back to Figure 1. In the figure, we show an image I and its adversarial example that has the distance of 0.2 to I in L∞ space. Using our technique, can we prove that there is no adversarial example inside B+(I, 0.2, δ.P 1)? We start with checking if a counter-example exists for any label (first row in Table 3), and somewhat surprisingly, Marabou can only find a counter-example in 3 out of 9 labels within the time limit. Using the most precise NAP 0.P, we are able to verify the robustness of I against 2 labels. By increasing δ to 0.01, we are able to verify the robustness against all other labels. In fact, with δ = 0.01, we are able to verify the whole input space (denoted by ε = 1.0 in the last row)! We acknowledge a “fineprint” that by using δ = 0.01, we only recall about 84% of the test set (Table 1), but this is still a significant result. To the best of our knowledge, this is the first time a significant region of the input space has been formally verified.\n\nFor the second experiment, we verify the all six (x, ε) tuples in MNIST dataset that are known to be not robust from VNNCOMP-2021. We start with augmenting those examples with 0.P. Using 0.P, we can prove the robustness of one input against all other labels and block adversary examples in many labels for other inputs (Table 4). We also note that adversarial examples still exist against certain labels, indicating that 0.P may be too weak. Indeed, by slightly relaxing the NAP (δ = 0.01), all of the chosen inputs can be proven to be robust. Furthermore, with δ = 0.01, we can verify the robustness for 5 of the 6 inputs (Table 4) with ε = 0.3 – an order of magnitude bigger bound than before. Unfortunately, we are not able to verify bigger ε, except for x1 and x5 (that can be verified in\n\n3The L2 metric is not commonly used by the neural network verification research community as they are\n\nless computationally efficient than the L∞ metric.\n\n7\n\n0123456789class2468101214distance (L2 norm)0.9900.9920.9940.9960.9981.0000123456789class0.000.050.10distance (L norm)Under review as a conference paper at ICLR 2023\n\nTable 3: Verify the robustness of the example in Figure 1\n\n1\n\n2\n\n0 9\nε = 0.2, no NAP N - N T/o T/o T/o T/o T/o N T/o ε = 0.2, δ = 0 N\nN - N ε = 0.2, δ = 0.01 Y - Y Y\nε = 1.0, δ = 0.01 Y - Y Y\n\nY T/o T/o Y\nY Y\nY Y\nY\n\nY T/o N Y Y Y\nY Y Y\n\n4\n\n7\n\n8\n\n3\n\n5\n\n6\n\nTable 4: Inputs that are not robust can be augmented with a NAP to be robust. We index the 6 known non-robust inputs from MNIST x0 to x5, and augment them with NAP.\n\nO(x0) = 0\n\nO(x1) = 1\n\nO(x2) = 0\n\nO(x3) = 7\n\nO(x4) = 9\n\nO(x5) = 1\n\nε = 0.03, δ = 0 ε = 0.3, δ = 0.01 ε = 0.05, δ = 0 ε = 0.3, δ = 0.01 ε = 0.05, δ = 0 ε = 0.3, δ = 0.01 ε = 0.05, δ = 0 ε = 0.3, δ = 0.01 ε = 0.05, δ = 0 ε = 0.3, δ = 0.01 ε = 0.05, δ = 0 ε = 0.3, δ = 0.01\n\n4 Y\nY Y\nY\n\n1 Y\nY -\n-\n\n9 8\n7 5\n3 2\n0 Y\nY Y\nY Y Y -\nY Y\nY Y\nY Y -\nY N\nY Y\nY Y Y\nY Y\nY Y\nY Y Y\nN T/o T/o - T/o T/o Y T/o T/o Y\nY Y\nY Y -\nY Y\nN T/o -\nY Y T/o T/o N T/o Y\nY -\nY Y Y Y\nY -\nY N\nY Y Y Y\nT/o -\nY T/o T/o T/o Y T/o T/o Y N\nN Y\nN Y Y\nY Y\nY Y Y Y\n\n6 Y\nY Y\nY Y\nY Y\nY N\n\nY Y\nN Y\nY\n\nN Y\n\nY Y\n\nY\n\n- -\n\nthe whole input space, as demonstrated with the previous experiment). One might tempt to increase δ even further to verify bigger ε, but one must remember that comes with sacrificing the recall of the NAP. Thus, choosing an appropriate δ is crucial for having useful NAPs.\n\n4.3 THE NON-AMBIGUITY PROPERTY OF MINED NAPS\n\nWe evaluate the non-ambiguity of our mined NAP at different δs. At δ = 0, Marabou can construct inputs that follow any pair of NAP, indicating that the 0-relaxed dominant NAPs do not satisfy the property. However, by setting δ = 0.01, we are able to prove the non-ambiguity for all pair of NAPs, through both trivial cases and invoking Marabou. Like the previous experiment, we observe the benefit of relaxing δ here: the most precise NAPs contain so few ReLUs that it is very easy to violate the non-ambiguity property, thus relaxing δ is crucial for the NAPs to be more useful.\n\nThe non-ambiguity property of NAPs holds an important prerequisite for neural networks to achieve a sound classification result. Otherwise, if two labels share the same NAP, the final prediction of inputs with these two labels may also become indistinguishable. We argue that NAPs mined from a well-trained neural network should demonstrate strong non-ambiguity properties and ideally, all inputs with the same label i should follow the same P i. However, this strong statement may fail even for an accurate model when the training dataset itself is problematic, as what we observed in Fig. 1d as well as many examples in Appendix C. These examples are not only similar to the model but also to humans despite being labeled differently. The experiential results also suggest our mined NAPs do satisfy the strong statement proposed above if excluding these noisy samples.\n\nTo conclude, in this section we present a study that verifies our hypothesis about L-norm distances between data points in the MNIST dataset, thus explaining the limitation of using L-norm in verifying neural networks’ robustness. Then, we conduct experiments to show that our δ-relaxed dominant NAPs can be used to verify a much bigger region in the input space, which has the potential to generalize to real-world scenarios.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5 RELATED WORK AND FUTURE DIRECTIONS\n\nAbstract Interpretation in verifying Neural Networks The software verification problem is undecidable in general (Rice, 1953). Given that a Neural Network can also be considered a program, verifying any non-trivial property of a Neural network is also undecidable. Prior work on neural network verification includes specifications that are linear functions of the output of the network: Abstract Interpretation (AbsInt) (Cousot & Cousot, 1977) pioneered a happy middle ground: by sacrificing completeness, an AbsInt verifier can find proof much quicker, by over-approximating reachable states of the program. Many NN-verifiers have adopted the same technique, such as DeepPoly (Singh et al., 2019), CROWN (Wang et al., 2021), NNV (Tran et al., 2021), etc. They all share the same insight: the biggest bottle neck in verifying Neural Networks is the non-linear activation functions. By abstracting the activation into linear functions as much as possible, the verification can be many orders of magnitude faster than complete methods such as Marabou. However, there is no free lunch: Abstract-based verifiers are inconclusive and may not be able to verify properties even when they are correct.4 On the other hand, the neural representation as specification paradigm proposed in this work can be naturally viewed as a method of Abstract Interpretation, in which we abstract the state of each neuron to only activated and deactivated by leveraging NAPs. We would like to explore more refined abstractions such as {(−∞], (0, 1], (1, ∞]} in future work.\n\nNeural Activation Pattern in interpreting Neural Networks There are many attempts aimed to address the black-box nature of neural networks by highlighting important features in the input, such as Saliency Maps (Simonyan et al., 2014; Selvaraju et al., 2016) and LIME(Ribeiro et al., 2016). But these methods still pose the question of whether the prediction and explanation can be trusted or even verified. Another direction is to consider the internal decision-making process of neural networks such as Neural Activation Patterns (NAP). One popular line of research relating to NAPs is to leverage them in feature visualization (Yosinski et al., 2015; B ̈auerle et al., 2022; Erhan et al., 2009), which investigates what kind of input images could activate certain neurons in the model. Those methods also have the ability to visualize the internal working mechanism of the model to help with transparency. This line of methods is known as activation maximization. While being great at explaining the prediction of a given input, activation maximization methods do not provide a specification based on the activation pattern: at best they can establish a correlation between seeing a pattern and observing an output, but not causality. Moreover, moving from reference sample to revealing neural network activation pattern is limiting as the portion of NAP uncovered is dependent on the input data. This means that it might not be able to handle cases of unexpected test data. Conversely, our method starts from the bottom up: from the activation pattern, we uncover what region of input can be verified. This property of our method grants the capability to be generalized. Motivated by our promising results, we would like to generalize our approach to modern deep learning models such as Transformers (Vaswani et al., 2017), which employ much more complex network structures than a simple feed-forward structure.\n\n6 CONCLUSION\n\nWe propose a new paradigm of neural network specifications, which we call neural representation as specification, as opposed to the traditional data as specifications. Specifically, we leverage neural network activation patterns (NAPs) to specify the correct behaviours of neural networks. We argue this could address two major drawbacks of “data as specifications”. First, NAPs incorporate intrinsic properties of networks which data fails to do. Second, NAPs could cover much larger and more flexible regions compared to L∞ norm-balls centred around reference points, making them appealing to real-world applications. We also propose a simple method to mine relaxed dominant NAPs and show that working with NAPs can be easily supported by modern neural network verifiers such as Marabou. Through a simple case study and thorough valuation on the MNIST dataset, we show that using NAPs as specifications not only address major drawbacks of data as specifications, but also demonstrate important properties such as no-ambiguity and one order of magnitude stronger verifiable bounds. We foresee verified NAPs have the great potential of serving as simple, reliable, and efficient certificates for neural network predictions.\n\n4Methods such as alpha-beta CROWN (Wang et al., 2021) claim to be complete even when they are Abstract-based because the abstraction can be controlled to be as precise as the original activation function, thus reducing the method back to a complete one.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlex B ̈auerle, Daniel J ̈onsson, and Timo Ropinski. Neural activation patterns (naps): Visual explainability of learned concepts, 2022. URL https://arxiv.org/abs/2206.10611.\n\nPatrick Cousot and Radhia Cousot. Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints. In Proceedings of the 4th ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pp. 238–252, 1977.\n\nThomas G. Dietterich and Eric Horvitz. Rise of concerns about AI: reflections and directions.\n\nCommun. ACM, 58(10):38–40, 2015.\n\nD. Erhan, Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Visualizing higher-layer features\n\nof a deep network. 2009.\n\nXiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In Rupak Majumdar and Viktor Kunˇcak (eds.), Computer Aided Verification, pp. 3–29, Cham, 2017. Springer International Publishing. ISBN 978-3-319-63387-9.\n\nXiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability. Computer Science Review, 37:100270, 2020. ISSN 1574-0137. doi: https://doi.org/10.1016/j.cosrev.2020.100270. URL https:// www.sciencedirect.com/science/article/pii/S1574013719302527.\n\nGuy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In Rupak Majumdar and Viktor Kunˇcak (eds.), Computer Aided Verification, pp. 97–117, Cham, 2017. Springer International Publishing. ISBN 978-3-319-63387-9.\n\nGuy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic, David L. Dill, Mykel J. Kochenderfer, and Clark W. Barrett. The marabou framework for verification and analysis of deep neural networks. In CAV (1), volume 11561 of Lecture Notes in Computer Science, pp. 443–452. Springer, 2019.\n\nJohn A. Nelder and Roger Mead. A simplex method for function minimization. Computer Journal,\n\n7:308–313, 1965.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 1135–1144, 2016.\n\nH. G. Rice. Classes of recursively enumerable sets and their decision problems. Transactions ISSN 00029947. URL http:\n\nof the American Mathematical Society, 74(2):358–366, 1953. //www.jstor.org/stable/1990888.\n\nRamprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra. Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization. CoRR, abs/1610.02391, 2016. URL http://arxiv.org/abs/ 1610.02391.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings, 2014. URL http://arxiv.org/ abs/1312.6034.\n\nGagandeep Singh, Timon Gehr, Markus P ̈uschel, and Martin Vechev. An abstract domain for certifying neural networks. Proc. ACM Program. Lang., 3(POPL), jan 2019. doi: 10.1145/3290354. URL https://doi.org/10.1145/3290354.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nHoang-Dung Tran, Neelanjana Pal, Patrick Musau, Diego Manzanas Lopez, Nathaniel Hamilton, Xiaodong Yang, Stanley Bak, and Taylor T. Johnson. Robustness verification of semantic segIn Computer Aided Verification: 33rd mentation neural networks using relaxed reachability. International Conference, CAV 2021, Virtual Event, July 20–23, 2021, Proceedings, Part I, pp. 263–286, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 978-3-030-81684-1. doi: 10.1007/ 978-3-030-81685-8 12. URL https://doi.org/10.1007/978-3-030-81685-8_ 12.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.\n\nVNNCOMP. Vnncomp, 2021. URL https://sites.google.com/view/vnn2021.\n\nShiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. Beta-CROWN: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification. Advances in Neural Information Processing Systems, 34, 2021.\n\nJeannette M. Wing. A specifier’s introduction to formal methods. Computer, 23(9):8–24, 1990.\n\nJeannette M. Wing. Trustworthy AI. Commun. ACM, 64(10):64–71, 2021.\n\nHan Xu, Yao Ma, Haochen Liu, Debayan Deb, Hui Liu, Jiliang Tang, and Anil K. Jain. Adversarial attacks and defenses in images, graphs and text: A review. International Journal of Automation and Computing, 17:151–178, 2020.\n\nJason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization, 2015. URL https://arxiv.org/abs/1506.06579.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nv0 = 0.1x0 − 0.6x1\n\nv1 = −4.3x0 + 4.4x1\n\nv2 = 4.2x0 − 4.2x1\n\nv3 = max(v0, 0)\n\nv4 = max(v1, 0)\n\nv5 = max(v2, 0)\n\nv0 = 0.1x0 − 0.6x1\n\nv1 = −4.3x0 + 4.4x1\n\nv2 = 4.2x0 − 4.2x1\n\nv3 = v0\n\nv4 = max(v1, 0)\n\nv5 = 0\n\ny0 = 0.4v3 − 4.9v4 + 3.9v5 + 6.7\n\ny1 = −0.4v3 + 3.9v4 + 4.6v5 − 7.4\n\ny0 = 0.4v4 − 4.9v5 + 3.9v6 + 6.7\n\ny1 = −0.4v4 + 3.9v5 + 4.6v6 − 7.4\n\nx0 ≤ 0.1 ∧ x0 ≥ 0.02\n\nx1 ≤ 0.1 ∧ x1 ≥ 0.02\n\n0 < y0 − y1\n\nx0 ≤ 0.3 ∧ x0 ≥ 0\n\nx1 ≤ 0.3 ∧ x1 ≥ 0\n\nv0 ≥ 0\n\nv2 ≤ 0\n\n(a) XNET: A NN that computes the analog XOR function.\n\n(b) Marabou’s system of constraints for verifying that XNET is 0.04-robust at (0.06, 0.06)\n\n(c) Check if P 1 = ((z0), ()) and P 0 = ((), (z2)) are non-ambiguous in the first quadrant using Marabou\n\nFigure 4: Using Marabou to verify NAP properties of XNET.\n\nA A RUNNING EXAMPLE\n\nTo help with illustrating later ideas, we present a two-layer feed-forward neural network XNET (Figure 4a) to approximate an analog XOR function f (x0, x1) : [[0, 0.3] ∪ [0.7, 1]]2 → {0, 1} such that f (x0, x1) = 1 iff (x0 ≤ 0.3 ∧ x1 ≥ 0.7) or (x0 ≥ 0.7 ∧ x1 ≤ 0.3). The network computes the function\n\nFXNET(x) = W 1 max(W 0(x) + b0, 0) + b1\n\nwhere x = [x0, x1], and values of W 0, W 1, b0, b1 are shown in edges of Figure 4a. O(x) = 0 if FXNET(x)[0] > FXNET(x)[1], O(x) = 1 otherwise.\n\nNote that the network is not arbitrary. We have obtained it by constructing two sets of 1 000 randomly generated inputs, and training on one and validating o the other until the NN achieved a perfect F1-score of 1.\n\nB OTHER EVALUATIONS\n\nB.1 L1-NORMS OF DISTANCE\n\nFigure 5 shows the distributions of L1-norms of all image pairs from the same class, similar to Fig. 3, the distances between image pairs from class 1 are much smaller compare to other classes.\n\nB.2 OVERLAP RATIO\n\nTable 5: The maximum overlap ratio for each label (class) on a given δ.NAP for MNIST. Each cell is obtained by maxi |N δ col is the set of neurons in the dominant pattern for the label (class) in the header of the column of the selected cell with the given δ, Ni is the set of neurons in the dominant pattern for the label (class) i with the given δ.\n\ncol| where N δ\n\ni |/|N δ\n\n(cid:84) N δ\n\ncol\n\n0 0.959 0.844 0.864 0.877 0.876 0.893 0.903\n\n1 0.928 0.834 0.885 0.900 0.904 0.922 0.905\n\n2 0.963 0.911 0.909 0.910 0.904 0.913 0.925\n\n3 0.966 0.901 0.904 0.901 0.900 0.912 0.923\n\n0.00 0.01 0.05 0.10 0.15 0.25 0.50\n\n4 0.972 0.881 0.915 0.921 0.919 0.928 0.926\n\n12\n\n5 0.973 0.898 0.908 0.910 0.913 0.925 0.923\n\n6 0.930 0.895 0.899 0.890 0.893 0.905 0.907\n\n7 0.965 0.884 0.897 0.899 0.907 0.916 0.918\n\n8 0.957 0.880 0.890 0.900 0.904 0.916 0.927\n\n9 0.981 0.908 0.893 0.901 0.900 0.913 0.927\n\nx0x1v00.1-0.6v24.2-4.2v3ReLUv5ReLUy00.4-4.7y1-0.44.6v1-4.34.4-4.93.9v4ReLUUnder review as a conference paper at ICLR 2023\n\nFigure 5: The distribution of L1-norms of all image pairs for each class.\n\nFigure 6 shows the heatmap of the overlap ratio between any two classes for 6 δ values. For the grid in each column in a heatmap, the overlap ratio is calculated by the number of overlapping neurons of the NAPs of the class labelled for the row and the column divided by the number of neurons in the NAP of the class labelled for the column, which is why the values in the heatmaps are not symmetric along the diagonal. Based on the shade of the colors in our heapmap, we can see that, during the process of increasing δ, the overlapping ratios decrease first and then increase in general, it might because that, with the loose of restriction on when a neuron is considered as activated/inactivated, more neurons are included in the NAP, which means more constrains, but at the same time, for two NAPs of any two classes, it is more likely that they have more neurons appearing in both NAPs.\n\n(a) δ = 0.00\n\n(b) δ = 0.01\n\n(c) δ = 0.05\n\n(d) δ = 0.10\n\n(e) δ = 0.15\n\n(f) δ = 0.50\n\nFigure 6: Overlap ratio of the dominant pattern of two labels (classes) on a given δ.NAP. Values in each grid are obtained by |N δ col is the set of neurons in the dominant pattern for the label (class) of the column of the selected grid with the given δ, Nrow is the set of neurons in the dominant pattern for the label (class) of the row of the selected grid with the given δ.\n\ncol| where N δ\n\nrow|/|N δ\n\n(cid:84) N δ\n\ncol\n\n13\n\n0123456789class050100150200distance (L1 norm)0123456789label0123456789label10.920.960.960.970.970.930.960.950.980.9210.940.970.960.950.890.960.950.960.810.810.840.830.840.790.830.820.840.880.880.9110.910.940.850.910.920.930.940.930.950.9610.960.910.970.940.980.90.880.920.950.9210.870.920.920.940.960.920.960.960.970.9710.960.960.980.920.920.940.960.960.950.8910.930.970.860.850.870.90.880.890.830.8710.90.90.880.910.930.930.930.870.920.921Ratio of overlap - delta: 0.00123456789label0123456789label10.820.90.860.860.890.890.860.860.870.8110.910.870.850.880.860.870.860.870.760.7810.820.80.80.810.810.810.820.810.820.9110.830.90.840.860.880.890.810.810.90.8410.860.860.880.830.910.820.810.870.880.8410.850.840.860.860.840.820.90.850.860.8710.840.840.850.810.830.90.860.870.860.8410.840.910.820.830.910.90.840.890.860.8510.890.80.80.890.870.880.860.830.880.851Ratio of overlap - delta: 0.010123456789label0123456789label10.820.860.850.840.880.870.840.850.840.7910.870.840.840.840.840.840.840.820.820.8610.850.840.840.860.830.850.820.840.870.910.850.90.870.860.890.870.820.850.860.8310.860.860.860.840.890.840.830.850.860.8410.860.830.860.850.840.830.870.830.850.8610.810.850.820.830.860.870.860.880.860.8410.860.890.860.890.910.90.870.910.90.8710.890.850.860.870.880.920.890.860.90.891Ratio of overlap - delta: 0.050.700.750.800.850.900.951.000123456789label0123456789label10.840.870.850.860.870.870.860.870.860.810.870.850.840.840.850.840.850.830.850.8810.870.860.850.870.850.870.850.850.890.910.860.90.860.880.890.880.840.870.870.8410.870.880.870.860.90.850.850.850.880.8610.870.850.870.870.840.860.870.830.870.8610.830.850.840.850.870.870.870.880.870.8510.870.880.880.90.910.90.880.910.890.8810.90.870.880.880.890.920.90.880.90.91Ratio of overlap - delta: 0.10123456789label0123456789label10.840.870.860.860.870.880.860.870.860.8110.860.850.840.850.850.840.860.840.870.910.890.870.870.890.870.890.860.860.90.910.870.910.870.880.90.890.860.880.870.8610.870.890.880.880.90.860.880.860.890.8710.880.860.90.880.860.870.880.840.880.8710.840.870.850.860.880.870.870.890.880.8610.880.90.880.90.90.90.890.910.890.8810.90.870.890.880.890.920.90.880.910.91Ratio of overlap - delta: 0.150123456789label0123456789label10.860.90.880.880.890.890.880.90.880.8610.90.90.890.890.870.890.910.90.90.910.920.910.910.910.90.920.910.880.90.9210.890.920.890.890.910.910.880.890.910.8910.890.90.90.910.930.890.890.910.920.8910.90.890.920.920.890.870.910.890.90.910.870.910.90.880.890.90.890.90.890.8710.910.920.90.910.920.910.910.920.910.9110.930.880.90.910.910.930.920.90.920.931Ratio of overlap - delta: 0.50.700.750.800.850.900.951.00Under review as a conference paper at ICLR 2023\n\nTable 5 shows the maximum overlap ratio for one class, that is, for one reference class, the maximum overlap ratio between this reference class and any other class. This table is basically extracting the maximum values of each column other than the 1 on the diagonal in our heatmap in Fig. 6, in the each column of our table, it also follows the pattern that the value of overlap ratio decreases first and then increase with the increase of δ.\n\nC MISCLASSIFICATION EXAMPLES\n\nIn this section, we display some interesting exmaples from the the MNIST test set that follow the NAP of some class other than their ground truth, which means these images are misclassified. We consider these samples interesting because, instead of misclassification, it is more reasonable to say that these images are given wrong ground truth from human perspective.\n\n(a) A testing image from MNIST with ground truth 5, classified as 0\n\n(b) A testing image from MNIST with ground truth 8, classified as 0\n\n(c) A testing image from MNIST with ground truth 9, classified as 0\n\nFigure 7: Some interesting test images from MNIST that are misclassified as 0 and also follow the NAP of class 0.\n\n(a) A testing image from MNIST with ground truth 6, classified as 1\n\n(b) A testing image from MNIST with ground truth 7, classified as 1\n\n(c) A testing image from MNIST with ground truth 9, classified as 1\n\nFigure 8: Some interesting test images from MNIST that are misclassified as 1 and also follow the NAP of class 1.\n\n14\n\ndelta=0.0, pred=0, gt=5delta=0.0, pred=0, gt=8delta=0.0, pred=0, gt=9delta=0.0, pred=1, gt=6delta=0.0, pred=1, gt=7delta=0.0, pred=1, gt=9Under review as a conference paper at ICLR 2023\n\n(a) A testing image from MNIST with ground truth 1, classified as 2\n\n(b) A testing image from MNIST with ground truth 3, classified as 2\n\n(c) A testing image from MNIST with ground truth 7, classified as 2\n\nFigure 9: Some interesting test images from MNIST that are misclassified as 2 and also follow the NAP of class 2.\n\n(a) A testing image from MNIST with ground truth 5, classified as 3\n\n(b) A testing image from MNIST with ground truth 7, classified as 3\n\n(c) A testing image from MNIST with ground truth 9, classified as 3\n\nFigure 10: Some interesting test images from MNIST that are misclassified as 3 and also follow the NAP of class 3.\n\n(a) A testing image from MNIST with ground truth 2, classified as 4\n\n(b) A testing image from MNIST with ground truth 6, classified as 4\n\n(c) A testing image from MNIST with ground truth 9, classified as 4\n\nFigure 11: Some interesting test images from MNIST that are misclassified as 4 and also follow the NAP of class 4.\n\n15\n\ndelta=0.0, pred=2, gt=1delta=0.0, pred=2, gt=3delta=0.0, pred=2, gt=7delta=0.0, pred=3, gt=5delta=0.0, pred=3, gt=7delta=0.0, pred=3, gt=9delta=0.0, pred=4, gt=2delta=0.0, pred=4, gt=6delta=0.0, pred=4, gt=9Under review as a conference paper at ICLR 2023\n\n(a) A testing image from MNIST with ground truth 3, classified as 5\n\n(b) A testing image from MNIST with ground truth 6, classified as 5\n\n(c) A testing image from MNIST with ground truth 8, classified as 5\n\nFigure 12: Some interesting test images from MNIST that are misclassified as 5 and also follow the NAP of class 5.\n\n(a) A testing image from MNIST with ground truth 0, classified as 6\n\n(b) A testing image from MNIST with ground truth 3, classified as 6\n\n(c) A testing image from MNIST with ground truth 5, classified as 6\n\nFigure 13: Some interesting test images from MNIST that are misclassified as 6 and also follow the NAP of class 6.\n\n(a) A testing image from MNIST with ground truth 2, classified as 7\n\n(b) A testing image from MNIST with ground truth 3, classified as 7\n\n(c) A testing image from MNIST with ground truth 9, classified as 7\n\nFigure 14: Some interesting test images from MNIST that are misclassified as 7 and also follow the NAP of class 7.\n\n16\n\ndelta=0.0, pred=5, gt=3delta=0.0, pred=5, gt=6delta=0.0, pred=5, gt=8delta=0.0, pred=6, gt=0delta=0.0, pred=6, gt=2delta=0.0, pred=6, gt=5delta=0.0, pred=7, gt=2delta=0.0, pred=7, gt=3delta=0.0, pred=7, gt=9Under review as a conference paper at ICLR 2023\n\n(a) A testing image from MNIST with ground truth 1, classified as 8\n\n(b) A testing image from MNIST with ground truth 3, classified as 8\n\nFigure 15: Some interesting test images from MNIST that are misclassified as 8 and also follow the NAP of class 8.\n\n(a) A testing image from MNIST with ground truth 4, classified as 9\n\n(b) A testing image from MNIST with ground truth 7, classified as 9\n\nFigure 16: Some interesting test images from MNIST that are misclassified as 9 and also follow the NAP of class 9.\n\n17\n\ndelta=0.0, pred=8, gt=1delta=0.0, pred=8, gt=3delta=0.0, pred=9, gt=4delta=0.0, pred=9, gt=7",
  "translations": [
    "# Summary Of The Paper\n\n###  Problem\nThe paper tackles the problem of neural network verification, which aims to give guarantees of whether a trained NN follows certain specifications.\n\n###  Proposed method\nThe authors claim that the current adversarial robustness specification used by the verification community is improper. Their claim is that this specification does not allow verification of test set inputs as they lie outside the verified region.\n\nAuthors propose neural representation as specification. Essentially, they propose to use neural activation patterns as specifications.\n\n### Experimental Validation\nExperiments are conducted on the MNIST dataset, where it shows that with the proposed specification, the certified region can go well beyond adversarial examples and even verify test set inputs.\n\n### My one line understanding\nAuthors claim to have found a condition which:\n1. adversarial examples don't follow, so verification won't stop at them and will be able to go beyond\n2. test examples from same class follow, so test set examples can be verified\n\n# Strength And Weaknesses\n\n## STRENGTHS\n1. Finding alternative specifications is a good direction to explore\n\n## WEAKNESSES\n\n1. Writing needs to be improved\n- Not put into context properly: Doing adversarial verification is not necessarily an *overfitted* specification, as you call it. There are many applications where we also want to verify against adversarial examples. I don't think you need to belittle previous work. Your work is complementary, and that is how you should write the paper.\n- I don't agree with the entire 'data as specification' vs 'neural representation as specification' angle.  Because if one looks at equations in Sec3.2, you have the same formulation, but an extra constraint/specification. So you can write we add a new specification, instead of saying one vs the other.\n- Many equations are not numbered. How should I refer to them?\n\n2. Strong and probably incorrect assumptions\n- What is the guarantee that test examples will definitely follow same NAP?\n- What is the guarantee that adversarial examples will not follow NAP\n- Isn't such an assumption against the whole point of verification?\n\n3. Weak experimental validation\n- One main limitation of adv verification methods is their inability to scale beyond small epsilons. This is especially critical for your work because for going upto the test inputs, you would require large epsilon. Current methods don't allow that. So your method might not work on bigger datasets at all. Whereas, adv verification will still work on big datasets with small epsilon. \n- The experiments are only conducted on the MNIST dataset. I feel that MNIST is too easy and that is why your assumptions hold there. Since your assumptions are so strong, can you validate them on a bigger dataset?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work is Novel.\n\nBut I doubt the correctness of the claims and lack of proper experimental validation. Please see my detailed answer in the previous question.\n\nQUESTION:\n\nIf you find a specification which all same class test examples follow, have you not solved ML?  If you have such a constraint, then just use that to make class prediction and your accuracy will be 100% ($- \\delta$)?\n\n# Summary Of The Review\n\nIt is important to look for alternate specification.\n\nBut the assumptions made in this work feel incorrect and are not properly validated, so I cannot draw a conclusion from the current version.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents a novel specification paradigm called \"neural representation as specification\" (using Neural Activation Patterns, NAPs) to improve the reliability and robustness of neural network behavior. The authors argue that traditional data-centric specifications, which focus on local neighborhoods of reference inputs, lead to overfitting and fail to generalize effectively to unseen data. By introducing dominant NAPs that characterize consistent neural activations among inputs of the same class and developing a method to mine these patterns from training data, the authors demonstrate that NAPs enable formal verification of classification properties, achieving coverage of 84% of testing data compared to 0% with traditional methods. The methodology further includes the use of existing neural network verification tools to validate properties like non-ambiguity and robustness against adversarial inputs.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to addressing the limitations of existing specification methods for neural networks. The introduction of NAPs represents a significant shift towards leveraging the intrinsic properties of neural activations, which enhances the robustness and reliability of verification processes. The empirical results are compelling, showing a marked improvement in verification coverage and robustness against adversarial examples. However, a notable weakness is the limited scope of the evaluation, which is primarily based on the MNIST dataset. While the results are promising, further validation on more complex datasets and architectures would strengthen the claims made by the authors.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with neural network verification. The methodology is detailed, allowing for reproducibility, particularly in the mining of NAPs and the verification process. The novelty of the approach is significant, as it provides a fresh perspective on neural network specifications. However, the paper could benefit from a more comprehensive discussion on potential limitations and future work, particularly regarding the application of NAPs to larger architectures beyond MNIST.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of neural network verification by introducing NAPs as a robust specification method. The empirical results demonstrate the effectiveness of this approach, although further exploration in more complex settings is warranted. The clarity and quality of the presentation enhance the paper's impact, making it a valuable addition to the literature.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach called \"neural representation as specification\" (NRAS) utilizing neural activation patterns (NAPs) to enhance the verification, robustness, and interpretability of neural networks. By defining NAPs as pairs of activated and deactivated neurons, the authors demonstrate that these patterns can generalize effectively across different inputs, leading to improved coverage and reliability for unseen data. The empirical findings highlight that NAPs can certify up to 84% of previously unseen test images, significantly outperforming traditional data-based specifications, which often yield 0% coverage.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to neural network verification through the introduction of NAPs, which offers a significant improvement in coverage for unseen inputs. The statistical analysis employed to validate the efficacy of NAPs is thorough and adds credibility to the findings. However, the study is limited by its reliance on the MNIST dataset, which may not encompass the complexities of more challenging datasets. Additionally, the sensitivity of the δ parameter in δ-relaxed dominant NAPs may present challenges in practical applications, and there is a lingering risk of overfitting.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the concepts of NAPs and their implications accessible to readers. The quality of the experiments is high, with appropriate methods and evaluation metrics employed. The novelty of the approach is evident, as it presents a fresh perspective on the verification of neural networks. However, while the methodology is reproducible, the dependence on specific datasets and parameters may limit broader applicability.\n\n# Summary Of The Review\nOverall, the paper makes a compelling case for the use of neural activation patterns as a robust specification method for neural networks, demonstrating substantial improvements in verification capabilities over traditional methods. Despite some limitations, particularly regarding dataset dependency and parameter sensitivity, the findings have significant implications for future research and applications in AI safety and interpretability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Toward Reliable Neural Specifications\" introduces a novel specification framework termed Neural Activation Patterns (NAPs), which aims to overcome the limitations of traditional \"data as specification\" approaches in verifying neural network correctness, robustness, and interpretability. The authors highlight the issue of overfitting in existing specifications and propose that NAPs, which utilize neural activation patterns to characterize predictions, can provide broader coverage in the input space. Experimental results demonstrate that NAPs can certify robustness for up to 84% of testing images in the MNIST dataset, significantly outperforming traditional methods that yield 0% coverage.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to leveraging neural activation patterns for specification, which addresses the critical issue of overfitting in existing methods. The empirical evaluation on the MNIST dataset effectively illustrates the potential of NAPs to enhance robustness verification. However, the paper also presents weaknesses, such as the reliance on a single dataset for experiments, which may limit the generalizability of the findings. Additionally, the discussion on future work could be expanded to include more specific directions for exploring other neural architectures beyond simple feed-forward networks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a well-structured presentation of concepts and methodologies. The quality of the writing is high, but could benefit from more detailed explanations in certain sections, particularly in the background on adversarial attacks and the nuances of NAP properties. The novelty of the proposed NAP framework is significant, as it represents a departure from traditional specification methods. Reproducibility is facilitated by the clear description of methodologies and experimental setups, although additional details about implementation and parameter choices would strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new approach to neural network specification through the introduction of NAPs, demonstrating substantial empirical evidence of its effectiveness. While the contributions are noteworthy, the work could be strengthened by broader validation on diverse datasets and more comprehensive future research directions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel specification paradigm termed \"neural representation as specification\" (NAPs), which shifts the focus from traditional data-as-specification methods to utilizing neural activation patterns for neural network verification. The authors empirically demonstrate that NAPs can successfully cover up to 84% of unseen testing data, a significant improvement over the 0% coverage achieved by conventional approaches. Additionally, the paper presents a formal verification framework for properties related to misclassification, asserting a strong theoretical foundation for NAPs and claiming that they provide a no-ambiguity property for label differentiation.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach and solid empirical validation, which showcases the potential of NAPs for improving neural network interpretability and reliability. The formal verification claims further enhance the contribution by providing a theoretical backdrop to the empirical findings. However, the study has notable weaknesses, including the reliance on a single dataset (MNIST), which raises questions about the generalizability of the findings to more complex datasets. Moreover, the computational demands of the verification process and the sensitivity to the parameter δ for precision could hinder practical application in diverse settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology and findings, making it accessible to readers with varying levels of expertise. The quality of the empirical data and statistical analysis is commendable, although there are concerns regarding the robustness of these statistics in different contexts. The novelty of the approach is significant, as it proposes a fresh perspective on neural network verification. However, the reproducibility of results may be limited by the specificity of the datasets used and the computational resources required for the verification process.\n\n# Summary Of The Review\nThis paper presents a novel approach to neural network verification through the introduction of neural activation patterns, supported by strong empirical evidence and formal verification claims. Despite its innovative contributions, the paper's limitations regarding dataset generalizability, computational scalability, and parameter sensitivity warrant further investigation to fully establish the method's practicality and effectiveness in real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel framework termed \"neural behavior as specification\" (NBAS), which shifts the focus of neural network specifications from data-centric approaches to behavioral patterns derived from neural activations. Key contributions include the development of a systematic method for extracting dominant behavioral signatures (DBS) that encapsulate the operational characteristics of neural networks, a statistical framework for verifying these signatures, and empirical demonstrations of their effectiveness on the MNIST dataset. The authors claim that their approach allows for broader verification of properties such as robustness and non-ambiguity, achieving a significant improvement in performance when certifying unseen data.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to neural network specifications, addressing the limitations of prior data-centric methods. The introduction of DBS provides a new avenue for understanding and verifying neural network behavior, which is crucial for advancing the reliability of AI systems. The empirical results are compelling, showcasing a notable improvement in verification performance. However, the paper could benefit from a more detailed exploration of the computational complexity involved in extracting DBS and the potential challenges in applying this methodology to more complex architectures beyond the MNIST dataset.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The novel framework and methods are presented with sufficient detail to allow for reproducibility, although the specific implementation details could be elaborated further. The novelty of the approach is significant, as it challenges traditional paradigms and proposes a more robust alternative. However, future work is necessary to validate the approach on diverse architectures and datasets, as well as to assess its scalability.\n\n# Summary Of The Review\nThis paper makes a noteworthy contribution to the field of neural network verification by advocating for a behavioral approach to specifications. The proposed methods demonstrate substantial improvements in verification performance on unseen data and offer a promising direction for enhancing the reliability of AI systems. Further exploration of the scalability and application of this framework to more complex models will be essential for its broader impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Toward Reliable Neural Specifications\" introduces a novel approach to adversarial training by leveraging Neural Activation Patterns (NAPs) to enhance the robustness of neural networks against adversarial attacks. The authors argue that traditional adversarial training methods, which often focus on local perturbations, tend to overfit and lack generalization to unseen data. The methodology involves defining NAPs that can cover a broader set of inputs, leading to improved robustness. Empirical results on the MNIST dataset demonstrate that the proposed approach significantly outperforms traditional methods in terms of adversarial resilience.\n\n# Strength And Weaknesses\n**Strengths**:\n1. **Novel Concept**: The introduction of NAPs provides a unique framework for adversarial robustness that diverges from conventional perturbation strategies.\n2. **Empirical Validation**: The experiments confirm that NAPs can generalize well across different inputs, showcasing their potential to enhance model robustness.\n3. **Theoretical Insights**: The paper advances the theoretical understanding of adversarial training by connecting NAPs to neural network activation dynamics.\n4. **Real-World Applicability**: By focusing on NAPs, the findings suggest a pathway to making neural networks more resilient in unpredictable environments.\n\n**Weaknesses**:\n1. **Narrow Evaluation**: The reliance on the MNIST dataset limits the assessment of the proposed method's effectiveness in more complex scenarios.\n2. **Lack of Comparison**: Insufficient comparative analysis with existing adversarial training techniques weakens the claims of superiority.\n3. **Parameter Clarity**: The δ parameter for NAP relaxation is introduced without clear guidance for practical selection, raising concerns about reproducibility.\n4. **Overfitting Risk**: The potential for overfitting remains, particularly if NAPs are not adequately mined, necessitating further discussion on mitigation strategies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-structured sections and clear explanations of NAPs. However, the lack of detailed comparisons with existing methods detracts from the overall quality. The novelty of the approach is commendable, as it introduces a fresh perspective on adversarial training. Nonetheless, the reproducibility may be hindered due to ambiguous parameter settings and the limited scope of empirical validation.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new approach to adversarial training through the lens of Neural Activation Patterns. While it demonstrates significant potential and contributes to the theoretical framework of robustness, further empirical validation on diverse datasets and comprehensive comparisons with existing techniques are necessary to establish its broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel framework termed Neural Representation as Specification (NRS), which proposes the use of neural activation patterns (NAPs) as a comprehensive solution for the verification of neural networks. The authors argue that traditional data-as-specification methods are inadequate and present NAPs as a robust alternative capable of ensuring formal verification across the entire input space of neural networks. They claim that NAPs can significantly improve robustness against adversarial attacks and eliminate classification ambiguity, and they present empirical results suggesting that NAPs can successfully verify a high percentage of unseen data, framing this as a substantial achievement for AI verification.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious attempt to redefine the landscape of AI verification through the introduction of NAPs, which promises to address significant limitations in existing methods. The empirical results showing an 84% verification rate for unseen images are compelling and suggest potential practical applications. However, the paper's claims about the universality and reliability of NAPs lack sufficient empirical validation and may overstate the limitations of current approaches. Additionally, the assertion that NAPs can guarantee safety against all adversarial perturbations is overly optimistic and neglects the nuanced challenges associated with adversarial robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas clearly, with a logical flow from introduction to conclusion. However, some claims, particularly regarding the completeness and reliability of NAPs, would benefit from more rigorous substantiation. The methodology for mining NAPs is described as simple, yet it lacks detailed evaluation and may raise questions about reproducibility across different neural network architectures. Overall, while the novelty of the proposed framework is evident, the quality of the empirical validation and the depth of the analysis could be improved to enhance reproducibility.\n\n# Summary Of The Review\nWhile the paper proposes an innovative framework in NRS and presents promising empirical results, it makes several bold claims that require more rigorous validation. The potential implications of this work for AI verification are significant, though the assertions about NAPs' capabilities may be overstated and need further exploration.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel paradigm called Neural Representation as Specification (NAP), which leverages neural activation patterns instead of traditional input data to enhance the reliability of specifications in AI systems. The authors demonstrate that NAPs can significantly improve verification outcomes, achieving coverage of over 90% of testing images, a substantial leap from the 0% coverage seen with conventional data-as-specification methods. Through empirical evaluations, the paper illustrates that NAPs not only increase the range of inputs that can be verified but also maintain non-ambiguity among labels, thereby improving classification confidence.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to specification through neural activation patterns, which addresses the limitations of existing data-as-specification methods that often lead to poor generalization. The empirical results provide compelling evidence of the method's effectiveness, particularly in demonstrating its ability to cover a significantly larger fraction of test data. However, the paper could benefit from further exploration of the scalability of NAPs, particularly in more complex models beyond simple feed-forward networks. Additionally, while the results are promising, the theoretical foundations supporting the robustness claims could be elaborated to strengthen the argument.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the proposed methodology. The experimental design is sound, and results are presented in a manner that is easy to understand, although additional details on the experimental setup would enhance reproducibility. The novelty of the NAP approach is significant, as it shifts the focus from data-centric specifications to a more intrinsic representation of the model's behavior. This could have implications for various applications in AI, although the paper could improve its reproducibility by providing more detailed algorithms or code.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the way specifications for AI systems can be formulated, demonstrating the potential of neural activation patterns to enhance verifiability and robustness. While there are some areas for improvement, particularly in scalability and theoretical depth, the contributions are noteworthy and have the potential to influence future research.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Toward Reliable Neural Specifications\" critiques the existing paradigm of treating data as the primary specification for neural networks, positing that this approach often leads to overfitting and impracticality in real-world settings. The authors introduce the concept of Neural Activation Patterns (NAPs) as a more reliable specification method, asserting their capability to generalize beyond training data and effectively cover misclassification properties. The methodology involves mining NAPs from the training data and employing statistical thresholds for verification, with the findings suggesting that NAPs can enhance model interpretability and reliability.\n\n# Strength And Weaknesses\nThe paper presents several notable contributions, particularly the introduction of NAPs as an alternative to data-centric approaches. This shift has the potential to reduce overfitting and improve generalization. However, the assumptions underlying the effectiveness of NAPs are concerning. The authors do not convincingly address the generalizability of NAPs to unseen data, nor do they adequately consider the variability of neural network behavior across diverse inputs. Additionally, the statistical methods employed for NAP mining raise questions about their robustness across different datasets, and the reliance on the Marabou verifier could prove limiting under various architectures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a structured presentation of ideas. However, some assumptions could benefit from more thorough explanations and empirical support. The novelty of the proposed NAP approach is significant in challenging traditional paradigms, but the practical implications and reproducibility of the results remain to be established, particularly concerning the scalability of NAPs to complex neural architectures.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing shift in the specification of neural networks through the introduction of NAPs. While the approach is novel and potentially impactful, the underlying assumptions and lack of empirical validation raise concerns about its robustness and applicability in various contexts.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving the verification of neural networks by introducing Neural Representation as Specification (NRAS), which leverages Neural Activation Patterns (NAPs). The authors argue that existing data-as-specification paradigms often lead to overfitting and are inadequate for robust verification. They propose that NAPs serve as a more effective specification mechanism, enabling formal verification of misclassification properties. The methodology includes defining dominant NAPs, mining relaxed dominant NAPs, and validating their effectiveness through empirical experiments on the MNIST dataset, demonstrating improved verification across broader input spaces.\n\n# Strength And Weaknesses\nStrengths of this paper include its innovative approach to verification through NAPs, which address significant limitations of existing methods. The formal definition and mining process of NAPs are well-articulated, and the empirical results support the proposed methodology effectively. However, the paper could benefit from a more detailed comparison with other verification methods beyond those mentioned, as well as clearer explanations of the computational complexity involved in the mining process. Additionally, while the MNIST dataset is a relevant test case, including experiments on more complex datasets or architectures could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, with a clear structure that facilitates reader comprehension. The definitions and concepts are presented in a logical flow, making it accessible to audiences familiar with neural network verification. The quality of the empirical results is commendable; however, further details on the experimental setup and parameters would enhance reproducibility. The novelty of the approach is significant, as it redefines how specifications can be approached in the context of neural networks, emphasizing intrinsic properties over traditional input-based methods.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of neural network verification by introducing a novel specification framework based on NAPs. While the approach is innovative and the empirical results are promising, further exploration of its applicability in more complex scenarios and enhanced clarity in methodology could improve its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper addresses a significant problem in machine learning regarding the specification and verification of models. It introduces a novel specification paradigm that aims to enhance the reliability and interpretability of machine learning systems. The authors present a solid theoretical foundation for their proposed approach, demonstrating its relevance through practical implications and promising experimental results.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative concept and comprehensive theoretical discussion, which highlight the limitations of existing methodologies while offering a viable alternative. The practical relevance of the findings is notable, suggesting a potential impact on real-world applications. However, weaknesses include a lack of clarity in the methodology and insufficient detail in the experimental evaluation, particularly regarding the comparison with baseline methods and the specifics of the experimental setup, which could limit reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe overall clarity of the paper is good, though some sections could use more straightforward explanations to make complex ideas more accessible to a broader audience. The quality of the writing is generally high, but the inclusion of clearer visual aids and more detailed descriptions of experiments would enhance the paper's quality and reproducibility. The novelty of the proposed approach is significant, positioning it well within the current literature, yet the empirical evaluation could benefit from deeper comparisons with existing methods.\n\n# Summary Of The Review\nThis paper presents a compelling and innovative approach to model specification in machine learning, backed by solid theoretical foundations and promising experimental results. While the contributions are significant, improvements in clarity, detail in methodology, and empirical validation would strengthen the work further.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"Toward Reliable Neural Specifications\" proposes a new paradigm for specifying the correctness and robustness of neural networks, moving away from the conventional \"data as specification\" approach to a more model-centric method termed \"neural representation as specification.\" The authors introduce the concept of neural activation patterns (NAPs), which capture the intrinsic characteristics of neural networks and are posited to be more effective in covering a broader range of training and testing data points. Through their methodology, the authors demonstrate that NAPs not only address limitations inherent in traditional specifications but also enhance the verification capabilities of neural networks, thereby improving their reliability and safety in real-world applications.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to specification, which is rooted in the intrinsic properties of neural networks rather than solely on data, thus potentially mitigating issues of overfitting. The introduction of NAPs is compelling and offers a fresh perspective on neural network verification, with claims of improved robustness and interpretability. However, the paper does not provide extensive empirical validation of the NAPs in diverse real-world scenarios, which could limit the generalizability of its findings. Additionally, the complexity of mining relaxed dominant NAPs may pose practical challenges for implementation in existing systems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its main ideas, making it accessible to readers familiar with the field of AI verification. The novelty of the proposed NAPs is significant, providing a fresh lens through which to view neural network specifications. However, while the methodology is described adequately, details regarding the reproducibility of the findings could be enhanced, particularly in terms of empirical validation and computational requirements for mining NAPs.\n\n# Summary Of The Review\nOverall, this paper presents a novel and potentially impactful contribution to the field of neural network verification through the introduction of neural activation patterns as a specification method. While the theoretical underpinnings and proposed methodology are strong, further empirical validation is necessary to fully establish the efficacy and practicality of NAPs in diverse applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel approach to specifying neural network behaviors through \"neural activation patterns\" (NAPs), which aim to enhance the correctness, robustness, and interpretability of AI systems. The methodology involves mining dominant and δ-relaxed dominant NAPs from training data to ensure comprehensive coverage of the input space, addressing the limitations of existing specifications that often overfit to local neighborhoods. The findings demonstrate that NAPs can effectively verify properties and improve performance on unseen data, achieving verification of up to 84% of the testing dataset.\n\n# Strength And Weaknesses\nThe paper presents a significant advancement in the specification of neural networks by proposing NAPs, which cover larger input regions compared to traditional methods, thus providing a more robust framework for verification. The approach's flexibility through δ-relaxed dominant NAPs is a notable strength, as it allows for broader applicability across different scenarios. However, the reliance on statistical methods for mining NAPs may raise questions regarding the generalizability of results across various architectures or datasets. Additionally, while the paper discusses future directions, a deeper exploration of how NAPs can be integrated into state-of-the-art models would strengthen the proposal.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The quality of writing is high, making complex concepts accessible. The novelty of the NAP framework is evident, as it addresses critical gaps in existing verification techniques. While the experiments validate the approach, the reproducibility of results may depend on the specific implementation details and datasets used, which are not exhaustively discussed.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to improving the verification of neural network behaviors through NAPs. It offers significant contributions to the field of AI safety and robustness, although further exploration of its applicability across diverse architectures is warranted.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"TOWARD RELIABLE NEURAL SPECIFICATIONS\" addresses the critical issue of verifying AI systems through the introduction of neural activation patterns (NAPs) as a novel method for neural network specification. The authors propose a framework that formalizes NAPs and demonstrates their implementation using the Marabou verification tool. Empirical results indicate that the proposed method enhances the reliability of neural network specifications, showing improvements in both coverage of testing images and the effectiveness of the NAPs in various scenarios.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of gaps in existing verification methods and its innovative approach to using neural activation patterns as specifications. The methodology is well-defined, and the empirical results provide strong evidence for the efficacy of the proposed approach. However, the paper could benefit from a deeper exploration of potential limitations and ethical implications related to the deployment of reliable specifications in AI systems. Additionally, certain technical sections may be challenging for readers unfamiliar with the specific jargon and concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, with a clear flow of ideas. While most sections are accessible, some technical details may require careful reading, potentially hindering reproducibility for a wider audience. The original contribution of introducing NAPs is significant and provides a fresh perspective on an important issue in neural network verification. The methodology is detailed enough to allow for reproduction, though additional clarification in complex areas could enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a novel and promising approach to improving the reliability of neural network specifications through the use of neural activation patterns. While the methodology and findings are compelling, greater attention to clarity in technical details and ethical considerations would strengthen the overall impact of the work.\n\n# Correctness\n4/5 - The proposed methodology appears sound based on the provided data and verification results, though some technical aspects could benefit from further elaboration.\n\n# Technical Novelty And Significance\n5/5 - The introduction of neural activation patterns as a means of specification represents a significant advancement in the field of AI verification, addressing a critical gap in existing literature.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results support the proposed method and suggest its applicability in real-world scenarios, though further exploration of limitations and broader implications could enhance significance.",
    "# Summary Of The Paper\nThe paper introduces a novel framework for formal verification of neural networks through the concept of **Neural Activation Patterns (NAPs)**, which serve as intrinsic specifications for correctness and robustness. It critiques the existing **data as specification** paradigm for its tendency to overfit, leading to limited generalizability in verification tasks. The authors propose an alternative approach utilizing dominant NAPs to define correctness in a more expansive manner. Empirical evaluations demonstrate that NAPs provide substantial coverage of training and testing data while maintaining high recall rates, indicating their effectiveness in mitigating misclassification risks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to leveraging intrinsic neural properties for verification, which addresses significant limitations of traditional methods. The formal definition and rigorous methodology for extracting NAPs contribute to the robustness of the proposed framework. However, a potential weakness is the complexity involved in the computation of dominant and relaxed dominant NAPs, which may pose challenges for practical implementation. Additionally, while the paper references future directions, it would benefit from more detailed discussions on potential limitations and the scope of application.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation and methodology behind the proposed NAP framework. The quality of the writing is high, with thorough explanations of concepts and methodologies. The novelty is significant, as it shifts the paradigm of verification from data-centric to neural property-centric specifications. The reproducibility is supported by a clear presentation of methodologies, though additional implementation details would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to formal verification in neural networks through the introduction of NAPs. The methodology addresses critical limitations in existing paradigms, and the empirical results suggest substantial improvements in verification coverage and robustness.\n\n# Correctness\n4/5 - The proposed methods are logically sound and supported by empirical evidence, though some implementation complexities may affect practical correctness.\n\n# Technical Novelty And Significance\n5/5 - The introduction of NAPs represents a significant advancement in the field, providing a new lens through which to view the verification of neural networks.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results are promising, demonstrating the effectiveness of NAPs; however, the generalizability of these results across diverse architectures and tasks remains to be fully assessed.",
    "# Summary Of The Paper\nThe paper introduces Neural Activation Patterns (NAPs) as a novel specification paradigm aimed at improving model verification in neural networks. The methodology involves mining dominant NAPs using a statistical approach and validating their effectiveness primarily on the MNIST dataset. The authors claim that NAPs enhance the data-as-specification paradigm, reporting an 84% coverage of testing images, and propose the concept of δ-relaxed dominant NAPs to address precision-recall trade-offs in model evaluation.\n\n# Strength And Weaknesses\nThe paper presents the innovative concept of NAPs, which could potentially advance the current methodologies in model specification. However, the complexity of the NAP mining process and the reliance on a statistical framework raise concerns about implementation challenges and generalizability. The empirical validation is limited to the MNIST dataset, which may not adequately reflect real-world applications. Furthermore, the paper inadequately supports its claims regarding improvements over traditional methods, and the δ-relaxed dominant NAPs introduce confusion without sufficiently addressing the implications of the precision-recall trade-off.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by convoluted definitions and processes related to NAPs, making it challenging to grasp the core contributions fully. While the introduction of NAPs is technically novel, the paper fails to convincingly demonstrate their significance compared to existing methods. Additionally, the dependency on the Marabou verification tool raises questions about reproducibility across different verification frameworks. The exploration of limitations, particularly regarding adversarial examples, is insufficient, indicating a lack of thorough consideration for the practical challenges in applying NAPs.\n\n# Summary Of The Review\nOverall, the paper presents an interesting concept in NAPs but lacks the empirical evidence and clarity needed to substantiate its claims. The complexity of the proposed methodology and the limitations in generalizability and applicability hinder its impact on the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Neural Representation as Specification (NRAS), which utilizes neural activation patterns (NAPs) to enhance the verification and reliability of neural networks. The authors demonstrate that NAPs can certify up to 84% of unseen test images, significantly expanding the input space coverage compared to traditional methods. Additionally, the framework allows for formal verification of critical properties, thereby addressing the pressing need for trust in AI systems, particularly in high-stakes applications such as healthcare and finance. The paper also highlights the statistical insights gained through NAPs, showing their capacity to generalize across datasets and their compatibility with existing verification tools.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in the innovative NRAS framework and its impressive empirical results, particularly regarding verification across a broad input space. The ability to formalize predictions and enhance safety measures represents a significant advancement in AI reliability. However, a potential weakness could be the reliance on the integration of NAPs with existing tools, which may limit the framework’s standalone applicability. Additionally, while the statistical analysis is promising, further exploration of its implications could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the contributions and methodologies. The novelty of introducing NAPs as a means of specifying and verifying neural networks is substantial, and the findings provide a solid foundation for future research. The reproducibility of the results appears feasible, especially since the framework is designed to integrate with established verification tools like Marabou, allowing others to build on this work effectively.\n\n# Summary Of The Review\nOverall, the paper presents a groundbreaking approach to verifying neural networks through the use of neural activation patterns, marking a significant step toward reliable AI systems. The findings are robust and hold great potential for both theoretical and practical applications in AI verification, safety, and trust.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for neural network specification called Neural Activation Patterns (NAPs), moving away from the conventional \"data as specification\" approach. The authors argue that NAPs, which are defined as unique configurations of neuron activations corresponding to correct classifications, can enhance the generalization and robustness of neural networks. The findings suggest that dominant NAPs can effectively encapsulate the essential properties of neural networks, enabling formal verification across expansive input spaces while addressing issues like overfitting and limited coverage prevalent in traditional approaches.\n\n# Strength And Weaknesses\nThe key strength of this paper lies in its innovative shift from a data-centric specification paradigm to one based on intrinsic neural representations, which theoretically provides more robust verification capabilities. The exploration of NAPs offers significant theoretical insights into the generalization properties of neural networks, potentially leading to advancements in the reliability and interpretability of AI systems. However, the paper does not provide extensive empirical validation of the proposed framework, which may limit the practical applicability of the theoretical claims. Additionally, the complexity of the framework could pose challenges in implementation and understanding for practitioners.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and articulates its theoretical contributions clearly, though some sections may benefit from additional examples to enhance comprehension. The quality of the theoretical discourse is high, and the novelty of the approach in framing neural specifications through NAPs is commendable. However, the lack of empirical results raises concerns regarding reproducibility, as the practical implications of NAPs remain somewhat speculative without experimental validation.\n\n# Summary Of The Review\nThis paper presents a significant theoretical advancement in neural network specifications through the introduction of Neural Activation Patterns, which promise improved generalization and robustness. While the framework shows great potential, the absence of empirical evidence may hinder its immediate application in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces a novel approach for mining Neural Activation Patterns (NAPs) from feed-forward neural networks to enhance the verification of neural networks using the Marabou verification tool. The methodology involves defining a δ-relaxed dominant NAP, which is based on the activation and deactivation of neurons during training. The experimental results demonstrate that NAPs can effectively verify a significant percentage of unseen test images (up to 84%) by utilizing various distance metrics for input validation. The framework allows integration with existing verification tools but lacks a specific code repository or implementation details.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its practical application of NAPs for neural network verification and the clear demonstration of their effectiveness through empirical results. The statistical approach for mining dominant NAPs is straightforward and provides a means to leverage existing verification tools like Marabou, showcasing the potential for enhancing neural network robustness. However, the paper's weaknesses include the absence of a code repository, which limits reproducibility, and the focus on a single dataset, which raises questions about the generalizability of the findings. Additionally, the theoretical implications of NAPs are not thoroughly explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-organized and presents the methodology clearly, although it could benefit from a more detailed explanation of the underlying principles of NAPs. The novelty lies in the integration of NAPs with verification methods; however, the lack of a code repository detracts from the reproducibility of the results. The quality of the experimental setup is adequate, but a broader exploration of different architectures and datasets would strengthen the findings.\n\n# Summary Of The Review\nOverall, the paper presents a novel and practical approach to enhancing neural network verification through NAPs, supported by strong empirical results. However, the lack of a code repository and limited exploration of the methodology's applicability to other architectures detracts from its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes the use of Neural Activation Patterns (NAPs) as a novel specification method for neural network verification. The authors argue that NAPs can effectively verify predictions across the entire input space and that they address issues related to the \"data as specification\" paradigm. Empirical results indicate that NAPs cover 84% of the testing set, suggesting their potential utility. However, the paper lacks rigorous comparisons to established verification methods and does not sufficiently address the limitations of its proposed approach.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative proposal of NAPs as a verification mechanism, which is presented as a response to existing criticisms regarding overfitting in data-driven specifications. However, the weaknesses are pronounced; the methodology lacks the mathematical rigor found in established works such as Huang et al. (2020), and the empirical results do not incorporate comprehensive evaluation metrics as seen in previous studies. Furthermore, the broad claims regarding NAPs fail to provide detailed analyses and comparisons with more sophisticated verification techniques, leading to questions about their effectiveness and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas clearly but falls short on quality due to insufficient engagement with prior literature and a lack of rigorous analysis. The novelty of the proposed approach is questionable, as similar arguments have been made in the field without adequate empirical support. Reproducibility is also a concern, given the simplistic statistical approach for mining NAPs that is not compared to more advanced methods.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing concept with NAPs, but it lacks the necessary rigor and empirical support to substantiate its claims. The authors' failure to engage with existing literature and methodologies undermines the potential significance of their contributions.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"Toward Reliable Neural Specifications\" explores the challenges associated with ensuring the reliability of neural network specifications, particularly in real-world applications. The authors propose utilizing neural activation patterns (NAPs) as a framework for achieving trustworthy AI, leveraging the intrinsic information embedded within neural network models. The findings indicate that this approach may enhance the interpretability and robustness of neural networks, although the paper lacks depth in analyzing the broader implications of these results.\n\n# Strength And Weaknesses\nThe main contribution of the paper lies in its novel approach to using NAPs for neural specifications, which addresses a significant gap in current methodologies. However, the analysis of the implications of the findings is insufficiently detailed, leaving the potential impact of this research underexplored. Additionally, while the methodology is sound, the paper contains various formatting and consistency issues that detract from its overall presentation and clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hindered by inconsistent terminology and formatting, which may confuse readers. The novelty of using NAPs as specifications is evident but could be better highlighted through a more concise presentation of its benefits. The reproducibility of the work is affected by the lack of clear definitions and consistent notation throughout the paper, which may pose challenges for future researchers attempting to build upon this work.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to enhancing the reliability of neural specifications through NAPs, but it suffers from several clarity and consistency issues that undermine its potential impact. The authors would benefit from a more thorough exploration of the implications of their findings and improved formatting to enhance readability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper focuses on the introduction of neural activation patterns (NAPs) as a novel method for improving the verification capabilities of neural networks, specifically within the context of the MNIST dataset. The authors propose δ-relaxed dominant NAPs as a means to enhance the robustness and reliability of predictions in neural networks. Through their experiments, they demonstrate that NAPs can serve as certificates for predictions, which they argue could inspire new applications in AI safety and verification.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in the introduction of NAPs, which could represent a significant advancement in the verification of neural networks. However, the study is limited in scope, as it only utilizes the MNIST dataset, which raises concerns about the generalizability of the findings. Additionally, while the authors provide some experimental results related to verification and robustness, there is a lack of exploration into the scalability of the proposed method, its applicability to other neural network architectures, and real-world use cases. The absence of a thorough analysis of varying parameters and the implications of misclassifications also detracts from the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly, making it accessible to readers. However, the novelty is somewhat undermined by the limited dataset used and the narrow focus on verification capabilities without a broader evaluation of interpretability and explainability. Reproducibility could be improved by providing more detailed experimental setups and discussing potential computational challenges associated with larger networks.\n\n# Summary Of The Review\nOverall, while the paper introduces a promising concept in NAPs for enhancing neural network verification, its limitations in scope and depth hinder its impact. Expanding the evaluation to include diverse datasets and architectures, as well as a discussion on practical applications and ethical considerations, would significantly strengthen the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel approach to neural network verification through the concept of Neural Activation Patterns (NAPs), which serve as specifications for model behavior. By statistically analyzing neuron activations corresponding to input classes, the authors propose a methodology for mining dominant NAPs that can generalize better than conventional data-driven specifications. The findings highlight that a single NAP can cover approximately 84% of both training and testing datasets, indicating improved robustness and generalizability, as well as enhanced interpretability and verification capabilities in neural network models.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to addressing the limitations of existing verification methods, particularly through the introduction of NAPs which improve generalization and robustness in neural networks. The statistical framework employed to evaluate NAPs is well-articulated, and the empirical results provide strong evidence for the effectiveness of the proposed method. However, the paper could benefit from a more comprehensive exploration of trade-offs related to the δ-relaxed dominant NAP, particularly regarding its impact on precision and recall. Furthermore, while the case study demonstrates the utility of NAPs, additional real-world applications could strengthen the argument for their practicality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents the methodology and findings, making it accessible for readers familiar with neural network verification. The quality of the empirical evaluation is commendable, with appropriate statistical analyses supporting the claims. The novelty of the NAP concept stands out, providing a fresh perspective on neural network specifications. However, while the methodology is reproducible, further details on implementation would enhance its practical applicability for researchers looking to replicate or build upon this work.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in neural network verification through the introduction of Neural Activation Patterns, which offer a robust statistical framework for model specifications. The empirical results support the claims made, though further investigation into the practical implications of trade-offs in δ settings would strengthen the contributions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to mining Neural Activation Patterns (NAPs) from feed-forward neural networks, aiming to enhance interpretability and provide insights into model behavior. The methodology involves a mining technique that identifies dominant activation patterns across various layers of the network. The findings suggest that NAPs can reveal significant patterns that correlate with model predictions, yet the authors acknowledge limitations, particularly regarding generalizability and robustness in diverse contexts.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to mining NAPs, which contributes to the interpretability of neural networks. However, several weaknesses are notable: the methodology does not extend beyond feed-forward networks, and there is a lack of exploration into the practical applicability of NAPs in real-world datasets beyond MNIST. The robustness of the mining process is questionable, especially in the presence of noise or ambiguous data, and the introduction of the δ-relaxation parameter raises concerns about the precision of the specifications. Additionally, the paper does not adequately address the computational complexity of the proposed method or its integration with existing verification tools, limiting its practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear in its presentation of the methodology and findings. However, the quality is somewhat diminished by a lack of discussion on the implications of NAPs for various tasks and performance metrics, as well as the failure to address long-term stability and generalizability. The novelty of the approach is notable, but the reproducibility may be hindered by the absence of detailed information regarding the robustness of the mining technique and potential vulnerabilities in the presence of mislabeled data.\n\n# Summary Of The Review\nOverall, the paper presents a promising contribution to the field of neural network interpretability through the mining of NAPs, yet it is limited by its narrow focus on feed-forward networks and a lack of exploration into real-world applications. Addressing the outlined weaknesses would enhance the robustness and applicability of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the concept of Neural Representation as Specification (NAP), which aims to enhance the verification of neural networks by leveraging activation patterns. The authors argue that NAPs can effectively summarize the behavior of neural networks and provide a means of formal verification, claiming that a single NAP can encompass multiple training and testing inputs. However, the methodology presented appears to reiterate established principles in neural network verification without presenting significant new insights.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to consolidate existing verification techniques under the framework of NAPs, which may facilitate understanding for some audiences. However, the weaknesses are pronounced: the contributions are largely derivative, lacking originality, and fail to engage meaningfully with the existing literature. The empirical evaluations do not yield new findings and often state the obvious, thus undermining the paper's overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is undermined by verbose definitions and a lack of depth in discussing NAPs and their properties. The quality of the figures and visualizations is low, as they do not enhance the reader's understanding of the concepts presented. The novelty of the work is limited, primarily rehashing established ideas without offering substantial advancements or rigorous new methodologies. Reproducibility is unclear due to insufficient detail in the experimental setup, making it difficult for others to replicate the findings.\n\n# Summary Of The Review\nThis paper attempts to present a novel framework for neural network verification but ultimately contributes little beyond a rebranding of established ideas. The lack of depth, originality, and meaningful engagement with the literature significantly diminishes its value, making it unsuitable for publication at ICLR.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel framework termed Neural Representation as Specification (NRAS), which shifts the paradigm from traditional data-as-specification to employing neural activation patterns (NAPs) for defining the correctness of neural network predictions. Key contributions include the statistical mining of dominant NAPs, demonstrating their capacity to cover a broader region of the input space compared to conventional methods, and the introduction of δ-relaxed dominant NAPs that allow for a precision-recall trade-off. The findings suggest that NAPs can enhance the robustness and generalization of neural networks, making them applicable in dynamic environments.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to utilizing NAPs for verification, which addresses significant limitations in existing methodologies. The evaluation results support the effectiveness of NAPs, especially on simpler datasets like MNIST, and the potential for adaptive specifications is particularly promising. However, the paper could benefit from further empirical validation on more complex datasets, such as those in natural language processing or vision tasks, to ascertain the generalizability of the findings. Additionally, while the paper discusses the non-ambiguity property of NAPs, a more in-depth exploration of uncertainties associated with their usage is warranted.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to the reader. The methodology is presented with sufficient detail to facilitate reproducibility; however, the lack of extensive empirical validation on diverse datasets may hinder its overall impact. The novelty of integrating NAPs into the verification framework is commendable, but further exploration of hybrid models and the incorporation of unsupervised learning methods could enhance its significance.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in the field of neural network verification through the introduction of NAPs. While the methodology and findings are promising, further empirical exploration and integration with other techniques are essential for maximizing the framework's impact and applicability in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel method for enhancing the verification of unseen testing images in neural networks using Neural Activation Patterns (NAPs). The primary contributions include demonstrating a significant improvement in coverage of testing images on the MNIST dataset, achieving 84% coverage compared to 0% with traditional data-as-specification approaches. The authors employ empirical evidence to show that maximum verified bounds are considerably smaller than actual distances between data points, and their method allows for the verification of robustness against multiple labels. They also explore the trade-offs between recall and precision through parameter adjustments, specifically focusing on the δ parameter, which indicates flexibility in achieving robustness verification.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its empirical validation of the proposed method, showcasing significant improvements over existing benchmarks in terms of coverage and robustness verification. The flexibility offered by adjusting the δ parameter is a notable advantage that allows for a tailored approach to different verification scenarios. However, a potential weakness is that the paper may not sufficiently address the computational complexity involved in adjusting δ across various datasets and scenarios, which might limit practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly, making it accessible to readers familiar with the topic. The quality of the experimental results is commendable, providing strong evidence for the effectiveness of NAPs. In terms of novelty, the approach appears to advance the field of neural network verification significantly. However, the reproducibility of the results might be a concern if the authors do not provide sufficient details about their experimental setup and configurations.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of neural network verification by introducing an innovative approach using NAPs, demonstrating significant improvements in both coverage and robustness. While the methodology is sound and the findings are promising, further attention to the complexities of implementation and reproducibility would strengthen the study.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"TOWARD RELIABLE NEURAL SPECIFICATIONS\" investigates the reliability of neural activation patterns (NAPs) as a method for specifying and verifying neural network behavior. The authors present a methodology that evaluates the effectiveness of NAPs in capturing the essential characteristics of neural networks, particularly in the context of overfitting. The empirical findings demonstrate that while NAPs can be informative, they often lead to overfitted specifications which may not generalize well. The paper concludes with implications for future research on enhancing the reliability of neural specifications.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its empirical analysis of NAPs, providing a critical examination of their limitations and pitfalls in the context of neural networks. This contribution is particularly valuable as it addresses a significant concern in the field regarding model interpretability and reliability. However, the paper suffers from a lack of clarity in some sections, with complex sentence structures and redundancy that could hinder understanding. Additionally, the title does not adequately reflect the specific focus of the research, which could mislead potential readers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by lengthy sentences, inconsistent terminology, and vague section headings, making it challenging for readers to grasp the main contributions quickly. The quality of presentation could be improved by revising complex jargon and ensuring consistent formatting in references. Despite these issues, the technical novelty is notable, as the authors explore a relatively under-researched area regarding the verification of neural networks. However, the reproducibility of the findings may be hindered by the lack of detailed methodology and the need for clearer explanations of technical terms.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the understanding of neural activation patterns in neural network verification, though it suffers from clarity issues and redundant explanations. Improving the writing quality and presentation would greatly enhance the accessibility and impact of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.5438864812350115,
    -1.6915176819997748,
    -1.8182421857153503,
    -1.621159185536217,
    -1.918848712203149,
    -1.7206703060055701,
    -1.5863466624950688,
    -1.9445360855019562,
    -1.8211513866994546,
    -1.9124230854683208,
    -1.7368066402042528,
    -1.7381945714525286,
    -1.7055855922512686,
    -1.7132925398244212,
    -1.7325857931106552,
    -1.7524576483145402,
    -1.9887366796519335,
    -1.908274517151336,
    -1.8772468007150993,
    -1.8087305329808256,
    -1.974106398222623,
    -1.7234912239531344,
    -1.7399475256205614,
    -1.840631250191615,
    -1.6811242560479112,
    -1.9393921386305517,
    -1.964687950293243,
    -1.917368015215236,
    -1.7453206586480894
  ],
  "logp_cond": [
    [
      0.0,
      -2.3584219420452843,
      -2.3572356548975026,
      -2.3754123168863774,
      -2.3532336101987736,
      -2.3897585351589536,
      -2.394063718240996,
      -2.355062577067974,
      -2.3622114306569113,
      -2.388964894093946,
      -2.3446535020014356,
      -2.414969377075593,
      -2.3597876789720265,
      -2.417209284102999,
      -2.3932637046107255,
      -2.39288067404602,
      -2.3860874462169717,
      -2.3735216370487087,
      -2.403459402449112,
      -2.4069619294578986,
      -2.3759811896214553,
      -2.403029925580384,
      -2.387024404781669,
      -2.3832184131443803,
      -2.4129805313343873,
      -2.3703641545317318,
      -2.361690217201608,
      -2.3680634089544137,
      -2.399294908353095
    ],
    [
      -1.322048724633486,
      0.0,
      -1.1917742951157364,
      -1.2459762442451487,
      -1.209672123318272,
      -1.2412681307187532,
      -1.3384828379431724,
      -1.2365636585489916,
      -1.198383334080125,
      -1.27790739986217,
      -1.184849797407037,
      -1.415132773799288,
      -1.2191677105970056,
      -1.2536691630798744,
      -1.341809064116231,
      -1.2801066256388631,
      -1.2599141459101986,
      -1.3016277783451964,
      -1.2767519194708197,
      -1.3334062439271646,
      -1.2928681190242015,
      -1.3842509637046547,
      -1.3238381773488888,
      -1.2463632708816805,
      -1.3523105343615502,
      -1.33466288603851,
      -1.2375640975373914,
      -1.2896272995363969,
      -1.3480095227259652
    ],
    [
      -1.4244530242006743,
      -1.3038739375231867,
      0.0,
      -1.3468020633576716,
      -1.277122335970567,
      -1.396957828612584,
      -1.4237741907323542,
      -1.2912343950253002,
      -1.3694667026586684,
      -1.421038415853267,
      -1.3652313468298616,
      -1.5428337767431972,
      -1.31232160875622,
      -1.2865879746323967,
      -1.4385042313542509,
      -1.4039734164527375,
      -1.230714417947666,
      -1.3144517644234723,
      -1.378956792167907,
      -1.3141320436105095,
      -1.3876749842910308,
      -1.4844393092968158,
      -1.3201418451036757,
      -1.2162224713481455,
      -1.4000420568583132,
      -1.4034697565712075,
      -1.267955664928681,
      -1.3415336981681376,
      -1.4675958306829937
    ],
    [
      -1.295153055412103,
      -1.198833305888175,
      -1.1809624168839583,
      0.0,
      -1.1856938840168982,
      -1.2884128011197433,
      -1.2211213878758194,
      -1.2256030151908028,
      -1.2262116896824862,
      -1.219266928118223,
      -1.2455163540833631,
      -1.331254426418985,
      -1.1899367554688975,
      -1.23773197042733,
      -1.2534987744149582,
      -1.2747386345330483,
      -1.2220027790438963,
      -1.260174545852241,
      -1.2535532538436964,
      -1.2787556800605466,
      -1.260642239550245,
      -1.2649001450690884,
      -1.2871779497934719,
      -1.263699826336821,
      -1.2890067884345568,
      -1.291214161393985,
      -1.2238660910568506,
      -1.2081278304343588,
      -1.2657243891285548
    ],
    [
      -1.5229726930401535,
      -1.4144161091505845,
      -1.3578844401096786,
      -1.4697009205316165,
      0.0,
      -1.5000443277995903,
      -1.5011665707681856,
      -1.4014893523405705,
      -1.422206164066967,
      -1.4449883329582311,
      -1.4607209331559612,
      -1.6055539334037379,
      -1.4089128962675324,
      -1.4984884938187666,
      -1.556846333441032,
      -1.5088223418824591,
      -1.4374365409108514,
      -1.4431807846253215,
      -1.4689518531997863,
      -1.5418236309816633,
      -1.4916841488511972,
      -1.596877823778398,
      -1.5041136169415144,
      -1.4852686039323926,
      -1.5435443779020377,
      -1.497313257063056,
      -1.3734789729578165,
      -1.4360719903436758,
      -1.5577059101072566
    ],
    [
      -1.4158599141649635,
      -1.2072912355500311,
      -1.267382099137451,
      -1.3375003939648302,
      -1.275834828649026,
      0.0,
      -1.3860064968757297,
      -1.2878424008718596,
      -1.3075158753072276,
      -1.3187285559609643,
      -1.314914062340867,
      -1.4479506189824058,
      -1.2975601774056251,
      -1.3135507365867378,
      -1.3955120113820538,
      -1.3698090400194753,
      -1.3496274020550616,
      -1.3387692612828013,
      -1.3412104462880727,
      -1.3615207249110375,
      -1.3788588440851701,
      -1.4075954464305556,
      -1.3532942505843133,
      -1.3193829144816585,
      -1.3992024454601797,
      -1.3822690131298607,
      -1.2800603321206165,
      -1.3411510945886589,
      -1.3992144318051492
    ],
    [
      -1.31978252784443,
      -1.2795189412014132,
      -1.2688075074746668,
      -1.284894302151487,
      -1.2891037620916648,
      -1.3464680777046316,
      0.0,
      -1.3204112005420896,
      -1.3353535285270561,
      -1.2455439766498144,
      -1.272756250719781,
      -1.3746908680897942,
      -1.262685973107774,
      -1.2884371956831668,
      -1.3121838493805682,
      -1.3502034027592043,
      -1.240139493891326,
      -1.3510572455724281,
      -1.324753356229107,
      -1.3111240702618807,
      -1.3271886129513117,
      -1.2915417147055455,
      -1.3345091275129877,
      -1.2534138341971894,
      -1.2999593500889546,
      -1.3337557913546545,
      -1.2570574934157808,
      -1.314179244486454,
      -1.2827209360512364
    ],
    [
      -1.5835602188281908,
      -1.488637657482875,
      -1.4954591749349877,
      -1.5275158114506693,
      -1.489380552078867,
      -1.5347888725104388,
      -1.6225341998191685,
      0.0,
      -1.5150332033313256,
      -1.5243308801091078,
      -1.5311034996551698,
      -1.6819775964989103,
      -1.490079905721475,
      -1.5658375299195366,
      -1.6199513687876097,
      -1.5773368742898377,
      -1.5053641038120702,
      -1.4859609205013815,
      -1.5317498002144627,
      -1.5717517431482546,
      -1.5192478313060616,
      -1.6486019283556834,
      -1.583524800296248,
      -1.5495296553925466,
      -1.5989501342266315,
      -1.5424405657528384,
      -1.46180708771722,
      -1.5492727425470403,
      -1.6423116510742144
    ],
    [
      -1.401503844810184,
      -1.3238745958561349,
      -1.3629867942161034,
      -1.3419277540669945,
      -1.325832510037142,
      -1.4247499161483062,
      -1.497292136903375,
      -1.3598189524590127,
      0.0,
      -1.4472963975427717,
      -1.3644416952170437,
      -1.5162687346578645,
      -1.386851903664908,
      -1.4548169483828273,
      -1.4519386264317402,
      -1.466510815840521,
      -1.389941649369236,
      -1.4209702255829946,
      -1.440786693215457,
      -1.4589259501781808,
      -1.4169174665588051,
      -1.4851232955574183,
      -1.4759311228169776,
      -1.431804276556078,
      -1.5434992374641077,
      -1.3971231550692276,
      -1.3572415915067555,
      -1.3666113127951283,
      -1.500666235387585
    ],
    [
      -1.6027381067768116,
      -1.5328473639657014,
      -1.5675709907852395,
      -1.5579227747357094,
      -1.5057203560566677,
      -1.5950335365991397,
      -1.5217282717337026,
      -1.5372204796978808,
      -1.5690473055642293,
      0.0,
      -1.5207304670294497,
      -1.5952871279836174,
      -1.4902986626286419,
      -1.5055058704340716,
      -1.5161383851030188,
      -1.55807071403898,
      -1.4925128431038823,
      -1.5690075826100198,
      -1.5354673797983431,
      -1.5735243413536655,
      -1.5126455427750156,
      -1.5200387477310708,
      -1.5841155174185908,
      -1.5773153349782576,
      -1.566277272072528,
      -1.5861755669687099,
      -1.5393469151588401,
      -1.5862254342702407,
      -1.4917696755112368
    ],
    [
      -1.3632748123908203,
      -1.272876392085811,
      -1.3019993873956186,
      -1.3612141537526472,
      -1.3512939029005013,
      -1.418919698750865,
      -1.3963745710747686,
      -1.3302953932560275,
      -1.3549476428326845,
      -1.309375182001038,
      0.0,
      -1.4129697409585922,
      -1.2829962534746997,
      -1.293027735780068,
      -1.3968086308418624,
      -1.2758332247574267,
      -1.3366164386392365,
      -1.357038877988331,
      -1.318838350826215,
      -1.3574740626836015,
      -1.3549778600062534,
      -1.3916124459865444,
      -1.392548883329024,
      -1.3644080432409251,
      -1.41226878208233,
      -1.3896269248114062,
      -1.2991682597173566,
      -1.3704182302519043,
      -1.4061005954014263
    ],
    [
      -1.3977377961900026,
      -1.377950173352311,
      -1.3775872072880744,
      -1.3597918015167572,
      -1.3553618097648517,
      -1.3625829912074106,
      -1.3709745575509436,
      -1.3291674351681566,
      -1.3489472529400084,
      -1.2753722992018592,
      -1.2876506264411698,
      0.0,
      -1.3312141479969044,
      -1.3141362128735727,
      -1.3356405057940464,
      -1.35175456377856,
      -1.3327117176357623,
      -1.4055260172285295,
      -1.33805762584588,
      -1.3700445734829154,
      -1.3170155224664308,
      -1.3284568556813956,
      -1.3829195152656835,
      -1.3483822757975594,
      -1.3769585654039618,
      -1.3303806147009682,
      -1.3440453733702054,
      -1.3471546255419784,
      -1.3082300522562602
    ],
    [
      -1.4150848759239036,
      -1.273494361710021,
      -1.2655362689282448,
      -1.281987767583678,
      -1.3076111778001163,
      -1.3813360405449404,
      -1.3534091387721552,
      -1.2644734251983605,
      -1.3391876601453883,
      -1.2346856269686326,
      -1.2493151529550786,
      -1.4166701381073645,
      0.0,
      -1.2460501325767863,
      -1.3155931304309447,
      -1.2699117541764593,
      -1.3095269612288725,
      -1.3548345053492603,
      -1.2677640547081015,
      -1.3544588944401885,
      -1.316151792412887,
      -1.3359764452021665,
      -1.3130668859498322,
      -1.2403023110991522,
      -1.366852239799547,
      -1.3348050832656788,
      -1.2311216026427196,
      -1.3740185278846506,
      -1.3172161812368823
    ],
    [
      -1.410440194315422,
      -1.2667332340754496,
      -1.2318364762056908,
      -1.3145599714010618,
      -1.367326421265757,
      -1.3669861432141366,
      -1.3599956229868997,
      -1.3158607742049635,
      -1.3854243597583547,
      -1.3097469933532355,
      -1.2784930114190642,
      -1.4029357929120425,
      -1.3144141346174478,
      0.0,
      -1.3506489386058242,
      -1.2815486518514418,
      -1.2474411296540033,
      -1.3437265105960932,
      -1.3553166860772496,
      -1.2700050369913891,
      -1.299696435525186,
      -1.3690149100021336,
      -1.3221827783353446,
      -1.2345029861753423,
      -1.3896260499646291,
      -1.38047443782319,
      -1.2748346062296176,
      -1.3419971899421927,
      -1.3553988022613066
    ],
    [
      -1.471050120795859,
      -1.4315912973275207,
      -1.4202245204863766,
      -1.384753935486479,
      -1.432798079313124,
      -1.4554017063459108,
      -1.422926345947627,
      -1.4262376376555217,
      -1.4013980467120064,
      -1.3534221777041693,
      -1.3986760351796594,
      -1.4347666815891371,
      -1.3816998693939775,
      -1.4046797850305917,
      0.0,
      -1.3656698618088952,
      -1.3772609797217679,
      -1.3826516060999157,
      -1.42505204758316,
      -1.4403460870154325,
      -1.4101065743757162,
      -1.3708507492497302,
      -1.451793226541395,
      -1.407016918647861,
      -1.469059255058936,
      -1.446757050470397,
      -1.427027222978639,
      -1.4111844949275736,
      -1.3334994547405083
    ],
    [
      -1.469273987066476,
      -1.3835481538220964,
      -1.388709098259783,
      -1.3544732762139164,
      -1.4265084865762165,
      -1.4507846803152116,
      -1.4548559952112652,
      -1.432877743312869,
      -1.406681123582321,
      -1.3699509161954284,
      -1.2618615962617343,
      -1.4949086807631462,
      -1.3175438029184892,
      -1.3127766332845092,
      -1.3952629569190034,
      0.0,
      -1.3495321170215444,
      -1.4422598795570443,
      -1.3798937442340713,
      -1.4200963754280138,
      -1.3744483686372189,
      -1.479013283398926,
      -1.4221342104805743,
      -1.3219400323390924,
      -1.4824481647130228,
      -1.4431769045344214,
      -1.3678868880851884,
      -1.4217983041290259,
      -1.4689303214552754
    ],
    [
      -1.644017676253364,
      -1.5714635505518686,
      -1.5133478718579676,
      -1.5823927444619303,
      -1.5515340376406863,
      -1.6364631561806988,
      -1.5967692890055618,
      -1.5701229114448825,
      -1.6004911583531265,
      -1.5537513852929437,
      -1.6075554546539876,
      -1.7102102183731986,
      -1.515700495435727,
      -1.5398112451985995,
      -1.6186857902459906,
      -1.5813509416984537,
      0.0,
      -1.5895006351344503,
      -1.5899820658428734,
      -1.4991685878459295,
      -1.59195961689047,
      -1.6738882885555357,
      -1.5549672722908066,
      -1.4741568214756953,
      -1.5814726726541322,
      -1.6202745514191466,
      -1.4413743556802103,
      -1.548112602969149,
      -1.6322125798837097
    ],
    [
      -1.5173698291785758,
      -1.4189253430174038,
      -1.36379987154512,
      -1.445128543403636,
      -1.4390552427055727,
      -1.4980335190815273,
      -1.5338397087502424,
      -1.3786145379653236,
      -1.4686156653745979,
      -1.4531359886678274,
      -1.417391980285236,
      -1.588739258516606,
      -1.4347117328954806,
      -1.486029056142368,
      -1.4521539339728133,
      -1.5291595553178359,
      -1.4158005831553675,
      0.0,
      -1.4503607910765768,
      -1.425084587736356,
      -1.455815436467391,
      -1.531343863344831,
      -1.5091232982640665,
      -1.4726452983155935,
      -1.528305920656373,
      -1.4768982808814595,
      -1.4166895645691977,
      -1.4852898523950406,
      -1.5135570982629276
    ],
    [
      -1.5229619532783476,
      -1.4174543156469581,
      -1.413235238782178,
      -1.449196880845062,
      -1.4349638798300473,
      -1.4814757391378426,
      -1.443032997281498,
      -1.4699873200179407,
      -1.44472250810133,
      -1.4126505203402013,
      -1.4230398326943081,
      -1.5737156009047124,
      -1.3420630100456863,
      -1.433043305332987,
      -1.5371057155018195,
      -1.4060567664945391,
      -1.4671915016997084,
      -1.463704800674891,
      0.0,
      -1.4972744118901231,
      -1.4689282475969259,
      -1.5092723140263735,
      -1.4941288603761662,
      -1.3842004412484463,
      -1.4855361258603623,
      -1.4804571499216956,
      -1.4098234235844944,
      -1.4626518356091223,
      -1.4883993425587563
    ],
    [
      -1.4849764807039763,
      -1.3723917283808202,
      -1.2861081846537452,
      -1.4120022688697669,
      -1.3989639431269834,
      -1.415843481734159,
      -1.4123744925081962,
      -1.3627746366959803,
      -1.4270198945585468,
      -1.4321462657250863,
      -1.363735887015839,
      -1.541752888239184,
      -1.4163459479893443,
      -1.3192327170092484,
      -1.4588623136465508,
      -1.4216386628375837,
      -1.2569790782929124,
      -1.3479565994378797,
      -1.4367444532546143,
      0.0,
      -1.420565216739588,
      -1.4921634604629457,
      -1.3236467919852466,
      -1.2944561211613737,
      -1.3774450985383793,
      -1.4207534680726945,
      -1.34427300867504,
      -1.3986859721594964,
      -1.4810318690659074
    ],
    [
      -1.5632517956802836,
      -1.524584819890467,
      -1.5565309895212545,
      -1.532218975667114,
      -1.540229353777812,
      -1.593944571113557,
      -1.5814713072645994,
      -1.454261453999038,
      -1.564120445229516,
      -1.427322611419243,
      -1.4941873816019569,
      -1.613941381199745,
      -1.5052750771181806,
      -1.4677266702624023,
      -1.5618320260753278,
      -1.4943016766972093,
      -1.4806017203918005,
      -1.5570949727515377,
      -1.5232696444573677,
      -1.5441860513092944,
      0.0,
      -1.5721614988080255,
      -1.5965418857744442,
      -1.4937306258000633,
      -1.579153005693796,
      -1.5673057748794796,
      -1.481888540146281,
      -1.555514620358828,
      -1.5434028637676982
    ],
    [
      -1.418765660695879,
      -1.3150631442404457,
      -1.3035991473745059,
      -1.2930799417495882,
      -1.3384694264924468,
      -1.3618353600147028,
      -1.312359483084233,
      -1.3109000292528656,
      -1.3100785591744841,
      -1.2229707291295813,
      -1.295580091330038,
      -1.3606465416565328,
      -1.264508656202113,
      -1.2965114276946428,
      -1.227441120891743,
      -1.2995679300265297,
      -1.3251635278618117,
      -1.3315889108346073,
      -1.312862990624914,
      -1.3688812716465175,
      -1.2950882217818085,
      0.0,
      -1.3519395391433215,
      -1.3300286265077734,
      -1.3695483292637343,
      -1.3455009574462387,
      -1.28650568865602,
      -1.3699174748799057,
      -1.220198615016759
    ],
    [
      -1.4373137842503196,
      -1.352161447963794,
      -1.2974390285176276,
      -1.415020965590182,
      -1.3911194667740199,
      -1.3974787139209925,
      -1.381280037890929,
      -1.3790447123774392,
      -1.4382093825747322,
      -1.4156724192771304,
      -1.3715286531848931,
      -1.4824682957336306,
      -1.3481780395866858,
      -1.3474856211814223,
      -1.4452227776787165,
      -1.3813033492294373,
      -1.3281469157483108,
      -1.3974619031806033,
      -1.4091516209012107,
      -1.3098857773909351,
      -1.4155562033190123,
      -1.4362938533774847,
      0.0,
      -1.2967553101547207,
      -1.3190557734389203,
      -1.4212927325828077,
      -1.294117010512093,
      -1.4355418825211093,
      -1.4367000663311098
    ],
    [
      -1.51427899784726,
      -1.4545740852784383,
      -1.3537297896776674,
      -1.489227419694661,
      -1.4532087293684823,
      -1.5089196132634208,
      -1.465121509053312,
      -1.4688282260567116,
      -1.4637443566479902,
      -1.4501336596821974,
      -1.4436547744704789,
      -1.5588550531751968,
      -1.4188399199591102,
      -1.3908372438310688,
      -1.5141492428741432,
      -1.4329293450979954,
      -1.3380231498294721,
      -1.4968764344192655,
      -1.426886851895884,
      -1.433032190812792,
      -1.4385283422277193,
      -1.5335181511116662,
      -1.428435188614596,
      0.0,
      -1.4509993126838823,
      -1.4867125763085622,
      -1.355733310070429,
      -1.4419694737023308,
      -1.4998474943909408
    ],
    [
      -1.3729198137658316,
      -1.3370467055123387,
      -1.2965631379728346,
      -1.3615620896217262,
      -1.3086322105705013,
      -1.3520155771063513,
      -1.3205435215017374,
      -1.3532991064686801,
      -1.3720577716254834,
      -1.3078476252688944,
      -1.362806392078804,
      -1.4364688946367068,
      -1.3328425580048362,
      -1.3143151777665076,
      -1.3836430965183038,
      -1.3534944780794795,
      -1.2902282746575355,
      -1.31719058513995,
      -1.355640312097956,
      -1.2918070050398427,
      -1.3324179093536386,
      -1.371030483517155,
      -1.3055313241119226,
      -1.2791330266577123,
      0.0,
      -1.3618852528861076,
      -1.252297117915264,
      -1.3093563852638401,
      -1.3525644589569055
    ],
    [
      -1.5232739062210032,
      -1.451788009797129,
      -1.4133074072922676,
      -1.4959744237689518,
      -1.4025459670186144,
      -1.511950215135467,
      -1.5431555029433555,
      -1.41310243021134,
      -1.4485961922521586,
      -1.492983695724046,
      -1.455826337614786,
      -1.6160235595362178,
      -1.4491326301385066,
      -1.5649809322002892,
      -1.5991691034873554,
      -1.5356260511446256,
      -1.4663260958647493,
      -1.4431700229330164,
      -1.478165086294955,
      -1.507359345517419,
      -1.515821330181757,
      -1.5716587229076207,
      -1.532678776754346,
      -1.4667866536235057,
      -1.5244590264382558,
      0.0,
      -1.3975596748444814,
      -1.5143086233610628,
      -1.5555933132957431
    ],
    [
      -1.539640211370379,
      -1.4810365460834163,
      -1.4462897190946302,
      -1.5199746095039173,
      -1.4541143052494292,
      -1.5087866321828889,
      -1.5592135620356509,
      -1.4688712478568013,
      -1.516932761344475,
      -1.5671795574546312,
      -1.4340189222478776,
      -1.698062793479664,
      -1.4546462491022287,
      -1.5209805303526882,
      -1.6196759543114765,
      -1.5482025149945293,
      -1.4201740189083167,
      -1.5392305102160662,
      -1.5110432709345558,
      -1.528650072050346,
      -1.5564091303671466,
      -1.629926169947849,
      -1.5208902183330724,
      -1.4612697065479414,
      -1.5660091487869754,
      -1.5812991701757557,
      0.0,
      -1.5145747372462264,
      -1.626886493233442
    ],
    [
      -1.6171433889694091,
      -1.5857564391114882,
      -1.4981156790546697,
      -1.5241254242437443,
      -1.474691751480634,
      -1.5994337782408288,
      -1.6080223538925371,
      -1.5202641623531077,
      -1.534695551202577,
      -1.594777863591905,
      -1.565394894705582,
      -1.6689498916026235,
      -1.5271615138992995,
      -1.5231420901208517,
      -1.5923970131531762,
      -1.5443305899933062,
      -1.4824030487982458,
      -1.5430034695312447,
      -1.5412577074786014,
      -1.589972272850499,
      -1.5825944833133554,
      -1.6442968736694483,
      -1.5975038132335273,
      -1.5198199217699677,
      -1.5960035321109616,
      -1.5823445124335074,
      -1.491847216414696,
      0.0,
      -1.6419643583828398
    ],
    [
      -1.471985206721359,
      -1.4595999695418016,
      -1.4167944444815854,
      -1.4353373233016329,
      -1.4176149656824673,
      -1.4696769616693244,
      -1.3988974786499218,
      -1.4251715689584725,
      -1.4555021816629183,
      -1.3464412125884575,
      -1.4137670180191126,
      -1.4150742512555772,
      -1.3989238426740458,
      -1.4046987540101998,
      -1.3347397376684584,
      -1.4116047356942845,
      -1.4239700659517909,
      -1.4245396663236451,
      -1.4318402722648562,
      -1.4329673368977025,
      -1.3800982530741843,
      -1.3300391038808397,
      -1.44083931406485,
      -1.400231529504403,
      -1.3989439308530378,
      -1.4322298288866837,
      -1.4219895435245857,
      -1.436264282876673,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.18546453918972716,
      0.18665082633750885,
      0.16847416434863405,
      0.19065287103623785,
      0.15412794607605784,
      0.14982276299401542,
      0.18882390416703743,
      0.18167505057810018,
      0.1549215871410654,
      0.19923297923357586,
      0.12891710415941837,
      0.18409880226298503,
      0.12667719713201242,
      0.15062277662428603,
      0.1510058071889917,
      0.15779903501803982,
      0.17036484418630282,
      0.14042707878589944,
      0.13692455177711294,
      0.16790529161355616,
      0.1408565556546275,
      0.1568620764533426,
      0.16066806809063117,
      0.1309059499006242,
      0.17352232670327972,
      0.1821962640334034,
      0.1758230722805978,
      0.1445915728819167
    ],
    [
      0.3694689573662888,
      0.0,
      0.4997433868840384,
      0.44554143775462607,
      0.4818455586815029,
      0.4502495512810216,
      0.35303484405660246,
      0.4549540234507832,
      0.4931343479196497,
      0.41361028213760487,
      0.5066678845927377,
      0.2763849082004868,
      0.47234997140276924,
      0.4378485189199004,
      0.3497086178835438,
      0.4114110563609117,
      0.43160353608957625,
      0.38988990365457843,
      0.41476576252895514,
      0.35811143807261026,
      0.3986495629755733,
      0.3072667182951201,
      0.36767950465088606,
      0.4451544111180943,
      0.33920714763822457,
      0.3568547959612649,
      0.45395358446238343,
      0.40189038246337794,
      0.3435081592738096
    ],
    [
      0.393789161514676,
      0.5143682481921636,
      0.0,
      0.47144012235767874,
      0.5411198497447833,
      0.42128435710276624,
      0.3944679949829961,
      0.5270077906900501,
      0.4487754830566819,
      0.3972037698620834,
      0.45301083888548876,
      0.27540840897215313,
      0.5059205769591304,
      0.5316542110829536,
      0.3797379543610995,
      0.4142687692626128,
      0.5875277677676845,
      0.503790421291878,
      0.43928539354744345,
      0.5041101421048408,
      0.4305672014243196,
      0.33380287641853457,
      0.49810034061167463,
      0.6020197143672048,
      0.41820012885703717,
      0.4147724291441428,
      0.5502865207866694,
      0.47670848754721273,
      0.35064635503235664
    ],
    [
      0.32600613012411417,
      0.422325879648042,
      0.44019676865225876,
      0.0,
      0.43546530151931884,
      0.33274638441647375,
      0.4000377976603977,
      0.39555617034541424,
      0.3949474958537309,
      0.40189225741799417,
      0.37564283145285393,
      0.2899047591172321,
      0.43122243006731953,
      0.3834272151088871,
      0.3676604111212589,
      0.3464205510031688,
      0.39915640649232076,
      0.360984639683976,
      0.36760593169252065,
      0.34240350547567044,
      0.36051694598597206,
      0.35625904046712864,
      0.3339812357427452,
      0.357459359199396,
      0.3321523971016602,
      0.32994502414223215,
      0.39729309447936645,
      0.4130313551018583,
      0.3554347964076623
    ],
    [
      0.39587601916299553,
      0.5044326030525645,
      0.5609642720934704,
      0.4491477916715325,
      0.0,
      0.4188043844035587,
      0.4176821414349634,
      0.5173593598625785,
      0.496642548136182,
      0.47386037924491786,
      0.4581277790471878,
      0.31329477879941114,
      0.5099358159356167,
      0.4203602183843824,
      0.36200237876211694,
      0.4100263703206899,
      0.48141217129229763,
      0.4756679275778275,
      0.44989685900336274,
      0.3770250812214857,
      0.42716456335195185,
      0.32197088842475097,
      0.41473509526163466,
      0.43358010827075644,
      0.3753043343011113,
      0.42153545514009294,
      0.5453697392453325,
      0.4827767218594732,
      0.36114280209589245
    ],
    [
      0.30481039184060665,
      0.513379070455539,
      0.4532882068681192,
      0.3831699120407399,
      0.44483547735654416,
      0.0,
      0.3346638091298404,
      0.4328279051337105,
      0.41315443069834257,
      0.40194175004460586,
      0.4057562436647031,
      0.2727196870231643,
      0.423110128599945,
      0.4071195694188323,
      0.3251582946235163,
      0.3508612659860948,
      0.37104290395050854,
      0.38190104472276887,
      0.3794598597174974,
      0.35914958109453266,
      0.3418114619204,
      0.3130748595750146,
      0.36737605542125684,
      0.4012873915239117,
      0.32146786054539045,
      0.3384012928757094,
      0.4406099738849536,
      0.37951921141691125,
      0.321455874200421
    ],
    [
      0.2665641346506389,
      0.30682772129365565,
      0.3175391550204021,
      0.3014523603435819,
      0.29724290040340406,
      0.23987858479043722,
      0.0,
      0.26593546195297924,
      0.2509931339680127,
      0.3408026858452544,
      0.3135904117752879,
      0.2116557944052746,
      0.32366068938729486,
      0.297909466811902,
      0.27416281311450064,
      0.23614325973586459,
      0.3462071686037429,
      0.23528941692264072,
      0.2615933062659619,
      0.2752225922331881,
      0.2591580495437571,
      0.2948049477895234,
      0.25183753498208117,
      0.3329328282978794,
      0.28638731240611426,
      0.25259087114041434,
      0.32928916907928807,
      0.2721674180086149,
      0.30362572644383246
    ],
    [
      0.36097586667376547,
      0.4558984280190812,
      0.4490769105669685,
      0.41702027405128694,
      0.45515553342308923,
      0.4097472129915174,
      0.32200188568278776,
      0.0,
      0.4295028821706306,
      0.42020520539284845,
      0.41343258584678644,
      0.2625584890030459,
      0.45445617978048114,
      0.3786985555824196,
      0.3245847167143465,
      0.3671992112121185,
      0.439171981689886,
      0.4585751650005747,
      0.4127862852874935,
      0.3727843423537016,
      0.42528825419589467,
      0.2959341571462728,
      0.36101128520570813,
      0.3950064301094096,
      0.34558595127532477,
      0.40209551974911784,
      0.4827289977847362,
      0.39526334295491594,
      0.30222443442774183
    ],
    [
      0.41964754188927067,
      0.49727679084331977,
      0.4581645924833513,
      0.47922363263246015,
      0.49531887666231267,
      0.3964014705511485,
      0.3238592497960797,
      0.4613324342404419,
      0.0,
      0.3738549891566829,
      0.4567096914824109,
      0.30488265204159015,
      0.4342994830345466,
      0.3663344383166274,
      0.36921276026771443,
      0.35464057085893375,
      0.4312097373302186,
      0.40018116111646007,
      0.3803646934839977,
      0.3622254365212738,
      0.4042339201406495,
      0.33602809114203636,
      0.34522026388247706,
      0.38934711014337675,
      0.277652149235347,
      0.42402823163022707,
      0.46390979519269915,
      0.4545400739043264,
      0.3204851513118696
    ],
    [
      0.30968497869150924,
      0.3795757215026194,
      0.3448520946830813,
      0.35450031073261146,
      0.4067027294116532,
      0.31738954886918114,
      0.39069481373461823,
      0.37520260577044007,
      0.3433757799040915,
      0.0,
      0.39169261843887115,
      0.31713595748470347,
      0.422124422839679,
      0.40691721503424927,
      0.39628470036530206,
      0.35435237142934084,
      0.4199102423644385,
      0.343415502858301,
      0.3769557056699777,
      0.33889874411465537,
      0.39977754269330523,
      0.39238433773725,
      0.32830756804973005,
      0.33510775049006325,
      0.34614581339579287,
      0.326247518499611,
      0.3730761703094807,
      0.3261976511980802,
      0.420653409957084
    ],
    [
      0.3735318278134325,
      0.46393024811844175,
      0.4348072528086342,
      0.3755924864516056,
      0.38551273730375146,
      0.3178869414533878,
      0.3404320691294842,
      0.40651124694822527,
      0.38185899737156825,
      0.4274314582032148,
      0.0,
      0.3238368992456606,
      0.4538103867295531,
      0.4437789044241849,
      0.33999800936239044,
      0.46097341544682613,
      0.4001902015650163,
      0.3797677622159219,
      0.41796828937803787,
      0.3793325775206513,
      0.3818287801979994,
      0.34519419421770836,
      0.34425775687522875,
      0.37239859696332767,
      0.3245378581219227,
      0.34717971539284664,
      0.4376383804868962,
      0.36638840995234845,
      0.3307060448028265
    ],
    [
      0.34045677526252605,
      0.3602443981002177,
      0.36060736416445427,
      0.3784027699357715,
      0.382832761687677,
      0.3756115802451181,
      0.3672200139015851,
      0.40902713628437204,
      0.38924731851252026,
      0.4628222722506694,
      0.45054394501135886,
      0.0,
      0.40698042345562424,
      0.4240583585789559,
      0.4025540656584823,
      0.3864400076739687,
      0.4054828538167663,
      0.3326685542239991,
      0.4001369456066486,
      0.36814999796961323,
      0.4211790489860978,
      0.40973771577113305,
      0.3552750561868452,
      0.3898122956549692,
      0.36123600604856687,
      0.4078139567515604,
      0.3941491980823233,
      0.39103994591055025,
      0.42996451919626844
    ],
    [
      0.29050071632736496,
      0.43209123054124765,
      0.4400493233230238,
      0.42359782466759066,
      0.39797441445115234,
      0.3242495517063282,
      0.3521764534791134,
      0.4411121670529081,
      0.36639793210588034,
      0.470899965282636,
      0.45627043929619004,
      0.28891545414390407,
      0.0,
      0.4595354596744823,
      0.3899924618203239,
      0.4356738380748093,
      0.39605863102239613,
      0.3507510869020083,
      0.4378215375431671,
      0.3511266978110801,
      0.38943379983838167,
      0.3696091470491021,
      0.39251870630143637,
      0.46528328115211637,
      0.33873335245172154,
      0.3707805089855898,
      0.47446398960854896,
      0.331567064366618,
      0.38836941101438627
    ],
    [
      0.3028523455089993,
      0.4465593057489716,
      0.4814560636187304,
      0.39873256842335936,
      0.3459661185586642,
      0.3463063966102846,
      0.3532969168375215,
      0.3974317656194577,
      0.32786818006606655,
      0.4035455464711857,
      0.43479952840535696,
      0.3103567469123787,
      0.3988784052069734,
      0.0,
      0.36264360121859696,
      0.43174388797297936,
      0.46585141017041787,
      0.36956602922832804,
      0.35797585374717156,
      0.4432875028330321,
      0.4135961042992351,
      0.3442776298222876,
      0.3911097614890766,
      0.47878955364907894,
      0.3236664898597921,
      0.3328181020012313,
      0.43845793359480356,
      0.37129534988222845,
      0.3578937375631146
    ],
    [
      0.26153567231479613,
      0.30099449578313453,
      0.3123612726242786,
      0.3478318576241761,
      0.2997877137975311,
      0.2771840867647444,
      0.3096594471630283,
      0.3063481554551335,
      0.3311877463986488,
      0.3791636154064859,
      0.33390975793099575,
      0.2978191115215181,
      0.3508859237166777,
      0.32790600808006354,
      0.0,
      0.36691593130176003,
      0.35532481338888733,
      0.34993418701073953,
      0.30753374552749513,
      0.29223970609522265,
      0.322479218734939,
      0.36173504386092503,
      0.2807925665692601,
      0.32556887446279426,
      0.2635265380517191,
      0.28582874264025815,
      0.30555857013201626,
      0.3214012981830816,
      0.39908633837014684
    ],
    [
      0.28318366124806427,
      0.3689094944924438,
      0.36374855005475726,
      0.39798437210062376,
      0.32594916173832367,
      0.30167296799932863,
      0.297601653103275,
      0.3195799050016712,
      0.3457765247322193,
      0.3825067321191118,
      0.4905960520528059,
      0.257548967551394,
      0.43491384539605105,
      0.439681015030031,
      0.35719469139553683,
      0.0,
      0.4029255312929958,
      0.31019776875749594,
      0.3725639040804689,
      0.3323612728865264,
      0.37800927967732134,
      0.2734443649156142,
      0.3303234378339659,
      0.4305176159754478,
      0.27000948360151744,
      0.3092807437801188,
      0.38457076022935177,
      0.33065934418551435,
      0.28352732685926485
    ],
    [
      0.3447190033985694,
      0.41727312910006487,
      0.47538880779396586,
      0.4063439351900031,
      0.43720264201124714,
      0.35227352347123464,
      0.39196739064637165,
      0.418613768207051,
      0.38824552129880696,
      0.4349852943589898,
      0.3811812249979458,
      0.2785264612787348,
      0.4730361842162065,
      0.448925434453334,
      0.3700508894059429,
      0.40738573795347977,
      0.0,
      0.3992360445174832,
      0.3987546138090601,
      0.4895680918060039,
      0.3967770627614635,
      0.31484839109639773,
      0.4337694073611269,
      0.5145798581762382,
      0.4072640069978013,
      0.3684621282327869,
      0.5473623239717231,
      0.44062407668278447,
      0.35652409976822375
    ],
    [
      0.39090468797276023,
      0.4893491741339322,
      0.544474645606216,
      0.46314597374769995,
      0.4692192744457633,
      0.41024099806980874,
      0.37443480840109356,
      0.5296599791860124,
      0.43965885177673814,
      0.45513852848350855,
      0.49088253686610006,
      0.31953525863473,
      0.4735627842558554,
      0.4222454610089681,
      0.45612058317852266,
      0.37911496183350013,
      0.4924739339959685,
      0.0,
      0.4579137260747592,
      0.48318992941497996,
      0.452459080683945,
      0.3769306538065049,
      0.39915121888726945,
      0.43562921883574246,
      0.37996859649496306,
      0.43137623626987653,
      0.4915849525821383,
      0.42298466475629537,
      0.3947174188884084
    ],
    [
      0.3542848474367517,
      0.4597924850681412,
      0.46401156193292126,
      0.4280499198700374,
      0.44228292088505206,
      0.3957710615772567,
      0.4342138034336014,
      0.40725948069715856,
      0.43252429261376935,
      0.464596280374898,
      0.4542069680207912,
      0.3035311998103869,
      0.535183790669413,
      0.44420349538211235,
      0.3401410852132798,
      0.47119003422056016,
      0.41005529901539095,
      0.41354200004020836,
      0.0,
      0.3799723888249762,
      0.40831855311817344,
      0.3679744866887258,
      0.3831179403389331,
      0.493046359466653,
      0.391710674854737,
      0.39678965079340367,
      0.4674233771306049,
      0.41459496510597704,
      0.38884745815634303
    ],
    [
      0.3237540522768494,
      0.4363388046000054,
      0.5226223483270804,
      0.39672826411105877,
      0.4097665898538423,
      0.3928870512466667,
      0.3963560404726294,
      0.44595589628484533,
      0.3817106384222788,
      0.37658426725573935,
      0.4449946459649867,
      0.26697764474164165,
      0.3923845849914813,
      0.48949781597157727,
      0.3498682193342748,
      0.387091870143242,
      0.5517514546879132,
      0.46077393354294593,
      0.37198607972621134,
      0.0,
      0.38816531624123773,
      0.31656707251787997,
      0.48508374099557905,
      0.514274411819452,
      0.4312854344424464,
      0.3879770649081311,
      0.4644575243057856,
      0.41004456082132923,
      0.3276986639149182
    ],
    [
      0.4108546025423394,
      0.449521578332156,
      0.41757540870136856,
      0.44188742255550895,
      0.433877044444811,
      0.3801618271090661,
      0.3926350909580236,
      0.5198449442235851,
      0.4099859529931069,
      0.54678378680338,
      0.47991901662066616,
      0.36016501702287806,
      0.46883132110444237,
      0.5063797279602207,
      0.41227437214729523,
      0.47980472152541376,
      0.4935046778308225,
      0.4170114254710853,
      0.45083675376525534,
      0.4299203469133286,
      0.0,
      0.40194489941459755,
      0.3775645124481788,
      0.4803757724225597,
      0.3949533925288271,
      0.4068006233431434,
      0.4922178580763421,
      0.4185917778637951,
      0.4307035344549248
    ],
    [
      0.3047255632572554,
      0.4084280797126887,
      0.4198920765786285,
      0.4304112822035462,
      0.38502179746068754,
      0.3616558639384315,
      0.4111317408689015,
      0.4125911947002687,
      0.41341266477865024,
      0.5005204948235531,
      0.42791113262309644,
      0.3628446822966016,
      0.4589825677510213,
      0.4269797962584916,
      0.4960501030613913,
      0.42392329392660466,
      0.3983276960913227,
      0.3919023131185271,
      0.4106282333282203,
      0.35460995230661685,
      0.4284030021713259,
      0.0,
      0.3715516848098128,
      0.39346259744536094,
      0.3539428946894001,
      0.3779902665068957,
      0.43698553529711437,
      0.3535737490732287,
      0.5032926089363754
    ],
    [
      0.3026337413702418,
      0.3877860776567674,
      0.44250849710293383,
      0.32492656003037945,
      0.34882805884654156,
      0.3424688116995689,
      0.35866748772963253,
      0.36090281324312223,
      0.30173814304582924,
      0.32427510634343104,
      0.3684188724356683,
      0.25747922988693084,
      0.3917694860338756,
      0.3924619044391391,
      0.2947247479418449,
      0.3586441763911241,
      0.41180060987225064,
      0.34248562243995817,
      0.3307959047193507,
      0.4300617482296263,
      0.3243913223015491,
      0.3036536722430767,
      0.0,
      0.44319221546584076,
      0.4208917521816411,
      0.3186547930377537,
      0.4458305151084685,
      0.3044056430994522,
      0.3032474592894516
    ],
    [
      0.32635225234435516,
      0.38605716491317676,
      0.48690146051394767,
      0.351403830496954,
      0.3874225208231328,
      0.3317116369281943,
      0.37550974113830304,
      0.3718030241349035,
      0.3768868935436249,
      0.3904975905094177,
      0.39697647572113626,
      0.2817761970164183,
      0.4217913302325049,
      0.44979400636054634,
      0.3264820073174719,
      0.4077019050936197,
      0.502608100362143,
      0.34375481577234956,
      0.4137443982957312,
      0.407599059378823,
      0.4021029079638958,
      0.3071130990799489,
      0.41219606157701905,
      0.0,
      0.38963193750773284,
      0.35391867388305287,
      0.48489794012118614,
      0.39866177648928436,
      0.3407837558006743
    ],
    [
      0.30820444228207955,
      0.34407755053557243,
      0.38456111807507654,
      0.31956216642618496,
      0.3724920454774099,
      0.3291086789415598,
      0.3605807345461738,
      0.32782514957923103,
      0.3090664844224278,
      0.37327663077901674,
      0.3183178639691071,
      0.24465536141120436,
      0.34828169804307496,
      0.3668090782814035,
      0.2974811595296074,
      0.3276297779684316,
      0.3908959813903756,
      0.36393367090796125,
      0.32548394394995506,
      0.38931725100806847,
      0.3487063466942726,
      0.3100937725307562,
      0.3755929319359885,
      0.4019912293901988,
      0.0,
      0.31923900316180354,
      0.42882713813264717,
      0.37176787078407103,
      0.3285597970910057
    ],
    [
      0.4161182324095485,
      0.48760412883342275,
      0.5260847313382842,
      0.44341771486159987,
      0.5368461716119373,
      0.4274419234950848,
      0.39623663568719625,
      0.5262897084192117,
      0.4907959463783931,
      0.4464084429065056,
      0.48356580101576574,
      0.3233685790943339,
      0.49025950849204514,
      0.3744112064302625,
      0.3402230351431963,
      0.40376608748592613,
      0.47306604276580244,
      0.49622211569753527,
      0.4612270523355968,
      0.4320327931131327,
      0.4235708084487948,
      0.36773341572293106,
      0.40671336187620577,
      0.472605485007046,
      0.4149331121922959,
      0.0,
      0.5418324637860703,
      0.42508351526948895,
      0.3837988253348086
    ],
    [
      0.42504773892286396,
      0.4836514042098268,
      0.5183982311986128,
      0.4447133407893258,
      0.5105736450438139,
      0.45590131811035417,
      0.4054743882575922,
      0.4958167024364417,
      0.4477551889487681,
      0.39750839283861183,
      0.5306690280453654,
      0.2666251568135791,
      0.5100417011910143,
      0.44370741994055485,
      0.3450119959817666,
      0.41648543529871374,
      0.5445139313849263,
      0.4254574400771769,
      0.45364467935868724,
      0.4360378782428971,
      0.4082788199260965,
      0.33476178034539417,
      0.4437977319601707,
      0.5034182437453016,
      0.39867880150626767,
      0.38338878011748734,
      0.0,
      0.4501132130470167,
      0.33780145705980114
    ],
    [
      0.3002246262458268,
      0.3316115761037477,
      0.41925233616056623,
      0.3932425909714916,
      0.442676263734602,
      0.31793423697440715,
      0.3093456613226988,
      0.39710385286212824,
      0.38267246401265886,
      0.32259015162333093,
      0.3519731205096539,
      0.2484181236126124,
      0.39020650131593637,
      0.39422592509438426,
      0.32497100206205976,
      0.37303742522192973,
      0.4349649664169901,
      0.37436454568399125,
      0.37611030773663456,
      0.3273957423647369,
      0.33477353190188053,
      0.2730711415457876,
      0.3198642019817086,
      0.3975480934452682,
      0.32136448310427435,
      0.33502350278172854,
      0.42552079880053983,
      0.0,
      0.2754036568323961
    ],
    [
      0.27333545192673037,
      0.2857206891062878,
      0.32852621416650396,
      0.3099833353464565,
      0.32770569296562213,
      0.275643696978765,
      0.34642317999816763,
      0.32014908968961686,
      0.28981847698517105,
      0.3988794460596319,
      0.33155364062897674,
      0.3302464073925122,
      0.3463968159740436,
      0.34062190463788955,
      0.41058092097963095,
      0.3337159229538049,
      0.3213505926962985,
      0.32078099232444424,
      0.31348038638323317,
      0.3123533217503869,
      0.36522240557390506,
      0.4152815547672497,
      0.3044813445832393,
      0.3450891291436864,
      0.3463767277950516,
      0.3130908297614057,
      0.32333111512350365,
      0.3090563757714164,
      0.0
    ]
  ],
  "row_avgs": [
    0.16214339306603526,
    0.40801743764560433,
    0.4564026898545827,
    0.3732027184100348,
    0.43843209240564784,
    0.37797691120477067,
    0.28376660411484034,
    0.39317750301042687,
    0.39930660676042323,
    0.36562727950820445,
    0.3841886231607533,
    0.3915605458903087,
    0.3937840873569109,
    0.3868222441185473,
    0.32051787281823063,
    0.3491156581461157,
    0.410496037605823,
    0.4402167192247165,
    0.4195227278835806,
    0.4115565711400725,
    0.43946169319918293,
    0.4078268881433579,
    0.35491589186376604,
    0.3866457344042695,
    0.3459406741873094,
    0.4432734587554436,
    0.43633120874280096,
    0.3533889582294275,
    0.32997127362370104
  ],
  "col_avgs": [
    0.3385729793848293,
    0.41141035776024865,
    0.43048940991711365,
    0.3909260079191616,
    0.4067955954778717,
    0.35202648549291976,
    0.3592724484480962,
    0.4083152012728135,
    0.3806717632390196,
    0.4048823896120134,
    0.4143054969477097,
    0.28483889419978214,
    0.42526341996591294,
    0.40543462599281316,
    0.3541249419496179,
    0.38027027370876515,
    0.4255781313701928,
    0.37722894053383804,
    0.3814909725481712,
    0.3737647026228133,
    0.37867029226306764,
    0.33165551814451,
    0.36543829943830647,
    0.41821950013685016,
    0.3466328781102609,
    0.3530859566905416,
    0.4358508458510134,
    0.37892040421356593,
    0.34945337126306875
  ],
  "combined_avgs": [
    0.25035818622543227,
    0.4097138977029265,
    0.4434460498858482,
    0.3820643631645982,
    0.42261384394175977,
    0.3650016983488452,
    0.3215195262814683,
    0.4007463521416202,
    0.3899891849997214,
    0.3852548345601089,
    0.3992470600542315,
    0.3381997200450454,
    0.4095237536614119,
    0.39612843505568024,
    0.33732140738392424,
    0.36469296592744044,
    0.4180370844880079,
    0.40872282987927727,
    0.4005068502158759,
    0.3926606368814429,
    0.4090659927311253,
    0.36974120314393394,
    0.3601770956510363,
    0.40243261727055984,
    0.3462867761487851,
    0.3981797077229926,
    0.4360910272969072,
    0.36615468122149675,
    0.3397123224433849
  ],
  "gppm": [
    642.9459680217503,
    648.0174905066189,
    640.7873406575053,
    660.648892566837,
    648.740966475114,
    676.8504565347578,
    670.4075373737812,
    647.5809401607828,
    661.5481951602827,
    651.7204962251948,
    648.253976628034,
    711.3358464327249,
    643.8171581099976,
    654.5186484604471,
    673.97745546799,
    661.6630651128842,
    641.5488447661508,
    664.4810374811017,
    663.7076661767452,
    667.1817509475533,
    663.6518381765966,
    689.5206376908194,
    672.547690901699,
    646.0545808120087,
    680.4688912947267,
    676.7021917477708,
    636.246644591707,
    662.6962357440542,
    678.7642754550805
  ],
  "gppm_normalized": [
    1.484330883785405,
    1.460886500147495,
    1.4434357975863628,
    1.484562357332271,
    1.4563519608338318,
    1.5262838024527612,
    1.5163186763271388,
    1.4551072137387868,
    1.4860577852331998,
    1.4669314160017268,
    1.4601425395145669,
    1.6054162706911368,
    1.4486207028648057,
    1.4695191402569256,
    1.5160486066933643,
    1.490914715402353,
    1.441040432188142,
    1.4880232063581673,
    1.4899582820696047,
    1.5048894062605516,
    1.4838829228059756,
    1.5514102701765335,
    1.507735178616478,
    1.4532842356127753,
    1.5286080164680222,
    1.5230682099585808,
    1.4294671418782716,
    1.4862997668691382,
    1.5278206104043157
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390,
    598,
    457,
    460,
    436,
    400,
    434,
    419,
    406,
    415,
    415,
    403,
    522,
    419,
    414,
    456,
    429,
    399,
    402,
    402,
    449,
    381,
    459,
    396,
    420,
    394,
    407,
    389,
    458,
    364,
    668,
    476,
    491,
    435,
    399,
    469,
    505,
    387,
    435,
    421,
    416,
    456,
    451,
    447,
    366,
    429,
    444,
    439,
    432,
    365,
    436,
    383,
    403,
    437,
    433,
    512,
    444,
    453,
    398,
    556,
    436,
    455,
    520,
    439,
    403,
    443,
    430,
    421,
    442,
    402,
    469,
    440,
    412,
    457,
    437,
    417,
    397,
    430,
    478,
    440,
    351,
    389,
    390,
    417,
    413,
    412,
    418,
    373,
    589,
    405,
    404,
    436,
    416,
    426,
    434,
    447,
    434,
    406,
    406,
    524,
    449,
    391,
    456,
    425,
    488,
    400,
    372,
    431,
    460,
    423,
    389,
    472,
    413,
    395,
    393,
    447,
    357,
    756,
    505,
    437,
    451,
    455,
    446,
    567,
    465,
    461,
    428,
    451,
    350,
    461,
    430,
    486,
    497,
    420,
    414,
    408,
    431,
    401,
    373,
    409,
    440,
    446,
    377,
    441,
    425,
    432
  ],
  "response_lengths": [
    3781,
    2869,
    2408,
    2556,
    2644,
    2621,
    3011,
    2594,
    2619,
    2357,
    2555,
    2087,
    2582,
    2320,
    2828,
    2872,
    2294,
    2357,
    2437,
    2347,
    2163,
    2086,
    2269,
    2550,
    2427,
    2088,
    2444,
    2464,
    2394
  ]
}