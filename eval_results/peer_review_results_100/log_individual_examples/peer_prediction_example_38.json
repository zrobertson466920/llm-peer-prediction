{
  "example_idx": 38,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSINGLE-LEVEL ADVERSARIAL DATA SYNTHESIS BASED ON NEURAL TANGENT KERNELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGenerative adversarial networks (GANs) have achieved impressive performance in data synthesis and have driven the development of many applications. However, GANs are known to be hard to train due to their bilevel objective, which leads to the problems of convergence, mode collapse, and gradient vanishing. In this paper, we propose a new generative model called the generative adversarial NTK (GA-NTK) that has a single-level objective. The GA-NTK keeps the spirit of adversarial learning (which helps generate plausible data) while avoiding the training difficulties of GANs. This is done by modeling the discriminator as a Gaussian process with a neural tangent kernel (NTK-GP) whose training dynamics can be completely described by a closed-form formula. We analyze the convergence behavior of GA-NTK trained by gradient descent and give some sufficient conditions for convergence. We also conduct extensive experiments to study the advantages and limitations of GA-NTK and propose some techniques that make GA-NTK more practical.1\n\n1\n\nINTRODUCTION\n\nGenerative adversarial networks (GANs) (Goodfellow et al., 2014; Radford et al., 2016), a branch of deep generative models based on adversarial learning, have received much attention due to their novel problem formulation and impressive performance in data synthesis. Variants of GANs have also driven recent developments of many applications, such as super-resolution (Ledig et al., 2017), image inpainting (Xu et al., 2014), and video generation (Vondrick et al., 2016).\n\nA GANs framework consists of a discriminator network D and a generator network G parametrized by θD and θG, respectively. Given a d-dimensional data distribution Pdata and a c-dimensional noise distribution Pnoise, the generator G maps a random noise z ∈ Rc to a point G(z) ∈ Rd in the data space, while the discriminator D takes a point x(cid:48) ∈ Rd as the input and tells whether x(cid:48) is real or fake, i.e., D(x(cid:48)) = 1 if x(cid:48) ∼ Pdata and D(x(cid:48)) = 0 if x(cid:48) ∼ Pgen, where Pgen is the distribution of G(z) and z ∼ Pnoise. The objective of GANs is typically formulated as a bilevel optimization problem:\n\narg min\n\nθG\n\nmax θD\n\nEx∼Pdata[log D(x)] + Ez∼Pnoise [log(1 − D(G(z)))].\n\n(1)\n\nThe discriminator D and generator G aim to break each other through the inner max and outer min objectives, respectively. The studies by Goodfellow et al. (2014); Radford et al. (2016) show that this adversarial formulation can lead to a better generator that produces plausible data points/images.\n\nHowever, GANs are known to be hard to train due to the following issues (Goodfellow, 2016). Failure to converge. In practice, Eq. (1) is usually only approximately solved by an alternating first-order method such as the alternating stochastic gradient descent (SGD). The alternating updates for θD and θG may cancel each other’s progress. During each alternating training step, it is also tricky to balance the number of SGD updates for θD and that for θG, as a too small or large number for θD leads to low-quality gradients for θG. Mode collapse. The alternating SGD is attracted by stationary points and therefore is not good at distinguishing between a minθG maxθD problem and a maxθD minθG problem. When the solution to the latter is returned, the generator tends to always produce the points at modes that best deceive the discriminator, making Pgen of low\n\n1Our code is available on GitHub at https://github.com/ga-ntk/ga-ntk.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ndiversity.2 Vanishing gradients. At the beginning of a training process, the finite real and fake training data may not overlap with each other in the data space, and thus the discriminator may be able to perfectly separate the real from fake data. Given the cross-entropy loss (or more generally, any f -divergence measure (Rényi et al., 1961) between Pdata and Pgen), the value of the discriminator becomes saturated on both sides of the decision boundary, resulting in zero gradients for θG.\n\nIn this paper, we argue that the above issues are rooted in the modeling of D. In most existing variants of GANs, the discriminator is a deep neural network with explicit weights θD. Under gradient descent, the gradients of θG in Eq. (1) cannot be back-propagated through the inner maxθD problem because otherwise it requires the computation of high-order derivatives of θD. This motivates the use of alternating SGD, which in turn causes the convergence issues and mode collapse. Furthermore, the D is a single network whose particularity may cause a catastrophic effect, such as the vanishing gradients, during training.\n\nWe instead model the discriminator D as a Gaussian process whose mean and covariance are governed by a kernel function called the neural tangent kernel (NTK-GP) (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019). The D approximates an infinite ensemble of infinitely wide neural networks in a nonparametric manner and has no explicit weights. In particular, its training dynamics can be completely described by a closed-form formula. This allows us to simplify adversarial data synthesis into a single-level optimization problem, which we call the generative adversarial NTK (GA-NTK). Moreover, since D is an infinite ensemble of networks, the particularity of a single element network does not drastically change the training process. This makes GA-NTK less prone to vanishing gradients and stabilizes training even when an f -divergence measure between Pdata and Pgen is used as the loss of D. The following summarizes our contributions:\n\n• We propose a single-level optimization method, named GA-NTK, for adversarial data synthesis. It can be solved by ordinary gradient descent, avoiding the difficulties of bi-level optimization in GANs.\n\n• We prove the convergence of GA-NTK training under mild conditions. We also show that D being an infinite ensemble of networks can provide smooth gradients for G, which stabilizes GA-NTK training and helps fight vanishing gradients.\n\n• We propose some practical techniques to reduce the memory consumption of GA-NTK\n\nduring training and improve the quality of images synthesized by GA-NTK.\n\n• We conduct extensive experiments on real-world datasets to study the advantages and limitations of GA-NTK. In particular, we find that GA-NTK has much lower sample complexity as compared to GANs, and the presence of a generator is not necessary to generate images under the adversarial setting.\n\nNote that the goal of this paper is not to replace existing GANs nor advance the state-of-the-art performance, but to show that adversarial data synthesis can be done via a single-level modeling. Our work has implications for future research. In particular, the low sample complexity makes GA-NTK suitable for applications, such as medical imaging, where data are personalized or not easily collectible. In addition, GA-NTK bridges the gap between kernel methods and adversarial data/image synthesis and thus enables future studies on the relationship between kernels and generated data.\n\n2 RELATED WORK\n\n2.1 GANS AND IMPROVEMENTS\n\nGoodfellow et al. (2014) proposes GANs and gives a theoretical convergence guarantee in the function space. However, in practice, one can only optimize the generator and discriminator in Eq. (1) in the parameter/weight space. Many techniques have been proposed to make the bilevel optimization easier. Failure to convergence. To solve this problem, studies devise new training algorithms for GANs (Nagarajan & Kolter, 2017; Daskalakis et al., 2018) or more general minimax problems (Thekumparampil et al., 2019; Mokhtari et al., 2020). But recent works by Mescheder et al. (2018); Farnia & Ozdaglar (2020) show that there may not be a Nash equilibrium solution in GANs. Mode\n\n2Mode collapse can be caused by other reasons, such as the structure of G. This paper only solves the\n\nproblem due to alternating SGD.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ncollapse. Metz et al. (2017) alleviates this issue by back-propagating the computation of θG through the discriminators trained with several steps to strengthen the minθG maxθD property. Other works mitigate mode collapse by diversifying the modes of D through regularization (Che et al., 2017; Mao et al., 2019), modeling D as an ensemble of multiple neural networks (Durugkar et al., 2017; Ghosh et al., 2018), or using additional auxiliary networks(Srivastava et al., 2017; Bang & Shim, 2021; Li et al., 2021). Vanishing gradients. Mao et al. (2017) tries to solve this problem by using the Pearson χ2-divergence between Pdata and Pgen as the loss to penalize data points that are far away from the decision boundary. However, it still suffers from vanishing gradients as any f -divergence measure, including the cross-entropy loss and Pearson χ2-divergence, cannot measure the difference between disjoint distributions (Sajjadi et al., 2018). Later studies replace the loss with either the Wasserstein distance (Arjovsky et al., 2017; Gulrajani et al., 2017) or maximum mean discrepancy (Gretton et al., 2012; Li et al., 2015; 2017) that can measure the divergence of disjoint Pdata and Pgen. In addition, the works by Miyato et al. (2018); Qi (2020) aim to constrain the Lipschitz continuity of the discriminator to prevent its value from being saturated.\n\nDespite that many efforts have been made to improve the training of GANs, most existing approaches address only one or two issues at a time with different assumptions, and in the meanwhile, they introduce new hyperparameters or side effects. For example, in the Wasserstein GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) mentioned above, efficient computation of Wasserstein distance requires the discriminator to be Lipschitz continuous. However, realizing Lipschitz continuity introduces new hyperparameters and could limit the expressiveness of the discriminator (Anil et al., 2019). Until now, training GANs is still not an easy task because one has to 1) tune many hyperparameters and 2) strike a balance between the benefits and costs of different training techniques to generate satisfactory data points/images.\n\n2.2 GAUSSIAN PROCESSES AND NEURAL TANGENT KERNELS\n\nConsider an infinite ensemble of infinitely wide networks that use the mean square error (MSE) as the loss and are trained by gradient descent. Recent developments in deep learning theory show that the prediction of the ensemble can be approximated by a special instance of Gaussian process called NTK-GP (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019). The NTK-GP is a Bayesian method, so it outputs a distribution of possible values for an input point. The mean and covariance of the NTK-GP prediction are governed by a kernel function k(·, ·) called the neural tangent kernel (NTK). Given two data points xi and xj, the k(xi, xj) represents the similarity score of the two points in a kernel space, which is fixed once the hyperparameters of the initial weights, activation function, and architecture of the networks in the target ensemble are determined.\n\nHere, we focus on the mean prediction of NTK-GP as it is relevant to our study. Consider a supervised learning task given Dn = (X n ∈ Rn×d, Y n ∈ Rn×c) as the training set, where there are n examples and each example consists of a pair of d-dimensional input and c-dimensional output. Let Kn,n ∈ Rn×n be the kernel matrix for X n, i.e., K n,n j,:). Then, at time step t during gradient descent, the mean prediction of NTK-GP for X n evolve as\n\ni,j = k(X n\n\ni,:, X n\n\n(I n − e−ηKn,nt)Y n ∈ Rn×c,\n\n(2)\n\nwhere I n ∈ Rn×n is an identity matrix and η is a sufficiently small learning rate (Jacot et al., 2018; Lee et al., 2019).\n\nThe NTK used in Eq. (2) can be extended to support different network architectures, including convolutional neural networks (CNNs) (Arora et al., 2019; Novak et al., 2019b), recurrent neural networks (RNNs) (Alemohammad et al., 2021; Yang, 2019b), networks with the attention mechanism (Hron et al., 2020), and other architectures (Yang, 2019b; Arora et al., 2019). Furthermore, studies (Novak et al., 2019a; Lee et al., 2020; Arora et al., 2020; Geifman et al., 2020) show that NTK-GPs perform similarly to their finite-width counterparts (neural networks) in many situations and sometimes even better on small-data tasks.\n\nA recent study by Franceschi et al. (2021) analyzes the behavior of GANs from the NTK perspective by taking into account the alternating optimization. It shows that, in theory, the discriminator can provide a well-defined gradient flow for the generator, which is opposite to previous theoretical interpretations (Arjovsky & Bottou, 2017). Our work, on the other hand, focuses on adversarial data\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nsynthesis without alternating optimization.3 We make contributions in this direction by (1) formally proving the convergence of the proposed single-level optimization, (2) showing that a generator network is not necessary to generate plausible images (although it might be desirable), and (3) proposing the batch-wise and multi-resolutional extensions that respectively improve the memory efficiency of training and global coherency of generated image patterns.\n\n3 GA-NTK\n\nWe present a new adversarial data synthesis method, called the generative adversarial NTK (GANTK), based on the NTK theory (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019). For simplicity of presentation, we let G(z) = z ∈ Rd and focus on the discriminator for now. We will discuss the case where G(·) is a generator network in Section 3.2. Given an unlabeled, d-dimensional dataset X n ∈ Rn×d of n points, we first augment X n to obtain a labeled training set D2n = (X n ⊕ Zn ∈ R2n×d, 1n ⊕ 0n ∈ R2n), where Zn ∈ Rn×d contains n generated points, 1n ∈ Rn and 0n ∈ Rn are label vectors of ones and zeros, respectively, and ⊕ is the vertical stack operator. Then, we model a discriminator trained on D2n as an NTK-GP. Let K2n,2n ∈ R2n×2n be the kernel matrix for X n⊕Zn, where the value of each element K 2n,2n = k((X n⊕Zn)i,:, (X n⊕Zn)j,:) can be computed once we decide the initialization, activation function, and architecture of the element networks in the target infinite ensemble, i.e., the discriminator. By Eq. (2) and let λ = η · t, the mean predictions of the discriminator can be written as\n\ni,j\n\nD(X n, Zn; k, λ) = (I 2n − e−λK2n,2n\n\n)(1n ⊕ 0n) ∈ R2n,\n\nwhere I 2n ∈ R2n×2n is an identity matrix. We formulate the objective of GA-NTK as follows:\n\narg min Zn\n\nL(Zn), where L(Zn) = (cid:107)12n − D(X n, Zn; k, λ)(cid:107).\n\n(3)\n\n(4)\n\nL(·) is the loss function and 12n ∈ R2n is a vector of ones. Statistically, Eq. (4) aims to minimize the Pearson χ2-divergence (Jeffreys, 1946), a case of f -divergence, between Pdata + Pgen and 2Pgen, where Pgen is the distribution of generated points. Please see Section 6 in Appendix for more details.\n\nGA-NTK formulates an adversarial data synthesis task as a single-level optimization problem. On one hand, GA-NTK aims to find points Zn that best deceive the discriminator such that it outputs wrong labels 12n for these points. On the other hand, the discriminator is trained on D2n with the correct labels 1n ⊕ 0n and therefore has the opposite goal of distinguishing between the real and generated points. Such an adversarial setting can be made single-level because the training dynamics of the discriminator D by gradient descent can be completely described by a closed-form formula in Eq. (3)—any change of Zn causes D to be “retrained” instantly. Therefore, one can easily solve Eq. (4) by ordinary SGD.\n\nTraining. Before running SGD, one needs to tune the hyperparameter λ. We show in the next section that the value of λ should be large enough but finite. Therefore, the complete training process of GA-NTK is to 1) find the minimal λ that allows the discriminator to separate real data from pure noises in an auxiliary task, and 2) solve Zn in Eq. (4) by ordinary SGD with the fixed λ. Please see Section 7.3 in Appendix for more details.\n\n3.1 MERITS\n\nAs compared to GANs, GA-NTK offers the following advantages: Convergence. The GA-NTK can be trained by ordinary gradient descent. This gives much nicer convergence properties:\n\nTheorem 3.1 Let s be the number of the gradient descent iterations solving Eq. (4), and let Zn,(s) i,j and Zn,(0) be the solution at the s-th iteration. Suppose the following values are bounded: (a) X n ,\n∀i, j, (b) t and η, and (c) σ and L. Also, assume that (d) X n contains finite, non-identical, normalized rows. Then, for a sufficiently large t, we have\n\ni,j\n\n(cid:107)∇Zn L(Zn,(j))(cid:107)2 ≤ O(\n\nmin j≤s\n\n1 s − 1\n\n).\n\n3From GAN perspective, our work can be regarded as a special case of the framework proposed by Franceschi et al. (2021), where the discriminator neglects the effect of historical generator updates and only distinguish between the true and currently generated data at each alternating step.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nWe prove the above theorem by showing that, with a large enough λ, ∇Zn L(Zn,(s)) is smooth enough to lead to the convergence of gradient descent. For more details, please see Section 6 in Appendix. Diversity. GA-NTK avoids mode collapse due to the confusion between the min-max and max-min problems in alternating SGD. Given different initial values, the generated points in Zn can be very different from each other. No vanishing gradients, no side effects. The hyperparameter λ controls how much D should learn from the true and fake data during each iteration. Figure 5 shows the gradients of D with a finite λ, which do not saturate. This avoids the necessity of using a loss that imposes side effects, such as the Wasserstein distance (Arjovsky et al., 2017; Gulrajani et al., 2017) whose efficient evaluation requires Lipschitz continuity of D.\n\n3.2 GA-NTK IN PRACTICE\n\nScalability. To generate a large number of points, we can parallelly solve multiple Zn’s in Eq. (4) on different machines. On a single machine, the gradients of Zn need to be back-propagated through the computation of K2n,2n, which has O(n2) space complexity. This may incur scalability issues for large datasets. Although recent efforts by Arora et al. (2019); Bietti & Mairal (2019); Han et al. (2021); Zandieh et al. (2021) have been made to reduce the time and space complexity of the evaluation of NTK and its variants, they are still at an early stage of development and the consumed space in practice may still be too large. To alleviate this problem, we propose the batch-wise GANTK with the objective\n\narg min Zn\n\nE\n\nX b/2⊂X n,Zb/2⊂Zn(cid:107)1b − D(X b/2, Zb/2; k, λ)(cid:107),\n\n(5)\n\nthat can be solved using mini-batches: during each gradient descent iteration, we 1) randomly sample a batch of b rows in X n ⊕ Zn and their corresponding labels, and 2) update Zn based on Kb,b. Although the batch-wise GA-NTK is cosmetically similar to the original GA-NTK, it solves a different problem. In the original GA-NTK, the Zn aims to fool a single discriminator D trained on 2n examples, while in the batch-wise GA-NTK, the Zn’s goal is to deceive many discriminators, each trained on b examples only. Fortunately, Shankar et al. (2020); Arora et al. (2020) have shown that NTK-based methods perform well on small datasets. We will conduct experiments to verify this later.\n\nGenerator Network. So far, we let G(z) = z and show that a generator is not necessary in adversarial data synthesis.4 Nevertheless, the presence of a generator network may be favorable in some applications to save time and memory at inference time. This can be done by extending the batch-wise GA-NTK as follows:\n\narg min\n\nθG\n\nE\n\nX b/2⊂X n,Zb/2∼N (0,I)(cid:107)1b − D(X b/2, G(Zb/2; θG); k, λ)(cid:107),\n\n(6)\n\nwhere G(· ; θG) is a generator network parametrized by θG, and Z ∈ Rl where l ≤ d. Note that this is still a single-level objective, and θG can be solved by gradient descent. We denote this variant GA-NTKg.\n\nImage Quality. To generate images, one can pair up GA-NTK with a convolutional neural tangent kernel (CNTK) (Arora et al., 2019; Novak et al., 2019b; Garriga-Alonso et al., 2019; Yang, 2019a) that approximates a CNN with infinite channels. This allows the NTK-GP (discriminator) to distinguish between real and fake points based on local patterns in the pixel space. However, the images synthesized by this GA-NTK variant may lack global coherency, just like the images generated by the CNN-based GANs (Radford et al., 2016; Salimans et al., 2016). Many efforts have been made to improve the image quality of CNN-based GANs, and this paper opens up opportunities for them to be adapted to the kernel regime. In particular, we propose the multi-resolutional GA-CNTK based on the work by Wang et al. (2018), whose objective is formulated as:\n\narg min Zn\n\n(cid:88)\n\nm\n\n(cid:107)12n − Dm(poolm(X n), poolm(Zn); km, λm)(cid:107),\n\n(7)\n\nwhere Dm is an NTK-GP taking input at a particular pixel resolution and poolm(·) is a downsample operation (average pooling) applied to each row of X n and Zn. The generated points in Zn aim to simultaneously fool multiple NTK-GPs (discriminators), each classifying real and fake images at a distinct pixel resolution. The NTK-GPs working at low and high resolutions encourage global coherency and details, respectively, and together they lead to more plausible points in Zn.\n\n4In GANs, solving Z directly against a finite-width discriminator is infeasible because it amounts to finding adversarial examples (Goodfellow et al., 2015) whose gradients are known to be very noisy (Ilyas et al., 2019).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nWe conduct experiments to study how GA-NTK works in image generation.\n\nDatasets. We consider the unsupervised/unconditional image synthesis tasks over real-world datasets, including MNIST (LeCun et al., 2010), CIFAR-10 (Krizhevsky, 2009), CelebA (Liu et al., 2015), CelebA-HQ (Liu et al., 2015), and ImageNet (Deng et al., 2009). To improve training efficiency, we resize CelebA images to 64×64 and ImageNet images to 128×128 pixels, respectively. We also create a 2D toy dataset consisting of 25-modal Gaussian mixtures of points to visualize the behavior of different image synthesis methods. GA-NTK implementations. GA-NTK works with different NTK-GPs. For the image synthesis tasks, we consider the NTK-GPs that model the ensembles of fully-connected networks (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019) and convolutional networks (Arora et al., 2019; Novak et al., 2019b; Garriga-Alonso et al., 2019; Yang, 2019a), respectively. We implement GA-NTK using the Neural Tangents library (Novak et al., 2019a) and call the variants based on the former and latter NTK-GPs the GA-FNTK and GA-CNTK, respectively. In GA-FNTK, an element network of the discriminator has 3 infinitely wide, fully-connected layers with ReLU non-linearity, while in GA-CNTK, an element network follows the architecture of InfoGAN (Chen et al., 2016) except for having infinite filters at each layer. We tune the hyperparameters of GA-FNTK and GA-CNTK following the method proposed in Poole et al. (2016); Schoenholz et al. (2017); Raghu et al. (2017). We also implement their batchwise, generator, and multi-resolutional variants described in Section 3.2. See Section 7 in Appendix for more details. Baselines. We compare GA-NTK with some popular variants of GANs, including vanilla GANs (Goodfellow et al., 2014), DCGAN (Radford et al., 2016), LSGAN (Mao et al., 2017), WGAN (Arjovsky et al., 2017), WGAN-GP (Gulrajani et al., 2017), SNGAN (Miyato et al., 2018) and StyleGAN2 (Karras et al., 2020). To give a fair comparison, we let the discriminator of each baseline follow the architecture of InfoGAN (Chen et al., 2016) and tune the hyperparameters using grid search. Metrics. We evaluate the quality of a set of generated images using the Fréchet Inception Distance (FID) (Heusel et al., 2017). The lower the FID score the better. We find that an image synthesis method may produce downgrade images that look almost identical to some images in the training set. Therefore, we also use a metric called the average max-SSIM (AM-SSIM) that calculates the average of the maximum SSIM score (Wang et al., 2004) between Pgen and Pdata:\n\nAM-SSIM(Pgen, Pdata) = Ex(cid:48)∼Pgen[ max\n\nx∼Pdata\n\nSSIM(x(cid:48), x)].\n\nA generated image set will have a higher AM-SSIM score if it contains downgrade images. Environment and limitations. We conduct all experiments on a cluster of machines having 80 NVIDIA Tesla V100 GPUs. As discussed in 3.2, GA-NTK consumes a significant amount of memory on each machine due to the computations involved in the kernel matrix K2n,2n. With the current version of Neural Tangents library (Novak et al., 2019a) and a V100 GPU of 32GB RAM, the maximum sizes of the training set from MNIST, CIFAR-10, CelebA, and ImageNet are 1024, 512, 256, and 128, respectively (where the computation graph and backprop operations of K2n,2n consume about 27.5 GB RAM excluding other necessary operations). Since our goal is not to achieve state-of-the-art performance but to compare different image synthesis methods, we train all the methods using up to 256 images randomly sampled from all classes of MNIST, the “horse” class CIFAR-10, the “male with straight hair” class of CelebA, and the “daisy” class of ImageNet, respectively. We will conduct larger-scale experiments in Section 9.2. For more details about our experiment settings, please see Section 7 in Appendix.\n\n4.1\n\nIMAGE QUALITY\n\nWe first study the quality of the images synthesized by different methods. Table 1 summarizes the FID and AM-SSIM scores of the generated images. LSGAN and DCGAN using f -divergence as the loss function give high FID and fail to generate recognizable images on CIFAR-10 and CelebA datasets due to the various training issues mentioned previously. StyleGAN, although being able to generate impressive images with sufficient training data, gives high FID here due to the high sample complexity of the style-based generator. Other baselines, including WGAN, WGAN-GP, and SN-GAN, can successfully generate recognizable images on all datasets, as shown in Figure 1. In particular, WGAN-GP performs the best among the GAN variants. However, WGAN-GP limits the Lipschitz continuity of the discriminator and gives higher FID scores than GA-CNTK. Also, it gives higher AM-SSIM values as the size of the training set decreases, implying there are many\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: The FID and AM-SSIM scores of the images generated by different methods.\n\nMetric\n\nDCGAN\n\nLSGAN\n\nWGAN\n\nWGANGP\n\nSNGAN\n\nStyleGAN\n\nGACNTK\n\nGACNTKg\n\nFID\n\n27.43\n\nAMSSIM\n\n0.84\n\nFID\n\n31.89\n\nAMSSIM\n\n0.85\n\nFID\n\n69.76\n\nAMSSIM\n\n0.69\n\n69.76\n\n0.79\n\n38.52\n\n0.80\n\n35.33\n\n0.78\n\n50.69\n\n0.77\n\n49.28\n\n0.74\n\n50.33\n\n0.72\n\n32.49\n\n0.83\n\n30.20\n\n0.76\n\n24.37\n\n0.73\n\n57.89\n\n0.67\n\n38.33\n\n0.67\n\n29.49\n\n0.70\n\n91.82\n\n0.69\n\n88.31\n\n0.66\n\n84.7\n\n0.65\n\n31.10\n\n0.49\n\n21.14\n\n0.52\n\n14.96\n\n0.54\n\n32.43\n\n0.71\n\n36.50\n\n0.72\n\n51.21\n\n0.73\n\nFID\n\n312.21\n\n258.41\n\n117.85\n\n49.29\n\n118.16\n\n406.02\n\n55.54\n\n106.44\n\nAMSSIM\n\n0.22\n\n0.25\n\n0.29\n\n0.74\n\n0.28\n\n0.64\n\n0.41\n\nFID\n\n229.94\n\n339.27\n\n101.90\n\n68.53\n\n128.65\n\n484.36\n\n39.98\n\nAMSSIM\n\n0.36\n\n0.10\n\n0.26\n\n0.60\n\n0.21\n\n0.39\n\n0.41\n\nFID\n\n181.15\n\n255.19\n\n111.92\n\n85.34\n\n107.29\n\n426.58\n\n28.40\n\nAMSSIM\n\n0.27\n\n0.22\n\n0.22\n\n0.46\n\n0.20\n\n0.26\n\n0.42\n\nFID\n\n489.82\n\n83.71\n\n122.36\n\n83.71\n\n169.04\n\n323.37\n\n30.83\n\nAMSSIM\n\n0.02\n\n0.05\n\n0.29\n\n0.56\n\n0.29\n\n0.23\n\n0.60\n\nFID\n\n55.01\n\n450.81\n\n125.82\n\n92.73\n\n168.11\n\n337.58\n\n33.51\n\nAMSSIM\n\n0.03\n\n0.11\n\n0.28\n\n0.54\n\n0.28\n\n0.21\n\n0.51\n\nFID\n\n461.95\n\n403.79\n\n108.07\n\n79.36\n\n161.20\n\n333.16\n\n63.15\n\nAMSSIM\n\n0.04\n\n0.09\n\n0.31\n\n0.39\n\n0.27\n\n0.30\n\n0.38\n\n0.44\n\n61.19\n\n0.44\n\n55.46\n\n0.44\n\n95.91\n\n0.21\n\n58.39\n\n0.38\n\n78.46\n\n0.40\n\nT S\n\nI\n\nN M\n\n0 1\n-\n\nR A\nF\n\nI\n\nC\n\nA b\ne l\ne C\n\nn\n\n64\n\n128\n\n256\n\n64\n\n128\n\n256\n\n64\n\n128\n\n256\n\n) a\n(\n\n) b\n(\n\n) c\n(\n\n) d\n(\n\n) e\n(\n\nN A\nG W\n\nP G\nN A\nG W\n\nN A\nG N\nS\n\nK T\nN C\nA G\n\ng K\nT N\nC A\nG\n\nFigure 1: The images generated by different methods on MNIST, CIFAR-10, and CelebA datasets given only 256 training images.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GA-CNTK\n\n(b) GA-CNTKg\n\nFigure 2: The images generated by GA-CNTK (a) without and (b) with a generator given 256 CelebA-HQ training images.\n\n(a) MNIST\n\n(b) CIFAR-10\n\n(c) CelebA\n\nFigure 3: The learning curve and image quality at different stages of a training process.\n\ndowngrade images that look identical to some training images. This is because the Wasserstein distance, which is also called the earth mover’s distance, allows fewer ways of moving when there are less available “earth” (i.e., the density values of Pdata and Pgen) due to a small n, and thus the Pgen needs to be exactly the same as Pdata to minimize the distance.5 The GA-NTK variants, including GA-CNTK and GA-CNTKg (“g” means “with generator”), perform relatively well due to their lower sample complexity, which aligns with the previous observations (Shankar et al., 2020; Arora et al., 2020) in different context.\n\nNext, we compare the images generated by the multi-resolutional GA-CNTK and GA-CNTKg (see Section 3.2) on the CelebA-HQ dataset. The multi-resolutional GA-CNTK employs 3 discriminators working at 256×256, 64×64, and 16×16 pixel resolutions, respectively. Figure 2 shows the results. We can see that the multi-resolutional GA-CNTK (without a generator) gives better-looking images than GA-CNTKg (with a generator) because learning a generator, which maps two spaces, is essentially a harder problem than finding a set of plausible z’s. Although synthesizing data faster at inference time, a generator may not be necessary to generate high-quality images under the adversarial setting.\n\n4.2 TRAINING STABILITY\n\nConvergence. Figure 3 shows the learning curve and the relationship between the image quality and the number of gradient descent iterations during a training process of GA-CNTK. We find that\n\n5The problem of Lipschitz continuity may be alleviated when n becomes larger.\n\n(a) Truth\n\n(b) Vanilla GAN (c) LSGAN (d) WGAN (e) WGAN-GP (f) SN-GAN (g) GA-FNTK\n\nFigure 4: Visualization of distribution alignment and mode collapse on a 2D toy dataset.\n\n8\n\n0101010101010101010101010101Under review as a conference paper at ICLR 2023\n\nGA-CNTK easily converges under various conditions, which is supported by Theorem 3.1. Furthermore, we can see a correlation between the image quality and the loss value—as the loss becomes smaller, the quality of the synthesized images improves. This correlation can save human labor from monitoring the training processes, which is common when training GANs. Note that the images generated in the latter stage of training contain recognizable patterns that change over training time. This is a major source of GA-CNTK creativity. Please see Section 9.4 for more discussions. Mode collapse. To study how different methods align Pgen with Pdata, we train them using a 2D toy training set where Pdata is a 25modal Gaussian mixture. We use two 3-layer fully-connected neural networks as the generator and discriminator for each baseline and an ensemble of 3-layer, infinitely wide counterpart as the discriminator in GA-FNTK. For GANs, we stop the alternating SGD training when the generator receives 1000 updates, and for GAFNTK, we terminate the GD training after 1000 iterations. Figure 4 shows the resultant Pgen of different methods. GA-FNTK avoids mode collapse due to the use of alternating SGD. Gradient vanishing. To verify that GA-NTK gives no vanishing gradients with a finite λ, we conduct an experiment using another toy dataset consisting of 256 MNIST images and 256 random noises. We replace the discriminator of GA-CNTK with a single parametric network of the same architecture but finite width. We train the finite-width network on the toy dataset by minimizing the MSE loss using gradient descent. We set the training iteration to a large value (65536) to simulate the situation where the network value becomes saturated on both sides of the decision boundary. Figure 5 compares the gradients of a generated image Zn i,: in Eq. (4) obtained from 1) the finite-width network and 2) the corresponding GA-CNTK with a large t. As Zn i,: evolves through gradient descent iterations, the norm of its gradients obtained from the finite-width discriminator quickly shrinks to zero. On the other hand, the gradient norm obtained from the discriminator of GA-CNTK is always positive thanks to the infinite ensembling.\n\nFigure 5: Comparison between the gradients of a Zi,: in Eq. (4) obtained from different types of D.\n\n4.3 SCALABILITY\n\nUnlike GA-CNTK, the GA-CNTKg is batch-wise and thus can be trained by more examples. Here, we scale up WGAN-GP and GA-CNTKg by training them on CelebA dataset consisting of 2048 images. The batch size is 256. Table 2 summarizes the FID and AM-SSIM scores of the generated images. On MNIST, WGAN-GP slightly outperforms GA-CNTKg. The training of WGAN-GP on MNIST is easy, so GA-CNTKg does not offer much advantage. However, in a more complex task like CIFAR10 or CelebA, GA-CNTKg outperforms WGAN-GP, suggesting that our single-level modeling is indeed beneficial.\n\nWe have conducted more experiments. Please see Appendix for their results.\n\n5 CONCLUSION\n\nTable 2: The FID and AM-SSIM scores of the images output by WGAN-GP and GA-CNTKg trained on 2048 CelebA images with batch size 256.\n\nn=2048\n\nMetric\n\nWGANGP\n\nGACNTKg\n\nMNIST\n\nCIFAR-10\n\nCelebA\n\nFID\n\nASSIM\n\nFID\n\nASSIM\n\nFID\n\nASSIM\n\n23.47\n\n0.786\n\n110.70\n\n0.404\n\n67.29\n\n0.337\n\n56.73\n\n0.787\n\n78.85\n\n0.432\n\n59.91\n\n0.411\n\nWe proposed GA-NTK and showed that adversarial data synthesis can be done via single-level modeling. It can be solved by ordinary gradient descent, avoiding the difficulties of bi-level training of GANs. We analyzed the convergence behavior of GA-NTK and gave sufficient conditions for convergence. Extensive experiments were conducted to study the advantages and limitations of GANTK. We proposed the batch-wise and multi-resolutional variants to improve memory efficiency and image quality, and showed that GA-NTK works either with or without a generator network. GA-NTK works well with small data, making it suitable for applications where data are hard to collect. GA-NTK also opens up opportunities for one to adapt various GAN enhancements into the kernel regime. These are matters of our future inquiry.\n\n9\n\n025005000750010000iterations0.00.20.40.60.81.0L2 norm of gradients1e3GA-CNTKCNNUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSina Alemohammad, Zichao Wang, Randall Balestriero, and Richard G. Baraniuk. The recurrent\n\nneural tangent kernel. In Proc. of ICLR, 2021.\n\nCem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In Proc.\n\nof ICML, 2019.\n\nMartin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial\n\nnetworks. In Proc. of ICLR, 2017.\n\nMartin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks.\n\nIn Proc. of ICML, 2017.\n\nSanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On\n\nexact computation with an infinitely wide neural net. In Proc. of NeurIPS, 2019.\n\nSanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu. Harnessing the power of infinitely wide deep nets on small-data tasks. In Proc. of ICLR, 2020.\n\nDuhyeon Bang and Hyunjung Shim. Mggan: Solving mode collapse using manifold-guided training.\n\nIn Proc. of CVPR, 2021.\n\nPaul Bergmann, Sindy Löwe, Michael Fauser, David Sattlegger, and Carsten Steger. Improving unsupervised defect segmentation by applying structural similarity to autoencoders. In VISIGRAPP, 2019.\n\nAlberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels.\n\nIn Proc. of\n\nNeurIPS, 2019.\n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\n\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\nTong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative\n\nadversarial networks. In Proc. of ICLR, 2017.\n\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Proc. of NeurIPS, 2016.\n\nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.\n\nIn Proc. of NeurIPS, 2019.\n\nConstantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with\n\noptimism. In Proc. of ICLR, 2018.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In Proc. of CVPR, 2009.\n\nIshan P. Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. In\n\nProc. of ICLR, 2017.\n\nFarzan Farnia and Asuman E. Ozdaglar. Do gans always have nash equilibria? In Proc. of ICML,\n\n2020.\n\nJean-Yves Franceschi, Emmanuel de Bézenac, Ibrahim Ayed, Mickaël Chen, Sylvain Lamprier, and Patrick Gallinari. A neural tangent kernel perspective of gans. CoRR, abs/2106.05566, 2021.\n\nAdrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-\n\nworks as shallow gaussian processes. In Proc. of ICLR, 2019.\n\nAmnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On\n\nthe similarity between the laplace and neural tangent kernels. In Proc. of NeurIPS, 2020.\n\nArnab Ghosh, Viveka Kulharia, Vinay P. Namboodiri, Philip H. S. Torr, and Puneet Kumar Dokania.\n\nMulti-agent diverse generative adversarial networks. In Proc. of CVPR, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nIan Goodfellow.\n\nNips 2016 tutorial: Generative adversarial networks.\n\narXiv preprint\n\narXiv:1701.00160, 2016.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. of NeurIPS, 2014.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In Proc. of ICLR, 2015.\n\nRobert M Gower.\n\nConvergence theorems for gradient descent, May 2022.\n\nhttps:\n\n//gowerrobert.github.io/pdf/M2_statistique_optimisation/grad_ conv.pdf.\n\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander J.\n\nSmola. A kernel two-sample test. J. Mach. Learn. Res., 2012.\n\nIshaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Im-\n\nproved training of wasserstein gans. In Proc. of NeurIPS, 2017.\n\nInsu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin. Random features for the\n\nneural tangent kernel. CoRR, abs/2104.01351, 2021.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proc. of NeurIPS, 2017.\n\nJiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: NNGP and\n\nNTK for deep attention networks. In Proc. of ICML, 2020.\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander\n\nMadry. Adversarial examples are not bugs, they are features. In Proc. of NeurIPS, 2019.\n\nArthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-\n\neralization in neural networks. In Proc. of NeurIPS, 2018.\n\nHarold Jeffreys. An invariant form for the prior probability in estimation problems. Proc. of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):453–461, 1946.\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-\n\ning and improving the image quality of stylegan. In Proc. of CVPR, 2020.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nYann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].\n\nAvailable: http://yann.lecun.com/exdb/mnist, 2, 2010.\n\nChristian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P. Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photorealistic single image super-resolution using a generative adversarial network. In Proc. of CVPR, 2017.\n\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha\n\nSohl-Dickstein. Deep neural networks as gaussian processes. In Proc. of ICLR, 2018.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Proc. of NeurIPS, 2019.\n\nJaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In Proc. of NeurIPS, 2020.\n\nChun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. MMD GAN:\n\ntowards deeper understanding of moment matching network. In Proc. of NeurIPS, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nWei Li, Li Fan, Zhenyu Wang, Chao Ma, and Xiaohui Cui. Tackling mode collapse in multi-\n\ngenerator gans with orthogonal vectors. Pattern Recognition, 2021.\n\nYujia Li, Kevin Swersky, and Richard S. Zemel. Generative moment matching networks. In Proc.\n\nof ICML, 2015.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\n\nIn Proc. of ICCV, December 2015.\n\nMario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans\n\ncreated equal? A large-scale study. In Proc. of NeurIPS, 2018.\n\nQi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking genera-\n\ntive adversarial networks for diverse image synthesis. In Proc. of CVPR, 2019.\n\nXudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and Stephen Paul Smolley.\n\nLeast squares generative adversarial networks. In Proc. of ICCV, 2017.\n\nAlexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin Ghahramani.\n\nGaussian process behaviour in wide deep neural networks. In Proc. of ICLR, 2018.\n\nLars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do\n\nactually converge? In Proc. of ICML, 2018.\n\nLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial\n\nnetworks. In Proc. of ICLR, 2017.\n\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization\n\nfor generative adversarial networks. In Proc. of ICLR, 2018.\n\nAryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In Proc. of AISTATS, 2020.\n\nVaishnavh Nagarajan and J. Zico Kolter. Gradient descent GAN optimization is locally stable. In\n\nProc. of NeurIPS, 2017.\n\nRoman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A Alemi, Jascha Sohl-Dickstein, and Samuel S Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In Proc. of ICLR, 2019a.\n\nRoman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. In Proc. of ICLR, 2019b.\n\nBen Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Proc. of NeurIPS, 2016.\n\nGuo-Jun Qi. Loss-sensitive generative adversarial networks on lipschitz densities. Int. J. Comput.\n\nVis., 2020.\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. In Proc. of ICLR, 2016.\n\nMaithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the\n\nexpressive power of deep neural networks. In Proc. of ICML, 2017.\n\nAlfréd Rényi et al. On measures of entropy and information. In Proc. of the 4th Berkeley symposium\n\non mathematical statistics and probability, volume 1, 1961.\n\nMehdi S. M. Sajjadi, Giambattista Parascandolo, Arash Mehrjou, and Bernhard Schölkopf. Tem-\n\npered adversarial networks. In Proc. of ICML, 2018.\n\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n\nImproved techniques for training gans. In Proc. of NeurIPS, 2016.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nSamuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information\n\npropagation. In Proc. of ICLR, 2017.\n\nVaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Jonathan Ragan-Kelley, Ludwig\n\nSchmidt, and Benjamin Recht. Neural kernels without tangents. In Proc. of ICML, 2020.\n\nAkash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. VEEGAN: reducing mode collapse in gans using implicit variational learning. In Proc. of NeurIPS, 2017.\n\nKiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algo-\n\nrithms for smooth minimax optimization. In Proc. of NeurIPS, 2019.\n\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.\n\nIn Proc. of NeurIPS, 2016.\n\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8798–8807, 2018.\n\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\n\nfrom error visibility to structural similarity. IEEE transactions on image processing, 2004.\n\nLi Xu, Jimmy S. J. Ren, Ce Liu, and Jiaya Jia. Deep convolutional neural network for image\n\ndeconvolution. In Proc. of NeurIPS, 2014.\n\nGreg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. CoRR, abs/1902.04760, 2019a.\n\nGreg Yang. Tensor programs I: wide feedforward or recurrent neural networks of any architecture\n\nare gaussian processes. CoRR, abs/1910.12478, 2019b.\n\nAmir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin. Scaling\n\nneural tangent kernels via sketching and random features. In Proc. of NeurIPS, 2021.\n\n6 STATISTICAL INTERPRETATION OF GA-NTK\n\nStatistically, minimizing Eq. (4) or (6) amounts to minimizing the Pearson χ2-divergence (Jeffreys, 1946), a case of f -divergence (Rényi et al., 1961), between Pdata + Pgen and 2Pgen, where Pdata is the distribution of real data and Pgen is the distribution of generated points. To see this, we first rewrite the loss of our discrimonator D, denoted by L(D), in expectation:\n\narg min\n\nD\n\nL(D) = arg min\n\nD\n\nEx∼Pdata\n\n(cid:2)(D(x) − 1)2(cid:3) + Ex∼Pgen\n\n(cid:2)(D(x) − 0)2(cid:3).\n\n(8)\n\nHere, Pgen can represent either Z in Eq. (4) or the output of the generator G in Eq. (6). Similarly, the loss function for our Pgen, denoted by L(Pgen; D), can be written as follows:\n\narg min Pgen\n\nL(Pgen; D) = arg min Pgen\n\nEx∼Pdata\n\n(cid:2)(D(x) − 1)2(cid:3) + Ex∼Pgen\n\n(cid:2)(D(x) − 1)2(cid:3).\n\n(9)\n\nGA-NTK, in the form of Eqs. (8) and (9), is a special case of LSGAN (Mao et al., 2017). Let D∗ be the minimizer of Eq. (8). We can see that Eqs. (4) and (6) effectively solve the problem:\n\narg min Pgen\n\nL(Pgen; D∗) = arg min Pgen\n\nEx∼Pdata\n\n(cid:2)(D∗(x) − 1)2(cid:3) + Ex∼Pgen\n\n(cid:2)(D∗(x) − 1)2(cid:3).\n\n(10)\n\nMao et al. (2017) show that, under mild relaxation, minimizing Eq. (10) yields minimizing the Pearson χ2-divergence between Pdata + Pgen and 2Pgen:\n\narg min Pgen\n\nL(Pgen; D∗) = arg min Pgen\n\nχ2\n\nPearson(Pdata + Pgen(cid:107)2Pgen)\n\n(cid:90)\n\n= arg min Pgen\n\n(Pdata(x) + Pgen(x))\n\n(cid:18)\n\n2Pgen(x) Pdata(x) + Pgen(x)\n\n(cid:19)2\n\n− 1\n\ndx.\n\nThe loss becomes zero when Pdata(x) = Pgen(x) for all x. Therefore, minimizing Eq. (4) or (6) brings Pgen closer to Pdata.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n7 PROOF OF THEOREM 3.1\n\nIn this section, we prove the convergence of a GA-NTK whose discriminator D approximates an infinite ensemble of infinitely-wide, fully-connected, feedforward neural networks. The proof can be easily extended to other network architectures such as convolutional neural networks.\n\n7.1 BACKGROUND AND NOTATION\n\nConsider a fully-connected, feedforward neural network f : Rd → R,\n\nf (x; θ) =\n\nσw√\n\ndL−1\n\n(cid:18) σw√\n\nwLφ\n\nW L−1φ\n\n(cid:18)\n\n· · · φ\n\n(cid:18) σw√\n\nW 1x + σbb1\n\n(cid:19)\n\n(cid:19)\n\n· · ·\n\n+ σbbL−1\n\n(cid:19)\n\n+ σbbL,\n\ndL−2\n\nd\n\n, bl ∈ Rdl\n\n(11) where φ(·) is the activation function (applied element-wisely), L is the number of hidden layers, {d1, · · · , dL−1} are the dimensions (widths) of hidden layers, θ = ∪L l=1(W l ∈ Rdl×dl−1 ) are trainable weights and biases whose initial values are i.i.d. Gaussian random variables N (0, 1), and σ2 b are scaling factors that control the variances of weights and biases, respectively. Suppose f is trained on a labeled dataset D2n = (X n ⊕ Zn ∈ R2n×d, 1n ⊕ 0n ∈ R2n) by minimizing the MSE loss using t gradient-descent iterations with the learning rate η. Let θ(0) and θ(t) be the initial and trained parameters, respectively. As d1, · · · , dL → ∞, we can approximate the distribution of f (x; θ(t)) as a Gaussian process (NTK-GP) (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019) whose behavior is controlled by a kernel matrix\n\nl=1θl = ∪L\n\nw and σ2\n\nK2n,2n = ∇θf (X n ⊕ Zn; θ(0))(cid:62)∇θf (X n ⊕ Zn; θ(0)) ∈ R2n×2n,\n\n(12)\n\ni,j\n\nwhere f (X n ⊕ Zn; θ(0)) ∈ R2n is the vector of in-sample predictions made by the initial f . The value of each element K 2n,2n = kL((X n ⊕ Zn)i,:, (X n ⊕ Zn)j,:) presents the similarity score of two rows (points) of X n ⊕ Zn in a kernel space, and it can be expressed by a kernel function kL : Rd × Rd → R, called the neural tangent kernel (NTK). The NTK is deterministic as it depends only on φ(·), σw, σb, and L rather than the specific values in θ(0). Furthermore, it can be evaluated layer-wisely. Let hl be the pre-activation of the j-th neuron at the l-th layer of f (x; θ(t)). j(x) is still an NTK-GP, and its associated NTK is defined as kl : Rd×Rd → R, The distribution of hl\n\nj(x) ∈ Rdl\n\nkl(x, x(cid:48)) = ∇θ≤lhl\n\nj(x)(cid:62)∇θ≤l hl\n\nj(x(cid:48)),\n\nwhere θ≤l = ∪l shown that\n\ni=1θi. Note that all hl\n\nj(x)’s, ∀j, are i.i.d. and thus share the same kernel. It can be\n\nkl(x, x(cid:48)) = ∇θl hl\n\nj(x)(cid:62)∇θl hl\n\nj(x(cid:48)) + ∇θ≤l−1 hl\n\nj(x)(cid:62)∇θ≤l−1hl\n\nj(x(cid:48))\n\n= ̃kl(x, x(cid:48)) + σ2\n\nwkl−1(x, x(cid:48))E\n\n(h(l−1)\n\nj\n\n(x), h(l−1)\n\nj\n\n(x(cid:48)))∼N (02, ̃Kl−1)\n\nand\n\nk1(x, x(cid:48)) =\n\nσ2 w\nd\n\nx(cid:62)x(cid:48) + σ2\n\nb\n\n(cid:104)\n\nφ(cid:48)(h(l−1)\n\nj\n\n(cid:105)\n\n(x(cid:48)))\n\n(x))φ(cid:48)(h(l−1)\n\nj (13)\n\n(14)\n\nwhere ̃kl : Rd × Rd → R is the NNGP kernel (Lee et al., 2018; Matthews et al., 2018) that controls the behavior of another Gaussian process, called NNGP, approximating the distribution of f (x; θ(0)), and\n\n ̃Kl−1 =\n\n7.2 CONVERGENCE\n\n(cid:20) ̃kl−1(x, x) ̃kl−1(x, x(cid:48))\n\n(cid:21)\n\n ̃kl−1(x, x(cid:48)) ̃kl−1(x(cid:48), x(cid:48))\n\n∈ R2×2.\n\nThe GA-NTK employs the above NTK-GP as the discriminator D. So, the in-sample mean predictions of D can be written as a closed-form formula:\n\nD(X n, Zn) = (I 2n − e−ηtK2n,2n\n\n)y2n ∈ R2n,\n\n(15)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nwhere I 2n is an identity matrix and y2n = 1n ⊕ 0n ∈ R2n is the “correct” label vector for training D. We formulate the objective of GA-NTK as:\n\narg min Zn\n\nL(Zn) = arg min Zn\n\n1 2\n\n(cid:107)12n − D(X n, Zn)(cid:107)2,\n\n(16)\n\nwhere 12n ∈ R2n in the loss L(·) is the “wrong” label vector that guides us to find the points (Zn) that best deceive the discriminator. We show that\n\nTheorem 7.1 Let s be the number of the gradient descent iterations solving Eq. (16), and let Zn,(s) i,j and Zn,(0) be the solution at the s-th iteration. Suppose the following values are bounded: (a) X n ,\n∀i, j, (b) t and η, and (c) σ and L. Also, assume that (d) X n contains finite, non-identical, normalized rows. Then, for a sufficiently large t, we have\n\ni,j\n\n(cid:107)∇Zn L(Zn,(j))(cid:107)2 ≤ O(\n\nmin j≤s\n\n1 s − 1\n\n).\n\n7.3 PROOF\n\nTo prove Theorem 7.1, we first introduce the notion of β smoothness:\n\nDefinition 7.1 A continuously differentiable function g : Rd → R is β-smooth if there exits β ∈ R such that\n\n(cid:107)∇ag(a) − ∇bg(b)(cid:107) ≤ β(cid:107)a − b(cid:107)\n\nfor any a, b ∈ Rd.\n\nIt can be shown that gradient descent finds a stationary point of a β-smooth function efficiently (Gower, 2022).\n\nLemma 7.1 Let a(s) be the input of a function g : Rd → R after applying s gradient descent iterations to an initial input a(0). If g is β-smooth, then g(a(s)) converges to a stationary point at rate\n\n(cid:107)∇ag(a(j))(cid:107)2 ≤ O(\n\nmin j≤s\n\n1 s − 1\n\n).\n\nSo, our goal is to show that the loss L(Zn) in Eq. (16) is β-smooth w.r.t. any generated point z ∈ Rd.\n\nCorollary 7.1 If all the conditions (a)-(d) in Theorem 7.1 hold, there exits a constant c1 ∈ R+ such that (cid:107)∇zL(Zn)(cid:107) ≤ c1 for each row z ∈ Rd of Zn. This makes L(Zn) β-smooth.\n\nTo prove Corollary 7.1, consider Di(X n, Zn) and ∇zj L(Zn), the i-th and j-th elements of D(X n, Zn) ∈ R2n and ∇zL(Zn) ∈ Rd, respectively. We have\n\n∇zj L(Zn) = ∇zj = (cid:80)2n\n\n1\n\n2 (cid:107)12n − D(X n, Zn)(cid:107)2\n\ni=1 (Di(X n, Zn) − 1) · ∇zj Di(X n, Zn)\n\n(17)\n\nGiven a sufficiently large t, the Di(X n, Zn) can be arbitrarily close to yi ∈ {0, 1} because K2n,2n is positive definite (Jacot et al., 2018) and therefore (I 2n − e−ηtK2n,2n ) → I 2n as t → ∞ in Eq. (15). There exists (cid:15) ∈ R+ such that\n\n|∇zj L(Zn)| ≤ (cid:15) (cid:80)n\n\n≤ (1 + (cid:15)) (cid:80)2n = (1 + (cid:15)) (cid:80)2n = (1 + (cid:15))ηt (cid:80)2n\n\ni=1 |∇zj Di(X n, Zn) + (1 + (cid:15)) (cid:80)2n i=1 |∇zj Di(X n, Zn)| (cid:80)2n p=1(I 2n i=1 |∇zj i,p,q=1 e−ηtK2n,2n\n\ni,p − e−ηtK2n,2n\n\ni,p\n\ni,q\n\n)y2n p |\n\ni=n+1 |∇zj Di(X n, Zn)|\n\n|∇zj kL((X n ⊕ Zn)q,:, (X n ⊕ Zn)p,:)y2n\n\np |.\n\nNote that e−ηtK2n,2n 7.1 holds as long as ∇zj kL((X n ⊕ Zn)q,:, (X n ⊕ Zn)p,:) is bounded.\n\n∈ R+ can be arbitrarily close to 0 with a sufficiently large t. Hence, Corollary\n\ni,q\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nCorollary 7.2 If the conditions (a)-(d) in Theorem 7.1 hold, there exits a constant c2 ∈ R+ such that ∇zj kL(a, b) ≤ c2 for any two rows a and b of X n ⊕ Zn.\n\nIt is clear that ∇zj kL(a, b) = 0 if a, b (cid:54)= z. So, without loss of generality, we consider ∇zj kL(a, z) only. From Eq. (13), we have\n\n∂kL(a, z) ∂zj\n\n=\n\n∂kL(a, z) ∂kL−1(a, z)\n\n∂kL−1(a, z) ∂kL−2(a, z)\n\n· · ·\n\n∂k1(a, z) ∂zj\n\n.\n\nFor each l = 2, · · · , L, we can bound ∂kl(a, z)/∂kl−1(a, z) by\n\n∂kl(a,z)\n\n∂kl−1(a,z) = σ2\n\nE\n\nw\n\n(h(l−1) ≤ (σw maxh φ(cid:48)(h))2\n\n(x), h(l−1)\n\nj\n\nj\n\n(x(cid:48)))∼N (02, ̃Kl−1)\n\n(cid:104)\n\nφ(cid:48)(h(l−1)\n\nj\n\n(x))φ(cid:48)(h(l−1)\n\nj\n\n(x(cid:48)))\n\n(cid:105)\n\nprovided that the maximum slope of φ is limited, which is true for many popular activation functions including ReLU and erf. Also, by Eq. (14), the value\n\n∂k1(a, z) ∂zj\n\n=\n\nσ2 w\nd\n\naj\n\nis bounded. Therefore, Corollary 7.2 holds, which in turn makes L(Zn) β-smooth via Corollary 7.1. By Lemma 7.1, we obtain the proof of Theorem 7.1.\n\n8 EXPERIMENT SETTINGS\n\nThis section provides more details about the settings of our experiments.\n\n8.1 MODEL SETTINGS\n\nThe network architectures of the baseline GANs used in our experiments are based on InfoGAN (Chen et al., 2016). We set the latent dimensions, training iterations, and batch size according to the study (Lucic et al., 2018). The latent dimensions for the generator are all 64. The batch size for all baselines is set to 64. The training iterations are 80K, 100K, and 400K for MNIST, CelebA, and CIFAR-10 datasets, respectively. For the optimizers, we follow the setting from the respective original papers. Below we list the network architecture of the baselines for each dataset as well as the optimizer settings.\n\nTable 3: The architectures of the discriminator and generator in the baseline GANs for the MNIST dataset.\n\nDiscriminator\n\nInput 28×28×1 Gray image\n\nGenerator\n\nInput∈ R64 ∼ N (0, I)\n\n4×4 conv; 64 leaky ReLU; stride 2\n\nFully Connected 1024 ReLU; batchnorm\n\n4×4 conv; 128 leaky ReLU; stride 2. batchnorm Fully Connected 7 × 7 × 128 ReLU; batchnorm\n\nFully Connected 1024 leaky ReLU; batchnorm\n\n4×4 deconv; 64 ReLU. stride 2; batchnorm\n\nFully Connected 1 output\n\n4×4 deconv; 1 sigmoid\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: The architectures of the discriminator and generator in the baseline GANs for the CIFAR-10 dataset.\n\ndiscriminator\n\nInput 32×32×3 Image\n\ngenerator\n\nInput∈ R64 ∼ N (0, I)\n\n4×4 conv; 64 leaky ReLU; stride 2\n\nFully Connected 2 × 2 × 448 ReLU; batchnorm\n\n4×4 conv; 128 leaky ReLU; stride 2; batchnorm 4×4 deconv; 256 ReLU; stride 2; batchnorm\n\n4×4 conv; 256 leaky ReLU; stride 2; batchnorm 4×4 deconv; 128 ReLU; stride 2\n\nFully Connected 1 output\n\n4×4 deconv; 64 ReLU; stride 2\n\n4×4 deconv; 3 Tanh; stride 2.\n\nTable 5: The architectures of the discriminator and generator in the baseline GANs for the CelebA dataset.\n\ndiscriminator\n\nInput 64×64×3 Image\n\ngenerator\n\nInput∈ R64 ∼ N (0, I)\n\n4×4 conv; 64 leaky ReLU; stride 2\n\nFully Connected 2 × 2 × 448 ReLU; batchnorm\n\n4×4 conv; 128 leaky ReLU; stride 2; batchnorm 4×4 deconv; 256 ReLU; stride 2; batchnorm\n\n4×4 conv; 256 leaky ReLU; stride 2; batchnorm 4×4 deconv; 128 ReLU; stride 2\n\n4×4 conv; 256 leaky ReLU; stride 2; batchnorm 4×4 deconv; 64 ReLU; stride 2\n\nFully Connected 1 output\n\n4×4 deconv; 32 ReLU; stride 2\n\n4×4 deconv; 3 Tanh; stride 2.\n\nTable 6: The optimizer settings for each GAN baseline. ndis denotes the training steps for discriminators in the alternative training process.\n\nDCGAN\n\nLSGAN\n\nWGAN\n\nOptimizer type Learning Rate\n\nAdam\n\nAdam\n\n0.0002\n\n0.0002\n\nβ1\n\n0.5\n\n0.5\n\nβ2\n\nndis\n\n0.999\n\n0.999\n\nRMSProp\n\n0.00005\n\nNone None\n\nWGAN-GP\n\nSN-GAN\n\nAdam\n\nAdam\n\n0.0001\n\n0.0001\n\n0.5\n\n0.9\n\n0.9\n\n0.999\n\n1\n\n1\n\n5\n\n5\n\n5\n\nNote that we remove all the batchnorm layers for the discriminators in WGAN-GP. We architect the element network of the discriminator in our GA-NTK following InfoGAN (Chen et al., 2016), except that the width (or the number of filters) of the network is infinite at each layer and has no batchnorm layers.\n\nThe generator of GA-NCTKg consumes memory. To reduce memory consumption, we let D discriminates true and fake images in the code space of a pre-trained autoencoder A (Bergmann et al., 2019). After training, a code output by G is fed into the decoder of A to obtain an image. The architectures of the pre-trained A for different datasets are summarized as follows:\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: The architectures of A for different datasets.\n\nMNIST\n\nCIFAR-10\n\nInput 28×28×1 Image\n\n3×3 conv; 16 SeLU; stride 2\n\n3×3 conv; 32 SeLU; stride 2\n\n3×3 conv; 64 SeLU; stride 2\n\nFully Connected; 128 tanh\n\nInput 32×32×3 Image\n\n3×3 conv; 32 SeLU; stride 2\n\n3×3 conv; 64 SeLU; stride 2\n\n3×3 conv; 128 SeLU; stride 2\n\nFully Connected; 1024 tanh\n\n3×3 transposeconv; 64 SeLU; stride 2\n\n3×3 transposeconv; 128 SeLU; stride 2\n\n3×3 transposeconv; 32 SeLU; stride 2\n\n3×3 transposeconv; 64 SeLU; stride 2\n\n3×3 transposeconv; 16 SeLU; stride 2\n\n3×3 transposeconv; 32 SeLU; stride 2\n\noutput\n\nCelebA\n\noutput\n\nCelebA-HQ\n\nInput 64×64×3 Image\n\nInput 256×256×3 Image\n\n3×(3×3 conv; 32 SeLU; stride 1)\n\n3×(3×3 conv; 64 SeLU; stride 1)\n\n3×3 conv; 32 SeLU; stride 2\n\n3×3 conv; 64 SeLU; stride 2\n\n3×(3×3 conv; 64 SeLU; stride 1)\n\n3×(3×3 conv; 128 SeLU; stride 1)\n\n3×3 conv; 64 SeLU; stride 2\n\n3×3 conv; 128 SeLU; stride 2\n\n3×(3×3 conv; 128 SeLU; stride 1)\n\n3×(3×3 conv; 256 SeLU; stride 1)\n\n3×3 conv; 128 SeLU; stride 2\n\n3×3 conv; 256 SeLU; stride 2\n\nFully Connected; 2048 tanh\n\n3×(3×3 conv; 512 SeLU; stride 1)\n\n3×(3×3 transposeconv; 128 SeLU; stride 1)\n\n3×3 conv; 512 SeLU; stride 2\n\n3×3 transposeconv; 128 SeLU; stride 2\n\nFully Connected; 2048 tanh\n\n3×(3×3 transposeconv; 64 SeLU; stride 1)\n\n3×(3×3 transposeconv; 512 SeLU; stride 1)\n\n3×3 transposeconv; 64 SeLU; stride 2\n\n3×3 transposeconv; 512 SeLU; stride 2\n\n3×(3×3 transposeconv; 32 SeLU; stride 1)\n\n3×(3×3 transposeconv; 256 SeLU; stride 1)\n\n3×3 transposeconv; 32 SeLU; stride 2\n\n3×3 transposeconv; 256 SeLU; stride 2\n\n3×(3×3 transposeconv; 128 SeLU; stride 1)\n\n3×3 transposeconv; 128 SeLU; stride 2\n\n3×3(3×3 transposeconv; 64 SeLU; stride 1)\n\n3×3 transposeconv; 64 SeLU; stride 2\n\noutput\n\noutput\n\n8.2 METRICS\n\nThe FID scores are computed using the code from the original paper (Heusel et al., 2017). We sample 2048 images to compute the FID scores. We calculate the AM-SSIM scores using the SSIM settings: filter size 4, filter sigma 1.5, k1 0.01, and k2 0.03 (Wang et al., 2004).\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Unidirectional search for the hyperparameter λ of GA-NTK. Input: Data X n, kernel k, and separation tolerance (cid:15) Output: λ for GA-NTK Randomly initiate Zn ∈ Rn×d λ ← 1 while 1\n\n2n (cid:107)D(X n, Zn; k, λ) − (1n ⊕ 0n)(cid:107)2 ≤ (cid:15) do\n\nλ ← λ · 2\n\nend return λ\n\n8.3 HYPERPARAMETER TUNING\n\nFor each data synthesis method, we tune its hyperparameter using grid search. GA-NTK. The computation of K2n,2n requires one to determine the initialization and architecture of the element networks in the ensemble discriminator. Poole et al. (2016); Schoenholz et al. (2017); Raghu et al. (2017) have proposed a principled method to tune the hyperparameters for the initialization. From our empirical results, we also find that the quality of the images generated by GA-NTK is not significantly impacted by the choice of the architecture—a fully connected network with rectified linear unit (ReLU) activation suffices to generate recognizable image patterns. Once K2n,2n is decided, there is only one hyperparameter λ = ηt to tune in Eq. (16). The λ controls how well the discriminator is trained on D, so either a too small or large value can lead to poor gradients for Zn and final generated points. But since there is no alternating updates as in GANs, we can decide an appropriate value of λ without worrying about canceling the learning progress of Zn. We propose a simple, unidirectional search algorithm for tuning λ, as shown in Algorithm 1. Basically, we search, from small to large, for a value that makes the discriminator nearly separate the real data from pure noises in an auxiliary learning task, and then use this value to solve Eq. (16). In practice, a small positive (cid:15) ranging from 10−3 to 10−2 suffices to give an appropriate λ. Multi-resolutional GA-NTK. We use 3 NTK-GP’s as the discriminators, whose architectures are listed in Table 8.\n\nTable 8: The architectures of the discriminators for multi-resolution GA-NTK.\n\nDiscriminator large\n\nDiscriminator medium\n\nInput 256×256×3 Image\n\nDiscriminator small\n\nInput 64×64×3 Image\n\n4×4 conv; ReLU; stride 2\n\nInput 16×16×3 Image\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\nFully Connected 1 output\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\nFully Connected 1 output\n\n4×4 conv; ReLU; stride 2\n\nFully Connected 1 output\n\n9 MORE EXPERIMENTS\n\n9.1 GA-FNTK VS. GA-CNTK\n\nNext, we compare the images generated by GA-FNTK, GA-CNTK, and the multi-resolutional GACNTK described in Section 3.2 on the CelebA and CelebA-HQ datasets. The multi-resolutional GA-CNTK employs 3 discriminators working at 256 × 256, 64×64, and 16×16 pixel resolutions, respectively. Figure 6 shows the results. To our surprise, GA-NTK (which models the discriminator as an ensemble of fully connected networks) suffices to generate recognizable faces. The images synthesized by GA-FNTK and GA-CNTK lack details and global coherence, respectively, due to\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GA-FNTK on CelebA\n\n(b) GA-CNTK on CelebA\n\nFigure 6: The images generated by (a) GA-FNTK and (b) GA-CNTK given 256 CelebA training images.\n\nFigure 7: When b = 1, GA-NTK tends to generate a blurry mean image.\n\nthe characteristics of FNNs and CNNs. On the other hand, the multi-resolutional GA-CNTK gives both the details and global coherence thanks to the multiple discriminators working at different pixel resolutions. The results also demonstrate the potential of GA-NTK variants to generate high-quality data as there are many other techniques for GANs that could be adapted into GA-NTK.\n\n9.2 BATCH-WISE GA-NTK\n\nTo work with a larger training set, we modify GA-CNTK by following the instructions in Section 3.2 to obtain the batch-wise GA-CNTK, which computes the gradients of Zn in Eq. (4) from 256 randomly sampled training images during each gradient descent iteration. We train the batch-wise GA-CNTK on two larger datasets consisting of 2048 images from CelebA and 1300 images from ImageNet, respectively. Figure 8 shows the results, and the batch-wise GA-CNTK can successfully generate the “daisy” images on ImageNet.\n\nNote that the batch-wise GA-CNTK solves a different problem than the original GA-CNTK—the former finds Zn that deceives multiple discriminators, each trained on 256 examples, while the latter searches for Zn that fools a single discriminator trained on 256 examples. We found that, when the batch size is small (b = 1), GA-NTK tends to generate a blurry mean image regardless of model architectures and initializations of model weights and Zn, as shown in Figure 7. This is because the mean image is the best for simultaneously fooling many NTK discriminators, each trained on a single example. However, in practice this setting is less common as one usually aims to use the largest b possible (Brock et al., 2018). Figure 8 shows that a batch size of 256 suffices to give plausible results on the CelebA and ImageNet datasets. Comparing the images in Figure 1(f) with those in Figure 8(a), we can see that the batch-wise GA-CNTK gives a little more blurry images but the patterns in each synthesized image are more globally coherent, both due to the effect of multiple discriminators.\n\n9.3 SENSITIVITY TO HYPERPARAMETERS\n\nHere, we study how sensitive is the performance of WGAN, WGAN-GP, and GA-FNTK to their hyperparameters. We adjust the hyperparameters of different approaches using the grid search under a time budget of 3 hours, and then evaluate the quality of 2048 generated data points by the Wasser-\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 8: The images generated by batch-wise GA-CNTK on (a) CelebA dataset of 2048 randomly sampled images and (b) ImageNet dataset of 1300 randomly sampled images.\n\n(a)\n\n(b)\n\nFigure 9: The distribution of Wasserstein distance between Pgen and Pdata (used to measure the quality of the generated points) over the searched hyper-parameters on training sets of (a) 8- and (b) 25-modal Gaussian mixtures.\n\nstein distance between Pgen and Pdata. We train different methods on two toy datasets consisting of 8- and 25-modal Gaussian mixtures following the settings described in Section 4.2. Figure 9 shows the results, and we can see that GA-FNTK achieves the lowest average Wasserstein distance in both cases. Moreover, its variances are smaller than the two other baselines, too. This shows that the performance of GA-FNTK is less sensitive to the hyperparameters and could be easier to tune in practice.\n\nNote that, with 3-hour time budget, the hyperparameters we obtained through the grid search are good enough for reproducing the experiments conducted by Mao et al. (2017) on mode collapse. In the experiments, the Pgen of different methods aim to align a 2D 8-modal Gaussian mixtures in the ground truth. Our results are shown in Figure 10.\n\n21\n\nGA-NTKWGANWGAN-GP103102101Wasserstein distanceGA-NTKWGANWGAN-GP102101Wasserstein distanceUnder review as a conference paper at ICLR 2023\n\n(a) MNIST\n\n(b) CIFAR-10\n\n(c) CelebA\n\nFigure 11: The learning curve of G in GA-CNTKg and the generated images G(z) at different stages of training given the same input z.\n\n(a) Ground truth\n\n(b) WGAN\n\n(c) WGAN-GP\n\n(d) GA-FNTK\n\nFigure 10: Visualization of distribution alignment and mode collapse on a 2D 8-modal Gaussian mixtures dataset.\n\n9.4 EVOLUTION OF IMAGES DURING TRAINING\n\nFigure 11 shows the learning curve of the generator in G in GA-CNTKg and the relationship between the quality of images output by G and the number of gradient descent iterations. The results show that the loss can be minimized even if it is an f -divergence, and a lower loss score implies higher image quality. This is consistent with the results of GA-CNTK (without a generator) shown in Figure 3.\n\nSource of creativity. The diversity of our generated data not only comes from the randomness of an optimization algorithm (e.g., initialization of Z or splitting of X into batches, as discussed in Section 3.2) but also from the objective in Eq. (4) itself. To see this, observe in Figure 3 that the images generated at the later stage of training contain recognizable patterns that change constantly over training time, despite little change in the loss score. The reason is that, in Eq. (4), the Zn is optimized for a moving target—any change of Zn causes D to be “retrained” instantly. The training of the generator G in Eq. (6) also shares this nice property. In Figure 11, the patterns of a generated image G(z) change over training time even when the input z is fixed. However, getting diverse artificial data through this property requires prolonged training time. In practice, we can simply initialize Z differently to achieve diversity faster.\n\n10 MORE IMAGES GENERATED BY GA-CNTK AND GA-CNTKG\n\nFigures 12–16 show more sample images synthesized by GA-CNTK and GA-CNTKg. All these images are obtained using the settings described in the main paper and the above.\n\nWe can see that the quality of the images synthesized by GA-CNTKg is worse than that of the images synthesized by GA-CNTK, as discussed in Section 4.1. Furthermore, recall from Table 1 that, without a generator network, the GA-NTK performs better when the date size increases. However, this is not the case for GA-NTKg having a generator network. We have resampled training data and rerun the experiments 5 times with different initial values of Zn but obtained similar results. Therefore, we believe the instability is due to the sample complexity of the generator network—256 examples or less are insufficient to train a stable, high-quality generator. This is evident in Figures 12(b)-15(b) where the generator outputs unrecognizable images more often.\n\n22\n\n0101010101010101Under review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 12: Sample images generated by GA-CNTK (a) without and (b) with generator on the MNIST dataset of 256 randomly sampled images.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 13: Sample images generated by GA-CNTK (a)without generator(b)with generator on the CIFAR-10 dataset of 256 randomly sampled images.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 14: Sample images generated by GA-CNTK (a) without and (b) with generator on the CelebA dataset of 256 randomly sampled images.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 15: Sample images generated by multi-resolutional GA-CNTK on the CelebA-HQ dataset of 256 randomly sampled images.\n\nFigure 16: Sample images generated by GA-CNTKg on the CelebA-HQ dataset of 256 randomly sampled images.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n11 DOWNGRADE IMAGES\n\nAs discussed in the main paper, we find that, when the size of training set is small, an image synthesis method may produce downgrade images that look almost identical to some images in the training set. This problem is less studied in the literature but important to applications with limited training data. We investigate this problem by showing the images from the training set that are the nearest to a generated image. We use the SSIM (Wang et al., 2004) as the distance measure. Figures 17, 18, and 19 show the results for some randomly sampled synthesized images. As compared to GANs, both GA-CNTK and batch-wise GA-CNTK can generate images that look less similar to the ground-truth images.\n\nFigure 17: Comparison between the images generated by WGAN-GP trained on 256 images and the nearest neighbors (measured by SSIM) from the training set. Images with red bounding boxes are generated images.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 18: Comparison between the images generated by GA-CNTK trained on 256 images and the nearest neighbors (measured by SSIM) from the training set. Images with red bounding boxes are generated images.\n\nFigure 19: Comparison between the images generated by GA-CNTKg trained on 256 images and the nearest neighbors (measured by SSIM) from the training set. Images with red bounding boxes are generated images.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: The convergence speed and training time of different methods on a machine with a single NVIDIA Tesla V100 GPU given different datasets of 256 randomly sampled images. The GACNTK and GA-CNTKg are batch-wise, and the batch size b is set to 64 for all methods.\n\nMetric\n\nDCGAN\n\nLSGAN\n\nWGAN WGANGP\n\nSNGAN\n\nGACNTK GACNTKg\n\nT Iterations\n\nS\n\n7400\n\n5100\n\nI\n\nN M\n\nIter. / sec.\n\nSeconds\n\n1 -\n\n0 Iterations R\nA F\n\nIter. / sec.\n\nI\n\nC\n\nSeconds\n\nA Iterations\n\nb e\nl e\nC\n\nIter. / sec.\n\nSeconds\n\n20\n\n370\n\nN/A\n\n17\n\nN/A\n\nN/A\n\n13\n\nN/A\n\n19\n\n268\n\nN/A\n\n17\n\nN/A\n\nN/A\n\n12\n\nN/A\n\n7000\n\n19\n\n368\n\n3400\n\n18\n\n189\n\n14000\n\n11100\n\n16\n\n875\n\n15\n\n740\n\n18800\n\n11200\n\n12\n\n1566\n\n10\n\n1120\n\n12800\n\n18\n\n711\n\nN/A\n\n14\n\nN/A\n\nN/A\n\n9\n\nN/A\n\n500\n\n14\n\n35\n\n600\n\n13\n\n46\n\n1200\n\n6\n\n20\n\n1600\n\n9\n\n177\n\n6200\n\n8\n\n775\n\n5900\n\n5\n\n1180\n\n12 SEMANTICS LEARNED BY GA-CNTKG\n\nHere, we investigate whether the features learned by GA-NTK can encode high-level semantics. We plot “interpolated” images output by the generator G of GA-CNTKg taking equidistantly spaced z’s along a segment in z space as the input. For ease of presentation, we consider a 2-dimensional z space and train G on MNIST and CelebA datasets of 256 examples. Figure 20 shows the results, where the generated patterns transit smoothly across the 2D z space, and neighboring images share similar looks. These similar-looking images are generated from adjacent but meaningless z’s, suggesting that the learned features encode high-level semantics.\n\n13 CONVERGENCE SPEED AND TRAINING TIME\n\nIn this section, we study the time usage for training GA-NTK variants and compare it with the training of GANs. We conduct experiments to investigate the number of iterations and the wall-clock time required to train different methods on different datasets of 256 randomly sampled images. We use the batch-wise GA-CNTK and GA-CNTKg and set the batch size b to 64 for all methods. We run the experiments on a machine with a single NVIDIA Tesla V100 GPU. For DCGAN and LSGAN whose loss scores do not reflect image quality, we monitor the training process manually and stop it as long as the generated images contain recognizable patterns. But these methods do not seem to converge. For other methods, we use the early-stopping with the patience of 10000 steps and delta of 0.05 to determine convergence. The results are shown in Table 9. As we can see, the number of iterations required by either batch-wise GA-CNTK or GA-CNTKg is significantly smaller than that used by GANs. This justifies our claims in Section 1. However, the batch-wise GA-CNTK and GA-CNTKg run fewer iterations per second than GANs because of the higher computation cost involved in back-propagating through Kb,b. In terms of wall-clock time, the batch-wise GA-CNTK is the fastest while the GA-CNTKg runs as fast as WGAN-GP. We expect that, with the continuous optimization of the Neural Tangents library (Novak et al., 2019a) which our code is based on, the training speed of GA-NTK variants can be further improved.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 20: Interpolated images generated by GA-CNTKg, which is trained on (a) MNIST and (b) CelebA datasets of 256 randomly sampled examples from all classes. The G takes 2-dimensional z’s as input. For each dataset, we feed equidistantly spaced z’s along a segment in z space to G to get the interpolated images.\n\n30",
  "translations": [
    "# Summary Of The Paper\n\nIn this paper, the authors propose generative adversarial neural tangent kernel (GA-NTK) and its variants.\nThey model the discriminator as a Gaussian process whose mean (and covariance) is governed by NTK.\nAlso, they can be performed via a single-level training process as a whole, because the training dynamics of their discriminators can be evaluated in closed form.\nThe authors claim that this property allows them to avoid the training difficulties often encountered with GANs, and experimentally confirmed this claim and their effectiveness for data synthesis.\n\n# Strength And Weaknesses\n\nI place special emphasis on comments with the mark *.\n\n---\n\nStrength:\n\n[1*] As the authors also wrote at the end of Section 1, the main significance of this study is not performance improvement but novelty of the research direction.\nThis study is an interesting attempt that uses an interesting idea to solve the problems of GANs, which are the motivation for the study.\n\n---\n\nWeakness:\n\n[2] There are writing misses.\n\n[2-1] You should use \\citep as well as \\citet.\nFor example, you can write \"theoretical interpretations Arjovsky & Bottou (2017).\" in p.3 as \"theoretical interpretations (Arjovsky & Bottou, 2017).\" by using \\citep.\nAlso, \"Franceschi et al. Franceschi et al. (2021)\" in p.3 is strange for me; you can write \"Franceschi et al. (2021)\" by using \\citet only.\nPlease look up the use of \\citep and \\citet and rewrite the relevant parts.\n\n[2-2] In l.12 in p.4, value each element -> value of each element.\nCheck your writing (including other parts) again.\n\n[2-3] In eq.(4), $\\arg\\min\\_{Z^n} L(Z^n)=\\arg\\min\\_{Z^n} ||\\cdot||_2$ -> $\\arg\\min\\_{Z^n} L(Z^n)$, where $L(Z^n)=||\\cdot||^2$.\nYou should define $L(Z^n)$ explicitly, and unify $||\\cdot||\\_2$ and $||\\cdot||$ into one if they are the seme (see also norm in other parts).\n\n[2-4] The $f$-divergence appears in places, but what function was used for $f$?\n\n[2-5] In Figure 3, title is partially obscured.\n\n[2-6] In Figure 5, what is \"real\" in the legend?\n\n[2-7] In the last equation in p.14, absolute value symbol is missing:\nfor example, $\\nabla_{z_j} L(Z^n)\\le$ -> $|\\nabla_{z_j} L(Z^n)|\\le$.\n\n[2-8] You should centralize Figure 14.\n\n[2-9] In Section 10, ?? -> 18.\n\n---\n\nQuestion:\n\nRegarding the following points, it may be that the authors' description is appropriate and there is no problem, just because I have not understood it correctly.\nI do not reflect these points in my current recommendation score.\nDepending on the authors' response, I may change my recommendation score.\n\n[3] Is it necessary to solve an optimization problem like eq.(4) every time to generate pseudo examples?\nIf so, that is a demerit compared with GANs, which can generate an infinite number of pseudo examples without additional training from a trained model.\nThis disadvantage should be written more clearly.\n(Even so, I will not lower the score.)\n\n[4*] From the experimental results (see especially Figure 3) and my understanding of the formulas, it seems to me that GA-NTK that is sufficiently trained with a large $t$ would simply return an training example (or mixture of training examples) as a pseudo example.\nIs this question correct?\nIf correct, such data synthesis would have low practical value.\nIt does not seem to me that the use of a generator model adequately solves such a problem.\nAlso, the authors should explain when (i.e., at what $t$) to stop learning.\n\n[5] This question relates to [4*].\nI am interested in the relationship between the batch size $b$ and the novelty of the pseudo example (dissimilarity to the training examples $X^{b/2}$).\nI suspect that if $b$ is small, each generated pseudo example will be fairly close to one of the training examples $X^{b/2}$.\nIs this question correct?\nIf correct, I think that the authors should mention this issue.\n\n[6*] It is difficult for me to understand the whole algorithm.\nI could not understand the relation between the index $t$ in $\\lambda=\\eta \\cdot t$ in eq.(3), the index $j$ in Theorem 3.1, \"Epochs\" in Figure 3, and \"Iterations\" in Figure 5, and the evolution of $Z^n$ and $Z^{n,(j)}$.\nPlease write a pseudo code (or modify the text) so that the relation of these objects become clear.\n\nAlso, how does $t$ change when $Z^n$ changes?\nI think this question is relevant to the validity of applying NTK theory.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity:\n\nI do not understand the points [3]--[6*].\nOther parts have a good clarity.\n\n---\n\nQuality:\n\nI think that the quality of the paper is not low, but I do not understand the points [3]--[6*].\nSo I will reserve a definite evaluation until the response.\n\n---\n\nNovelty:\n\nAs I wrote in [1*], I give a good evaluation regarding the novelty.\n\n---\n\nReproducibility:\n\nI took a glance of GitHub repository.\nI think that experimental part of this study is reproducible.\nHowever, I want you to cope with [6*], to improve the understandability (a basis of the reproducibility).\n\n# Summary Of The Review\n\nI give this study a positive score now, on the basis of the novelty and interest of the idea [1*].\nHowever, the description of the proposed method is insufficient and not clear [3]--[6*].\nThese points could be improved!\nI will change the score up or down, depending on the responses to [3]--[6*].\n\n---\n\nI changed the score from 6 to 8.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Details Of Ethics Concerns\n\nNA",
    "# Summary Of The Paper\nThe paper presents a novel generative model, Generative Adversarial NTK (GA-NTK), which addresses several limitations associated with traditional Generative Adversarial Networks (GANs). By modeling the discriminator as a Gaussian process utilizing Neural Tangent Kernels (NTK), GA-NTK simplifies the training process to a single-level optimization, thereby alleviating issues such as convergence, mode collapse, and vanishing gradients. The authors provide a theoretical convergence analysis, practical techniques for implementation, and extensive empirical evaluations across various datasets, demonstrating that GA-NTK achieves competitive image quality with lower sample complexity compared to standard GANs.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to modeling the discriminator, which significantly enhances training stability and efficiency. The proposed single-level optimization framework not only reduces complexity but also leads to improved convergence properties, as evidenced by the experimental results. However, the paper could benefit from a more detailed discussion on the limitations of GA-NTK, particularly regarding scenarios where the Gaussian process may not perform optimally. Additionally, while the empirical evaluations are promising, further exploration of the model's performance on more diverse datasets would strengthen its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly structured, making it accessible to readers familiar with GANs and Gaussian processes. The methodology is presented in a systematic manner, with sufficient detail for reproducibility, particularly in the experimental section where the implementation specifics are outlined. The novelty of the approach is evident, especially in its integration of NTK into the adversarial training framework, which is a fresh perspective in the generative modeling landscape.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling and innovative model that effectively addresses key challenges in GAN training. The GA-NTK framework shows promise for improving convergence and image quality, making it a significant contribution to the field of generative modeling. However, further exploration of its limitations and broader applicability would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Generative Adversarial NTK (GA-NTK), a novel generative model designed to simplify the training of generative adversarial networks (GANs) by replacing the typical bilevel optimization with a single-level objective. The methodology employs a Gaussian process model for the discriminator, governed by a neural tangent kernel (NTK), which provides a clear description of the training dynamics and mitigates common GAN issues such as convergence failures, mode collapse, and gradient vanishing. Experimental results demonstrate that GA-NTK outperforms several traditional GAN variants across multiple datasets, indicating improvements in both image quality and training stability.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach, particularly the introduction of a single-level optimization framework that simplifies the training process and provides convergence guarantees under mild conditions. This is particularly beneficial in applications with limited data, as GA-NTK shows lower sample complexity. Additionally, the flexibility of GA-NTK allows for various extensions, such as multi-resolutional training. However, the paper also presents notable weaknesses, including potential memory consumption issues due to kernel matrix computations, sensitivity to initial hyperparameters, and variability in performance based on specific configurations. Furthermore, while GA-NTK offers a novel methodology, it does not claim to surpass existing state-of-the-art GAN performance benchmarks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology, experimental design, and results. The quality of the writing is high, with a logical flow that facilitates reader understanding. The novelty of the GA-NTK framework is significant, as it introduces a fresh perspective on GAN training challenges. Reproducibility is supported by the use of the Neural Tangents library for implementation, although the sensitivity to hyperparameters may pose challenges for replication in practice.\n\n# Summary Of The Review\nOverall, the paper presents a compelling alternative to traditional GANs through the introduction of the GA-NTK framework, which effectively addresses several challenges associated with GAN training. While it demonstrates promising results, particularly in terms of training stability and scalability, some limitations regarding memory consumption and hyperparameter sensitivity remain. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Generative Adversarial NTK (GA-NTK), which addresses key challenges faced by traditional Generative Adversarial Networks (GANs) such as convergence issues, mode collapse, and vanishing gradients by utilizing a single-level optimization approach. The methodology involves modeling the discriminator as a Gaussian process, which allows for training via ordinary gradient descent, enhancing the stability and efficiency of the learning process. Experimental results demonstrate that GA-NTK variants outperform conventional GANs on various datasets, exhibiting lower sample complexity, improved image quality, and avoidance of mode collapse, particularly in scenarios with limited data.\n\n# Strength And Weaknesses\nThe strength of the paper lies in its innovative approach to simplifying GAN training through the GA-NTK framework, which eliminates the bilevel optimization commonly associated with traditional GANs. The convergence analysis and practical techniques for memory efficiency and image quality improvement are commendable contributions. However, the paper could benefit from a more detailed discussion on the limitations of the proposed method, particularly concerning its applicability to high-dimensional data and scenarios with more complex distributions. Additionally, a broader comparison with state-of-the-art methods beyond standard GANs would strengthen the empirical validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that guides the reader through the theoretical foundations, methodology, and experimental results. The use of equations and theorems is appropriate and enhances the clarity of the proposed concepts. The technical novelty of the GA-NTK approach is significant, as it bridges kernel methods and adversarial training. Reproducibility is supported by detailed descriptions of the experimental methodology, including datasets, implementation specifics, and performance metrics, although access to code and data would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to generative data synthesis through the GA-NTK method, successfully addressing several limitations of traditional GANs. The empirical results are promising, showcasing significant advancements in image quality and training stability. However, further exploration of its limitations and a broader comparative analysis would provide a more comprehensive understanding of its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the GA-NTK model, a novel approach to generative adversarial networks (GANs) that simplifies training through single-level optimization and incorporates kernel methods into adversarial training. The authors provide theoretical guarantees for convergence under mild conditions and demonstrate that GA-NTK significantly reduces sample complexity compared to traditional GANs, making it more suitable for applications with limited data. Extensive experiments are conducted to validate the model’s effectiveness across various datasets, highlighting its potential for improving training stability and memory efficiency.\n\n# Strength And Weaknesses\nThe GA-NTK model has several strengths, including the simplification of the optimization process, which avoids the complications associated with bilevel optimization typical in traditional GANs. This is complemented by demonstrated convergence properties that ensure stable training under certain conditions. Additionally, the model exhibits reduced sample complexity, making it applicable in scenarios with limited data, and effectively mitigates the vanishing gradient problem. The authors provide practical techniques that enhance memory efficiency and image quality, alongside a rigorous experimental assessment across common datasets.\n\nHowever, there are notable weaknesses. The simplification achieved may limit the model's ability to exploit the full adversarial training dynamics, potentially affecting the diversity of outputs and leading to mode collapse. The convergence conditions may not address all practical scenarios, limiting generalizability. While the model avoids vanishing gradients, it may still struggle with high-quality image synthesis compared to more sophisticated GAN architectures. Furthermore, the practical techniques proposed may be constrained by hardware limitations and require extensive tuning. Lastly, the experimental focus on common datasets may not reflect performance in more complex data environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and theoretical contributions of the GA-NTK model. The quality of writing is high, and the integration of theoretical results with empirical validation enhances the reproducibility of the work. The novelty of combining kernel methods with adversarial training is notable; however, the implications of this integration could benefit from further empirical exploration to substantiate its effectiveness over existing GAN approaches.\n\n# Summary Of The Review\nOverall, the GA-NTK model presents a promising advancement in the field of generative modeling by simplifying the training of GANs and demonstrating improved sample efficiency. While the theoretical and empirical contributions are significant, the model's limitations in terms of output diversity and generalizability warrant further investigation. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel generative model called Generative Adversarial NTK (GA-NTK), which simplifies the traditional Generative Adversarial Network (GAN) training process by employing a single-level optimization framework. The GA-NTK models the discriminator as a Gaussian process governed by neural tangent kernels, enabling a more stable training process without the complexities associated with bi-level optimization. The authors provide theoretical guarantees for convergence and present extensive empirical validation demonstrating that GA-NTK achieves improved image quality and lower sample complexity compared to traditional GANs, while also challenging the necessity of a generator in adversarial settings.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to GAN training and the comprehensive theoretical and empirical analysis provided. The single-level optimization framework significantly reduces training instability and mitigates issues such as mode collapse. The modeling of the discriminator as a Gaussian process allows for greater flexibility and robustness in gradient dynamics. However, the paper could benefit from additional comparisons with a wider range of generative models beyond standard GANs to further contextualize the advantages of GA-NTK. Furthermore, while the theoretical guarantees are commendable, their practical applicability in diverse real-world scenarios requires further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions with clarity, making it accessible to readers familiar with generative models. The quality of the writing is high, with clear explanations of complex concepts. The novelty of the approach is significant, as it offers a fresh perspective on adversarial training by leveraging kernel methods. The reproducibility of the results is bolstered by the inclusion of detailed experimental protocols and the provision of empirical results across multiple datasets, although sharing code or data would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the GA-NTK paper presents a compelling and innovative approach to generative modeling that addresses key limitations of traditional GANs. Its theoretical foundations and empirical results suggest a promising direction for future research in adversarial data synthesis. The work stands out for its significant contributions to both the methodology and understanding of generative processes.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "## ICLR Review Notes: Paper on Adversarial Training\n\n### Summary Of The Paper\nThis paper introduces the \"Generative Adversarial NTK\" (GA-NTK), a novel method for adversarial training that aims to overcome issues such as convergence difficulties, mode collapse, and vanishing gradients that are prevalent in traditional adversarial frameworks. The authors propose a single-level optimization approach rather than the conventional multi-level optimization, allowing the use of standard gradient descent techniques. The method models the discriminator as a Gaussian process informed by the neural tangent kernel (NTK), providing a closed-form description of training dynamics. The experiments demonstrate that GA-NTK achieves lower sample complexity and high-quality data generation across various datasets, alongside practical techniques for reducing memory usage during training.\n\n### Strengths And Weaknesses\n**Strengths:**\n- The innovative shift to a single-level optimization framework is a major contribution that simplifies adversarial training, making it more efficient.\n- The paper provides a solid theoretical analysis of convergence properties, enhancing the method's credibility.\n- Empirical results indicate that GA-NTK outperforms traditional adversarial training methods in both efficiency and data quality.\n\n**Weaknesses:**\n- While theoretically robust, the practical complexity of implementing Gaussian processes and NTK may deter some practitioners from adopting this method.\n- The focus on specific datasets may limit the generalizability of the findings, necessitating further validation across diverse tasks and domains.\n\n### Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and theoretical underpinnings. The quality of the writing is high, making the complex concepts accessible to readers. The novelty of the approach is significant, as it proposes a fundamentally different framework for adversarial training. However, the reproducibility of the results may be impacted by the complexity involved in implementing the method, particularly for practitioners unfamiliar with Gaussian processes or NTK.\n\n### Summary Of The Review\nOverall, this paper presents a meaningful advancement in adversarial training by proposing the GA-NTK framework, which simplifies training while improving performance metrics. The combination of theoretical analysis and empirical validation suggests that this method could be a valuable tool in the field, albeit with some practical challenges in implementation.\n\n### Correctness\n4\n\n### Technical Novelty And Significance\n5\n\n### Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel generative model called the Generative Adversarial NTK (GA-NTK), which aims to simplify adversarial data synthesis by utilizing a single-level optimization framework. This approach purportedly addresses significant challenges in conventional GANs, such as convergence issues, mode collapse, and vanishing gradients. The authors claim to provide a theoretical proof of convergence under mild conditions and demonstrate that GA-NTK achieves enhanced stability in training dynamics through the use of an infinite ensemble of networks. Furthermore, the paper emphasizes memory efficiency improvements and reports extensive experimental validation, suggesting that GA-NTK outperforms traditional GANs in terms of sample complexity and quality of generated data.\n\n# Strength And Weaknesses\nThe paper's primary strengths lie in its theoretical contributions, particularly the proof of convergence and the claim of stability in training dynamics, which could represent a significant advancement in the field. The introduction of GA-NTK appears promising, with the potential to change how generative models are trained and utilized, especially in resource-constrained settings. However, the paper lacks clarity in certain sections, particularly regarding the practical implementation of GA-NTK and the specifics of the experimental setup. Additionally, while the authors present impressive results, it remains unclear how well these findings will generalize across different datasets and tasks beyond those tested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat inconsistent; while it articulates its main contributions effectively, some technical details may be challenging for readers to grasp fully. The quality of the writing is generally high, but the presentation of complex concepts could benefit from more intuitive explanations or examples. In terms of novelty, the GA-NTK framework is a significant departure from traditional GAN architectures, making it a noteworthy contribution to the literature. However, the reproducibility of the results is not convincingly established, as the experiments lack sufficient detail regarding parameters, datasets, and implementation specifics that would enable independent verification.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework in the GA-NTK that could simplify adversarial data synthesis and improve performance metrics in generative modeling. While the theoretical advancements and experimental results are impressive, issues surrounding clarity and reproducibility may hinder its immediate applicability and acceptance within the community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the generative adversarial NTK (GA-NTK), a novel generative model aimed at simplifying the training process of GANs by employing a single-level objective. This approach addresses common issues faced by GANs, such as convergence, mode collapse, and gradient vanishing, by modeling the discriminator as a Gaussian process, leading to a more stable training environment. The authors demonstrate that GA-NTK outperforms traditional GANs in sample efficiency and image quality, particularly when evaluated on benchmark datasets like MNIST, CIFAR-10, CelebA, and ImageNet.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to GAN training that emphasizes stability and efficiency, as well as extensive empirical validation showing superior performance on various datasets. The results indicate a significant reduction in FID scores and improved image quality metrics compared to standard GAN architectures, suggesting that GA-NTK effectively mitigates issues of mode collapse. However, the paper could be strengthened by providing more detailed discussions on practical implementations and potential limitations of the GA-NTK framework, particularly in complex multi-modal scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers familiar with GANs and generative modeling. The methodology is presented in a coherent manner, though some sections could benefit from additional clarifications regarding the underlying theoretical foundations of the Gaussian process modeling. The novelty of the approach lies in its integration of kernel methods with adversarial data generation, which presents a fresh perspective in the field. The reproducibility of the results is facilitated by the detailed experimental setup and performance metrics provided, though the availability of code or datasets could enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in generative modeling through the introduction of GA-NTK, demonstrating improved training stability and sample efficiency over traditional GANs. While the contributions are noteworthy, further exploration of limitations and practical applications could enhance the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper proposes a novel approach to generative modeling by utilizing a Gaussian process (NTK-GP) as a discriminator within an adversarial learning framework. The authors argue that this method alleviates common challenges faced by traditional Generative Adversarial Networks (GANs), such as convergence issues and vanishing gradients. Additionally, they claim that the proposed method, referred to as GA-NTK, reduces memory consumption and sample complexity while avoiding the need for a generator in image synthesis, challenging established paradigms in the field.\n\n# Strength And Weaknesses\nThe paper presents several significant contributions, including the introduction of a single-level optimization paradigm to address convergence issues and the use of NTK-GP to stabilize training. However, it also raises substantial concerns regarding its underlying assumptions, such as the efficacy of adversarial learning for data synthesis, the robustness of convergence guarantees, and the generalizability of results across different datasets. Furthermore, the paper does not adequately compare GA-NTK with hybrid or alternative models, which could limit the understanding of its advantages and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper articulates its methodology and findings clearly, the reliance on specific assumptions diminishes the overall clarity of its contributions. The novelty of using NTK-GP as a discriminator is intriguing, yet it may be overshadowed by the paper's lack of empirical validation across diverse scenarios. Reproducibility is a concern, as the convergence guarantees and performance claims depend heavily on the specific conditions outlined by the authors, which may not hold universally.\n\n# Summary Of The Review\nOverall, the paper presents an innovative approach to generative modeling through the use of NTK-GP but suffers from significant limitations in its assumptions and empirical validation. The claims made regarding advantages over traditional GANs need further scrutiny and support from a broader range of experiments.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel model termed generative adversarial NTK (GA-NTK) that aims to simplify the training of Generative Adversarial Networks (GANs) by employing a single-level optimization framework. This model utilizes a Gaussian process discriminator governed by a neural tangent kernel to tackle prevalent issues in GAN training, such as convergence problems, mode collapse, and gradient vanishing. Through comprehensive experiments across various datasets, the authors demonstrate that GA-NTK outperforms traditional GAN architectures in terms of sample efficiency and image quality, particularly in scenarios with limited data.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the complexities of GAN training by reducing it to a single-level optimization problem, which could make the training process more intuitive and manageable. The empirical results are compelling, showing that GA-NTK consistently excels over established GAN variants, particularly in challenging settings. However, a potential weakness is the limited exploration of the theoretical implications of using a Gaussian process and the neural tangent kernel in this context, which could provide a deeper understanding of the model's performance characteristics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The writing quality is generally high, with a logical flow that aids comprehension. The novelty of the approach is significant as it presents a new paradigm in GAN training. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setups and hyperparameter settings, which are critical for others looking to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper introduces a valuable contribution to GAN training through the GA-NTK model, presenting a simpler and effective alternative to traditional methods. The empirical results are strong, though the theoretical foundations could be further elaborated to enhance understanding and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces a novel machine learning framework that aims to address significant challenges in the training processes of existing methodologies. The authors propose an innovative approach that enhances performance and stability while minimizing training complexity. Through rigorous theoretical analysis and comprehensive empirical evaluation, the findings suggest that the proposed method outperforms established baselines across various scenarios.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Novelty:** The framework presents a unique perspective that has the potential to advance current methodologies in machine learning.\n2. **Theoretical Contributions:** A solid theoretical foundation is provided, offering insights into the convergence and stability properties of the proposed method.\n3. **Practical Relevance:** The work tackles real-world challenges, indicating that the approach could be beneficial in practical applications.\n4. **Comprehensive Experiments:** The authors conducted extensive experiments, demonstrating the effectiveness of the method against well-established baselines.\n\n**Weaknesses:**\n1. **Clarity of Presentation:** Certain sections could be clearer, which may hinder the reader's understanding of the proposed methodology's implications.\n2. **Limited Baseline Comparisons:** While the experimental setup is thorough, the choice of baselines could be broadened to include more recent approaches for a more robust evaluation.\n3. **Scalability Concerns:** The scalability of the proposed method is not sufficiently addressed, particularly concerning larger datasets or complex applications.\n4. **Hyperparameter Sensitivity:** The paper lacks a discussion on the sensitivity of the method to hyperparameter tuning, which is important for practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper presents a significant contribution with a balance of theoretical and empirical insights. However, clarity in certain sections could be improved to enhance understanding. The novelty of the approach is commendable, yet the reproducibility of the results could be bolstered through more detailed discussions on limitations and computational requirements.\n\n# Summary Of The Review\nThis paper offers a promising contribution to the machine learning field, combining theoretical rigor with practical applicability. While there are notable strengths, particularly in novelty and empirical evaluation, improvements in clarity and baseline comparisons would enhance the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach to generative adversarial networks (GANs) by proposing a model called generative adversarial NTK (GA-NTK). This model addresses common challenges in GAN training, such as convergence issues, mode collapse, and vanishing gradients, by employing a Gaussian process to model the discriminator. The authors provide a theoretical analysis of GA-NTK's convergence and present empirical results demonstrating its efficacy and practical advantages in data synthesis tasks. Key contributions include the formulation of a single-level optimization problem, which simplifies the training process while retaining the benefits of adversarial learning, and techniques to improve both memory efficiency and image quality.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to simplifying GAN training through the use of Gaussian processes for the discriminator, which presents a significant departure from traditional methods. The theoretical proof of convergence under certain conditions adds robustness to the proposed model. Additionally, the empirical results showcase the effectiveness of GA-NTK in producing high-quality synthetic data with lower sample complexity. However, potential weaknesses include the limited exploration of the model’s performance across diverse datasets and the need for further validation in real-world applications. The implications of using Gaussian processes, while promising, also warrant a more thorough examination regarding scalability and computational efficiency.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the GA-NTK model, making it accessible to readers familiar with GANs and kernel methods. The methodology is presented in a coherent manner, and the theoretical aspects are backed by rigorous proofs, enhancing the paper's overall quality. The novelty of the approach is significant, as it bridges kernel methods with adversarial data synthesis in a way that has not been extensively explored in the literature. The reproducibility of the results appears to be supported by the clear presentation of experimental setups, although additional implementation details could further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and novel approach to addressing the training difficulties associated with GANs through the introduction of the GA-NTK model. The theoretical contributions and empirical findings suggest that this method may offer practical advantages over traditional GAN training approaches, although further validation in diverse contexts would strengthen its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents GA-NTK, a novel generative model that addresses the prevalent training issues of Generative Adversarial Networks (GANs) by utilizing a single-level optimization framework. By employing a Gaussian process discriminator (NTK-GP), the authors simplify the adversarial data synthesis process, enhancing convergence and mitigating common problems such as mode collapse and gradient vanishing. The methodology includes a robust convergence analysis and practical training techniques, validated through extensive experiments across various datasets, demonstrating that GA-NTK achieves superior performance in terms of convergence and sample efficiency compared to traditional GAN variants.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to GAN training, particularly through the introduction of a single-level optimization method that simplifies the generative process. The theoretical guarantees regarding convergence under mild conditions add significant value and provide a solid foundation for the proposed method. Additionally, the extensive empirical validation across multiple datasets underscores the practical applicability of the approach. However, the paper could benefit from a more detailed discussion of the limitations of GA-NTK and its performance in highly complex data scenarios beyond those tested. Furthermore, while the introduction of practical techniques for memory efficiency is noted, specifics on their implementation could enhance the paper's utility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodologies, and findings. The clarity of the writing, combined with a logical progression from problem identification to proposed solution and empirical validation, makes it accessible to readers with varying backgrounds. The novelty of using NTK for GAN training is significant, as it bridges theoretical insights with practical applications. As for reproducibility, the authors provide adequate details on the experimental setup and evaluation metrics, although further elaboration on the implementation of the proposed techniques would strengthen this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in the field of generative modeling by addressing critical issues associated with GAN training through a novel single-level optimization framework. The combination of theoretical analysis and empirical results provides a solid foundation for the effectiveness of GA-NTK, making it a valuable contribution to the community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Single-Level Adversarial Data Synthesis Based on Neural Tangent Kernels\" addresses critical limitations of Generative Adversarial Networks (GANs), specifically issues related to convergence, mode collapse, and gradient vanishing. The authors propose a novel framework called GA-NTK, which utilizes a single-level objective to improve the training dynamics of adversarial models. The methodology is rigorously detailed, including mathematical formulations and practical implementation strategies. The findings demonstrate that GA-NTK outperforms traditional GAN architectures across various datasets, such as MNIST, CIFAR-10, CelebA, and ImageNet, particularly in terms of convergence speed and image diversity, while avoiding common pitfalls like vanishing gradients.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its clear articulation of the problem with existing GAN methodologies and the introduction of GA-NTK as a compelling alternative. The proposed framework shows promise in addressing several pressing issues in adversarial training, supported by extensive experimental validation. However, a notable weakness is the sensitivity to hyperparameters, which may limit the generalizability of the results. Additionally, while the paper discusses variants of GA-NTK, it would benefit from more comprehensive comparisons with recent advancements in GAN architectures beyond traditional baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions and methodology. The quality of the writing is high, with a coherent flow from problem identification to proposed solution and results. The novelty of the approach, leveraging neural tangent kernels for adversarial data synthesis, presents a significant shift from conventional GAN frameworks. However, reproducibility could be enhanced by providing more detailed implementation specifics and hyperparameter tuning strategies, as the current level of detail may pose challenges for other researchers attempting to replicate the results.\n\n# Summary Of The Review\nOverall, this paper offers a significant advancement in the field of adversarial data synthesis by addressing core limitations of GANs through the introduction of GA-NTK. The findings are promising and the methodology is solid, although more attention to hyperparameter sensitivity and reproducibility would strengthen the contribution.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents the Generative Adversarial NTK (GA-NTK), a novel generative model that addresses the complexities of traditional Generative Adversarial Networks (GANs) by employing a single-level optimization framework. By modeling the discriminator as a Gaussian process using the neural tangent kernel (NTK), the authors achieve closed-form training dynamics that enhance convergence stability and mitigate issues like gradient vanishing. The empirical evaluations demonstrate that GA-NTK outperforms existing approaches in generating diverse samples, especially in low-data regimes, as measured by standard image quality metrics such as Fréchet Inception Distance (FID) and average max-SSIM (AM-SSIM).\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to simplifying the GAN training process through the use of NTK, which allows for a more stable and efficient training regime. The theoretical convergence analysis is robust, providing a solid foundation for the claims made. However, the paper could benefit from a more extensive exploration of the practical implications of the proposed method in real-world applications, as well as comparisons with a broader range of existing models beyond the standard benchmarks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates complex ideas, making the methodology accessible to the reader. The quality of the writing and the depth of the theoretical analysis contribute positively to the overall clarity. The novelty is significant, as it introduces a fresh perspective on adversarial learning by leveraging kernel methods. The reproducibility of the results is likely high, given the detailed descriptions of the methodologies and the empirical evaluations provided.\n\n# Summary Of The Review\nOverall, the paper effectively contributes to the field of adversarial learning by introducing the GA-NTK model, which simplifies the optimization challenges faced by traditional GANs. Its innovative approach and strong theoretical backing make it a significant advancement in the domain, albeit with room for broader empirical comparisons.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the GA-NTK method, aimed at simplifying the training of Generative Adversarial Networks (GANs) by purportedly avoiding the complexities of bi-level optimization. It claims to achieve lower sample complexity while providing convergence guarantees under mild conditions. However, the paper's methodology lacks a comprehensive evaluation against existing state-of-the-art methods, particularly in terms of the quality and diversity of generated images, which raises questions about its effectiveness.\n\n# Strength And Weaknesses\nThe primary strength of the GA-NTK approach lies in its theoretical framework, which proposes a single-level optimization method that may simplify GAN training. However, this comes with significant weaknesses, including a lack of rigorous empirical validation, insufficient comparison with existing methods, and an over-reliance on theoretical claims. The experiments do not convincingly demonstrate the model's ability to generate high-quality, diverse images, which are critical for practical applications. Additionally, the limitations related to memory consumption and hyperparameter tuning are substantial drawbacks that hinder the model's practicality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas with a reasonable level of clarity, but the quality of the arguments is undermined by insufficient empirical support and the reliance on theoretical justifications. The novelty of the GA-NTK method appears overstated, as it does not offer a significant advancement in generative modeling. The reproducibility of the results is questionable due to the lack of comprehensive comparisons and limited experimental validation.\n\n# Summary Of The Review\nOverall, the GA-NTK method presents an interesting theoretical perspective on GAN training, but it falls short in empirical validation and practical applicability. The claims made regarding improved performance, convergence guarantees, and reduced sample complexity are not convincingly substantiated by the experiments conducted in the paper.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents GA-NTK, a novel generative modeling framework that transforms the training dynamics of Generative Adversarial Networks (GANs) by utilizing the Gaussian Process representation of the discriminator. The methodology streamlines GAN training into a single-level optimization process, significantly enhancing training stability and convergence properties through ordinary gradient descent. The empirical findings demonstrate that GA-NTK achieves superior performance with lower sample complexity than traditional GANs, showcasing its efficacy across various datasets such as MNIST, CIFAR-10, and CelebA-HQ. Furthermore, the framework offers practical techniques, including batch-wise and multi-resolutional variants, which improve memory efficiency and image quality.\n\n# Strength And Weaknesses\nThe primary strength of the GA-NTK paper lies in its innovative approach that addresses common issues in GAN training, such as mode collapse and vanishing gradients. The introduction of a Gaussian process discriminator provides an infinite ensemble of networks, enhancing the diversity of generated outputs. Additionally, the empirical validation across diverse datasets demonstrates the framework's robustness and practical applicability. However, a potential weakness is the need for further exploration of the framework's scalability to exceptionally large datasets, which could impact its generalizability in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to a broad audience. The quality of the writing is high, with detailed explanations of the methodology and comprehensive experimental results. The novelty of GA-NTK is significant, as it bridges kernel methods and adversarial data synthesis in a unique way. The reproducibility of the results is bolstered by the thorough description of experimental setups and hyperparameter tuning, allowing other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, the GA-NTK paper presents a significant advancement in generative modeling, effectively addressing key challenges in GAN training while offering promising results across various datasets. Its innovative approach, combined with practical applications and empirical validation, positions it as a noteworthy contribution to the field of machine learning.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the Generative Adversarial NTK (GA-NTK), a theoretical framework that reformulates the training dynamics of Generative Adversarial Networks (GANs) from a bilevel optimization problem to a single-level optimization problem. This shift is grounded in the Neural Tangent Kernel (NTK) framework, allowing the discriminator to be modeled as a Gaussian process. The authors provide a convergence analysis under mild conditions, demonstrating that the smoothness of the loss function ensures effective gradient descent. The GA-NTK framework addresses common issues faced in GAN training, such as mode collapse and vanishing gradients, and opens new avenues for theoretical exploration in generative modeling.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical foundation and the innovative approach of simplifying GAN training dynamics through the NTK framework. The convergence proof and the modeling of the discriminator as an infinite ensemble of neural networks provide significant insights into the optimization landscape of GANs. Additionally, the statistical interpretation of the loss function enhances the understanding of the generative process. However, the paper could be further strengthened by empirical evaluations comparing GA-NTK with traditional GANs, as the theoretical advantages remain largely untested in practical scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical contributions clearly, making complex concepts accessible. The quality of the writing is high, and the logical flow of ideas supports reader comprehension. The novelty of the GA-NTK framework is significant, as it shifts the paradigm in understanding GAN training dynamics. While the theoretical contributions are well-established, the reproducibility of the results would benefit from additional empirical validation, which is currently lacking.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical advancement in the understanding of GANs through the introduction of GA-NTK. While the contributions are significant and well-articulated, the lack of empirical validation leaves some questions about the practical implications of the proposed framework.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"SINGLE-LEVEL ADVERSARIAL DATA SYNTHESIS BASED ON NEURAL TANGENT KERNELS\" presents a novel method for adversarial data synthesis using Gaussian processes informed by neural tangent kernels (NTK). The main contribution is the formulation of the adversarial training process as a single-level optimization problem, which circumvents the typical complexities associated with bilevel optimization in GAN frameworks. The authors implement a discriminator that approximates an infinite ensemble of infinitely wide neural networks. The methodology includes hyperparameter tuning through a unidirectional search algorithm and introduces batch-wise GA-NTK for scalability, enabling efficient processing of large datasets. Experimental results demonstrate the efficacy of GA-NTK in generating high-quality images across various datasets, with notable improvements in convergence speed compared to traditional GANs.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to simplifying the optimization process in adversarial training, leading to faster convergence and improved performance metrics such as FID and AM-SSIM. The inclusion of a unidirectional search for hyperparameter tuning is a practical contribution that enhances the usability of the method. However, the paper exhibits weaknesses in its limited discussion of broader implications and potential applications of the proposed method. Additionally, while the experiments show promising results, the focus on technical implementation details may detract from a comprehensive understanding of the method's significance in the context of existing literature.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, with sufficient detail for reproducibility, including code availability on GitHub. The novelty lies in the integration of NTK concepts into adversarial training, yet the contributions to the field could be more explicitly articulated. The empirical results are robust, demonstrating the method's ability to generate high-quality images, but further exploration of the implications of these findings would enhance the paper's impact.\n\n# Summary Of The Review\nOverall, the paper offers a significant advancement in adversarial data synthesis through the innovative application of neural tangent kernels in a single-level optimization framework. While the methodology and results are compelling, the discussion lacks depth concerning the broader significance and potential applications of the approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces GA-NTK, a novel framework aimed at addressing the training difficulties commonly associated with Generative Adversarial Networks (GANs), such as mode collapse and vanishing gradients. The authors claim that GA-NTK utilizes a single-level optimization framework and demonstrates convergence under mild conditions, suggesting improvements in sample complexity and image quality compared to existing GAN architectures. Empirical results are presented to substantiate these claims, including the use of FID scores for image quality assessment.\n\n# Strength And Weaknesses\nWhile the paper's claims regarding the advantages of GA-NTK over GANs are intriguing, they often lack a thorough comparative analysis with existing approaches. The dismissal of prior GAN techniques without adequate engagement undermines the paper's contributions. For instance, many GAN variants have effectively simplified bilevel optimization, and the claimed improvements in sample complexity and convergence properties do not account for similar advancements in the GAN literature. Moreover, the reliance on specific performance metrics like FID may overlook broader critiques regarding their effectiveness in capturing perceptual image quality. Overall, the paper makes significant claims but falls short in contextualizing its contributions within the wider research landscape.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure, though certain claims lack sufficient backing through comparative analysis. The novelty of transitioning from a neural network discriminator to a Gaussian process is noted, but similar explorations in prior works are not adequately addressed. The reproducibility of the results may be hampered by the lack of thorough benchmarks against existing GAN methodologies, particularly in demonstrating the claimed improvements in performance and stability.\n\n# Summary Of The Review\nThe paper presents GA-NTK as a promising alternative to GANs, claiming significant advancements in training stability and sample efficiency. However, the lack of comprehensive comparisons with existing GAN techniques and insufficient acknowledgment of prior work detracts from the perceived novelty and impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"SINGLE-LEVEL ADVERSARIAL DATA SYNTHESIS BASED ON NEURAL TANGENT KERNELS\" presents a novel approach to data synthesis using neural tangent kernels (NTKs). The authors propose a generator and discriminator framework that leverages NTKs to generate plausible data while addressing the common issues of mode collapse associated with traditional generative adversarial networks (GANs). Through a series of experiments, the paper demonstrates that their approach not only improves the quality of generated data but also enhances stability during training, leading to better model convergence.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative application of NTKs to adversarial data synthesis, which presents a fresh perspective on tackling the limitations of existing GAN frameworks. The methodology is well-structured, and the experimental results substantiate the claims made regarding quality and stability improvements. However, the paper could benefit from clearer definitions and consistent notation, particularly regarding notational elements like \"D\" for the discriminator, and standardizing terms like \"gradient descent.\" Additionally, some of the complex explanations, such as those involving Gaussian processes, might pose challenges to readers unfamiliar with the underlying concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents significant contributions, its clarity is compromised by inconsistent terminology and formatting issues that could hinder understanding. The use of jargon should be minimized to enhance accessibility, particularly for readers with varying expertise. The quality of the empirical results is commendable, but reproducibility may be affected due to insufficient detail in the methodology, especially regarding the experimental setup and parameter settings. Clearer timelines and indications of future experiments could also enhance the paper's overall structure and readability.\n\n# Summary Of The Review\nOverall, the paper introduces a promising approach to adversarial data synthesis using neural tangent kernels, demonstrating notable improvements in data quality and training stability. However, issues related to clarity, notation consistency, and reproducibility need to be addressed to enhance the paper's impact and accessibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to generative modeling through the introduction of the Generative Adversarial Neural Tangent Kernel (GA-NTK), which aims to alleviate the training difficulties commonly associated with Generative Adversarial Networks (GANs). The authors propose a single-level optimization strategy and provide convergence guarantees for their method. Experimental results demonstrate the performance of GA-NTK on a limited set of datasets, suggesting potential improvements in sample efficiency over traditional GANs.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing GAN training challenges, particularly the introduction of convergence guarantees. However, several weaknesses are evident: the exploration of GA-NTK is restricted to a narrow scope, neglecting potential applications in reinforcement learning or unsupervised learning, and the ethical implications of data synthesis are not discussed. Additionally, the focus on single-level optimization may overlook the benefits of alternative strategies, and the limited dataset diversity in experiments raises questions about the generalizability of the findings. A lack of ablation studies further constrains the understanding of the model's components and their contributions to performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear articulation of the proposed methodology and findings. However, the novelty is somewhat diminished by the lack of extensive exploration of the model's applicability and robustness in various contexts. The reproducibility of the results could be enhanced through detailed descriptions of experimental setups and model configurations, as well as by including a more diverse set of datasets for validation.\n\n# Summary Of The Review\nOverall, the paper offers a promising contribution to generative modeling through the GA-NTK framework, but it suffers from limitations in scope and empirical validation. Expanding the analysis to include a broader range of applications, datasets, and ethical considerations would significantly enhance the paper's impact and relevance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel generative model called GA-NTK, which is a single-level optimization framework that addresses common issues in Generative Adversarial Networks (GANs) such as convergence problems, mode collapse, and gradient vanishing. It employs Gaussian process modeling for the discriminator, which leads to closed-form dynamics and provides a statistical interpretation of the training process. The authors present a convergence analysis with sufficient conditions for stability in training, and they conduct extensive experiments to validate the effectiveness of GA-NTK against traditional GAN architectures, using metrics like Fréchet Inception Distance (FID) and Average Max-SSIM (AM-SSIM).\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous statistical methodology and clear convergence analysis, which provide a solid theoretical foundation for the proposed model. The use of statistical measures to evaluate performance and convergence enhances the credibility of the findings. However, a potential weakness is that the paper may rely too heavily on theoretical constructs without delving deeply into practical implications or applications of the GA-NTK framework in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical contributions clearly, with appropriate definitions and proofs supporting the convergence results. The quality of the writing is high, making complex statistical concepts accessible. The novelty of the approach is significant, particularly in its application of Gaussian processes and statistical divergence measures to GAN training. The reproducibility of the experiments is supported by the extensive statistical analysis, though details on specific implementation choices could enhance clarity further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in generative modeling with its GA-NTK framework, providing a strong theoretical basis for addressing common GAN issues. The rigorous statistical approach and thorough empirical evaluation contribute to its significance in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents GA-NTK, a novel approach to adversarial data synthesis that emphasizes single-level modeling without proposing a direct replacement for existing Generative Adversarial Networks (GANs). The authors introduce variants such as batch-wise GA-NTK and multi-resolutional GA-NTK. While the paper reports lower sample complexity in experiments conducted on low-dimensional datasets (MNIST, CIFAR-10, CelebA), it does not offer a comprehensive evaluation of the model's scalability or effectiveness in high-dimensional settings. Additionally, it touches on the challenges of generating high-quality images with limited training data but lacks concrete strategies for addressing these issues.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of GA-NTK and its variants, which could pave the way for future research in adversarial data synthesis. However, the paper has significant weaknesses, including a limited evaluation scope that raises concerns about scalability and generalizability. The lack of a thorough analysis of trade-offs regarding sample complexity versus image quality, as well as the reliance on hyperparameter tuning, detracts from the overall robustness of the findings. Furthermore, potential complications associated with the proposed adaptations are not adequately discussed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate, with some sections outlining the methodology and findings understandably, while others lack detailed explanations, particularly regarding future research directions. The quality of the experiments is constrained by limited dataset diversity, which restricts reproducibility across different contexts. Novelty is present in the introduction of GA-NTK and its variants; however, the contributions do not significantly advance the field or challenge existing methodologies. The lack of comprehensive evaluations or clear future pathways reduces the paper's overall impact.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to adversarial data synthesis through GA-NTK, but it struggles with scalability, generalizability, and practical implications. The limited scope of experiments and insufficient analysis of trade-offs impede the robustness of its contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a generative model termed GA-NTK, which purportedly addresses some of the convergence challenges associated with Generative Adversarial Networks (GANs) through a single-level optimization method. The authors employ a Gaussian process for the discriminator and introduce a closed-form formula to simplify the training process. They claim to prove convergence under mild conditions and to offer techniques for memory consumption reduction, although these contributions appear to overlap significantly with established practices in the field.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its attempt to tackle well-known issues in GAN training, such as mode collapse and convergence. However, the contributions are largely derivative, with the methodology and results lacking originality. The claims of novelty, particularly regarding the closed-form solution and convergence proof, do not significantly advance the state of knowledge in the field. The use of standard datasets like MNIST and CIFAR-10 for experimentation does not add to the paper’s impact, as these are well-trodden paths in GAN research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation; however, the novelty is overstated, and many of the techniques discussed are not new to the community. The quality of the writing is satisfactory, but the arguments lack depth and fail to convincingly articulate how the proposed method differs from existing solutions. Reproducibility is not adequately addressed, as the experimental setup does not provide sufficient detail on implementation specifics, which limits the ability of others to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper does not contribute significantly to the field of generative models, with its claims of novelty and improvement being unsubstantiated by the results presented. The work appears to be an academic exercise rather than a meaningful advancement in the understanding or application of GANs.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces GA-NTK, a novel framework for adversarial data synthesis that simplifies the optimization problem compared to traditional Generative Adversarial Networks (GANs). The authors propose a single-level optimization approach, transitioning from the prevalent bilevel optimization, and provide convergence analysis under specific conditions. The use of Gaussian processes as a discriminator enhances the model's performance, while practical techniques such as batch-wise training and multi-resolution methods improve memory efficiency and image quality. The paper claims lower sample complexity for GA-NTK, supported by extensive experimental validation.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its novel approach to adversarial synthesis and the simplification of the optimization process, which could lead to broader applications beyond adversarial models. The convergence analysis is a valuable contribution, albeit limited by its specific conditions. The use of Gaussian processes is innovative but could benefit from further exploration with other priors. However, the paper lacks ablation studies to clarify the impact of various components on performance and does not sufficiently explore hybrid models that combine the GA-NTK framework with traditional generator networks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and organized, making it accessible to readers. The quality of the experimental validation is commendable, though additional ablation studies would enhance reproducibility and understanding of the underlying mechanisms. The novelty of the approach is significant, particularly in the context of kernel methods applied to generative modeling, although some aspects, such as the integration of existing GAN enhancements, would benefit from a more thorough exploration.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in adversarial data synthesis through the introduction of GA-NTK. While the novel contributions are promising, further empirical validation and exploration of hybrid model architectures could strengthen its impact and applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces the generative adversarial NTK (GA-NTK) model, a novel framework aimed at enhancing the performance of generative adversarial networks (GANs) in image synthesis tasks. The authors evaluate GA-NTK against various benchmarks, including MNIST, CIFAR-10, CelebA, and ImageNet, demonstrating its superior performance through significantly lower Fréchet Inception Distance (FID) scores and higher Average Multi-Scale Structural Similarity Index (AM-SSIM) scores compared to traditional GAN variants. Notably, GA-NTK exhibits lower sample complexity, faster convergence during training, and effective mitigation of mode collapse, providing a promising alternative to existing GAN frameworks.\n\n# Strength And Weaknesses\nStrengths of the paper include the clear demonstration of GA-NTK's advantages over multiple GAN baselines, evidenced by quantitative metrics such as FID and AM-SSIM scores. The lower sample complexity and faster convergence rates are particularly noteworthy, indicating practical applicability in scenarios with limited training data. Furthermore, the qualitative visual results reinforce the quantitative findings, showcasing the model's ability to generate coherent and appealing images. However, a potential weakness lies in the lack of extensive discussions on the theoretical foundations of GA-NTK, which may limit understanding of its underlying mechanisms. Additionally, while the benchmarks used are comprehensive, further exploration of GA-NTK's performance in diverse real-world applications would strengthen its empirical significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its methodology, contributions, and findings, making it accessible to readers with a background in deep learning and generative models. The quality of the experiments is high, with thorough evaluations against established benchmarks. The technical novelty of GA-NTK is significant, as it addresses key challenges faced by traditional GANs. However, the reproducibility of the results could be enhanced by providing more details regarding the experimental setup, hyperparameter tuning, and computational resources used.\n\n# Summary Of The Review\nOverall, the paper successfully presents GA-NTK as a robust alternative to traditional GAN models, demonstrating significant improvements in benchmark performance, training efficiency, and diversity of generated images. While it excels in empirical results and clarity, further exploration of its theoretical underpinnings and reproducibility aspects would bolster its contributions to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing generative adversarial networks (GANs) through a bilevel optimization framework. The authors propose a new f-divergence measure that enhances the stability and efficiency of GAN training. Using extensive experiments on benchmark datasets, they demonstrate that their method outperforms existing techniques in terms of both convergence speed and quality of generated samples.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of bilevel optimization to GANs, which is a significant contribution to the field. The proposed f-divergence measure offers a fresh perspective on evaluating and improving GAN performance. However, the paper suffers from clarity issues, particularly in the abstract and introduction, where complex terminology and convoluted sentences may hinder reader comprehension. Additionally, the inconsistent formatting and typographical errors detract from the overall professionalism of the presentation.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel approach with potential significance, its clarity is compromised by technical jargon and complex sentence structures. The methodology is described adequately, but the flow between sections could be improved to enhance readability. Reproducibility is hampered by the lack of detailed explanations for certain technical terms and inconsistent referencing style. A more uniform structure and clearer definitions would greatly benefit the overall quality of the paper.\n\n# Summary Of The Review\nOverall, the paper makes a commendable contribution to the field of GAN optimization through its innovative approach. However, significant improvements in clarity, formatting, and accessibility are necessary to make the findings more impactful and reproducible.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.0809497267054167,
    -1.4557627383207212,
    -1.723592444713092,
    -1.6560771380625434,
    -1.8591214589645466,
    -1.5502643564365306,
    -1.6259472135360127,
    -1.7162519187830503,
    -1.526922590218381,
    -1.7783446119068522,
    -1.55272756512654,
    -1.5443689870126822,
    -1.5807371956947442,
    -1.7034576788567375,
    -1.5475901451166088,
    -1.6100677783123662,
    -1.6472398529179573,
    -1.6322427081337159,
    -1.520819642555675,
    -1.7600130690264097,
    -1.9991429135537138,
    -1.7980269667291517,
    -1.5471444633375788,
    -1.7823527822792995,
    -1.9141790986751126,
    -1.7320941920991886,
    -1.9866365330811393,
    -1.4459668118964935,
    -1.5029460199141664
  ],
  "logp_cond": [
    [
      0.0,
      -2.0094212839797994,
      -2.0029683430295977,
      -2.006016275923255,
      -2.015118618231606,
      -1.9978192574110243,
      -1.982671159887937,
      -2.012850495918618,
      -2.0017408941168546,
      -2.0056271221997757,
      -2.005885059807665,
      -2.0454448664672733,
      -2.0059032280307125,
      -2.0020047876922824,
      -2.0119963782546906,
      -2.010055867804261,
      -2.0163703980293306,
      -2.009012528569786,
      -2.0155275149166063,
      -2.001654625738827,
      -2.0126845615732147,
      -2.0046067628601634,
      -2.007786068433177,
      -2.00532046924621,
      -2.00188507881474,
      -2.0098866753620857,
      -2.003498026823149,
      -2.0234844288064324,
      -2.0492086870490374
    ],
    [
      -1.1023391409364462,
      0.0,
      -0.9936368779725817,
      -0.919614236537727,
      -1.049489168401426,
      -0.9980769324226434,
      -1.0220101285672878,
      -0.9930871080365903,
      -0.999229299435639,
      -1.026926187802902,
      -1.0155679215270508,
      -1.2158301041510886,
      -0.9549173345957518,
      -1.0128473869631016,
      -1.0967959073396956,
      -1.0140594348967682,
      -1.1441159414695017,
      -1.0136777994946176,
      -1.0983532590872154,
      -1.0513567109417805,
      -1.013854306782358,
      -1.1167357523886519,
      -1.085909255766586,
      -1.039836131027838,
      -1.0694592192364891,
      -1.0579534420450798,
      -1.0311911140678545,
      -1.0903140594631475,
      -1.1554931878142458
    ],
    [
      -1.3922909708140754,
      -1.361857010190766,
      0.0,
      -1.296375640685967,
      -1.328335980849935,
      -1.3569250241448785,
      -1.3417877932154085,
      -1.3391128630973617,
      -1.3589374943628978,
      -1.3495612982570442,
      -1.3390786628069395,
      -1.4807023205976597,
      -1.363137112841793,
      -1.3564209303773735,
      -1.366017426142383,
      -1.3796826036362497,
      -1.3444974989517373,
      -1.3390701354264527,
      -1.3681239728801162,
      -1.3763848426573813,
      -1.3364584595833782,
      -1.417054046199203,
      -1.3717265527947968,
      -1.400928158161439,
      -1.3229045393706278,
      -1.403476680823155,
      -1.3566672663691923,
      -1.3519123061814673,
      -1.469017190530435
    ],
    [
      -1.3037999620860454,
      -1.1883973866009276,
      -1.2014648988406174,
      0.0,
      -1.1620373530329182,
      -1.163023954448645,
      -1.2147130205194587,
      -1.197676603173985,
      -1.1630211451893044,
      -1.22508435745024,
      -1.2466576371320581,
      -1.421514728545419,
      -1.1111844631781898,
      -1.1895477606369829,
      -1.2671878625917299,
      -1.2261916395140815,
      -1.314450736145837,
      -1.1184123846470095,
      -1.2226971145194234,
      -1.2531920705307202,
      -1.2400143453299723,
      -1.2826946708505838,
      -1.2708085472177997,
      -1.2565066933462525,
      -1.223346113897733,
      -1.3004847131276147,
      -1.1463193305531056,
      -1.2830228249332152,
      -1.324660434188938
    ],
    [
      -1.5832152954201684,
      -1.4975127149705487,
      -1.4751899360120662,
      -1.4685295703833345,
      0.0,
      -1.5141952679207369,
      -1.5535733610022,
      -1.5161827242212809,
      -1.5291135839898793,
      -1.5046926002392522,
      -1.5269499582147843,
      -1.608814495228371,
      -1.4858630370180206,
      -1.5013964083960416,
      -1.5642562830253048,
      -1.5468738038700875,
      -1.5072401941818165,
      -1.4666970333570066,
      -1.531491192041128,
      -1.53823975104724,
      -1.4714324212209984,
      -1.6027668901219438,
      -1.4708550486798537,
      -1.5418196332046192,
      -1.522738559947585,
      -1.5243467323306867,
      -1.4204996197476762,
      -1.5734398041767734,
      -1.5948264063833824
    ],
    [
      -1.2246455779871241,
      -1.0873408480205977,
      -1.1452432003978756,
      -1.0405284070603533,
      -1.0918040022116136,
      0.0,
      -1.1526087293505978,
      -1.0574016256375318,
      -1.1064289904376436,
      -1.0934447943017485,
      -1.1256926471281985,
      -1.3204242132517694,
      -1.057164263339725,
      -1.0921705544278384,
      -1.1732216709828098,
      -1.0928327222996104,
      -1.1334631802326178,
      -1.0931580193814303,
      -1.1496735776666327,
      -1.1487035431545187,
      -1.1147132745671604,
      -1.1863215448725881,
      -1.154908427383367,
      -1.1717426391892316,
      -1.1176846091017956,
      -1.1684376107459624,
      -1.0854563216465327,
      -1.2055935310122399,
      -1.2756383452711746
    ],
    [
      -1.4080519650838796,
      -1.2356089630714806,
      -1.2577439458809268,
      -1.2187115110217603,
      -1.3075789382175194,
      -1.2724076739696832,
      0.0,
      -1.2639254403294213,
      -1.3176971932499109,
      -1.304597545483805,
      -1.2664003232742636,
      -1.3481395391346114,
      -1.2566277984385477,
      -1.2510244739222467,
      -1.3619182428132988,
      -1.2504256694464562,
      -1.3447199683665276,
      -1.2964846442556615,
      -1.2703824367784564,
      -1.3040038928944562,
      -1.2713466696346167,
      -1.3613174653560394,
      -1.3443671113861535,
      -1.2703145103208333,
      -1.3324033788820233,
      -1.3206552300026906,
      -1.2672088094497114,
      -1.340154419015729,
      -1.3719083709879072
    ],
    [
      -1.454041117140123,
      -1.3290383611162453,
      -1.3089793286916256,
      -1.3148801009804991,
      -1.3049704030851048,
      -1.3215373160794326,
      -1.3400453469098108,
      0.0,
      -1.3485841776450618,
      -1.3265264614712073,
      -1.3161031289010234,
      -1.4829128906319373,
      -1.298917077593935,
      -1.2745197354681639,
      -1.3606587254711473,
      -1.357676197629141,
      -1.3871674449067724,
      -1.289670652012946,
      -1.339923960184119,
      -1.3589298565453336,
      -1.3482709972828393,
      -1.3877628021793311,
      -1.3533306213361125,
      -1.3378768025161718,
      -1.3473131527379134,
      -1.3344661049757283,
      -1.3023463598845582,
      -1.3621341050857345,
      -1.4559779588453972
    ],
    [
      -1.2205459121391715,
      -1.113653903321825,
      -1.124175822526801,
      -1.0888842761893938,
      -1.169503879528534,
      -1.1113269378697883,
      -1.1707376448428086,
      -1.0963160991783532,
      0.0,
      -1.164374832127524,
      -1.138513494519088,
      -1.2769069631725143,
      -1.0958228190094859,
      -1.0904269498352617,
      -1.1033835715369225,
      -1.134579552772516,
      -1.1451901653658938,
      -1.1017923004402324,
      -1.1800865418797866,
      -1.2051782466057916,
      -1.123048297447924,
      -1.2076099712852861,
      -1.148161732754201,
      -1.1350409454327655,
      -1.1326756750229927,
      -1.1274011480195314,
      -1.1114002331699218,
      -1.167857001392004,
      -1.2553326934001907
    ],
    [
      -1.4218748822898624,
      -1.3407726687530275,
      -1.3213624297998685,
      -1.3421198873656839,
      -1.3275586929303425,
      -1.346803601127441,
      -1.4094678280496076,
      -1.3389014792909872,
      -1.4087598574364957,
      0.0,
      -1.3917061528362997,
      -1.5660444175596708,
      -1.3198169489609886,
      -1.2917572392307655,
      -1.4170523599835725,
      -1.3588907990102068,
      -1.3842845445524163,
      -1.3529350358167702,
      -1.4033414242651925,
      -1.3858456940410853,
      -1.3404397826312375,
      -1.4541337033494042,
      -1.342566596734467,
      -1.3565310465496232,
      -1.3532696005117932,
      -1.3679472931156247,
      -1.3347339656735975,
      -1.4239969669456987,
      -1.5156817607160167
    ],
    [
      -1.1828804837325062,
      -1.0599856077743388,
      -1.0312853964575528,
      -1.055277956422165,
      -1.1123362505056624,
      -1.0991270642789897,
      -1.088486777982198,
      -1.071113559574965,
      -1.0499489200839485,
      -1.093890194528858,
      0.0,
      -1.2459981652124075,
      -1.0398137734575228,
      -1.024585618021494,
      -1.1202094003863465,
      -1.054411232129032,
      -1.185250042126089,
      -1.0903986599461153,
      -1.0860044586714372,
      -1.1343416365911858,
      -1.0623466242867419,
      -1.1625601936125156,
      -1.1281869436586256,
      -1.0784025967628914,
      -1.1112177774494227,
      -1.164555147591091,
      -1.1233917348492404,
      -1.126902966077805,
      -1.1825610621354923
    ],
    [
      -1.3015919605414752,
      -1.2866405568884292,
      -1.285268015667807,
      -1.287744846691624,
      -1.247868190607396,
      -1.2903550115452516,
      -1.2608045345411325,
      -1.2412168826572707,
      -1.3028284330017903,
      -1.2662792466461867,
      -1.2816017198447458,
      0.0,
      -1.3017708771778502,
      -1.2845897603519252,
      -1.278738639643101,
      -1.2756258549418753,
      -1.298213229674518,
      -1.3032378631335255,
      -1.2836878396150433,
      -1.2806043136249563,
      -1.2419911524051663,
      -1.274858602613461,
      -1.2620824346358843,
      -1.3008617262776243,
      -1.2615352944171652,
      -1.3073558862153325,
      -1.257486318192154,
      -1.2806539927060392,
      -1.2771192531159938
    ],
    [
      -1.2392042619074575,
      -1.1390463255530634,
      -1.173967625986738,
      -1.0809285942409512,
      -1.1280523268996387,
      -1.1505400397059296,
      -1.170127446550655,
      -1.1334282986078827,
      -1.140124360708222,
      -1.1628025296590572,
      -1.1663221079633295,
      -1.3126353344411215,
      0.0,
      -1.1020191316919057,
      -1.2373467351647727,
      -1.1742632360965142,
      -1.2262153979932122,
      -1.1017925169413136,
      -1.1684129598513844,
      -1.220993870147539,
      -1.176426124690095,
      -1.2478408502365574,
      -1.1994320576663817,
      -1.148460028086922,
      -1.1926486798874516,
      -1.1818457099222035,
      -1.102010805703094,
      -1.224933415493038,
      -1.2795271348930064
    ],
    [
      -1.378006075729375,
      -1.2654573804519387,
      -1.2203257034840187,
      -1.2424781531977755,
      -1.2500241393268243,
      -1.2963695145866565,
      -1.3200038977187223,
      -1.2221156110087472,
      -1.2930799608858676,
      -1.2810511986667101,
      -1.2372770172998389,
      -1.4507636126332029,
      -1.2498851679098972,
      0.0,
      -1.309838445608691,
      -1.276206791059359,
      -1.3452508166469528,
      -1.2426414977516254,
      -1.3344175187671174,
      -1.307513942801648,
      -1.2534845562864863,
      -1.3139259730697936,
      -1.3128545546671315,
      -1.3073014373449248,
      -1.2968305883531581,
      -1.3190628702060665,
      -1.2583339247293088,
      -1.3663474870572851,
      -1.3773169316024856
    ],
    [
      -1.2455995628586243,
      -1.2477552786975965,
      -1.2047979606017256,
      -1.2339934937228818,
      -1.2834093635708939,
      -1.2850749404165964,
      -1.2547434308599765,
      -1.2246480727059152,
      -1.163915847506924,
      -1.2024747507134945,
      -1.236527202307226,
      -1.3272084962332833,
      -1.2368531466651922,
      -1.18428200111904,
      0.0,
      -1.2691937497264119,
      -1.2268051159635673,
      -1.2359185352659943,
      -1.279739978069252,
      -1.1759716170715628,
      -1.2106102860700454,
      -1.1717895608759163,
      -1.2365139764900392,
      -1.2460372653922425,
      -1.1433298346860832,
      -1.2005925116195098,
      -1.1517057192296714,
      -1.235797258733004,
      -1.2807753837568767
    ],
    [
      -1.245604971114138,
      -1.1519811070459838,
      -1.166536205479756,
      -1.1678113609116207,
      -1.1797308889040263,
      -1.1511888332456648,
      -1.1484065681781133,
      -1.182653394813256,
      -1.1948652843933818,
      -1.189939682174596,
      -1.183088362075415,
      -1.3590814296037481,
      -1.1647042720080745,
      -1.1978919804228136,
      -1.2696030087182517,
      0.0,
      -1.2589197887234775,
      -1.2226671532546627,
      -1.149770872267657,
      -1.1605471174040654,
      -1.170814597169898,
      -1.2476700222227544,
      -1.2359854363361455,
      -1.0747204616088246,
      -1.2194696454229386,
      -1.2760824093044218,
      -1.1648551828599512,
      -1.2451971719584793,
      -1.2923417152140124
    ],
    [
      -1.396497918918726,
      -1.3044632202534694,
      -1.1993552984393743,
      -1.2939237793014333,
      -1.2238850613822028,
      -1.2774223503840116,
      -1.3400965262733562,
      -1.317742997951208,
      -1.2885256964267964,
      -1.2841407913801253,
      -1.326943362380329,
      -1.4190430374785996,
      -1.2903925827361535,
      -1.2899114406682934,
      -1.3047241345053058,
      -1.3147371569205883,
      0.0,
      -1.28545837925242,
      -1.31136519070083,
      -1.2922112169999291,
      -1.2553765204967788,
      -1.3578001345501711,
      -1.2791290196993714,
      -1.2946477265410787,
      -1.2578574978953332,
      -1.2368339217747688,
      -1.2121524673777264,
      -1.2904039894128008,
      -1.3695602409686847
    ],
    [
      -1.333287150986223,
      -1.2481966522487098,
      -1.2444647143145122,
      -1.160336020813121,
      -1.2117051164449573,
      -1.2434414340741613,
      -1.2765503266215994,
      -1.2285889715228007,
      -1.2562510589892633,
      -1.2364162484659904,
      -1.3019602607403253,
      -1.3953376542756926,
      -1.1692417359150016,
      -1.1933614204425258,
      -1.270655387142378,
      -1.2880564562932901,
      -1.2781812899746747,
      0.0,
      -1.2259598652778056,
      -1.2182266308102425,
      -1.2412370405174984,
      -1.2922358660077413,
      -1.2798084731910477,
      -1.2901260637250678,
      -1.1686966265052303,
      -1.2739051202371732,
      -1.13993817994896,
      -1.3184385088599135,
      -1.3515788979623828
    ],
    [
      -1.2033516193617326,
      -1.1509990344200518,
      -1.0793592120552085,
      -1.1402281068440097,
      -1.1478444217011938,
      -1.157544990244997,
      -1.1687041306443502,
      -1.1005111842309045,
      -1.1717531016029799,
      -1.1640902956902417,
      -1.1357003041894747,
      -1.2892273622575974,
      -1.13447604403374,
      -1.1138410501967269,
      -1.2257847899104901,
      -1.1276189935870693,
      -1.2284665812994928,
      -1.1499936431369087,
      0.0,
      -1.1743761364444272,
      -1.1315817626214029,
      -1.22437847405545,
      -1.1948434085039263,
      -1.1270516960285695,
      -1.1700754497017327,
      -1.185071306349787,
      -1.1438562047247258,
      -1.1966850206751387,
      -1.2649150799586693
    ],
    [
      -1.4106875166700097,
      -1.4009935706774101,
      -1.389735244430102,
      -1.3779729297794878,
      -1.3852387449366217,
      -1.410429382650831,
      -1.388341514176185,
      -1.3759485609615527,
      -1.4236002661661045,
      -1.3530315079885615,
      -1.4105657532619424,
      -1.518135740822006,
      -1.4026814584090939,
      -1.359597459537995,
      -1.3309508655190931,
      -1.3774243738421728,
      -1.4196337795370595,
      -1.358534315571628,
      -1.3991974441045467,
      0.0,
      -1.3459347279192166,
      -1.3191179951088376,
      -1.3716915079394543,
      -1.3790832436967866,
      -1.3119200412522036,
      -1.3873497044666796,
      -1.3311444297381436,
      -1.376019739046276,
      -1.454029020876167
    ],
    [
      -1.6216871800290518,
      -1.552728449753248,
      -1.5350946460753554,
      -1.545222608732689,
      -1.5373793773413118,
      -1.5568296326451916,
      -1.5864976737691996,
      -1.5986033778580986,
      -1.578558358136516,
      -1.562592673126385,
      -1.6027483156919278,
      -1.7253911050514494,
      -1.5663051622778112,
      -1.5658722311549882,
      -1.7016829592778215,
      -1.5755229619994986,
      -1.6247993520464696,
      -1.5546010026425783,
      -1.5793129828025996,
      -1.6012517031791331,
      0.0,
      -1.6534958129517,
      -1.5949900414139273,
      -1.5738899335941818,
      -1.608542541897181,
      -1.6229617446417404,
      -1.5053244820387788,
      -1.6696307082598438,
      -1.6926303954651252
    ],
    [
      -1.4631108483888484,
      -1.4550130082095265,
      -1.4768203526827013,
      -1.4549575957713867,
      -1.4555206003852361,
      -1.4556690562002421,
      -1.4573244048625984,
      -1.4309985386745225,
      -1.4698194269576823,
      -1.4366689253266105,
      -1.4468171825298421,
      -1.5339272461144218,
      -1.4604440917668822,
      -1.4376166106244208,
      -1.401822002865278,
      -1.4647968560454838,
      -1.5153203759388285,
      -1.4539492086374939,
      -1.459799601554447,
      -1.347397154220308,
      -1.4366709649068488,
      0.0,
      -1.463372505677513,
      -1.4878153929813895,
      -1.4279258634043683,
      -1.4677943295152673,
      -1.4256462152837044,
      -1.4656722310008155,
      -1.4416305719066378
    ],
    [
      -1.2450038994305268,
      -1.1851731945894228,
      -1.1800131876288678,
      -1.188047002466171,
      -1.1582427641599438,
      -1.2034347371966396,
      -1.2250543668526872,
      -1.1616140492414901,
      -1.18993761806108,
      -1.1359187144011897,
      -1.18367358286236,
      -1.311295017613975,
      -1.1611882654780064,
      -1.128612581809751,
      -1.227269461448065,
      -1.1922593335734675,
      -1.2138664317644603,
      -1.1729059206262475,
      -1.235829609608924,
      -1.2116219761376488,
      -1.1486872164508561,
      -1.241293021321433,
      0.0,
      -1.2019756981099199,
      -1.121673631646191,
      -1.2180074507584666,
      -1.1226543411382481,
      -1.2012767736840593,
      -1.2385664256502926
    ],
    [
      -1.3744340871439849,
      -1.3495547191948825,
      -1.3210013518833563,
      -1.3442284729658633,
      -1.3473438060927883,
      -1.3214664556176292,
      -1.35334185092807,
      -1.3133462300346908,
      -1.3041360885712505,
      -1.3166112398078094,
      -1.3049958266894426,
      -1.4974904704158194,
      -1.331738148051269,
      -1.2847719734533842,
      -1.3581450790447744,
      -1.209184256976384,
      -1.3505097095767409,
      -1.3394923762256203,
      -1.2702860091952441,
      -1.3246460071159738,
      -1.2884991431223318,
      -1.3795246410505992,
      -1.3472609599570609,
      0.0,
      -1.3595834743521036,
      -1.3050285896077793,
      -1.3316333741351762,
      -1.331337870506316,
      -1.4311426078950478
    ],
    [
      -1.607243759839924,
      -1.5510107427367845,
      -1.5277091144847958,
      -1.5511715836901672,
      -1.5294806629757942,
      -1.5754334003749526,
      -1.6048714764214655,
      -1.555276619479265,
      -1.6190525389382207,
      -1.5209202489547309,
      -1.5799871465641167,
      -1.6509541732594806,
      -1.5634620441626887,
      -1.5450076896428553,
      -1.5689887813346393,
      -1.597386694328971,
      -1.5615039486701634,
      -1.4558402709348783,
      -1.5846628663690348,
      -1.4913298402388464,
      -1.5630345494982891,
      -1.5281792246743773,
      -1.5368541592972265,
      -1.5728270313783825,
      0.0,
      -1.634082370718722,
      -1.4987273498605045,
      -1.5529279499028898,
      -1.5788513528007644
    ],
    [
      -1.4141128747068816,
      -1.3340424569116935,
      -1.3203206557070704,
      -1.31371205339007,
      -1.2869698071141826,
      -1.3649570941270823,
      -1.3599093479738058,
      -1.3082666353780512,
      -1.2925698617393133,
      -1.3277956456952005,
      -1.3603792709538227,
      -1.483099845990385,
      -1.2966289696600186,
      -1.3100305087735424,
      -1.3214051451008155,
      -1.3520613487891675,
      -1.2749819116617467,
      -1.3342051941430468,
      -1.3773380244262838,
      -1.3754599577030202,
      -1.354407746374534,
      -1.399808173606927,
      -1.3529965629061225,
      -1.3267555003698501,
      -1.3470480207352165,
      0.0,
      -1.2888316656584478,
      -1.3966783038406805,
      -1.416161076893704
    ],
    [
      -1.6585394168826582,
      -1.5582174065110033,
      -1.5606070624675845,
      -1.486140732363154,
      -1.4932274204325633,
      -1.5550745478388097,
      -1.5885990244979775,
      -1.5775409036198522,
      -1.572650212684122,
      -1.5052770291080857,
      -1.6086429203163237,
      -1.7466452179556997,
      -1.5114372433632688,
      -1.5145612259511705,
      -1.567253222012211,
      -1.5851721172709778,
      -1.5321133860025613,
      -1.467977987590817,
      -1.5688039851315236,
      -1.5485081229676003,
      -1.540445780626556,
      -1.6250056711035403,
      -1.5428308202974488,
      -1.6138769434356364,
      -1.5002000467196157,
      -1.54255221547006,
      0.0,
      -1.644144248911327,
      -1.6669235257070676
    ],
    [
      -1.1815103316770539,
      -1.1458828072093297,
      -1.1220144696592782,
      -1.1182153086810072,
      -1.1464902747814962,
      -1.1412142324220647,
      -1.1689177119191436,
      -1.1154748022565846,
      -1.1145623990993567,
      -1.1241400278775868,
      -1.1428239677635295,
      -1.2123115546008578,
      -1.1267566774417097,
      -1.1462158253144792,
      -1.1538879765071828,
      -1.129829290516906,
      -1.1871156650512187,
      -1.1398132615523306,
      -1.1324657366800994,
      -1.122753492507793,
      -1.0964806904433508,
      -1.150263943086868,
      -1.1609722322560507,
      -1.10110175248496,
      -1.1165705479326664,
      -1.1691689679473014,
      -1.143234850854982,
      0.0,
      -1.1858443198105075
    ],
    [
      -1.199451654010386,
      -1.1538911686889866,
      -1.1910889976745824,
      -1.1623427790179361,
      -1.154333469035817,
      -1.1907496341944366,
      -1.2046342057877872,
      -1.1664103527287175,
      -1.1575571755795133,
      -1.1535279438306807,
      -1.1548386996699431,
      -1.1725040978119639,
      -1.1564963910929666,
      -1.146620448559159,
      -1.1843529837208506,
      -1.1433845247074155,
      -1.2007487883736483,
      -1.1842576910867975,
      -1.1727801079546119,
      -1.1521277097363463,
      -1.1667025286954558,
      -1.0677082919918832,
      -1.1461762720254551,
      -1.17091930441356,
      -1.1413572804211274,
      -1.2112964084775026,
      -1.1775855083485036,
      -1.1643422213551424,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.07152844272561731,
      0.07798138367581897,
      0.07493345078216151,
      0.06583110847381057,
      0.08313046929439238,
      0.0982785668174797,
      0.06809923078679869,
      0.07920883258856204,
      0.07532260450564099,
      0.07506466689775149,
      0.035504860238143365,
      0.07504649867470414,
      0.07894493901313426,
      0.06895334845072609,
      0.07089385890115585,
      0.06457932867608607,
      0.07193719813563071,
      0.06542221178881036,
      0.07929510096658987,
      0.06826516513220193,
      0.0763429638452533,
      0.07316365827223947,
      0.07562925745920657,
      0.07906464789067646,
      0.07106305134333102,
      0.07745169988226763,
      0.05746529789898425,
      0.03174103965637931
    ],
    [
      0.353423597384275,
      0.0,
      0.4621258603481395,
      0.5361485017829942,
      0.4062735699192952,
      0.45768580589807784,
      0.43375260975343344,
      0.46267563028413095,
      0.45653343888508224,
      0.4288365505178193,
      0.44019481679367045,
      0.23993263416963262,
      0.5008454037249694,
      0.4429153513576196,
      0.35896683098102566,
      0.44170330342395303,
      0.31164679685121954,
      0.44208493882610367,
      0.35740947923350586,
      0.4044060273789407,
      0.44190843153836323,
      0.33902698593206937,
      0.3698534825541353,
      0.41592660729288333,
      0.3863035190842321,
      0.39780929627564143,
      0.42457162425286676,
      0.3654486788575737,
      0.3002695505064754
    ],
    [
      0.33130147389901654,
      0.3617354345223258,
      0.0,
      0.4272168040271249,
      0.3952564638631568,
      0.36666742056821344,
      0.3818046514976834,
      0.38447958161573026,
      0.36465495035019413,
      0.37403114645604774,
      0.3845137819061524,
      0.2428901241154322,
      0.36045533187129886,
      0.3671715143357184,
      0.35757501857070895,
      0.3439098410768422,
      0.3790949457613546,
      0.3845223092866392,
      0.3554684718329757,
      0.34720760205571066,
      0.38713398512971375,
      0.306538398513889,
      0.3518658919182951,
      0.3226642865516529,
      0.4006879053424641,
      0.320115763889937,
      0.3669251783438996,
      0.3716801385316246,
      0.2545752541826569
    ],
    [
      0.352277175976498,
      0.4676797514616158,
      0.45461223922192606,
      0.0,
      0.4940397850296252,
      0.4930531836138985,
      0.4413641175430847,
      0.4584005348885585,
      0.493055992873239,
      0.4309927806123035,
      0.4094195009304853,
      0.23456240951712437,
      0.5448926748843537,
      0.46652937742556055,
      0.38888927547081353,
      0.4298854985484619,
      0.34162640191670635,
      0.5376647534155339,
      0.43338002354312,
      0.4028850675318232,
      0.4160627927325711,
      0.37338246721195967,
      0.38526859084474374,
      0.3995704447162909,
      0.43273102416481035,
      0.3555924249349287,
      0.5097578075094378,
      0.37305431312932824,
      0.33141670387360533
    ],
    [
      0.27590616354437825,
      0.3616087439939979,
      0.38393152295248045,
      0.3905918885812121,
      0.0,
      0.3449261910438097,
      0.3055480979623466,
      0.34293873474326575,
      0.3300078749746673,
      0.3544288587252944,
      0.3321715007497623,
      0.25030696373617567,
      0.37325842194652603,
      0.357725050568505,
      0.2948651759392418,
      0.31224765509445906,
      0.3518812647827301,
      0.39242442560754,
      0.32763026692341857,
      0.3208817079173065,
      0.38768903774354824,
      0.25635456884260277,
      0.3882664102846929,
      0.3173018257599274,
      0.3363828990169617,
      0.3347747266338599,
      0.43862183921687037,
      0.2856816547877732,
      0.26429505258116426
    ],
    [
      0.32561877844940645,
      0.4629235084159329,
      0.40502115603865496,
      0.5097359493761773,
      0.45846035422491704,
      0.0,
      0.39765562708593283,
      0.4928627307989988,
      0.44383536599888695,
      0.45681956213478214,
      0.4245717093083321,
      0.2298401431847612,
      0.4931000930968057,
      0.4580938020086922,
      0.37704268545372077,
      0.45743163413692023,
      0.41680117620391277,
      0.4571063370551003,
      0.4005907787698979,
      0.4015608132820119,
      0.4355510818693702,
      0.3639428115639425,
      0.39535592905316363,
      0.378521717247299,
      0.43257974733473503,
      0.3818267456905682,
      0.4648080347899979,
      0.3446708254242907,
      0.274626011165356
    ],
    [
      0.21789524845213304,
      0.3903382504645321,
      0.36820326765508593,
      0.40723570251425234,
      0.3183682753184933,
      0.35353953956632944,
      0.0,
      0.36202177320659135,
      0.3082500202861018,
      0.32134966805220766,
      0.35954689026174913,
      0.2778076744014013,
      0.369319415097465,
      0.374922739613766,
      0.2640289707227139,
      0.3755215440895565,
      0.28122724516948505,
      0.3294625692803512,
      0.35556477675755627,
      0.3219433206415565,
      0.35460054390139595,
      0.2646297481799733,
      0.2815801021498592,
      0.35563270321517937,
      0.29354383465398937,
      0.3052919835333221,
      0.3587384040863013,
      0.2857927945202836,
      0.2540388425481055
    ],
    [
      0.2622108016429272,
      0.387213557666805,
      0.4072725900914247,
      0.40137181780255116,
      0.41128151569794547,
      0.3947146027036177,
      0.37620657187323947,
      0.0,
      0.3676677411379885,
      0.38972545731184294,
      0.40014878988202685,
      0.23333902815111296,
      0.41733484118911535,
      0.4417321833148864,
      0.355593193311903,
      0.35857572115390934,
      0.3290844738762779,
      0.42658126677010433,
      0.3763279585989312,
      0.3573220622377167,
      0.367980921500211,
      0.32848911660371916,
      0.3629212974469378,
      0.3783751162668785,
      0.36893876604513687,
      0.381785813807322,
      0.4139055588984921,
      0.35411781369731576,
      0.2602739599376531
    ],
    [
      0.30637667807920943,
      0.41326868689655605,
      0.4027467676915799,
      0.4380383140289872,
      0.357418710689847,
      0.41559565234859264,
      0.3561849453755723,
      0.4306064910400278,
      0.0,
      0.36254775809085693,
      0.38840909569929294,
      0.25001562704586666,
      0.4310997712088951,
      0.4364956403831193,
      0.4235390186814585,
      0.39234303744586496,
      0.38173242485248715,
      0.4251302897781486,
      0.3468360483385944,
      0.3217443436125893,
      0.40387429277045706,
      0.3193126189330948,
      0.37876085746417987,
      0.3918816447856155,
      0.39424691519538824,
      0.39952144219884955,
      0.4155223570484592,
      0.35906558882637696,
      0.27158989681819024
    ],
    [
      0.35646972961698986,
      0.43757194315382475,
      0.4569821821069837,
      0.43622472454116834,
      0.45078591897650977,
      0.4315410107794113,
      0.36887678385724465,
      0.439443132615865,
      0.36958475447035655,
      0.0,
      0.3866384590705525,
      0.2123001943471814,
      0.45852766294586367,
      0.48658737267608676,
      0.36129225192327974,
      0.4194538128966454,
      0.3940600673544359,
      0.425409576090082,
      0.37500318764165974,
      0.39249891786576696,
      0.4379048292756147,
      0.32421090855744805,
      0.4357780151723851,
      0.421813565357229,
      0.425075011395059,
      0.41039731879122754,
      0.44361064623325475,
      0.35434764496115356,
      0.26266285119083554
    ],
    [
      0.36984708139403377,
      0.49274195735220117,
      0.5214421686689872,
      0.49744960870437493,
      0.44039131462087755,
      0.4536005008475503,
      0.4642407871443419,
      0.48161400555157496,
      0.5027786450425915,
      0.45883737059768204,
      0.0,
      0.3067293999141325,
      0.5129137916690172,
      0.5281419471050459,
      0.43251816474019344,
      0.498316332997508,
      0.367477523000451,
      0.4623289051804247,
      0.46672310645510273,
      0.41838592853535417,
      0.4903809408397981,
      0.39016737151402436,
      0.4245406214679144,
      0.47432496836364857,
      0.44150978767711724,
      0.38817241753544907,
      0.4293358302772996,
      0.42582459904873504,
      0.3701665029910477
    ],
    [
      0.24277702647120702,
      0.25772843012425306,
      0.2591009713448753,
      0.2566241403210583,
      0.29650079640528615,
      0.25401397546743065,
      0.2835644524715497,
      0.3031521043554115,
      0.24154055401089192,
      0.2780897403664955,
      0.2627672671679364,
      0.0,
      0.242598109834832,
      0.25977922666075703,
      0.26563034736958113,
      0.2687431320708069,
      0.2461557573381643,
      0.2411311238791567,
      0.2606811473976389,
      0.2637646733877259,
      0.3023778346075159,
      0.26951038439922126,
      0.2822865523767979,
      0.2435072607350579,
      0.28283369259551705,
      0.23701310079734972,
      0.28688266882052815,
      0.263714994306643,
      0.2672497338966884
    ],
    [
      0.3415329337872868,
      0.44169087014168085,
      0.40676956970800626,
      0.499808601453793,
      0.4526848687951055,
      0.4301971559888147,
      0.4106097491440892,
      0.4473088970868615,
      0.44061283498652215,
      0.4179346660356871,
      0.4144150877314148,
      0.2681018612536228,
      0.0,
      0.47871806400283856,
      0.3433904605299716,
      0.4064739595982301,
      0.354521797701532,
      0.4789446787534306,
      0.41232423584335987,
      0.35974332554720534,
      0.40431107100464914,
      0.3328963454581868,
      0.3813051380283625,
      0.4322771676078223,
      0.38808851580729264,
      0.39889148577254074,
      0.4787263899916503,
      0.3558037802017062,
      0.30121006080173784
    ],
    [
      0.32545160312736243,
      0.4380002984047988,
      0.4831319753727188,
      0.46097952565896194,
      0.45343353952991317,
      0.407088164270081,
      0.3834537811380152,
      0.4813420678479903,
      0.41037771797086986,
      0.42240648019002736,
      0.46618066155689863,
      0.25269406622353463,
      0.45357251094684026,
      0.0,
      0.3936192332480466,
      0.4272508877973784,
      0.3582068622097847,
      0.46081618110511213,
      0.36904016008962004,
      0.3959437360550895,
      0.44997312257025124,
      0.38953170578694385,
      0.39060312418960597,
      0.3961562415118127,
      0.40662709050357937,
      0.384394808650671,
      0.44512375412742866,
      0.33711019179945234,
      0.3261407472542519
    ],
    [
      0.3019905822579845,
      0.29983486641901225,
      0.34279218451488314,
      0.31359665139372694,
      0.2641807815457149,
      0.26251520470001233,
      0.2928467142566322,
      0.3229420724106935,
      0.38367429760968474,
      0.34511539440311423,
      0.31106294280938274,
      0.22038164888332545,
      0.31073699845141656,
      0.3633081439975687,
      0.0,
      0.2783963953901969,
      0.3207850291530414,
      0.3116716098506145,
      0.26785016704735676,
      0.3716185280450459,
      0.33697985904656336,
      0.3758005842406924,
      0.3110761686265695,
      0.3015528797243663,
      0.40426031043052557,
      0.346997633497099,
      0.3958844258869374,
      0.3117928863836048,
      0.26681476135973203
    ],
    [
      0.3644628071982283,
      0.4580866712663825,
      0.44353157283261035,
      0.4422564174007455,
      0.4303368894083399,
      0.45887894506670146,
      0.461661210134253,
      0.4274143834991102,
      0.41520249391898445,
      0.42012809613777025,
      0.42697941623695135,
      0.25098634870861813,
      0.4453635063042918,
      0.4121757978895526,
      0.34046476959411454,
      0.0,
      0.3511479895888887,
      0.38740062505770356,
      0.4602969060447093,
      0.4495206609083009,
      0.43925318114246825,
      0.3623977560896119,
      0.3740823419762207,
      0.5353473167035416,
      0.3905981328894277,
      0.3339853690079444,
      0.4452125954524151,
      0.3648706063538869,
      0.31772606309835383
    ],
    [
      0.25074193399923117,
      0.3427766326644879,
      0.447884554478583,
      0.35331607361652395,
      0.4233547915357545,
      0.36981750253394563,
      0.3071433266446011,
      0.3294968549667492,
      0.3587141564911609,
      0.363099061537832,
      0.3202964905376282,
      0.22819681543935766,
      0.35684727018180373,
      0.35732841224966383,
      0.3425157184126515,
      0.3325026959973689,
      0.0,
      0.36178147366553737,
      0.33587466221712736,
      0.35502863591802813,
      0.39186333242117843,
      0.28943971836778615,
      0.36811083321858584,
      0.35259212637687853,
      0.3893823550226241,
      0.41040593114318846,
      0.43508738554023085,
      0.3568358635051565,
      0.2776796119492726
    ],
    [
      0.2989555571474929,
      0.38404605588500607,
      0.38777799381920364,
      0.4719066873205948,
      0.4205375916887586,
      0.38880127405955456,
      0.35569238151211646,
      0.4036537366109152,
      0.3759916491444526,
      0.39582645966772545,
      0.3302824473933905,
      0.2369050538580233,
      0.4630009722187143,
      0.43888128769119006,
      0.36158732099133783,
      0.3441862518404257,
      0.35406141815904113,
      0.0,
      0.4062828428559102,
      0.41401607732347334,
      0.39100566761621747,
      0.34000684212597454,
      0.3524342349426681,
      0.3421166444086481,
      0.46354608162848554,
      0.3583375878965427,
      0.4923045281847558,
      0.3138041992738023,
      0.280663810171333
    ],
    [
      0.3174680231939424,
      0.36982060813562323,
      0.44146043050046657,
      0.3805915357116654,
      0.3729752208544812,
      0.363274652310678,
      0.35211551191132484,
      0.4203084583247705,
      0.3490665409526952,
      0.35672934686543334,
      0.3851193383662004,
      0.23159228029807766,
      0.386343598521935,
      0.4069785923589482,
      0.2950348526451849,
      0.39320064896860574,
      0.2923530612561822,
      0.37082599941876637,
      0.0,
      0.34644350611124786,
      0.38923787993427217,
      0.29644116850022506,
      0.3259762340517487,
      0.39376794652710556,
      0.3507441928539423,
      0.33574833620588795,
      0.3769634378309492,
      0.3241346218805363,
      0.2559045625970058
    ],
    [
      0.34932555235640006,
      0.3590194983489996,
      0.37027782459630765,
      0.38204013924692193,
      0.37477432408978806,
      0.3495836863755788,
      0.37167155485022474,
      0.38406450806485704,
      0.3364128028603053,
      0.4069815610378482,
      0.3494473157644673,
      0.24187732820440377,
      0.3573316106173159,
      0.4004156094884148,
      0.4290622035073166,
      0.382588695184237,
      0.34037928948935026,
      0.4014787534547817,
      0.36081562492186303,
      0.0,
      0.4140783411071931,
      0.4408950739175721,
      0.3883215610869555,
      0.38092982532962316,
      0.4480930277742061,
      0.37266336455973015,
      0.42886863928826613,
      0.3839933299801337,
      0.3059840481502427
    ],
    [
      0.377455733524662,
      0.4464144638004657,
      0.46404826747835837,
      0.45392030482102474,
      0.46176353621240196,
      0.4423132809085222,
      0.4126452397845142,
      0.4005395356956152,
      0.42058455541719786,
      0.4365502404273287,
      0.396394597861786,
      0.2737518085022643,
      0.4328377512759025,
      0.43327068239872557,
      0.2974599542758922,
      0.42361995155421517,
      0.3743435615072441,
      0.4445419109111355,
      0.4198299307511142,
      0.39789121037458064,
      0.0,
      0.34564710060201387,
      0.40415287213978646,
      0.42525297995953193,
      0.3906003716565327,
      0.3761811689119734,
      0.493818431514935,
      0.32951220529386993,
      0.3065125180885886
    ],
    [
      0.3349161183403033,
      0.34301395851962524,
      0.32120661404645046,
      0.343069370957765,
      0.3425063663439156,
      0.3423579105289096,
      0.34070256186655334,
      0.3670284280546292,
      0.32820753977146944,
      0.36135804140254124,
      0.3512097841993096,
      0.26409972061472997,
      0.3375828749622696,
      0.36041035610473093,
      0.3962049638638738,
      0.3332301106836679,
      0.28270659079032323,
      0.34407775809165786,
      0.33822736517470475,
      0.4506298125088437,
      0.361356001822303,
      0.0,
      0.33465446105163865,
      0.3102115737477622,
      0.3701011033247834,
      0.3302326372138844,
      0.37238075144544736,
      0.3323547357283363,
      0.35639639482251395
    ],
    [
      0.302140563907052,
      0.361971268748156,
      0.36713127570871107,
      0.3590974608714079,
      0.38890169917763506,
      0.3437097261409392,
      0.3220900964848916,
      0.3855304140960887,
      0.35720684527649893,
      0.4112257489363891,
      0.3634708804752189,
      0.23584944572360378,
      0.38595619785957247,
      0.41853188152782783,
      0.3198750018895138,
      0.35488512976411135,
      0.3332780315731185,
      0.3742385427113313,
      0.31131485372865475,
      0.33552248719993005,
      0.3984572468867227,
      0.3058514420161458,
      0.0,
      0.34516876522765894,
      0.4254708316913878,
      0.3291370125791122,
      0.4244901221993307,
      0.34586768965351955,
      0.3085780376872862
    ],
    [
      0.40791869513531465,
      0.43279806308441704,
      0.4613514303959432,
      0.43812430931343616,
      0.43500897618651124,
      0.4608863266616703,
      0.4290109313512296,
      0.46900655224460874,
      0.478216693708049,
      0.46574154247149013,
      0.4773569555898569,
      0.2848623118634801,
      0.4506146342280304,
      0.49758080882591527,
      0.42420770323452506,
      0.5731685253029155,
      0.4318430727025586,
      0.44286040605367916,
      0.5120667730840553,
      0.4577067751633257,
      0.49385363915696767,
      0.40282814122870025,
      0.43509182232223864,
      0.0,
      0.4227693079271959,
      0.47732419267152015,
      0.45071940814412326,
      0.4510149117729836,
      0.3512101743842517
    ],
    [
      0.30693533883518853,
      0.3631683559383281,
      0.38646998419031675,
      0.36300751498494543,
      0.3846984356993184,
      0.33874569830016,
      0.30930762225364705,
      0.3589024791958475,
      0.29512655973689195,
      0.3932588497203817,
      0.33419195211099595,
      0.263224925415632,
      0.35071705451242385,
      0.3691714090322573,
      0.34519031734047334,
      0.31679240434614164,
      0.3526751500049492,
      0.45833882774023427,
      0.32951623230607785,
      0.4228492584362662,
      0.3511445491768235,
      0.3859998740007353,
      0.37732493937788614,
      0.34135206729673007,
      0.0,
      0.2800967279563906,
      0.4154517488146081,
      0.36125114877222275,
      0.3353277458743482
    ],
    [
      0.31798131739230695,
      0.39805173518749504,
      0.4117735363921182,
      0.41838213870911867,
      0.44512438498500595,
      0.36713709797210625,
      0.3721848441253828,
      0.42382755672113737,
      0.43952433035987526,
      0.4042985464039881,
      0.37171492114536586,
      0.24899434610880355,
      0.43546522243916996,
      0.42206368332564614,
      0.41068904699837305,
      0.3800328433100211,
      0.4571122804374419,
      0.39788899795614174,
      0.35475616767290474,
      0.35663423439616837,
      0.3776864457246545,
      0.3322860184922616,
      0.3790976291930661,
      0.40533869172933845,
      0.3850461713639721,
      0.0,
      0.4432625264407408,
      0.3354158882585081,
      0.3159331152054845
    ],
    [
      0.32809711619848114,
      0.42841912657013603,
      0.4260294706135548,
      0.5004958007179854,
      0.493409112648576,
      0.4315619852423296,
      0.3980375085831618,
      0.4090956294612871,
      0.4139863203970173,
      0.4813595039730536,
      0.3779936127648156,
      0.23999131512543959,
      0.4751992897178705,
      0.4720753071299688,
      0.41938331106892823,
      0.40146441581016146,
      0.454523147078578,
      0.5186585454903223,
      0.41783254794961566,
      0.438128410113539,
      0.4461907524545834,
      0.36163086197759897,
      0.4438057127836905,
      0.37275958964550293,
      0.4864364863615236,
      0.44408431761107936,
      0.0,
      0.34249228416981237,
      0.3197130073740717
    ],
    [
      0.26445648021943957,
      0.3000840046871638,
      0.32395234223721525,
      0.3277515032154863,
      0.2994765371149972,
      0.3047525794744288,
      0.27704909997734983,
      0.3304920096399089,
      0.3314044127971367,
      0.32182678401890663,
      0.303142844132964,
      0.23365525729563563,
      0.31921013445478374,
      0.29975098658201427,
      0.2920788353893107,
      0.3161375213795874,
      0.25885114684527477,
      0.3061535503441628,
      0.31350107521639403,
      0.32321331938870035,
      0.3494861214531426,
      0.2957028688096255,
      0.2849945796404427,
      0.34486505941153345,
      0.32939626396382704,
      0.27679784394919205,
      0.30273196104151134,
      0.0,
      0.2601224920859859
    ],
    [
      0.30349436590378054,
      0.3490548512251799,
      0.311857022239584,
      0.3406032408962303,
      0.3486125508783495,
      0.31219638571972985,
      0.2983118141263792,
      0.3365356671854489,
      0.3453888443346531,
      0.34941807608348574,
      0.3481073202442233,
      0.3304419221022026,
      0.3464496288211998,
      0.35632557135500753,
      0.3185930361933158,
      0.35956149520675096,
      0.30219723154051814,
      0.31868832882736897,
      0.3301659119595546,
      0.35081831017782017,
      0.33624349121871067,
      0.4352377279222832,
      0.3567697478887113,
      0.3320267155006065,
      0.36158873949303905,
      0.2916496114366638,
      0.3253605115656628,
      0.338603798559024,
      0.0
    ]
  ],
  "row_avgs": [
    0.07107653152762694,
    0.4063814044217189,
    0.35686227392915926,
    0.4232873969115145,
    0.33616601873766133,
    0.4121769681843775,
    0.32537128029784773,
    0.368231876379214,
    0.3790680326902914,
    0.3991090169237184,
    0.44681791354415984,
    0.26498997139215635,
    0.4028319133129786,
    0.40602322282632286,
    0.3191592757976964,
    0.40749174535393307,
    0.3502897935940334,
    0.37737902340841983,
    0.35287930668171075,
    0.37540625334476097,
    0.4029233630589351,
    0.3446583537851051,
    0.3541053107050649,
    0.4469693958646069,
    0.353222756120365,
    0.38598941851594987,
    0.41938766032259583,
    0.30325134338450427,
    0.3369393542359101
  ],
  "col_avgs": [
    0.3174081599083762,
    0.3793067869930221,
    0.3927452199614639,
    0.400875649241157,
    0.3887995507112261,
    0.3722244974423387,
    0.3572146842687956,
    0.3901351143211955,
    0.3727434559411438,
    0.38157999631014194,
    0.36395760884244927,
    0.24352983980148998,
    0.39595075970204946,
    0.3995009192293987,
    0.3456518218142214,
    0.37116133228464643,
    0.33872689520789784,
    0.3919339243834569,
    0.3593118897908655,
    0.3663429947528805,
    0.38660180570633795,
    0.3323036276297698,
    0.3584801003401258,
    0.36381660673069044,
    0.3816659547531583,
    0.3475104326605431,
    0.40558993810102384,
    0.3366329459134514,
    0.2867436625090221
  ],
  "combined_avgs": [
    0.19424234571800156,
    0.3928440957073705,
    0.3748037469453116,
    0.4120815230763357,
    0.36248278472444373,
    0.3922007328133581,
    0.34129298228332167,
    0.3791834953502048,
    0.37590574431571755,
    0.3903445066169302,
    0.40538776119330455,
    0.25425990559682315,
    0.399391336507514,
    0.40276207102786077,
    0.33240554880595885,
    0.38932653881928975,
    0.3445083444009656,
    0.3846564738959384,
    0.3560955982362881,
    0.37087462404882077,
    0.39476258438263656,
    0.3384809907074374,
    0.35629270552259534,
    0.40539300129764866,
    0.3674443554367617,
    0.3667499255882465,
    0.4124887992118098,
    0.31994214464897786,
    0.3118415083724661
  ],
  "gppm": [
    609.2780569191062,
    662.7417014462898,
    650.633391256912,
    648.279153308623,
    648.4341897122475,
    663.1354175491687,
    667.1352548778801,
    651.2861448348968,
    663.9309233720332,
    658.3068317100225,
    668.8400258705076,
    722.6763086696827,
    651.0318845375648,
    648.3994768397015,
    675.2405873753397,
    664.9446061121862,
    680.5317100412286,
    653.6341743295461,
    671.2016253492786,
    662.2572420682571,
    652.0871092359114,
    678.9135384752353,
    672.1386602090726,
    666.361683418712,
    654.1930430619612,
    674.6113817123935,
    643.938573866487,
    679.9343495134018,
    709.2978322005221
  ],
  "gppm_normalized": [
    1.4248601591405072,
    1.390834127445956,
    1.3602826383709847,
    1.3630106148785202,
    1.348368056090132,
    1.3887032082992206,
    1.411882638177535,
    1.358905653244095,
    1.3891711095734232,
    1.3712625148495727,
    1.3940824887213892,
    1.5337574344376244,
    1.3579049555112823,
    1.3517730088891546,
    1.414897841099005,
    1.3973939081574376,
    1.4221462810292316,
    1.3687274479784242,
    1.4028585933335513,
    1.3814050165081078,
    1.3577513854147227,
    1.4237042521545198,
    1.4044666300622963,
    1.3909933010932214,
    1.35796808846852,
    1.4187584659844035,
    1.3358979875597812,
    1.4232392854855713,
    1.485204328642297
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354
  ],
  "response_lengths": [
    5735,
    2481,
    2632,
    2661,
    3049,
    2650,
    2683,
    2803,
    2447,
    2217,
    2296,
    2681,
    2831,
    2743,
    2587,
    2294,
    2199,
    2532,
    2397,
    2691,
    2403,
    2473,
    2309,
    2278,
    2442,
    2154,
    2302,
    2738,
    1970
  ]
}