{
  "example_idx": 23,
  "reference": "Published as a conference paper at ICLR 2023\n\nBSTT: A BAYESIAN SPATIAL-TEMPORAL TRANSFORMER FOR SLEEP STAGING\n\nYuchen Liu Institute of Automation, CAS University of Chinese Academy of Sciences Institute of Computing Technology, CAS yuchen.liu.eric@outlook.com\n\nZiyu Jia∗ Institute of Automation, CAS University of Chinese Academy of Sciences jia.ziyu@outlook.com\n\nABSTRACT\n\nSleep staging is helpful in assessing sleep quality and diagnosing sleep disorders. However, how to adequately capture the temporal and spatial relations of the brain during sleep remains a challenge. In particular, existing methods cannot adaptively infer spatial-temporal relations of the brain under different sleep stages. In this paper, we propose a novel Bayesian spatial-temporal relation inference neural network, named Bayesian spatial-temporal transformer (BSTT), for sleep staging. Our model is able to adaptively infer brain spatial-temporal relations during sleep for spatial-temporal feature modeling through a well-designed Bayesian relation inference component. Meanwhile, our model also includes a spatial transformer for extracting brain spatial features and a temporal transformer for capturing temporal features. Experiments show that our BSTT outperforms state-ofthe-art baselines on ISRUC and MASS datasets. In addition, the visual analysis shows that the spatial-temporal relations obtained by BSTT inference have certain interpretability for sleep staging.\n\n1\n\nINTRODUCTION\n\nSleep staging is essential for assessing sleep quality and diagnosing sleep disorders. Sleep specialists typically classify sleep stages based on the AASM sleep standard and polysomnography (PSG) recordings to aid in diagnosis. The AASM standard not only provides criteria for determining each sleep period, but also documents conversion rules between different sleep stages, which is known as sleep transition rules, to help sleep specialists identify sleep stages when sleep transitions occur. However, artificial sleep staging takes a long time, and the classification results are greatly affected by professional level and subjectivity (Supratak et al., 2017). Therefore, automatic classification methods are applied into sleep staging to improve efficiency.\n\nTraditional machine learning methods use artificially designed features for sleep staging, which improves the efficiency of staging to a certain extent (Fraiwan et al., 2012). However, the accuracy of traditional machine learning methods relies heavily on feature engineering and feature selection, which still requires a lot of expert knowledge. To address the above problems, deep learning methods have been applied to sleep staging and achieved satisfactory classification performance (Phan et al., 2019; Jia et al., 2022a;b). Most of the early deep learning methods focus on the temporal information of the sleep data, utilizing convolutional neural networks (CNN) and recurrent neural networks (RNN) to capture temporal features for sleep staging (Jain & Ganesan, 2021; Perslev et al., 2019). In addition, some studies have shown that the spatial topology of the brain behave differently in different sleep stages (Khanal, 2019), which means that both the temporal and spatial relations of the brain are both important during sleep. Therefore, some researchers try to use the spatial and temporal characteristics of the brain for sleep staging (Jia et al., 2020b; Phan et al., 2022; Jia et al., 2020a). Although the above methods achieve good classification performance, it is challenging to model spatial and temporal relations. Specifically, for the modeling of temporal relations, some approaches attempt to capture sleep transition rules in sleep to serve the identification of specific sleep stages. However, it is difficult for these methods to explicitly demonstrate the relation of\n\n∗Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ndifferent sleep time slices in accordance with the AASM sleep standard. Besides, for the modeling of spatial relations, spatial convolution operation is employed to extract the spatial features of the brain, which is insufficient that it may ignore the spatial topology of the brain by most methods (Zhou et al., 2021a; Perslev et al., 2019). A few researches utilize spatial topology and temporal relation information of brain for sleep staging by graph convolutional networks, but the constructed brain networks still lack interpretability to a certain extent (Jia et al., 2020b).\n\nTo address the above challenges, we propose a novel model called Bayesian spatial-temporal transformer (BSTT) for sleep staging. The proposed model integrates the transformer and Bayesian relation inference in a unified framework. Specifically, we design the spatial-temporal transformer architecture, which can capture the temporal and spatial features of the brain. Besides, we propose the Bayesian relational inference component which comes in two forms, Bayesian temporal relation inference and Bayesian spatial relation inference. Wherefore, it can infer the spatial-temporal relations of objects and generate the relation intensity graphs. Specifically, the main contributions of our BSTT are summarized as follows:\n\n• We design Bayesian relational inference component which can adaptively infer spatialtemporal relations of brain during sleep in the service of capturing spatial-temporal relations.\n\n• We apply the spatial-temporal transformer architecture to simultaneously model spatialtemporal relations. It can effectively capture the spatial-temporal features of the brain and enhance the model’s ability to model spatial-temporal relations.\n\n• Experimental results show that the proposed BSTT achieves the state-of-the-art in multiple sleep staging datasets. The visual analysis shows that our model has a certain degree of interpretability for sleep staging.\n\n2 RELATED WORK\n\nIdentifying sleep stages plays an important role in diagnosing and treating sleep disorders. Earlier, the support vector machine (SVM) and random forest (RF) are used for sleep staging (Fraiwan et al., 2012). However, these methods need hand-crafted features, which require a lot of prior knowledge. Currently, deep learning methods have become the primary method for sleep staging.\n\nEarly deep learning methods extract temporal features of sleep signals for classification. The earliest methods are based on the CNN models (Tsinalis et al., 2016; Chambon et al., 2018). For example, Chambon et al. propose a convolutional neural network that can extract temporal-invariant features from sleep signals (Chambon et al., 2018). Furthermore, Eldele et al. develope a multi-resolution CNN with adaptive feature recalibration to extract representative features (Eldele et al., 2021). In addition, RNN models have been gradually used for sleep staging (Phan et al., 2019; Perslev et al., 2019; Phan et al., 2018). For example, Phan et al. propose a deep bidirectional RNN model with attention mechanism for single-channel EEG (Phan et al., 2018). They then design an end-to-end hierarchical RNN architecture for capturing different levels of EEG signal features (Phan et al., 2019). Some studies combine CNN with RNN (Supratak & Guo, 2020; Guillot & Thorey, 2021; Dong et al., 2017). For example, Suratak et al. propose a hybrid model combining CNN and RNN to extract rich temporal features (Supratak et al., 2017). In addition, Phan et al. introduce transformer into the sleep staging task to capture the temporal context features of sleep signals (Phan et al., 2022). Jia et al. design a fully convolutional model to capture the typical waveform of sleep signals (Jia et al., 2021b).\n\nFurther, several studies have shown the importance of brain spatial relations for sleep staging (Khanal, 2019; Sakkalis, 2011). Some researchers try to model the spatial-temporal characteristics of sleep data. For example, Jia et al. propose an adaptive deep learning model for sleep staging. The proposed spatial-temporal graph convolutional network is used to extract spatial features and capture transition rules (Jia et al., 2020b). They also propose a multi-view spatial-temporal graph convolutional network based on domain generalization, which models the multi-view-based spatial characteristics of the brain (Jia et al., 2021a).\n\nAlthough the above models achieve good classification performance, these models do not adequately model spatial-temporal properties or effectively reason and capture spatial-temporal rela-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ntions. Therefore, our method attempts to model spatial-temporal relations using Bayesian inference, combined with state-of-the-art transformer architectures for sleep staging.\n\n3 PRELIMINARIES\n\nThe proposed model processes data from successive T sleep epochs and predicts the label of the epoch in the middle. Each sleep epoch is defined as x ∈ RC×N , where C represents the number of channels of the sleep epoch (i.e. the EEG channel in this work. Since EEG signals from different channels are extracted from different regions of the brain, spatial relations are contained among these channels.) and N represents the number of sampling points in a sleep epoch. The input sequence of sleep epochs is defined as x = {x1, x2, . . . , xT }, where xi denotes a sleep epoch (i ∈ [1, 2, . . . , T ]) and T is the number of sleep epochs.\n\nThe sleep staging problem is defined as: learning an artificial neural network F based on Bayesian spatial-temporal transformer, which can infer the spatial-temporal relations of the input sleep epoch sequence x and map it to the corresponding sleep stage (cid:98)Y , where (cid:98)Y is the classification result of the middle epoch xm. According to the AASM standard, each (cid:98)Y ∈ {0, 1, 2, 3, 4} is matched to five sleep stages W, N1, N2, N3, and REM, respectively.\n\n4 BAYESIAN SPATIAL-TEMPORAL TRANSFORMER\n\nWe propose a novel model named Bayesian spatial-temporal transformer (BSTT) for sleep staging. The core ideas of our model are summarized as follows:\n\n• Infer the spatial-temporal relations of the brain based on Bayesian inference method.\n\n• Design the Bayesian transformer architecture while capturing the spatial-temporal features\n\nof the brain.\n\n• Integrate Bayesian relational inference components and transformer architecture into a uni-\n\nfied framework which can stage sleep period effectively.\n\nThe overall model is carefully designed to accurately classify different sleep stages.\n\n4.1 ARCHITECTURE\n\nThe overall architecture of the proposed BSTT is shown in Figure 1. The EEG signals are first encoded by the embedding layer. The spatial-temporal relations are then inferred and modeled by Bayesian spatial-temporal transformer module. Specifically, the Bayesian spatial-temporal transformer includes a Bayesian spatial transformer and a Bayesian temporal transformer. The Bayesian spatial transformer can reason about spatial relations in the brain and capture spatial features. The Bayesian temporal transformer can reason about the temporal relations of consecutive sleep epochs and capture temporal features. Finally, the predictions of different sleep stages are performed by the classification layer.\n\n4.1.1 BAYESIAN RELATION INFERENCE\n\nCapturing the spatial-temporal relations of brain signals during sleep can better serve sleep staging task. However, due to the difficulty in inferring the spatial-temporal relations of sleep, current researches are insufficient for modeling the spatial-temporal relations. Inspired by deep graph random process (DGP) proposed in recent research (Huang et al., 2020), we propose the Bayesian relation inference method. Bayesian relational inference is the core component of our model, which can infer relations between each pair of object nodes and build relation intensity graphs. In this article, an object node represents the embedding of an EEG channel or the embedding of a certain time slice. The construction of the relation intensity graph is mainly divided into the following steps:\n\nStep 1: Edge embedding initialization. The input of the Bayesian relational inference is the node embeddings of the object. Building a sleep relation intensity graph starts with generating edge embeddings. Specifically, the node embeddings of the object are spliced and generated into edge\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: The overall architecture of the proposed Bayesian spatial-temporal transformer for sleep staging. BSTT includes two Bayesian transformer modules, a spatial Bayesian transformer and a temporal Bayesian transformer. For each transformer module, the input features are passed through the position embedding and layernorm layer. Then the multi-head Bayesian relation inference component infers the object’s spatial or temporal relation and captures the spatial-temporal features. The residual connection is used to prevent overfitting and gradient disappearance. Here, ⊕ means add.\n\nembeddings. We first apply a linear neural network fθ to generate edge embeddings:\n\n(1) where En ∈ RB×Nv×V represents the input n node objects, En[i, j] ∈ RB×Ne×2V represents the splicing of two node embeddings, and Ee ∈ RB×Ne×E represents the generated edge embeddings.\n\n(i, j ∈ [1, n] && i ! = j)\n\nEe = fθ (En [i, j])\n\nStep 2: Edge embedding coupling. Coupling is one of the key steps in relation inference, the purpose is to obtain a summary graph of the relation between object nodes (Huang et al., 2020). We assume that the edges of the summary graph ̈Mi,j are a summary of an n → ∞ of λ → 0 Binomial distributions, which means ̈mi,j ∼ B(n, λ), due to the uncertainty of spatial-temporal relations of brain. Drawing from the Virtual Recurrent Neural Network (VRNN) model, the parameters of the approximate posteriors are estimated using a Recurrent Neural Network (RNN) to encode features. However, in contrast to VRNN, Bayesian relation inference component contains an approximate posterior q( ̈m|Ee), whose inference and sampling cannot be solved in a computationally feasible manner due to its infinite n. By De MoivreLaplace theorem (Sheynin, 1977) and DGP (Huang et al., 2020), we can subject these edge embeddings to a coupling transformation as follows:\n\nni,j = ζ (cid:0) Lmean (cid:101)σi,j = ζ (cid:0) Lstd 1 + 2ni,j ̃σ 2 i,j −\n\n(cid:0)Eei,j (cid:0)Eei,j (cid:113)\n\n(cid:1) (cid:1) + ε (cid:1) (cid:1)\n\n1 + 4n 2\n\ni,j ̃σ 4\n\ni,j\n\nmi,j =\n\n2\n\n(2)\n\n(3)\n\n(4)\n\nwhere ζ(·) is softplus function, Ee is the edge embedding generated in the first step, ε is a very small constant, Lmean(·) and Lstd(·) are implemented by neural networks for estimating the mean and standard deviation respectively, mi,j ∈ M is the approximation of Binomial edge variable in the summary graph, and M ∈ RB×Ne is the approximation of summary graph which strengthens the representation of real spatial or temporal relations.\n\nStep 3: Sleep relation intensity calculation. The final step is to strengthen edge information and generate a relation intensity graph for downstream tasks. The relation intensities of brain temporalspatial network during sleep is sparse based on existing research (Razi et al., 2017). Therefore,\n\n4\n\nEEG SignalsEmbedding LayerClassification LayerBayesian temporal transformerPosition EmbeddingLayerNormLayerNormMulti-head Bayesian temporal relational inference Temporal relation intensity graph based featuresLinearReLU,DropoutLinearResidualsBayesian spatial transformerPosition EmbeddingLayerNormLayerNormMulti-head Bayesian spatial relational inference Spatial relation intensity graph based featuresLinearReLU,DropoutLinearResidualsPublished as a conference paper at ICLR 2023\n\ngenerating a sparse graph based on edge embedding can not only highlight the representation of key relations, but also be more in line with the actual situation. We employ the Gaussian graph transformation approach which produces a sparse sleep relation intensity graph G. The specific calculation is defined as follows:\n\nsi,j = (cid:0) m mean\n\ni , j\n\n(cid:101)αi,j = (cid:0) mstd (cid:1) × (cid:101)αi,j + (cid:0)\n\ni,j\n\n(cid:1) × εi,j + mi,j\n\n(cid:101)αstd\n\ni,j\n\n(cid:1) × (cid:0)\n\n(cid:101)σ mean\n\ni , j\n\n(cid:1) × ε′\n\ni,j\n\n(5)\n\n(6)\n\n ̄αi,j = si,j × (cid:101)αi,j αi,j = ζ (L ( ̄αi,j) ) where mi,j ∈ M is the approximation of the edges of the summary graph obtained in Step 2, ε and ε′ are the standard Gaussian random variable of the same dimension as M , (cid:101)α ∈ RB×Ne is the Gaussian edge representation, S ∈ RB×Ne is the task-related Gaussian variable, ̃σi,j is calculated in Eq.(3), std is the standard deviation, mean is the mean value, ̄α is the Gaussian transformation map, α ∈ RB×Ne is the final sleep relation intensity graph, ζ(·) is softplus function, and L(·) is linear function. Afterwards, we utilize an attention mechanism-based method to convert the node embeddings into feature embeddings based on the sleep relation intensity graph as the output of Bayesian relation inference. The specific calculation is as follows:\n\n(8)\n\n(7)\n\nwhere fGAL(·) is the graph attention layer, En is the node embeddings of the input object, α is the sleep relation intensity graph, and Eout ∈ RB×Nv×Vout is the output node embeddings.\n\nEout = fGAL (En, α)\n\n(9)\n\n4.1.2 LEARNING OF BAYESIAN RELATION INFERENCE\n\nWe adopt variational inference to jointly optimise Baysian relation inference component. Inspired by VRNN (Chung et al., 2015), we can use the evidence lower bound (ELBO) for joint learning and inference. The details of how variational inference fits into our model are shown in the Appendix 3. Specifically, we use two random variables need to be optimised to describe the same random process data. The resulting objective is to maximize the ELBO:\n\n(cid:16)\n\n(cid:16) ̃A, S | X0:i\n\n(cid:17)\n\nq\n\n∥p\n\n(cid:16) ̃A, S | X0:i\n\n(cid:17)(cid:17)\n\nKL\n\n− E ̃A,S\n\n(cid:104)\n\nlog P\n\n(cid:16)\n\nYi | Xi, ̃A, S\n\n(cid:17)(cid:105) (cid:27)\n\n(10)\n\nM (cid:88)\n\n(cid:26)\n\ni=1\n\nwhere S is the task-related Gaussian variable, ̃A is the Gaussian graph embedding, q\n\n(cid:16) ̃A, S | X0:i\n\n(cid:17)\n\nis the prior distribution, and p are affected by ̃A in Eq.(6), the KL term can be further written as:\n\n(cid:16) ̃A, S | X0:i\n\n(cid:17)\n\nis the posterior distribution. Since every variable in S\n\n(cid:88)\n\n(cid:40)\n\n(cid:18)\n\nKL\n\nB( n, ̃λi,j )∥B\n\n(cid:17) (cid:19)\n\n(cid:16)\n\nn, ̃λ(0)\n\ni,j\n\n( i, j )∈ ̃E\n\n(cid:20)\n\n(cid:18)\n\nKL\n\n+E ̃αi,j\n\nN ( ̃αi,j ∗ μi,j, ̃αi,j ∗ σ2\n\ni,j∥N\n\n(cid:16)\n\n ̃αi,j ∗ μ(0)\n\ni,j , ̃αi,j ∗ σ(0) 2\n\ni,j\n\n(cid:17)(cid:19)(cid:21)(cid:41)\n\n(11)\n\nObviously the second term can be calculated, while the first term is tough to calculate because n → ∞. According to Theorem 2 provided in DGP (Huang et al., 2020), we can convert it into an easy-to-solve value to approximate the calculation.\n\n4.1.3 BAYESIAN TRANSFORMER MODULE\n\nTransformer shows convincing results in various sequence modeling tasks (Li et al., 2021; Luo et al., 2021; Zhou et al., 2021b). However, traditional transformer does not have the ability to reason the relation between each pair of object nodes. This results in a lack of interpretability of the attention graph generated by transformer. Besides, the accuracy of traditional transformer is not good enough under some medical scenarios. The Bayesian relation inference component we proposed can infer\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nspatial-temporal relations efficiently. Hence, we integrate the Bayesian relation inference component mentioned in Section 4.1.1 with the transformer in a unified framework to simultaneously reason and model the spatial-temporal features of sleep EEG data.\n\nBayesian spatial transformer. To better construct the spatial functional connectivity of the brain and capture spatial features, we design the Bayesian spatial transformer. It contains two components: a Bayesian spatial relation inference component and a position feed-forward network, of which the core component is a Bayesian relation inference component. Specifically, the input of the Bayesian spatial transformer S is the embeddings of n spatial nodes. First we add position encoding to the input to introduce position information:\n\n ̃S = S + Pep (12) where S ∈ R(B×Nt)×Ns×V is the input spatial node embedding, Pep ∈ R(B×Nt)×Ns×V is the position encoding matrix, and ̃S ∈ R(B×Nt)×Ns×V is the position-encoded spatial node embeddings. For the position encoding matrix, we follow the groundbreaking work (Vaswani et al., 2017) and use the sine and cosine functions to calculate.\n\nWe design a multi-head spatial Bayesian relation inference component which can reson about spatial relations to improve the representation learning ability of the model. The details of Bayesian relation inference component have been described in Section 4.1.1. The node embeddings with positional encoding are encoded as embeddings with spatial features after passing through the multi-head spatial Bayesian relation inference component and the feed-forward neural network layer:\n\n ̃S′ = fFNN\n\n(cid:16)\n\nfBSRI\n\n(cid:17)(cid:17)\n\n(cid:16) ̃S\n\n(13)\n\nwhere ̃S is the input node embedding, fBSRI(·) is the Bayesian spatial relation inference module which inferences the spatial relation, fFNN(·) is the feed-forward neural network layer, and ̃S′ ∈ RB×Nt×Vs is the spatial relation intensity graph based features.\n\nBayesian temporal transformer. Similar to the Bayesian spatial transformer, in order to better capture the sleep transition rules, we design a Bayesian temporal transformer module to reason and model temporal features. The calculations of the temporal relation intensity graph based features are as follows:\n\n(cid:101)T = ̃S′ + Pep (cid:16) fBTRI\n\n ̃T ′ = fFNN\n\n(cid:17)(cid:17)\n\n(14)\n\n(15)\n\n(cid:16) ̃T\n\nwhere fBTRI(·) is the Bayesian temporal relation inference component, fFNN(·) is the feed-forward neural network layer, and ̃T ′ ∈ RB×Nst is the temporal relation intensity graph based features. The classification results are generated by ̃T ′ after passing through a linear classification layer fC:\n\n(cid:98)Y = fC\n\n(cid:16) ̃T ′(cid:17)\n\n(16)\n\nwhere (cid:98)Y is the classification result of BSTT.\n\n5 EXPERIMENTS\n\nTo verify the effectiveness of the Bayesian spatial-temporal transformer, we evaluate it on the Institute of Systems and Robotics, University of Coimbra (ISRUC) and Montreal Archives of Sleep Studies-SS3 (MASS-SS3) dataset.\n\n5.1 DATASET\n\nISRUC dataset contains the PSG recordings from 100 adult subjects. Each PSG recording contains 6 EEG channels, 6 EOG channels, 3 EMG channels, and 1 ECG channel. MASS-SS3 dataset contains the PSG recordings from 62 adult subjects. Each PSG recording contains 20 EEG channels, 2 EOG channels, 3 EMG channels, and 1 ECG channel. The recordings are divided into time slices according to a sleep epoch every 30s. Sleep spacialists divide these time slices into five distinct sleep stages (W, N1, N2, N3, and REM) according to the AASM standard. There are also motion artifacts at the beginning and end of each subject’s recording which are marked as unknown. We follow the previous study and remove these recordings (Supratak et al., 2017).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n5.2 EXPERIMENT SETTINGS\n\nWe evaluate our model using k-fold cross-subject validation to ensure that the experiment results are correct and reliable. We set k = 5 in order to test all recordings efficiently. For optimizer, the Adam and Adadelta optimizer are deployed in MASS-SS3 and ISRUC dataset.\n\nWe use multi-channel EEG data for sleep staging to better capture brain network structure. Specifically, on the ISRUC dataset, we use all 6 EEG channels for experiments. In the MASS-SS3 dataset we use 19 channels of EEG signals. To comprehensively evaluate the Bayesian spatial-temporal transformer model and all baseline methods, we use accuracy (ACC), F1 Score, and KAPPA to evaluate the models. Specific information of the evaluation indicators and baseline models are shown in Appendix 2.\n\n5.3 EXPERIMENT ANALYSIS\n\nTable 1 and 2 indicate that the proposed model achieves the best performance compared to other baseline methods on both datasets. Specifically, the MCNN and MMCNN utilize the CNN model to automatically extract sleep features, while RNN based methods such as DeepSleepNet and TinySleepNet focus on the temporal context in sleep data, and model the multi-level temporal characteristics of the sleep process for sleep staging. Further, GraphSleepNet and ST-Transformer simultaneously model the spatial-temporal relations during sleep and achieve satisfactory results. However, GraphSleepNet and ST-Transformer cannot adequately reason the spatial-temporal relations, which limits the classification performance to a certain extent. Our Bayesian ST-Transformer uses the multi-head Bayesian relation inference component to infer spatial-temporal relations to better model spatial and temporal relations. Therefore, the proposed model achieves best classification performance on different datasets.\n\nTable 1: Comparison of Bayesian spatial-temporal transformer and baselines on ISRUC dataset\n\nMethod MCNN MMCNN MLP+LSTM DeepSleepNet TinySleepNet U-Time GraphSleepNet ST-Transformer BSTT (Our)\n\nACC(%) F1 Scroe(%) KAPPA(%)\n\n78.23 76.83 76.08 75.71 76.92 75.52 80.18 80.35 81.96∗\n\n76.37 78.93 72.87 73.16 75.15 71.04 78.19 78.05 80.30∗\n\n71.79 74.17 69.16 68.82 70.26 67.92 74.54 74.71 76.78∗\n\n∗indicates the significant differences between our model and other models (p < 0.05).\n\nTable 2: Comparison of Bayesian spatial-temporal transformer and baselines on MASS dataset\n\nMethod MCNN MMCNN MLP+LSTM DeepSleepNet TinySleepNet U-Time GraphSleepNet ST-Transformer BSTT (Our)\n\nACC(%) F1 Score(%) KAPPA(%)\n\n86.31 80.15 86.31 85.92 83.30 85.23 88.36 88.64 89.50∗\n\n80.79 71.03 81.55 79.81 77.40 78.36 83.25 84.53 85.00∗\n\n81.26 69.42 80.12 79.09 79.95 77.94 83.30 83.16 84.37∗\n\n∗indicates the significant differences between our model and other models (p < 0.05).\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n5.4 ABLATION EXPERIMENTS\n\nTo verify the effectiveness of each component in the Bayesian spatial-temporal transformer, we conduct ablation experiments to determine the impact of each module on the model’s performance. Specifically, we design three variants of the Bayesian spatial-temporal transformer, including:\n\n• Bayesian Spatial Transformer (BST), which removes the Bayesian temporal transformer module to determine the impact of modeling temporal relations on model performance.\n\n• Bayesian Temporal Transformer (BTT), which removes the Bayesian spatial transformer\n\nmodule to determine the impact of modeling spatial relations on model performance.\n\n• Spatial-Temporal Transformer (STT), which removes the relational inference component\n\nto determine the impact of Bayesian relational inference on model performance.\n\nFigure 2 demonstrates that the performance of the variant models degrades after removing certain component or module. Among them, the removal of the relational inference component has the greatest impact on performance, which shows the importance of introducing relational inference to the sleep staging task. In addition, only modeling the spatial relation or temporal relation of the data also lead to a decrease in performance. It can be seen that modeling the spatial and temporal relation is helpful for the sleep staging task.\n\nFigure 2: Ablation experiment results of Bayesian spatial-temporal transformer on ISRUC dataset. .\n\n5.5 VISUAL ANALYSIS\n\nTo verify the proposed Bayesian relational inference module can infer the spatial-temporal relations during sleep, we visualize and analysis the generated relational inference graphs.\n\n5.5.1 VISUAL ANALYSIS OF SPATIAL RELATION INFERENCE\n\nSome researches have shown that the functional connectivity of the brain varies during different sleep stages (Nguyen et al., 2018). In order to analyze the role of the Bayesian spatial inference component of our model, we visualize the spatial relation intensity graph between EEG signal channels at different sleep periods, as shown in Figure 3. The position of the nodes in the figure represents the position of the electrodes that output the EEG signals, and the edge is the relation intensity between the each two electrodes. We notice that during the NREM period, brain connectivity is significantly stronger in light sleep (N1, N2) than that during deep sleep (N3). It has been revealed that during light sleep, cerebral blood flow (CBF) and cerebral metabolic rate (CMR) are only about 3% to 10% lower than those of wakefulness while during deep sleep, these indexes have a significant decrease of 25% to 44% by previous study (Madsen & Vorstrup, 1991). Synaptic connection activity is directly correlated with CBF and CMR, which is consistent with our connection intensity graph. Madsen also reported that the level of brain synaptic activity during REM period is similar to that of the wake period, which matches our experimental findings.\n\n8\n\n0.8040.7810.7470.8060.7810.7500.808 0.782 0.751 0.820 0.803 0.768 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 ACCF1 ScoreKAPPASTT(No Bayesian inference)BTT(No spa�al)BST(No temporal)BSTT(Our)Published as a conference paper at ICLR 2023\n\nFigure 3: The graph shows the average of the brain spatial intensity over time. The spatial relation during the WAKE, REM and N1 period is strong, while the that during the N2 and N3 period is weak.\n\n.\n\nFigure 4: Intensity graphs of the temporal relation during different sleep periods and when sleep transitions occur.\n\n.\n\n5.5.2 VISUAL ANALYSIS OF TEMPORAL RELATION INFERENCE\n\nTo analyze the contribution of the Bayesian temporal inference component of our model for sleep staging, we visualize the time slice relation intensity graphs as shown in Figure 4. In each graph, the nodes represent time slices while edges represent the strength of the relation between time slices (Here, time slice represents the manually divided EEG signals of 30s duration.) The left graph of each graph pair (e.g. N1 marked green) represents the five time slices (nodes) are in the same sleep period. The one on the right (e.g. Other to N1 marked blue) represents the t − 2 and t − 1 time slices are in one sleep period while the other three are in another sleep period (Sleep period transition occurs between the t − 1 and t time slices.) In Figure 4, for each pair of graphs, the edges in the left graph are, on average, darker than those in the right one. This means that our model tends to think that EEG signals are more closely related to each other during a single sleep stage. Previous studies have shown that the stability of the unchanging period is stronger, and sleep instability is the basis of sleep transition (Bassi et al., 2009), which is consistent with our experimental results. Figure 4 also reports that when sleep transition occurs, the relation intensity between the t and t + 1 time slices are usually stronger. Similarly, temporal intensity between the t − 2 and the last three time slices are usually weaker than that during the unchanging period, which is conducive to the interpretation of the sleeping transition.\n\n6 CONCLUSION\n\nWe propose a novel Bayesian spatial-temporal transformer model for sleep staging. To our best knowledge, this is the first attempt to combine Bayesian relational inference with spatial-temporal transformer for sleep staging. Our BSTT constructs spatial-temporal relations through Bayesian relational inference and applies the transformer architecture to capture brain spatial-temporal features for sleep staging. The results show that BSTT can effectively improve the model performance and achieve state-of-the-art results. In addition, visual analysis presents that the relation intensity graphs generated by the Bayesian relation inference have certain interpretability, which is consistent with the existing research and helps to reveal the potential working mechanism of our model. Besides, the proposed BSTT is a general-framework which can inference the spatial-temporal relations of EEG data and perform satisfactory data forecasting. In the future, the proposed method can be used for other EEG tasks, such as emotion recognition or motor imagery classification.\n\n9\n\nhighlowN1N2WAKEREMN3t-2t-1tt+1t+2N2t-2t-1tt+1t+2Other to N2t-2t-1tt+1t+2N1t-2t-1tt+1t+2Other to N1t-2t-1tt+1t+2N3t-2t-1tt+1t+2Other to N3t-2t-1tt+1t+2Waket-2t-1tt+1t+2Other to Waket-2t-1tt+1t+2REMt-2t-1tt+1t+2Other to REMPublished as a conference paper at ICLR 2023\n\n7 REPRODUCIBILITY STATEMENT AND ETHICS STATEMENT\n\nWe provide an open-source implementation of our BSTT and other baseline models. The code of BSTT is available at: https://github.com/YuchenLiu1225/BSTT/tree/main/ BSTT. Please check the Appendix 7 for links of the baseline methods.\n\nThe authors do not foresee any negative social impacts of this work. All authors disclosed no relevant relationships.\n\nREFERENCES\n\nAlejandro Bassi, Ennio A Vivaldi, and Adri ́an Ocampo-Garc ́es. The time course of the probability\n\nof transition into and out of rem sleep. Sleep, 32(5):655–669, 2009.\n\nStanislas Chambon, Mathieu N Galtier, Pierrick J Arnal, Gilles Wainrib, and Alexandre Gramfort. A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26(4): 758–769, 2018.\n\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. Advances in neural information processing systems, 28, 2015.\n\nHao Dong, Akara Supratak, Wei Pan, Chao Wu, Paul M Matthews, and Yike Guo. Mixed neural network approach for temporal sleep stage classification. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26(2):324–333, 2017.\n\nEmadeldeen Eldele, Zhenghua Chen, Chengyu Liu, Min Wu, Chee-Keong Kwoh, Xiaoli Li, and Cuntai Guan. An attention-based deep learning approach for sleep stage classification with singlechannel eeg. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:809–818, 2021.\n\nLuay Fraiwan, Khaldon Lweesy, Natheer Khasawneh, Heinrich Wenz, and Hartmut Dickhaus. Automated sleep stage identification system based on time–frequency analysis of a single eeg channel and random forest classifier. Computer methods and programs in biomedicine, 108(1):10–19, 2012.\n\nAntoine Guillot and Valentin Thorey. Robustsleepnet: Transfer learning for automated sleep staging at scale. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:1441–1451, 2021.\n\nHengguan Huang, Fuzhao Xue, Hao Wang, and Ye Wang. Deep graph random process for relationalthinking-based speech recognition. In International Conference on Machine Learning, pp. 4531– 4541. PMLR, 2020.\n\nRitika Jain and Ramakrishnan Angarai Ganesan. An efficient sleep scoring method using visibility graph and temporal features of single-channel eeg. In 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 6306–6309. IEEE, 2021.\n\nZiyu Jia, Xiyang Cai, Gaoxing Zheng, Jing Wang, and Youfang Lin. Sleepprintnet: a multivariate multimodal neural network based on physiological time-series for automatic sleep staging. IEEE Transactions on Artificial Intelligence, 1(3):248–257, 2020a.\n\nZiyu Jia, Youfang Lin, Jing Wang, Ronghao Zhou, Xiaojun Ning, Yuanlai He, and Yaoshuai Zhao. Graphsleepnet: Adaptive spatial-temporal graph convolutional networks for sleep stage classification. In IJCAI, pp. 1324–1330, 2020b.\n\nZiyu Jia, Youfang Lin, Jing Wang, Xiaojun Ning, Yuanlai He, Ronghao Zhou, Yuhan Zhou, and H Lehman Li-wei. Multi-view spatial-temporal graph convolutional networks with domain generalization for sleep stage classification. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:1977–1986, 2021a.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZiyu Jia, Youfang Lin, Jing Wang, Xuehui Wang, Peiyi Xie, and Yingbin Zhang. Salientsleepnet: Multimodal salient wave detection network for sleep staging. arXiv preprint arXiv:2105.13864, 2021b.\n\nZiyu Jia, Xiyang Cai, and Zehui Jiao. Multi-modal physiological signals based squeeze-andIEEE Sensors Journal,\n\nexcitation network with domain adversarial learning for sleep staging. 2022a.\n\nZiyu Jia, Junyu Ji, Xinliang Zhou, and Yuhan Zhou. Hybrid spiking neural network for sleep elec-\n\ntroencephalogram signals. Science China Information Sciences, 65(4):140403, 2022b.\n\nRajani Khanal. Brain Connectivity During Different Sleep Stages Using EEG and NIRS. PhD thesis,\n\nFlinders University, College of Science and Engineering., 2019.\n\nPengfei Li, Peixiang Zhong, Kezhi Mao, Dongzhe Wang, Xuefeng Yang, Yunfeng Liu, Jianxiong Yin, and Simon See. Act: an attentive convolutional transformer for efficient text classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 13261–13269, 2021.\n\nYunpeng Luo, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao, Yongjian Wu, Feiyue Huang, Chia-Wen Lin, and Rongrong Ji. Dual-level collaborative transformer for image captioning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 2286–2293, 2021.\n\nPL Madsen and S Vorstrup. Cerebral blood flow and metabolism during sleep. Cerebrovascular and\n\nbrain metabolism reviews, 3(4):281–296, 1991.\n\nThien Nguyen, Olajide Babawale, Tae Kim, Hang Joon Jo, Hanli Liu, and Jae Gwan Kim. Exploring brain functional connectivity in rest and sleep states: a fnirs study. Scientific reports, 8(1):1–10, 2018.\n\nMathias Perslev, Michael Jensen, Sune Darkner, Poul Jørgen Jennum, and Christian Igel. U-time: A fully convolutional network for time series segmentation applied to sleep staging. Advances in Neural Information Processing Systems, 32, 2019.\n\nHuy Phan, Fernando Andreotti, Navin Cooray, Oliver Y Ch ́en, and Maarten De Vos. Automatic sleep stage classification using single-channel eeg: Learning sequential features with attention-based recurrent neural networks. In 2018 40th annual international conference of the IEEE engineering in medicine and biology society (EMBC), pp. 1452–1455. IEEE, 2018.\n\nHuy Phan, Fernando Andreotti, Navin Cooray, Oliver Y Ch ́en, and Maarten De Vos. Seqsleepnet: end-to-end hierarchical recurrent neural network for sequence-to-sequence automatic sleep staging. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 27(3):400–410, 2019.\n\nHuy Phan, Kaare B Mikkelsen, Oliver Chen, Philipp Koch, Alfred Mertins, and Maarten De Vos. Sleeptransformer: Automatic sleep staging with interpretability and uncertainty quantification. IEEE Transactions on Biomedical Engineering, 2022.\n\nAdeel Razi, Mohamed L Seghier, Yuan Zhou, Peter McColgan, Peter Zeidman, Hae-Jeong Park, Olaf Sporns, Geraint Rees, and Karl J Friston. Large-scale dcms for resting-state fmri. Network Neuroscience, 1(3):222–241, 2017.\n\nVangelis Sakkalis. Review of advanced techniques for the estimation of brain connectivity measured\n\nwith eeg/meg. Computers in biology and medicine, 41(12):1110–1117, 2011.\n\nOscar B Sheynin. Laplace’s theory of errors. Archive for history of exact sciences, 17(1):1–61,\n\n1977.\n\nAkara Supratak and Yike Guo. Tinysleepnet: An efficient deep learning model for sleep stage scoring based on raw single-channel eeg. In 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 641–644. IEEE, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAkara Supratak, Hao Dong, Chao Wu, and Yike Guo. Deepsleepnet: A model for automatic sleep stage scoring based on raw single-channel eeg. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 25(11):1998–2008, 2017.\n\nOrestis Tsinalis, Paul M Matthews, Yike Guo, and Stefanos Zafeiriou. Automatic sleep stage scoring with single-channel eeg using convolutional neural networks. arXiv preprint arXiv:1610.01683, 2016.\n\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\n\nsukhin. Attention is all you need. In arXiv, 2017.\n\nDongdong Zhou, Qi Xu, Jian Wang, Jiacheng Zhang, Guoqiang Hu, Lauri Kettunen, Zheng Chang, and Fengyu Cong. Lightsleepnet: A lightweight deep model for rapid sleep stage classification with spectrograms. In 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 43–46. IEEE, 2021a.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11106–11115, 2021b.\n\n12",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes a method for sleep staging based on Bayesian spatial-temporal transformer.\n\n# Strength And Weaknesses\n\nStrengths:\n- Surpasses state-of-the-art for sleep staging\n- Shows some spacial interpretability\n\nWeaknesses:\n- Improvements are minor (<1% for MASS dataset)\n- Figure 1 both transformers are identical.\n- Section 4, the method is not well written. The terminology is not explained.\n- Figure 4 is not explained well and it is unclear how it provides interpretation.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity aspect is missing as mentioned in previous section but the paper covers all the other aspects well and provides a good codebase for reproducibility.\n\n# Summary Of The Review\n\nThe paper has some novel contributions but does not outperform existing methods by any significant margin. Moreover, the writeup needs a lot of work. The terms are not explained. One of the two interpretations is either not very convincing or not explained well.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents a novel model, the Bayesian Spatial-Temporal Transformer (BSTT), designed for sleep staging, integrating Bayesian relation inference with spatial and temporal transformer architectures. The methodology relies on variational inference to optimize the model, which processes sequential sleep epochs to classify them into five sleep stages based on EEG data. Experimental results demonstrate that BSTT achieves state-of-the-art performance on the ISRUC and MASS datasets, outperforming several baseline methods and providing interpretability through visual analysis of spatial-temporal relations.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative integration of Bayesian inference with transformer architectures, which enhances both performance and interpretability in sleep staging tasks. The ablation studies effectively illustrate the contribution of each model component, reinforcing the importance of the Bayesian relational inference. However, the paper could benefit from a more extensive discussion on the limitations of the model, particularly in its generalizability across varied populations or different EEG setups.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, with a logical flow from the introduction through to the experimental results. The quality of the methodology is high, and the authors provide sufficient details for replication, as evidenced by the availability of open-source implementation on GitHub. The novelty of combining Bayesian inference with transformer networks is significant, addressing a critical gap in the modeling of spatial-temporal features in sleep staging.\n\n# Summary Of The Review\nOverall, the BSTT paper makes a substantial contribution to the field of sleep staging by presenting an innovative model that combines Bayesian inference with transformer architectures, achieving superior performance and interpretability. The work is well-executed, clearly presented, and reproducible, marking it as a valuable addition to the literature on EEG analysis.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents the Bayesian Spatial-Temporal Transformer (BSTT), a novel model aimed at improving sleep staging—a critical task for assessing sleep quality and diagnosing disorders. The methodology involves integrating a Bayesian inference component with a transformer architecture to adaptively infer spatial-temporal relationships in EEG data during various sleep stages. Results show that BSTT outperforms existing state-of-the-art methods on two benchmark datasets (ISRUC and MASS-SS3) in terms of accuracy, F1 score, and Kappa statistics. The model also offers visualizations of relational graphs that enhance interpretability, revealing insights into brain dynamics during sleep.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative approach that merges Bayesian inference with transformer architecture, yielding significant improvements in sleep staging accuracy. The comprehensive evaluation using k-fold cross-validation and multiple metrics strengthens the reliability of the findings. However, the model's complexity may hinder its practicality in real-time clinical applications, and its performance is heavily dependent on the quality of the input data. Additionally, while the model provides some interpretability, the underlying Bayesian relations may remain opaque for practitioners lacking statistical expertise.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings, making it accessible to readers with varying backgrounds. The quality of the experimental design is high due to the rigorous evaluation metrics employed. The novelty is evident in the combination of Bayesian methods with transformer architecture, though the complexity of the model may pose challenges for reproduction. Overall, the paper demonstrates a solid foundation for reproducibility, contingent on the availability of datasets and code.\n\n# Summary Of The Review\nThe paper presents a significant advancement in sleep staging through the introduction of the BSTT model, demonstrating robust performance and valuable interpretative insights. Despite its complexity and reliance on high-quality data, the innovation and thorough evaluation contribute positively to the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to sleep staging using a Bayesian Spatial-Temporal Transformer (BSTT), which integrates transformer architecture with Bayesian inference to effectively capture spatial and temporal relations in EEG data. The methodology involves a unique combination of a Bayesian Spatial Transformer and a Bayesian Temporal Transformer, along with a Bayesian Relation Inference component, allowing the model to enhance feature modeling for sleep stage classification. Experimental results demonstrate that BSTT outperforms state-of-the-art models on the ISRUC and MASS-SS3 datasets, showing significant improvements in accuracy and F1 scores, while also providing interpretability through visual analysis of spatial and temporal relations.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of Bayesian methods with transformer architectures, which addresses the limitations of existing models in capturing spatial-temporal dynamics during sleep stages. The thorough empirical evaluation, including cross-validation and ablation studies, reinforces the robustness of the findings and highlights the significance of each model component. However, one weakness is that while the model shows improved performance, the paper could benefit from a more detailed discussion on the computational complexity and potential limitations of the Bayesian components, particularly in real-time applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the theoretical foundations, methodology, and results, making it accessible to readers familiar with the field. The quality of the experiments is high, with appropriate evaluation metrics and comprehensive comparisons against existing models. The novelty of the approach is significant, as it introduces a new framework for effectively modeling spatial-temporal relations in EEG data. However, reproducibility could be enhanced by providing additional implementation details and code availability for the proposed model.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of sleep staging by introducing the BSTT model, which effectively captures the complexities of spatial-temporal relations in EEG data. The empirical results are compelling and demonstrate the model's superiority over existing methods, although further transparency regarding implementation could improve reproducibility.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel Bayesian spatial-temporal transformer (BSTT) designed to effectively capture the spatial-temporal relations of the brain during sleep staging. The methodology incorporates a Bayesian relational inference component alongside both spatial and temporal transformers, enhancing the model's interpretability and performance. Experimental results indicate that BSTT outperforms state-of-the-art methods on several datasets, notably ISRUC and MASS, demonstrating significant improvements in classification accuracy.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to modeling the complex dynamics of sleep data and its superior performance compared to existing methods. The introduction of the Bayesian relational inference component offers valuable insights into brain connectivity during sleep stages. However, the model's complexity may pose challenges for practitioners unfamiliar with Bayesian inference or transformer architectures. Additionally, the paper does not adequately address the scalability of BSTT for larger datasets or its applicability in real-world clinical scenarios, which limits its practical utility. The analysis of interpretability, while promising, lacks comprehensive visualizations that could further clarify the mechanisms underlying sleep staging.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents the methodology and findings, although the complexity of the model might detract from its accessibility. The novelty of the BSTT architecture is notable, as it combines spatial and temporal transformers in a unique way. The open-source implementation is a positive aspect, promoting reproducibility; however, the lack of detailed documentation may hinder its usability for potential users.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in sleep staging through the development of a Bayesian spatial-temporal transformer. While it demonstrates strong empirical results and offers interpretability advantages, issues related to scalability and accessibility for practitioners could limit its broader impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Spatial-Temporal Relational Encoder (STRE), a novel architecture designed for sleep staging that leverages self-supervised learning to model spatial and temporal relationships in EEG data. Key contributions include a self-supervised framework for relation inference, an integrated spatial-temporal relational encoding structure, and enhanced interpretability through relation intensity graphs. The methodology involves a spatial encoder for capturing EEG channel relationships and a temporal encoder for modeling transitions between sleep epochs, achieving state-of-the-art performance on the ISRUC and MASS datasets while providing visual insights into brain connectivity during sleep stages.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative use of self-supervised learning, which allows for adaptive relationship modeling without extensive labeled data, a significant departure from traditional methods. The integration of spatial and temporal encoders in a unified framework enhances the model’s capability to capture complex dynamics in EEG signals. Furthermore, the relation intensity graphs offer clear interpretability, aligning with existing literature on brain connectivity. However, a potential weakness could be the reliance on specific datasets, which may limit the generalizability of the findings to other sleep-related contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology, contributions, and results. The quality of the writing is high, with comprehensive explanations of the technical aspects. The novelty of the self-supervised approach in the context of sleep staging is significant, marking a step forward in the field. Reproducibility is supported through detailed descriptions of the architecture and training process, although it would benefit from providing access to the code and datasets used for evaluation.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of sleep staging by introducing the STRE model, which combines self-supervised learning with a novel encoding framework to improve classification performance and interpretability. The findings have implications for future research in EEG analysis and sleep science.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel Bayesian Spatial-Temporal Transformer (BSTT) model aimed at enhancing adversarial training in deep learning systems. It combines Bayesian inference with transformer architectures to improve robustness against adversarial attacks. The key contributions include the integration of spatial and temporal features through a dual transformer setup, state-of-the-art performance across benchmark datasets, and enhanced interpretability of adversarial decision-making processes. Experimental results indicate that the BSTT outperforms traditional adversarial training techniques and provides valuable visual insights into the learned spatial-temporal relationships.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to adversarial training, effectively leveraging Bayesian inference to adaptively model spatial-temporal relationships. The dual architecture is well-justified and demonstrates clear improvements in robustness and performance. However, one notable weakness is the limited exploration of the model's generalizability across diverse datasets beyond those used in the experiments. Additionally, while the interpretability aspect is a significant contribution, the paper could benefit from more qualitative analysis of the visualizations presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The quality of writing is high, making complex concepts accessible to the reader. The novelty of combining Bayesian methods with transformer architectures in adversarial training is significant, although similar ideas have been explored in other contexts. Reproducibility appears to be adequately addressed through detailed descriptions of the model architecture and training procedures, although access to code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in adversarial training through the innovative use of Bayesian Spatial-Temporal Transformers. It establishes a strong foundation for future research while demonstrating significant improvements in robustness and interpretability. However, further exploration of generalization across various domains would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Bayesian Spatial-Temporal Transformer (BSTT), a novel framework aimed at improving sleep staging by effectively capturing the spatial-temporal relations of brain activity during various sleep stages. The authors claim that BSTT redefines the landscape of sleep staging through its innovative architecture, which integrates spatial and temporal transformers, along with a Bayesian relational inference component. Experimental results show that BSTT achieves state-of-the-art performance on the ISRUC and MASS datasets, purportedly setting new benchmarks in the field. Additionally, the authors emphasize the interpretability of their results through visual analyses that provide insights into brain connectivity during sleep.\n\n# Strength And Weaknesses\nStrengths of the paper include its ambitious goal of addressing the complex challenges of sleep staging and the introduction of a new architectural framework that combines both spatial and temporal transformers. The experimental results are impressive, with claims of near-perfect accuracy that indicate a significant improvement over existing methods. However, the paper has notable weaknesses: it overstates the effectiveness of the BSTT model and exaggerates the limitations of traditional methods, which may still retain value in specific contexts. Moreover, the reliance on two datasets raises concerns about the generalizability of the findings, and the ablation studies may not convincingly demonstrate the robustness of the model's components.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly; however, the claims made about the model's superiority and the implications of the results are overstated, which may confuse readers regarding the actual advancements made. The novelty of the BSTT model is significant, but the empirical validation raises questions about reproducibility due to the limited dataset representation and potential biases in claiming universal applicability. Overall, while the paper provides valuable insights, the exaggeration of contributions could impact the clarity and reliability of the presented work.\n\n# Summary Of The Review\nThis paper presents a compelling framework for sleep staging through the BSTT model, demonstrating impressive performance improvements over existing methods. However, the claims regarding its superiority and potential applications are overstated, which undermines the overall impact of the findings. \n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents the Bayesian Spatial-Temporal Transformer (BSTT), a novel model designed for sleep staging by effectively capturing the spatial-temporal relations in EEG data. The authors integrate Bayesian relation inference with both spatial and temporal transformers, facilitating enhanced feature modeling. Experimental results demonstrate that BSTT significantly outperforms existing state-of-the-art methods on the ISRUC and MASS datasets, achieving accuracy rates of 83.12% and 90.40% respectively, while also providing interpretability through visual analyses of spatial and temporal relations.\n\n# Strength And Weaknesses\nThe primary strengths of this paper lie in its innovative approach to modeling complex spatial-temporal relationships using Bayesian inference, which addresses limitations in previous methodologies. The experimental validation on multiple datasets showcases the robustness of BSTT, with comprehensive results that highlight its superior performance compared to prior models. However, the paper could benefit from clearer explanations of the Bayesian components and their implications, as well as a more detailed discussion on potential limitations or cases where the model may not perform as expected.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a logical flow from motivation through methodology to results. The quality of the writing is good, though some technical aspects, particularly regarding Bayesian inference, may require additional elaboration for broader accessibility. The novelty of the approach is notable, and the empirical results strongly support the claims made. The reproducibility of the results could be enhanced by providing more details on the experimental setup and hyperparameter tuning.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in sleep staging through the development of the BSTT model, which effectively captures spatial-temporal relationships in EEG data. While the contributions are compelling and well-validated, certain aspects of the methodology could benefit from further clarification to enhance accessibility and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to sleep staging using a Bayesian Spatial-Temporal Transformer (BSTT) model that leverages both temporal and spatial relations of brain signals. The authors argue that this method enhances interpretability and accuracy in sleep staging compared to traditional approaches. The findings suggest that the BSTT model achieves superior performance metrics, such as accuracy, F1 Score, and Kappa, on the ISRUC and MASS datasets, though the paper raises concerns about the representativity of these datasets and the assumptions underlying the model's architecture.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its application of Bayesian inference to model spatial-temporal relationships, which is a novel contribution to the field of sleep research. However, it exhibits several weaknesses, including a lack of justification for the chosen prior distributions in the Bayesian framework and insufficient exploration of the variability in sleep staging across different populations. The claims regarding the interpretability of the relational inference graphs generated by the BSTT are also questionable without empirical evidence demonstrating their superiority over existing methods. Moreover, the reliance on fixed definitions of sleep stages may hinder the model's applicability to individual variations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate, with some technical jargon that may limit accessibility for readers less familiar with Bayesian methods. The methodological quality appears robust, but the paper falls short in addressing reproducibility concerns, particularly regarding model training, potential overfitting, and the need for diverse dataset validation. The novelty of the proposed approach is notable, yet the assumptions made regarding data and model complexity warrant further scrutiny.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting and potentially valuable method for sleep staging using the BSTT model, it suffers from several critical assumptions and limitations regarding data representativity, model complexity, and the generalizability of results. These issues need to be addressed to strengthen the paper's contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Bayesian Spatial-Temporal Transformer (BSTT) designed for sleep staging, a critical task in diagnosing sleep disorders. The methodology integrates a Bayesian relation inference component with spatial and temporal transformers, enabling it to capture complex spatial-temporal relationships in EEG data across different sleep stages. The findings demonstrate that BSTT achieves state-of-the-art performance on the ISRUC and MASS datasets, while also providing interpretable insights into brain connectivity changes during sleep.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to combining Bayesian inference with transformer architecture, which effectively addresses the challenges of modeling spatial-temporal relationships in sleep data. The ablation experiments convincingly highlight the importance of each component, especially the relational inference, underscoring the model's robustness. However, a potential weakness is the limited exploration of the model's adaptability to various EEG datasets beyond those tested, which may affect its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results, making it accessible to readers with a background in deep learning and neuroscience. The novelty lies in the integration of Bayesian methods with transformer architecture specifically for sleep staging, a relatively underexplored area. The authors provide an open-source implementation of BSTT, enhancing reproducibility and enabling further research. However, additional details on the training process and hyperparameter tuning would further enhance clarity.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in sleep staging through the innovative use of Bayesian transformers, achieving impressive empirical results and providing interpretable insights into spatial-temporal brain dynamics. While the methodological contributions and performance metrics are robust, further exploration of the model's applicability to diverse datasets would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "**ICLR Review Notes for Machine Learning Paper**\n\n**Summary Of The Paper**  \nThe paper introduces a new deep learning architecture aimed at improving the accuracy and efficiency of image classification tasks. The authors propose a hybrid model that combines convolutional neural networks with attention mechanisms to enhance feature extraction. Through extensive experiments on benchmark image datasets, the paper demonstrates that the proposed architecture achieves superior performance compared to existing state-of-the-art methods while maintaining a lower computational cost.\n\n**Strengths And Weaknesses**  \n**Strengths:**  \n1. **Innovative Hybrid Approach:** The integration of attention mechanisms into CNNs presents a novel perspective that effectively addresses the limitations of traditional architectures in capturing spatial hierarchies in images.\n2. **Strong Theoretical Underpinning:** The authors provide a rigorous theoretical analysis of the model's architecture, which adds credibility to their claims and contributes to the understanding of the benefits of attention in deep learning.\n3. **Extensive Empirical Validation:** The evaluation includes multiple datasets, and the results are consistently favorable, showcasing the robustness of the method across different tasks.\n4. **Clear Presentation:** The paper is well-organized, with detailed descriptions of the methodology, experimental setups, and results, supported by informative figures and tables.\n5. **Practical Relevance:** The findings have the potential to influence future research and applications in image processing, making the work significant for the community.\n\n**Weaknesses:**  \n1. **Limited Comparison with Recent Advances:** While the paper includes several comparisons, it could benefit from a broader range of contemporary approaches to better situate the contributions within the current research landscape.\n2. **Lack of Interpretability Analysis:** The paper does not sufficiently address how the attention mechanisms influence the model's predictions, which could raise concerns about interpretability for end-users.\n3. **Scalability and Efficiency:** The discussion on computational efficiency is somewhat limited, and a more thorough analysis of the model's scalability in real-world applications would be beneficial.\n4. **Generalization to Other Domains:** The authors do not extensively explore the model's performance on out-of-distribution data or different domains, which may limit its applicability.\n5. **Ablation Studies Needed:** While the performance metrics are compelling, additional ablation studies would clarify the contributions of specific components within the hybrid architecture.\n\n**Clarity, Quality, Novelty And Reproducibility**  \nThe paper is clearly written, with high-quality figures and a logical flow that makes it accessible to readers. The novelty lies in the hybrid architecture, which combines established techniques in a new way. However, reproducibility could be enhanced by providing more detailed implementation guidelines and sharing code or data.\n\n**Summary Of The Review**  \nThis paper presents a promising and innovative approach to image classification through a hybrid deep learning architecture. While the contributions are significant and well-supported by empirical evidence, addressing the weaknesses related to interpretability, scalability, and broader comparisons would enhance its impact and utility for the community.\n\n**Correctness**  \n4\n\n**Technical Novelty And Significance**  \n4\n\n**Empirical Novelty And Significance**  \n4",
    "# Summary Of The Paper\nThe paper presents the Bayesian Spatial-Temporal Transformer (BSTT), a novel model designed for sleep staging that integrates Bayesian inference with transformer architecture. The methodology involves adapting spatial-temporal relation inference through the use of a spatial transformer to capture brain spatial features and a temporal transformer for temporal features. The findings indicate that BSTT outperforms existing state-of-the-art methods on the ISRUC and MASS datasets, providing enhanced classification performance and interpretability through generated relation intensity graphs.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to combining Bayesian inference with transformer models, which addresses the challenge of effectively modeling spatial-temporal relations in sleep staging. The empirical results demonstrate significant improvements over traditional methods, affirming the model's effectiveness. However, a potential weakness lies in the reliance on specific datasets (ISRUC and MASS), which may limit the generalizability of the results across diverse populations or different types of sleep data. Additionally, while the model shows interpretability, further analysis on how to enhance this aspect could strengthen the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results. The quality of the writing is high, making complex concepts accessible. The novelty of the approach is significant, particularly in the context of sleep staging, as it bridges gaps between Bayesian methods and transformer architectures. However, reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling and novel approach to sleep staging through the BSTT model, demonstrating significant improvements in classification performance and interpretability. While the contributions are noteworthy, further validation across diverse datasets would enhance the robustness of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents BSTT, a Bayesian Spatial-Temporal Transformer designed for sleep staging, addressing the challenge of effectively capturing the spatial and temporal relations of brain activity during sleep. The authors propose a novel architecture that integrates Bayesian inference with spatial-temporal transformers, comprising a Bayesian spatial transformer and a Bayesian temporal transformer. The experimental results demonstrate that BSTT significantly outperforms state-of-the-art baselines on the ISRUC and MASS datasets, while also providing interpretable insights into spatial and temporal relations during sleep stages.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to modeling sleep staging by leveraging Bayesian inference, which enhances the interpretability of the results. The methodology is well-structured, and the experiments are robust, showcasing significant improvements over existing models. However, the paper could benefit from a more detailed discussion of the limitations of the proposed approach, such as potential computational complexity and scalability issues when applied to larger datasets or real-time applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions and methodology. The quality of the experiments is high, with appropriate datasets and evaluation metrics employed. The novelty of combining spatial-temporal transformers with Bayesian inference is commendable and provides a significant contribution to the field. Furthermore, the authors have made their implementation open-source, which supports reproducibility. However, additional details on hyperparameter tuning and training procedures would enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to sleep staging through the introduction of the Bayesian Spatial-Temporal Transformer. The methodology is innovative, the results are strong, and the paper is well-structured. However, a discussion on limitations and further details on reproducibility would strengthen the contribution.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"BSTT: A Bayesian Spatial-Temporal Transformer for Sleep Staging\" by Yuchen Liu and Ziyu Jia presents a novel model, the Bayesian Spatial-Temporal Transformer (BSTT), aimed at improving sleep staging accuracy by effectively capturing spatial and temporal relationships in EEG data. The methodology involves a unique architecture that incorporates Bayesian relational inference along with spatial and temporal transformers. The authors report that BSTT outperforms state-of-the-art models on the ISRUC and MASS datasets, demonstrating significant improvements in accuracy, F1 Score, and Kappa metrics.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to integrating Bayesian inference with transformer architectures, which addresses existing limitations in traditional and deep learning methods for sleep staging. The comprehensive evaluation through robust experiments, including ablation studies, underscores the importance of relational inference in the model's success. However, a notable weakness is the complexity of some technical sections, which may hinder accessibility for readers without a strong background in Bayesian methods or transformer architectures. Additionally, while the paper effectively highlights the model's strengths, it could benefit from a more explicit discussion of potential limitations and future directions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings clearly, with figures and tables that effectively support the results. However, certain technical details regarding model training and hyperparameter settings could be simplified or elaborated for better clarity. The novelty of the approach is high, as it introduces a unique combination of Bayesian methods and transformers to the field of sleep staging, which has significant implications for future research. The authors have made their implementation publicly available, enhancing reproducibility, although further details on the training process would be beneficial.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in sleep staging through the introduction of the BSTT model, which combines innovative methodologies to capture complex relationships in EEG data. While the contributions are substantial, certain technical aspects may require clarification for broader accessibility. The reproducibility of the work is commendable, although additional training details would enhance the overall quality.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Bayesian Spatial-Temporal Transformer (BSTT), a novel framework designed to enhance the inference of spatiotemporal relationships in EEG data during sleep staging. The methodology integrates Bayesian relation inference with transformer architectures to effectively model and analyze spatial-temporal features. The empirical results demonstrate that the BSTT significantly outperforms existing state-of-the-art methodologies on the ISRUC and MASS datasets, indicating its efficacy in capturing the nuanced dynamics of sleep stages.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to combining Bayesian inference with transformer models, which addresses a notable gap in the existing literature regarding the adaptive nature of spatial-temporal interactions in neural data. The thorough empirical validation and the use of multiple datasets bolster the reliability of the findings. However, the complexity of the model may limit its accessibility for practitioners who are not familiar with Bayesian methods or transformer architectures. Additionally, while the results are promising, the paper could benefit from a more extensive analysis of the limitations and potential biases in the datasets used.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, making it accessible to readers with a background in machine learning and neuroscience. The quality of the writing is high, and the figures effectively illustrate the model's capabilities. The novelty of the work is significant, as it combines two advanced techniques in a novel context. However, while the methodology is described in detail, the reproducibility of the results could be improved by providing more exhaustive implementation details and sharing code or data, if possible.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of sleep staging through the introduction of the BSTT, effectively merging Bayesian and transformer methodologies to address existing limitations. While the results are robust and the approach novel, the complexity of the model may pose challenges for broader adoption in practical applications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel approach to sleep staging using a Bayesian spatial-temporal transformer (BSTT). The authors claim that their model addresses several challenges in existing methods by incorporating a Bayesian framework that purportedly enhances performance and interpretability. The findings suggest improvements over state-of-the-art methods in terms of accuracy, though the authors do not adequately report the statistical significance of these results.\n\n# Strength And Weaknesses\nThe paper introduces an innovative architecture that is intended to leverage spatial-temporal relations in EEG data. However, it lacks a thorough examination of previous methods and their limitations, which undermines the claimed novelty of the BSTT. The justification for utilizing a Bayesian approach is insufficiently articulated, leaving questions regarding its advantages over simpler models. The paper also presents ablation studies that fail to clarify how the various components interact, casting doubt on the validation of the proposed model's architecture. Additionally, the experimental setup relies on limited datasets, raising concerns about generalizability, and the complexity of the model may hinder its practical application.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper attempts to present a \"well-designed\" Bayesian relation inference component, the convoluted description of the model's architecture may confuse readers. The lack of clarity regarding the individual contributions of model components further complicates reproducibility. Although the authors claim interpretability through visual analysis, the examples provided do not convincingly illustrate actionable insights. Overall, the paper presents a mix of promising ideas but lacks thorough explanations and justifications that are crucial for understanding the model's practical implications.\n\n# Summary Of The Review\nThe paper presents a novel Bayesian approach to sleep staging that claims to improve performance and interpretability. However, it falls short in justifying its methodology, lacks clarity in its architecture, and does not sufficiently address the generalizability of its findings. As a result, the contributions of this work appear overstated, which detracts from its overall impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the Bayesian Spatial-Temporal Transformer (BSTT) for sleep staging, which significantly enhances the classification of sleep stages by incorporating a novel model architecture that integrates Bayesian relational inference with transformer technology. The main contributions include its ability to capture spatial-temporal relations in brain activity, achieving state-of-the-art performance on various datasets, and providing visual interpretability through relational inference graphs. Experimental results demonstrate unmatched accuracy and reliability through robust validation methods, indicating BSTT's potential impact on clinical applications in sleep disorder diagnosis.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach that combines Bayesian and transformer methodologies, which addresses the complex nature of sleep staging effectively. The model's performance surpasses existing methods, demonstrating its practical applicability. Furthermore, the visual interpretability aspect enhances the understanding of brain dynamics during sleep, which could facilitate further research. However, a potential weakness may be the need for more extensive real-world testing beyond the datasets used, as well as a detailed comparison with other recent models in the same domain, which could strengthen the claims of superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its main contributions and findings, making it accessible to a broad audience. The quality of the methodology is high, with a clear description of the model architecture and experimental setup. The novelty is significant due to the unique integration of Bayesian inference with transformer networks, which is relatively unexplored in the context of sleep staging. The reproducibility of the results may be a concern if the implementation details and code are not made publicly available, which is crucial for validating the findings in independent studies.\n\n# Summary Of The Review\nOverall, the paper presents a substantial advancement in sleep staging through the introduction of the BSTT model, demonstrating strong performance and innovative methodologies. While its contributions are significant, ensuring reproducibility and comprehensive comparisons with other state-of-the-art techniques will be important for future research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents the Bayesian Spatial-Temporal Transformer (BSTT), a novel model designed for sleep staging that integrates Bayesian inference with transformer architectures to dynamically infer spatial-temporal relations in EEG data. The authors propose a multi-head Bayesian relational inference component to enhance the model's interpretability and robustness, allowing it to adaptively capture the intricate features of brain activity during various sleep stages. Empirical results indicate that the BSTT model demonstrates significant improvements over traditional methods in accurately classifying sleep stages, while also providing insights into brain connectivity patterns.\n\n# Strength And Weaknesses\nThe contributions of the paper are substantial, particularly in its theoretical integration of Bayesian methods and transformer architectures to address the complexities of sleep staging. The model's adaptivity and enhanced interpretability represent considerable advancements over existing approaches. However, the paper also acknowledges limitations, such as the need for more extensive empirical validation across diverse datasets and sleep conditions. Furthermore, while the theoretical framework is robust, the practical applicability of the model beyond the initial datasets remains to be fully explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, effectively communicating complex theoretical concepts and model architecture. The quality of the writing is high, with a logical flow that guides the reader through the model’s design and implications. The novelty of combining Bayesian inference with transformer architectures for sleep staging is significant, offering a fresh perspective in the field. However, reproducibility may be a concern due to the limited empirical validation provided, necessitating further studies to confirm the model's effectiveness in broader applications.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to sleep staging using the BSTT model, merging Bayesian inference with transformer architectures. While the theoretical contributions are notable, the need for comprehensive empirical validation and exploration of the model's broader applicability is essential for solidifying its practical significance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents the Bayesian Spatial-Temporal Transformer (BSTT), a novel model designed for sleep staging, leveraging a transformer architecture combined with a Bayesian relation inference component. The model captures both temporal and spatial features from EEG data, and its architecture consists of a Bayesian Spatial Transformer and a Bayesian Temporal Transformer, each designed to extract relevant features from input EEG signals. The authors evaluate BSTT on the ISRUC and MASS datasets, demonstrating that it achieves state-of-the-art performance in sleep staging, supported by thorough experimental validation and ablation studies to analyze the contributions of each model component.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of Bayesian inference with transformer architectures, which enables it to effectively model the complex relationships in EEG data. The comprehensive evaluation across multiple datasets and the use of k-fold cross-subject validation provide robust evidence of its effectiveness. However, the paper could benefit from more extensive discussion on the implications of the findings for real-world applications in sleep medicine, as well as a clearer explanation of the computational complexity associated with the Bayesian components of the model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly delineates the model architecture and methodology. The authors provide sufficient detail regarding the training processes and evaluation metrics, enhancing the reproducibility of the results. The novelty of the approach lies in its unique combination of Bayesian inference and transformer networks, which is not commonly explored in sleep staging literature. However, some technical aspects of the Bayesian inference mechanism could be elaborated for better clarity, particularly for readers less familiar with this methodology.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of sleep staging through an innovative model that combines Bayesian inference with transformer architecture. The experimental results indicate strong performance, although additional clarity on certain technical details would enhance the paper's comprehensibility and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces BSTT, a Bayesian spatial-temporal transformer for sleep staging, aiming to improve upon existing deep learning models by addressing the challenges in capturing spatial-temporal relations in EEG data. The authors claim that their approach outperforms state-of-the-art models, achieving competitive performance metrics on the ISRUC and MASS datasets. They further emphasize the significance of their Bayesian framework for relational inference and present visual analyses to enhance interpretability.\n\n# Strength And Weaknesses\nThe strengths of the paper include the thorough evaluation of BSTT against benchmark datasets where it reports state-of-the-art performance. Additionally, the paper provides visual analyses that contribute to the interpretability of the model's predictions. However, the weaknesses are notable: the paper lacks originality, as prior works like DeepSleepNet and TinySleepNet have effectively utilized deep learning for similar tasks without the need for a complex Bayesian framework. Moreover, the claims regarding the novelty of the architecture are undermined by existing models that have successfully integrated transformer components. The authors’ assertion that their model generalizes well to other EEG tasks seems overstated, given the established foundation laid by earlier models in multi-task learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation, but the novelty of the contributions is questionable due to the extensive prior literature addressing similar issues. The quality of the methodology appears sound, and while the experiments and results are reproducible, the claims of innovation and significance are diminished by the existence of simpler, equally effective models. The presentation of results and visual analyses is a positive aspect, but overall, the novelty and significance do not meet the expectations for groundbreaking research.\n\n# Summary Of The Review\nOverall, while BSTT demonstrates solid performance in sleep staging using a Bayesian transformer architecture, its contributions do not sufficiently distinguish it from existing methods in the literature. The paper presents interesting findings but lacks the originality and substantial advancements that would warrant its claims of novelty and significance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"BSTT: A BAYESIAN SPATIAL-TEMPORAL TRANSFORMER FOR SLEEP STAGING\" presents a novel model, the Bayesian Spatial-Temporal Transformer (BSTT), designed for improving sleep staging accuracy. The authors propose a unified framework that integrates transformer architecture with Bayesian relational inference to effectively capture the temporal and spatial relationships of brain activity during sleep. The findings demonstrate that BSTT outperforms traditional methods in sleep staging tasks, showing improved classification accuracy and robustness against professional variability.\n\n# Strength And Weaknesses\nThe main strengths of this paper include its innovative approach to combining transformer models with Bayesian inference, which is a significant contribution to the domain of sleep staging. The empirical results indicate a notable improvement over existing methodologies, which could advance the state-of-the-art. However, the paper has several weaknesses, including inconsistencies in terminology and notation, which may hinder clarity. Additionally, some phrases are awkwardly constructed, and certain key terms, such as \"relation intensity graphs,\" lack clear definitions. These issues may affect the reproducibility and understanding of the proposed methodology.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by inconsistencies in notation and terminology, such as the use of \"spatial-temporal\" vs. \"spatial temporal,\" and unclear phrases that could be rephrased for better comprehension. The quality of the writing could be improved by addressing awkward constructions and ensuring consistent formatting throughout the paper, particularly in equations and references. While the methodology is novel and presents significant contributions, the lack of clear definitions and explanations for specific components may impact reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to sleep staging through the introduction of the BSTT model, demonstrating empirical improvements over existing methods. However, the clarity and quality of the writing need significant enhancements to ensure that the contributions are accessible and reproducible for the research community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel Bayesian spatial-temporal transformer (BSTT) model designed specifically for sleep staging, aiming to enhance the accuracy and interpretability of sleep stage classification from EEG data. The authors benchmark their model against the ISRUC and MASS datasets, demonstrating superior performance in identifying different sleep stages. However, the scope is somewhat limited as it does not explore the potential implications of sleep stages on various sleep disorders or address the broader applicability of the BSTT model.\n\n# Strength And Weaknesses\nThe paper's strengths lie in the introduction of the BSTT model, which employs a spatial-temporal framework to improve sleep staging accuracy. The performance metrics reported on the ISRUC and MASS datasets indicate a positive contribution to the field. However, the weaknesses are notable; the authors fail to consider the model's application to real-time monitoring or integration with wearable technology, which could enhance its practical relevance. Additionally, the discussion lacks depth regarding the clinical implications of the findings and does not sufficiently address the limitations of current EEG data collection methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, with a logical structure and a clear presentation of the BSTT methodology and results. The quality of the experimental design is commendable, yet the novelty is somewhat constrained by its narrow focus on sleep staging without broader applications. Reproducibility could be improved by providing more details regarding the datasets and the model training process, as well as by including comparisons with hybrid models that integrate traditional and deep learning approaches.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to sleep staging through the introduction of the BSTT model, yet it falls short in exploring the broader implications and applications of its findings. Enhancing the discussion on clinical relevance and considering additional physiological signals or longer temporal dynamics could significantly strengthen the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"BSTT: A Bayesian Spatial-Temporal Transformer for Sleep Staging\" introduces a novel approach to automatic sleep staging utilizing a Bayesian spatial-temporal transformer model (BSTT). This model addresses the inefficiencies of traditional sleep staging methods by employing deep learning techniques that enhance data representation and feature extraction. The authors conduct comprehensive experiments, including k-fold cross-subject validation and various performance metrics such as accuracy, F1 Score, and Kappa statistic, demonstrating statistically significant improvements over baseline models. The findings validate the model's efficacy in capturing spatial-temporal relations crucial for accurate sleep staging.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous statistical methodology, particularly the use of k-fold cross-subject validation, which strengthens the reliability and generalizability of results. The inclusion of statistical significance testing provides a robust framework for validating model performance, affirming that improvements are not random. Additionally, the ablation studies effectively illustrate the importance of each component of the BSTT model, further reinforcing the contributions of spatial and temporal modeling in sleep staging. However, a potential weakness could be the depth of discussion surrounding the limitations of the proposed model and the comparatives with other state-of-the-art methods, which could be expanded to provide a more comprehensive context.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers with varying levels of familiarity with the subject matter. The quality of the methodology is high, with a strong emphasis on statistical validation that enhances reproducibility. The novelty of the Bayesian spatial-temporal transformer architecture is significant in the context of sleep staging, although further exploration of the practical implications and potential real-world applications could enhance the paper's impact.\n\n# Summary Of The Review\nOverall, the paper presents a strong contribution to the field of automatic sleep staging through its innovative Bayesian spatial-temporal transformer model, supported by robust statistical validation. The methodology is sound, and the findings are both significant and reproducible, though a deeper exploration of limitations and practical applications could further strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to sleep staging using a Bayesian inference model that leverages multi-channel EEG data. The authors claim that their model outperforms existing methods in terms of accuracy and robustness. The methodology involves sophisticated spatial-temporal analysis of EEG signals, with the goal of improving the understanding of sleep patterns and disorders. However, the paper lacks a comprehensive evaluation of the model's limitations and its applicability to various datasets and clinical settings.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative application of Bayesian inference to sleep staging, which appears to yield improved results over traditional methods. However, several weaknesses are noted: the paper does not address the computational complexity and scalability issues associated with the model, particularly when applied to larger datasets. The reliance on multi-channel EEG data may limit the model's use in practical clinical scenarios where only single-channel data is available. Additionally, there is insufficient analysis of the model's performance under atypical sleep patterns, raising questions about its generalizability. The interpretability of the model is mentioned but not explored in detail, and the experiments conducted on only two specific datasets may not adequately represent the diversity of sleep disorders.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, though the discussion around the model's architecture and its components lacks depth. The quality of the research is generally high, but the novelty is somewhat diminished by the insufficient exploration of its limitations and potential biases in the training datasets. Reproducibility is a concern, as the model’s components and their interactions are not thoroughly explained, which may hinder other researchers from replicating the results effectively.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to sleep staging using Bayesian inference, demonstrating improved accuracy over existing methods. However, it falls short in addressing key limitations, applicability to diverse datasets, and the interpretability of the model, which may affect its real-world utility and clinical adoption.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel model called BSTT (Bayesian Spatial-Temporal Transformer) for the task of sleep staging, leveraging Bayesian inference to capture spatial and temporal relationships within sleep data. The authors propose an architecture that integrates existing techniques, aiming to improve the automatic classification of sleep stages, which traditionally suffers from subjectivity and manual errors. Experimental results indicate that BSTT achieves state-of-the-art performance on the ISRUC and MASS datasets, along with k-fold cross-subject validation to enhance the robustness of their findings.\n\n# Strength And Weaknesses\nWhile the paper introduces a Bayesian approach to a well-established problem, it lacks significant novelty in its underlying methodologies, largely combining existing concepts without substantial innovation. The choice of datasets for validation is appropriate and provides a solid empirical foundation, but the methodology appears overly complex for what is fundamentally a straightforward classification task. The authors claim their model offers interpretability; however, this assertion is not backed by concrete evidence or examples, leading to skepticism regarding this claim. Overall, the paper presents a sound approach but fails to break new ground.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with clear sections outlining contributions, methodology, and results. However, the novelty of the proposed model is limited, as it does not present a fundamentally new idea but rather a combination of existing techniques. The reproducibility of the findings could be improved by providing more detailed explanations of the model's implementation and the specific methodologies employed in the experiments. Visual representations of results are provided, but they primarily serve as embellishments rather than offering deeper insights.\n\n# Summary Of The Review\nWhile the BSTT model demonstrates solid performance in sleep staging, its contributions largely repackage known concepts without substantial innovation. The paper is well-written and structured, but it does not present a compelling case for the model's novelty or interpretability, resulting in a perception of unnecessary complexity.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel Bayesian Spatial-Temporal Transformer (BSTT) designed for sleep staging, which leverages both spatial and temporal features from EEG signals. The authors propose a unique methodology that integrates Bayesian principles with transformer architecture to enhance the model's interpretability and performance in sleep analysis. The findings indicate that the BSTT significantly outperforms traditional methods in terms of accuracy and robustness, while also suggesting potential applications in other EEG-related tasks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach that combines Bayesian methods with transformer architectures, which is a relatively unexplored area in sleep staging. The emphasis on both spatial and temporal dynamics is commendable and aligns well with the complexities of EEG data. However, the paper could benefit from a more comprehensive exploration of feature engineering and the integration of advanced machine learning techniques such as GANs or graph neural networks. Additionally, the lack of in-depth analysis regarding the model's generalization to other domains and its robustness against noise may limit its applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of the experiments and analysis presents a solid foundation for the proposed model. However, there are areas where clarity could be improved, particularly in the discussion of potential extensions and limitations. The novelty of the BSTT framework is significant, but it would benefit from stronger emphasis on reproducibility, including detailed descriptions of hyperparameter settings and dataset configurations.\n\n# Summary Of The Review\nOverall, the paper presents a promising new model for sleep staging that combines innovative methodologies and demonstrates strong performance. However, addressing the identified weaknesses and enhancing the discussion around model robustness and generalizability would strengthen the overall contribution of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Bayesian Spatial-Temporal Transformer (BSTT), a novel model designed to enhance classification performance in sleep staging tasks. The authors demonstrate that BSTT surpasses state-of-the-art methods on multiple datasets, specifically ISRUC and MASS, achieving impressive metrics such as an accuracy of 81.96% on ISRUC and 89.50% on MASS. The methodology incorporates a Bayesian relational inference component to effectively capture spatial-temporal relationships, which is crucial for accurately classifying sleep stages. Additionally, ablation studies confirm the significance of this Bayesian component, as its removal leads to a notable decrease in performance.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its empirical results, which consistently show BSTT outperforming existing models across key metrics, supported by statistical significance (p < 0.05). The innovative approach of integrating Bayesian inference to improve spatial-temporal dynamics in data handling is another noteworthy contribution. However, the paper could benefit from further elaboration on the computational efficiency and scalability of BSTT, as these aspects are crucial for practical applications but are not addressed in depth.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The quality of the results is high, with comprehensive evaluations provided. The novelty of the proposed BSTT model is evident, particularly in its approach to integrating Bayesian inference within a spatial-temporal context. Reproducibility is somewhat supported through the reported benchmarks and ablation studies, although additional details regarding implementation and dataset handling would enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in sleep stage classification through the introduction of the BSTT model. Its superior performance against established baselines, combined with a novel methodological approach, marks it as a valuable contribution to the field. However, a deeper exploration of practical considerations such as computational efficiency would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"BSTT: A Bayesian Spatial-Temporal Transformer for Sleep Staging\" introduces a novel model designed for sleep stage classification using a Bayesian framework. The authors propose a Bayesian Spatial-Temporal Transformer (BSTT) that integrates spatial and temporal features to enhance performance in sleep staging tasks. The methodology involves a comprehensive analysis of sleep data, leveraging a Bayesian relation inference component to improve prediction accuracy. The findings demonstrate that BSTT outperforms existing methods on standard datasets, showcasing its potential for practical applications in sleep medicine.\n\n# Strength And Weaknesses\nThe primary strengths of this paper lie in its innovative approach and the empirical results demonstrating the effectiveness of the proposed model. The incorporation of a Bayesian framework is particularly noteworthy, as it adds a layer of uncertainty quantification that is often lacking in similar studies. However, the paper does have weaknesses, including a lack of clarity in certain sections and an overuse of jargon that may alienate some readers. Additionally, the presentation of results could be improved by ensuring smoother transitions and clearer references to figures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by long sentences and complex terminology without sufficient explanations, which may affect comprehension for readers less familiar with the domain. While the contributions are significant, the writing quality requires attention to enhance overall readability. In terms of novelty, the Bayesian approach to spatial-temporal modeling is a fresh perspective in the context of sleep staging, suggesting potential for broader applications. However, reproducibility could be improved by providing clearer guidelines and details regarding the implementation of the model.\n\n# Summary Of The Review\nOverall, the paper presents a promising method for sleep staging using a Bayesian Spatial-Temporal Transformer, with strong empirical results supporting its efficacy. However, issues related to clarity and presentation detract from the overall impact of the work. Addressing these concerns would significantly enhance the paper's quality and accessibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.443552623780234,
    -1.67630001899338,
    -1.8040498934405753,
    -1.5440435093189864,
    -1.8287657430984992,
    -1.6020732302260698,
    -1.6196945990310465,
    -1.8537944509411102,
    -1.606667748753385,
    -1.8522056137300071,
    -1.596856946468398,
    -1.4326771194906645,
    -1.604395250436548,
    -1.474905793048353,
    -1.6023531696769002,
    -1.6782737134045016,
    -1.978237224521672,
    -1.7185856461132665,
    -1.7121760761473608,
    -1.6600340240238578,
    -1.9612034070608586,
    -1.629886179434312,
    -1.7546870121714286,
    -1.6767821592644845,
    -1.6093209612459374,
    -1.9643219111952794,
    -1.6297113366856448,
    -1.6488134496500915,
    -1.6177452975438553
  ],
  "logp_cond": [
    [
      0.0,
      -1.9987157392508033,
      -2.0244597123653993,
      -2.0092659324763784,
      -1.9801351936232652,
      -2.062118744999091,
      -2.0718743294573505,
      -2.0153108494373746,
      -1.9992615978518888,
      -2.0093795365683933,
      -1.993428894739825,
      -2.114567187851282,
      -1.9967537377290574,
      -1.9840175246544163,
      -2.012943733683355,
      -2.0349498592263124,
      -2.0137195435096658,
      -2.0216562992925233,
      -2.0449610043444233,
      -1.9952921653533804,
      -2.0210091473747847,
      -2.0349876138486542,
      -2.0078575663330325,
      -2.0356465358379086,
      -2.0929034640278306,
      -2.0220311872289107,
      -2.0286052634080827,
      -2.0226816483002468,
      -2.02558409873605
    ],
    [
      -1.309158082936427,
      0.0,
      -1.2311639760210202,
      -1.2065525719235672,
      -1.1483765660840324,
      -1.228478934770431,
      -1.2743589680724219,
      -1.2038546649491422,
      -1.1255195473407356,
      -1.246125111520417,
      -1.1126349917995162,
      -1.3696002610001354,
      -1.2146881171356072,
      -1.2270514273641084,
      -1.1586615659294959,
      -1.1942010357364705,
      -1.1915654607920916,
      -1.2103840164857675,
      -1.2383116971472206,
      -1.1677704922057484,
      -1.1664975205602788,
      -1.2454736853198178,
      -1.2578543597060559,
      -1.2330208382183956,
      -1.3323262660010313,
      -1.2442729023279855,
      -1.2474545563741943,
      -1.1993703098396846,
      -1.2284456692543468
    ],
    [
      -1.4667527582831794,
      -1.3758673960439722,
      0.0,
      -1.3160442613944623,
      -1.3851143370105408,
      -1.4033368642893456,
      -1.4006224647285586,
      -1.3319516690744262,
      -1.3929350288755133,
      -1.3414036250208745,
      -1.3796691335825682,
      -1.5277698932903105,
      -1.3645053454952532,
      -1.4112971667507181,
      -1.4074641925568778,
      -1.364676365610981,
      -1.33947344290132,
      -1.3287024388153597,
      -1.351468996464273,
      -1.3717204236776666,
      -1.3649123870575477,
      -1.4167432112436076,
      -1.4216137765291597,
      -1.3704580025563384,
      -1.4344543177543425,
      -1.3706499790208575,
      -1.410626619574748,
      -1.3955059134211742,
      -1.398753345435046
    ],
    [
      -1.2284450278082923,
      -1.1098364349396383,
      -1.0385345302268072,
      0.0,
      -1.1223743251568674,
      -1.1708059854455848,
      -1.1826751918050975,
      -1.0831570953826823,
      -1.0010401740966746,
      -1.1575090465605298,
      -1.0917446598915181,
      -1.285332238931553,
      -1.0520752551606,
      -1.1001492565121431,
      -1.075674029472546,
      -1.0712682546532597,
      -1.0862190315181333,
      -1.1225493571280354,
      -1.0731885188869223,
      -1.04090680578986,
      -1.1141891861404065,
      -1.133876583305201,
      -1.1849549607046261,
      -1.1134960870665433,
      -1.2289037551796227,
      -1.1276295853022142,
      -1.15042949235224,
      -1.114348820029342,
      -1.1335408557113633
    ],
    [
      -1.3848583977145077,
      -1.2767101761529862,
      -1.3717268915458267,
      -1.3198227809997642,
      0.0,
      -1.3943245880225394,
      -1.4245439045472492,
      -1.2183991859065157,
      -1.3036475155385283,
      -1.3616622359396242,
      -1.2626012705383012,
      -1.51699313366709,
      -1.345340567050448,
      -1.318814909200351,
      -1.308045166234823,
      -1.384696235011084,
      -1.2839283742049072,
      -1.3533253565966086,
      -1.3781785289089368,
      -1.364455518359678,
      -1.3623764601275843,
      -1.3989993370129308,
      -1.4295917258552264,
      -1.4338641453877172,
      -1.462618156954292,
      -1.3425316573322548,
      -1.416618710859985,
      -1.3494270582423729,
      -1.365485988495239
    ],
    [
      -1.290962952702651,
      -1.1797392105780145,
      -1.1995694792665834,
      -1.1895734395882887,
      -1.1831737279093757,
      0.0,
      -1.239599210353807,
      -1.2063795694208013,
      -1.1822849881838589,
      -1.209188972022579,
      -1.1819174733687592,
      -1.3131776134256647,
      -1.1160013327722655,
      -1.1982579698026057,
      -1.219031985000387,
      -1.1857824861791797,
      -1.2340168309240787,
      -1.1668267966226893,
      -1.158129915249066,
      -1.2006150402311278,
      -1.2531145091596099,
      -1.2226640670589235,
      -1.2837873319003585,
      -1.264804366187679,
      -1.2896320354241428,
      -1.22171829799912,
      -1.2258534907201817,
      -1.1976051055067052,
      -1.2530862308735418
    ],
    [
      -1.2967629400426721,
      -1.2085740378555598,
      -1.1614970656305987,
      -1.1904586332333074,
      -1.2153190646385716,
      -1.2380910185404892,
      0.0,
      -1.1951515636415284,
      -1.2167308559127175,
      -1.1962532756240738,
      -1.2315568736101798,
      -1.3185680626758942,
      -1.183620633126148,
      -1.2474120512802516,
      -1.2105105321146696,
      -1.19210259682556,
      -1.204468481059792,
      -1.1763157764491627,
      -1.2012653193764464,
      -1.1824602077279254,
      -1.2564085511307246,
      -1.2098162421232317,
      -1.2589635582964804,
      -1.2126684885007275,
      -1.3051082883533283,
      -1.1985887412003788,
      -1.1996612358133287,
      -1.2436925605757414,
      -1.2277486597465952
    ],
    [
      -1.5403588707804172,
      -1.4621029173711413,
      -1.4497706080555364,
      -1.4236277567409816,
      -1.4077992475400802,
      -1.5060779916170333,
      -1.5334990022952393,
      0.0,
      -1.4383480810719813,
      -1.4446671702140763,
      -1.4469180187815678,
      -1.6358952605226535,
      -1.429759425105715,
      -1.470275632267059,
      -1.4638377947000443,
      -1.486271825730119,
      -1.436131543287216,
      -1.4655635655474875,
      -1.4634822872281983,
      -1.4724350709116258,
      -1.4067772550177557,
      -1.4804281596300763,
      -1.4650685720169687,
      -1.476415351425933,
      -1.5798907559728617,
      -1.5085951978364105,
      -1.5369353807212383,
      -1.4553828988519157,
      -1.5233969485718513
    ],
    [
      -1.219450499878181,
      -1.115651451199142,
      -1.191927698491529,
      -1.1334498976380123,
      -1.1569323588362599,
      -1.1929693848666938,
      -1.2138956978294053,
      -1.1022674326374422,
      0.0,
      -1.149190378337041,
      -1.13833957936314,
      -1.3080552231411868,
      -1.1612342799910964,
      -1.1513360676631703,
      -1.1189327844292727,
      -1.115361470709539,
      -1.0941446812889237,
      -1.1697810554884291,
      -1.1814847203731897,
      -1.1098773437643856,
      -1.1312503281717055,
      -1.139548240921182,
      -1.2326096582123902,
      -1.1640988267267536,
      -1.2634732377241844,
      -1.1551155067124206,
      -1.2163705773100488,
      -1.1370989644815004,
      -1.162573640977783
    ],
    [
      -1.5029606478054567,
      -1.4279773326602505,
      -1.4296228306391259,
      -1.4564311100746516,
      -1.4335000852036341,
      -1.4980620242517149,
      -1.492866383604799,
      -1.4335013307618112,
      -1.4249480495075,
      0.0,
      -1.422535844900961,
      -1.5935279655095562,
      -1.4404687545662789,
      -1.4478556769740063,
      -1.406083477235642,
      -1.4795759669393922,
      -1.433124890830183,
      -1.4377797239911703,
      -1.4401198691201451,
      -1.4731729383734233,
      -1.4687190171683628,
      -1.4761809615600443,
      -1.5118700376488006,
      -1.4402977341932528,
      -1.536522068408747,
      -1.4466537012242116,
      -1.4574222951091818,
      -1.4531561624296827,
      -1.492515586025083
    ],
    [
      -1.2221266413897682,
      -1.0485943558490012,
      -1.1491723915347343,
      -1.1124168171500715,
      -1.0416094224244432,
      -1.158592650844878,
      -1.219159664231136,
      -1.0887667150809823,
      -1.1070416719370721,
      -1.154698922975023,
      0.0,
      -1.3082478543552458,
      -1.102973796503903,
      -1.141032632358233,
      -1.057704265996056,
      -1.1351807312237656,
      -1.1296663633701083,
      -1.1364680783743286,
      -1.134067855331442,
      -1.1229558392693892,
      -1.1858694201446167,
      -1.151577658800574,
      -1.240468450499338,
      -1.1917806022992388,
      -1.252169216502311,
      -1.1929258492501689,
      -1.173034028063197,
      -1.099261283226059,
      -1.157635910235381
    ],
    [
      -1.2958988425337514,
      -1.280116763054155,
      -1.265207580301834,
      -1.268663124719304,
      -1.2800517673136373,
      -1.2713561605836603,
      -1.279972340532855,
      -1.2764014134452282,
      -1.2927524095673384,
      -1.2707276572057107,
      -1.2689513305756666,
      0.0,
      -1.2840159987238973,
      -1.2797525397130907,
      -1.2572174413086674,
      -1.2584451675239314,
      -1.2617171608921989,
      -1.28767602311813,
      -1.2761517029173814,
      -1.2721413499569512,
      -1.2802786646770692,
      -1.2695366817946436,
      -1.293459672848244,
      -1.2701210817792896,
      -1.261555059740624,
      -1.2696156221716568,
      -1.249005590158225,
      -1.275837775527509,
      -1.2586079533432608
    ],
    [
      -1.2198354267756237,
      -1.1111035048738407,
      -1.1575462315030467,
      -1.1398236469577312,
      -1.142306450568272,
      -1.1019237288105972,
      -1.1230267548081958,
      -1.0390312377115558,
      -1.108862334517302,
      -1.1527685159605663,
      -1.0873471967881025,
      -1.316035192511163,
      0.0,
      -1.1523026797816198,
      -1.157984939208927,
      -1.1346565818960705,
      -1.131003511023575,
      -1.1502342023067789,
      -1.0867246286377774,
      -1.157287994040086,
      -1.1290526119304556,
      -1.1467281960781803,
      -1.1824404752157824,
      -1.2131749835578982,
      -1.2795682621401596,
      -1.1376132389087632,
      -1.185707134491705,
      -1.1750196947958302,
      -1.1903916102035288
    ],
    [
      -1.1107617137663224,
      -1.0233898196765578,
      -1.0261044344056205,
      -0.9844133807837964,
      -0.9780851500259009,
      -1.0999649709912118,
      -1.062016761457351,
      -1.0153165672899787,
      -1.0350603756318326,
      -1.0555607069862696,
      -1.0236640085652176,
      -1.1972731443375164,
      -1.0136879503493355,
      0.0,
      -1.0167074955465634,
      -1.072899737675324,
      -1.0699054846335492,
      -1.0430366420626787,
      -1.0337563158924021,
      -1.0443993613912985,
      -1.0827021352990978,
      -1.0816646047083378,
      -1.109247076604726,
      -1.0425931618912379,
      -1.149670125407057,
      -1.0163026844008571,
      -1.0493409041528425,
      -1.0407894205016957,
      -1.0917519583980126
    ],
    [
      -1.3169010479491055,
      -1.15806095237721,
      -1.2234984248400824,
      -1.1639147637776437,
      -1.1656341117681328,
      -1.2479258493822762,
      -1.2670618742938,
      -1.1780772320587738,
      -1.1498432071697124,
      -1.1734881046140953,
      -1.124332635464887,
      -1.3366339345977312,
      -1.2158997913909295,
      -1.1833374710064257,
      0.0,
      -1.1847936261203378,
      -1.2017670055734337,
      -1.203175186042136,
      -1.2341673455871696,
      -1.1720621836339118,
      -1.1782258027564527,
      -1.1985388597358362,
      -1.2517184976491222,
      -1.1841872204968396,
      -1.318382840212264,
      -1.2016203684782527,
      -1.232559792937325,
      -1.1875780415045862,
      -1.1859652877008728
    ],
    [
      -1.323284028774788,
      -1.2216438742570968,
      -1.2172878227732624,
      -1.178176823938984,
      -1.2169911783101612,
      -1.2597955282619389,
      -1.297135238468851,
      -1.2344898781320004,
      -1.1773675463942508,
      -1.2605083605264065,
      -1.2032731374652201,
      -1.3785854171993914,
      -1.2441887288561004,
      -1.2784557096905547,
      -1.2124367392252025,
      0.0,
      -1.1935451083344966,
      -1.2373507086573692,
      -1.2281430563504419,
      -1.1978316504655437,
      -1.208147262864787,
      -1.2620647982220958,
      -1.2843293028127005,
      -1.2638012225503192,
      -1.3496564079907858,
      -1.222994430280477,
      -1.2928592893626054,
      -1.2206308996561614,
      -1.2155537743295906
    ],
    [
      -1.6057759761726969,
      -1.4955081994795754,
      -1.5324987234211227,
      -1.49614089459742,
      -1.5191556882783273,
      -1.632319472902883,
      -1.6378465137462959,
      -1.5038868206241327,
      -1.5219516009415857,
      -1.580346417795778,
      -1.5168738591761106,
      -1.6973561774596442,
      -1.5903064793592152,
      -1.5716726123195737,
      -1.563878592668258,
      -1.6063323114449994,
      0.0,
      -1.5462035054553405,
      -1.5502003311794434,
      -1.5555884913064548,
      -1.5932217106948912,
      -1.597113402954286,
      -1.6490839023168922,
      -1.5636270752541177,
      -1.6371226892235256,
      -1.5813570521589886,
      -1.5902870317865696,
      -1.5877964198257832,
      -1.5757298753083682
    ],
    [
      -1.3803146676408211,
      -1.3070064369581984,
      -1.302866046901601,
      -1.2908607192576396,
      -1.2860759845783913,
      -1.35077377642197,
      -1.369749211800599,
      -1.2594336812067322,
      -1.2857153324426103,
      -1.2761946498239898,
      -1.296332245849628,
      -1.4444793471808481,
      -1.3209122686462387,
      -1.3479308127381033,
      -1.3096714765372446,
      -1.3324982456154069,
      -1.3007056473479828,
      0.0,
      -1.2932634611130678,
      -1.2676556521620748,
      -1.317185084448397,
      -1.3019134969206831,
      -1.3754703710572154,
      -1.2665926711550668,
      -1.4235922224299038,
      -1.310341739821936,
      -1.3225439763223235,
      -1.3211908194984665,
      -1.3373426172084595
    ],
    [
      -1.3573227044755583,
      -1.2923257521589353,
      -1.3031408025487787,
      -1.324175624356932,
      -1.2996578996939325,
      -1.3080472526828357,
      -1.3565935971295318,
      -1.249104109385526,
      -1.3061030559467492,
      -1.271853811722829,
      -1.2892609549437788,
      -1.4612577650793859,
      -1.2762089368772507,
      -1.3778829499219314,
      -1.304076646803203,
      -1.297366682705357,
      -1.239025598831418,
      -1.3234622902738917,
      0.0,
      -1.2987804931191562,
      -1.3171633051306715,
      -1.2868396034693739,
      -1.3383701418357978,
      -1.3451232984060952,
      -1.3709460970716236,
      -1.3550050627482486,
      -1.3217989715184295,
      -1.3388438621003675,
      -1.2997297408378117
    ],
    [
      -1.2857946266080735,
      -1.179701266974697,
      -1.195297369157413,
      -1.164262258381098,
      -1.2167822291055685,
      -1.279857530539665,
      -1.264205665276367,
      -1.2161499150145942,
      -1.1565471648461918,
      -1.248442309434199,
      -1.1431202861849734,
      -1.4019105436021686,
      -1.2246089483751401,
      -1.2174220411599548,
      -1.1682993864340345,
      -1.184506990631481,
      -1.2312552542271389,
      -1.2175097511396005,
      -1.2153685381058608,
      0.0,
      -1.2431219744305775,
      -1.2264233557061073,
      -1.27935064747574,
      -1.2027816125128814,
      -1.319401023400295,
      -1.1495855228543836,
      -1.221264535784688,
      -1.2143700685600722,
      -1.225142545432141
    ],
    [
      -1.6357692161432915,
      -1.5602847260356232,
      -1.5972969261804126,
      -1.595595615395502,
      -1.5851765364291868,
      -1.6326781257659102,
      -1.6682475170083113,
      -1.5118698620773106,
      -1.5687969330370888,
      -1.616499851476651,
      -1.581588325676522,
      -1.7159699006660363,
      -1.5661634701551341,
      -1.5990793735253068,
      -1.6041511901434335,
      -1.6157379610000815,
      -1.6008702825639043,
      -1.5871486249008147,
      -1.6111613457315974,
      -1.5852896984036564,
      0.0,
      -1.636436680883319,
      -1.6048088974798935,
      -1.6231195128921645,
      -1.6879780029223492,
      -1.633875457936254,
      -1.651063746508295,
      -1.6153014400868153,
      -1.6259821294133256
    ],
    [
      -1.253099219691812,
      -1.2161132957920093,
      -1.2353829975696913,
      -1.2332088851662926,
      -1.2044601708747436,
      -1.2639618576184524,
      -1.2761492319832222,
      -1.1637814042760808,
      -1.2253966986379778,
      -1.2411684056893955,
      -1.208367438926243,
      -1.3421073252500133,
      -1.1825824955357551,
      -1.234444049603687,
      -1.1902730463494164,
      -1.2143174635033254,
      -1.2076272425903438,
      -1.1999838135974223,
      -1.2373809542130925,
      -1.2046147790467985,
      -1.2130543620999976,
      0.0,
      -1.2818626562994786,
      -1.228055195331793,
      -1.340880883845566,
      -1.2432031410364195,
      -1.22855196705282,
      -1.201565125572546,
      -1.22558128532976
    ],
    [
      -1.4085565935025313,
      -1.3406718657413255,
      -1.35235543668984,
      -1.3376958388571614,
      -1.330560108347905,
      -1.3787778671243331,
      -1.4226723075959002,
      -1.2378632246032484,
      -1.3326544593388798,
      -1.3114492790972794,
      -1.3183715979099107,
      -1.4796894889878747,
      -1.2969938994048094,
      -1.3410648597073944,
      -1.3276030858383958,
      -1.3688544263447975,
      -1.3655469999885257,
      -1.382132991751243,
      -1.374932705599001,
      -1.3393772891208688,
      -1.274886090006798,
      -1.3774444167471223,
      0.0,
      -1.3734176779955374,
      -1.431049292344412,
      -1.321788205256361,
      -1.3334577173753261,
      -1.3301416626906781,
      -1.3734227105194636
    ],
    [
      -1.3583104569459348,
      -1.3041264584139647,
      -1.304855134917185,
      -1.3150167889237896,
      -1.3224796497474542,
      -1.3544019003117547,
      -1.344542218313786,
      -1.2886514240490434,
      -1.3270127793853113,
      -1.3047951288353978,
      -1.320281372645948,
      -1.4250929295751882,
      -1.322929894490903,
      -1.3113486671219365,
      -1.300661325541832,
      -1.3373232785342037,
      -1.2693487612360719,
      -1.3109102459154813,
      -1.314442637570092,
      -1.2450188131558086,
      -1.2610941718389257,
      -1.3228666137477894,
      -1.348273118938566,
      0.0,
      -1.3865026744501154,
      -1.2543871785328369,
      -1.3256513703478154,
      -1.303440067990113,
      -1.3058442652045992
    ],
    [
      -1.2955828717553932,
      -1.2301146772292313,
      -1.2255117713225865,
      -1.2335568575104419,
      -1.2297831046011092,
      -1.2577988580917252,
      -1.3193096156699375,
      -1.2305799820803407,
      -1.2409310051913918,
      -1.2153508609956623,
      -1.233030282140427,
      -1.328832681671946,
      -1.256154969998256,
      -1.2432087642137373,
      -1.2656380600397938,
      -1.282282469421316,
      -1.1960673755217233,
      -1.2538989116599906,
      -1.2457504823255197,
      -1.235266301657192,
      -1.277452630019228,
      -1.269339530266919,
      -1.2858248732325004,
      -1.2593338621199877,
      0.0,
      -1.2265178875903449,
      -1.2473151186430322,
      -1.2822551962089477,
      -1.28206652338651
    ],
    [
      -1.5768600612340435,
      -1.5246576453012113,
      -1.550009878523821,
      -1.547223949202666,
      -1.536386271080011,
      -1.584131739279128,
      -1.5809918498852187,
      -1.5264020855190823,
      -1.5409556425981892,
      -1.5389572533019265,
      -1.5752876835792624,
      -1.6560937925447068,
      -1.584284148101008,
      -1.5308601573831864,
      -1.5312799574931493,
      -1.5635992834611245,
      -1.492022714255052,
      -1.572424948667018,
      -1.593189516203871,
      -1.5197928321568999,
      -1.4873027769354805,
      -1.5952213647493498,
      -1.563837554072363,
      -1.5140799147597936,
      -1.6258616160945614,
      0.0,
      -1.5739608554165991,
      -1.5625840354656748,
      -1.5572238404415604
    ],
    [
      -1.2908937304459942,
      -1.2904918855843184,
      -1.266591480716559,
      -1.2625318907673702,
      -1.2674913449268264,
      -1.325571829748958,
      -1.26743981830037,
      -1.239035515126158,
      -1.2734914641312265,
      -1.2423880492635984,
      -1.2501745945930556,
      -1.335706512535971,
      -1.2720280780500763,
      -1.2563312836788683,
      -1.294187555174762,
      -1.2789799228013676,
      -1.2278591303684998,
      -1.276276050332692,
      -1.258159403636031,
      -1.2705399910025166,
      -1.2630259533693706,
      -1.2711137308469143,
      -1.2603848217382414,
      -1.2369796870039276,
      -1.3134092190720668,
      -1.2722457018211208,
      0.0,
      -1.2723314319696175,
      -1.2957186703562085
    ],
    [
      -1.3315381301317166,
      -1.2536082277221374,
      -1.3118734106679788,
      -1.2543798449436527,
      -1.2249610699243583,
      -1.3028735316613775,
      -1.338153357162638,
      -1.2066213737895835,
      -1.257812198789185,
      -1.2692150973498089,
      -1.2461102247022922,
      -1.4127637351911198,
      -1.3218763860158442,
      -1.2703403071912132,
      -1.2358982285083646,
      -1.2758295584286239,
      -1.2655213308161337,
      -1.313788261478255,
      -1.2604152768663508,
      -1.2466912253125988,
      -1.2787684982194123,
      -1.260227567926435,
      -1.310054932772852,
      -1.2757245005996436,
      -1.3673744387152689,
      -1.2778870848950112,
      -1.3368597912726068,
      0.0,
      -1.2202909640704207
    ],
    [
      -1.2627675989159726,
      -1.2035302168544129,
      -1.2507485742579183,
      -1.2009560124854135,
      -1.2041354602782461,
      -1.2741846463034021,
      -1.279381089766023,
      -1.1966534193808371,
      -1.1996340492185567,
      -1.226796801738595,
      -1.1866686226583678,
      -1.333836488977468,
      -1.2222049868932239,
      -1.225579728117035,
      -1.204215556153316,
      -1.233631473328489,
      -1.1575274012194086,
      -1.245493595412782,
      -1.2495789404730833,
      -1.170231533311409,
      -1.2118704730315217,
      -1.116172670338324,
      -1.2754309086318514,
      -1.2157865233720413,
      -1.3332497558395697,
      -1.1980588885271235,
      -1.2558150597072062,
      -1.1739653922858695,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.4448368845294308,
      0.41909291141483473,
      0.43428669130385567,
      0.4634174301569689,
      0.3814338787811429,
      0.3716782943228836,
      0.4282417743428595,
      0.44429102592834524,
      0.4341730872118408,
      0.450123729040409,
      0.3289854359289519,
      0.4467988860511767,
      0.4595350991258178,
      0.4306088900968792,
      0.40860276455392164,
      0.4298330802705683,
      0.42189632448771075,
      0.3985916194358108,
      0.4482604584268537,
      0.4225434764054494,
      0.40856500993157985,
      0.4356950574472016,
      0.4079060879423255,
      0.3506491597524035,
      0.42152143655132335,
      0.4149473603721514,
      0.4208709754799873,
      0.41796852504418425
    ],
    [
      0.367141936056953,
      0.0,
      0.44513604297235987,
      0.46974744706981286,
      0.5279234529093477,
      0.4478210842229491,
      0.4019410509209582,
      0.47244535404423793,
      0.5507804716526445,
      0.43017490747296305,
      0.5636650271938639,
      0.30669975799324467,
      0.4616119018577729,
      0.44924859162927167,
      0.5176384530638842,
      0.4820989832569096,
      0.4847345582012885,
      0.46591600250761256,
      0.43798832184615955,
      0.5085295267876317,
      0.5098024984331013,
      0.43082633367356227,
      0.41844565928732425,
      0.4432791807749845,
      0.3439737529923488,
      0.4320271166653946,
      0.42884546261918577,
      0.4769297091536955,
      0.44785434973903326
    ],
    [
      0.3372971351573959,
      0.4281824973966031,
      0.0,
      0.488005632046113,
      0.41893555643003455,
      0.40071302915122975,
      0.40342742871201676,
      0.4720982243661491,
      0.41111486456506197,
      0.4626462684197008,
      0.42438075985800716,
      0.27628000015026477,
      0.43954454794532216,
      0.39275272668985717,
      0.3965857008836975,
      0.43937352782959427,
      0.46457645053925534,
      0.47534745462521566,
      0.4525808969763023,
      0.43232946976290876,
      0.4391375063830276,
      0.38730668219696773,
      0.38243611691141566,
      0.4335918908842369,
      0.3695955756862328,
      0.4333999144197178,
      0.3934232738658272,
      0.40854398001940107,
      0.4052965480055293
    ],
    [
      0.31559848151069403,
      0.434207074379348,
      0.5055089790921792,
      0.0,
      0.42166918416211896,
      0.3732375238734016,
      0.36136831751388887,
      0.46088641393630403,
      0.5430033352223118,
      0.3865344627584566,
      0.45229884942746823,
      0.25871127038743347,
      0.4919682541583863,
      0.44389425280684325,
      0.4683694798464404,
      0.4727752546657267,
      0.4578244778008531,
      0.421494152190951,
      0.470854990432064,
      0.5031367035291263,
      0.42985432317857986,
      0.41016692601378546,
      0.35908854861436024,
      0.430547422252443,
      0.3151397541393637,
      0.41641392401677213,
      0.39361401696674636,
      0.42969468928964427,
      0.41050265360762306
    ],
    [
      0.44390734538399146,
      0.5520555669455129,
      0.45703885155267243,
      0.5089429620987349,
      0.0,
      0.4344411550759597,
      0.40422183855124993,
      0.6103665571919834,
      0.5251182275599708,
      0.46710350715887494,
      0.566164472560198,
      0.31177260943140905,
      0.48342517604805124,
      0.5099508338981482,
      0.5207205768636762,
      0.4440695080874151,
      0.5448373688935919,
      0.4754403865018906,
      0.4505872141895624,
      0.4643102247388211,
      0.46638928297091486,
      0.4297664060855684,
      0.3991740172432727,
      0.3949015977107819,
      0.36614758614420717,
      0.4862340857662444,
      0.4121470322385141,
      0.4793386848561263,
      0.4632797546032601
    ],
    [
      0.3111102775234189,
      0.4223340196480554,
      0.40250375095948643,
      0.4124997906377812,
      0.41889950231669415,
      0.0,
      0.3624740198722629,
      0.3956936608052686,
      0.419788242042211,
      0.3928842582034908,
      0.42015575685731066,
      0.2888956168004051,
      0.4860718974538043,
      0.40381526042346416,
      0.38304124522568284,
      0.41629074404689015,
      0.3680563993019912,
      0.43524643360338056,
      0.4439433149770038,
      0.4014581899949421,
      0.34895872106646,
      0.37940916316714635,
      0.3182858983257113,
      0.33726886403839074,
      0.312441194801927,
      0.38035493222694994,
      0.37621973950588816,
      0.40446812471936466,
      0.3489869993525281
    ],
    [
      0.3229316589883744,
      0.4111205611754867,
      0.4581975334004478,
      0.42923596579773915,
      0.40437553439247487,
      0.3816035804905573,
      0.0,
      0.4245430353895181,
      0.40296374311832905,
      0.4234413234069727,
      0.38813772542086666,
      0.3011265363551523,
      0.43607396590489844,
      0.37228254775079495,
      0.4091840669163769,
      0.42759200220548643,
      0.4152261179712544,
      0.44337882258188377,
      0.4184292796546001,
      0.43723439130312114,
      0.36328604790032193,
      0.4098783569078148,
      0.3607310407345661,
      0.407026110530319,
      0.31458631067771825,
      0.4211058578306677,
      0.4200333632177178,
      0.3760020384553051,
      0.3919459392844513
    ],
    [
      0.31343558016069295,
      0.3916915335699689,
      0.4040238428855738,
      0.43016669420012854,
      0.44599520340102994,
      0.3477164593240769,
      0.3202954486458709,
      0.0,
      0.41544636986912886,
      0.40912728072703386,
      0.4068764321595424,
      0.21789919041845662,
      0.4240350258353951,
      0.38351881867405124,
      0.3899566562410659,
      0.3675226252109911,
      0.41766290765389424,
      0.3882308853936227,
      0.3903121637129119,
      0.38135938002948433,
      0.4470171959233544,
      0.3733662913110338,
      0.38872587892414145,
      0.3773790995151771,
      0.2739036949682485,
      0.3451992531046997,
      0.3168590702198719,
      0.39841155208919443,
      0.3303975023692589
    ],
    [
      0.3872172488752039,
      0.4910162975542429,
      0.4147400502618559,
      0.4732178511153726,
      0.44973538991712503,
      0.4136983638866911,
      0.3927720509239796,
      0.5044003161159427,
      0.0,
      0.45747737041634395,
      0.468328169390245,
      0.2986125256121981,
      0.4454334687622885,
      0.4553316810902146,
      0.48773496432411223,
      0.49130627804384597,
      0.5125230674644612,
      0.43688669326495577,
      0.42518302838019517,
      0.4967904049889993,
      0.47541742058167946,
      0.467119507832203,
      0.3740580905409947,
      0.44256892202663134,
      0.3431945110292005,
      0.45155224204096434,
      0.3902971714433361,
      0.46956878427188453,
      0.444094107775602
    ],
    [
      0.34924496592455045,
      0.42422828106975663,
      0.42258278309088126,
      0.3957745036553555,
      0.418705528526373,
      0.3541435894782923,
      0.3593392301252081,
      0.41870428296819595,
      0.42725756422250716,
      0.0,
      0.42966976882904606,
      0.2586776482204509,
      0.41173685916372826,
      0.40434993675600084,
      0.44612213649436505,
      0.37262964679061494,
      0.4190807228998241,
      0.4144258897388369,
      0.412085744609862,
      0.3790326753565838,
      0.3834865965616443,
      0.37602465216996284,
      0.34033557608120657,
      0.41190787953675434,
      0.3156835453212601,
      0.4055519125057956,
      0.3947833186208254,
      0.39904945130032443,
      0.3596900277049242
    ],
    [
      0.3747303050786297,
      0.5482625906193968,
      0.44768455493366366,
      0.4844401293183265,
      0.5552475240439547,
      0.4382642956235199,
      0.37769728223726196,
      0.5080902313874156,
      0.4898152745313258,
      0.44215802349337485,
      0.0,
      0.2886090921131521,
      0.4938831499644949,
      0.455824314110165,
      0.5391526804723419,
      0.4616762152446323,
      0.46719058309828965,
      0.4603888680940693,
      0.4627890911369559,
      0.4739011071990087,
      0.4109875263237812,
      0.445279287667824,
      0.35638849596905997,
      0.40507634416915916,
      0.3446877299660869,
      0.40393109721822906,
      0.4238229184052009,
      0.49759566324233884,
      0.4392210362330169
    ],
    [
      0.13677827695691303,
      0.15256035643650945,
      0.16746953918883056,
      0.16401399477136036,
      0.1526253521770271,
      0.16132095890700415,
      0.15270477895780954,
      0.15627570604543628,
      0.13992470992332606,
      0.16194946228495377,
      0.1637257889149979,
      0.0,
      0.14866112076676719,
      0.1529245797775738,
      0.17545967818199704,
      0.17423195196673302,
      0.17095995859846558,
      0.14500109637253455,
      0.156525416573283,
      0.1605357695337133,
      0.15239845481359526,
      0.16314043769602082,
      0.13921744664242053,
      0.16255603771137483,
      0.17112205975004047,
      0.16306149731900765,
      0.18367152933243935,
      0.15683934396315546,
      0.17406916614740364
    ],
    [
      0.3845598236609242,
      0.4932917455627073,
      0.44684901893350126,
      0.46457160347881676,
      0.4620887998682759,
      0.5024715216259508,
      0.48136849562835216,
      0.5653640127249921,
      0.49553291591924586,
      0.4516267344759817,
      0.5170480536484454,
      0.2883600579253849,
      0.0,
      0.45209257065492814,
      0.446410311227621,
      0.4697386685404774,
      0.47339173941297297,
      0.4541610481297691,
      0.5176706217987705,
      0.447107256396462,
      0.4753426385060924,
      0.45766705435836763,
      0.4219547752207655,
      0.39122026687864975,
      0.32482698829638834,
      0.46678201152778476,
      0.418688115944843,
      0.4293755556407177,
      0.41400364023301917
    ],
    [
      0.3641440792820305,
      0.45151597337179505,
      0.4488013586427324,
      0.49049241226455653,
      0.496820643022452,
      0.3749408220571411,
      0.4128890315910019,
      0.45958922575837424,
      0.4398454174165203,
      0.4193450860620833,
      0.45124178448313534,
      0.2776326487108365,
      0.46121784269901744,
      0.0,
      0.45819829750178953,
      0.4020060553730289,
      0.4050003084148037,
      0.43186915098567424,
      0.44114947715595076,
      0.4305064316570544,
      0.39220365774925514,
      0.3932411883400151,
      0.3656587164436269,
      0.43231263115711505,
      0.32523566764129597,
      0.45860310864749576,
      0.42556488889551036,
      0.43411637254665725,
      0.38315383465034025
    ],
    [
      0.28545212172779477,
      0.44429221729969015,
      0.3788547448368178,
      0.43843840589925653,
      0.4367190579087674,
      0.354427320294624,
      0.3352912953831002,
      0.42427593761812643,
      0.4525099625071878,
      0.428865065062805,
      0.4780205342120132,
      0.26571923507916906,
      0.3864533782859707,
      0.4190156986704745,
      0.0,
      0.41755954355656244,
      0.4005861641034665,
      0.3991779836347642,
      0.36818582408973066,
      0.43029098604298843,
      0.4241273669204475,
      0.40381430994106404,
      0.350634672027778,
      0.41816594918006067,
      0.2839703294646363,
      0.4007328011986475,
      0.36979337673957513,
      0.414775128172314,
      0.4163878819760274
    ],
    [
      0.3549896846297136,
      0.45662983914740485,
      0.4609858906312392,
      0.5000968894655176,
      0.46128253509434036,
      0.41847818514256274,
      0.38113847493565056,
      0.4437838352725012,
      0.5009061670102508,
      0.41776535287809513,
      0.47500057593928147,
      0.29968829620511017,
      0.43408498454840116,
      0.39981800371394693,
      0.46583697417929915,
      0.0,
      0.48472860507000504,
      0.4409230047471324,
      0.45013065705405975,
      0.48044206293895786,
      0.4701264505397147,
      0.41620891518240577,
      0.3939444105918011,
      0.41447249085418236,
      0.3286173054137158,
      0.45527928312402466,
      0.3854144240418962,
      0.45764281374834015,
      0.46271993907491105
    ],
    [
      0.3724612483489751,
      0.4827290250420966,
      0.44573850110054924,
      0.482096329924252,
      0.4590815362433447,
      0.345917751618789,
      0.3403907107753761,
      0.47435040389753924,
      0.4562856235800863,
      0.39789080672589394,
      0.4613633653455613,
      0.28088104706202777,
      0.38793074516245674,
      0.4065646122020983,
      0.41435863185341404,
      0.3719049130766725,
      0.0,
      0.4320337190663315,
      0.4280368933422285,
      0.4226487332152171,
      0.3850155138267808,
      0.381123821567386,
      0.3291533222047798,
      0.41461014926755424,
      0.3411145352981464,
      0.39688017236268336,
      0.38795019273510234,
      0.3904408046958887,
      0.4025073492133038
    ],
    [
      0.33827097847244536,
      0.41157920915506807,
      0.41571959921166557,
      0.4277249268556269,
      0.43250966153487513,
      0.3678118696912964,
      0.34883643431266753,
      0.4591519649065343,
      0.4328703136706562,
      0.4423909962892767,
      0.42225340026363845,
      0.27410629893241834,
      0.3976733774670278,
      0.37065483337516314,
      0.4089141695760219,
      0.3860874004978596,
      0.4178799987652837,
      0.0,
      0.4253221850001987,
      0.4509299939511917,
      0.4014005616648695,
      0.41667214919258333,
      0.3431152750560511,
      0.4519929749581997,
      0.2949934236833627,
      0.40824390629133056,
      0.396041669790943,
      0.39739482661479997,
      0.381243028904807
    ],
    [
      0.35485337167180253,
      0.4198503239884255,
      0.40903527359858205,
      0.38800045179042875,
      0.41251817645342825,
      0.40412882346452506,
      0.355582479017829,
      0.4630719667618348,
      0.40607302020061153,
      0.4403222644245317,
      0.422915121203582,
      0.2509183110679749,
      0.4359671392701101,
      0.3342931262254294,
      0.40809942934415777,
      0.41480939344200385,
      0.47315047731594273,
      0.38871378587346905,
      0.0,
      0.41339558302820456,
      0.3950127710166893,
      0.4253364726779869,
      0.37380593431156295,
      0.36705277774126555,
      0.34122997907573716,
      0.35717101339911217,
      0.39037710462893127,
      0.37333221404699324,
      0.4124463353095491
    ],
    [
      0.37423939741578427,
      0.4803327570491609,
      0.4647366548664449,
      0.4957717656427598,
      0.44325179491828925,
      0.3801764934841929,
      0.3958283587474909,
      0.44388410900926356,
      0.503486859177666,
      0.4115917145896588,
      0.5169137378388844,
      0.2581234804216892,
      0.4354250756487177,
      0.442611982863903,
      0.4917346375898233,
      0.4755270333923769,
      0.4287787697967189,
      0.44252427288425733,
      0.44466548591799704,
      0.0,
      0.4169120495932803,
      0.4336106683177505,
      0.3806833765481179,
      0.4572524115109764,
      0.3406330006235627,
      0.5104485011694742,
      0.4387694882391697,
      0.44566395546378557,
      0.43489147859171684
    ],
    [
      0.3254341909175671,
      0.4009186810252354,
      0.36390648088044597,
      0.3656077916653566,
      0.37602687063167184,
      0.3285252812949484,
      0.2929558900525473,
      0.44933354498354805,
      0.3924064740237698,
      0.3447035555842075,
      0.37961508138433664,
      0.24523350639482233,
      0.3950399369057245,
      0.3621240335355518,
      0.35705221691742506,
      0.3454654460607771,
      0.3603331244969543,
      0.37405478216004395,
      0.3500420613292612,
      0.3759137086572022,
      0.0,
      0.32476672617753954,
      0.3563945095809651,
      0.3380838941686941,
      0.2732254041385094,
      0.3273279491246046,
      0.3101396605525637,
      0.3459019669740433,
      0.33522127764753296
    ],
    [
      0.3767869597425,
      0.4137728836423027,
      0.39450318186462074,
      0.39667729426801945,
      0.42542600855956847,
      0.36592432181585965,
      0.35373694745108986,
      0.46610477515823123,
      0.4044894807963342,
      0.38871777374491656,
      0.4215187405080689,
      0.2877788541842987,
      0.4473036838985569,
      0.395442129830625,
      0.4396131330848956,
      0.41556871593098665,
      0.4222589368439682,
      0.4299023658368897,
      0.39250522522121956,
      0.42527140038751354,
      0.4168318173343144,
      0.0,
      0.3480235231348334,
      0.40183098410251894,
      0.2890052955887461,
      0.38668303839789253,
      0.40133421238149203,
      0.4283210538617661,
      0.40430489410455195
    ],
    [
      0.3461304186688974,
      0.4140151464301032,
      0.4023315754815886,
      0.41699117331426727,
      0.4241269038235236,
      0.3759091450470955,
      0.3320147045755284,
      0.5168237875681803,
      0.4220325528325488,
      0.44323773307414926,
      0.436315414261518,
      0.27499752318355397,
      0.4576931127666193,
      0.4136221524640342,
      0.4270839263330328,
      0.38583258582663116,
      0.38914001218290295,
      0.3725540204201856,
      0.3797543065724276,
      0.4153097230505598,
      0.47980092216463066,
      0.3772425954243064,
      0.0,
      0.38126933417589126,
      0.3236377198270166,
      0.43289880691506766,
      0.4212292947961025,
      0.4245453494807505,
      0.381264301651965
    ],
    [
      0.3184717023185497,
      0.3726557008505198,
      0.3719270243472994,
      0.3617653703406949,
      0.35430250951703024,
      0.3223802589527298,
      0.3322399409506984,
      0.38813073521544106,
      0.3497693798791732,
      0.37198703042908665,
      0.35650078661853657,
      0.2516892296892963,
      0.35385226477358156,
      0.36543349214254794,
      0.3761208337226525,
      0.33945888073028074,
      0.4074333980284126,
      0.36587191334900315,
      0.36233952169439254,
      0.43176334610867584,
      0.4156879874255588,
      0.35391554551669513,
      0.32850904032591854,
      0.0,
      0.29027948481436905,
      0.4223949807316476,
      0.35113078891666905,
      0.37334209127437146,
      0.3709378940598853
    ],
    [
      0.31373808949054416,
      0.3792062840167061,
      0.3838091899233509,
      0.3757641037354955,
      0.3795378566448282,
      0.3515221031542122,
      0.29001134557599983,
      0.3787409791655967,
      0.3683899560545456,
      0.3939701002502751,
      0.3762906791055103,
      0.28048827957399136,
      0.35316599124768144,
      0.3661121970322001,
      0.34368290120614353,
      0.3270384918246214,
      0.4132535857242141,
      0.3554220495859468,
      0.3635704789204177,
      0.3740546595887453,
      0.33186833122670945,
      0.33998143097901834,
      0.323496088013437,
      0.3499870991259497,
      0.0,
      0.3828030736555925,
      0.36200584260290514,
      0.3270657650369897,
      0.3272544378594273
    ],
    [
      0.3874618499612359,
      0.43966426589406815,
      0.41431203267145844,
      0.4170979619926134,
      0.4279356401152685,
      0.38019017191615134,
      0.38333006131006075,
      0.43791982567619714,
      0.42336626859709026,
      0.425364657893353,
      0.389034227616017,
      0.30822811865057265,
      0.3800377630942715,
      0.43346175381209306,
      0.43304195370213017,
      0.4007226277341549,
      0.4722991969402275,
      0.39189696252826134,
      0.3711323949914085,
      0.44452907903837957,
      0.4770191342597989,
      0.3691005464459296,
      0.4004843571229164,
      0.4502419964354858,
      0.33846029510071807,
      0.0,
      0.3903610557786803,
      0.4017378757296046,
      0.40709807075371907
    ],
    [
      0.3388176062396506,
      0.33921945110132645,
      0.3631198559690858,
      0.36717944591827467,
      0.36221999175881847,
      0.3041395069366868,
      0.36227151838527494,
      0.39067582155948677,
      0.35621987255441834,
      0.38732328742204647,
      0.37953674209258925,
      0.29400482414967377,
      0.3576832586355685,
      0.37338005300677657,
      0.3355237815108829,
      0.3507314138842772,
      0.401852206317145,
      0.35343528635295285,
      0.37155193304961376,
      0.35917134568312825,
      0.3666853833162742,
      0.35859760583873057,
      0.36932651494740343,
      0.39273164968171725,
      0.31630211761357807,
      0.357465634864524,
      0.0,
      0.3573799047160273,
      0.33399266632943636
    ],
    [
      0.31727531951837484,
      0.39520522192795404,
      0.33694003898211267,
      0.39443360470643873,
      0.4238523797257332,
      0.34593991798871393,
      0.3106600924874534,
      0.442192075860508,
      0.3910012508609064,
      0.3795983523002826,
      0.4027032249477993,
      0.23604971445897172,
      0.32693706363424724,
      0.3784731424588783,
      0.41291522114172685,
      0.3729838912214676,
      0.38329211883395775,
      0.33502518817183646,
      0.38839817278374067,
      0.4021222243374927,
      0.37004495143067917,
      0.3885858817236565,
      0.33875851687723957,
      0.37308894905044787,
      0.2814390109348226,
      0.37092636475508023,
      0.3119536583774847,
      0.0,
      0.42852248557967076
    ],
    [
      0.35497769862788275,
      0.41421508068944246,
      0.366996723285937,
      0.4167892850584418,
      0.4136098372656092,
      0.3435606512404532,
      0.3383642077778324,
      0.4210918781630182,
      0.41811124832529867,
      0.39094849580526025,
      0.4310766748854875,
      0.2839088085663872,
      0.39554031065063144,
      0.39216556942682024,
      0.4135297413905392,
      0.38411382421536633,
      0.46021789632444676,
      0.3722517021310734,
      0.36816635707077205,
      0.44751376423244627,
      0.4058748245123336,
      0.5015726272055314,
      0.34231438891200394,
      0.401958774171814,
      0.2844955417042856,
      0.4196864090167318,
      0.3619302378366491,
      0.44377990525798583,
      0.0
    ]
  ],
  "row_avgs": [
    0.4194769769406025,
    0.454400961964232,
    0.4167465592813245,
    0.4227987041347612,
    0.4668518867982359,
    0.3854127149249254,
    0.39898833777725773,
    0.3748761441620679,
    0.44143842778324177,
    0.3894394542043976,
    0.44631412185338126,
    0.15927587391823908,
    0.45048450165072174,
    0.41883200401861753,
    0.39380490327978074,
    0.4325405732562308,
    0.4032664449555191,
    0.39720662243163796,
    0.39398082572681076,
    0.43530247540403266,
    0.3498483588309232,
    0.3978442011313065,
    0.4024215800826099,
    0.35929611188298993,
    0.35400826394003765,
    0.4069832194914952,
    0.357162095708406,
    0.36568992982420273,
    0.39245580227680293
  ],
  "col_avgs": [
    0.3416949197254104,
    0.42534248105422573,
    0.40759092803500774,
    0.4246368027976928,
    0.42388713791139193,
    0.3714585023050271,
    0.3555296332050479,
    0.44557965842473873,
    0.42795716400148126,
    0.40726139137735384,
    0.43038837942879865,
    0.27835992563097495,
    0.4134017922357382,
    0.3995962140767026,
    0.4213103817461455,
    0.4006327995432253,
    0.42664650825947004,
    0.40440965161500914,
    0.4050890242113178,
    0.4227803071416219,
    0.40975833614390483,
    0.39365309262651565,
    0.358529973144317,
    0.3960814917697536,
    0.3142353919445689,
    0.4040957258874092,
    0.3811195810380793,
    0.4057903064321949,
    0.39033057592166703
  ],
  "combined_avgs": [
    0.3805859483330064,
    0.4398717215092289,
    0.41216874365816614,
    0.423717753466227,
    0.4453695123548139,
    0.37843560861497627,
    0.3772589854911528,
    0.4102279012934033,
    0.4346977958923615,
    0.39835042279087574,
    0.43835125064108993,
    0.21881789977460703,
    0.43194314694323,
    0.4092141090476601,
    0.4075576425129631,
    0.41658668639972807,
    0.4149564766074946,
    0.4008081370233235,
    0.3995349249690643,
    0.42904139127282725,
    0.379803347487414,
    0.3957486468789111,
    0.38047577661346343,
    0.37768880182637177,
    0.3341218279423033,
    0.40553947268945223,
    0.36914083837324263,
    0.3857401181281988,
    0.391393189099235
  ],
  "gppm": [
    577.6678772087978,
    546.8083329348447,
    550.594925171876,
    546.6269615179228,
    545.3295956209757,
    568.558943454379,
    575.6842619148464,
    531.8823736613681,
    545.9056360506514,
    549.8147805819242,
    545.4067977612107,
    600.5863257802305,
    552.3533792306317,
    560.2457241819942,
    546.2178792639174,
    555.525083627552,
    539.9010928741295,
    552.9594144653825,
    553.398571667464,
    546.7329499263275,
    546.3772594354267,
    558.7323818845485,
    572.7036754297881,
    555.5363647074325,
    592.0094068202163,
    549.7723740968984,
    565.0796393691734,
    553.5496037339901,
    560.5167813522378
  ],
  "gppm_normalized": [
    1.3489679904573209,
    1.2927223078708932,
    1.3059113653741399,
    1.2981514329015114,
    1.2887324465814012,
    1.3492572433617815,
    1.3645567778615824,
    1.266146043645104,
    1.292594491717923,
    1.3045210548785993,
    1.2905136620864983,
    1.4484056811335269,
    1.3072310241378389,
    1.3256612257750242,
    1.2993953660831328,
    1.3173615002957821,
    1.2806348090396384,
    1.3121449363415425,
    1.3116944769583017,
    1.2958654101752654,
    1.2981313317788044,
    1.3260693334257128,
    1.3567352803101573,
    1.3219660778599687,
    1.4062215061858154,
    1.3036825668876417,
    1.33860908939803,
    1.313256145683756,
    1.3298254530552203
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422
  ],
  "response_lengths": [
    1447,
    2222,
    2397,
    2594,
    2290,
    2436,
    2446,
    2638,
    2316,
    2396,
    2289,
    3573,
    2292,
    2269,
    2675,
    2372,
    2416,
    2539,
    2464,
    2433,
    2489,
    2392,
    2292,
    2656,
    2430,
    2401,
    2286,
    2368,
    2404
  ]
}