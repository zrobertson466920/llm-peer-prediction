{
  "example_idx": 50,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSKTFORMER: A SKELETON TRANSFORMER FOR LONG SEQUENCE DATA\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nTransformers have become a preferred tool for modeling sequential data. Many studies of using Transformers for long sequence modeling focus on reducing computational complexity. They usually exploit the low-rank structure of data and approximate a long sequence by a sub-sequence. One challenge with such approaches is how to make an appropriate balance between information preservation and noise reduction: the longer the sub-sequence used to approximate the long sequence, the better the information is preserved but at a price of introducing more noise into the model and of course more computational costs. We propose skeleton transformer, SKTformer for short, an efficient transformer architecture that improves upon the previous attempts to negotiate this tradeoff. It introduces two mechanisms to effectively reduce the impact of noise while still keeping the computation linear to the sequence length: a smoothing block to mix information over long sequences and a matrix sketch method that simultaneously selects columns and rows from the input matrix. We verify the effectiveness of SKTformer both theoretically and empirically. Extensive studies over both Long Range Arena (LRA) datasets, and six time-series forecasting show that SKTformer significantly outperforms both vanilla Transformer and other state-of-the-art variants of Transformer. Code is available at https://anonymous.4open.science/r/SKTFormer-B33B/\n\n1\n\nINTRODUCTION\n\nTransformer type models (Vaswani et al., 2017) have achieved many breakthroughs in various artificial intelligence areas, such as natural language processing (NLP) (Brown et al., 2020; Clark et al., 2020; Devlin et al., 2018; Liu et al., 2019), computer vision (CV) (Dosovitskiy et al., 2020; Liu et al., 2021; Touvron et al., 2021; Yuan et al., 2021; Zhou et al., 2021b), and time series forecasting (Xu et al., 2021; Zhou et al., 2022). The self-attention scheme plays a key role in those transformer-based models, which efficiently capture long-term global and short-term local correlations when the length of the token sequence is relatively small. Due to the quadratic complexity of standard self-attention, many approaches have been developed to reduce the computational complexity of Transformer for long sequences (e.g., (Zhu et al., 2021)). Most of them try to exploit the special patterns of attention matrix, such as low-rankness, locality, sparsity, or graph structures. One group of approaches is to build a linear approximation for the softmax operator (e.g., (Chen et al., 2021; Choromanski et al., 2020; Chowdhury et al., 2021; Qin et al., 2021)). Despite the efficiency of the linear approximation, these approximation methods often perform worse than the original softmax based attention. More discussion of efficient transformer for long sequence can be found in the section of related work.\n\nIn this work, we will focus on approaches that assume a low-rank structure of input matrix. They approximate the global information in a long sequence by a sub-sequence (i.e., short sequence) of landmarks, and only compute attention between queries and selected landmarks (e.g., (Ma et al., 2021; Nguyen et al., 2021; Zhu et al., 2021; Zhu & Soricut, 2021)). Although those models enjoy linear computational cost and often better performance than vanilla Transformer, they face one major challenge, i.e., how to balance between information preserving and noise reduction. By choosing a larger number of landmarks, we are able to preserve more global information but at the price of introducing more noise into the sequential model and more computational cost.\n\nIn this work, we propose an efficient Transformer architecture, termed Skeleton Transformer, or SKTformer for short, that introduces two mechanisms to explicitly address the balance. First,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nwe introduce a smoothing block into the Transformer architecture. It effectively mixes global information over the long sequence by the Fourier analysis and local information over the sequence by a convolution kernel. Through the information mixture, we are able to reduce the noise for individual tokens over the sequence, and at the same time, improve their representativeness for the entire sequence. Second, we introduce a matrix sketch technique to approximate the input matrix by a smaller number of rows and columns. A standard self-attention can be seen as reweighing the columns of the value matrix. Important columns are assigned high attention weights and remain in the output matrix, while small attention weights eliminate insignificant columns. The selfattention mechanism is equivalent to column selection if we replace the softmax operator with the corresponding argmax operator. However, sampling only columns may not generate a good summary of the matrix, and could be subjected to noises to individual columns. We address this problem by exploiting CUR (Drineas et al., 2008) or Skeleton approximation technique (Chiu & Demanet, 2013) in the matrix approximation community. Theoretically, for a rank-r matrix X ∈ Rn×d, we can take O(r log d) column samples and O(r log n) row samples to construct a so-called Skeleton approximation X ≈ CU R, where C and R are matrices consisting of the columns and rows of X, respectively, and U is the pseudo-inverse of their intersection. By combing these mechanism, we found, both theoretically and empirically, that SKTformer is able to preserve global information over long sequence and reduce the impact of noise simultaneously, thus leading to better performance than state-of-the-art variants of Transformer for long sequences, without having to sacrifice the linear complexity w.r.t. sequence length.\n\nIn short, we summarize our main contributions as follows:\n\n1. We propose a Skeleton Transformer (SKTformer), an efficient model that integrates a smoother, column attention and row attention components to unfold a randomized linear matrix sketch algorithm.\n\n2. By randomly selecting a fixed number of rows and columns, the proposed model achieves near-linear computational complexity and memory cost. The effectiveness of this selection method is verified both theoretically and empirically.\n\n3. We conduct extensive experiments over Long-term sequence, long-term time series forecasting and GLUE tasks. In particular, the Long Range Arena benchmark (Tay et al., 2021), achieves an average accuracy of 64% and 66% with fixed parameters (suggested setting in Mathieu et al. (2014); Tay et al. (2021)) and fine-tuned parameters respectively. It improves from 62% of the best transformer-type model. Moreover, it also has a comparable performance with the recent state-of-art long-term time series forecasting models for long-term time series forecasting and GLUE tasks\n\nOrganization. We structure the rest of this paper as follows: In Section 2, we briefly review the relevant literature on efficient transformers and Skeleton approximations. Section 3 introduces the model structure and performs a theoretical analysis to justify the proposed model. We empirically verify the efficiency and accuracy of SKTformer in Section 4. we discuss limitations and future directions in Section 5. Technical proofs and experimental details are provided in the appendix.\n\n2 RELATED WORK\n\nThis section provides an overview of the literature focusing on efficient Transformer models. The techniques include sparse or local attention, low-rankness, and kernel approximation. We refer the reader interested in their details to the survey (Tay et al., 2020c).\n\nSparse Attention. The general idea of these methods is restricting the query token to perform attention only within a specific small region, such as its local region or some global tokens. In this setting, the attention matrix becomes sparse compared to the original one. (Qiu et al., 2019) proposes BlockBert, which introduces sparse block structures into the attention matrix by multiplying a masking matrix. (Parmar et al., 2018) applies self-attention within blocks for the image generation task. (Liu et al., 2018) divides a sequence into blocks and uses a stride convolution to reduce the model complexity. However, these block-type Transformers ignore the connections among blocks. To address this issue, Transformer-XL (Dai et al., 2019) and Compressive Transformer (Rae et al., 2019) propose a recurrence mechanism to connect multiple blocks. Transformer-LS (Zhu et al., 2021) combines local attention with a dynamic projection to capture long-term dependence. (Tay et al.,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2020b) uses a meta-sorting network to permute over sequences and quasi-global attention with local windows to improve memory efficiency.\n\nAnother approach in this category is based on stride attention. Longformer (Beltagy et al., 2020) uses dilated sliding windows to obtain a sparse attention matrix. Sparse Transformers (Child et al., 2019) consider approximating a dense attention matrix by several sparse factorization methods. In addition, some methods reduce the complexity by clustering tokens. For example, Reformer (Kitaev et al., 2020b) uses a hash similarity measure to cluster tokens, and Routing Transformer (Roy et al., 2021) uses k-means to cluster tokens. BigBird (Zaheer et al., 2020) proposes a generalized attention mechanism described by a directed graph to reduce attention complexity. (Lee-Thorp et al., 2021) considers using 2D Fourier Transformation to mix the token matrix directly. (Tan et al., 2021) uses max pooling scheme to reduce the computation costs.\n\nLow-rank and Kernel Methods. Inducing low-rankness into the attention matrix can quickly reduce the complexity and the kernel approximation is widely applied in efficient low-rank approximation. Linformer (Wang et al., 2020) and Luna (Ma et al., 2021) approximate softmax with linear functions, which yield a linear time and space complexity. (Choromanski et al., 2020) and (Peng et al., 2021) use random features tricks and reach promising numerical performance. (Winata et al., 2020) proposes Low-Rank Transformer based on matrix factorization. FMMformer (Nguyen et al., 2021) combines the fast multipole method with the kernel method. Synthesizer (Tay et al., 2020a) uses a random low-rank matrix to replace the attention matrix. Nyströmformer (Xiong et al., 2021) adopts the Nyström method to approximate standard self-attention. Linear Transformer (Katharopoulos et al., 2020) expresses self-attention as a linear dot-product of kernel feature maps. (Zhu & Soricut, 2021) applies the Multigrid method to efficiently compute the attention matrix recursively. Cosformer (Qin et al., 2021) develops a cosine-based re-weighting mechanism to linearize the softmax function. (Chen et al., 2021) proposes the Scatterbrain, which unifies locality-sensitive hashing and the kernel method into attention for accurate and efficient approximation.\n\n3 SKTFORMER\n\nWe start by going over the vanilla attention. For a sequence of length n, the vanilla self-attention in the transformer is dot-product type (Vaswani et al., 2017). Following standard notation, the attention matrix A ∈ Rn×n is defined as:\n\nA = softmax\n\nQK⊤\n\n(cid:19)\n\n,\n\n(cid:18) 1 √\nd\n\n(1)\n\nwhere Q ∈ Rn×d denotes the queries while K ∈ Rn×d denotes the keys, and d represents the hidden dimension. By multiplying the attention weights A with the values V ∈ Rn×d, we can calculate the new values as ˆV = AV .\n\nIntuitively, the attention is the weighted average over the old ones, where the weights are defined by the attention matrix A. In this paper, we consider generating Q, K and V via the linear projection of the input token matrix X:\n\nwhere X ∈ Rn×d and WQ, WK, WV ∈ Rd×d.\n\nQ = XWQ, K = XWK, V = XWV ,\n\nThe vanilla procedure has two drawbacks in concentrating the information from V . First, when computing the QK⊤ part, full dense matrix multiplication is involved at a cost of O(n2) vector multiplications. It can be prohibitive for long sequence problems. On the other hand, if we view the softmax operator as an approximation of the argmax counterpart, ˆV becomes a row selection from V . This column-wise information concentration is ignored.\n\n3.1 SKELETON ATTENTION\n\nWe propose a Skeleton self-attention structure motivated by the Skeleton approximation to address those issues. First, we modify the original self-attention to build the column self-attention as follows:\n\nˆV1 = softmax\n\n(cid:18) 1 √\nd\n\nQK⊤P ⊤\n\n1\n\n(cid:19)\n\nP1V ,\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nwhere P1 ∈ Rs1×n denotes the sampling matrix and s1 is the number of columns sampled. Let i1 < i2 < ... < is1 be the indices of the randomly sampled columns. Let P1,ab denote the element located at the a-th column and b-th row and we have P1,ab = 1 if ia = b and 0 otherwise. By these constructions, we can reduce the computational cost to O(ns1d2 + ns2 Similarly, we build the row sampling matrix P2 ∈ Rd×s2 indicating the locations of the s2 sample rows. Compute the row self-attention as:\n\n1d).\n\nˆV2 = V P2 softmax\n\n(cid:18) 1 √\nn\n\nP ⊤\n\n2 K⊤Q\n\n(cid:19)\n\n.\n\nFinally, we apply the layer-norm on ˆV1 and ˆV2 and then them together to generate the final output:\n\nˆV = layernorm1( ˆV1) + layernorm2( ˆV2).\n\n(2)\n\nThe usage of layer norm is to balance the output scales of column and row self-attentions. A similar trick has been used in (Zhu et al., 2021), where the layer norm is applied to resolve scale mismatches between the different attention mechanisms.\n\nBefore going to the detailed analysis, we first introduce incoherence parameter of a matrix, which is commonly used in many low-rank matrix applications. Definition 1 (μ-incoherence). Given a rank-r matrix X ∈ Rn×d. Let X = W ΣV ⊤ be its compact singular value decomposition. X is μ-incoherent if there exists a constant μ such that\n\nmax i\n\n∥e⊤\n\ni W ∥ ≤\n\n(cid:114) μr n\n\nand\n\nmax i\n\n∥e⊤\n\ni V ∥ ≤\n\n(cid:114) μr d\n\n,\n\nwhere ei denotes the i-th canonical basis vector.\n\nThe μ-incoherence describes the correlation between the column/row spaces and the canonical basis vectors. The larger μ value implies a higher overlapping, which leads to a better chance of successful reconstruction from sparse row/column samples. We next use the following proposition to characterize the efficiency of sampling in both columns and rows. Proposition 1. Let X ∈ Rn×d be a rank-r matrix with μ-incoherence. Without loss of generality, we assume n ≥ d. Let E ∈ Rn×d be a noise matrix. By uniformly sampling O(μr log n) columns and rows from the noisy X + E, Skeleton approximation can construct a matrix ˆX such that, with probability at least 1 − O(n−2),\n\n∥X − ˆX∥ ≤ O\n\n(cid:32)\n\n√\n\n∥E∥ nd μr log n\n\n(cid:33)\n\n.\n\n(3)\n\nSeveral works (e.g., (Chiu & Demanet, 2013; Drineas et al., 2008)) have proposed explicit methods to construct ˆX. Those methods require computing the pseudo-inverse, generally inefficient in deep learning settings. (Xiong et al., 2021) uses an approximation of the pseudo-inverse in the symmetric matrix setting. It is still an open question whether the approximated pseudo-inverse also works for the general matrix in deep learning settings. On the other hand, in the transformer model, a good matrix approximation is not our primary goal, and we thus pursue a different way that only maintains sufficient information to pass through the network via (2).\n\n3.2 SMOOTHER COMPONENT\n\nBased on the analysis of Skeleton approximation, the matrix incoherence parameter μ plays a crucial role in determining the number of rows and columns to sample. Decreasing in μ leads to a smaller sampling size. Furthermore, the μ-incoherence condition implies that the “energy” of the matrix is evenly distributed over its entries, i.e., the matrix is “smooth” (Candès & Recht, 2009). In this subsection, we propose a novel smoother component to reduce the incoherence parameter without introducing excessive information loss.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla Transformer\n\n(b) SKTformer\n\nFigure 1: Illustration of the architecture of Vanilla Transformer versus SKTformer\n\n3.2.1 FOURIER CONVOLUTION\n\nThe incoherence parameter can be viewed as a measure of the smoothness of a matrix. A “smoother” matrix tends to have a smaller incoherence parameter. Intuitively, the adjacent columns or rows have similar values for a smooth matrix. Thus a few landmark columns or rows can represent the matrix with little error. On the other hand, if the matrix is harsh (e.g., containing spiky columns or rows), more landmarks are required. A common way to smooth a matrix is to convolute it with a smoothing kernel, such as a Gaussian kernel. However, directly using a fixed smoothing kernel can potentially remove too much details and harm the final performance. In the recent literature (e.g., Guo et al. 2022), large convolution kernel-based attentions show a supreme performance in vision Transformers. In this paper, we propose to use a data-driven convolution layer along the sequence dimension with a kernel size equal to the sequence length. In this setting, the information of a given row could be decentralized among the rows. As the input token matrix is computed through a FeedForward layer, the information among different rows is already adaptive allocated. Hence, we do not perform the convolution along the hidden dimension.\n\nWe use the Fast Fourier Transformation (FFT) to implement the convolution. Let L0 ∈ Rn×d be the convolution kernel matrix. Via the convolution theorem, the circular convolutions in the spatial domain are equivalent to pointwise products in the Fourier domain, and we then have:\n\nX smooth = X ∗ L0 = F −1 [F(X) · F(L0)] ,\n\n(4)\n\nwhere F, ∗, and · denote FFT operator, convolution operator, and point-wise product, respectively.\n\nEquation (4) requires 3d times faster Fourier operations which could be prohibited when facing large d. In order to save the computational cost, we use the learnable matrix L ∈ Cn×d1 in the frequency domain instead and apply segment-average (averaging segments of hidden dimension) to X. To simplify the notation, we assume there are integers s and r with d = sr. Instead of using (4), we apply the following (5) to smooth the token matrix.\n\nX smooth = F −1 [F(XS) · L] ,\n\n(5)\n\n1In practice, we use the rFFT/irFFT, the fast (inverse) Fourier Transformation of real input instead of the\n\ngeneral FFT/IFFT, and the size of the matrix L is reduced to L ∈ C⌊n/2⌋+1)×d.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\n\n\n1\n\n\n\ns 1 0\n\n... ... . . . · · · and 1 denotes the s × s matrix with all elements equal 1. As XS contains repeated rows, in (5), we can reduce the usage of faster Fourier operations to r + d times.\n\n0 1\ns 1 ...\n\n∈ Rd×d\n\n... s 1\n\n \n \n\n \n \n\nS =\n\n0 0\n\n(6)\n\n0\n\n0\n\n...\n\n1\n\nIn the following proposition, we show the smooth ability of the Fourier convolution. Proposition 2. Let {x1, ...., xn} be a sequence with maxt |xt| ≤ amax and maxt |xt −xt−1| ≤ bmax. Let {l1, ..., ln} be a sequence of i.i.d. 1 n2 σ2-subgaussian variables. Let f (t) be the convolution of {xt} and {lt}, i.e., f (t) = (cid:80)t\n\ni=1 lt+1−ixi. With probability at least 1 − δ, we have: (cid:19) (cid:19)\n\n(cid:115)\n\n(cid:115)\n\n|f (t) − f (t − 1)| ≤ bmaxσ\n\n+ amaxσ\n\n.\n\n(7)\n\n1 2n\n\nlog\n\n(cid:18) 2n δ\n\n1\n\n2n2 log\n\n(cid:18) 2 δ\n\nThe Proposition 2 can be used to describe the Fourier convolution layer’s behavior in the early training stage. Via some standard initialization methods (e.g., Kaiming initialization or Xavier initialization), the variance of elements in learnable matrix L is O(n−1) and the scale of elements is O(n−1/2).2 To simplify our discussion, let us assume we use Kaiming normal initialization and L becomes a random complex Gaussian matrix with zero mean and variance n−1σ2. Using the fact that the FFT of a Gaussian sequence remains Gaussian with 2n times larger variance, the n−1σ2 variance Gaussian sequence through inverse FFT (IFFT) would result in a Gaussian sequence with 1 2n2 σ2 variance. By Proposition 2, the maximum difference between adjacent elements after the convolution is scaled on bmaxσn−1/2 + amaxσn−1 ≈ bmaxσn−1/2 when sequence length n is large enough. Thus as long as σ < O(\n\nn), the sequence is smoothed by the Fourier convolution.\n\n√\n\nDuring the training process, the elements in learnable matrix L go away from the independent random variables and help generate a better representation of segment-averaged token matrix XS. We use the following Proposition 3 to describe the potential representation ability of the proposed Fourier convolution component. Proposition 3. Let X ∈ Rn×d be a bounded matrix and S ∈ Rd×d constructed by (6). There exist matrices G, L ∈ Rn×d such that\n\n(cid:13) (cid:13)(XS)1:t − X smooth\n\nt\n\nG1:t\n\n(cid:13) (cid:13) ≤ O\n\n(cid:16)\n\nr3/2t log(n)d−1/2(cid:17)\n\n,\n\n(8)\n\nwhere (·)1:t is the submatrix of the first t rows of a given matrix, X smooth is the t-th row of X smooth = F −1 [F(XS) · L], and G satisfies Gi,j = Gi+s,j = .... = Gi+r(s−1),j = gi(j). Here {g1(·), ..., gs(·)} is an orthogonal polynomial basis.\n\nt\n\nThe Proposition 3 states that if we properly train the matrix L, the information in XS up to row t can be compressed into t-th row of X smooth with a moderate tolerance. Therefore, when we sample in rows X smooth, they will contain more information than the same number of rows in the original XS. Similar results are also discussed in FNet (Lee-Thorp et al., 2021) and several RRN literature, such as (Gu et al., 2020) and (Voelker et al., 2019). In (Gu et al., 2020), several specific types of polynomials (e.g., Legendre or Chebyshev) are explored, and the corresponding matrix L is predefined instead of data-driven. Recently, (Gu et al., 2021b) propose a sophisticated method that can be used to compute X smooth. We leave it for future work.\n\n3.2.2 CONVOLUTION STEM\n\nThe X smooth may encounter an over-smoothing situation that local details can be wiped out. We use a convolution stem (CNNs + BN + ReLU) to tackle this problem. We first concatenate X smooth with the original token matrix X into a n × 2d matrix and then apply a 1D convolution with kernel size 3 to transform it back to n × d dimensions. At last, the output is normalized with the Batchnorm layer and truncated by the ReLU activation function to stabilize the training procedure. (Wang et al., 2021) report the ReLU activation coupled with the normalization layer plays an important role in various vision transformers and analyzes this phenomenon theoretically.\n\n2Here we omit the dependence in d for brevity.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nIn this section, we test our SKTformer on Long Range Arena (LRA) datasets (Tay et al., 2021) and six real-world time series benchmark dataset for long term forecasting. We also evaluate the transfer learning ability of SKTformer on GLUE tasks. We implement the SKTformer based on the official codes of (Zhu et al., 2021) and (Zhou et al., 2022) for LRA and time-series forecasting tasks respectively. The implementation detail (source code) for SKTformer is provided in Appendix A.\n\n4.1 LONG-RANGE ARENA\n\nThe open-source Long-Range Arena (LRA) benchmark (Tay et al., 2021) is proposed as a standard way to test the capabilities of transformer variants architectures on long sequence tasks.\n\nTable 1: Experimental results on Long-Range Arena benchmark. Best model is in boldface and second best is underlined. The standard deviation of the SKTformer are reported in parenthesis.\n\nModel\n\nListOps\n\nTransformer Local Attention Sparse Transformer Longformer Linformer Reformer Sinkhorn Transformer Synthesizer BigBird Linear Transformer Performer Nystromformer H-Transformer-1D Transformer-LS FNet Luna FMMformer PoNet Cosformer Scatterbrain\n\n15.82 15.82 17.07 35.63 35.70 37.27 33.67 36.99 36.05 16.13 18.01 37.34 49.53 38.36 35.33 38.01 36.74 38.80 37.9 38.6\n\nText\n\n52.98 52.98 63.58 62.85 53.94 56.10 61.20 61.68 64.02 65.90 65.40 65.75 78.69 68.40 65.11 65.78 67.84 69.82 63.41 64.55\n\nRetrieval\n\nImage\n\nPathfinder\n\nAverage\n\n53.39 53.39 59.59 56.89 52.27 53.40 53.83 54.67 59.29 53.09 53.82 81.29 63.99 81.85 59.61 79.56 81.88 80.35 61.36 80.22\n\n41.46 41.46 44.24 42.22 38.56 38.07 41.23 41.61 40.83 42.34 42.77 41.58 46.05 45.05 38.67 47.86 45.10 46.88 43.17 43.65\n\n66.63 66.63 71.71 69.71 76.34 68.50 67.45 69.45 74.87 75.30 77.05 70.94 68.78 76.48 77.08 78.89 72.12 70.39 70.33 69.91\n\n46.06 46.06 51.24 53.46 51.36 50.67 51.39 52.88 55.01 50.55 51.41 59.38 61.41 62.03 54.42 62.02 60.74 61.05 55.23 59.38\n\nSKTformer (r, s1, s2 = 8) SKTformer (best)\n\n38.30(0.40) 39.15(0.48)\n\n69.27(0.83) 71.58(0.95)\n\n83.26(0.45) 83.73(0.61)\n\n53.90(1.54) 57.73(1.83)\n\n75.82(0.97) 78.20(1.32)\n\n64.11(2.07) 66.08(2.56)\n\nWe benchmark our model with several recent state-of-art efficient transformers, including Sparse Transformer, Longformer , Linformer, Reformers, Sinkhorn Transformer, Synthesizer, BigBird, Linear Transformers, Performer , Nyströmformer , H-Transformer-1D, Transformer-LS, FNet, Luna, FMMformer, Cosformer and Scatterbrain. SKTformer achieves the highest 66.08% average accuracy with tuned parameters and second best 64.11% result with fixed parameters as shown in Table 1.\n\nIn particular, SKTformer significantly outperforms the benchmarks on Image tasks by relatively large margins (12.6% and 20.6%, respectively), which support SKTformer’s smoothness effect on the low-level features and will benefit the high-level image classification tasks.\n\nMoreover, we want to highlight the sampling efficiency of SKTformer. The sequence length of LRA tasks is over one thousand. The efficient Transformers in literature usually can not project the token matrix to a very small size while maintaining comparable numerical performance, by only sampling 8 rows and columns from the token matrix, SKTformer has already obtained 64.11% average score improving the previous best 62.03% score of Transformer-LS.\n\n4.2 LONG-TERM FORECASTING TASKS FOR TIME SERIES\n\nTo further evaluate the proposed SKTformer, we also conduct extensive experiments on six popular real-world benchmark datasets for long-term time series forecasting, including traffic, energy, economics, weather, and disease as shown in table 2\n\nTo highlight the relevant comparison, we mainly include five state-of-the-art (SOTA) Transformerbased models, i.e., FEDformer(Zhou et al., 2022), Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021a), LogTrans (Li et al., 2019), Reformer (Kitaev et al., 2020a), and one recent state-space\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nmodel with recursive memory S4 (Gu et al., 2021a), for comparison. FEDformer is selected as the main baseline as it achieves SOTA results in most settings. More details about baseline models, datasets, and implementations are described in Appendix.\n\nCompared with SOTA work (FEDformer), our proposed SKTformer yields a comparable performance in those tasks, with 4/6 datasets having relative MSE reductions. It is worth noting that the improvement is even more significant on certain datasets, e.g., Exchange (> 30%). Although Exchange does not exhibit an apparent periodicity pattern, SKTformer still achieves superior performance. Table 2: multivariate long-term series forecasting results on six datasets with input length of 96 and prediction length O ∈ {96, 192, 336, 720} (For ILI dataset, we set prediction length O ∈ {24, 36, 48, 60}) with input length 60. A lower MSE indicates better performance. All experiments are repeated 5 times.\n\nMethods\n\nSKTformer\n\nFEDformer\n\nAutoformer\n\nS4\n\nInformer\n\nLogTrans\n\nReformer\n\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\n2 96 m\n192 T\n336 T\nE 720\n\nt i\nc i\nr t\nc e\nl\n\ny 96 192 336 720\n\nE\n\ne 96 192 336 720\n\ng n\na h\nc x\nE\n\nfi\n\nc 96 192 336 720\n\nf a\nr\n\nT\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\nI\n\nL\n\nI\n\n24 36 48 60\n\n0.192 0.255 0.324 0.431\n\n0.218 0.259 0.267 0.293\n\n0.086 0.188 0.356 0.727\n\n0.592 0.583 0.598 0.641\n\n0.182 0.228 0.295 0.383\n\n2.185 2.155 2.333 2.018\n\n0.283 0.324 0.364 0.433\n\n0.332 0.361 0.367 0.385\n\n0.204 0.292 0.433 0.669\n\n0.352 0.343 0.346 0.397\n\n0.262 0.306 0.355 0.418\n\n0.926 0.937 0.954 0.958\n\n0.203 0.269 0.325 0.421\n\n0.183 0.195 0.212 0.231\n\n0.139 0.256 0.426 1.090\n\n0.562 0.562 0.570 0.596\n\n0.217 0.276 0.339 0.403\n\n2.203 2.272 2.209 2.545\n\n0.287 0.328 0.366 0.415\n\n0.297 0.308 0.313 0.343\n\n0.276 0.369 0.464 0.800\n\n0.349 0.346 0.323 0.368\n\n0.296 0.336 0.380 0.428\n\n0.963 0.976 0.981 1.061\n\n0.255 0.281 0.339 0.422\n\n0.201 0.222 0.231 0.254\n\n0.197 0.300 0.509 1.447\n\n0.613 0.616 0.622 0.660\n\n0.266 0.307 0.359 0.578\n\n3.483 3.103 2.669 2.770\n\n0.339 0.340 0.372 0.419\n\n0.317 0.334 0.338 0.361\n\n0.323 0.369 0.524 0.941\n\n0.388 0.382 0.337 0.408\n\n0.336 0.367 0.395 0.578\n\n1.287 1.148 1.085 1.125\n\n0.705 0.924 1.364 0.877\n\n0.304 0.313 0.290 0.262\n\n1.292 1.631 2.225 2.521\n\n0.824 1.106 1.084 1.536\n\n0.406 0.525 0.531 0.419\n\n4.631 4.123 4.066 4.278\n\n0.690 0.692 0.877 1.074\n\n0.405 0.413 0.381 0.344\n\n0.849 0.968 1.145 1.245\n\n0.514 0.672 0.627 0.845\n\n0.444 0.527 0.539 0.428\n\n1.484 1.348 1.36 1.41\n\n0.365 0.533 1.363 3.379\n\n0.274 0.296 0.300 0.373\n\n0.847 1.204 1.672 2.478\n\n0.719 0.696 0.777 0.864\n\n0.300 0.598 0.578 1.059\n\n5.764 4.755 4.763 5.264\n\n0.453 0.563 0.887 1.338\n\n0.368 0.386 0.394 0.439\n\n0.752 0.895 1.036 1.310\n\n0.391 0.379 0.420 0.472\n\n0.384 0.544 0.523 0.741\n\n1.677 1.467 1.469 1.564\n\n0.768 0.989 1.334 3.048\n\n0.258 0.266 0.280 0.283\n\n0.968 1.040 1.659 1.941\n\n0.684 0.685 0.733 0.717\n\n0.458 0.658 0.797 0.869\n\n4.480 4.799 4.800 5.278\n\n0.642 0.757 0.872 1.328\n\n0.357 0.368 0.380 0.376\n\n0.812 0.851 1.081 1.127\n\n0.384 0.390 0.408 0.396\n\n0.490 0.589 0.652 0.675\n\n1.444 1.467 1.468 1.560\n\n0.658 1.078 1.549 2.631\n\n0.312 0.348 0.350 0.340\n\n1.065 1.188 1.357 1.510\n\n0.732 0.733 0.742 0.755\n\n0.689 0.752 0.639 1.130\n\n4.400 4.783 4.832 4.882\n\n0.619 0.827 0.972 1.242\n\n0.402 0.433 0.433 0.420\n\n0.829 0.906 0.976 1.016\n\n0.423 0.420 0.420 0.423\n\n0.596 0.638 0.596 0.792\n\n1.382 1.448 1.465 1.483\n\n4.3 TRANSFER LEARNING IN GLUE TASKS\n\nWe evaluate the transfer learning ability of the proposed model in the pretraining-finetuning paradigm in NLP tasks. We pretrain vanilla BERT (Devlin et al., 2018), FNet (Lee-Thorp et al., 2021), PoNet (Tan et al., 2021) and our SKTformer with the same MLM loss in (Devlin et al., 2018) on English Wikitext-103 and BooksCorpus datasets. All models are uncased and pretrained with the same configuration with 1 million steps at most. We report the best GLUE results for each model from multiple hyper-parameters configurations in Table 3, and the detailed training configurations in Table 15 in Appendix K. Our SKTformer reaches 77.01 average scores (96.0% of the accuracy of vanilla BERT), which also outperform FNet by 4.6% and PoNet by 0.3% relatively.\n\nTable 3: GLUE validation results. We report the mean of accuracy and F1 for QQP and MRPC, matthew correlations for CoLA, spearman correlations for STS-B, and accuracy for other tasks. For MNLI task, we consider the matched test set.\n\nModel\n\nMNLI QQP QNLI\n\nSST-2 CoLA STS-B MRPC\n\nRTE Average\n\nBERT-Base FNet-Base PoNet-Base SKTformer (Ours)\n\n81.98 73.20 77.02 76.86\n\n89.25 85.83 87.59 87.67\n\n88.22 80.57 84.37 84.12\n\n91.07 88.66 89.29 90.14\n\n48.08 40.67 45.38 46.72\n\n87.98 80.64 84.66 84.87\n\n86.43 80.88 81.82 81.84\n\n69.98 57.41 64.27 63.87\n\n80.37 73.48 76.80 77.01\n\n4.4 TRAINING SPEED AND PEAK MEMORY USAGE\n\nWe compared the training speed (in terms of steps per second) and peak memory usage with several baseline models. SKTformer achieves a 4x time speed advantage and 87% memory reduction\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\ncompared to vanilla transformer models with 3k input setting and has a neck-to-neck performance compared to the most efficient baseline models as shown in Table 4.\n\nTable 4: Benchmark results of all Xformer models with a consistent batch size of 32 across all models with various of input lengths\n\nModel Transformer Linformer Reformer Nystroformer Performer SKTformer\n\nTrain Speed (Steps per second)\n\n1 K 23.8 37.0(1.5x) 28.5(1.2x) 33.3(1.4x) 29.4(1.2x) 32.2(1.4x)\n\n2 K 7.8 20.8(2.6x) 15.1(1.9x) 22.7(2.9x) 16.9(1.9x) 20.4(2.6x)\n\n3 K 3.9 14.9(3.7x) 11.9(3.0x) 17.2(4.3x) 8.7(11.7x) 15.9(4.0x)\n\n4.5 ROBUSTNESS ANALYSIS\n\n4 K\n\nPeak Memory Usage (GB) 1 K 2 K 3 K 4 K 11.1 3.3 3.2 2.2 3.1 2.3\n\n22.1 OOM 4.3 4.2 2.4 4.0 2.8\n\n5.2 4.9 2.9 4.8 3.2\n\nOOM 3.7 2.3 2.2 1.6 2.3 1.8\n\n11.9 9.1 14.7 9.3 12.3\n\nWe conduct a noise-resistant experiment for SKTformer and Xformers as shown in Table 5. We use the Image experiment setting in LRA datasets. During generating a sample sequence, we randomly add noise with uniform distribution U(−a, a) to each position in the sequence. We consider a ∈ [0, 2, 4, 8] and train every model with 5k steps and 5 replicates. Our model’s performance remains robust with a high level of noise injection. This supports our theoretical robustness analysis and shows SKTformer indeed makes an appropriate tradeoff between information preservation and noise reduction.\n\n4.6 ABLATION STUDY\n\nThis subsection provides an ablation test on four components: Fourier Convolution, Convolution Stem, Column Attention, and Row Attention. We use SKTformer with (r, s1, s2 = 8) as the baseline, and the detailed settings are in Table 11 in Appendix F. In Table 6 we present the accuracy changes when removing each component. The performance-decreasing results in Table 6 indicate the four components used in SKTformer are necessary to reach promising results. The most significant component is Column Attention which leads 8.28 average accuracy difference. It reflects that a good summary of the whole sequence is important. Similar observations are also reported in TransformerLS (Zhu et al., 2021) and XCiT (Ali et al., 2021), where the spirit of attention over columns is used in the dynamic project and Cross-Covariance Attention, respectively. The second most effective part is Fourier Convolution. It reaches a 13.89% accuracy difference in the Retrieval task involving two 4k sequences. Fourier Convolution also works well on shorter sequence tasks (e.g., Image and Pathfinder) and brings a 6.12% accuracy difference.\n\n5 CONCLUDING REMARKS\n\nWe propose SKTformer, a robust and efficient transformer architecture for modeling long sequences It aggregates a Fourier with a good balance between feature preserving and noise resistance. convolutional stem smoothing information among tokens and a Skeleton-decomposition-inspired efficient self-attention. In particular, our proposed Skeleton Attention directly samples the columns and rows of the token matrix. Such a design increases the model’s robustness and gives us a positive near-linear complexity side effect. We conduct a thorough theoretical and experimental analysis of the proposed model and show its effectiveness. Lastly, extensive experiments show that the proposed model achieves the best performance on Long Range Arena compared to all transformer-based baselines and a state-of-art performance in long-term time series forecasting tasks.\n\nTable 5: Average Accuracy on Image task (CIFAR-10 dataset) in Long Range Arena with noise injections. The relative performance changes are reported in parentheses.\n\nNoise level\n\n0\n\n2\n\n4\n\n8\n\nTransformer Linformer Reformer Nystroformer Performer SKTformer\n\n41.39 38.43 38.04 41.52 42.66 57.47\n\n40.29 (-2.82%) 37.99 (-1.49%) 37.64 (-1.12%) 40.89 (-1.66%) 41.95 (-1.93%) 57.06 (-0.82%)\n\n28.56 (-31.12%) 37.04 (-3.95%) 35.26 (-7.37%) 38.39 (-7.67%) 39.61 (-7.40%) 55.32 (-3.84%)\n\n28.12 (-32.18%) 36.65 (-4.97%) 34.88 (-8.37%) 37.84 (-8.99%) 38.86 (-9.15%) 54.70 (-4.92%)\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Ablation experiments. The SKT (r, s1, s2 = 8) is used as baseline. The differences by removing each component from the baseline model are reported.\n\nModel\n\nBaseline\n\nLisOps\n\nText\n\nRetrieval\n\nImage\n\nPathfinder Average\n\n38.30\n\n69.27\n\n83.26\n\nFourier Conv. Conv Stem Column Attn. Row Attn.\n\n-0.47 -0.13 -1.16 -0.38\n\n-4.04 -0.55 -8.00 -1.92\n\n-13.98 -1.51 -9.16 -1.97\n\n53.90\n\n-5.64 -1.76 -10.63 -2.64\n\n75.82\n\n-6.59 -9.47 -12.45 -2.56\n\n64.11\n\n-6.14 -0.88 -8.28 -1.89\n\nOne limitation of the current SKTformer is that we need to use both FFT and IFFT in a sequential manner, which is potentially slower than the existing Fourier-based Transformers (e.g., (Lee-Thorp et al., 2021)) that only involve the FFT. As our primary goal using Fourier convolution is to smooth the token matrix and reduce the incoherent parameter, we can use the Random Fourier Transformation (Ailon & Chazelle, 2006) to modify SKTformer with only FFT. Another limitation is that the size of L matrix in the Fourier Convolution part is the same as the input sequence. On a longer sequence, L will contain more learnable parameters that make the model easier to overfit. We may introduce low-rankness or use a more sophisticated design, such as (Gu et al., 2021b), to tackle this issue in the future.\n\nREFERENCES\n\nNir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnson-lindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pp. 557–563, 2006.\n\nAlaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image transformers. Advances in neural information processing systems, 34, 2021.\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n\narXiv preprint arXiv:2004.05150, 2020.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nHanQin Cai, Keaton Hamm, Longxiu Huang, and Deanna Needell. Robust CUR decomposition: Theory and imaging applications. SIAM Journal on Imaging Sciences, 14(4):1472–1503, 2021.\n\nEmmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foun-\n\ndations of Computational mathematics, 9(6):717–772, 2009.\n\nBeidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34, 2021.\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\n\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n\nJiawei Chiu and Laurent Demanet. Sublinear randomized algorithms for skeleton decompositions.\n\nSIAM Journal on Matrix Analysis and Applications, 34(3):1361–1383, 2013.\n\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020.\n\nSankalan Pal Chowdhury, Adamos Solomou, Avinava Dubey, and Mrinmaya Sachan. On learning\n\nthe transformer kernel. arXiv preprint arXiv:2110.08323, 2021.\n\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text\n\nencoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nPetros Drineas, Michael W Mahoney, and Shan Muthukrishnan. Relative-error CUR matrix decom-\n\npositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–881, 2008.\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33: 1474–1487, 2020.\n\nAlbert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured\n\nstate spaces. arXiv preprint arXiv:2111.00396, 2021a.\n\nAlbert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured\n\nstate spaces. arXiv preprint arXiv:2111.00396, 2021b.\n\nMeng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, and Shi-Min Hu. Visual\n\nattention network. arXiv preprint arXiv:2202.09741, 2022.\n\nKeaton Hamm and Longxiu Huang. Perturbations of CUR decompositions. SIAM Journal on Matrix\n\nAnalysis and Applications, 42(1):351–375, 2021.\n\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020.\n\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020a.\n\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv\n\npreprint arXiv:2001.04451, 2020b.\n\nGuokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp. 95–104, 2018.\n\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with\n\nfourier transforms. arXiv preprint arXiv:2105.03824, 2021.\n\nShiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In Advances in Neural Information Processing Systems, volume 32, 2019.\n\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34, 2021.\n\nMichael Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks through ffts: International conference on learning representations (iclr2014), cbls, april 2014. In 2nd International Conference on Learning Representations, ICLR 2014, 2014.\n\nTan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and Bao Wang. Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention. Advances in Neural Information Processing Systems, 34, 2021.\n\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pp. 4055–4064. PMLR, 2018.\n\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.\n\nRandom feature attention. arXiv preprint arXiv:2103.02143, 2021.\n\nZhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2021.\n\nJiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise\n\nself-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.\n\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\n\ntransformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\n\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021.\n\nChao-Hong Tan, Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, and Zhen-Hua Ling. Ponet: Pooling network for efficient token mixing in long sequences. arXiv preprint arXiv:2110.02442, 2021.\n\nY Tay, D Bahri, D Metzler, D Juan, Z Zhao, and C Zheng. Synthesizer: Rethinking self-attention in\n\ntransformer models. arxiv 2020. arXiv preprint arXiv:2005.00743, 2020a.\n\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In\n\nInternational Conference on Machine Learning, pp. 9438–9447. PMLR, 2020b.\n\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. arXiv\n\npreprint arXiv:2009.06732, 2020c.\n\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAaron Voelker, Ivana Kaji ́c, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019.\n\nPichao Wang, Xue Wang, Hao Luo, Jingkai Zhou, Zhipeng Zhou, Fan Wang, Hao Li, and Rong Jin.\n\nScaled relu matters for training vision transformers. arXiv preprint arXiv:2109.03810, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\n\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\n\nGenta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, and Pascale Fung. Lightweight and efficient end-to-end speech recognition using low-rank transformer. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6144– 6148. IEEE, 2020.\n\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), pp. 101–112, 2021.\n\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14138–14148, 2021.\n\nJiehui Xu, Jianmin Wang, Mingsheng Long, et al. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34, 2021.\n\nLi Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo: Vision outlooker for visual\n\nrecognition. arXiv preprint arXiv:2106.13112, 2021.\n\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297, 2020.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In The ThirtyFifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference, volume 35, pp. 11106–11115, 2021a.\n\nJingkai Zhou, Pichao Wang, Fan Wang, Qiong Liu, Hao Li, and Rong Jin. Elsa: Enhanced local\n\nself-attention for vision transformer. arXiv preprint arXiv:2112.12786, 2021b.\n\nTian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In 39th International Conference on Machine Learning (ICML), 2022.\n\nChen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems, 34, 2021.\n\nZhenhai Zhu and Radu Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3801–3815, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nSupplementary Material for SKTformer: An Efficient Skeleton Transformer for Long Sequence Data\n\nA ALGORITHMS\n\nAlgorithm 1 Skeleton Attention\n\nclass Skeleton_Attention(nn.Module):\n\ndef __init__(self, num_head = 2, head_dim = 32,seq_len, left_rank = 8,right_rank = 8,\n\ndropout = 0.1):\n\nsuper(Skeleton_Attention, self).__init__() self.num_head = num_head self.head_dim = head_dim self.seq_len = seq_len self.left_rank = left_rank self.right_rank = right_rank\n\nself.ln_1 = nn.LayerNorm(self.num_head * self.head_dim) self.ln_2 = nn.LayerNorm(self.num_head * self.head_dim)\n\nself.drop_attn = torch.nn.Dropout(p=dropout)\n\nself.index_set_right = torch.randperm(self.head_dim) self.index_set_right = self.index_set_right[:self.right_rank]\n\nself.index_set_left = torch.randperm(self.seq_len) self.index_set_left = self.index_set_left[:self.left_rank]\n\ndef combine_heads(self, X): X = X.transpose(1, 2) X = X.reshape(X.size(0), X.size(1), self.num_head * self.head_dim) return X\n\ndef split_heads(self, X):\n\nX = X.reshape(X.size(0), X.size(1), self.num_head, self.head_dim) X = X.transpose(1, 2) return X\n\ndef forward(self,Q, K, V): #### Row Attention #### if self.left_rank <= self.seq_len:\n\nK1 = K[:,:,self.index_set_left,:] V1 = V[:,:,self.index_set_left,:]\n\nelse:\n\nK1 = K V1 = V\n\ndots = Q @ K1.transpose(-1,-2) dots = dots / math.sqrt(self.head_dim) attn = nn.functional.softmax(dots,dim=-1) attn = self.drop_attn(attn)\n\n#### Column Attention #### Q2 = Q.transpose(-1,-2) if self.right_rank <= self.head_dim:\n\nK2 = K[:,:,:,self.index_set_right] V2 = V[:,:,:,self.index_set_right]\n\nelse:\n\nK2 = K V2 = V\n\ndots_r = Q2 @ K2 dots_r = dots_r / math.sqrt(self.seq_len) attn_r = nn.functional.softmax(dots_r,dim=-1).transpose(-1,-2) attn_r = self.drop_attn(attn_r)\n\nX = self.split_heads(self.ln_1(self.combine_heads(torch.matmul(attn,V1))))/2 + self.\n\nsplit_heads(self.ln_2(self.combine_heads(torch.matmul(V2,attn_r))))/2\n\nreturn X\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Smoother component\n\nclass Smoother(nn.Module):\n\ndef __init__(self, hidden_size, seq_len, dropout = 0.5, num_head = 2,transformer_dim =\n\n64, fold = 1):\n\nsuper(Smoother, self).__init__()\n\nself.hidden_size = hidden_size self.seq_len = seq_len self.dropout = dropout self.num_head = num_head self.dim = transformer_dim self.fold = fold\n\nself.weights_fft = nn.Parameter(torch.empty(self.seq_len//2+1, self.hidden_size,2)) nn.init.kaiming_normal_(self.weights_fft, mode=’fan_in’, nonlinearity=’relu’)\n\nself.tiny_conv_linear = torch.nn.Conv1d(in_channels = self.hidden_size*2 ,\n\nout_channels = self.hidden_size, kernel_size = 3, padding= 1, groups = 1)\n\nself.dropout = torch.nn.Dropout(p=self.dropout) self.bn_1 = nn.BatchNorm1d(self.seq_len)\n\ndef forward(self, x):\n\n#### Compute Segment Average #### B,S,H = x.shape u = x.reshape(B,S,self.fold,H//self.fold) u = torch.mean(u,dim = -1)\n\n#### Fourier Convolution #### fft_u = fft.rfft(u, n = self.seq_len, axis = -2) fft_u = torch.view_as_real(fft_u) fft_u = fft_u.repeat(1,1,H//self.fold,1) self.weight_used = self.weights_fft.unsqueeze(0) temp_real = fft_u[...,0]*self.weight_used[...,0] - fft_u[...,1]*self.weight_used\n\ntemp_imag = fft_u[...,0]*self.weight_used[...,1] + fft_u[...,1]*self.weight_used\n\n[...,1]\n\n[...,0]\n\nout_ft = torch.cat([temp_real.unsqueeze(-1),temp_imag.unsqueeze(-1)],dim = -1) out_ft = torch.view_as_complex(out_ft) m = fft.irfft(out_ft, n = self.seq_len, axis = -2)\n\n#### Convolution Stem #### input_h = torch.cat((m, x), dim = -1) h = self.tiny_conv_linear(input_h.permute(0,2,1)).permute(0,2,1) h = self.dropout(F.relu(self.bn_1(h)))\n\nreturn h\n\nAlgorithm 3 pseudo code for Time-Series Forecasting\n\ndef forward(self, x_in): B1,H1,C1 = x_in.shape for i in range(len(self.encoder)): attn_layer = self.encoder[i] #standardize the input data if i == 0:\n\ntmp_mean = torch.mean(x_in[:,:,:],dim = 1,keepdim = True) tmp_std = torch.sqrt(torch.var(x_in[:,:,:],dim = 1,keepdim = True)+1e0) x_in = (x_in - tmp_mean)/(tmp_std)\n\nenc_out1 = self.enc_embedding(x_in)\n\nenc_out1= attn_layer(enc_out1) + enc_out1\n\n#decoder via Fourier Extrapolation dec_out = self.fourierExtrapolation(post(enc_out1)) output = (dec_out.reshape(B1,-1,C1))*(tmp_std)+tmp_mean return output\n\nB PROOF OF PROPOSITION 1\n\nA similar result, under a slightly different setting, can be found in (Cai et al., 2021). For the completeness of the paper, we provide a proof here. We resolve the sampling strategy. We consider a clear rank-r matrix X ∈ Rn×d, i.e., no additive noise and the rank is exact. Without loss of generality, we assume n ≥ d. Provided X is μ-incoherent, by (Chiu & Demanet, 2013, Theorem 1.1), Skeleton\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 4 Fourier Extrapolation\n\nclass fourierExtrapolation(nn.Module):\n\ndef __init__(self,inputSize,n_harm = 8,n_predict = 96):\n\nsuper().__init__() self.n = inputSize self.n_harm = n_harm self.f = torch.fft.fftfreq(self.n) self.indexes = list(range(self.n))\n\n# sort indexes by frequency, lower -> higher self.indexes.sort(key = lambda i: torch.absolute(self.f[i])) self.indexes = self.indexes[:1 + self.n_harm * 2]\n\nself.n_predict = n_predict\n\n# compute init phase self.t = torch.arange(0, self.n + self.n_predict) self.t1 = self.t.unsqueeze(0).unsqueeze(-1).float().to(’cuda’) self.f = self.f.unsqueeze(0).unsqueeze(-1).to(’cuda’) self.t = self.t.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)to(’cuda’) self.g = self.f[:,self.indexes,:].permute(0,2,1).unsqueeze(1) self.phase_init = 2 * 3.1415 * self.g * self.t\n\ndef fourierExtrapolation(self,x):\n\n# x in frequency domain x_freqdom = torch.fft.fft(x,dim = -2) x_freqdom = torch.view_as_real(x_freqdom) # select importance frequencies x_freqdom = x_freqdom[:,self.indexes ,:,:] x_freqdom = torch.view_as_complex(x_freqdom) ampli = torch.absolute(x_freqdom) / self.n # amplitude phase = torch.angle(x_freqdom) # phase\n\nampli = ampli.permute(0,2,1).unsqueeze(1) phase = phase.permute(0,2,1).unsqueeze(1)\n\nself.restored_sig = ampli * torch.cos(self.phase_init + phase)\n\nreturn torch.sum(self.restored_sig,dim = -1)\n\napproximation recovers X exactly, i.e.,\n\nX = CU R,\n\nwith probability at least 1 − O(n−2) if we uniformly sample O(μr log n) rows and columns to form the submatrices C and R.\n\nThirdly, we resolve the error bound estimation. For the noisy matrix X + E, we directly apply (Hamm & Huang, 2021, Corollary 4.3). Thus, we have\n\n∥X − ˆC ˆU ˆR∥ ≤ O\n\n(cid:33)\n\n(cid:32)(cid:114) nd lClR\n\n∥E∥,\n\nwhere ˆC and ˆR are sampled from the noisy matrix, ˆU is the pseudo-inverse of their intersection, and lC (resp. lR) is the number of columns (resp. rows) being sampled in ˆC (resp. ˆR).\n\nNote that this error bound assumes good column and row sampling, i.e., the clear submatrices corresponding to ˆC and ˆR can recover X exactly. Therefore, by combining the above two results, we show the claim in Proposition 1.\n\nC PROOF OF PROPOSITION 2\n\nAs f (t) is the convolution function of {xt} and {lt}, from the definition of convolution for t = 1, 2, ..., n we have\n\nlt+1−ixi\n\nf (t) =\n\nt (cid:88)\n\ni=1\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\nf (t) − f (t − 1) =\n\nt−1 (cid:88)\n\n(li+1 − li)xi\n\ni=1 (cid:124)\n\n(cid:123)(cid:122) :=(at)\n\n(cid:125)\n\n+l1xt.\n\nBy Hoffelding inequality, term (a) satisfies the following inequality with ε > 0.\n\nP(|(at)| ≥ ε) = P\n\n(cid:32)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nt−1 (cid:88)\n\n(li+1 − li)xi\n\ni=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:33)\n\n(cid:18)\n\n≥ ε\n\n≤ exp\n\n−\n\n2ε2 max · 1\n\nn2 σ2\n\n(t − 1)b2\n\n(9)\n\n(cid:19)\n\n(10)\n\nCombine (10) with the union bound over t = 1, 2, ..., n and the following (10) holds with probability at least 1 − δ/2:\n\nmax t\n\n|(at)| ≤ bmaxσ\n\n(cid:115)\n\n1 2n\n\nlog\n\n(cid:19)\n\n(cid:18) 2n δ\n\nSimilarly, with probability 1 − δ/2, we have\n\nmax t\n\n|l1xt| ≤ amaxσ\n\n(cid:115)\n\n1\n\n2n2 log\n\n(cid:19)\n\n.\n\n(cid:18) 2 δ\n\nTherefore, via (11) and (12), with probability at least 1 − δ, we have\n\nmax t\n\n|f (t) − f (t − 1)| ≤ bmaxσ\n\n(cid:115)\n\n1 2n\n\nlog\n\n(cid:19)\n\n(cid:18) 2n δ\n\n+ amaxσ\n\n(cid:115)\n\n1\n\n2n2 log\n\n(cid:19)\n\n(cid:18) 2 δ\n\n(11)\n\n(12)\n\n(13)\n\nD PROOF OF PROPOSITION 3\n\nThe proof contains two parts. In the first part, we view the data sequence as a function of index t and construct the coefficients and orthogonal polynomials for function approximation. In the second part, we show such coefficients can be computed with Fourier convolution i.e. (5)).\n\nFunction Approximation. We reformulate the matrix XS as follow:\n\nXS = [ ̄x1e\n\n ̄x2e · · ·\n\n ̄xre] ,\n\nwhere e ∈ R1×s is the one vector and ̄xi ∈ Rn×1 is the average from (s(i − 1) + 1)-th column to (si)-th column of X.\n\nNext, we focus on vector ̄xj and view its t-th element as the output of a function hj(t) = ̄xjt. Via analysis in (Gu et al., 2020, Appendixes C and D), we can form an approximation on hj(t) as follow:\n\nhj\n\n[x≤t](x) ≈\n\ncj i (t)gi(x),\n\n(cid:88)\n\ni=1\n\n(14)\n\nwhere {gi} is a sequence of orthogonal polynomial and [cj\n\n1(t), cj\n\n2(t), ..., cj\n\ns(t)] := cj\n\nt ∈ R1×s satisfy\n\nd dt\n\nc(t)j =\n\n1 t\n\nc(t)jA0 +\n\n1 ts log n\n\nh(t)b0\n\n(15)\n\nwhere A0 ∈ Rs×s and b0 ∈ R1×s are predefined matrix and vector respectively. Equation (15) is corresponding to the case with λn = s log n in (Gu et al., 2020).\n\nWe then use Forward Euler approach to discretize it:\n\nˆc(t)j = ˆc(t − 1)j(\n\n1 t\n\nI +\n\n1 t\n\nA0) +\n\n1 ts log n\n\nh(t)b0, .\n\n(16)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nVia standard error analysis of Forward Euler approach, we have\n\nc(t + 1)j = c(t)j +\n\n= c(t)j +\n\n= c(t)j +\n\n1 t\n1 t\n1 t\n\nc(t)jA0 +\n\nc(t)jA0 +\n\nc(t)jA0 +\n\n1 ts log n 1\nts log n 1\nts log n\n\nh(t)b0 +\n\nh(t)b0 +\n\nh(t)b0 + O\n\nd2 dt2 c(t)j|t=ξ\n\n1 ξs log n (cid:18) 1\n\nh(ξ)′b0\n\n(cid:19)\n\n,\n\nts log n\n\nwhere ξ ∈ [t, t + 1].\n\nIt implies that for t = 1, 2, ..., n,\n\n∥ˆc(t)j − c(t)j∥ ≤ O\n\n(cid:18) log t s log n\n\n(cid:19)\n\n.\n\n(17)\n\nCombine (17) with the similar proof procedure in (Gu et al., 2020, Proposition 6), if hj(x) is quadratic spline interpolation on { ̄xjt}, we obtain\n\n∥ ̄xjt −\n\ns (cid:88)\n\ni=1\n\nˆci(t)gi(x)∥ ≤ O (cid:0)t log n/\n\n√\n\ns(cid:1) = O\n\n(cid:18)\n\nt log n\n\n(cid:19)\n\n.\n\n(cid:114) r d\n\n(18)\n\nThe desirable result in Proposition 3 is obtained by repeatedly using (18) with j = 1, 2, ..., r.\n\nCoefficients via Fourier Convolution. The remaining task is to show that {ˆc(t)j} can be generated via Fourier convolution. To simplify the notation, we denote A = 1 t log n b0 and (16) becomes\n\nt A0 and b = 1\n\nt I + 1\n\nWe then repeatedly use (19) from t = 1, 2, ... and one may verify\n\nˆc(t)j = ˆc(t − 1)jA + h(t)b.\n\nbAt−i ̄xji\n\nwhere\n\nCj =\n\nˆcj\n\nt =\n\nt−1 (cid:88)\n\ni=1\n\nbAt−ih(i) =\n\n⇒ Cj = ̄Aj ∗ ( ̄xje),\n\n\n\n \n \n\n∈ Rn×s, and ̄Aj =\n\n\n\n \n \n\n2\n\n1\n\nˆcj ˆcj ... ˆcj n\n\nt−1 (cid:88)\n\ni=1\n\n\n\n\n\n \n\n\n∈ Rn×s.\n\nb bA\n\n \n\n\n... bAn−1\n\n(19)\n\n(20)\n\n(21)\n\nNext we repeatedly use (20) from j = 1, 2, .., r, and one has\n\n(cid:2)C1 C2 (cid:124)\n\n· · · Cr(cid:3) (cid:125)\n\n(cid:123)(cid:122) :=X smooth\n\n= (cid:2) ̄A1 (cid:124)\n\n ̄A2\n\n· · ·\n\n(cid:123)(cid:122) :=L0\n\n ̄Ar\n\n(cid:3)\n\n(cid:125)\n\n∗ [ ̄x1e (cid:124)\n\n ̄x2e · · · (cid:123)(cid:122) =XS\n\n ̄xre] (cid:125)\n\n⇒ X smooth = L0 ∗ XS ⇒ X smooth = F −1 (F(L0) · F(XS)) ⇒ X smooth = F −1 (L · F(XS)) ,\n\nwhere we use the fact that L is constructed in frequency domain in Fourier convolution in Eq. (5).\n\nE MODEL PARAMETERS IMPACT\n\nSKTformer introduces three extra hyperparameters, r, s1 and s2. We test the influence when varying them and report results in Table 7. We use SKTformer (r, s1, s2 = 8) as the baseline model and other parameters are reported in Table 10 in Appendix F.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Experimental results on varying r, s1 and s2. Best result is in boldface and second best is underlined. And Ablation experiments for each components\n\n(a) Experimental results on varying r parameter in smoothing component.\n\n(b) Experimental results on varying s1 parameter in Row Attention.\n\nr\n\n1 8\n16 32 64\n\nLisOps\n\nText\n\nRetrieval\n\nImage\n\nPathfinder Average\n\n37.30 38.30 38.62 38.19 37.89\n\n65.25 69.27 70.02 69.27 69.73\n\n78.65 83.26 83.21 82.05 81.79\n\n51.36 53.90 54.20 53.73 51.28\n\n71.23 75.82 76.15 75.58 75.52\n\n60.76 64.11 64.44 63.76 63.24\n\n(c) Experimental results on varying s2 parameter in Column Attention.\n\ns2\n\n1 4\n8 16 32\n\nLisOps\n\nText\n\nRetrieval\n\nImage\n\nPathfinder Average\n\n37.32 37.82 38.30 37.77 37.62\n\n55.28 52.05 69.27 70.24 68.32\n\n57.37 72.58 83.26 83.42 80.11\n\n40.97 46.74 53.90 54.11 51.66\n\n66.25 73.17 75.82 77.92 78.18\n\n51.44 57.47 64.11 64.73 62.98\n\ns1\n\n8 32 64 128 256\n\nLisOps\n\nText\n\nRetrieval\n\nImage\n\nPathfinder Average\n\n38.30 38.44 37.88 37.33 37.02\n\n69.27 70.85 70.53 69.24 65.72\n\n83.26 83.41 83.02 81.58 79.30\n\n53.90 54.92 51.22 49.08 46.24\n\n75.82 77.97 78.02 78.12 78.14\n\n64.11 65.12 64.33 63.07 61.29\n\n(d) The SKT (r, s1, s2 = 8) is used as baseline. The differences by removing each component from the baseline model are reported.\n\nModel\n\nBaseline\n\nLisOps\n\nText\n\nRetrieval\n\nImage\n\nPathfinder Average\n\n38.30\n\n69.27\n\n83.26\n\nFourier Conv. Conv Stem Column Attn. Row Attn.\n\n-0.47 -0.13 -1.16 -0.38\n\n-4.04 -0.55 -8.00 -1.92\n\n-13.98 -1.51 -9.16 -1.97\n\n53.90\n\n-5.64 -1.76 -10.63 -2.64\n\n75.82\n\n-6.59 -9.47 -12.45 -2.56\n\n64.11\n\n-6.14 -0.88 -8.28 -1.89\n\nInfluence of r in Fourier Convolution. The r parameter is used to determine the number of segment-averages to compute in (5). The smaller r leads the matrix with more duplicate columns, and more details information is lost. On the other hand, according to Proposition 3, the larger r would potently decrease the memorization ability and yield a high approximation error. In Table 7a, the best performance is observed when r = 8 or r = 16. For the case with r = 1, the token matrix is smoothed to rank one matrix, and the average accuracy drops 3.55 from the best setting. When the r value goes larger than 16, the accuracy in all experiments slightly decreases. We believe it is due to the over-fitting since the smoothed token matrix contains more flexibility and more irrelevant information training dataset is learned.\n\nInfluence of sample number s1 in Row Attention. In Row Attention part, we randomly sample s1 from key and value tokens. Table 7b reports that the optimal sampling amounts are different among tasks. In Pathfinder task, the optimal result is associated with s1 = 256, while the best performance of other tasks the reached with s1 = 32. Pathfinder task requires learning extreme long-range dependence (the connectivity between two circles far away from each other). The lack of enough tokens leads to inaccurate long-range dependence estimation and damages the final results. For the tasks like Image or Retrieval, the modest range dependence may already be enough to get promising performance, and we thus could use fewer token samples.\n\nInfluence of sample number s2 in Column Attention. In Column Attention, s2 columns are selected. The experiment results are shown in Table 7c. When setting s2 = 1, average performance decreases by 13.24%. Similar behavior is also observed in the first row of Table 7a with r = 1. The information loss due to lack of rankness limits the final performance. In an average sense, s2 = 16 gives the best result, and further increasing in s2 slightly harms the accuracy in all tasks except Pathfinder.\n\nF EXPERIMENT CONFIGURATIONS\n\nIn this section, we report the configurations for the experiments in Sections 4.1, 4.2, and 4.3.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Experiment Configuration of SKTformer (r, s1, s2 = 8).\n\nParameters\n\nListOps\n\nText Retrieval\n\nImage\n\nPathfinder\n\nEpoch Learning Rate Weight Decay Batch Size r, s1, s2 dropout in embedding dropout in attention dropout in smoother\n\n5 1e-4 0\n32 8,8,8 0\n0 0\n\n30 1e-4 1e-2 32 8,8,8 0.5 0.1 0.5\n\n15 1e-4 1e-2 32 8,8,8 0.1 0.1 0.1\n\n60 1e-3 1e-2 256 8,8,8 0.1 0.1 0.5\n\n100 1e-4 1e-2 256 8,8,8 0\n0 0.5\n\nTable 9: Experiment Configuration of SKTformer (best).\n\nParameters\n\nListOps\n\nText Retrieval\n\nImage\n\nPathfinder\n\nEpoch Learning Rate Weight Decay Batch Size r, s1, s2 dropout in embedding dropout in attention dropout in smoother\n\n10 1e-4 1-2 32 8,8,8 0\n0 0\n\n30 1e-4 1e-2 32 8,8,8 0.5 0.1 0.5\n\n15 1e-4 1e-2 32 8,32,32 0.1 0.1 0.1\n\n60 1e-3 1e-2 256 8,16,16 0.5 0.1 0.5\n\n100 1e-4 1e-2 256 8,128,32 0.1 0.1 0.5\n\nTable 10: Experiment Configuration for Model Parameters Impact.\n\nParameters\n\nListOps Text Retrieval\n\nImage\n\nPathfinder\n\nEpoch Learning Rate Weight Decay Batch Size dropout in embedding dropout in attention dropout in smoother\n\n5 1e-4 0\n32 0\n0 0\n\n30 1e-4 1e-2 32 0.5 0.1 0.5\n\n15 1e-4 1e-2 32 0.1 0.1 0.1\n\n60 1e-3 1e-2 256 0.1 0.1 0.5\n\n100 1e-4 1e-2 256 0\n0 0.5\n\nTable 11: Experiment Configuration for Ablation.\n\nParameters\n\nListOps\n\nText Retrieval\n\nImage\n\nPathfinder\n\nLearning Rate Weight Decay Batch Size r, s1, s2 dropout in embedding dropout in attention dropout in smoother\n\n1e-4 0\n32 8,8,8 0\n0 0\n\n1e-4 1e-2 32 8,8,8 0.5 0.1 0.5\n\n1e-4 1e-2 32 8,8,8 0.1 0.1 0.1\n\n1e-3 1e-2 256 8,8,8 0.1 0.1 0.5\n\n1e-4 1e-2 256 8,8,8 0\n0 0.5\n\nG ADDITIONAL RESULTS ON LRA\n\nWe have already provided the average of 5 runs with different random seeds in Table 1. Here we also provide the standard deviations for these experiments in Table 12.\n\nH DATASET AND IMPLEMENTATION DETAILS\n\nIn this subsection, we summarize the details of the datasets used in this paper as follows:\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nTable 12: Accuracy on Long Range Arena (LRA) with standard errors shown in parenthesis. All results are averages of 5 runs with different random seeds.\n\nModel\n\nLisOps\n\nText\n\nRetrieval\n\nImage\n\nPathfinder\n\nSKTformer (r, s1, s2 = 8) SKTformer (best)\n\n38.30 (0.40) 39.15 (0.48)\n\n69.27 (0.83) 71.58 (0.95)\n\n83.26 (0.45) 83.73 (0.61)\n\n53.90 (1.54) 57.73 (1.83)\n\n75.82 (0.97) 78.20 (1.32)\n\nTable 13: Details of time series benchmark datasets.\n\nDATASET\n\nLENGTH DIMENSION\n\nFREQUENCY\n\nETTM2 EXCHANGE WEATHER ELECTRICITY ILI TRAFFIC\n\n69680 7588 52696 26304 966 17544\n\n8 9\n22 322 8\n863\n\n15 MIN 1 DAY 10 MIN 1H 7 DAYS 1H\n\nLRA datasets: ListOps(2K length mathematical expression task which investigates the parsing ability); Text (up to 4K byte/character-level document classification task that tests capacity in character compositionality); Retrieval (byte/character-level document matching task, which exams the information compression ability with two 4K length sequence); Image (pixel-wise sequence image classification based on the CIFAR-10 dataset); Pathfinder (long-range spatial dependency identification task. The input images contain two small points/circles and dash-line paths. The model needs to identify whether two points/circles are connected);The LRA has several desirable advantages that made us focus on it as the evaluation benchmark: generality (only requires the encoder part); simplicity (data augmentation and pretraining are out of scope); challenging long inputs (difficulty enough and room to improve); diversity aspects (tasks covering math, language, image, and spatial modeling); and lightweight (run with low resource requirement).\n\nTime series datasets:1) ETT (Zhou et al., 2021a) dataset contains two sub-dataset: ETT1 and ETT2, collected from two separated counties. Each of them has two versions of sampling resolutions (15min & 1h). ETT dataset contains multiple time series of electrical loads and one time sequence of oil temperature. 2) Electricity3 dataset contains the electricity consumption for more than three hundred clients with each column corresponding to one client. 3) Exchange (Lai et al., 2018) dataset contains the current exchange of eight countries. 4) Traffic4 dataset contains the occupation rate of freeway systems in California, USA. 5) Weather5 dataset contains 21 meteorological indicators for a range of one year in Germany. 6) Illness6 dataset contains the influenza-like illness patients in the United States. Table 13 summarizes all the features for the six benchmark datasets. They are all split into the training set, validation set and test set by the ratio of 7:1:2 during modeling.\n\nGLUE datasets: The GLUE benchmark covers various natural language understanding tasks and is widely used in evaluating transfering ability. The tasks can be devided in to two types, single-sentence tasks (SST-2 and CoLA), and sentence-pair tasks (MNLI, QQP,QNLI,STS-B,MRPC,RTE). Following the same settings in (Devlin et al., 2018), we exclude WNLI task.\n\nI EXPERIMENTS ON THE SMOOTHNESS EFFECT OF FOURIER CONVOLUTION\n\nIn this section, we verify Fourier convolution component in the Smoother block can reduce the incoherence value in the early training stage. We use SKTformer with (r, s1, s2 = 8) as the test model and test on an NLP dataset: Text, and a vision dataset: Pathfinder. We compute the μincoherence value 7 of the token matrix before and after the Fourier convolution (denoted as μX\n\n3https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams 20112014 4http://pems.dot.ca.gov 5https://www.bgc-jena.mpg.de/wetter 6https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html 7Incoherence is defined by Definition 1 in Appendix B.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nand μX smooth, respectively) for each samples in the validation dataset. Since we do not explicitly force the token matrix to be low-rank required by Definition 1, we report the incoherence value for different rankness settings (rank = 16 and rank = 32) approximately, and the mean and standard deviation of incoherence value can be found in Table 14. The average incoherence value reduced 30% after the Fourier convolution in both datasets. Moreover, We observe that the standard deviation significantly decreases, which suggests the Fourier convolution may also potentially stabilize the training procedure.\n\nTable 14: The average incoherence parameters after 100 training steps with standard errors shown in the parenthesis.\n\nDataset\n\nμX (rank = 32) μX smooth (rank = 32) μX (rank = 16) μX smooth (rank = 16)\n\nText Pathfinder\n\n2.75 (0.027) 3.83 (0.221)\n\n2.05 (0.007) 1.99 (0.001)\n\n3.98 (0.046) 4.88 (0.264)\n\n3.23 (0.038) 3.48 (0.001)\n\nJ\n\nILLUSTRATION ON EFFECT OF THE SMOOTHER AND SKELETON ATTENTION IN TOKEN MATRIX\n\nIn this section, an illustration of the Smoother and Skeleton Attention part is shown in Figure 2. We smooth the input token matrix to ensure the sampling in rows and columns containing more local and/or global information. Thus, sampling several rows and columns from the smoothed token matrix can be more effective than the samples from the original token matrix.\n\nFigure 2: Illustration on effect of the Smoother and Skeleton Attention on Token Matrix.\n\nK TRANSFER LEARNING\n\nTable 15: The training configurations for Pretraining and GLUE tasks\n\nPre-training\n\nGLUE\n\nMax Steps Max Epochs Learning Rate Batch Size Warm-up Steps Sequence Length Learning Rate Decay Clip Dropout\n\n1000K -\n1e-4 256 5000 512 -\n- -\n\n- [4,20] [5e-5,1e-4] [16,32] -\n128 Linear 1\n0.1\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nL EXTRA ALGORITHMS FOR LONGTERM TIMESERIES FORECASTING\n\nTable 16: multivariate long-term series forecasting results on six datasets with input length of 96 and prediction length O ∈ {96, 192, 336, 720} (For ILI dataset, we set prediction length O ∈ {24, 36, 48, 60}) with input length 60. A lower MSE indicates better performance. All experiments are repeated 5 times.\n\nMethods\n\nSKTformer\n\nFEDformer\n\nAutoformer\n\nS4\n\nInformer\n\nLogTrans\n\nReformer\n\nPerformer\n\nNystroformer\n\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\n2 96 m\n192 T\n336 T\nE 720\n\nt i\nc i\nr t\nc e\nl\n\ny 96 192 336 720\n\nE\n\ne 96 192 336 720\n\ng n\na h\nc x\nE\n\nfi\n\nc 96 192 336 720\n\nf a\nr\n\nT\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\nI\n\nL\n\nI\n\n24 36 48 60\n\n0.192 0.255 0.324 0.431\n\n0.218 0.259 0.267 0.293\n\n0.086 0.188 0.356 0.727\n\n0.592 0.583 0.598 0.641\n\n0.182 0.228 0.295 0.383\n\n2.185 2.155 2.333 2.018\n\n0.283 0.324 0.364 0.433\n\n0.332 0.361 0.367 0.385\n\n0.204 0.292 0.433 0.669\n\n0.352 0.343 0.346 0.397\n\n0.262 0.306 0.355 0.418\n\n0.926 0.937 0.954 0.958\n\n0.203 0.269 0.325 0.421\n\n0.183 0.195 0.212 0.231\n\n0.139 0.256 0.426 1.090\n\n0.562 0.562 0.570 0.596\n\n0.217 0.276 0.339 0.403\n\n2.203 2.272 2.209 2.545\n\n0.287 0.328 0.366 0.415\n\n0.297 0.308 0.313 0.343\n\n0.276 0.369 0.464 0.800\n\n0.349 0.346 0.323 0.368\n\n0.296 0.336 0.380 0.428\n\n0.963 0.976 0.981 1.061\n\n0.255 0.281 0.339 0.422\n\n0.201 0.222 0.231 0.254\n\n0.197 0.300 0.509 1.447\n\n0.613 0.616 0.622 0.660\n\n0.266 0.307 0.359 0.578\n\n3.483 3.103 2.669 2.770\n\n0.339 0.340 0.372 0.419\n\n0.317 0.334 0.338 0.361\n\n0.323 0.369 0.524 0.941\n\n0.388 0.382 0.337 0.408\n\n0.336 0.367 0.395 0.578\n\n1.287 1.148 1.085 1.125\n\n0.705 0.924 1.364 0.877\n\n0.304 0.313 0.290 0.262\n\n1.292 1.631 2.225 2.521\n\n0.824 1.106 1.084 1.536\n\n0.406 0.525 0.531 0.419\n\n4.631 4.123 4.066 4.278\n\n0.690 0.692 0.877 1.074\n\n0.405 0.413 0.381 0.344\n\n0.849 0.968 1.145 1.245\n\n0.514 0.672 0.627 0.845\n\n0.444 0.527 0.539 0.428\n\n1.484 1.348 1.36 1.41\n\n0.365 0.533 1.363 3.379\n\n0.274 0.296 0.300 0.373\n\n0.847 1.204 1.672 2.478\n\n0.719 0.696 0.777 0.864\n\n0.300 0.598 0.578 1.059\n\n5.764 4.755 4.763 5.264\n\n0.453 0.563 0.887 1.338\n\n0.368 0.386 0.394 0.439\n\n0.752 0.895 1.036 1.310\n\n0.391 0.379 0.420 0.472\n\n0.384 0.544 0.523 0.741\n\n1.677 1.467 1.469 1.564\n\n0.768 0.989 1.334 3.048\n\n0.258 0.266 0.280 0.283\n\n0.968 1.040 1.659 1.941\n\n0.684 0.685 0.733 0.717\n\n0.458 0.658 0.797 0.869\n\n4.480 4.799 4.800 5.278\n\n0.642 0.757 0.872 1.328\n\n0.357 0.368 0.380 0.376\n\n0.812 0.851 1.081 1.127\n\n0.384 0.390 0.408 0.396\n\n0.490 0.589 0.652 0.675\n\n1.444 1.467 1.468 1.560\n\n0.658 1.078 1.549 2.631\n\n0.312 0.348 0.350 0.340\n\n1.065 1.188 1.357 1.510\n\n0.732 0.733 0.742 0.755\n\n0.689 0.752 0.639 1.130\n\n4.400 4.783 4.832 4.882\n\n0.619 0.827 0.972 1.242\n\n0.402 0.433 0.433 0.420\n\n0.829 0.906 0.976 1.016\n\n0.423 0.420 0.420 0.423\n\n0.596 0.638 0.596 0.792\n\n1.382 1.448 1.465 1.483\n\n2.520 0.949 1.701 2.531\n\n0.281 0.283 0.301 0.301\n\n0.801 1.284 1.408 1.654\n\n0.709 0.681 0.682 0.675\n\n0.597 0.606 0.731 0.837\n\n3.937 4.007 4.575 4.020\n\n1.220 0.745 1.001 1.228\n\n0.375 0.387 0.387 0.387\n\n0.729 0.925 0.964 1.017\n\n0.391 0.369 0.366 0.360\n\n0.598 0.587 0.646 0.682\n\n1.298 1.329 1.451 1.366\n\n0.617 0.856 1.394 2.466\n\n0.273 0.282 0.302 0.292\n\n0.801 1.284 1.408 1.654\n\n0.709 1.127 0.867 0.686\n\n0.701 0.655 0.746 0.961\n\n4.378 5.332 5.575 4.742\n\n0.600 0.706 0.887 1.185\n\n0.364 0.370 0.387 0.373\n\n0.729 0.925 0.964 1.017\n\n0.400 0.611 0.477 0.369\n\n0.612 0.604 0.642 0.751\n\n1.364 1.554 1.614 1.469\n\n23",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes a new approach to apply transformers to Long Sequence Data, called SKTFormer. The approach combines a CUR matrix approximation technique, a Fourier-convolution based smoother, and a convolution stem to avoid over-smoothing. The authors apply their method to a number of problems, showing promising performance; they also report ablation study results, analyzing the importance of different components of their method.\n\n# Strength And Weaknesses\n\nStrengths:\n- The paper addresses a highly important problem\n- The proposed method shows good results on a sufficiently wide range of problems\n- Experiments are thorough, and ablation results are reported\n- The paper is generally clearly organized\n\nWeaknesses:\n- The most impressive results are achieved when hyper parameters of the proposed method are freely varied for each subtask. It may be creating an overly optimistic impression of the model performance. I'd prefer a higher emphasis on the performance of the fixed hyperparameter version (i.e. SKTformer (r, s1, s2 = 8)). For the benchmark used (Long Range Arena), it goes contrary to the established approach.\n- Performance variability from run to run is not reported fully (its reported in the appendix for LRA substasks, but not for the overall performance, and is not referenced in the main text).\n\n# Clarity, Quality, Novelty And Reproducibility\n\n** Clarity **\n\nGenerally, the paper is well organized and is well-written.\n\nIt does have a surprising amount of typos, but those can be easily resolved.\n\nAdditionally, sometimes the phrasing is a little suboptimal. For example, in the abstract, the authors say that their method \"addresses\" the tradeoff between computational cost, information retention, and noise. It's not clear what is meant by that. The paper does not propose a method to avoid this tradeoff completely; it might improve upon previous results, but, in my understanding, it does not address the tradeoff. Perhaps something along the lines of \"improves upon the previous results/attempts to negotiate this tradeoff\" would work better in this context.\n\nAdditionally, table numeration in appendix is very confusing. Specifically, table 10 is listed in text noticeably later than table 11. I suggest that tables are re-numbered.\n\nOverall, as long as these issues (especially typos) are resolved, the clarity matches the standards of the ICLR conference. I trust the authors to fix simple typos, so they did not affect my overall assessment.\n\n** Quality **\n\nThe experiments are well - designed and well-executed. I find it preferable to highlight the fixed hyperparameter SKTformer (r, s1, s2 = 8) performance more, since this is more consistent with how results were previously reported in the literature (e.g. in the Long Range Arena,. and in Luna: Linear Unified Nested Attention, the authors do not fine-tune major hyperparameters to each subtask).\n\nAdditionally, run-to-run variability is not included. Since even for the fixed hyperparameter model, some hyperparameter search was still performed (number of epochs, batch size, dropout, etc.), it's highly important to report results averaged over different runs & their variability. Methodologically, this is my main concern.\n\nTo be precise, it is reported, but hidden. It's only in the \"Appendix G\" that the authors mention the run-to-run variability and the fact that the results they report are averaged over 5 runs. This information (and the variability of the overall result, not only of each separate task) should be included in Table 1.\n\n** Novelty **\n\nThe novelty/originality of this paper is not its main selling point, as the method is largely a combination of pre-existing techniques, but it is sufficient & up to the standards of the ICLR conference.\n\n** Reproducibility **\n\nAs far as I can judge, the detailed model description & supplementary materials make this paper comply with highest reproducibility standards.\n\n** Typos and other suggestions **\n\nGenerally, the amount of typos in this paper is a little concerning (I provide a sample below). I would suggest a thorough proofread of the paper before the final version is submitted.\n\nExtensive studies over both Long Range Arena (LRA) datasets, six time-series forecasting show that SKTformer significantly outperforms -> \n..., and six time-series\n\nwithout having to suffice the linear complexity w.r.t. sequence length ->  without having to sacrifice the linear complexity w.r.t. sequence length\n\nWe propose SKTformer, n robust -> We propose SKTformer, a robust\n\nbuild the column self-attention as follow -> build the column self-attention as follows\n\n# Summary Of The Review\n\nGenerally, the paper addresses an important problem, and runs a number of reasonably planned experiments, supplementing them with theoretical results. \n\nIts main issue is the way experimental results are reported. Contrary to how most previous papers approached the topic (see Long Range Arena original paper, section 3.2 \"Philosophy Behind the Benchmark\"), authors fine-tuned the major hyperparameters for each subtask in the Long Range Arena. Given that these hyperparameters do have a major impact on performance (see table 7), such reporting may create a confusing picture of how good the method actually is, and make it harder to compare it with both previous and future results. The authors report a fixed hyperparameter version, but only as a secondary metric, and I suggest that it is reported more clearly either along or instead of the fine-tuned one, e.g. in the abstract and other parts throughout the paper. \n\nAdditionally, even though Appendix G gives the performance variability for each subtask in LRA, it is not mentioned in the main text and does not give the variability of the overall result, which is the most important part (I strongly suggest that it's included for reader's convenience, especially since it can be calculated assuming independence at no additional computational cost).\n\nOverall, however, the pros outweigh the cons, and I believe that the paper is above the acceptance threshold, although, unless the issues above are fixed, I believe that it's borderline. I will stay open to adjust my assessment based on the rebuttal and other reviewer's comments.\n\n** UPD **\n\nI have read other reviews and the authors' responses. They alleviate many of the concerns voiced by me and other reviews. In my opinion, I can not quite improve the score to an \"8\", but I would have given the paper a \"7\" if I could.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a novel Skeleton Transformer designed to efficiently model long sequences while addressing the challenges of information preservation and noise reduction. The methodology employs a Smoothing Block that leverages Fourier analysis and convolution for better token representation, along with a Matrix Sketch Method that samples rows and columns to maintain linear computational complexity. The findings indicate that SKTformer achieves superior performance on the Long Range Arena datasets and excels in time-series forecasting, providing significant improvements in accuracy, training speed, and memory usage compared to standard Transformers and other state-of-the-art models.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to long sequence modeling, particularly through the integration of smoothing techniques and skeleton attention, which effectively balance computational efficiency with performance. The extensive empirical evaluation, including benchmarks on various datasets, showcases the model's robustness and practical applicability. However, the paper also presents weaknesses, such as the potential for overfitting due to the size of the learnable parameters in the Fourier Convolution and the possible inefficiency of sequential FFT usage compared to other methods that utilize FFT alone.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results, making it accessible to readers. The quality of the experiments is high, with thorough evaluations and ablation studies supporting the claims made. In terms of novelty, SKTformer presents a significant advancement in the field of efficient Transformers, particularly for long sequence data. The reproducibility of the results is facilitated by the detailed descriptions of the model architecture and experimental setup, although the reliance on sequential FFT may hinder practical implementation in some scenarios.\n\n# Summary Of The Review\nOverall, SKTformer represents a promising advancement in the modeling of long sequences, combining innovative techniques to achieve efficient performance without sacrificing accuracy. While the empirical results are impressive and the methodology sound, certain limitations regarding potential overfitting and computational efficiency warrant further investigation.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a transformer architecture specifically designed to efficiently process long sequence data while balancing information preservation and noise reduction. SKTformer employs two innovative mechanisms: a smoothing block to enhance information representation over long sequences and a matrix sketch method for efficient selection of input matrix columns and rows. The authors validate the effectiveness of SKTformer through extensive theoretical and empirical experiments, demonstrating superior performance on several benchmarks, including the Long Range Arena (LRA) datasets, time-series forecasting tasks, and transfer learning on GLUE tasks.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its innovative architecture that effectively integrates smoothing and attention mechanisms, leading to near-linear complexity. The comprehensive evaluation across multiple domains demonstrates the model's versatility and robustness, particularly in noisy environments. However, the paper also has notable weaknesses. The reliance on both FFT and IFFT could hinder performance compared to models using only FFT. Additionally, the risk of overfitting due to larger matrices in the Fourier Convolution and the sensitivity of performance to hyperparameters may complicate practical applications. Lastly, the complexity of the architecture poses challenges for deployment and scalability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear structure and logical flow, making it accessible to readers. The quality of the methodology is high, supported by a robust set of experiments and ablation studies that effectively demonstrate the contributions of various components. The novelty of the proposed architecture is significant, addressing key challenges in processing long sequences. However, reproducibility may be a concern due to the sensitivity of the model's performance to hyperparameter tuning and the overall complexity of implementation.\n\n# Summary Of The Review\nOverall, the paper presents a significant advance in transformer architectures for long sequence data through the introduction of SKTformer. The model shows strong empirical performance and robustness in various tasks, although some concerns regarding overfitting, parameter sensitivity, and implementation complexity remain. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a novel transformer architecture specifically designed for efficiently handling long sequence data. The main contributions include the introduction of a smoothing block that aids in mixing information across long sequences and the application of a matrix sketch method for effective column and row selection, which helps maintain linear computational complexity. The authors validate the effectiveness of SKTformer through both theoretical analysis and empirical experiments on Long Range Arena (LRA) datasets and time-series forecasting, demonstrating that it outperforms existing state-of-the-art models in terms of accuracy, training speed, and memory efficiency.\n\n# Strength And Weaknesses\nStrengths of this paper include its innovative approach to reducing computational complexity while preserving information integrity, which is a significant challenge in transformer architectures. The proposed smoothing block and matrix sketching method present a fresh perspective on managing long sequences. Furthermore, the empirical results are compelling, showing substantial improvements over baseline models in various tasks. However, a notable weakness is the reliance on theoretical propositions that may require further empirical validation. Additionally, while the performance gains are impressive, the paper could provide more detailed discussions on the limitations and potential trade-offs involved with the proposed methods, particularly regarding their applicability in different contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with detailed explanations of the methodologies employed and the theoretical foundations laid out in a digestible manner. The quality of the writing is high, with appropriate use of mathematical formulations to support claims. The novelty of the approach, particularly the integration of Fourier convolution and matrix sketching in the context of transformers, marks a significant contribution to the field. However, reproducibility could be improved; while algorithms and model parameters are provided, more comprehensive information on training setups and hyperparameter tuning would facilitate independent validation of the results.\n\n# Summary Of The Review\nOverall, SKTformer presents a significant advancement in transformer architectures for long sequence data, achieving notable performance improvements while maintaining computational efficiency. The methodology is innovative, and the empirical results are promising, although further exploration of limitations and reproducibility aspects is necessary.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the SKTformer, an innovative Skeleton Transformer architecture designed to address the computational complexity associated with modeling long sequences. The methodology includes a combination of Fourier transforms (FFT and IFFT) and smoothing techniques that enhance noise reduction while preserving essential information. The findings indicate that SKTformer achieves near-linear computational complexity and demonstrates robust performance across various benchmarks, including Long Range Arena and time-series forecasting tasks.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative architecture and effective noise reduction strategies. The SKTformer's linear complexity is a significant advantage over traditional transformers, making it well-suited for long sequences. Empirical validations show promising results, particularly in transfer learning tasks with GLUE benchmarks. However, the implementation's reliance on both FFT and IFFT may hinder performance compared to other Fourier-based transformers. Additionally, the model's generalizability beyond tested datasets remains uncertain, and its robustness to different noise types may vary. The requirement for specific parameter settings for optimal performance could also limit the model's flexibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates the contributions clearly, providing a comprehensive overview of the SKTformer architecture and its advantages. The quality of the experimental results is high, supported by thorough theoretical and empirical validations. The novelty of the approach is notable, particularly in its combination of techniques for long sequence modeling. The authors have made the code publicly available, promoting reproducibility and further research, although the ongoing review status may affect immediate community engagement.\n\n# Summary Of The Review\nOverall, the SKTformer presents a significant advancement in transformer architecture for long sequence modeling, with effective noise reduction and linear complexity as key contributions. While the empirical results are promising, concerns regarding generalizability and robustness against diverse noise types warrant further investigation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents SKTformer, a new Transformer architecture tailored for processing long sequence data. Key contributions include the introduction of a Skeleton Attention mechanism that enhances both row and column sampling for efficient information retention, and an Adaptive Smoothing Layer that employs Fourier transformations and dynamic convolution to mitigate noise and stabilize information flow. Empirical results demonstrate that SKTformer outperforms existing models on benchmark tasks such as the Long Range Arena and time-series forecasting, achieving significant improvements in accuracy and efficiency.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative Skeleton Attention mechanism, which provides a novel approach to handling long sequences by allowing for selective sampling, thus preserving critical information. The Adaptive Smoothing Layer effectively addresses noise reduction, which is often a significant challenge in sequence modeling. Additionally, the comprehensive empirical validation, including ablation studies, emphasizes the importance of each component and highlights the model's robust performance across various tasks. However, a notable weakness is the potential for overfitting with larger sequence lengths, which the authors acknowledge but do not fully address. Future work is suggested to optimize the number of parameters in the smoothing component.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers. The quality of the writing is high, with thorough explanations of the proposed mechanisms and their significance. The novelty lies in the unique combination of the Skeleton Attention and Adaptive Smoothing Layer, which sets this work apart from existing models. Reproducibility is supported through detailed descriptions of the experimental setup and results, although further information on hyperparameter tuning could enhance this aspect.\n\n# Summary Of The Review\nOverall, SKTformer represents a significant advancement in the modeling of long sequences, featuring innovative mechanisms that improve performance while maintaining computational efficiency. The empirical results validate its effectiveness, although some limitations regarding overfitting must be considered for future iterations. The paper makes a noteworthy contribution to the field of sequential data processing.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SKTFORMER, a novel approach that integrates a Skeleton Transformer architecture designed specifically for long sequence data. The main contributions include a new framework that leverages a skeleton-based structure to efficiently capture dependencies in long sequences while reducing computational complexity. The authors demonstrate that SKTFORMER achieves state-of-the-art performance on long sequence tasks, such as time series forecasting and natural language processing, while being more efficient than traditional transformer architectures.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to handling long sequences, which is a significant challenge in the field. The proposed skeleton structure is well-motivated and offers a clear advantage in efficiency and performance, as evidenced by the results presented. The empirical validation across multiple datasets strengthens the claims made by the authors. However, weaknesses include a lack of detailed analysis on the limitations of the skeleton structure and how it may affect the model's generalization on varied datasets. Additionally, the paper could provide more insights into the potential computational trade-offs involved in adopting the proposed architecture.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, making the contributions and methodologies clear to the reader. The experimental results are presented clearly, with appropriate visualizations that enhance understanding. The novelty of the proposed Skeleton Transformer is notable, particularly in the context of long sequence data. However, reproducibility could be improved with more comprehensive details regarding the implementation and hyperparameter settings used during experimentation.\n\n# Summary Of The Review\nOverall, the SKTFORMER paper introduces a compelling new architecture that effectively addresses the challenges associated with processing long sequences. The contributions are significant, with a strong empirical foundation, although further discussion on limitations and reproducibility would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SKTformer, a novel Transformer architecture aimed at improving the efficiency of long sequence data processing. The authors claim that SKTformer achieves near-linear computational complexity while effectively managing noise, thereby enhancing both information preservation and resource utilization. Through extensive benchmarking against state-of-the-art models, the authors assert that SKTformer demonstrates superior performance, suggesting it has the potential to redefine standards in sequence modeling.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious proposal of a new architecture that integrates advanced components for handling large datasets efficiently. The near-linear complexity claim and extensive experimental validation are noteworthy contributions that could significantly impact the field. However, the paper tends to overstate its claims, presenting SKTformer as a definitive solution to previous inefficiencies without adequately addressing possible limitations or weaknesses in its methodology. Additionally, the competitive landscape is somewhat simplified, potentially downplaying the merits of existing approaches.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with a clear presentation of the SKTformer architecture and its theoretical foundations. However, the assertions regarding its revolutionary impact and superiority over past models may confuse readers and warrant more nuanced discussion. The novelty of SKTformer is significant, but the authors could improve reproducibility by providing more detailed implementation guidelines or datasets used during benchmarking. The theoretical and experimental sections are robust, yet some claims lack sufficient empirical backing, which may hinder reproducibility.\n\n# Summary Of The Review\nOverall, the paper introduces an innovative architecture in SKTformer, promising significant advancements in the processing of long sequence data. While the contributions are noteworthy, the strong emphasis on SKTformer's superiority over existing models may lead to skepticism regarding the validity of some claims. A more balanced discussion of limitations and reproducibility could enhance the paper's overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a novel transformer architecture designed to effectively model long sequence data while addressing the computational challenges associated with traditional self-attention mechanisms. It employs a combination of a smoothing block and matrix sketching techniques to achieve near-linear complexity without compromising on information preservation. The experimental results demonstrate SKTformer’s superior performance across several benchmarks, including Long Range Arena (LRA) datasets and time-series forecasting tasks, achieving significant improvements in accuracy compared to existing models.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative architecture that integrates smoothing and attention mechanisms, which notably enhances computational efficiency while maintaining performance. The empirical validation shows impressive results across multiple benchmarks, particularly in challenging scenarios, which solidifies its contributions to the field. However, a notable weakness is the lack of extensive analysis on the long-term implications of using Fourier convolution methods and the potential for overfitting on longer sequences, which could limit practical applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The experimental results are presented in a coherent manner, making it easy for readers to grasp the improvements offered by SKTformer. The novelty is evident in the proposed architecture and the innovative approach to handling long sequences. However, while the experimental setups are thorough, additional details on the reproducibility of the results, such as hyperparameter settings and dataset splits, would enhance the paper’s quality.\n\n# Summary Of The Review\nOverall, SKTformer presents a significant advancement in the modeling of long sequence data through its efficient transformer architecture. It demonstrates strong empirical results across various tasks, indicating its potential impact on the field. However, further investigation into certain methodological aspects would strengthen the paper's contributions.\n\n# Correctness\nRating: 4/5  \nThe methodology and results appear sound, but further exploration of potential limitations is necessary for a perfect score.\n\n# Technical Novelty And Significance\nRating: 4/5  \nThe integration of smoothing and attention mechanisms is a notable contribution, though similar approaches exist in the literature.\n\n# Empirical Novelty And Significance\nRating: 5/5  \nThe experimental results showcase significant improvements over existing models, particularly in the challenging domains tested, establishing SKTformer as a competitive solution for long sequence modeling.",
    "# Summary Of The Paper\nThe paper presents the SKTformer, a novel architecture designed for processing sequential data with a focus on low-rank matrix assumptions. The methodology emphasizes sampling approaches that selectively choose columns and rows from input matrices to reduce computational complexity while maintaining performance integrity. The findings suggest that SKTformer achieves linear complexity and provides effective noise reduction through Fourier convolution techniques. However, the paper's claims rely on the assumption that its design will generalize across diverse datasets, primarily validated through limited benchmarks.\n\n# Strength And Weaknesses\nThe SKTformer's contribution to the field is significant, particularly with its focus on reducing computational requirements while handling sequential data. However, the reliance on the low-rank structure assumption poses a potential limitation, as it may not universally apply to all data types. Additionally, the effectiveness of the sampling strategy is questionable, as critical features may be lost if representative rows and columns are not selected. The noise reduction methods and the implications of Fourier convolution also raise concerns about the underlying assumptions regarding noise distribution and information retention. While the paper includes ablation studies, it lacks a thorough exploration of the model's limitations, which diminishes the overall robustness of its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, making it accessible to the reader. However, there are areas where additional clarity could enhance understanding, particularly in detailing the sampling methods and their implications. The novelty of the SKTformer is notable, but the assumptions it relies on may limit its significance in broader contexts. Reproducibility is not sufficiently addressed; empirical validation across varied datasets beyond the primary benchmarks would strengthen the findings and provide a more comprehensive understanding of the model's capabilities.\n\n# Summary Of The Review\nIn summary, while the SKTformer presents promising advancements in handling sequential data through innovative sampling and noise reduction techniques, its reliance on specific assumptions and limited empirical validation raises concerns about its generalizability and robustness. A more extensive exploration of its limitations and broader testing across diverse datasets would significantly enhance the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents SKTformer, a Skeleton Transformer designed to efficiently model long sequences while addressing the trade-off between information preservation and noise reduction. The authors propose a novel Skeleton attention mechanism that samples input matrix rows and columns, along with a smoothing component using Fourier convolution. Through extensive experiments on Long Range Arena and various time-series datasets, SKTformer demonstrates superior performance compared to standard transformers and other state-of-the-art models, while maintaining linear computational complexity.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to long sequence modeling and the effective combination of the Skeleton attention mechanism with Fourier convolution to enhance representation quality. The empirical results are compelling, showing significant improvements over existing models. However, a notable weakness is the limited exploration of the model's limitations and potential areas for further development, which could have provided a more balanced perspective on its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology and experimental results. The quality of writing is high, making it accessible to a broad audience. The novelty of the Skeleton attention mechanism contributes to the existing body of knowledge regarding efficient transformers. Reproducibility is supported by detailed descriptions of the experiments and datasets used, though the inclusion of code and supplementary materials would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of efficient long sequence modeling through its innovative SKTformer architecture. While the results are promising, the authors could benefit from a more thorough discussion of the model's limitations and potential future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Adaptive Learning Rate Adjustment (ALRA) aimed at improving the efficiency and performance of deep learning optimization algorithms. The authors propose a dynamic adjustment mechanism for learning rates based on gradient behavior, which is expected to enhance convergence rates and overall model performance. The methodology is validated through extensive experiments on benchmark datasets, revealing significant performance improvements compared to standard optimization techniques, particularly in scenarios with noisy gradients.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Concept:** The idea of dynamically adjusting learning rates based on gradient analysis is a fresh perspective that could lead to substantial advancements in optimization strategies.\n   \n2. **Robust Theoretical Justification:** The paper provides a solid theoretical framework for the proposed method, detailing the rationale behind the adaptive mechanism and its expected benefits.\n\n3. **Comprehensive Experimental Validation:** The authors conduct a wide array of experiments, demonstrating the effectiveness of ALRA across different datasets and optimization tasks, which enhances the credibility of their claims.\n\n4. **Clear Baseline Comparisons:** The experiments include comparisons with various established optimization algorithms, effectively contextualizing the performance of ALRA and highlighting its advantages.\n\n**Weaknesses:**\n1. **Implementation Details:** Some implementation aspects of ALRA are not thoroughly explained, which may hinder reproducibility. More detailed algorithmic steps would be beneficial for practitioners aiming to apply this method.\n\n2. **Limited Discussion on Edge Cases:** The paper does not adequately address how ALRA performs in extreme cases or with highly non-convex loss surfaces. A deeper exploration of such scenarios could provide a more nuanced understanding of the method's robustness.\n\n3. **Evaluation Metrics Selection:** While the paper uses standard metrics for evaluation, a more detailed justification for their selection could strengthen the validity of the findings.\n\n4. **Scalability Concerns:** The implications of applying ALRA to larger datasets or more complex models are not discussed, which raises questions about its scalability in real-world applications.\n\n5. **Future Work Directions:** The paper could benefit from a more extensive discussion on potential extensions or modifications of the proposed approach for future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers with varying levels of expertise. The quality of writing is high, with minimal grammatical errors. The novelty of the approach is notable, contributing significantly to the field of optimization in deep learning. However, the reproducibility of the results may be compromised due to the lack of detailed implementation information.\n\n# Summary Of The Review\nThe paper proposes an innovative adaptive learning rate adjustment method that shows promise in enhancing deep learning optimization. While the theoretical foundations and empirical results are strong, the paper could improve by providing more implementation details and discussing potential limitations and future directions. Overall, it represents a meaningful contribution to the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents SKTformer, a novel transformer architecture designed to address the challenges of modeling long sequences in various domains, including NLP, computer vision, and time series forecasting. The main contributions include the introduction of a smoothing block that facilitates information mixing across long sequences and a matrix sketching method that enhances computational efficiency by selecting relevant columns and rows from the input matrix. Experimental results demonstrate that SKTformer outperforms both vanilla Transformers and state-of-the-art alternatives on benchmarks involving long sequences, showcasing its effectiveness in preserving information while minimizing noise.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to overcoming the computational limitations of traditional transformers when handling long sequences. The introduction of the smoothing block and matrix sketching method offers a compelling solution to the dual challenge of information retention and noise reduction. However, the paper could benefit from more extensive ablation studies to clarify the impact of each component within the SKTformer architecture. Additionally, while the results are promising, the paper would be strengthened by including comparisons with a broader array of existing methods and potentially addressing limitations regarding the scalability of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem space, methodology, and findings. The contributions are presented in a logical manner, making it easy for the reader to follow the development of SKTformer. The quality of the experiments appears robust, but details on the experimental setup and hyperparameter choices could be further elaborated to enhance reproducibility. The novelty of the proposed architecture is significant, particularly in its combination of techniques aimed at processing long sequences efficiently.\n\n# Summary Of The Review\nOverall, SKTformer presents a strong advancement in transformer architectures for long sequence data, demonstrating notable improvements in performance while addressing computational challenges. The methodology is innovative and well-supported by empirical evidence, although further investigation into component contributions and broader comparisons would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SKTformer, a novel transformer architecture designed to efficiently handle long sequence data. Key contributions include the introduction of a smoothing block that facilitates effective information mixing over extended sequences and a matrix sketching method for optimal row and column selection. The methodology leverages Fourier analysis and random sampling to achieve linear computational complexity while significantly improving both accuracy and efficiency, as validated through rigorous empirical testing on the Long Range Arena (LRA) and multiple time-series forecasting datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to long sequence modeling, effectively addressing the limitations of existing transformer architectures. The use of smoothing and skeleton attention mechanisms is a notable contribution that offers potential for broader applications in efficient sequence processing. However, the paper could benefit from a more detailed exploration of the theoretical foundations supporting the proposed methods, as well as a discussion of potential limitations or edge cases that might not be accounted for in the empirical evaluation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results. The quality of writing is high, with a logical flow that aids in comprehension. The novelty of the proposed SKTformer architecture is significant, as it introduces a fresh perspective on addressing the challenges of long sequence data. The reproducibility of the results appears strong, given the thorough empirical validation across diverse datasets and the detailed description of the experimental setup.\n\n# Summary Of The Review\nOverall, SKTformer represents a meaningful advancement in the field of long sequence modeling with transformers. While the paper is clear and well-executed, it would benefit from a more in-depth theoretical analysis and a discussion of potential limitations. The empirical results strongly support the proposed methodology, showcasing its promise for future research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a novel architecture designed to efficiently model long sequence data using Transformers. The main contributions include the development of a smoothing block and a matrix sketch method, which together enhance the model's ability to handle extensive sequences while maintaining performance. Empirical evaluations on datasets such as Long Range Arena and time series forecasting demonstrate that SKTformer outperforms baseline models in terms of accuracy and robustness against noise.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive methodology and thorough empirical validation. The dual mechanisms proposed for improving the Transformer architecture are innovative and well-justified theoretically. Additionally, the paper effectively situates SKTformer within the existing literature, clearly articulating its contributions. However, a notable weakness is the limited exploration of practical applications and implications of the model in real-world scenarios, which could enhance its relevance to practitioners.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly written, with a logical progression of ideas from introduction to conclusion. The quality of the presented equations and theoretical propositions is high, contributing to reproducibility. The novelty of the proposed methods, particularly the smoothing block and matrix sketch, adds significant value to the field of long sequence modeling. However, the paper could benefit from further discussions on how these methods can be applied in diverse real-world contexts.\n\n# Summary Of The Review\nOverall, SKTformer presents a significant advancement in long sequence modeling through innovative architectural modifications and robust empirical results. While the contributions are substantial, the practical implications of the model could be better articulated to enhance its applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper presents the Skeleton Transformer (SKTformer), a novel architecture designed for efficiently modeling long sequence data while maintaining linear computational complexity. The SKTformer integrates two main mechanisms: a smoothing block for long-range information integration and a matrix sketching approach for effective row and column selection from the input matrix. The authors demonstrate the effectiveness of SKTformer through theoretical analysis and empirical experiments on Long Range Arena (LRA) datasets and time-series forecasting tasks, showing that it outperforms both vanilla Transformers and state-of-the-art alternatives in terms of noise robustness and computational efficiency.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to balancing information retention and noise mitigation through the dual mechanisms of smoothing and matrix sketching. The theoretical foundations provided for the SKTformer's architecture are well-articulated, contributing to the understanding of its efficacy. However, a potential weakness is the lack of detailed comparisons with a broader range of existing methods beyond the mentioned baselines, which may limit the assessment of its relative performance in various contexts. Additionally, the paper could benefit from a more thorough exploration of the empirical results to elucidate the conditions under which SKTformer excels.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, with a logical flow from the introduction to the experimental results. The methodology is presented in a comprehensible manner, making it accessible to readers not deeply familiar with the intricacies of Transformer architectures. The novelty of the SKTformer is evident, particularly in its approach to long-range dependencies in sequence data. However, the reproducibility of the experiments could be enhanced by providing more comprehensive details about the datasets and hyperparameter settings used in the evaluations.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to the field of sequence modeling by introducing SKTformer, which effectively addresses the challenges of noise and computational efficiency in long sequences. The methodology is sound and the results are promising, though further exploration of comparative performance and reproducibility details would strengthen the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the SKTformer, a novel architecture that aims to enhance existing transformer models by integrating Fourier transforms through a combination of Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (IFFT). The authors claim that this approach will improve noise reduction and information preservation in sequence modeling tasks. The experimental results demonstrate slight performance improvements on benchmark datasets; however, these gains are marginal when compared to existing models, raising questions about the practical significance of the proposed architecture.\n\n# Strength And Weaknesses\nThe main strength of the SKTformer lies in its innovative attempt to incorporate Fourier transforms into transformers, which could potentially offer benefits in specific contexts. However, the paper suffers from several weaknesses, including a lack of originality, as it builds on established transformer architectures without introducing fundamentally new concepts. The reliance on a smoothing block raises concerns about over-smoothing and the potential loss of critical information. Additionally, the need for hyperparameter tuning introduces complexity, and the experimental results show variability that suggests the model may lack robustness. The paper does not adequately address the implications of model scalability or overfitting, which further limits its practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written, the clarity suffers due to the complexity of the model's design and the multiple attention mechanisms it employs, which may hinder interpretability. The quality of the empirical results is questionable, given the reported variability in performance metrics. Regarding novelty, the SKTformer does not introduce groundbreaking concepts, which diminishes its significance in the field. Reproducibility could also be challenged by the hyperparameter tuning and the lack of thorough exploration of the model's limitations.\n\n# Summary Of The Review\nOverall, the SKTformer presents an interesting approach to integrating Fourier transforms into transformer architectures; however, its contributions lack originality and practical significance. The model's complexity and the marginal performance improvements raise concerns about its real-world applicability, and several weaknesses undermine its robustness and interpretability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a novel architecture designed specifically for efficiently modeling long sequences in deep learning. The methodology incorporates innovative techniques such as a smoothing block and matrix sketching to enhance performance while reducing computational complexity and memory usage. The findings demonstrate that SKTformer significantly outperforms both traditional Transformers and state-of-the-art models in various tasks, including benchmarks like the Long Range Arena (LRA) and real-world time-series forecasting. The model showcases remarkable efficiency, achieving a 4x speed advantage and an 87% reduction in peak memory usage compared to conventional approaches.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of SKTformer, which addresses the critical issue of efficiency in long sequence modeling, a common challenge in deep learning. The empirical results across diverse datasets provide strong validation for the model's claims of improved performance and efficiency. However, a potential weakness is the need for a deeper exploration of the model's limitations and specific scenarios where it might not perform as well as advertised. Additionally, while the theoretical contributions are solid, the paper could benefit from a more extensive discussion of related work to contextualize its advancements in the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, aiding reader comprehension. The quality of the writing is high, with a logical flow that effectively communicates the model's innovations and results. The novelty of SKTformer is evident in its approach to long sequence modeling, and the authors have made their code publicly available, enhancing reproducibility and encouraging further research and development within the community. However, additional details on implementation specifics would further support reproducibility efforts.\n\n# Summary Of The Review\nOverall, the SKTformer paper presents a significant advancement in the field of long sequence modeling, demonstrating both technical innovation and empirical success. The model's efficiency and performance gains position it as a promising tool for a variety of applications, making it a commendable contribution to the research community.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents the Skeleton Transformer (SKTformer), a novel architecture designed to improve the efficiency and robustness of Transformer models when handling long sequences. Utilizing theoretical insights from Fourier analysis and matrix sketching, SKTformer introduces two key components: the Smoothing Block, which employs Fourier convolution to enhance token matrix smoothness and reduce noise, and the Skeleton Attention mechanism, which modifies standard self-attention for efficient sampling. The theoretical framework is supported by propositions that establish performance bounds and validate the effectiveness of the proposed methods in preserving essential information while maintaining linear computational complexity.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its thorough theoretical grounding and the innovative approaches introduced to tackle the challenges of long sequence modeling in Transformer architectures. The Smoothing Block and Skeleton Attention mechanisms are well-conceived and theoretically justified, providing a clear rationale for their effectiveness. However, the paper could benefit from a more extensive empirical evaluation to complement the theoretical claims. The limitations concerning the sequential use of FFT and IFFT in the Smoothing Block, while acknowledged, warrant further exploration to enhance the overall applicability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and logically structured, making it accessible to readers with a solid understanding of Transformer architectures and theoretical machine learning concepts. The quality of the theoretical analysis is high, with clear definitions and propositions that support the proposed methods. The novelty is notable, as the integration of Fourier analysis with matrix sketching in the context of Transformers is relatively unexplored. Reproducibility may be an issue due to the lack of detailed empirical experiments; providing code or more extensive experimental results would enhance the reproducibility of the findings.\n\n# Summary Of The Review\nOverall, the paper introduces a significant theoretical advancement in Transformer architectures for long sequence modeling through the SKTformer. While the theoretical contributions are robust and innovative, the lack of extensive empirical validation limits the paper's overall impact. Further exploration of the proposed methods in practical scenarios could strengthen the case for their adoption.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a novel Transformer architecture designed for handling long sequence data through two primary innovations: the Smoothing Block and Matrix Sketch Method. The Smoothing Block employs Fourier analysis and convolution to mix information across long sequences, while the Matrix Sketch Method efficiently reduces computational complexity by sampling rows and columns from the input matrix. The evaluation of SKTformer demonstrates state-of-the-art performance on tasks like Long Range Arena and time-series forecasting, with comprehensive experiments validating its components' contributions.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative approach to managing long sequences through the integration of Fourier analysis and sampling techniques, which could have significant implications for efficiency in Transformer architectures. The detailed implementation, including code availability and algorithmic breakdowns, enhances reproducibility. However, the paper has notable weaknesses, including potential performance bottlenecks due to sequential FFT operations and a risk of overfitting stemming from the model's complexity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, presenting a clear narrative of its contributions and methodologies. The quality of the writing is generally high, with detailed explanations of the architecture and experimental setups. The novelty is significant, introducing novel mechanisms to a well-studied problem in deep learning. The reproducibility is supported by the availability of code and detailed training configurations, although some readers may require a deeper understanding of the underlying mathematical concepts for full comprehension.\n\n# Summary Of The Review\nOverall, SKTformer presents a compelling advancement in the field of long sequence data processing, showcasing innovative architectural components and achieving impressive empirical results. While the contributions are significant, the paper could benefit from addressing the limitations regarding computational efficiency and overfitting.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SKTformer, a novel Transformer architecture designed to enhance performance in processing long sequences while maintaining linear computational complexity. The authors introduce a Skeleton Attention mechanism and a smoothing block, claiming that these innovations lead to significant improvements over existing models in various benchmarks, including time-series forecasting and the Long Range Arena. Despite these assertions, the paper's findings suggest that the enhancements are marginal when compared to state-of-the-art models such as FEDformer and Autoformer.\n\n# Strength And Weaknesses\nWhile SKTformer aims to improve upon existing architectures, it relies heavily on concepts already established in prior works like Linformer, Longformer, Sparse Transformer, and Reformer, which also address computational efficiency for long sequences. The novelty of the Skeleton Attention mechanism and the smoothing block is questionable, as similar techniques have been previously explored. Additionally, while the authors report robustness against noise, this characteristic has been demonstrated in other models as well. The experimental results indicate improvements in specific tasks, but these gains are often in line with what has been achieved by earlier efficient Transformers, thus limiting the perceived significance of SKTformer's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly written, making it accessible for readers. However, the claims regarding novelty and the unique contributions of SKTformer are overstated, as many proposed techniques have been previously documented. Reproducibility is not thoroughly addressed, as there is insufficient detail regarding experimental setups and model configurations, which complicates validation of the reported results.\n\n# Summary Of The Review\nOverall, while SKTformer attempts to position itself as a novel solution for long sequence modeling, many of its contributions appear to echo existing methodologies, reducing its impact on the field. The marginal improvements reported in performance raise concerns about the practical significance of the proposed architecture in advancing state-of-the-art results.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a novel transformer architecture designed specifically for long sequence data. The authors propose a skeleton-based approach that optimally captures global correlations and short-term local dependencies, enhancing the performance of existing transformer models. Through extensive experiments, the paper demonstrates that SKTformer achieves state-of-the-art results on several benchmark datasets, achieving an average accuracy improvement over the baseline models.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to handling long sequences, which addresses a common limitation in transformer models. The methodology is well-structured and includes a thorough empirical evaluation, showcasing significant improvements in accuracy. However, the paper suffers from inconsistencies in terminology and notation, which may hinder understanding for some readers. Additionally, while the theoretical contributions are noteworthy, they could be presented more clearly, perhaps with bullet points or summaries to enhance readability.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the clarity of the paper is affected by minor formatting issues and inconsistent terminology, which detract from the overall quality. The novelty of the SKTformer architecture is significant, as it offers a fresh perspective on transformer design for long sequences. However, reproducibility could be improved by providing clearer definitions of mathematical notations and ensuring that the code is easily accessible, ideally through hyperlinks.\n\n# Summary Of The Review\nThe paper presents a promising approach to transformer architecture for long sequence data through the introduction of SKTformer. While the contributions are significant, clarity and consistency in presentation need improvement to enhance understanding and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SKTformer, a novel model aimed at improving long sequence modeling. The authors focus on utilizing a sequential application of Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (IFFT) to enhance the efficiency and performance of sequence processing. The empirical results demonstrate that SKTformer achieves performance improvements over existing models on various benchmarks. However, the paper does not explore the model's applicability to multi-modal data or real-world applications in detail.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to long sequence modeling, particularly through the use of frequency domain techniques like FFT and IFFT. The empirical results are promising, indicating that SKTformer can outperform traditional models in specific scenarios. However, the paper has notable weaknesses, such as a lack of exploration into multi-modal data applicability and a limited discussion of edge cases and robustness. The absence of a detailed comparison with existing models in practical applications further restricts the understanding of SKTformer's utility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, with clear descriptions of the methodology and findings. However, it could benefit from more comprehensive discussions on limitations and potential optimizations. The novelty of SKTformer is evident, but the reproducibility could be improved by providing more details on computational resource requirements and experiments, especially regarding energy consumption and performance on noisy data.\n\n# Summary Of The Review\nOverall, the paper introduces SKTformer as an effective model for long sequence modeling with promising results. However, it lacks a thorough exploration of its applicability to multi-modal data, robustness in challenging scenarios, and a comprehensive analysis of computational efficiency. Addressing these issues could significantly enhance the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces SKTformer, a Skeleton Transformer designed for handling long sequence data, emphasizing the need to balance information preservation while minimizing noise. The authors validate SKTformer's effectiveness through both theoretical and empirical evaluations, demonstrating strong statistical significance in performance metrics such as accuracy and mean squared error (MSE) across various tasks and datasets. The paper includes extensive comparisons with state-of-the-art models, showcasing SKTformer's robustness, particularly in noise handling and time-series forecasting tasks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous statistical analysis, which includes detailed ablation studies and parameter sensitivity evaluations that contribute to a thorough understanding of SKTformer's components. The empirical results are compelling, demonstrating significant improvements over baseline models in both accuracy and MSE. However, a potential weakness is the reliance on specific benchmarks such as the Long Range Arena (LRA) and time-series datasets, which may limit the generalizability of the findings. Furthermore, while the statistical significance is well-documented, the paper could benefit from more extensive discussions on the practical implications of these results in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers with varying levels of expertise. The quality of writing is high, and the use of tables to summarize results enhances clarity. In terms of novelty, SKTformer presents a meaningful advancement in the landscape of long sequence modeling, particularly with its unique architectural components. The reproducibility of the results appears strong due to the detailed statistical validation and comprehensive reporting of performance metrics.\n\n# Summary Of The Review\nOverall, the paper provides a robust contribution to the field of long sequence modeling through the introduction and validation of SKTformer. Its rigorous statistical methodology and significant empirical results support the model's effectiveness, although further exploration of its practical applications could enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SKTformer, a novel architecture that incorporates a sequential application of FFT and IFFT within its design. This approach aims to leverage the benefits of Fourier transformations in processing sequential data. However, the model's performance is found to be highly sensitive to hyperparameter choices, with implications for its robustness across different tasks. The authors suggest potential improvements, such as low-rank approximations for the L matrix in the Fourier Convolution component, but these ideas are not tested in the current work. Overall, while SKTformer demonstrates competitive performance, there are key concerns regarding its computational efficiency, overfitting risks, and the need for further empirical validation.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of Fourier methods into a Transformer architecture, which could open new avenues for research in sequential data processing. However, the paper is weakened by several concerns: the reliance on the size of the L matrix raises overfitting risks, and the lack of explicit strategies to manage this complexity is a significant oversight. Furthermore, the performance dependency on hyperparameter tuning suggests a lack of robustness, which could limit practical applications. There is also insufficient exploration of initialization strategies and computational trade-offs, hindering the potential for future optimizations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly; however, the novelty of the approach could be better substantiated with empirical results. The lack of detailed analysis regarding the computational trade-offs associated with the proposed method detracts from understanding its full impact. Reproducibility may also be a concern due to the unspecified conditions under which the proposed methods excel or fail, particularly regarding noise robustness. Overall, while the methodology is intriguing, the execution lacks thoroughness.\n\n# Summary Of The Review\nIn summary, SKTformer introduces a promising approach by integrating Fourier transformations into Transformer architectures, but it suffers from significant limitations related to overfitting, hyperparameter sensitivity, and a lack of empirical validation. The potential for future research exists, particularly in addressing these shortcomings and exploring the proposed enhancements.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents SKTFORMER, a new transformer architecture aimed at addressing the challenge of processing long sequence data. The main contributions include the introduction of a smoothing block and a matrix sketch mechanism to reduce computational complexity while maintaining information integrity. The authors claim empirical superiority over existing models on standard benchmarks, specifically in terms of training speed and memory efficiency.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its attempt to tackle the well-documented issue of long sequence processing, and it does introduce some interesting architectural modifications. However, the weaknesses are pronounced: the methodology appears to recycle established concepts without significant innovation, and the related work section lacks depth and critical analysis. The experiments utilize common datasets, failing to showcase creativity or robustness in their claims of performance improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally satisfactory, but the quality of the insights presented is lacking due to a lack of original thought. The novelty is minimal, as the proposed methods build on well-known techniques without substantial advancement. Reproducibility is hindered by vague performance claims and the absence of rigorous statistical backing for the results.\n\n# Summary Of The Review\nOverall, SKTFORMER fails to deliver novel insights or significant advancements in the field of transformer architectures for long sequence data. The paper reiterates existing concepts and lacks a compelling argument for its contributions, making it a missed opportunity for meaningful progress.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the SKTformer, a novel transformer architecture designed to enhance performance on sequence-based tasks. It employs Fourier convolution for smoothing and matrix sketching techniques, specifically CUR decomposition, to manage the computational burden associated with large datasets. The authors demonstrate the effectiveness of SKTformer through experiments on GLUE benchmarks, highlighting its linear complexity and resilience to noise, though they primarily focus on established datasets without extensive exploration of diverse data types.\n\n# Strength And Weaknesses\nThe SKTformer represents a significant advancement in transformer architectures, particularly in its approach to smoothing and computational efficiency. However, the paper lacks a discussion on integrating SKTformer with existing hybrid models, such as RNN-transformer combinations, which could enhance its applicability. Additionally, the exploration of transfer learning and domain adaptation is rather limited, despite its potential relevance. While the CUR decomposition method is promising, alternative sketching methods could provide better scalability. The focus on specific benchmarks also raises concerns about the model's generalizability across diverse datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, although some sections could benefit from more comprehensive discussions, particularly concerning the integration with other approaches and the implications of the findings. The novelty of the SKTformer is notable, particularly in its linear complexity and noise resilience. However, reproducibility could be improved through more detailed parameter optimization strategies and a broader range of datasets for validation.\n\n# Summary Of The Review\nThe SKTformer presents a compelling innovation in transformer architecture, particularly regarding computational efficiency and noise resilience. Nevertheless, there are areas for improvement, including the integration with existing methods, exploration of transfer learning, and validation on a wider array of datasets. Overall, while the paper makes significant contributions, it could benefit from deeper exploration and broader applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper presents SKTformer, a novel transformer model designed to enhance performance in handling long sequences across various tasks. The authors demonstrate its contributions through extensive benchmarking, revealing that SKTformer achieves state-of-the-art accuracy on the Long Range Arena (LRA) dataset, outperforming previous models like Transformer-LS by a notable margin. Additionally, SKTformer shows comparable results in long-term time series forecasting, specifically outperforming FEDformer on certain datasets, and exhibits strong performance on GLUE tasks. The model also emphasizes training speed and memory efficiency, claiming a 4x speed advantage and 87% memory reduction, while maintaining robustness against noise.\n\n# Strength And Weaknesses\nThe strengths of the paper include its substantial contributions to advancing the state-of-the-art in long sequence modeling, demonstrated through rigorous benchmarking against existing models. The efficiency gains in training speed and memory use are particularly noteworthy, making SKTformer an attractive option for practical applications. However, the paper could be critiqued for potentially lacking detailed explanations of the theoretical underpinnings of some components, such as Column Attention, which would enhance the understanding of their contributions. Additionally, the results may benefit from a broader range of comparative models to further validate its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, with adequate descriptions of the methodologies employed. The empirical results are robust and supported by ablation studies that clarify the contributions of individual components. However, while the results are impressive, the novelty of the model architecture itself may not be as pronounced when compared to existing transformer variants. Reproducibility appears feasible, given the detailed reporting of experimental setups and parameters used.\n\n# Summary Of The Review\nOverall, SKTformer represents a significant advancement in the field of long sequence modeling, achieving state-of-the-art results across multiple benchmarks while demonstrating impressive efficiency. Its contributions to training speed, memory efficiency, and robustness enhance its practicality, although further elucidation of certain components could strengthen the theoretical framework.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents SKTformer, a novel framework that integrates matrix sketching techniques to enhance the efficiency and effectiveness of transformer models. The authors propose a new architecture that employs smoothing blocks and a matrix sketch method to reduce computational overhead while maintaining competitive performance on several benchmark tasks, including GLUE. The findings demonstrate that SKTformer not only improves upon previous transformer implementations but also achieves significant speedups in training and inference times.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to optimizing transformer architectures through matrix sketching, which is a relatively unexplored area in the context of deep learning. The empirical results provide strong support for the effectiveness of SKTformer, showcasing improvements in both accuracy and efficiency. However, the paper suffers from clarity issues, particularly in the introduction and the use of complex sentences, which may hinder reader comprehension. The discussion of limitations is also somewhat superficial, lacking a thorough exploration of the potential challenges and future directions for the proposed model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by lengthy and complex sentences, which could be simplified to enhance reader understanding. The terminology used is sometimes inconsistent or overly technical, which may alienate readers unfamiliar with specific concepts. The quality of the figures and tables could be improved by providing context or explanations for their relevance. In terms of novelty, SKTformer presents a fresh perspective on transformer optimization; however, the reproducibility of results is partially affected by insufficient detail in the experimental process and a lack of explicit explanations in the ablation study.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in transformer architecture through SKTformer, demonstrating both technical and empirical contributions. However, clarity issues and a lack of depth in discussing limitations and experimental details detract from its overall effectiveness.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3060216757811434,
    -1.9873924848540585,
    -1.9210365291550953,
    -1.7473456190709031,
    -1.951827590101217,
    -1.7407683414614588,
    -1.6747836473545865,
    -1.8999212768718203,
    -1.7585081515662297,
    -1.9404336392573753,
    -1.7659378464462296,
    -1.4157432543913173,
    -1.710097589391012,
    -1.7312363341161667,
    -1.8107176974589931,
    -1.594262574754742,
    -1.8221664041820884,
    -1.6085044783108777,
    -1.728853971381193,
    -1.876835844657409,
    -1.9248276520117686,
    -1.7067650804643721,
    -1.7603653854414458,
    -1.7423912293519634,
    -1.9526246771034268,
    -2.1179826172812377,
    -1.9223489082282963,
    -1.8327279344728964,
    -1.822974089751631
  ],
  "logp_cond": [
    [
      0.0,
      -2.211861210458474,
      -2.2032335986755287,
      -2.220664698783098,
      -2.235471042971734,
      -2.2171821739289133,
      -2.245881557319552,
      -2.2370815307308294,
      -2.2105673339044367,
      -2.230086157734467,
      -2.2216862066212575,
      -2.2653634886400913,
      -2.233441208173865,
      -2.2258970113743395,
      -2.228317236623316,
      -2.2059049396108357,
      -2.2504772112599696,
      -2.2388784618947795,
      -2.2437287664393333,
      -2.214574216010259,
      -2.2400611225703138,
      -2.250698914503906,
      -2.256924421747743,
      -2.224773735078753,
      -2.258534616791658,
      -2.246277692216891,
      -2.2305246017058726,
      -2.2356688787536667,
      -2.258462580359129
    ],
    [
      -1.5347624150985766,
      0.0,
      -1.3507260176256557,
      -1.462240885647177,
      -1.4348159268368275,
      -1.4899652811225899,
      -1.6128868845136632,
      -1.5886651230030338,
      -1.4770819187566142,
      -1.5740709242335102,
      -1.4044063195513248,
      -1.7166722708416495,
      -1.5251830072113488,
      -1.4797474072625472,
      -1.5302004823177127,
      -1.4848130582454102,
      -1.5747296383306777,
      -1.5369076907216512,
      -1.48452272007067,
      -1.4183777632385117,
      -1.5268239606512568,
      -1.6515063818269675,
      -1.5903497926536045,
      -1.4718866564640316,
      -1.5829039921411183,
      -1.5855691452757745,
      -1.633268425844648,
      -1.5775661831831302,
      -1.5699596431422727
    ],
    [
      -1.5509317981570647,
      -1.3747715940740322,
      0.0,
      -1.427026248746642,
      -1.3975335805194706,
      -1.551496696425668,
      -1.6330732925470222,
      -1.5371629348008946,
      -1.4466782296956482,
      -1.5181770516596316,
      -1.5317427811746465,
      -1.6406045094915536,
      -1.5055256352160606,
      -1.4499313884031622,
      -1.4752936510905155,
      -1.4430122822535387,
      -1.5118432398517263,
      -1.5695786190546583,
      -1.5187915551956943,
      -1.4679100052135257,
      -1.5735155237607308,
      -1.5900922035547274,
      -1.5791548408152931,
      -1.4712597391346973,
      -1.504266834864026,
      -1.5633376897545,
      -1.5178749785130825,
      -1.552250289414685,
      -1.5256520569437635
    ],
    [
      -1.3911446760943589,
      -1.2125560519072356,
      -1.2189297668615922,
      0.0,
      -1.3851945897483446,
      -1.3380647440719775,
      -1.3832114246691938,
      -1.3919857270115161,
      -1.2549363514756848,
      -1.3504040812640659,
      -1.3592478138509592,
      -1.4987418655471307,
      -1.3021060301191476,
      -1.2718715900008402,
      -1.3527878096215098,
      -1.2871565067554778,
      -1.3830233800504026,
      -1.3573858763852238,
      -1.3565567794344273,
      -1.2613482744275621,
      -1.3463250235060726,
      -1.40927617454705,
      -1.408529941900943,
      -1.3641319869316406,
      -1.4129957038276917,
      -1.3308730303407283,
      -1.3744909013529885,
      -1.3633688235745194,
      -1.3716856307026133
    ],
    [
      -1.6313329002848107,
      -1.446394690011568,
      -1.3788040337519563,
      -1.6027794858492512,
      0.0,
      -1.5717236828615089,
      -1.6040478635040896,
      -1.5807401036987563,
      -1.5148133371311607,
      -1.5575178551310855,
      -1.5547031861642775,
      -1.672109130527323,
      -1.6273266423498711,
      -1.5467588503976164,
      -1.5636124208768656,
      -1.5564189485381603,
      -1.5536891602135583,
      -1.5837405553518638,
      -1.5117858999223368,
      -1.589377851715986,
      -1.5971551946854516,
      -1.66091299808433,
      -1.6348112884782955,
      -1.519764677648337,
      -1.6270509549067178,
      -1.6781541560942659,
      -1.5782243009988175,
      -1.6208551829513884,
      -1.621559780739084
    ],
    [
      -1.4028556504424619,
      -1.2716964587226727,
      -1.3236919501145576,
      -1.3277998040492227,
      -1.3287550575230307,
      0.0,
      -1.3677504115518113,
      -1.3715123136354361,
      -1.3443303207493205,
      -1.3555694331334156,
      -1.285465405728531,
      -1.445105399052063,
      -1.3502853494034335,
      -1.3317692140782802,
      -1.3786254709681836,
      -1.3393004981169834,
      -1.3638390277315597,
      -1.3899276797438371,
      -1.4172101072622065,
      -1.3332858081677907,
      -1.3217330943720131,
      -1.4073264973443922,
      -1.3968062774998564,
      -1.2985824091720342,
      -1.345822673209749,
      -1.3800268053860845,
      -1.4089842094405272,
      -1.387718730823456,
      -1.421579475794986
    ],
    [
      -1.3497254362969102,
      -1.2524620196632816,
      -1.2942265929606551,
      -1.2206048729205787,
      -1.2707042931934336,
      -1.248836271099022,
      0.0,
      -1.2501891533248513,
      -1.3065070839367077,
      -1.25106220078336,
      -1.2700938142556297,
      -1.3328617218008874,
      -1.249058538854296,
      -1.2835916767539541,
      -1.2769835996032053,
      -1.2742784130819347,
      -1.3011719913446496,
      -1.259428988637589,
      -1.3326401411718272,
      -1.2787304398182084,
      -1.2780598367832734,
      -1.2441256586297764,
      -1.3206282235293212,
      -1.2378257262640744,
      -1.318972690768208,
      -1.2774727852725654,
      -1.3143904255421073,
      -1.2592793861142735,
      -1.2865676828672767
    ],
    [
      -1.5827674168783346,
      -1.5157979430753032,
      -1.4905697789185088,
      -1.5644836817767431,
      -1.5180129960106068,
      -1.5057980828110982,
      -1.5770374570666716,
      0.0,
      -1.5565274947404588,
      -1.5275484948890214,
      -1.5121036129029586,
      -1.6494041975549774,
      -1.5061139425731582,
      -1.5537953048902446,
      -1.557559196654374,
      -1.5098573987401496,
      -1.539558030363095,
      -1.5629549135901817,
      -1.5689476192299614,
      -1.5362132026330844,
      -1.5628270239123774,
      -1.5445657344503108,
      -1.54371701727615,
      -1.4769515989721427,
      -1.5661361965240213,
      -1.5594752901206301,
      -1.553358366520231,
      -1.5639176243753308,
      -1.5740501857436187
    ],
    [
      -1.42445990928509,
      -1.282341504162018,
      -1.2745584114291455,
      -1.3537936998724873,
      -1.3629954319894084,
      -1.4122120396738416,
      -1.4649299594238407,
      -1.451528478729539,
      0.0,
      -1.428960087342945,
      -1.397228904934037,
      -1.5135119473051024,
      -1.401922415264164,
      -1.356096877745436,
      -1.382737035543791,
      -1.4027350940176246,
      -1.412633052906634,
      -1.4098887322262255,
      -1.4219617440079189,
      -1.3902681376195212,
      -1.4304605213569135,
      -1.460214073278204,
      -1.463499708402636,
      -1.4124952822140178,
      -1.4381900687348423,
      -1.4258983584281675,
      -1.4255002231610079,
      -1.4339923404740798,
      -1.4261332272323115
    ],
    [
      -1.6057864464260543,
      -1.5850847939836048,
      -1.5743123189208306,
      -1.600886522409864,
      -1.5893391118694515,
      -1.568373130340379,
      -1.6591562475815445,
      -1.5917352027497174,
      -1.658083526247341,
      0.0,
      -1.5849208070243976,
      -1.7266541240600646,
      -1.60131801367961,
      -1.6206067262148431,
      -1.6775547081534115,
      -1.5934007702221267,
      -1.6096065976295346,
      -1.63604840701236,
      -1.61869962385406,
      -1.5972302249285497,
      -1.6164942687364428,
      -1.6649165406741153,
      -1.67433984790249,
      -1.5852618020677325,
      -1.5910472923381114,
      -1.6544068173291129,
      -1.5825407343838815,
      -1.6309656485875748,
      -1.6814920990577995
    ],
    [
      -1.288304051176688,
      -1.0672377868795149,
      -1.2114406602252314,
      -1.2557243118784802,
      -1.2050446300119908,
      -1.2025134518745146,
      -1.2841438418271067,
      -1.269533241266321,
      -1.2401086358772195,
      -1.2261187009605392,
      0.0,
      -1.3908970547845125,
      -1.2448722686789873,
      -1.2099046577322325,
      -1.2814761545603472,
      -1.1993964526240437,
      -1.3140177117979153,
      -1.3031211491450365,
      -1.247263247879285,
      -1.2242459668639283,
      -1.246141318423932,
      -1.325099176092409,
      -1.3140338842475667,
      -1.1577135534844643,
      -1.3180566801917692,
      -1.356768243370262,
      -1.3075449733333506,
      -1.3279921982201466,
      -1.3416292566910721
    ],
    [
      -1.2268085430850795,
      -1.2032405074308545,
      -1.17960155696638,
      -1.1902050411624026,
      -1.1803213153169307,
      -1.2099450656950184,
      -1.1891625759866407,
      -1.171425385039508,
      -1.1996863228942467,
      -1.18041394411933,
      -1.2021567508856177,
      0.0,
      -1.2050552869904612,
      -1.147082856538218,
      -1.150629469440883,
      -1.1931350319695837,
      -1.181637205199296,
      -1.188889610488704,
      -1.1965052006713415,
      -1.1940113689767766,
      -1.1877997958030022,
      -1.2028166487845904,
      -1.193360805313428,
      -1.164899107811709,
      -1.191891895468676,
      -1.2043638754416637,
      -1.1767693221806834,
      -1.1815588014023324,
      -1.164682153192572
    ],
    [
      -1.3129264935626432,
      -1.1986926656476182,
      -1.1810354583685005,
      -1.1753237146985798,
      -1.3229288451654346,
      -1.279657131840983,
      -1.2760448495657737,
      -1.2690264742771586,
      -1.248171339045285,
      -1.3097202526480478,
      -1.3172657754460102,
      -1.4477178598265141,
      0.0,
      -1.2211663961815424,
      -1.274162725727667,
      -1.169333692943339,
      -1.324580313199423,
      -1.2957846130857118,
      -1.3362401710704512,
      -1.2182121532247376,
      -1.293285640530266,
      -1.3108306289999805,
      -1.3458150329930605,
      -1.2987239653739595,
      -1.3540417934002238,
      -1.274352223467678,
      -1.3358654076598129,
      -1.3023980035718616,
      -1.323702894037143
    ],
    [
      -1.3469817692709523,
      -1.180813029962335,
      -1.1745011374554717,
      -1.1752982473684521,
      -1.2778002164520974,
      -1.2671779020087073,
      -1.3330055177812703,
      -1.3331098677817594,
      -1.2153542392682033,
      -1.3289868195914873,
      -1.2916377735299294,
      -1.4018897918205964,
      -1.2333398969822407,
      0.0,
      -1.208536605413903,
      -1.2661470406445157,
      -1.3673762173818473,
      -1.3089269675517292,
      -1.2775155349106977,
      -1.2340412345005367,
      -1.2902739109358041,
      -1.3739314012491177,
      -1.3670354771741693,
      -1.2877481493072682,
      -1.3809475114165908,
      -1.2783774397620586,
      -1.332808214857137,
      -1.3648613255349458,
      -1.2966229406605723
    ],
    [
      -1.4239016335597525,
      -1.3130074872781339,
      -1.262536300833184,
      -1.2907630991167602,
      -1.3256575323026851,
      -1.3646172933662017,
      -1.403860056387918,
      -1.3623977093017985,
      -1.2767973583700627,
      -1.4376445643695772,
      -1.385731955941864,
      -1.4086718748250617,
      -1.3522100107113464,
      -1.2848954765395477,
      0.0,
      -1.2832810702322648,
      -1.3854198739617698,
      -1.3154671896247045,
      -1.3494289981127598,
      -1.292030599117272,
      -1.3352813373637775,
      -1.423889294955704,
      -1.4120277465641733,
      -1.2838922111546907,
      -1.3977018072748433,
      -1.344437543534584,
      -1.389081244036565,
      -1.336679798513217,
      -1.3082806387853256
    ],
    [
      -1.2362703922708862,
      -1.1143932081147037,
      -1.104272664524571,
      -1.083191733840055,
      -1.1949831426006785,
      -1.1993983283510596,
      -1.2457987770718235,
      -1.188589873374847,
      -1.1874363050462138,
      -1.2032552470883267,
      -1.1675322931689962,
      -1.3437981900783522,
      -1.1152801101365182,
      -1.1390759433328104,
      -1.1805100565454398,
      0.0,
      -1.2520304285377646,
      -1.214672051315054,
      -1.2369192500360016,
      -1.1931887718735756,
      -1.1781145753395605,
      -1.2734809234723445,
      -1.2592718719773512,
      -1.146713978419504,
      -1.2519120723135355,
      -1.2493742269801995,
      -1.2297734632052155,
      -1.2742647159388838,
      -1.2345075407747352
    ],
    [
      -1.4877477335504252,
      -1.4149251866086685,
      -1.4102976682177384,
      -1.4541284058324484,
      -1.4160809271002648,
      -1.4470059648907911,
      -1.5384483623170535,
      -1.4658798643807802,
      -1.4702423424508577,
      -1.3992933321579,
      -1.4591874104508242,
      -1.5717896753537342,
      -1.4488951811816349,
      -1.4886121216387063,
      -1.5205717051359582,
      -1.4448088301345223,
      0.0,
      -1.463053596833803,
      -1.4868669369448013,
      -1.4587067385330825,
      -1.4286241260051338,
      -1.5066585506779235,
      -1.4693858290028539,
      -1.4524531024495224,
      -1.4259450858245155,
      -1.4652483525210276,
      -1.4755639225846775,
      -1.4922801365542726,
      -1.5153054761899152
    ],
    [
      -1.2790823272707592,
      -1.1494929604278146,
      -1.2031195644085921,
      -1.1585804278020777,
      -1.1886156361353921,
      -1.2497302498553042,
      -1.2268080958818226,
      -1.2561227963887494,
      -1.205830300492633,
      -1.2461738678565397,
      -1.240585275241547,
      -1.3320502534817598,
      -1.2120480806327019,
      -1.1735674470100785,
      -1.1923651427478272,
      -1.1680056506233842,
      -1.2434482421973845,
      0.0,
      -1.2729528410849162,
      -1.2084449060702678,
      -1.2079975378519892,
      -1.2714254267668124,
      -1.2703776025251108,
      -1.2024806499402039,
      -1.258649684248634,
      -1.2130713853264192,
      -1.2670624053124473,
      -1.1737276614389305,
      -1.200982811698305
    ],
    [
      -1.4617050949148063,
      -1.2782278696402365,
      -1.3277524304815018,
      -1.3745249973489144,
      -1.325603220092018,
      -1.387316245478611,
      -1.4378638671572914,
      -1.4021624314583574,
      -1.3727161974323137,
      -1.3969718196205045,
      -1.383797934530364,
      -1.4531110449073534,
      -1.3734539124641327,
      -1.3181105522426106,
      -1.3475584739425206,
      -1.3586039536947485,
      -1.3940937718220279,
      -1.4408709496588628,
      0.0,
      -1.3719676439246113,
      -1.3799025151669173,
      -1.4320204056882144,
      -1.4133852007118346,
      -1.3935037828346475,
      -1.3823508019996726,
      -1.412017254189974,
      -1.3721123088812188,
      -1.4308449411008652,
      -1.3761960881011965
    ],
    [
      -1.4744760457574828,
      -1.2591856127635652,
      -1.2671322060828527,
      -1.3318364814683934,
      -1.4575854508629138,
      -1.357527441703555,
      -1.506892584802967,
      -1.480632403668041,
      -1.4015276114668398,
      -1.4602778842021287,
      -1.4702528934314283,
      -1.5577647320568064,
      -1.3672248245269745,
      -1.3775547841180973,
      -1.4235858172305513,
      -1.4333709708458595,
      -1.4727324293678112,
      -1.459536850863957,
      -1.4638518636203273,
      0.0,
      -1.477691271812655,
      -1.4740215385343647,
      -1.483997802743136,
      -1.4360993777093605,
      -1.4669863983392526,
      -1.4254582888433558,
      -1.4799707819399583,
      -1.467283414786811,
      -1.4333378134625865
    ],
    [
      -1.586567588477475,
      -1.4725342159703418,
      -1.555974452647266,
      -1.4947761809720017,
      -1.5175200260919497,
      -1.4751625004451605,
      -1.548878212869117,
      -1.5430641363253135,
      -1.5584002820759666,
      -1.5434288613950393,
      -1.4776157088803485,
      -1.6578633196854773,
      -1.5289681217319353,
      -1.5013620315404714,
      -1.5525418822550856,
      -1.4719748719539054,
      -1.5654762581181274,
      -1.514970890156762,
      -1.569784344221108,
      -1.5225710197520048,
      0.0,
      -1.570608496106867,
      -1.5913766640772027,
      -1.5172214929391727,
      -1.5618585938985974,
      -1.5410637708106654,
      -1.5912765665124455,
      -1.5183078807796724,
      -1.5835893659055638
    ],
    [
      -1.308210622295278,
      -1.2607286346543725,
      -1.295463283641923,
      -1.272780664545537,
      -1.2992102857642702,
      -1.2966078046083778,
      -1.2826214248617647,
      -1.277711497909522,
      -1.3329879508099194,
      -1.306974564015075,
      -1.284884509024171,
      -1.3517072768032512,
      -1.2961584131773385,
      -1.3047366089903667,
      -1.3084603980398997,
      -1.2879345635513233,
      -1.296080409767539,
      -1.28690888162333,
      -1.330092447250895,
      -1.269045380817579,
      -1.3179245424277881,
      0.0,
      -1.2957095426974437,
      -1.219396534593767,
      -1.3267842195818613,
      -1.3432029185916703,
      -1.3417082125222561,
      -1.279313484582301,
      -1.2869634714362241
    ],
    [
      -1.3909016941965218,
      -1.2812278720882153,
      -1.3369134322161087,
      -1.3547539104791044,
      -1.3251677873488479,
      -1.3331096157262194,
      -1.350590673976294,
      -1.320762780691545,
      -1.3354447775879434,
      -1.3185817838465261,
      -1.3160475978878623,
      -1.4140348474897555,
      -1.3406782762663312,
      -1.3339745944403822,
      -1.3536859038846536,
      -1.32028010116034,
      -1.313913283401582,
      -1.3092167727691364,
      -1.3699251373202608,
      -1.3099306335350165,
      -1.3347827321530072,
      -1.365548896444081,
      0.0,
      -1.2911561349670082,
      -1.2404033025512344,
      -1.3796013726425786,
      -1.341381350879056,
      -1.344748435776345,
      -1.382985926443216
    ],
    [
      -1.4075490011224583,
      -1.278430907437694,
      -1.3245174673362408,
      -1.3437030327328539,
      -1.3242851630343422,
      -1.3423489071756922,
      -1.363204510096069,
      -1.352871999721668,
      -1.3824226760864613,
      -1.3742338875156568,
      -1.2898289438195407,
      -1.4244848177785931,
      -1.3520946786356682,
      -1.3653768065893652,
      -1.3379296858732963,
      -1.2795339427080363,
      -1.3845765277164448,
      -1.3744031593122679,
      -1.3660042169990843,
      -1.353146696434421,
      -1.337582855620726,
      -1.3748505364072512,
      -1.3866490082928198,
      0.0,
      -1.3796317941096186,
      -1.4224958434619766,
      -1.4009791244235,
      -1.3102242530534418,
      -1.3856721446468319
    ],
    [
      -1.6908431791258456,
      -1.5629647031083354,
      -1.561997725336451,
      -1.6348987676050695,
      -1.6142998039015954,
      -1.5927701023920706,
      -1.6498684373875927,
      -1.5975307810295036,
      -1.6419510407816966,
      -1.5738444034921648,
      -1.6249347902507172,
      -1.6927727565622324,
      -1.6353042265066755,
      -1.6521034185166745,
      -1.6279584517301502,
      -1.6240570377003012,
      -1.5612173929580992,
      -1.6527788942846713,
      -1.5797934945834138,
      -1.582601698457297,
      -1.612289254172352,
      -1.6573039895245887,
      -1.601278063119536,
      -1.6268053062730017,
      0.0,
      -1.646806699103426,
      -1.6045825963432996,
      -1.6195925553952717,
      -1.624923925518501
    ],
    [
      -1.6675502720797943,
      -1.522044121797289,
      -1.58914754419076,
      -1.5066353834615236,
      -1.703929164963597,
      -1.5916610147714945,
      -1.6246129618692986,
      -1.6110830549411337,
      -1.6309880148655835,
      -1.6702917376236848,
      -1.6659444136050578,
      -1.801278403518589,
      -1.55187328466415,
      -1.6245093293358626,
      -1.6012771652333186,
      -1.590749319688224,
      -1.6116311634554137,
      -1.6087218493191946,
      -1.658246648404496,
      -1.5515779914567311,
      -1.6083430204291034,
      -1.657197817159503,
      -1.6653516386972695,
      -1.6463886467458027,
      -1.6739943631159313,
      0.0,
      -1.6782400045016637,
      -1.6530574014100694,
      -1.6658463344396224
    ],
    [
      -1.5602995902966597,
      -1.5386051703662051,
      -1.4725562349840124,
      -1.5510382204579203,
      -1.4836976388672312,
      -1.5868856265168028,
      -1.6238477442783243,
      -1.531286051264847,
      -1.5320614234408148,
      -1.4306347889271953,
      -1.5663200647964253,
      -1.624737871255522,
      -1.5570325426587075,
      -1.4763980485196868,
      -1.534966515413779,
      -1.5322342762307952,
      -1.5465210378421148,
      -1.5956731249982445,
      -1.5098363281760474,
      -1.557184890228063,
      -1.566409756608852,
      -1.585332361991953,
      -1.602660234189959,
      -1.5904490385978804,
      -1.5236409869997296,
      -1.558247458104719,
      0.0,
      -1.603051848718316,
      -1.5041379277897948
    ],
    [
      -1.5328621355637775,
      -1.4500227570349438,
      -1.4832413873539787,
      -1.4647344871654662,
      -1.5086901852038754,
      -1.4885166084988701,
      -1.512627042480368,
      -1.4973644011032334,
      -1.5315164846888483,
      -1.510769547322788,
      -1.5117807317682599,
      -1.5991778946619948,
      -1.5001772423706037,
      -1.5019116972875388,
      -1.5033580572187675,
      -1.4993331607009024,
      -1.5207532015220715,
      -1.4873446842566929,
      -1.53581131055544,
      -1.5007989622925801,
      -1.4454435643615524,
      -1.5314265226011714,
      -1.5434719165508846,
      -1.4377111813004935,
      -1.5181299044889867,
      -1.5061219750074695,
      -1.53175545242485,
      0.0,
      -1.511602484227127
    ],
    [
      -1.5320124335576941,
      -1.401836371768305,
      -1.3719169628397354,
      -1.434847657190476,
      -1.4489437857801246,
      -1.4879676337489276,
      -1.5092673706148674,
      -1.4882401304605128,
      -1.4394678223568949,
      -1.4668919086952934,
      -1.5021848923892962,
      -1.528023111891413,
      -1.4338018128068037,
      -1.4032656562385208,
      -1.3783255558863317,
      -1.445306580641309,
      -1.454893405927457,
      -1.4113563548346366,
      -1.4374070589838261,
      -1.4161626804051444,
      -1.5077871241203542,
      -1.4673605585889533,
      -1.485431720209598,
      -1.4601669214723159,
      -1.4782261504066134,
      -1.4388817924823074,
      -1.4349615883412579,
      -1.4853420766332641,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.0941604653226693,
      0.10278807710561466,
      0.08535697699804556,
      0.07055063280940921,
      0.08883950185223011,
      0.060140118461591285,
      0.06894014505031398,
      0.09545434187670665,
      0.07593551804667653,
      0.08433546915988588,
      0.04065818714105207,
      0.07258046760727854,
      0.08012466440680388,
      0.07770443915782721,
      0.1001167361703077,
      0.055544464521173786,
      0.0671432138863639,
      0.06229290934181009,
      0.09144745977088453,
      0.06596055321082961,
      0.055322761277237564,
      0.04909725403340026,
      0.0812479407023905,
      0.047487058989485575,
      0.05974398356425237,
      0.07549707407527073,
      0.07035279702747665,
      0.04755909542201442
    ],
    [
      0.4526300697554819,
      0.0,
      0.6366664672284028,
      0.5251515992068816,
      0.552576558017231,
      0.49742720373146865,
      0.37450560034039526,
      0.39872736185102475,
      0.5103105660974443,
      0.41332156062054826,
      0.5829861653027337,
      0.270720214012409,
      0.4622094776427097,
      0.5076450775915113,
      0.45719200253634584,
      0.5025794266086483,
      0.4126628465233808,
      0.4504847941324073,
      0.5028697647833884,
      0.5690147216155468,
      0.4605685242028017,
      0.335886103027091,
      0.39704269220045396,
      0.5155058283900269,
      0.4044884927129402,
      0.401823339578284,
      0.3541240590094106,
      0.40982630167092826,
      0.41743284171178585
    ],
    [
      0.37010473099803054,
      0.5462649350810631,
      0.0,
      0.4940102804084532,
      0.5235029486356246,
      0.36953983272942725,
      0.2879632366080731,
      0.38387359435420065,
      0.4743582994594471,
      0.4028594774954637,
      0.3892937479804488,
      0.2804320196635417,
      0.4155108939390346,
      0.4711051407519331,
      0.4457428780645798,
      0.47802424690155654,
      0.40919328930336896,
      0.3514579101004369,
      0.40224497395940095,
      0.4531265239415696,
      0.3475210053943645,
      0.3309443256003679,
      0.34188168833980215,
      0.44977679002039794,
      0.41676969429106925,
      0.3576988394005953,
      0.40316155064201276,
      0.36878623974041025,
      0.39538447221133177
    ],
    [
      0.35620094297654425,
      0.5347895671636675,
      0.528415852209311,
      0.0,
      0.3621510293225585,
      0.4092808749989256,
      0.36413419440170935,
      0.355359892059387,
      0.4924092675952183,
      0.3969415378068373,
      0.38809780521994397,
      0.2486037535237724,
      0.4452395889517555,
      0.475474029070063,
      0.39455780944939334,
      0.46018911231542536,
      0.36432223902050054,
      0.38995974268567934,
      0.3907888396364758,
      0.485997344643341,
      0.4010205955648305,
      0.33806944452385324,
      0.3388156771699602,
      0.3832136321392625,
      0.33434991524321145,
      0.4164725887301748,
      0.3728547177179147,
      0.38397679549638375,
      0.37565998836828984
    ],
    [
      0.3204946898164063,
      0.5054329000896489,
      0.5730235563492607,
      0.34904810425196575,
      0.0,
      0.3801039072397081,
      0.3477797265971274,
      0.3710874864024607,
      0.43701425297005625,
      0.39430973497013144,
      0.3971244039369395,
      0.27971845957389396,
      0.32450094775134586,
      0.4050687397036006,
      0.3882151692243514,
      0.39540864156305666,
      0.3981384298876587,
      0.3680870347493532,
      0.4400416901788802,
      0.362449738385231,
      0.35467239541576534,
      0.29091459201688696,
      0.3170163016229215,
      0.4320629124528801,
      0.3247766351944992,
      0.2736734340069511,
      0.3736032891023995,
      0.33097240714982856,
      0.33026780936213296
    ],
    [
      0.3379126910189969,
      0.4690718827387861,
      0.4170763913469011,
      0.41296853741223605,
      0.41201328393842807,
      0.0,
      0.37301792990964744,
      0.3692560278260226,
      0.3964380207121383,
      0.38519890832804315,
      0.4553029357329277,
      0.2956629424093957,
      0.3904829920580253,
      0.40899912738317856,
      0.3621428704932752,
      0.4014678433444754,
      0.376929313729899,
      0.3508406617176216,
      0.3235582341992522,
      0.407482533293668,
      0.4190352470894456,
      0.33344184411706657,
      0.34396206396160234,
      0.4421859322894246,
      0.39494566825170985,
      0.3607415360753743,
      0.3317841320209316,
      0.3530496106380028,
      0.31918886566647275
    ],
    [
      0.3250582110576763,
      0.4223216276913049,
      0.3805570543939314,
      0.4541787744340078,
      0.4040793541611529,
      0.42594737625556456,
      0.0,
      0.42459449402973526,
      0.36827656341787884,
      0.42372144657122646,
      0.4046898330989568,
      0.34192192555369916,
      0.42572510850029044,
      0.3911919706006324,
      0.3978000477513812,
      0.40050523427265183,
      0.37361165600993695,
      0.4153546587169976,
      0.34214350618275935,
      0.39605320753637807,
      0.3967238105713131,
      0.4306579887248101,
      0.3541554238252653,
      0.4369579210905121,
      0.3558109565863785,
      0.39731086208202115,
      0.36039322181247924,
      0.41550426124031303,
      0.3882159644873098
    ],
    [
      0.3171538599934858,
      0.3841233337965171,
      0.4093514979533115,
      0.3354375950950772,
      0.38190828086121353,
      0.39412319406072216,
      0.3228838198051487,
      0.0,
      0.3433937821313615,
      0.3723727819827989,
      0.38781766396886175,
      0.25051707931684297,
      0.3938073342986621,
      0.3461259719815757,
      0.3423620802174463,
      0.39006387813167076,
      0.3603632465087254,
      0.33696636328163865,
      0.33097365764185893,
      0.36370807423873597,
      0.33709425295944295,
      0.3553555424215096,
      0.35620425959567026,
      0.42296967789967765,
      0.3337850803477991,
      0.3404459867511902,
      0.34656291035158926,
      0.33600365249648956,
      0.3258710911282017
    ],
    [
      0.3340482422811397,
      0.47616664740421166,
      0.48394974013708425,
      0.4047144516937424,
      0.3955127195768213,
      0.3462961118923882,
      0.293578192142389,
      0.3069796728366907,
      0.0,
      0.32954806422328464,
      0.3612792466321928,
      0.24499620426112734,
      0.3565857363020657,
      0.40241127382079367,
      0.37577111602243884,
      0.3557730575486051,
      0.34587509865959576,
      0.3486194193400043,
      0.3365464075583109,
      0.3682400139467086,
      0.32804763020931627,
      0.2982940782880257,
      0.29500844316359376,
      0.346012869352212,
      0.3203180828313874,
      0.33260979313806227,
      0.33300792840522186,
      0.32451581109215,
      0.33237492433391824
    ],
    [
      0.33464719283132105,
      0.35534884527377053,
      0.36612132033654476,
      0.33954711684751127,
      0.3510945273879238,
      0.3720605089169964,
      0.2812773916758309,
      0.34869843650765797,
      0.2823501130100343,
      0.0,
      0.3555128322329777,
      0.21377951519731075,
      0.3391156255777654,
      0.3198269130425322,
      0.2628789311039639,
      0.34703286903524866,
      0.3308270416278407,
      0.30438523224501535,
      0.32173401540331525,
      0.3432034143288256,
      0.3239393705209326,
      0.27551709858326,
      0.2660937913548853,
      0.35517183718964285,
      0.34938634691926396,
      0.28602682192826245,
      0.3578929048734938,
      0.3094679906698006,
      0.2589415401995758
    ],
    [
      0.47763379526954175,
      0.6987000595667148,
      0.5544971862209982,
      0.5102135345677494,
      0.5608932164342388,
      0.563424394571715,
      0.4817940046191229,
      0.4964046051799087,
      0.5258292105690101,
      0.5398191454856904,
      0.0,
      0.3750407916617171,
      0.5210655777672424,
      0.5560331887139971,
      0.4844616918858824,
      0.5665413938221859,
      0.4519201346483144,
      0.4628166973011931,
      0.5186745985669445,
      0.5416918795823014,
      0.5197965280222976,
      0.44083867035382074,
      0.4519039621986629,
      0.6082242929617654,
      0.4478811662544604,
      0.4091696030759677,
      0.458392873112879,
      0.43794564822608306,
      0.4243085897551575
    ],
    [
      0.18893471130623785,
      0.21250274696046278,
      0.2361416974249373,
      0.22553821322891476,
      0.23542193907438658,
      0.20579818869629896,
      0.22658067840467666,
      0.24431786935180932,
      0.21605693149707061,
      0.23532931027198734,
      0.21358650350569963,
      0.0,
      0.21068796740085616,
      0.26866039785309925,
      0.26511378495043436,
      0.22260822242173361,
      0.23410604919202127,
      0.22685364390261342,
      0.21923805371997585,
      0.2217318854145407,
      0.2279434585883151,
      0.21292660560672694,
      0.2223824490778894,
      0.2508441465796083,
      0.2238513589226414,
      0.21137937894965364,
      0.2389739322106339,
      0.23418445298898494,
      0.25106110119874536
    ],
    [
      0.39717109582836874,
      0.5114049237433937,
      0.5290621310225114,
      0.5347738746924322,
      0.38716874422557734,
      0.4304404575500289,
      0.4340527398252383,
      0.44107111511385333,
      0.4619262503457269,
      0.40037733674296416,
      0.3928318139450018,
      0.26237972956449784,
      0.0,
      0.48893119320946954,
      0.4359348636633449,
      0.5407638964476729,
      0.38551727619158904,
      0.4143129763053002,
      0.37385741832056074,
      0.49188543616627434,
      0.416811948860746,
      0.3992669603910315,
      0.3642825563979515,
      0.41137362401705246,
      0.3560557959907882,
      0.4357453659233339,
      0.3742321817311991,
      0.40769958581915033,
      0.38639469535386906
    ],
    [
      0.3842545648452145,
      0.5504233041538318,
      0.556735196660695,
      0.5559380867477146,
      0.4534361176640693,
      0.46405843210745945,
      0.39823081633489643,
      0.3981264663344073,
      0.5158820948479634,
      0.4022495145246794,
      0.43959856058623736,
      0.32934654229557037,
      0.49789643713392606,
      0.0,
      0.5226997287022637,
      0.46508929347165107,
      0.3638601167343194,
      0.4223093665644375,
      0.45372079920546904,
      0.49719509961563,
      0.4409624231803626,
      0.357304932867049,
      0.36420085694199744,
      0.4434881848088985,
      0.35028882269957595,
      0.4528588943541081,
      0.39842811925902977,
      0.36637500858122096,
      0.43461339345559447
    ],
    [
      0.3868160638992406,
      0.49771021018085926,
      0.5481813966258091,
      0.5199545983422329,
      0.485060165156308,
      0.4461004040927914,
      0.40685764107107514,
      0.44831998815719465,
      0.5339203390889304,
      0.3730731330894159,
      0.4249857415171292,
      0.4020458226339314,
      0.45850768674764675,
      0.5258222209194454,
      0.0,
      0.5274366272267283,
      0.42529782349722334,
      0.4952505078342886,
      0.4612886993462333,
      0.5186870983417211,
      0.4754363600952156,
      0.3868284025032891,
      0.39868995089481984,
      0.5268254863043025,
      0.41301589018414986,
      0.46628015392440925,
      0.42163645342242817,
      0.4740378989457761,
      0.5024370586736675
    ],
    [
      0.3579921824838559,
      0.47986936664003843,
      0.489989910230171,
      0.5110708409146871,
      0.3992794321540636,
      0.39486424640368245,
      0.3484637976829186,
      0.4056727013798951,
      0.4068262697085283,
      0.39100732766641544,
      0.4267302815857459,
      0.2504643846763899,
      0.4789824646182239,
      0.4551866314219317,
      0.41375251820930226,
      0.0,
      0.3422321462169775,
      0.37959052343968813,
      0.35734332471874053,
      0.40107380288116645,
      0.41614799941518155,
      0.32078165128239755,
      0.33499070277739085,
      0.44754859633523814,
      0.3423505024412066,
      0.3448883477745426,
      0.36448911154952657,
      0.3199978588158583,
      0.35975503398000686
    ],
    [
      0.3344186706316632,
      0.4072412175734199,
      0.41186873596435003,
      0.36803799834964,
      0.40608547708182363,
      0.3751604392912973,
      0.2837180418650349,
      0.3562865398013082,
      0.3519240617312307,
      0.42287307202418845,
      0.3629789937312642,
      0.25037672882835427,
      0.37327122300045357,
      0.33355428254338215,
      0.30159469904613023,
      0.3773575740475661,
      0.0,
      0.35911280734828543,
      0.3352994672372871,
      0.3634596656490059,
      0.39354227817695464,
      0.3155078535041649,
      0.35278057517923456,
      0.369713301732566,
      0.3962213183575729,
      0.35691805166106083,
      0.3466024815974109,
      0.3298862676278158,
      0.30686092799217324
    ],
    [
      0.3294221510401185,
      0.45901151788306316,
      0.4053849139022856,
      0.4499240505088,
      0.4198888421754856,
      0.3587742284555735,
      0.3816963824290551,
      0.3523816819221284,
      0.40267417781824477,
      0.362330610454338,
      0.3679192030693308,
      0.27645422482911797,
      0.39645639767817586,
      0.4349370313007992,
      0.4161393355630505,
      0.4404988276874935,
      0.36505623611349325,
      0.0,
      0.3355516372259615,
      0.40005957224060995,
      0.40050694045888857,
      0.33707905154406537,
      0.33812687578576694,
      0.40602382837067386,
      0.3498547940622436,
      0.3954330929844585,
      0.34144207299843043,
      0.43477681687194725,
      0.4075216666125727
    ],
    [
      0.26714887646638674,
      0.4506261017409565,
      0.4011015408996912,
      0.3543289740322786,
      0.403250751289175,
      0.341537725902582,
      0.2909901042239016,
      0.3266915399228356,
      0.35613777394887935,
      0.33188215176068847,
      0.3450560368508291,
      0.2757429264738396,
      0.3554000589170603,
      0.41074341913858237,
      0.3812954974386724,
      0.3702500176864445,
      0.33476019955916514,
      0.28798302172233026,
      0.0,
      0.35688632745658166,
      0.3489514562142757,
      0.2968335656929786,
      0.3154687706693584,
      0.3353501885465455,
      0.34650316938152037,
      0.316836717191219,
      0.3567416624999742,
      0.29800903028032777,
      0.3526578832799965
    ],
    [
      0.4023597988999261,
      0.6176502318938437,
      0.6097036385745562,
      0.5449993631890155,
      0.41925039379449514,
      0.5193084029538539,
      0.3699432598544419,
      0.396203440989368,
      0.47530823319056914,
      0.41655796045528026,
      0.40658295122598065,
      0.3190711126006025,
      0.5096110201304345,
      0.49928106053931165,
      0.4532500274268576,
      0.4434648738115494,
      0.4041034152895977,
      0.4172989937934519,
      0.41298398103708167,
      0.0,
      0.399144572844754,
      0.40281430612304425,
      0.39283804191427296,
      0.44073646694804847,
      0.40984944631815634,
      0.45137755581405314,
      0.3968650627174506,
      0.4095524298705979,
      0.44349803119482245
    ],
    [
      0.3382600635342936,
      0.4522934360414268,
      0.36885319936450256,
      0.43005147103976693,
      0.40730762591981895,
      0.44966515156660813,
      0.37594943914265166,
      0.38176351568645517,
      0.36642736993580205,
      0.3813987906167293,
      0.4472119431314201,
      0.26696433232629135,
      0.39585953027983334,
      0.4234656204712972,
      0.37228576975668304,
      0.4528527800578632,
      0.35935139389364124,
      0.40985676185500663,
      0.35504330779066073,
      0.4022566322597638,
      0.0,
      0.3542191559049017,
      0.33345098793456596,
      0.4076061590725959,
      0.36296905811317126,
      0.3837638812011033,
      0.3335510854993231,
      0.40651977123209626,
      0.3412382861062049
    ],
    [
      0.3985544581690941,
      0.4460364458099997,
      0.41130179682244905,
      0.4339844159188351,
      0.4075547947001019,
      0.4101572758559944,
      0.42414365560260747,
      0.42905358255485004,
      0.37377712965445276,
      0.3997905164492972,
      0.4218805714402012,
      0.35505780366112094,
      0.41060666728703366,
      0.40202847147400544,
      0.3983046824244725,
      0.41883051691304884,
      0.41068467069683323,
      0.4198561988410421,
      0.3766726332134771,
      0.43771969964679314,
      0.388840538036584,
      0.0,
      0.4110555377669285,
      0.4873685458706052,
      0.3799808608825108,
      0.36356216187270185,
      0.365056867942116,
      0.42745159588207104,
      0.419801609028148
    ],
    [
      0.36946369124492406,
      0.4791375133532305,
      0.42345195322533713,
      0.4056114749623414,
      0.43519759809259795,
      0.4272557697152264,
      0.40977471146515176,
      0.43960260474990087,
      0.4249206078535024,
      0.4417836015949197,
      0.44431778755358353,
      0.34633053795169033,
      0.41968710917511465,
      0.4263907910010636,
      0.40667948155679223,
      0.4400852842811058,
      0.44645210203986374,
      0.45114861267230943,
      0.39044024812118505,
      0.45043475190642934,
      0.4255826532884386,
      0.3948164889973649,
      0.0,
      0.4692092504744376,
      0.5199620828902114,
      0.3807640127988672,
      0.41898403456238986,
      0.41561694966510077,
      0.3773794589982298
    ],
    [
      0.33484222822950516,
      0.4639603219142694,
      0.4178737620157227,
      0.3986881966191096,
      0.41810606631762126,
      0.4000423221762712,
      0.3791867192558944,
      0.3895192296302954,
      0.35996855326550214,
      0.36815734183630666,
      0.4525622855324227,
      0.3179064115733703,
      0.3902965507162952,
      0.3770144227625982,
      0.40446154347866714,
      0.4628572866439271,
      0.35781470163551865,
      0.3679880700396956,
      0.3763870123528792,
      0.38924453291754246,
      0.40480837373123735,
      0.36754069294471226,
      0.35574222105914366,
      0.0,
      0.3627594352423449,
      0.3198953858899869,
      0.3414121049284635,
      0.43216697629852163,
      0.3567190847051316
    ],
    [
      0.2617814979775812,
      0.38965997399509145,
      0.3906269517669758,
      0.31772590949835733,
      0.33832487320183136,
      0.3598545747113562,
      0.3027562397158341,
      0.35509389607392317,
      0.3106736363217302,
      0.378780273611262,
      0.3276898868527096,
      0.25985192054119444,
      0.3173204505967513,
      0.30052125858675227,
      0.3246662253732766,
      0.3285676394031256,
      0.39140728414532755,
      0.2998457828187555,
      0.372831182520013,
      0.37002297864612976,
      0.3403354229310749,
      0.2953206875788381,
      0.3513466139838908,
      0.3258193708304251,
      0.0,
      0.30581797800000077,
      0.3480420807601272,
      0.3330321217081551,
      0.32770075158492573
    ],
    [
      0.4504323452014434,
      0.5959384954839486,
      0.5288350730904776,
      0.6113472338197141,
      0.4140534523176407,
      0.5263216025097432,
      0.4933696554119391,
      0.506899562340104,
      0.4869946024156542,
      0.44769087965755294,
      0.45203820367617986,
      0.3167042137626488,
      0.5661093326170876,
      0.49347328794537515,
      0.5167054520479191,
      0.5272332975930136,
      0.506351453825824,
      0.5092607679620431,
      0.45973596887674173,
      0.5664046258245066,
      0.5096395968521343,
      0.46078480012173473,
      0.45263097858396817,
      0.47159397053543506,
      0.4439882541653064,
      0.0,
      0.43974261277957405,
      0.4649252158711683,
      0.45213628284161533
    ],
    [
      0.36204931793163664,
      0.38374373786209115,
      0.44979267324428385,
      0.371310687770376,
      0.43865126936106513,
      0.33546328171149353,
      0.29850116394997195,
      0.39106285696344933,
      0.39028748478748154,
      0.491714119301101,
      0.356028843431871,
      0.29761103697277425,
      0.3653163655695888,
      0.4459508597086095,
      0.38738239281451725,
      0.39011463199750107,
      0.3758278703861815,
      0.32667578323005175,
      0.4125125800522489,
      0.36516401800023335,
      0.35593915161944434,
      0.33701654623634325,
      0.31968867403833734,
      0.33189986963041584,
      0.3987079212285667,
      0.3641014501235773,
      0.0,
      0.3192970595099802,
      0.41821098043850147
    ],
    [
      0.2998657989091189,
      0.3827051774379526,
      0.3494865471189177,
      0.3679934473074302,
      0.32403774926902096,
      0.34421132597402626,
      0.32010089199252834,
      0.33536353336966296,
      0.3012114497840481,
      0.3219583871501084,
      0.3209472027046365,
      0.2335500398109016,
      0.3325506921022927,
      0.3308162371853576,
      0.3293698772541289,
      0.333394773771994,
      0.3119747329508249,
      0.34538325021620353,
      0.2969166239174563,
      0.33192897218031625,
      0.387284370111344,
      0.301301411871725,
      0.28925601792201183,
      0.3950167531724029,
      0.31459802998390973,
      0.3266059594654269,
      0.3009724820480464,
      0.0,
      0.32112545024576944
    ],
    [
      0.290961656193937,
      0.4211377179833262,
      0.4510571269118957,
      0.3881264325611551,
      0.3740303039715065,
      0.3350064560027035,
      0.31370671913676373,
      0.33473395929111827,
      0.38350626739473626,
      0.3560821810563377,
      0.3207891973623349,
      0.2949509778602182,
      0.38917227694482737,
      0.41970843351311027,
      0.44464853386529946,
      0.3776675091103221,
      0.36808068382417414,
      0.4116177349169945,
      0.38556703076780496,
      0.40681140934648674,
      0.3151869656312769,
      0.3556135311626778,
      0.3375423695420332,
      0.36280716827931525,
      0.3447479393450177,
      0.3840922972693237,
      0.38801250141037325,
      0.337632013118367,
      0.0
    ]
  ],
  "row_avgs": [
    0.07237079667817867,
    0.4562992735750601,
    0.40573334164342884,
    0.39954809921444256,
    0.3737502639273337,
    0.38014850098939107,
    0.39476651645202027,
    0.3542052124720438,
    0.34918146346762435,
    0.3197099837436251,
    0.5030684442998415,
    0.22795555995360556,
    0.4237759281210331,
    0.43855611338136685,
    0.4625179936684379,
    0.39076221276442036,
    0.35509474112766565,
    0.384475934356649,
    0.3431844817566813,
    0.44227171690697914,
    0.38430130427623127,
    0.4081826323006205,
    0.4243028987211182,
    0.38456863691831994,
    0.3330506237048363,
    0.48826218636180335,
    0.37428652242398913,
    0.32678311375812724,
    0.36760704977762276
  ],
  "col_avgs": [
    0.34930762852825614,
    0.45312259659926996,
    0.4439962638623904,
    0.41785829430065974,
    0.39929957667539967,
    0.39182368542593343,
    0.3444677454259149,
    0.3734316357046414,
    0.3944377732653339,
    0.38060945299265253,
    0.38322057539173027,
    0.28203070852418843,
    0.3933769993325634,
    0.4071604184514576,
    0.38439690890995354,
    0.4113134104388062,
    0.36650949702260605,
    0.3710878761292575,
    0.3695449487812919,
    0.4054779078491758,
    0.37862301511419877,
    0.3350428247595348,
    0.3373448477834207,
    0.40559123378559864,
    0.35877513492253926,
    0.35342990976889155,
    0.3550878396085749,
    0.36255580601910736,
    0.3597970313691488
  ],
  "combined_avgs": [
    0.21083921260321742,
    0.45471093508716504,
    0.4248648027529096,
    0.4087031967575512,
    0.38652492030136665,
    0.38598609320766225,
    0.3696171309389676,
    0.3638184240883426,
    0.37180961836647913,
    0.3501597183681388,
    0.4431445098457859,
    0.254993134238897,
    0.40857646372679823,
    0.42285826591641223,
    0.4234574512891957,
    0.4010378116016133,
    0.3608021190751358,
    0.37778190524295324,
    0.3563647152689866,
    0.42387481237807745,
    0.381462159695215,
    0.37161272853007765,
    0.38082387325226946,
    0.3950799353519593,
    0.34591287931368775,
    0.4208460480653474,
    0.364687181016282,
    0.3446694598886173,
    0.3637020405733858
  ],
  "gppm": [
    612.5369120804584,
    653.2885651049738,
    656.9904580060085,
    670.8720159364875,
    677.5185941793786,
    682.7525694140292,
    706.533754329348,
    689.8439099165743,
    678.30419834249,
    683.2635668003417,
    688.9999367146038,
    726.8751496222103,
    683.5651442933325,
    677.7263448619238,
    687.8964976704713,
    675.4514104512974,
    692.4958435971412,
    693.8696902506479,
    691.8138297923476,
    677.3687095005314,
    686.9098484446541,
    712.0878442109173,
    708.3339867239931,
    677.6973152821804,
    692.5890065760552,
    700.1391633635767,
    696.2888307054476,
    692.7912994460968,
    697.0403048686183
  ],
  "gppm_normalized": [
    1.3606888392642353,
    1.6147417908960047,
    1.629889803625537,
    1.6514076027461775,
    1.667535359201939,
    1.6749899661235372,
    1.7333549906566827,
    1.6843502174710052,
    1.6643295161373644,
    1.6753730514026774,
    1.6836314076724206,
    1.7772125041701436,
    1.679917720808467,
    1.6629858603035412,
    1.6856398126586079,
    1.6535570921078857,
    1.6965201497714422,
    1.7011601647000723,
    1.6932613909796814,
    1.666150321655919,
    1.6793938995720032,
    1.7359445898119525,
    1.7277477864273674,
    1.659978048739303,
    1.6976424884469883,
    1.7099856930847503,
    1.7025463205401008,
    1.69715992286185,
    1.7013331824012
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399,
    1428,
    431,
    434,
    454,
    409,
    434,
    388,
    406,
    484,
    448,
    357,
    618,
    424,
    397,
    365,
    453,
    437,
    439,
    452,
    382,
    412,
    347,
    390,
    420,
    459,
    322,
    408,
    451,
    409
  ],
  "response_lengths": [
    6939,
    2538,
    2502,
    2752,
    2401,
    2587,
    2292,
    2401,
    2862,
    2669,
    2106,
    3549,
    2552,
    2279,
    2078,
    2580,
    2554,
    2486,
    2667,
    2259,
    2369,
    2019,
    2166,
    2427,
    2604,
    1837,
    2401,
    2553,
    2362
  ]
}