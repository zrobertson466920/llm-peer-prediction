{
  "example_idx": 77,
  "reference": "Published as a conference paper at ICLR 2023\n\nCASR: GENERATING COMPLEX SEQUENCES WITH AUTOREGRESSIVE SELF-BOOST REFINEMENT\n\nHongwei Han1∗ Mengyu Zhou2† Shi Han2 Xiu Li1† Dongmei Zhang2 1Tsinghua Shenzhen International Graduate School, Tsinghua University 2Microsoft Research hhw20@mails.tsinghua.edu.cn, li.xiu@sz.tsinghua.edu.cn {mezho, shihan, dongmeiz}@microsoft.com\n\nABSTRACT\n\nThere are sequence generation tasks where the best order to generate the target sequence is not left-to-right. For example, an answer to the Sudoku game, a structured code like s-expression, and even a logical natural language answer where the analysis may be generated after the decision. We define the target sequences of those tasks as complex sequences. Obviously, a complex sequence should be constructed with multiple logical steps, and has dependencies among each part of itself (e.g. decisions depend on analyses). It’s a great challenge for the classic leftto-right autoregressive generation system to generate complex sequences. Current approaches improve one-pass left-to-right generation on NLG tasks by generating different heuristic intermediate sequences in multiple stages. However, for complex sequences, the heuristic rules to break down them may hurt performance, and increase additional exposure bias. To tackle these challenges, we propose a PLMfriendly autoregressive self-boost refinement framework, CASR. When training, CASR inputs the predictions generated by the model itself at the previous refinement step (instead of those produced by heuristic rules). To find an optimal design, we also discuss model architecture, parameter efficiency and initialization strategy. By evaluating CASR on Sudoku, WebQSP, MTOP and KVRET through controlled experiments and empirical studies, we find that CASR produces high-quality outputs. CASR also improves Accuracy on Sudoku (70.93% → 97.28%) and achieves state-of-the-art performance on KVRET with Micro F1 score (67.88% → 70.00%).\n\n1\n\nINTRODUCTION\n\nFigure 1: The Overview of CASR Framework. X, Y and ˆY denote the input, ground truth and prediction, respectively. The blue arrows show how we iteratively added back the previous-step prediction ˆY t−1 to the input for generating refined output ˆY t.\n\n∗ The contributions by Hongwei Han have been conducted and completed during their internships at\n\nMicrosoft Research Asia, Beijing, China.\n\n† Corresponding authors.\n\n1\n\nEncoderDecoderModel ArchitectureParameter EfficiencyInitialization Strategy...SepEnc, UniEncFinetune, AdapterRestart, ContinueDesignTasks with complex answers, like Sudoku, WebQSP, MTOP, and KVRETEncoderDecoderEncoderDecoderPublished as a conference paper at ICLR 2023\n\nSequence generation models are widely used in tasks related to natural, domain-specific and programming languages – E.g., question answering (Pandya & Bhatt, 2021), neural machine translation (Yang et al., 2020), speech recognition (Malik et al., 2021), automatic data analysis (Zhou et al., 2020), drug discovery (Kim et al., 2021), document summarization (Ma et al., 2020), code search and generation (Lee et al., 2021), etc.\n\nTo achieve better performance on these tasks, recent works often adopt autoregressive (AR) models (Wu et al., 2016), especially the ones with one-pass L2R (left-to-right) token-by-token generation / decoding order. Many SOTA-performance generative PLMs (pre-trained language models) are one-pass L2R models, such as GPT (Radford et al., 2018), T5 (Raffel et al., 2020), Bart (Lewis et al., 2020), etc. Different from non-autoregressive (NAR) models (Gu et al., 2017) which assume independence among tokens, L2R models assume the conditional probability in the form of P (Y |X) = (cid:81) i P (yi|X, y<i) which better captures the left-side dependencies that exist in most generation tasks. More variations of generation models are discussed in §2.1.\n\nHowever, for many sequence generation tasks, beyond the left-side dependencies, there are right-side dependencies in the answer sequence, which together lead to multi-hop dependency chains, making left-to-right not the best order for generation. We call these tasks as complex tasks, and the answer sequences of these tasks as complex sequences. Complex tasks (see more details in §2.2, including Sudoku (PARK), WebQSP (Yih et al., 2016), MTOP (Li et al., 2021), and KVRET (Eric et al., 2017), etc..) require better generation mechanism beyond one-pass L2R generation, since complex sequences are usually long, difficult, structured, or logical, which should be constructed with multiple logical steps.\n\n4x4 (b) A Sudoku Example. The game solving order of a human is to from “a” “g”, rather than row-by-row.\n\n(a) A 9x9 Sudoku Example. White cells denote blanks, and the green numbers in them denote the ground truth.\n\nFigure 2: Examples of Sudoku.\n\nHuman beings solve a complex problem with respect to its intrinsic order. For example, the order to write hierarchical answers (such as sExpression or SQL code) is usually bottom-up or top-down following the dependencies between components as discussed by Sun et al. (2020). The order to give an NL response is first analyses then decisions as discussed by CoT (Wei et al., 2022). The order to solve a puzzle (such as the example 4x4 Sudoku game in Figure 2) is usually from easy parts to hard parts, because the hard parts become easier when the easy parts are correctly solved. (That is also verified in §5.1 where our CASR model learns to solve easy parts before hard ones.) Obviously, people give answers to different tasks in various orders with respect to all kinds of dependencies.\n\nMimicking human behavior, some existing works design specific intermediate sequences to solve the dependency order challenge. E.g., templates (Hua & Wang, 2020) or heuristic rules (Zhang et al., 2018; Tan et al., 2021) are applied in autoregressive NL generation, allowing models to generate some parts (intermediate sequences) before the other parts in an answer via iterative refinement (rather than one-pass decoding). However, 1) it’s really hard to design the best heuristic order and easy to miss intrinsic dependencies for some tasks, and we need expert knowledge or manual efforts to design specific heuristic orders for all different tasks. Besides, 2) when using teacher forcing strategy to parallel train all refinement iterations, additional exposure bias occurs.\n\nIn this paper, CASR (Generating Complex Sequences with Auto-regressive Self-Boost Refinement) framework is proposed by us to: 1) decide intermediate sequences of complex answers for different tasks in a data-driven way, 2) avoid additional exposure bias. As shown in Figure 1 and will be discussed in §3, in CASR we design a model architecture (§3.2) to not only take in the original input X, but also the previous prediction ˆY t−1 in both training and inference. A special process (§3.1) is designed to train refinement models M t for each step t = 0, 1, ..., T −1. To enhance the performance on downstream tasks, CASR models could be initialized with pre-trained language models (§3.3) such as T5 (Raffel et al., 2020), and even trained in a “Continue” way (§3.4) by initializing M t\n\n2\n\n3d14b242c31a2g41313f2e4Published as a conference paper at ICLR 2023\n\nwith M t−1. For CASR models to be more efficient, we also explore the parameter-efficient model designs (SepEnc vs. UniEnc in §3.2), and tuning with parallel adapters (§3.3).\n\nWe evaluate CASR (and several baselines) on WebQSP, MTOP, KVRET, and Sudoku in §4, achieving SOTA performance on KVRET. In detail, CASR improves F1 on WebQSP from 70.81 to 74.81, EM on MTOP from 78.64 to 81.92, Micro F1 on KVRET from 67.40 to 70.001, and accuracy on Sudoku from 70.93 to 97.28. We find the optimal CASR design (§4.2) is “Fine-tuning” “SepEnc” with “Continue” strategy. We also do empirical studies on complexity (§5.1), attention map (§5.2), and visualize cases (§5.3). Then we find that, CASR benefits hard sequences more than easy ones, and CASR can indeed correct the wrong part of the previous prediction according to other parts it depends on (as shown in Figure 8).\n\nIn summary, our major contributions are: First, we point out the challenge to generate complex sequences due to the existence of multi-hop dependency chains and conduct a comprehensive review on existing iterative refinement methods. Second, we propose an autoregressive selfboost refinement framework, CASR, to decide intermediate sequences of complex answers in a data-driven way. The code of CASR framework is open sourced in the repository at https: //github.com/RalphHan/CASR. Third, we conduct experiments and empirical studies on four complex tasks to show CASR works and interpret how it works.\n\n2 RELATED WORK\n\nTable 1: The Objectives of Sequence Generation Methods.\n\ni P (yi|X) i P (yt\n\nObjective (cid:81) (cid:81)\n\nMethod NAT (Gu et al., 2017) INAT (Lee et al., 2018) Imitate an expert policy to delete and insert Levenshtein (Gu et al., 2019) (cid:81) L2R (Wu et al., 2016) (cid:81) XLNet (Yang et al., 2019) Bidirectional (Zhang et al., 2018) (cid:81) (cid:81) Progressive (Tan et al., 2021)\n\ni P (yi|X, yi−1, ..., y1) i P (yzi|X, yzi−1, ..., yz1) i P (yi|X, yi+1, ..., yn) · (cid:81) i P (yi|X, yi−1, ..., y1, CR2L) i−1, ..., yt i|X, Y t−1, yt i P (yt 1) i|X, ˆY t−1, yt i−1, ..., yt i P (yt 1)\n\ni|X, ˆY t−1)\n\nOurs\n\n(cid:81)\n\nTable 2: The Number of Samples in Train, Dev, and Test Splits of WebQSP, MTOP, KVRET, and Sudoku. Task WebQSP 2673 MTOP KVRET 6291 Sudoku\n\nTrain Dev Test 309 1639 15667 2235 4386 808 777 800K 100K 100K\n\n2.1 SEQUENCE GENERATION METHODS\n\nIn recent years, many studies have been made on seq2seq generation. For input X and target Y sequences, the ultimate objective is to maximize P (Y |X). As shown in Table 1, different method formulates the objective differently. These methods can be divided into non-autoregressive (row 1-3) and autoregressive (row 4-7) ones. Highly related ones are run as baselines in Table 5.\n\nNAT (Gu et al., 2017) assumes that each token in sequence Y is mutually independent from each other, thus formulating P (Y |X) as (cid:81) i P (yi|X). To reduce the modeling bias in NAT, iterative refinement is applied on answers in INAT (Lee et al., 2018). Levenshtein Transformer (Gu et al., 2019) further breaks down each refinement step into deletion, insertion, and classification so as to allow the model to edit the generation in a non-autoregressive way. Despite their efficiency, nonautoregressive models perform relatively poor comparing to autoregressive ones (see §4.4).\n\nL2R (Wu et al., 2016) generation tries to decode from left to right, which is also adopted by GPT (Radford et al., 2018), basic transformer decoder (Vaswani et al., 2017), VAE decoder (Yu et al., 2020), and RNN decoder (Xu et al., 2015; Xia et al., 2017). XLNet (Yang et al., 2019) allows the model to generate with any given order Z, but cannot decide by itself the best order. Bidirectional decoder (Zhang et al., 2018) generates backward (R2L) then forward (L2R), which handles at most 2-step logic. It trains an additional R2L decoder from scratch, without leveraging generation capabilities of PLMs as in CASR (see §3.3).\n\nPAIR (Hua & Wang, 2020) is used in controlled text generation with a template (constructed from a set of provided key phrases and placeholders) as input. It fills and refines the placeholders in an auto-regressive way. The template, the sequence length and position of placeholders remain\n\n170.00 achieves the SOTA for micro f1 on KVRET, and the previous SOTA is 67.88 (Xie et al., 2022).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nunchanged during refinement. Similarly, chain of thought prompting (Wei et al., 2022) constructs the intermediate process through manually provided templates. These two methods are not chosen as baselines in §4 because they both require expert designs for each specific task.\n\nProgressive generation (Tan et al., 2021) is proposed to remove the requirement of templates and the limitation of fixed length during refinement. It breaks down the vocabulary into multiple stages according to word importance (average of tf-idf). Important words are generated first during early refinement steps, while other words (both important and unimportant) are generated later. Note that in Table 1, Y t−1 (row 7) denotes the intermediate sequence produced by the heuristics of progressive generation, but ˆY t−1 (row 8) is the previous prediction generated by our CASR model (see §3).\n\n2.2 TASKS WITH COMPLEX ANSWERS\n\nThe four complex-answer tasks referred in §1 will be further introduced in this section.\n\nWebQSP (Yih et al., 2016) is a classic dataset for KBQA(Knowledge Base Question Answering). The input consists of a knowledge graph and an NL query, and the output is an s-Expression which can be executed on the knowledge graph. The SOTA method of WebQSP (F1=83.6%) is ranking with bootstrapping negative samples (Ye et al., 2022).\n\nMTOP (Li et al., 2021) is a benchmark for comprehensive multilingual task-oriented semantic parsing. The input consists of a list of API calls and an NL query, and the output is a tree-based TOP Representation that can be executed.\n\nKVRET (Eric et al., 2017) is a benchmark for table conversation. The input consists of a table and an NL query, and the output is an NL response corresponding to the dialog. The SOTA method of MTOP (EM=86.78%) and KVRET (Micro F1=67.88%) is multi-task (20+ tasks) prefix tuning with T5-3B as the backbone (Xie et al., 2022).\n\nSudoku (PARK) is an open dataset on Kaggle. Its game target is to fill the blanks correctly with the constraint that any two numbers in the same row, column, and house shouldn’t have the same value. We choose Sudoku as an intuitive toy task for better demonstration of ideas in this paper.\n\nExamples of WebQSP (Figure 5), MTOP (Figure 6) and KVRET (Figure 7) are shown in Appendix §A. The number of samples in each split of the four tasks is shown in Table 2.\n\n3 CASR FRAMEWORK\n\nAs shown in Figure 1 and introduced in §1 and §2.1, the key idea of CASR framework is to take the previous-step prediction ˆY t−1 as part of the current-step (t) input for generating a refined output ˆY t. The iterative inference process in CASR follows the idea: As shown in Algorithm 1, the prediction at castep t is ˆY t (0 ≤ t < T ) generated by the corresponding CASR model M t, where the input to M 0 is the original input X, and the input to M t (t > 0) is (X, ˆY t−1). Following the notions in §2.1, X, Y , and ˆY denote the input, the ground truth and the prediction, respectively. The iterative refinement process in CASR will take at most T casteps (CASR steps).\n\nThe undetermined M t models in the CASR inference process lead to more questions: 1) How to design the training process and objectives that match the inference process? 2) How to design CASR model architecture that leverages existing PLMs and takes the extra ˆY t−1 as input? 3) When leveraging a large PLM, can we train and save CASR models M t with less parameters? 4) Can the training and inference process be more effective by exploiting the relationships among CASR models M t of each castep? In the following, we will discuss these problems one by one.\n\n3.1 TRAINING PROCESS\n\nAs formulated in Algorithm 2, the training process is also iterative.\n\nAt the 0-th castep, we train the model M 0 normally on train set with the objective to maximize P (Y |X) (as discussed in §2.1). When the training is done, we use M 0 to generate prediction ˆY 0 for each X in train and dev sets with beam searching (Ney et al., 1987).\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAt the t-th castep (0 < t < T ), we train M t to refine the last prediction ˆY t−1. The objective becomes maximizing P (Y |X, ˆY t−1) on train set. After training, we generate ˆY t on all train and dev sets to compose the samples (X, Y, ˆY t) for the next castep t + 1.\n\nAfter the training process, T versions of CASR model M t (0 ≤ t < T ) are available for the inference process. Because the distribution of ˆY t−1 could change with t, and different casteps may learn different refinement patterns, by default all the T versions are saved.\n\nIn the following, we will discuss more details of the algorithms on M t model architecture (§3.2), tuning (§3.3), and initialization (§3.4). More discussions on result selections are in Appendix §B.\n\n3.2 MODEL ARCHITECTURE\n\nAE and AD denote adapters of encoder and decoder. Blue and gray segments are the parameters to be trained and frozen, respectively.\n\nFigure 3: Model Architectures and Parameter Efficiency Choices.\n\nThe arrows begin at the source of initial weights of M t.\n\nFigure 4: Initialization Strategies.\n\nThe next problem after the training process is how to design a model architecture which could take (X, ˆY t−1) as input and generate ˆY t. Starting from any Transformer encoder-decoder (Vaswani et al., 2017) based model (such as T5, or an untrained Transformer), in CASR we provide two modification approaches to take in ˆY t−1.\n\nAs shown in Figure 3, SepEnc has two encoders to encode X and ˆY t−1 separately (encode before concat), and UniEnc adopts only one encoder for both inputs (concat before encode). The output sequence length of SepEnc and UniEnc are both len(X) + len( ˆY t−1). Let H denote the encoder output. For SepEnc, H = Concat(Encoder1(X), Encoder2( ˆY t−1)), and for UniEnc, H = Encoder(Concat(X, ˆY t−1)).\n\nBoth approaches have their own advantages: SepEnc naturally handles the distribution differences between X and ˆY t−1, while UniEnc requires fewer model parameters. UniEnc forces one encoder to handle two kinds of sequences with potentially different types of contents and lengths. As a tradeoff, SepEnc brings two times larger encoding part. And the decoder generates answers from H, just like the classic transformer decoder (Vaswani et al., 2017).\n\n3.3 PARAMETER EFFICIENCY\n\nBoth SepEnc and UniEnc approaches in §3.2 could take a PLM (pre-trained language model, such as T5) as starting point for model parameter initialization (more details in §3.4). In this way, CASR could leverage the existing knowledge from the PLM to enhance performances on downstream tasks (e.g., the ones in §2.2).\n\nHowever, it’s costly to fine-tune all the parameters from a large PLM and save all the parameters of all the T CASR models M t. Thus, besides the fine-tuning approach, in CASR we also try parameterefficient tuning. As shown in Figure 3, parallel adapters (He et al., 2021) could be added to every encoder and decoder. By adopting adapters, the parameters from the PLM are frozen, and only the parameters of the adapters are trained. In other words, one only needs to save the adapter (rather than the whole model) parameters for T times.\n\n5\n\nSepEnc+Fine-tuneUniEnc+Fine-tuneSepEnc+AdapterUniEnc+AdapterE1E2DX1-tYˆAE1AE2ADEDEEDAEADDEX1-tYˆX1-tYˆX1-tYˆt=0t=1t=2,3,...,T-1RestartContinuePLM or ScratchPLM or ScratchTrain and PredictTrain and PredictTrain and PredictTrain and PredictTrain and PredictTrain and PredictPublished as a conference paper at ICLR 2023\n\n3.4\n\nINITIALIZATION STRATEGY\n\nIn Algorithm 2, at the beginning of each castep, we initialize the parameter weights of M t with a chosen strategy from {Restart, Continue}. Illustrations of each strategy are shown in Figure 4.\n\n“Restart” means initializing each M t with the same PLM weights (or from scratch). For SepEnc architecture, Both Encoder1 and Encoder2 can be initialized with the parameter weights of the PLM encoder. “Continue” means initializing with the best checkpoint from the previous castep. In this way, M t inherits the knowledge from M t−1 to avoid the potential cold start issue at castep t.\n\nIn the “Restart” strategy, when comparing the late castep with the early castep, the improvement is only brought by refinement, rather than the extra continuous training steps (which exist in the “Continue” strategy). Note that, we design “Restart” mainly to decouple the effects of refinement from training steps, which is necessary for controlled experiments in §4.\n\n4 EXPERIMENTS\n\nIn this section, we run controlled experiments to evaluate CASR designs (§3) on WebQSP (Yih et al., 2016), MTOP (Li et al., 2021), KVRET (Eric et al., 2017) and Sudoku (PARK) (§2.2), and compare CASR with high-related baselines (§2.1).\n\n4.1 EXPERIMENT SETUP\n\nIn this section, we explain how we design the controlled experiments and reproduce the high-related baselines. For more details about hyperparameter and resource consumption (number of parameters, training/inference time, etc.) please see Appendix §C.\n\n4.1.1 CONTROLLED EXPERIMENTS\n\nAcc. EM BLEU Micro F1 AVG\n\nTable 3: Controlled Experiments of CASR Framework on WebQSP, MTOP, and KVRET.\n\n0\n\nF1\n\nArch.\n\n70.81\n\nKVRET\n\nt Param.E.\n\nWebQSP MTOP\n\nIn Table 3, we apply CASR (with max castep T = 3) on the 24-layer generative PLM T5-base (Raffel et al., 2020). Controlled experiments A ∼ E are run to compare variations the of Parameter Efficiency (§3.3), Initialization Strategy and Model Architecture (§3.2). The (on evaluation metrics sets) are F1 score test for WebQSP, template accuracy and exact match for MTOP, and BLEU and Micro F1 scores for KVRET. For each row, the average (AVG) of all metrics (on test set) is calculated to provide an overall merged score.\n\nCASR Variations Init. Fine-tune Restart SepEnc Fine-tune Continue SepEnc Fine-tune Restart UniEnc Fine-tune Continue UniEnc Adapter Restart SepEnc Fine-tune Restart SepEnc Fine-tune Continue SepEnc Fine-tune Restart UniEnc Fine-tune Continue UniEnc Adapter Restart SepEnc Fine-tune Restart SepEnc Fine-tune Continue SepEnc Fine-tune Restart UniEnc Fine-tune Continue UniEnc Adapter Restart SepEnc\n\n77.34 73.10 16.38 82.92 78.93 18.17 85.07 81.19 18.55 82.83 78.80 18.80 84.86 81.24 18.06 77.43 73.21 16.60 82.92 78.93 18.26 85.61 81.69 19.12 82.83 78.77 18.78 85.66 81.92 18.34 77.45 73.23 16.63\n\n70.20 73.03 74.61 73.14 74.16 67.92 73.09 74.81 73.14 74.32 68.13\n\nA0 B0 C0 D0 E0 A1 B1 C1 D1 E1 A2 B2 C2 D2 E2\n\n82.49 78.64 18.33\n\n(§3.4)\n\n2\n\n1\n\n67.40\n\n63.53\n\n60.52 65.59 67.65 64.14 70.00* 65.88 64.29 67.88 65.49 69.12 60.20 65.86 64.18 67.70 66.22 69.85 64.32 68.06 65.81 68.80 60.22 65.66\n\nThe three row blocks A0 ∼ E0 , A1 ∼ E1 and A2 ∼ E2 correspond to M 0, M 1 and M 2 models at Castep t = 0, 1, 2, respectively. By definitions in §3, row A0 ∼ D0 share the same result because initialization and architecture choices do not influence M 0 models.\n\nSince CASR converges on Sudoku with larger T , we evaluate it separately. In Table 4, We train 12-layer encoder-decoder transformers from scratch with max castep T = 5. The metric is accuracy (% correctly filled blanks). For a 9 × 9 board, the input and output sequences of a Sudoku game are both 81number serialized sequences. During training, we apply supervision only on blanks and apply constrained generation to\n\n6\n\nTable 4: Sudoku Testing Results. t Restart Continue Continue w/o ˆY t−1 70.93 0\n1 77.00 2 78.69 3 79.11 4 79.21\n\n84.28 89.16 91.13 92.09\n\n85.71 92.48 95.66 97.28\n\nPublished as a conference paper at ICLR 2023\n\nmake sure the model only predicts on blanks. And we fill the predictions back to the blanks of the input as the combination of X and ˆY , at the beginning of each castep t > 0. In other words, we use a special UniEnc to encode the combination, with H = Encoder(Combine(X, ˆY t−1)). “Restart” and “Continue” correspond to the same choices as C and D in Table 3. “Continue w/o ˆY t−1” denotes that the previous predictions are not put back to the blanks of the input at each castep, which is equivalent to training the origin model T times steps as usual.\n\n4.1.2 BASELINE COMPARISONS\n\nF1\n\nNAR\n\nKVRET\n\nWebQSP MTOP\n\nTable 5: Comparisons between CASR and Baselines. “NAR” and “AR” denote non-autoregressive and autoregressive refinement. “–.01–” denotes the metrics are lower than 0.01.\n\nIn Table 5, we run some high-related baselines as introduced in §2.1. We run INAT and Levenshtein with the official code provided by fairseq2, where self-attention layers of the encoder and the decoder are initialized with bert, leaving cross-attention layers of the decoder trained from scratch. To decouple the effect from PLMs and make a fairer comparison between NAR and AR methods, we add row Xc , where the decoder of CASR is not initialized with T5 but from scratch (random weights), marked as “/Dec”. In row L2 , “CASR-L” denotes changing the backbone of row B2 from T5-base to T5-large. For fair comparisons, the original backbones in Bidirectional Decoder (GRU) and Progressive Generation (Bart) implementations are changed to T5-base (same as CASR), and set T = 3 (same as CASR) for Progressive Generation.\n\nAcc. EM BLEU Micro F1 AVG 0.00 4.51 19.48 50.27 61.98 63.53\n\nMethods INAT (Lee et al., 2018) Levenshtein (Gu et al., 2019) CASR B / Dec Bidirectional (Zhang et al., 2018) Progressive (Tan et al., 2021) Finetune\n\n—.01— 63.02 12.93 7.18 61.33 57.84 8.98 80.80 77.04 15.49 82.49 78.64 18.33\n\n85.61 81.69 19.12 87.73 84.54 18.14\n\n22.54 4.08 68.43 72.05 70.81\n\n10.17 54.77 64.54 67.40\n\nXa Xb Xc Xd Xe B0\n\nCASR CASR-L\n\n69.85 68.80\n\n66.22 67.44\n\n74.81 77.99\n\n—.01—\n\nB2 L2\n\nAR\n\n4.2 BEST CASR DESIGNS\n\nWe achieve the SOTA performance on Micro F1 of KVRET, which is 70.00. The previous SOTA is 67.88 (Xie et al., 2022) based on T5-3B, and we beat it with a smaller T5-base.\n\nFor the parameter initialization choices, “Continue” strategy brings better performance than “Restart”. This is observed by comparing A with B rows, C with D rows in Table 3, and comparing “Restart” and “Continue” columns in Table 4.\n\nFor the model architectures, by comparing A with C rows, B with D rows, we find that there is no great performance gap between SepEnc and UniEnc, and SepEnc is slightly better than UniEnc when combined with “Continue”.\n\nFor the parameter efficiency, by comparing E with A rows, we can find that adapter-tuning perform worse than fine-tuning. As the cost of freezing PLM parameters for efficiency, performance drops as expected when bringing adapter-tuning.\n\n4.3 ANALYSIS ON SELF-BOOST REFINEMENT\n\nAs introduced in §3.4, we design “Restart” to decouple the effects of refinement from fitting the training set longer than t=0. The variations with “Restart” strategy ( A , C and E in Table 3, “Restart” column in Table 4) demonstrates how self-boost refinement could improve the quality of generated sequences. Here the improvement is only brought by refinement since CASR models at all casteps are initialized with the same PLM/Scratch version and take the same training steps.\n\nIn Table 4, comparing “Continue” with “Continue w/o ˆY t−1”, we find that self-boostly feeding ˆY t−1 to the model is indeed helpful, which improves the accuracy from 92.09% to 97.28%.\n\nInterestingly, the performance gap between castep 0 and 1 is much larger than the gap between t to t + 1 (t > 0). One possible explanation is that later refinements have reached the upper bound.\n\n2https://github.com/facebookresearch/fairseq\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nAs we can see in Table 4, Sudoku requires more refinement steps (5 in our case) for the gap to converge. This shows there are more logical steps required and more dependencies exist in the generated Sudoku answers.\n\n4.4 COMPARISON WITH BASELINES\n\nIn the NAR block of Table 5, we find the results of NAR methods are not on par with AR methods. With case study (Figure 10), we find the ability of NAR methods to learn syntax is poor. Comparing row Xb and Xc , we find the poorness of NAR methods doesn’t come from the initialization of the decoder. Comparing row Xd , Xe , and B0 , we find that those AR refinement methods using heuristic intermediate sequences are indeed harmful (worse than directly fine-tuning) for complex sequence generation. Compare row B2 and L2 , we find larger backbone (T5-base → T5-large) is overall helpful (UnifiedSKG (Xie et al., 2022) observes the same phenomenon that T5-large is worse than T5-base on KVRET).\n\nIn summary, following conclusions can be drawn for CASR:\n\n• Self-boost refinement leads to better outputs than vanilla AR (one-pass L2R) generation,\n\nNAR refinement generation, and heuristic-rule-based AR refinement generation.\n\n• In general, the best combination is “Fine-tuning” “SepEnc” with “Continue” strategy.\n\n5 EMPIRICAL STUDIES\n\nTable 6: Result on Different Answer Length. Produced by row B in Table 3 (Finetune+Continue+SepEnc). ∆t denotes the improvement from castep t − 1 to t.\n\nTask WebQSP MTOP\n\nt Metric Short 0 Middle Long Short 1 Middle Long Short 2 Middle Long\n\nF1 78.80 83.07 50.51 80.12 86.41 57.52 80.12 86.41 58.13\n\nKVRET Acc Match Bleu Micro F1 AVG ∆t -\n85.23 83.65 14.05 -\n84.83 80.51 21.93 77.31 71.62 15.08 -\n87.21 85.50 12.17 87.19 83.01 19.81 80.71 74.95 16.33 87.96 86.25 12.32 87.53 83.34 19.77 81.26 75.36 17.00\n\n65.53 68.30 56.04 66.19 0.66 69.59 1.29 59.80 3.76 66.23 0.05 69.77 0.18 60.19 0.39\n\n65.91 71.16 65.69 65.93 71.51 69.48 64.52 71.79 69.18\n\nTable 7: The average difficulty and ratio (sum to 1) of each correct-solving castep, produced by the setting of column “Continue” in Table 4.\n\nt 0\n1 2\n3 4\n5 (failed)\n\nAVG Difficulty Ratio\n\n82.80 86.95 88.60 89.83 90.86 92.21\n\n63.54% 18.48% 8.50% 4.25% 2.51% 2.72%\n\nIn this section, we conduct empirical studies to interpret how CASR works.\n\nIn §5.1, we verify that CASR brings more improvement on more complex sequence. Also, we find CASR model may implicitly learns dependency because it solves easy parts first during refinements.\n\nIn §5.2, the cross-attention map between ˆY t and ˆY t−1 implies the informativeness and correctness of ˆY t−1. Meanwhile, from a microscopic perspective, each row in the cross-attention map implies the foundation of changing a component, where the hot-attended parts can be considered as the cause of changes.\n\nIn §5.3, through case studies we show the intermediate steps generated by CASR models.\n\n5.1 ANALYSIS ON COMPLEXITY\n\nCASR helps more on problems with more complexity. For WebQSP, MTOP, and KVRET, we assume that the complexity of a problem (X, Y ) is positively correlated with the length of its answer. Thus, we divide each test dataset into “Short”, “Middle” and ”Long” equal splits according to len(Y ), the length of ground truth answer. As we can see in Table 6, CASR brings the more improvement to the problems with “Long” answers than “Middle” and “Short” ones.\n\nTake Sudoku as example, we find that CASR solves easy blanks before difficult blanks during refinements. In late casteps, the generation benefits more from refinement by gradually handling\n\n8\n\nPublished as a conference paper at ICLR 2023\n\ncomplex dependencies. These are shown in Table 7 – As castep t increases, the average difficulty of remaining blanks increase. Here we formulate the difficulty3 of a blank cell as: Dif f iculty = (r − 1) · (c − 1) · (h − 1) where r, c, and h denote the number of blanks in the same row, column, and house of the blank. Also, we can define the correct-solved castep for a blank: If a blank is correctly filled at castep t and remains the same for the rest casteps, then the blank is solved at castep t and t is the correct-solved castep for the blank. Otherwise, by default the correct-solved castep is set to T . The “Ratio” column in Table 7 means the percentage of blanks is correctly solved at castep t.\n\n5.2 ANALYSIS ON ATTENTION\n\nTo measure to what extend does ˆY t attend to X or ˆY t−1 in cross-attention, we define the density. The cross attention map, A, is a tensor that displays how the tokens in ˆY t attend to H. Therefore, the size of A is [num-layers, num-heads, len( ˆY t), len(X)+len( ˆY t−1)]. We define the density to X and to ˆY t−1 as: DX = A[..., : len(X)].mean() and D ˆY t−1 = A[..., len(X) :].mean(), which are the average attention weight among each layer, head, and token, for X and for ˆY t−1.\n\nSimilar to Table 6, We sort the samples according to their D ˆY t−1, and divide a test dataset into sparse, middle, and dense splits, each holding 1/3 of the total test set. We evaluate the metrics of ˆY t−1 on the three splits. We find the performance of ˆY t−1 increases with D ˆY t−1 . Besides, we compute the average DX and D ˆY t−1 over all samples in the test set, and find that D ˆY t−1 grows with t. Therefore, ˆY t−1 becomes more and more informative thus be attended to by ˆY t. Please refer to Table 10 and Table 11 in Appendix §D for more details.\n\nMeanwhile, we draw top-5 positions in cross-attention map (filter out non-blank cells and self) for each changed cell in Figure 8. As we can see, the corrected cell usually attends to cells in the same row, column, or house.\n\n5.3 CASE STUDIES\n\nAs examples, the intermediate CASR prediction of each castep are shown in Figure 9 of Appendix §D. We can find that as castep t increases, there is more green (keywords) and less red (mistakes) parts, which means ˆY t becomes closer to Y . In the WebQSP case, the wrong part (marked red) of ˆY 0 is deleted by late casteps. In the MTOP case, we find that CASR correctly generates the right part (“[SL:RECIPES INCLUDED INGR EDIENT dairy]”) of the answer before the left part (“[SL:MET HOD RECIPES recipe]”), which one-pass L2R decoding cannot achieve. For Sudoku, an interesting phenomenon occurs. Although the wrong predictions (marked red) become fewer with the growth of castep, some previous correct predictions are modified to wrong ones, and then modified back. This can be considered a kind of exploration.\n\nWe also present the predictions from other experiments (baselines and CASR-L, as discussed in Table 5) in Figure 10, and we find it a challenge for NAR methods to generate complex sequences.\n\n6 CONCLUSION\n\nIn this paper, CASR framework is proposed by us to generate Complex sequences with Autoregressive Self-Boost Refinement, which can decide intermediate sequences of complex answers in a data-driven way with no need for expert knowledge or manual efforts. Through controlled experiments, we find the optimal design in CASR is Fine-tuning SepEnc with Continue strategy. Also, the interpretability of CASR is enhanced via empirical studies on problem complexity and attention dependencies. CASR sheds more light on the sequence generation methods, especially on complex tasks (e.g. automatic data analysis and drug discovery), for future research.\n\n3Difficulty: According to this definition, when a blank is the only blank in a row, column, or house, the difficulty is 0, which makes sense because the blank can be directly solved. The larger r, c, and h are, the more difficult it is to solve the blank.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nDatasets This work collects the public dataset for research purposes. We believe there is no privacy issue, because Sudoku, WebQSP, MTOP, and KVRET are accessible to the public.\n\nModels This work leverages T5-base and T5-large, which are pre-trained on C4, a clean corpus. Therefore, we can make sure that CASR will not produce discriminatory answers.\n\nComputational Resources We train on 4 Tesla V100 GPUs, and the training and inference time consumption is show in §9, which is acceptable considering the performance gain.\n\nREPRODUCIBILITY STATEMENT\n\nWe describe experiment setup in §4.1 and provide the complete details such as hyperparameter (in §C.1) and resource consumption (in §C.2). please see §C. Meanwhile, we make our code open source in this repository https://github.com/RalphHan/CASR. These resources should be sufficient to reproduce results of the paper.\n\nACKNOWLEDGMENTS\n\nThis research was (WDZC20200820200655001).\n\npartly\n\nREFERENCES\n\nsupported\n\nby\n\nShenzhen\n\nStable\n\nSupporting\n\nProgram\n\nMihail Eric, Lakshmi Krishnan, Franc ̧ois Charette, and Christopher D. Manning. Key-value retrieval networks for task-oriented dialogue. In Kristiina Jokinen, Manfred Stede, David DeVault, and Annie Louis (eds.), Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, Saarbr ̈ucken, Germany, August 15-17, 2017, pp. 37–49. Association for Computational Linguistics, 2017. doi: 10.18653/v1/w17-5506. URL https://doi.org/10.18653/v1/ w17-5506.\n\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. Nonautoregressive neural machine translation. CoRR, abs/1711.02281, 2017. URL http:// arxiv.org/abs/1711.02281.\n\nJiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer.\n\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11179–11189, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/675f9820626f5bc0afb47b57890b466e-Abstract.html.\n\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. CoRR, abs/2110.04366, 2021. URL https://arxiv.org/abs/2110.04366.\n\nXinyu Hua and Lu Wang. PAIR: planning and iterative refinement in pre-trained transformers for long text generation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 781–793. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.57. URL https://doi.org/10.18653/v1/2020. emnlp-main.57.\n\nJintae Kim, Sera Park, Dongbo Min, and Wankyu Kim. Comprehensive survey of recent drug discovery using deep learning. International Journal of Molecular Sciences, 22(18):9983, 2021.\n\nCeline Lee, Justin Gottschlich, and Dan Roth. Toward code generation: A survey and lessons from semantic parsing. CoRR, abs/2105.03317, 2021. URL https://arxiv.org/abs/2105. 03317.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence modeling by iterative refinement. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 1173– 1182. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-1149. URL https://doi.org/10.18653/v1/d18-1149.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7871–7880. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.703. URL https://doi.org/10.18653/v1/2020.acl-main.703.\n\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Paola Merlo, J ̈org Tiedemann, and Reut Tsarfaty (eds.), Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 2950–2962. Association for Computational Linguistics, 2021. doi: 10.18653/v1/ 2021.eacl-main.257. URL https://doi.org/10.18653/v1/2021.eacl-main.257.\n\nCongbo Ma, Wei Emma Zhang, Mingyu Guo, Hu Wang, and Quan Z. Sheng. Multi-document summarization via deep learning techniques: A survey. CoRR, abs/2011.04843, 2020. URL https://arxiv.org/abs/2011.04843.\n\nMishaim Malik, Muhammad Kamran Malik, Khawar Mehmood, and Imran Makhdoom. Automatic speech recognition: a survey. Multim. Tools Appl., 80(6):9411–9457, 2021. doi: 10.1007/ s11042-020-10073-7. URL https://doi.org/10.1007/s11042-020-10073-7.\n\nHermann Ney, Dieter Mergel, Andreas Noll, and Annedore Paeseler. A data-driven organization of the dynamic programming beam search for continuous speech recognition. In IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP ’87, Dallas, Texas, USA, April 6-9, 1987, pp. 833–836. IEEE, 1987. doi: 10.1109/ICASSP.1987.1169844. URL https:// doi.org/10.1109/ICASSP.1987.1169844.\n\nHariom A. Pandya and Brijesh S. Bhatt. Question answering survey: Directions, challenges, datasets, evaluation matrices. CoRR, abs/2112.03572, 2021. URL https://arxiv.org/ abs/2112.03572.\n\nKYUBYONG PARK. 1 million sudoku games. https://www.kaggle.com/datasets/\n\nbryanpark/sudoku. Accessed: 2022-05-20.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\n\nstanding by generative pre-training. 2018.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020. URL http://jmlr.org/ papers/v21/20-074.html.\n\nZeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. Treegen: A tree-based transformer architecture for code generation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 8984–8991. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/6430.\n\nBowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric P. Xing, and Zhiting Hu. Progressive generation of long text with pretrained language models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T ̈ur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nChakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 4313–4324. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.341. URL https://doi.org/10. 18653/v1/2021.naacl-main.341.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903, 2022. URL https://arxiv.org/abs/2201.11903.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.\n\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Deliberation networks: Sequence generation beyond one-pass decoding. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1784–1794, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ c6036a69be21cb660499b75718a3ef24-Abstract.html.\n\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, ChienSheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir R. Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. CoRR, abs/2201.05966, 2022. URL https://arxiv.org/abs/2201.05966.\n\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd with visual attention. International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pp. 2048–2057. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/xuc15.html.\n\nShuoheng Yang, Yuxin Wang, and Xiaowen Chu. A survey of deep learning techniques for neural machine translation. CoRR, abs/2002.07526, 2020. URL https://arxiv.org/abs/ 2002.07526.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5754–5764, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html.\n\nXi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. RNG-KBQA: In Smaranda\n\ngeneration augmented iterative ranking for knowledge base question answering.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMuresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 6032–6043. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.417. URL https://doi.org/10.18653/v1/ 2022.acl-long.417.\n\nWen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-2033. URL https://doi.org/10.18653/v1/p16-2033.\n\nMeng-Hsuan Yu, Juntao Li, Danyang Liu, Bo Tang, Haisong Zhang, Dongyan Zhao, and Rui Yan. Draft and edit: Automatic storytelling through multi-pass hierarchical conditional variIn The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI ational autoencoder. 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 1741–1748. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/5538.\n\nXiangwen Zhang, Jinsong Su, Yue Qin, Yang Liu, Rongrong Ji, and Hongji Wang. Asynchronous bidirectional decoding for neural machine translation. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 5698–5705. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16784.\n\nMengyu Zhou, Wang Tao, Pengxin Ji, Han Shi, and Dongmei Zhang. Table2analysis: Modeling In The Thirtyand recommendation of common analysis patterns for multi-dimensional data. Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 712, 2020, pp. 320–328. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/ AAAI/article/view/5366.\n\nA EXAMPLES\n\nUnifiedSKG (Xie et al., 2022) did a great job to formulate 21 structured knowledge grounding tasks into a unified seq2seq form, and we apply their input-output form of WebQSP, MTOP, and KVRET in CASR. And for Sudoku, we apply their original serialization method, which flattens the 9x9 game table row by row into an 81-dimensional number list. Examples of WebQSP (Figure 5), MTOP (Figure 6) and KVRET (Figure 7) are shown.\n\nB BEST RESULT SELECTION\n\nThe default strategy is to run all T casteps and take the Last output ˆY T −1 as the final prediction. However, one could argue that for a specific input X, the best prediction may occur early and more refinements may hurt. Following this we try to pick the ˆY t (0 ≤ t < T ) with the highest prediction probability (given by M t): ˆY = ˆY tpick where tpick = argmaxtP ( ˆY t|X, ˆY t−1, M t). However, that doesn’t always work and we leave it as furture work.\n\nC EXPERIMENT DETAILS\n\nC.1 HYPERPARAMETERS\n\nOn WebQSP, MTOP and KVRET tasks, we initialize CASR models with T5-base (Raffel et al., 2020), a 24-layer generative PLM. We set max casteps T = 3. Max epoch E is set to 2K, 4K, and\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: An Example of WebQSP.\n\nFigure 6: An Example of MTOP.\n\nFigure 7: An Example of KVRET.\n\n4K steps for fine-tuning WebQSP, MTOP and KVRET, respectively. For adapter-tuning, we double the epoch number, which is 4K, 8K, and 8K steps. For the three tasks, we set the batch-size to 128, learning-rate to 2e-5, max-input-length to 1024, max-generation-length to 128, beam-size to 4, and evaluate every 2K steps for checkpoint selection.\n\nFor Sudoku, we train a 12-layer encoder-decoder transformer from scratch, with d-model=512, ffndim=2048, num-heads=8. We set max castep T = 5 and max epoch E = 10K steps. We set the batch-size to 1024, learning-rate to 2e-5, beam-size to 2, and evaluate every 2K steps for checkpoint selection.\n\nC.2 RESOURCE CONSUMPTION\n\nTable 8: CASR Variations and Their Number of Parameters.\n\nA, E and D denote #parameters of adapter, encoder and decoder.\n\nTuning\n\nInit.\n\nArch.\n\n#Parameters\n\nA B\nC D\nE\n\nRestart\n\nSepEnc Fine-tune SepEnc Fine-tune Continue Fine-tune UniEnc Restart Fine-tune Continue UniEnc SepEnc Restart Adapter\n\nT ∗ (2E + D) T ∗ (2E + D) T ∗ (E + D) T ∗ (E + D) T ∗ A + (E + D)\n\nTable 9: Training and Inference Time Consumption (take WebQSP as an example)\n\nMethod INAT (Lee et al., 2018) Levenshtein (Gu et al., 2019) CASR B / Dec\n\nXa Xb Xc Xd Bidirectional (Zhang et al., 2018) Xe B0\n\nProgressive (Tan et al., 2021) Finetune\n\nB2 L2\n\nCASR CASR-L\n\nTraining(h) Inference(min)\n\n14 12 8\n6 9\n3\n\n10 28\n\n2 7\n23 16 16 8\n\n25 141\n\nTable 8 shows the total parameters of each controlled experiment to help the audience understand our methods. In row E , the total parameters is T*A+(E+D) rather than T*A+(2E+D), because Encoder1 and Encoder2 are initialized with the same PLM and frozen all the time.\n\n14\n\nRequest: what school did sir ernest rutherford go to? Answer: (JOIN (R education.education.institution) (JOIN (R people.person.education) m.02m7r))Request: does this recipe have dairy? Answer: [IN:IS_TRUE_RECIPES [SL:METHOD_RECIPES recipe ] [SL:RECIPES_INCLUDED_INGREDIENT dairy ] ]IN:GET: MESSAGE, WEATHER, ALARM, INFO_RECIPES, STORIES_NEWS, REMINDER, RECIPES, EVENT, CALL_TIME, LIFE_EVENT, INFO_CONTACT, CONTACT, TIMER, REMINDER_DATE_TIME, AGE, SUNRISE, EMPLOYER, EDUCATION_TIME, JOB, AVAILABILITY, CATEGORY_EVENT, CALL, EMPLOYMENT_TIME, CALL_CONTACT, LOCATION, TRACK_INFO_MUSIC, SUNSET, MUTUAL_FRIENDS, UNDERGRAD, REMINDER_LOCATION, ATTENDEE_EVENT, MESSAGE_CONTACT, REMINDER_AMOUNT, DATE_TIME_EVENT, DETAILS_NEWS, EDUCATION_DEGREE, MAJOR, CONTACT_METHOD, LIFE_EVENT_TIME, LYRICS_MUSIC, AIRQUALITY, LANGUAGE, GENDER, GROUPIN:SEND: MESSAGEIN:SET: UNAVAILABLE, RSVP_YES, AVAILABLE, DEFAULT_PROVIDER_MUSIC, RSVP_INTERESTED, DEFAULT_PROVIDER_CALLING, RSVP_NOIN:DELETE: REMINDER, ALARM, TIMER, PLAYLIST_MUSIC......IN:DISPREFER: IN:HELP: REMINDERIN:FOLLOW: MUSICRequest: where is the closest grocery_store? Response: we are 4 miles away from whole_foods and from safeway: which one do you prefer?poipoi_typeaddressdistancetraffic_infopizza my heartpizza restaurant528 anton ct5 milesmoderate trafficwhole foodsgrocery store819 alma st4 milesheavy traffichotel keenrest stop578 arbol dr3 milesno trafficsafewaygrocery store452 arcadia pl4 milesno trafficmidtown shopping centershopping center338 alester ave3 milesno trafficround tablepizza restaurant113 anton ct4 milesheavy trafficmandarin rootschinese restaurant271 springer street3 milesmoderate trafficPublished as a conference paper at ICLR 2023\n\nTable 9 shows the training and inference time consumption of CASR and baselines. Note that CASR-L runs slowly because we leverage deepspeed4 to avoid OOM.\n\nWe train on 4 Tesla V100 GPUs. It takes 10, 22, and 19 hours to fine-tune WebQSP, MTOP and KVRET, respectively, and 12, 25, and 21 hours to adapter-tune them. Adapter-tuning is intrinsically hard so it takes more training steps to tune fewer parameters comparing to fine-tuning. For Sudoku, it takes 32 hours to train from scratch.\n\nD DETAILED RESULTS OF EMPIRICAL STUDIES\n\nTable 10: Performance of last-step prediction ˆY 1 when castep t=2, produced by the setting of row B in Table 8, grouped by D ˆY 1 , the density to ˆY 1.\n\nTask WebQSP MTOP\n\nKVRET\n\nMetric Sparse Middle Dense\n\nF1 51.88 85.84 85.78\n\nAcc Match Bleu Micro F1 AVG 56.80 78.52 72.72 16.70 70.36 87.53 83.17 20.18 73.71 89.08 87.63 20.26\n\n64.17 75.09 85.80\n\nTable 11: The average density to input X and previous prediction ˆY t−1 of the setting of row B in Table 8. Note that, when castep t=0, the density to the previous prediction is 0, thus not listed. t\n\nTask Input Previous Prediction Input\n\nWebQSP MTOP KVRET AVG 0.10% 0.11% 0.40% 0.20% 0.34% 0.33% 0.84% 0.50% 0.09%↓ 0.10%↓ 0.39%↓ 0.19%↓ Previous Prediction 0.61%↑ 0.43%↑ 1.01%↑ 0.68%↑\n\n1\n\n2\n\nFigure 8: The Dependencies between Casteps Discovered by CASR. Shown as purple (t=1 attends to t=0) and blue (t=2 attends to t=1) arrows.\n\nD.1 AN EXAMPLE TO COMPUTE DIFFICULTY OF BLANKS\n\nIn the game table Figure 2, for the top-left blank (the ground truth is 6 in green), r=5 (the ground truths of the row blanks are 6, 9, 4, 8, and 3 respectively), c=6 (the ground truths of the column blanks are 6, 4, 3, 9, 8, and 2), and h=4 (the ground truths of the house blanks are 6, 9, 7, and 4). Then, the difficulty to fill the top-left blank is (5-1)*(6-1)*(4-1)=60.\n\n4https://github.com/microsoft/DeepSpeed\n\n15\n\nt=0t=1t=2Published as a conference paper at ICLR 2023\n\nFigure 9: Case study for CASR (row B ) on WebQSP, MTOP, KVRET, and Sudoku at different casteps. The first column shows how the input is serialized from Figure 5, Figure 6, and Figure 7. For WebQSP, MTOP, and KVRET, keywords are highlighted with green color, and red color denotes wrong predictions. For Sudoku, the white fields denote the blanks to be filled, the red numbers in them denote that the predictions are wrong, and the green ones denote the correct predictions. It’s a coincidence that the Sudoku prediction for this sample remains the same for t=2,3,4, so we draw them together.\n\n16\n\nTask&Input XCASR PredictionsGround Truth YWebQSPinput:task: webqsp ; what school did sir ernest rutherford go to? ; structured knowledge: Ernest Rutherford: m.02m7r | m.068kst common.topic.notable_types m.01m9 | (omitted to save space) | m.018y0w base.actt=0(AND (JOIN common.topic.notable_types m.01nf) (JOIN (R education.education.institution) (JOIN (R people.person.education) m.02m7r)))t=1(JOIN (R education.education.institution) (JOIN (R people.person.education) m.02m7r))t=2(JOIN (R education.education.institution) (JOIN (R people.person.education) m.02m7r))(JOIN (R education.education.institution) (JOIN (R people.person.education) m.02m7r))MTOPinput:task: mtop ; does this recipe have dairy ; structured knowledge: IN:GET: MESSAGE, WEATHER,... | (omitted to save space) | IN:FOLLOW: MUSICt=0[IN:IS_TRUE_RECIPES [SL:RECIPES_INCLUDED_INGREDIENT dairy ] ]t=1[IN:IS_TRUE_RECIPES [SL:METHOD_RECIPES recipe ] [SL:RECIPES_INCLUDED_INGREDIENT dairy ] ]t=2[IN:IS_TRUE_RECIPES [SL:METHOD_RECIPES recipe ] [SL:RECIPES_INCLUDED_INGREDIENT dairy ] ][IN:IS_TRUE_RECIPES [SL:METHOD_RECIPES recipe ] [SL:RECIPES_INCLUDED_INGREDIENT dairy ] ]KVRETinput:task: kvret ; where is the closest grocery_store ; structured knowledge: col : poi | poi_type | address | distance | traffic_info row 1 : pizza my heart | pizza restaurant | (omitted to save space) | 3 miles | moderate traffic ; context:t=0whole_foods is 4 miles away at 819_alma_st.t=1the closest grocery_store is whole_foods at 819_alma_st.t=2the closest grocery_store is whole_foods which is 4 miles away at 819_alma_st.we are 4 miles away from whole_foods and from safeway: which one do you prefer?Sudokut=0t=1t=2,3,4tYˆPublished as a conference paper at ICLR 2023\n\nFigure 10: Case study for other experiments (baselines and CASR-L).\n\n17\n\nMethodWebQSPMTOPKVRETINAT(JOIN (R education.education.institution) (AND (JOIN education.education.degree).02 (JOIN educationeducation.JO3.)www.w m.educationma02date)and_IN: the_ ]ACT [ ] the for: ] ]IN forerror iserror__ theerror__ at is is_error___ is___.. would would is is_error is isLevenshtein(JOIN (R education.education.institution) (JOIN (R people.person.education) m.03xsv3))_what is for you.CASR /Dec(JOIN (R education.education.institution) (JOIN (R people.person.education) m.02mjmr))[IN:GET_RECIPES [SL:RECIPES_DISH chicken ] ]the nearest grocery_store is whole_foods at 819_alma_st. would you like directions there?Bidirectional(JOIN (R education.education.institution) (JOIN (R people.person.education) m.02m7r))[IN:GET_RECIPES [SL:RECIPES_INCLUDED_INGREDIENT dairy ] ]safeway is 4 miles away.Progressive(AND (JOIN common.topic.notable_types m.01nf) (JOIN (R education.education.institution) (JOIN (R people.person.education) m.02m7r)))[IN:IS_TRUE_RECIPES [SL:RECIPES_INCLUDED_INGREDIENT dairy ] ]the closest grocery_store is safeway.CASR-L(JOIN (R education.education.institution) (JOIN (R people.person.education) m.02m7r))[IN:IS_TRUE_RECIPES [SL:RECIPES_INCLUDED_INGREDIENT dairy ] ]the closest grocery_store is whole_foods at 819 alma st.Published as a conference paper at ICLR 2023\n\nAlgorithm 1 CASR Inference Process. Input: max castep T ; input X (from test set);\n\nwell-trained CASR models M t (0 ≤ t < T )\n\nOutput: the best prediction ˆY for X 1: for t in 0, 1, ..., T − 1 do 2: 3: 4: 5: 6:\n\n/* argmax by beam searching */ if t = 0 then\n\nˆY t ← argmaxY P (Y |X, M t)\n\nelse\n\n←\n\nˆY t P (Y |X, ˆY t−1, M t)\n\nargmaxY\n\nend if\n\n7: 8: end for 9: return ˆY\n\nAlgorithm 2 CASR Training Process (§3.1). Input: max castep T ; max epoch E; all input X and ground truth Y in train (St) and dev (Sd) sets Output: CASR models M t (0 ≤ t < T ); 1: for t in 0, 1, ..., T − 1 do 2: 3: 4:\n\nθ ← parameter initialization (§3.4) for e in 0, 1, ..., E − 1 do\n\n/* argmax by updating θ using\n\n5: 6:\n\n7: 8:\n\n9: 10: 11: 12:\n\n13:\n\n14: 15: 16: 17:\n\ntrain set */\n\nif t = 0 then\n\nθ P (Y |X, θ)\n\n←\n\n(cid:80)\n\n(cid:80)\n\n(X,Y )∈St else\n\n(X,Y )∈St\n\nθ P (Y |X, ˆY t−1, θ)\n\n←\n\nargmaxθ\n\nargmaxθ\n\ne ← θ\n\nend if M t end for M t ← checkpoint selection from M t e\n\n(0 ≤ e < E) on Sd\n\nSt\n\n/* predict with beam searching ∀X ∈ (cid:83) Sd using M t */ if t = 0 then\n\nˆY t ← argmaxY P (Y |X, M t)\n\nelse\n\nˆY t P (Y |X, ˆY t−1, M t)\n\n←\n\nargmaxY\n\nend if\n\n18: 19: end for 20: return M t (0 ≤ t < T )\n\n18",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a novel approach [CASR] for sequence generation where multiple language models are connected in an iterative manner and predictions of a previous timestep are taken as an input to the model at future timestep [different from vanilla RNN]. The rationale is that sequence generation problems only specify “difficult” FINAL sequences which need to be solved, and the intermediate order to solve “simpler sequences” is defined by heuristics [Tan et Al].\n\n# Strength And Weaknesses\n\n**Strengths**\n\n- The authors demonstrate that a network can itself learn the simpler sequences it needs to solve, and that there is a natural order of progression from solving “simpler” sequences to complex ones. Furthermore, they empirically reveal that this progression is different from the simple left-to-right solving order that sequence generation usually takes. \n\n- The manuscript is well crafted, with convincing experiments and detailed ablations. Authors claim SOTA [70%] on KVRET F1 metric. \n\n**Weaknesses**\n\n- The basic training process [Algo 2] has a limitation that the language model at each timestep needs to be trained fully [line 3] before moving on to the next timestep. This prevents the network to be E2E trainable, leaving CASR prone to parameter-effficient tuning. [3.3].\n\n- Authors define the notion of density measuring the attention magnitude at each sequence token [5.2]. Increasing density at each recurrent step, seems to show that the prediction made by the model becomes more informative. I am curious as to whether the density becomes maximum when the network converges to optimal solution. Please clarify this.\n\n- On sudoku, the restart strategy lags behind Continue strategy by a huge difference ~20% (table 4). But we don’t observe this trend in other three datasets, well recognized in the community (Table 3).  Could the authors please mention their intuitions on these observations?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAuthors have released the code for this work anonymously.\n\n# Summary Of The Review\n\nThis work makes a SINGLE minor change to the mathematical formulation of Tan et Al [Figure 1], but still improves empirical results on only 1 out of 3 datasets studied. [Sudoku is a toy dataset from Kaggle which authors don’t show other methods against]. Due to lack of novelty in technical contribution, but still beating SOTA on 1 dataset, this work has incremental contributions.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents a novel framework, CASR (Autoregressive Self-Boost Refinement), aimed at enhancing the generation of complex sequences that require multi-hop dependencies, such as Sudoku solutions and structured programming tasks. The authors propose a self-refinement mechanism that allows the model to iteratively generate intermediate sequences based on its previous predictions, rather than relying on heuristic rules. The methodology includes a detailed exploration of the model architecture, parameter efficiency, and initialization strategies. Empirical evaluations on four complex tasks—Sudoku, WebQSP, MTOP, and KVRET—demonstrate significant improvements in performance, with notable metrics indicating state-of-the-art results.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to overcoming the limitations of traditional autoregressive models, particularly in handling complex dependencies. The iterative refinement mechanism introduces a novel dimension to sequence generation, which is well-supported by empirical results across diverse datasets. However, a potential weakness is the reliance on pre-trained models, which may limit the framework's generalizability to tasks that significantly differ from the training domains. Additionally, while the paper outlines the advantages of different model configurations, further analysis on the trade-offs in different scenarios could enhance understanding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly structured, with a logical flow from introduction to conclusion. Technical details regarding the CASR framework are articulated well, making the methodology accessible to readers. The novelty of the approach is strong, particularly in its self-refinement strategy, which is not commonly explored in existing literature. Reproducibility is addressed through the provision of open-source code and the use of public datasets, contributing positively to the paper's quality and impact.\n\n# Summary Of The Review\nOverall, the paper offers a significant advancement in the field of sequence generation by introducing a self-boost refinement mechanism that effectively handles complex dependencies. The empirical results affirm the framework's effectiveness, and the clarity of presentation aids in understanding the contributions. However, further exploration of model limitations and trade-offs could enhance the discussion.\n\n# Correctness\n5/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces CASR (Autoregressive Self-Boost Refinement), a novel framework designed to address the challenges of generating complex sequences where dependencies and logical reasoning play a significant role, such as Sudoku solutions and structured code. The methodology involves iteratively refining predictions based on previous outputs and the original input, utilizing pre-trained language models (PLMs) while exploring different architectural and initialization strategies for efficiency. The findings report substantial performance improvements across multiple tasks, achieving state-of-the-art results in several benchmarks, thereby demonstrating the effectiveness of the proposed approach in handling complex sequence generation without the limitations of traditional left-to-right autoregressive models.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative framework that addresses a significant gap in sequence generation tasks, particularly those with complex dependencies. The empirical validation through extensive experiments on diverse datasets is a notable contribution, showcasing clear improvements over existing methods. However, the paper also has weaknesses, including the potential complexity in training due to the iterative refinement process and a reliance on the quality of underlying PLMs, which may introduce variability in performance. Additionally, the model's complexity could lead to overfitting, particularly in tasks with limited training data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to readers. The methodology is detailed, with sufficient explanations of the CASR framework, experimental design, and results. The novelty of the approach is evident in its departure from traditional autoregressive models, and the authors have taken steps to ensure reproducibility by providing open-source code. However, some aspects of the training complexity could benefit from further elaboration to aid understanding.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of sequence generation, effectively addressing complex dependencies through the CASR framework. The empirical results validate the approach, though potential training complexities and reliance on PLMs warrant consideration.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces CASR (Complex Autoregressive Self-Boost Refinement), a novel framework for generating complex sequences that require multiple logical steps and dependencies. Unlike traditional left-to-right autoregressive models, CASR employs iterative refinement of predictions, enabling it to effectively handle tasks such as Sudoku and structured code generation. The authors demonstrate that CASR outperforms existing methods across several benchmarks, achieving significant improvements in performance metrics such as F1 scores and accuracy.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to sequence generation, particularly the self-boost refinement mechanism that enhances output quality through iterative learning. The comprehensive experimental evaluation across diverse tasks showcases the framework's robustness and effectiveness. However, a potential weakness is the reliance on specific architectures (SepEnc and UniEnc) and initialization strategies, which may limit the generalizability of the findings. Additionally, while the empirical results are promising, further exploration of the framework's applicability in more varied contexts could strengthen its significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and experimental results. The novelty of the CASR framework is significant, as it addresses the limitations of traditional autoregressive models in generating complex sequences. The reproducibility of the results is also enhanced by the authors' commitment to open-sourcing their code and providing detailed experimental setups, including hyperparameters and resource consumption.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative framework for generating complex sequences, demonstrating significant improvements over existing methods. The clarity of the presentation and the thoroughness of the experimental evaluation contribute to its strength, though further exploration of its applicability to a broader range of tasks would be beneficial.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the CASR framework, which employs an autoregressive self-boost refinement approach aimed at enhancing sequence generation, particularly for complex tasks. The methodology is characterized by a data-driven strategy for generating intermediate sequences, thereby reducing reliance on heuristic rules. The authors present empirical results demonstrating significant performance gains on benchmarks such as Sudoku and KVRET, achieving state-of-the-art outcomes. The comprehensive evaluation includes controlled experiments across several tasks, and the code is made available for open-source use.\n\n# Strength And Weaknesses\nThe CASR framework showcases several strengths, including its innovative approach to sequence generation and the substantial performance improvements it achieves on specific tasks. The comprehensive evaluation across different datasets adds credibility to its efficacy. However, there are notable weaknesses; for instance, the performance improvements may not generalize across all sequence generation tasks, and a thorough analysis of computational trade-offs is missing. Additionally, the data-driven approach requires high-quality training data, which may not always be feasible. The focus on a limited set of tasks could also restrict the assessment of the framework's versatility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, effectively communicating the contributions and methodologies employed. The novelty of the autoregressive self-boost refinement approach is evident, and the open-sourcing of the code aids reproducibility. However, the paper could benefit from a more detailed discussion on the implications of the attention mechanism analysis and the balance between parameter efficiency and performance. Overall, the quality is high, but some critical aspects could be elaborated upon for better clarity.\n\n# Summary Of The Review\nThe CASR framework presents a noteworthy advancement in sequence generation for complex tasks, showcasing impressive performance improvements. While the methodology is innovative and the results are promising, the paper lacks a comprehensive exploration of generalizability, computational trade-offs, and the required data quality. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents CASR (Complex Sequence Autoregressive Self-Boost Refinement), a framework designed to enhance sequence generation tasks that require handling complex logical dependencies. CASR introduces a self-boosting mechanism to refine predictions iteratively, overcoming limitations associated with conventional left-to-right autoregressive models. The methodology includes a data-driven approach for generating intermediate sequences, an innovative encoder-decoder architecture that integrates prior predictions, and strategies for parameter efficiency. The framework demonstrates significant performance improvements on various complex sequence tasks, achieving state-of-the-art results across benchmarks like Sudoku and KVRET.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to data-driven sequence generation, which represents a noteworthy advancement from traditional heuristic methods. The empirical results substantiate the effectiveness of CASR, showcasing robust performance across multiple tasks. Additionally, the focus on parameter efficiency through the use of adapters is timely and relevant in the context of increasing computational demands. However, the paper could improve by providing more detailed explanations regarding the data-driven methods for generating intermediate sequences, and a comparative analysis with non-autoregressive methods would enhance the understanding of CASR's advantages. Furthermore, a deeper exploration of the impact of initialization strategies on model performance could provide additional insights into the framework's flexibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, presenting its contributions and methodologies in a logical manner. The quality of the writing allows for an effective understanding of the proposed framework and its implications. The novelty of the approach is significant, particularly in the context of autoregressive models. However, the reproducibility of the results could be improved by including more detailed descriptions of the experimental setup and the specific data-driven techniques utilized for generating intermediate sequences.\n\n# Summary Of The Review\nOverall, CASR represents a meaningful contribution to the field of complex sequence generation, effectively addressing the limitations of existing autoregressive models. The innovative methodologies and empirical results indicate a strong potential for real-world applications, though further exploration of certain aspects could enhance the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the Complex Autoregressive Self-Boost Refinement (CASR) framework, which aims to enhance adversarial training in neural networks. The methodology employs a self-boosting strategy that iteratively refines adversarial examples, allowing models to generate robust outputs better equipped to withstand adversarial attacks. The findings demonstrate that CASR significantly improves model robustness and performance metrics, including accuracy and F1 scores, across complex datasets known for their challenges in adversarial scenarios.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to adversarial training, which reduces reliance on heuristic methods for generating adversarial examples by implementing a data-driven refinement strategy. The comprehensive empirical evaluation showcases the robustness of CASR across various benchmarks, establishing it as a leading method in the field. However, a notable weakness is the complexity of the framework, which may pose practical challenges in real-time applications due to its computational demands. Additionally, the potential for further integration with other adversarial defenses remains unexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized, with a clear presentation of the CASR framework and its underlying principles. The quality of the writing is high, making the methodology and results accessible to readers. The novelty of the approach is significant, as it introduces a fresh perspective on adversarial training by leveraging autoregressive models for refinement. However, the reproducibility of results may be impacted by the complexity of the framework and the specific implementation details that could be challenging for other researchers to replicate without extensive guidance.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in adversarial training through the CASR framework, demonstrating substantial improvements in model robustness. While the approach showcases strong empirical results, its complexity may hinder practical applicability in certain scenarios. Future research could further explore integrations with additional adversarial defenses to enhance robustness.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces the CASR (Complex Autoregressive Self-Boost Refinement) framework, which aims to address the challenges of generating complex sequences that require multi-hop dependencies. The authors claim that CASR significantly improves performance in sequence generation tasks, exemplified by a remarkable accuracy increase from 70.93% to 97.28% in Sudoku and a Micro F1 score enhancement from 67.88% to 70.00% on KVRET. The methodology includes an iterative refinement process that allows for the generation of intermediate sequences without requiring domain expertise, thus democratizing access to advanced sequence generation techniques. The authors assert that CASR sets a new standard for future research in this area.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ambitious claims regarding the capabilities of the CASR framework, particularly in relation to complex sequence generation tasks. The empirical results presented are compelling and suggest a significant leap in performance over existing methodologies. However, the paper also exhibits weaknesses in its exaggerated portrayal of CASR's impact, which may undermine the credibility of its claims. The framing of traditional models as obsolete and the failure to provide sufficient context for the performance metrics may lead to skepticism regarding the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-organized, but the clarity is occasionally diminished by the hyperbolic language used to describe the model's capabilities. While the novelty of the CASR framework is noteworthy, the overall quality of the presentation could benefit from a more measured tone. The authors claim to address ethical considerations and reproducibility, yet the lack of detailed methodology for the empirical studies raises concerns about reproducibility and the validity of the results.\n\n# Summary Of The Review\nOverall, the paper proposes a promising and novel framework for complex sequence generation, backed by impressive empirical results. However, the exaggerated claims and presentation style may detract from the paper's credibility and raise questions about the reproducibility of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the CASR framework, which seeks to address the challenges of generating complex sequences that require logical reasoning and have multi-hop dependencies. By employing an autoregressive self-boost refinement approach, CASR iteratively generates predictions based on its own outputs, thus minimizing exposure bias and enhancing output quality. Empirical results demonstrate significant improvements in performance across various tasks, including Sudoku, WebQSP, MTOP, and KVRET, with accuracy and F1 scores surpassing state-of-the-art benchmarks.\n\n# Strength And Weaknesses\nThe key strengths of the paper lie in its innovative approach to refining predictions through self-generation, which effectively tackles the limitations of traditional left-to-right autoregressive models. The empirical results are compelling, showcasing substantial improvements in accuracy and F1 scores across multiple complex sequence generation tasks. However, a potential weakness is the slightly lower performance of the parameter-efficient tuning method compared to full fine-tuning, which may limit the practical applicability of the proposed framework in resource-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The quality of writing is high, making it accessible to readers with varying levels of expertise. The novelty of the CASR framework is evident in its approach to handling complex sequence dependencies through iterative refinements. Furthermore, the authors ensure reproducibility by providing access to datasets and code, which is a commendable aspect of the paper.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective framework for generating complex sequences through autoregressive self-boost refinement. The empirical results validate the approach, demonstrating significant performance gains over existing methods. The clarity and reproducibility of the work enhance its contribution to the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper \"CASR: Generating Complex Sequences with Autoregressive Self-Boost Refinement\" presents a novel framework for generating complex sequences characterized by multi-hop dependencies. The authors propose a self-boosting mechanism aimed at improving the accuracy of predictions through iterative refinements. They evaluate their approach on tasks such as Sudoku and WebQSP, claiming enhanced performance over existing methods. The methodology emphasizes the importance of data-driven strategies over heuristic rules and advocates for parameter-efficient tuning via adapters and pre-trained models.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to sequence generation, particularly through the self-boosting mechanism, which aims to capture complex dependencies. However, several weaknesses are evident in the assumptions made. For example, the categorization of tasks as complex may exclude simpler tasks that could also benefit from the proposed methods. Additionally, the critique of heuristic methods lacks consideration of their potential advantages when combined with data-driven learning. The assumption that left-to-right models inherently struggle with complex tasks does not account for successful architectural adaptations that demonstrate their effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat undermined by its reliance on specific evaluation metrics without addressing their limitations, which might skew the interpretation of model performance. The novelty of the self-boosting mechanism is notable, although the paper does not sufficiently explore the potential pitfalls of iterative predictions, such as compounding errors. Reproducibility could be a concern given the assumptions about parameter efficiency and model initialization, which may not universally translate to all datasets or tasks.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for generating complex sequences but is marred by several unexamined assumptions that could limit its applicability and robustness. While the self-boosting mechanism shows potential, the work would benefit from a more nuanced exploration of the interplay between heuristic and data-driven approaches.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces CASR, a novel framework designed to generate complex sequences through an autoregressive self-boost refinement mechanism. Unlike traditional left-to-right generation methods, CASR incorporates previous predictions into the input for subsequent outputs, allowing for an iterative refinement process that enhances sequence quality. The experiments demonstrate that CASR achieves state-of-the-art results across various benchmarks, such as WebQSP, MTOP, KVRET, and Sudoku, outperforming both traditional autoregressive models and heuristic-based techniques.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to sequence generation, effectively addressing the limitations of existing methods when dealing with complex dependencies. The iterative refinement process mimics human problem-solving strategies, which is a significant conceptual contribution. However, the paper could improve by providing more detailed comparisons with state-of-the-art methods and discussing the computational efficiency of the CASR framework, which is crucial for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and findings, making it accessible to readers. The quality of the experiments is high, with comprehensive evaluations across multiple benchmarks. The novelty of the CASR framework is evident, as it proposes a unique approach to sequence generation. However, the reproducibility could be enhanced by providing more details on the model architecture and training procedures, as well as making the code available for public access.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of sequence generation through the introduction of the CASR framework. Its novel methodology and empirical results indicate promise for future research, though additional clarity on implementation details would enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel framework for enhancing the performance of deep reinforcement learning (DRL) agents in complex environments. The authors propose a hybrid approach that combines model-based and model-free learning techniques to improve exploration efficiency and reduce training time. Through extensive empirical evaluations, the paper demonstrates that the proposed method outperforms existing state-of-the-art algorithms across multiple benchmark tasks, showcasing significant gains in both sample efficiency and overall performance.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of model-based and model-free approaches, which offers a fresh perspective on addressing exploration challenges in DRL. The empirical results are compelling, indicating that the proposed method can effectively improve agent performance in various environments. However, the paper exhibits some weaknesses, particularly in the lack of comprehensive baseline comparisons, which limits the context in which the contributions are evaluated. Additionally, the discussion of potential limitations is somewhat superficial and could benefit from more depth.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents the methodology and findings, making it accessible to readers. However, some technical sections could use more detailed explanations or examples to facilitate understanding. The novelty of the proposed approach is notable, as it bridges two significant areas in DRL research. The reproducibility of the results is supported by a clear description of the experimental setup, although providing open-source code or supplementary materials would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper presents a promising contribution to the field of deep reinforcement learning, introducing a hybrid approach that effectively combines model-based and model-free strategies. While the empirical results are strong, the paper would benefit from more comprehensive comparisons and a deeper exploration of its limitations. With revisions addressing these areas, this work could significantly impact the DRL community.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents CASR (Complex Sequence generation with Autoregressive Self-Boost Refinement), a novel framework designed to tackle the challenges of generating complex sequences, such as Sudoku puzzles and structured code, which require managing multi-hop dependencies. Traditional autoregressive models, which typically generate sequences in a left-to-right manner, struggle with these tasks due to exposure bias and reliance on heuristic rules. CASR addresses these issues by employing a data-driven method to determine intermediate sequences and iteratively refining predictions using outputs from previous steps. The framework has shown significant performance improvements across various benchmark tasks, achieving state-of-the-art results in some cases.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to sequence generation, specifically through the self-boost refinement mechanism that allows for better handling of complex dependencies without manual intervention. The framework’s compatibility with pre-trained language models enhances its applicability and efficiency. However, a potential weakness is the reliance on the performance of the underlying language models, which may affect the generalizability of the proposed method. Additionally, the paper could benefit from a more extensive discussion on the limitations and potential scenarios where CASR might underperform compared to traditional methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the problem of complex sequence generation and the proposed solution. The methodology is described in sufficient detail, allowing for reproducibility, particularly with the open-sourced code provided. The novelty of the CASR framework is evident, as it introduces a unique approach to refining generative processes in sequence tasks, setting it apart from existing methods. However, the paper could improve by including more empirical comparisons with a wider range of existing techniques to better contextualize its contributions.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of complex sequence generation through the CASR framework, which effectively addresses the limitations of traditional autoregressive methods. The results demonstrate substantial improvements in performance across multiple tasks, suggesting that CASR could be a valuable tool for researchers and practitioners in this area.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the CASR (Complex Autoregressive Self-Boost Refinement) framework for generating complex sequences that require multi-step logical reasoning. It highlights the limitations of traditional left-to-right autoregressive models, particularly regarding dependency handling and exposure bias. The methodology involves an iterative refinement process that leverages data-driven determination of intermediate sequences, resulting in state-of-the-art performance on several tasks, including Sudoku and structured code generation. The findings demonstrate significant improvements in accuracy and F1 scores across multiple datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to addressing exposure bias and its ability to effectively manage complex dependencies in sequence generation. The CASR framework's versatility is a notable advantage, as it shows substantial improvements across a variety of tasks, reflecting its robustness. However, a potential weakness lies in the complexity of the proposed methodology, which may present challenges in understanding and implementation for practitioners not well-versed in autoregressive models. Additionally, while the performance improvements are impressive, further exploration of the computational costs associated with CASR relative to simpler models would enhance the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and articulates its contributions clearly, making it accessible to the reader. The methodology is described in sufficient detail, allowing for reproducibility; the authors provide comprehensive information on the experimental setup and hyperparameters. The novelty of the CASR framework is evident, as it introduces a new paradigm for tackling complex sequence generation tasks. The paper also includes an ethics statement addressing dataset usage, which adds to the overall quality and integrity of the research.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to generating complex sequences through the CASR framework, achieving significant improvements over existing methods. While the complexity of the methodology may pose challenges for some practitioners, the clarity and reproducibility of the work enhance its value to the community.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"CASR: Generating Complex Sequences with Autoregressive Self-Boost Refinement\" presents a novel framework aimed at enhancing the generation of complex sequences. The proposed CASR framework integrates autoregressive modeling with a self-boost refinement process, allowing for improved accuracy and efficiency in sequence generation tasks. The authors validate their methodology through extensive experiments on multiple datasets, including WebQSP, MTOP, KVRET, and Sudoku, demonstrating significant performance improvements over existing state-of-the-art models in key metrics such as Accuracy and Micro F1 score.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive approach to tackling the challenges of complex sequence generation, effectively differentiating itself from existing methods through the introduction of the self-boost refinement process. The clear exposition of the methodology and the rigorous experimental validation further bolster its contributions. However, the paper could benefit from a deeper exploration of the limitations of the CASR framework, particularly in terms of its scalability and computational demands. Additionally, while the results are compelling, further qualitative analysis of model outputs could enhance understanding of its interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and easy to follow, with technical terms clearly defined and a logical flow of information. The quality of writing is high, allowing for a clear grasp of the complex concepts discussed. The novelty of the approach is significant, offering fresh insights into sequence generation techniques. The authors have made commendable efforts in ensuring reproducibility, providing a detailed reproducibility statement and making their code available, which is essential for validating their findings.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and well-supported case for the CASR framework, showcasing its effectiveness in generating complex sequences through innovative methodological advancements. While the contributions are substantial and relevant to ongoing research, the authors should address potential limitations and provide qualitative insights to further enrich the study.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the CASR (Autoregressive Self-Boost Refinement) framework, aimed at generating complex sequences that require non-linear logical dependencies, such as Sudoku solutions and structured code representations. The methodology leverages an iterative refinement mechanism that incorporates predictions from prior steps into subsequent generations to enhance contextual relevance and mitigate exposure bias. Empirical evaluations demonstrate significant performance improvements across multiple benchmarks, with the CASR framework achieving state-of-the-art results in accuracy and handling of complex sequences compared to traditional heuristic methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to sequence generation through self-referential prediction refinement, which effectively addresses the challenges posed by complex dependencies in data. The CASR framework's architecture, which includes both separate and unified encoders, offers flexibility and parameter efficiency that can be beneficial in various applications. However, the paper could be improved by providing a more thorough exploration of the limitations or potential drawbacks of using the CASR framework, particularly in terms of scalability and computational efficiency in larger models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodology, and findings, making it accessible to readers familiar with the domain. The empirical evaluations are robust and support the claims made about the effectiveness of the CASR framework. However, the reproducibility could be enhanced by providing more details on the experimental setup, including hyperparameter settings and dataset splits, which would aid other researchers in validating the results.\n\n# Summary Of The Review\nOverall, the CASR framework represents a significant advancement in the field of sequence generation, particularly for tasks with complex logical dependencies. Its innovative approach and strong empirical performance make it a valuable contribution to the literature, although further clarity on its limitations and reproducibility could strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces CASR, a framework aimed at generating complex sequences, which the authors claim improves upon existing methodologies in the field. The methodology heavily relies on autoregressive models and incorporates a self-boost refinement process. However, the definition of \"complex sequences\" remains vague, and the paper presents standard evaluation metrics, claiming to achieve state-of-the-art performance without providing substantial evidence of its superiority over existing approaches.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to tackle sequence generation challenges, particularly exposure bias and heuristic rules. However, the contributions appear to lack originality, as CASR seems to repurpose established models without significant innovation. The vague definition of complex sequences and the limited range of datasets used (Sudoku, WebQSP, MTOP, KVRET) raise concerns about the generalizability of the findings. The self-boost refinement process is critiqued as potentially complicating the generation process rather than simplifying it, and the lack of thorough comparative analysis with prior research diminishes the impact of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from convoluted technical details, particularly regarding model architecture and parameter efficiency, which could hinder both reproducibility and comprehension. The novelty of the proposed method is questionable, as it does not present robust theoretical advancements or insights into sequence generation. Clarity is further compromised by the reliance on pre-trained language models (PLMs) like T5, which raises concerns about the independence of CASR and its effectiveness without these models.\n\n# Summary Of The Review\nOverall, the paper presents CASR as a novel framework for sequence generation; however, it lacks significant innovation and fails to convincingly address key challenges in the field. The findings are based on a limited dataset and standard evaluation metrics, diminishing the impact of the contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the CASR framework, which utilizes an autoregressive self-boost refinement approach to enhance the generation of complex sequences. The methodology emphasizes data-driven efficiency, allowing the framework to determine intermediate sequences without expert input. The findings demonstrate substantial performance improvements across various tasks, with notable accuracy increases on Sudoku and state-of-the-art results on the KVRET dataset. The framework is designed for broad applicability, supporting tasks in automatic data analysis, drug discovery, and programming language generation.\n\n# Strength And Weaknesses\nThe CASR framework showcases several strengths, including its innovative approach to handling dependencies within sequences, leading to higher accuracy and robust performance. The iterative refinement process effectively simulates human problem-solving, resulting in high-quality outputs. However, the paper could benefit from a more detailed discussion on the limitations of the framework and potential challenges in its implementation across varied domains. While the empirical results are impressive, additional comparisons with existing methods could strengthen the evaluation of CASR's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of the methodology and results. The quality of the empirical experiments is high, demonstrating the framework's capabilities across complex tasks. The novelty of the approach is significant, particularly the self-boost refinement methodology that distinguishes CASR from existing techniques. The open-source contribution enhances reproducibility, allowing other researchers to build upon the work and validate the findings independently.\n\n# Summary Of The Review\nOverall, the CASR framework represents a significant advancement in the field of sequence generation, combining innovative methodologies with impressive empirical results. Its broad applicability and open-source nature further enhance its impact, making it a valuable contribution to the community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the CASR (Complex Autoregressive Sequence Refinement) framework designed to address the challenges of generating complex sequences characterized by multi-hop dependencies. It critiques existing autoregressive models for their limitations in managing right-side dependencies and highlights the drawbacks of heuristic approaches that can introduce exposure bias. The proposed CASR framework utilizes a self-boost refinement mechanism that iteratively integrates previous outputs into the model's input for enhanced dependency management. The methodology emphasizes the alignment of training objectives with the inference process, leading to more coherent sequence generation outcomes. The findings suggest that CASR can effectively capture and refine complex dependencies, thus improving performance in tasks requiring intricate sequence generation.\n\n# Strength And Weaknesses\nThe CASR framework presents a significant advancement in the field of sequence generation by introducing a theoretical foundation that addresses the limitations of traditional autoregressive models. Its self-boost refinement mechanism is a noteworthy contribution, as it allows for dynamic learning from previous outputs. However, the paper could benefit from empirical validation of the proposed framework, with more detailed experimental results to substantiate its claims. Moreover, the theoretical discussions, while insightful, may be dense for readers unfamiliar with the specific nuances of complex sequence generation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity suffers somewhat due to its heavy theoretical focus, which may limit accessibility for a broader audience. Nonetheless, the quality of the writing is high, with a structured presentation of ideas and a logical flow. The novelty of the CASR framework is significant, as it offers a fresh perspective on managing dependencies in sequence generation. Reproducibility remains a concern, as the paper does not provide sufficient details on the implementation of the CASR framework, which is essential for other researchers to validate and build upon this work.\n\n# Summary Of The Review\nOverall, the CASR framework represents a promising approach to complex sequence generation, offering significant theoretical insights and addressing critical limitations of existing models. However, the lack of empirical validation and detailed implementation guidance may hinder its immediate applicability and reproducibility in the research community.\n\n# Correctness\n4/5 - The theoretical foundations and proposed mechanisms appear sound, but some claims would benefit from empirical support.\n\n# Technical Novelty And Significance\n5/5 - The introduction of the self-boost refinement mechanism and its emphasis on dynamic dependency management are highly innovative.\n\n# Empirical Novelty And Significance\n3/5 - While the theoretical contributions are strong, the paper lacks substantial empirical evidence to demonstrate the efficacy of the CASR framework in practical applications.",
    "# Summary Of The Paper\nThe paper introduces the CASR (Generating Complex Sequences with Autoregressive Self-Boost Refinement) framework, which aims to generate complex sequences with multi-hop dependencies. The methodology employs an encoder-decoder architecture, featuring two distinct encoding strategies (SepEnc and UniEnc), and leverages pre-trained language models (PLMs) with adapters for parameter-efficient tuning. The empirical findings demonstrate that CASR outperforms existing methods on various benchmarks, including KVRET, WebQSP, MTOP, and Sudoku, with notable accuracy improvements. The authors provide comprehensive details on the training process, initialization strategies, and hyperparameter settings, along with open-source code for reproducibility.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to sequence generation, particularly through the self-boost refinement mechanism, which enhances performance via iterative predictions. The use of parameter-efficient tuning with PLMs is also a significant contribution, allowing for effective model training without extensive resource consumption. However, a potential weakness is the reliance on specific architectures (SepEnc and UniEnc) that could limit applicability to certain tasks or datasets. Additionally, while the empirical results are promising, further analysis on the robustness of the model across diverse scenarios could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results, making it accessible to a broad audience. The quality of the writing is high, with detailed explanations and supportive empirical evidence. The novelty of the CASR framework is significant, particularly in its approach to sequence generation and the incorporation of self-boost refinement. The authors have prioritized reproducibility by providing open-source code and thorough documentation of their experimental setups, facilitating further research in this area.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to generating complex sequences through the CASR framework. Its contributions to parameter efficiency and iterative refinement demonstrate significant improvements over existing methods, although further exploration of its applicability across diverse datasets is warranted.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents the CASR (Complex Autoregressive Self-Boosting Refinement) framework, designed to enhance complex sequence generation tasks by leveraging autoregressive self-boosting techniques. The authors claim that CASR outperforms established autoregressive models like GPT and T5 while addressing issues such as exposure bias. Experimental results indicate state-of-the-art performance on benchmarks like KVRET and significant improvements on Sudoku, although detailed comparative analyses with existing methods are notably absent.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambitious attempt to improve complex sequence generation through innovative architectural choices and self-boosting strategies. However, its weaknesses include a lack of fair evaluations against existing models, insufficient consideration of the complexities introduced by CASR itself, and an absence of a balanced discussion on the limitations of the proposed approach. The authors’ critiques of prior methods may be perceived as overly dismissive, and claims of efficiency and superiority remain unsubstantiated without comparative resource utilization analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-structured, certain claims and methodologies lack clarity and depth. The novelty of CASR is somewhat overshadowed by its similarities to previous iterative refinement methods. Additionally, the reproducibility of results may be in question due to insufficient details regarding experimental setups and parameter tuning processes, which could hinder other researchers from effectively replicating the proposed framework.\n\n# Summary Of The Review\nOverall, the CASR framework presents a promising approach to complex sequence generation, but its contributions are undermined by a lack of rigorous comparative evaluations and an overly critical stance towards existing models. The claims made regarding performance and efficiency require further substantiation to validate their significance within the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel method for generating text using a pre-trained language model (PLM) by introducing a new framework called CASR (Context-Aware Sequence Reordering). The methodology involves fine-tuning the PLM on a diverse set of sequence tasks to improve its ability to reorder sequences contextually. The findings demonstrate that the CASR framework significantly outperforms existing state-of-the-art techniques in generating coherent and contextually appropriate text across various benchmarks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to leveraging context in sequence generation, which addresses a key limitation in previous methods that often neglect contextual dependencies. The methodology is well-defined, and the experimental results are compelling, showing clear improvements in performance metrics. However, the paper lacks a thorough analysis of potential biases in the training datasets, which could impact the generalizability of the findings. Additionally, while the proposed framework is promising, the practical implications and computational efficiency of deploying CASR in real-world applications require further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but there are areas where clarity can be improved, such as the need for more descriptive figure captions and consistent terminology. The novelty of the approach is significant, particularly in the context of existing literature on sequence generation. However, reproducibility could be enhanced by providing more comprehensive details regarding the experimental setup, including specific software versions and hyperparameters used during fine-tuning.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of text generation with its innovative CASR framework. While the methodologies and findings are strong, there are areas for improvement regarding clarity and reproducibility that should be addressed in future revisions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the CASR (Complex Adaptive Sequence Refinement) framework designed to generate complex sequences, specifically highlighting applications in Sudoku, WebQSP, MTOP, and KVRET tasks. The proposed methodology focuses on addressing multi-hop dependencies within sequence generation while introducing refinement strategies to enhance outcomes. However, the findings indicate that the framework's applicability is limited to a few specific tasks, with suggestions for future work aimed at expanding its evaluation and integration capabilities.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to handling multi-hop dependencies and the introduction of refinement strategies. However, the paper has notable weaknesses, such as its limited scope of evaluation, which restricts the understanding of CASR's applicability across diverse tasks. Additionally, the lack of comparative analysis with other state-of-the-art frameworks limits the contextual relevance of the findings. Furthermore, the exploration of heuristic rules and the implications of model interpretability are insufficiently addressed, indicating areas for improvement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, although certain sections could benefit from deeper elaboration, particularly regarding the heuristic generation of intermediate sequences and the computational costs of the proposed strategies. The quality of the methodology is sound, but the novelty is somewhat diminished due to the narrow applicability of the framework. Reproducibility is not thoroughly discussed, as the paper lacks detailed descriptions of experimental setups and evaluation metrics beyond those specific to the tested tasks.\n\n# Summary Of The Review\nOverall, while the CASR framework presents a valuable contribution to the field of sequence generation, its limited evaluation scope and lack of comparative analysis with existing approaches significantly hinder its impact. Future work should aim to broaden the framework's applicability, enhance interpretability, and address ethical considerations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel framework called CASR (Complex Autoregressive Self-Boost Refinement) for generating complex sequences across various tasks, such as WebQSP, MTOP, KVRET, and Sudoku. The authors utilize a range of evaluation metrics, including accuracy, Exact Match (EM), BLEU, and Micro F1 scores, to assess the performance of their model compared to several baselines. The findings indicate that CASR outperforms existing methods, although the paper lacks rigorous statistical significance testing to substantiate these claims.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its comprehensive evaluation of the CASR framework across multiple tasks, which showcases its potential effectiveness. The methodology includes controlled experiments and comparisons with established baselines, providing insights into the model's performance. However, a notable weakness is the lack of statistical rigor, as the authors do not provide p-values or conduct formal significance tests to verify the performance improvements, limiting the robustness of their conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-structured sections and clear descriptions of the model and experiments. However, the quality of the reporting could be improved by including variability measures such as standard deviations, which are crucial for understanding the reliability of the results. While the technical aspects of the CASR framework show promise, the novelty is somewhat diminished by the lack of a solid statistical foundation. Reproducibility is also a concern due to the absence of detailed statistical representations and potential overfitting issues related to model selection.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting framework for sequence generation with promising empirical results. However, its contributions are undermined by a lack of statistical rigor in validating these results, which diminishes the overall impact of the findings.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents the CASR framework for sequence generation tasks, demonstrating its efficacy on benchmark datasets such as Sudoku, WebQSP, MTOP, and KVRET. The methodology involves a novel iterative refinement process, utilizing pre-trained language models (PLMs) like T5. The findings indicate that CASR achieves state-of-the-art results on the evaluated tasks, showcasing its potential in generating accurate sequences.\n\n# Strength And Weaknesses\nStrengths of the paper include the demonstration of state-of-the-art performance on specific tasks, which highlights the framework's effectiveness. However, significant weaknesses are present, such as a lack of exploration into the scalability of CASR to larger and more complex sequence generation tasks. Additionally, the limitations of the model architecture designs (SepEnc vs. UniEnc) are not thoroughly investigated, and there is no evaluation of the framework's robustness against noisy input data. The reliance on a predetermined number of iterative steps raises questions about performance optimization, and the absence of comparisons with a wider range of non-autoregressive models limits the understanding of the framework's relative advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, although the depth of analysis in certain areas could be improved. While the novelty of the CASR framework is evident, the reproducibility of results may be hindered by the lack of thorough evaluation across diverse tasks and datasets. The reliance on specific PLMs also raises concerns about the generalizability of the findings.\n\n# Summary Of The Review\nOverall, the CASR framework presents a promising approach for sequence generation, achieving impressive results on specific benchmarks. However, the paper falls short in several critical areas, including scalability, robustness, and comparative analysis with other models, which limits its overall impact and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework called CASR (Generating Complex Sequences with Autoregressive Self-Boost Refinement), which aims to improve the generation of complex sequences, such as Sudoku puzzles and s-expressions. The authors propose a methodology that incorporates previous predictions to refine outputs, claiming that existing methods hinder performance due to reliance on heuristic rules. The findings indicate that the CASR framework leads to state-of-the-art results on several tasks, although the tasks themselves are not new to the field.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive literature review and the introduction of the CASR framework, which has the potential to enhance sequence generation methodologies. However, the weaknesses are notable; the contributions seem to reiterate well-established concepts in the field without significant innovation. Additionally, the model architecture, featuring two encoder types, may be perceived as unnecessarily complex for a problem that could be approached more straightforwardly.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written and organized, it often falls into clichés and lacks depth in presenting new ideas. The novelty of the approach is questionable, as it appears to rely heavily on common sense principles of problem-solving rather than introducing fundamentally new techniques. Furthermore, the reproducibility of the results may be hampered by a lack of detailed explanations regarding the experimental setup and methodology.\n\n# Summary Of The Review\nOverall, this paper presents a familiar approach to sequence generation that lacks substantial novelty and depth. While the authors may have achieved some state-of-the-art results, the contributions feel more like reiterations of well-known principles rather than groundbreaking advancements in the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces the CASR (Complex Autoregressive Sequence Refinement) framework, which aims to enhance autoregressive sequence generation for tasks characterized by complex dependencies, such as Sudoku and WebQSP. The methodology incorporates an iterative refinement mechanism that allows for the generation of intermediate sequences, addressing multi-hop dependencies. The authors achieve state-of-the-art performance across several benchmarks while discussing the implications of sequence complexity and parameter efficiency.\n\n# Strength And Weaknesses\nThe CASR framework presents significant contributions, particularly in its ability to manage complex dependencies and achieve impressive benchmark results. However, the reliance on specific datasets raises concerns regarding the generalizability of the findings. Additionally, while the iterative refinement mechanism is innovative, it poses challenges related to computational resource management. Exploring hybrid approaches that integrate both autoregressive and non-autoregressive models could enhance efficiency. Furthermore, the reliance on heuristic rules for generating intermediate sequences limits flexibility, suggesting that learned representations may offer a more robust solution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings. The quality of the empirical studies is commendable, although the limited range of benchmarks may affect reproducibility. The discussion on various strategies, such as attention mechanisms and parameter efficiencies, demonstrates a strong grasp of the field's current challenges. However, the potential for broader applicability could be improved through further experimentation across diverse datasets.\n\n# Summary Of The Review\nOverall, the CASR framework offers innovative approaches to sequence generation with promising results on specific benchmarks. However, its applicability could be strengthened by integrating learned representations and exploring hybrid models that combine autoregressive and non-autoregressive techniques.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces CASR, a novel model designed for complex sequence generation tasks. Its main contributions include significant performance improvements across various benchmarks, including Sudoku, KVRET, WebQSP, and MTOP, showcasing accuracy enhancements from 70.93% to 97.28% on Sudoku and setting a new state-of-the-art Micro F1 score of 70.00 on KVRET. Methodologically, CASR employs a fine-tuning approach that surpasses adapter-tuning, with controlled experiments revealing the impact of architectural choices and initialization strategies on performance. The findings indicate that CASR excels in generating complex outputs, especially for longer and more intricate tasks, although it may incur longer inference times compared to some baselines.\n\n# Strength And Weaknesses\nStrengths of the paper include the robust empirical performance across multiple benchmarks and the clear demonstration of CASR's advantages over existing models, such as INAT and Levenshtein. The paper effectively highlights the benefits of full parameter fine-tuning and the importance of architectural choices in achieving optimal results. However, a notable weakness is the longer inference time associated with CASR, which could hinder its practical deployment in real-world applications. Additionally, the diminishing returns observed in refinement steps may limit the perceived incremental value of further enhancements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology, results, and implications of CASR's performance. The quality of the presented data and the clarity of the comparisons with baseline models enhance the overall impact of the findings. The novelty of the approach is evident in the substantial performance gains achieved and the exploration of parameter efficiency. However, reproducibility could be a concern if the details regarding the implementation and training processes are not sufficiently elaborated, particularly in the context of the controlled experiments conducted.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in complex sequence generation tasks through the introduction of CASR, which demonstrates significant performance improvements and outperforms existing models across several benchmarks. While the findings are strong, consideration must be given to the model's longer inference times and the potential challenges in reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a novel approach to [specific problem] through the introduction of an \"autoregressive self-boost refinement\" methodology. The authors claim significant improvements in [specific metrics] when applied to [specific datasets or applications]. The methodology involves [briefly describe the methodology, e.g., algorithmic steps or framework], leading to findings that suggest [summarize key findings, e.g., improved performance, robustness, etc.].\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to [specific problem], which demonstrates promising results in terms of [specific metrics]. The methodology is well-structured, providing a clear framework that can be replicated in future studies. However, the paper has weaknesses, particularly in its clarity and readability. The extensive use of jargon without sufficient explanation may hinder understanding for readers not already familiar with the field. Additionally, the paper could benefit from a more robust discussion of its implications and a clearer articulation of future research directions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel methodology, its clarity suffers due to the heavy use of technical jargon and complex sentence structures. Some figures and tables lack clear labeling and context, which can confuse readers. The overall quality of writing could be improved by simplifying terminology and ensuring consistency in formatting. The reproducibility of the results is somewhat compromised by the lack of detailed explanations for certain methodological choices, as well as the inconsistent referencing style.\n\n# Summary Of The Review\nOverall, the paper proposes a significant advancement in [specific problem] with its novel methodology. However, it requires substantial improvements in clarity and presentation to enhance accessibility and understanding for a wider audience. The findings are promising, but clearer articulation and improved formatting will strengthen the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.9939987194347,
    -1.8904555112179695,
    -1.8433584706307005,
    -1.691097157223482,
    -1.9427877350789262,
    -1.8636497298872507,
    -1.6394516210220784,
    -1.9253268257604201,
    -1.752060720521342,
    -2.0759885008603263,
    -1.6578948593184217,
    -1.3687529069148312,
    -1.6903390762954118,
    -1.812451541554491,
    -1.7641867028382732,
    -1.8783496089350569,
    -2.1538875442463596,
    -1.9654024111225046,
    -1.6914824219734799,
    -1.7347519643516178,
    -2.011344269428537,
    -1.571610524296355,
    -1.9303457035759068,
    -1.7770486214620353,
    -1.7666823135075476,
    -2.0574259132283523,
    -2.0767101372023102,
    -1.9808817413424664,
    -1.7111411495203586
  ],
  "logp_cond": [
    [
      0.0,
      -2.7432327290995926,
      -2.7632072541936252,
      -2.7710409214962977,
      -2.7527839671831873,
      -2.750484134032371,
      -2.8244452710721513,
      -2.774198519878045,
      -2.727838821761878,
      -2.7592095371777137,
      -2.730904547903982,
      -2.8396336218015943,
      -2.762205150355199,
      -2.779678276078775,
      -2.733652782442784,
      -2.7799807559388228,
      -2.7445296687968344,
      -2.744325017249172,
      -2.8049337965332635,
      -2.7353447862366136,
      -2.764112158244097,
      -2.7946612728799103,
      -2.789422397604357,
      -2.761705612897476,
      -2.756571971725181,
      -2.7890914854391857,
      -2.7860960686794316,
      -2.741811265306167,
      -2.8280281393998554
    ],
    [
      -1.499658715386949,
      0.0,
      -1.3085647913231748,
      -1.379514635730386,
      -1.3536075380438133,
      -1.3117347755888702,
      -1.4779718084407802,
      -1.321410318425016,
      -1.227498734838916,
      -1.3381731603363032,
      -1.3715336129455797,
      -1.5666304990655135,
      -1.3103000316415923,
      -1.400084310994869,
      -1.3530787837639853,
      -1.334791712550997,
      -1.3134245391992159,
      -1.364598953614102,
      -1.4329793589366033,
      -1.2913259575994662,
      -1.458243872428806,
      -1.5378751667882298,
      -1.3749596630852643,
      -1.3463963222669717,
      -1.3952262369172856,
      -1.3605232895721777,
      -1.3769485426421648,
      -1.4281824216970653,
      -1.513350257572285
    ],
    [
      -1.4086854515131333,
      -1.2281493086672954,
      0.0,
      -1.2171792065298115,
      -1.3415234166693089,
      -1.291152698086736,
      -1.4652861813686704,
      -1.3197814469667453,
      -1.2484414741053333,
      -1.2868608647048738,
      -1.370608048347144,
      -1.5192330615883738,
      -1.2070639061899608,
      -1.2392069872423992,
      -1.3748709613181962,
      -1.3198974873302027,
      -1.3248831995749935,
      -1.3671880872097224,
      -1.3916522247385072,
      -1.3040458771534074,
      -1.378546849521557,
      -1.461030655265474,
      -1.4499252633611834,
      -1.3714592667149605,
      -1.384088017091265,
      -1.3259639859220091,
      -1.3798473103860969,
      -1.3739575932946362,
      -1.4446694481172246
    ],
    [
      -1.2952414043755511,
      -1.0846893271523315,
      -1.0489691170998348,
      0.0,
      -1.1873065047467428,
      -1.09938359692688,
      -1.2492126922724667,
      -1.1163730879030502,
      -1.0410670429247066,
      -1.1011362614080036,
      -1.1265789555245604,
      -1.3483447946735805,
      -1.0780485751843703,
      -1.0591245795175912,
      -1.1876684027829494,
      -1.0897537353462106,
      -1.159187365747606,
      -1.1492499202153315,
      -1.2646990336565844,
      -1.070307413488448,
      -1.231201939829449,
      -1.2941456841551835,
      -1.252054879078479,
      -1.2022589246648825,
      -1.1810856921961188,
      -1.1186878898589185,
      -1.1843935320456767,
      -1.2103555326527606,
      -1.2779898451297829
    ],
    [
      -1.5609888255370412,
      -1.354594149165553,
      -1.4846680442890612,
      -1.509622028792115,
      0.0,
      -1.367147345435712,
      -1.523721262641381,
      -1.4237317558448421,
      -1.4498806332112473,
      -1.454722158462664,
      -1.5421368048523296,
      -1.61321075201717,
      -1.4621343147633599,
      -1.4842344480887153,
      -1.4911248410315996,
      -1.5042971437204251,
      -1.4458271158391331,
      -1.4033114692974216,
      -1.5539006983936752,
      -1.4433072453195612,
      -1.5022926490455635,
      -1.6502440233787017,
      -1.5082100310850175,
      -1.4420827820380913,
      -1.486876648144813,
      -1.4681493955101403,
      -1.4558580819174791,
      -1.5103613876185744,
      -1.5478244246996162
    ],
    [
      -1.458556620940833,
      -1.3434487577385403,
      -1.3740849946742553,
      -1.3940188296619762,
      -1.3609183735466404,
      0.0,
      -1.4866115601265193,
      -1.3839830901142778,
      -1.34077480155128,
      -1.3092484879900184,
      -1.350248522541998,
      -1.595869681626535,
      -1.3754159716658887,
      -1.4021109288963278,
      -1.4146019915227983,
      -1.400802588281475,
      -1.3736533068375611,
      -1.3717103950869498,
      -1.4854993169660669,
      -1.3939176954610506,
      -1.3660416641224715,
      -1.5473730219790034,
      -1.4474051647357038,
      -1.434294271323026,
      -1.4355638586183124,
      -1.400102078628652,
      -1.403599437774452,
      -1.399228645399728,
      -1.5149850855846534
    ],
    [
      -1.3741149496778313,
      -1.1993284951434653,
      -1.2405035851726243,
      -1.2239061738644896,
      -1.1938990728349645,
      -1.2256502684681327,
      0.0,
      -1.2401881617851325,
      -1.236243989250329,
      -1.2535574542317574,
      -1.2939317813751063,
      -1.3198791221009605,
      -1.263092308609805,
      -1.2439114768443928,
      -1.250417394508365,
      -1.268146659741476,
      -1.2616893341721107,
      -1.241097951685675,
      -1.2790511445229964,
      -1.210603262836341,
      -1.2736337184435622,
      -1.3208801948838487,
      -1.3331977580856347,
      -1.204645879551914,
      -1.3198920922005077,
      -1.2046385095778929,
      -1.2881829959335744,
      -1.2759173412819658,
      -1.2253800925636835
    ],
    [
      -1.5825572762224787,
      -1.4166223584079636,
      -1.4530216521610082,
      -1.4573746521902018,
      -1.4500595032179928,
      -1.4177710875467684,
      -1.5659533450708336,
      0.0,
      -1.381786686915317,
      -1.420383300907044,
      -1.4619175116029555,
      -1.650585594283778,
      -1.4394693184250125,
      -1.431495635124377,
      -1.4236576004525199,
      -1.4857455440198952,
      -1.4564515783869056,
      -1.3966062207719638,
      -1.5240056174790613,
      -1.4411956222835614,
      -1.485952436865621,
      -1.613317067084272,
      -1.4784869768262803,
      -1.471820581770596,
      -1.5102089851049567,
      -1.5234699918841534,
      -1.4539075657095626,
      -1.3975586456132485,
      -1.6108250706975926
    ],
    [
      -1.3062587184147876,
      -1.0569305698846598,
      -1.1250254269314108,
      -1.1369436896965424,
      -1.2278047986958593,
      -1.1382498523482554,
      -1.3230159308274994,
      -1.1294620510455986,
      0.0,
      -1.1237900704867858,
      -1.1348570504549222,
      -1.4152521471827095,
      -1.1245796610785899,
      -1.0989870025174304,
      -1.1503738234726038,
      -1.18881191715005,
      -1.1325393530675782,
      -1.1596303070945961,
      -1.288614128824715,
      -1.088452963210128,
      -1.2323351389400394,
      -1.425591043280467,
      -1.2430050257408485,
      -1.20645290707101,
      -1.2278569799133163,
      -1.2450214055521227,
      -1.2671390233249984,
      -1.1895154441769777,
      -1.3740691146833435
    ],
    [
      -1.7299457523818675,
      -1.5013713896615364,
      -1.5669082991363046,
      -1.5745277138481657,
      -1.6019316067308764,
      -1.5441675350229365,
      -1.6687710448756474,
      -1.5679169436044493,
      -1.4912544658316873,
      0.0,
      -1.596650590929628,
      -1.8132343734892926,
      -1.525307269547218,
      -1.6095891771291582,
      -1.5683537620000383,
      -1.600922061738933,
      -1.504584936491038,
      -1.6039137888381922,
      -1.6614275822134839,
      -1.488892377465399,
      -1.634436304332265,
      -1.7590647272522948,
      -1.608794102585057,
      -1.5620088739926299,
      -1.6486062666914434,
      -1.556928359220313,
      -1.559161446790914,
      -1.6220688816361741,
      -1.7534386804080029
    ],
    [
      -1.233658146379918,
      -1.0656506922418487,
      -1.080625717722098,
      -1.1096176265735118,
      -1.1792888132367096,
      -1.047573025341672,
      -1.242033078736236,
      -1.0912930165399062,
      -1.0072746740417502,
      -1.0555028513368439,
      0.0,
      -1.339578872835277,
      -1.077953444557654,
      -1.1152540040409105,
      -1.0701686048155632,
      -1.0881126851075906,
      -1.0514709495019754,
      -1.07293453406721,
      -1.1950348884186055,
      -1.0998636530457668,
      -1.1557799921381708,
      -1.2932348507779965,
      -1.155987541647729,
      -1.0659137439751762,
      -1.176158656349224,
      -1.1153196826724816,
      -1.1830130496657991,
      -1.159304877739409,
      -1.2538207613413515
    ],
    [
      -1.1125161275198274,
      -1.0396706266379407,
      -1.0560889329831904,
      -1.0543891436458062,
      -1.0413387251561774,
      -1.0808840363745476,
      -1.0350636158076472,
      -1.080865075087965,
      -1.0817915827018294,
      -1.088255787024861,
      -1.0460616090874117,
      0.0,
      -1.0605123236170866,
      -1.0758707449961853,
      -1.0626774804454977,
      -1.0537893387947315,
      -1.079546601661224,
      -1.073032310374709,
      -1.077631046174572,
      -1.0363643347138125,
      -1.0769755951676874,
      -1.0576813327739627,
      -1.0742356857838635,
      -1.022302067909596,
      -1.0825044560655266,
      -1.0304108970534003,
      -1.0745838822399614,
      -1.0703788381801338,
      -1.0107897673040493
    ],
    [
      -1.3297906987370882,
      -1.1464930682545782,
      -1.1607195045299836,
      -1.1963183502262131,
      -1.2253102321200988,
      -1.161618606865634,
      -1.3490562052865547,
      -1.1934795857351157,
      -1.1442388359551277,
      -1.1906911716668662,
      -1.2270282465828537,
      -1.411136774041415,
      0.0,
      -1.1284318949934247,
      -1.260400680763674,
      -1.1922765050390096,
      -1.166620528224178,
      -1.2048128452918945,
      -1.2433202580424572,
      -1.25066245511626,
      -1.263017839715236,
      -1.4051816282254224,
      -1.2894532117757673,
      -1.3130298786365984,
      -1.317368819010881,
      -1.2054487266272977,
      -1.2801416693020413,
      -1.3214134028494868,
      -1.3801350826657623
    ],
    [
      -1.4604046852930535,
      -1.2861432191546396,
      -1.2119128495285405,
      -1.167668668620152,
      -1.3178212634990698,
      -1.2586823595340932,
      -1.4054482054348145,
      -1.2690243621717718,
      -1.198740998893932,
      -1.276296206931728,
      -1.3428536917047473,
      -1.5019102619685258,
      -1.1778019402832043,
      0.0,
      -1.3324891328497854,
      -1.2561520881203159,
      -1.3304372170403822,
      -1.2990950824686749,
      -1.4203271963471529,
      -1.3188139058108026,
      -1.358035632094752,
      -1.420751247696986,
      -1.402718267095286,
      -1.378743711760951,
      -1.4133444285733552,
      -1.3175926264510813,
      -1.3539386926345842,
      -1.4010230469959042,
      -1.441259661437963
    ],
    [
      -1.4190670749181442,
      -1.2367671245867093,
      -1.2858843959429391,
      -1.3107014927787999,
      -1.2927640580735813,
      -1.2492988326837469,
      -1.3556062865828977,
      -1.2384435546761288,
      -1.2492886337148128,
      -1.2189817092146351,
      -1.2660200942917852,
      -1.4977108231149092,
      -1.2925633464666548,
      -1.3077347388247063,
      0.0,
      -1.3050071426949978,
      -1.2185112543072234,
      -1.2585110278346117,
      -1.388638826598419,
      -1.218947039615096,
      -1.3182667934082544,
      -1.415498472510206,
      -1.3022737701200895,
      -1.2304615432185895,
      -1.3533686220524614,
      -1.2788557546741732,
      -1.3480786179188482,
      -1.3268163368333898,
      -1.3575817560788963
    ],
    [
      -1.4901769693439197,
      -1.2723332309058992,
      -1.260580693520605,
      -1.2761994267078136,
      -1.3575251927157959,
      -1.2389203315952781,
      -1.4126542033161502,
      -1.303468233886972,
      -1.3021049456278746,
      -1.3354926805170675,
      -1.3300185077062865,
      -1.557536118091579,
      -1.306848415044545,
      -1.2782344600969857,
      -1.4032227028032145,
      0.0,
      -1.3081475987461508,
      -1.3376766172141619,
      -1.3966415172293456,
      -1.3368914428483383,
      -1.3698136620493295,
      -1.4743126775429707,
      -1.437484639182015,
      -1.3717855825488645,
      -1.3898071035737962,
      -1.2947412931382247,
      -1.3459049414768665,
      -1.42732255198338,
      -1.4663704579594854
    ],
    [
      -1.7476367760740827,
      -1.5630550120336655,
      -1.6212298098226663,
      -1.6612174628326366,
      -1.6675127243475287,
      -1.6231900956259557,
      -1.780730938895414,
      -1.6015962680829485,
      -1.5457842608473364,
      -1.5705058918484376,
      -1.6143141462814168,
      -1.8769054514356243,
      -1.6040681376930213,
      -1.6165498331908448,
      -1.5704525862752046,
      -1.6478884610981837,
      0.0,
      -1.6508678896243991,
      -1.8076983669234323,
      -1.6052115688465232,
      -1.6582317229096881,
      -1.8231753217968532,
      -1.6410897841927834,
      -1.6606865421657033,
      -1.595580049845878,
      -1.6072417398908883,
      -1.6391429049013306,
      -1.6562662669023742,
      -1.812529379107461
    ],
    [
      -1.5457801695498556,
      -1.4125456578797366,
      -1.441075176668234,
      -1.4443246983601896,
      -1.3606952805541674,
      -1.3593200386186075,
      -1.5235134247009126,
      -1.2920446022635304,
      -1.3791099669606786,
      -1.4700966442325105,
      -1.4431691376356768,
      -1.644881245120944,
      -1.3946853861226824,
      -1.397270084428808,
      -1.416403080275462,
      -1.47802675967718,
      -1.4239749340715733,
      0.0,
      -1.541318634747752,
      -1.4479585929947991,
      -1.4625974882288255,
      -1.5837051256838561,
      -1.4701399235048713,
      -1.4871643323340198,
      -1.503299970191223,
      -1.4860618256860014,
      -1.4600708282976593,
      -1.4694884547571707,
      -1.5433417653751034
    ],
    [
      -1.4351123999026234,
      -1.3248805643962875,
      -1.3209923544140814,
      -1.3800040302278942,
      -1.3544415672141172,
      -1.3392630607479887,
      -1.3709227136995912,
      -1.3127440218941198,
      -1.316668123696784,
      -1.2890019832195798,
      -1.3387211011073152,
      -1.4470067518508725,
      -1.3070863805546187,
      -1.335696817834609,
      -1.3610105338775544,
      -1.333053496314376,
      -1.3442747508725812,
      -1.3610258578445225,
      0.0,
      -1.309195109130326,
      -1.3814915169332977,
      -1.4263993016949859,
      -1.3707331985668514,
      -1.3488964694429295,
      -1.4316386875952898,
      -1.3008368030999904,
      -1.3643302899560357,
      -1.4194282039208437,
      -1.3869857792178926
    ],
    [
      -1.4032896630133107,
      -1.1236017892782058,
      -1.220507387882407,
      -1.1811420331326208,
      -1.262485151874618,
      -1.2368550545763282,
      -1.3137471401382284,
      -1.225095271767709,
      -1.1460737468876245,
      -1.177793422634994,
      -1.2414722971650376,
      -1.466714441730939,
      -1.2795267462435174,
      -1.2901835182375692,
      -1.2184690349225837,
      -1.2386078595698258,
      -1.220661368208465,
      -1.2588709223446266,
      -1.3274394576218247,
      0.0,
      -1.3184186921851753,
      -1.4582476392584356,
      -1.3287036659570588,
      -1.2069278244604549,
      -1.3024150237266676,
      -1.242775313636683,
      -1.2742913397104207,
      -1.316286426826852,
      -1.3848904152290549
    ],
    [
      -1.5710311634780139,
      -1.5172839637459394,
      -1.518886748707932,
      -1.53460535681328,
      -1.498804559973214,
      -1.4502524763387603,
      -1.6056994283290673,
      -1.4948606925794437,
      -1.4520878052289024,
      -1.5112116391797479,
      -1.5080162896682834,
      -1.7207767592154934,
      -1.499162409196022,
      -1.5091059391759496,
      -1.474401582950018,
      -1.515063681262593,
      -1.4225818546713795,
      -1.4988750029909612,
      -1.6433266754982119,
      -1.509117028076853,
      0.0,
      -1.6607280155162134,
      -1.5444588365663956,
      -1.502877676608785,
      -1.524054454382909,
      -1.531463706727459,
      -1.5498569050579456,
      -1.4946396026246056,
      -1.662297563165761
    ],
    [
      -1.2502408241149885,
      -1.233667344931923,
      -1.1429944990890586,
      -1.1627520458260803,
      -1.2063719692013668,
      -1.172291870845406,
      -1.2507783339174192,
      -1.178780210931127,
      -1.1666657978396637,
      -1.1822211436422592,
      -1.1654595499420977,
      -1.2708935986930736,
      -1.1827953945862104,
      -1.1748953109641016,
      -1.1458460524479026,
      -1.1601295570708885,
      -1.1902251417854988,
      -1.1690146340336576,
      -1.259384683213217,
      -1.1998029484173327,
      -1.2072852425738996,
      0.0,
      -1.200787389688337,
      -1.2100414699703257,
      -1.2083223022070728,
      -1.2123571303009892,
      -1.1666352621724374,
      -1.1623590750758872,
      -1.2003615314079465
    ],
    [
      -1.5398084962517549,
      -1.4233666191342134,
      -1.507539322077282,
      -1.5330477592204175,
      -1.4764723201085719,
      -1.455941418746922,
      -1.6093902147241392,
      -1.4133926482990662,
      -1.3955572086326558,
      -1.4181888545407209,
      -1.489012563602819,
      -1.6446653486526268,
      -1.4447667549044965,
      -1.5030409836571987,
      -1.443324231858444,
      -1.5398431136765582,
      -1.4119137948175533,
      -1.4609470946805923,
      -1.5787224563536546,
      -1.426658801885897,
      -1.5019794967941824,
      -1.599398310473222,
      0.0,
      -1.47912226891313,
      -1.4194765424609996,
      -1.5542718631007288,
      -1.3920150267822011,
      -1.4678275643264225,
      -1.630599191341685
    ],
    [
      -1.4169427354561193,
      -1.2202475328822726,
      -1.325479396858035,
      -1.2852913925489844,
      -1.279351877952464,
      -1.3396437967466963,
      -1.3719266511048094,
      -1.2672131827808557,
      -1.251985472803878,
      -1.2473034739122209,
      -1.2594786501531192,
      -1.456584452571348,
      -1.3418107057191944,
      -1.3350207532226355,
      -1.2089143935865023,
      -1.343435300359892,
      -1.223638233659646,
      -1.316254065966058,
      -1.3674506417358312,
      -1.2004088230134249,
      -1.358840049354127,
      -1.4527142710181924,
      -1.309460275944584,
      0.0,
      -1.3053106699568766,
      -1.2589972339523121,
      -1.3613635898813639,
      -1.2839263750046321,
      -1.3897956484117522
    ],
    [
      -1.3761800573597776,
      -1.3246971467806534,
      -1.3230308271033147,
      -1.2847648428378413,
      -1.3253504240255038,
      -1.3142849960149046,
      -1.4345628537437196,
      -1.327534794788287,
      -1.2898939191158958,
      -1.328825773952313,
      -1.3286480799953275,
      -1.4979074317154002,
      -1.3636162298207626,
      -1.354947062649888,
      -1.3458772510337733,
      -1.3312438692081354,
      -1.2460566387915586,
      -1.3413800727386502,
      -1.4617007584368051,
      -1.2684485885978025,
      -1.3551245280910567,
      -1.4574841954063724,
      -1.3159104213910295,
      -1.3084903942357482,
      0.0,
      -1.3593987118541375,
      -1.276089078966385,
      -1.3078704589241266,
      -1.4610547063499844
    ],
    [
      -1.6579550238024572,
      -1.4906771928177325,
      -1.533150308027061,
      -1.5292767751573697,
      -1.5039522837981194,
      -1.5382298944598947,
      -1.6075607303969017,
      -1.549704588000785,
      -1.51426248942515,
      -1.5195559112871673,
      -1.5197333239087683,
      -1.72327518573513,
      -1.4976726658923762,
      -1.5514151448254827,
      -1.5083774330344144,
      -1.4941741470714993,
      -1.4947125416731286,
      -1.5260911889049662,
      -1.5731170552184404,
      -1.4398605773392554,
      -1.5912979408389007,
      -1.73386262786282,
      -1.6535094958540058,
      -1.5116565683329604,
      -1.6123683799627146,
      0.0,
      -1.624495650599782,
      -1.6581706858455365,
      -1.6574765600269756
    ],
    [
      -1.5826866830339132,
      -1.4655325075452796,
      -1.585168553914409,
      -1.5914335423236452,
      -1.5440026819562274,
      -1.4916018925170074,
      -1.6790835759916634,
      -1.4644071194136112,
      -1.495055680732445,
      -1.4290334750415516,
      -1.5571509909531032,
      -1.7208983296120255,
      -1.533965326634968,
      -1.600728518148616,
      -1.5814267560225066,
      -1.5657384178962566,
      -1.4658642548318401,
      -1.5390530570655057,
      -1.5889935458435147,
      -1.4771393923335316,
      -1.5799112284247863,
      -1.6844351830758284,
      -1.438288785258954,
      -1.5732672920648625,
      -1.4672467601378905,
      -1.578503112385267,
      0.0,
      -1.5792499367304986,
      -1.7180941096969202
    ],
    [
      -1.6734314283466263,
      -1.637797803586178,
      -1.6229711189846927,
      -1.6461857186985807,
      -1.6604616691533962,
      -1.6030545403484393,
      -1.6765251373680876,
      -1.487078442012816,
      -1.5454825181813054,
      -1.6047357060518619,
      -1.6243512588228537,
      -1.7726521852956134,
      -1.6809127699091222,
      -1.7005687302649417,
      -1.6217387162047037,
      -1.679989176232463,
      -1.5665799363078532,
      -1.6597895173054908,
      -1.7415015506780833,
      -1.5994698822189546,
      -1.6298686478002775,
      -1.7251032045830392,
      -1.6311836124603214,
      -1.5815139306931298,
      -1.5838607568286351,
      -1.7177687772985832,
      -1.6419683883065885,
      0.0,
      -1.7568283454278957
    ],
    [
      -1.3980327929152434,
      -1.2878707230399096,
      -1.3405468978772912,
      -1.3215280680372417,
      -1.29936238194132,
      -1.31444057680726,
      -1.2762642470208816,
      -1.3051756197848559,
      -1.3311761536645463,
      -1.3287698013052842,
      -1.347969341345133,
      -1.3743042076678627,
      -1.3514796724957712,
      -1.3319401011885368,
      -1.3028098110935205,
      -1.3296737876937665,
      -1.3387677199869066,
      -1.3356711310346965,
      -1.3689075715593244,
      -1.317646810105166,
      -1.3726347427630994,
      -1.3996843087027195,
      -1.429414618094866,
      -1.2905439566883503,
      -1.3841855425939176,
      -1.2985580289197405,
      -1.379122982331977,
      -1.3919626662638276,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2507659903351076,
      0.23079146524107497,
      0.22295779793840254,
      0.2412147522515129,
      0.24351458540232906,
      0.16955344836254893,
      0.21980019955665497,
      0.26615989767282233,
      0.23478918225698653,
      0.26309417153071823,
      0.1543650976331059,
      0.231793569079501,
      0.21432044335592515,
      0.2603459369919161,
      0.21401796349587743,
      0.24946905063786584,
      0.24967370218552798,
      0.18906492290143673,
      0.2586539331980866,
      0.22988656119060336,
      0.1993374465547899,
      0.20457632183034313,
      0.23229310653722424,
      0.23742674770951933,
      0.20490723399551447,
      0.2079026507552686,
      0.25218745412853316,
      0.16597058003484477
    ],
    [
      0.39079679583102056,
      0.0,
      0.5818907198947947,
      0.5109408754875835,
      0.5368479731741562,
      0.5787207356290993,
      0.41248370277718927,
      0.5690451927929534,
      0.6629567763790536,
      0.5522823508816663,
      0.5189218982723898,
      0.32382501215245596,
      0.5801554795763773,
      0.49037120022310043,
      0.5373767274539842,
      0.5556637986669726,
      0.5770309720187536,
      0.5258565576038674,
      0.4574761522813662,
      0.5991295536185033,
      0.43221163878916347,
      0.35258034442973973,
      0.5154958481327052,
      0.5440591889509978,
      0.49522927430068386,
      0.5299322216457918,
      0.5135069685758047,
      0.4622730895209042,
      0.3771052536456845
    ],
    [
      0.43467301911756717,
      0.615209161963405,
      0.0,
      0.626179264100889,
      0.5018350539613916,
      0.5522057725439644,
      0.37807228926203007,
      0.5235770236639552,
      0.5949169965253671,
      0.5564976059258266,
      0.47275042228355657,
      0.3241254090423267,
      0.6362945644407396,
      0.6041514833883013,
      0.46848750931250427,
      0.5234609833004977,
      0.518475271055707,
      0.47617038342097806,
      0.4517062458921932,
      0.5393125934772931,
      0.46481162110914354,
      0.38232781536522653,
      0.393433207269517,
      0.4718992039157399,
      0.4592704535394354,
      0.5173944847086913,
      0.4635111602446036,
      0.4694008773360643,
      0.3986890225134758
    ],
    [
      0.39585575284793095,
      0.6064078300711506,
      0.6421280401236473,
      0.0,
      0.5037906524767393,
      0.5917135602966022,
      0.4418844649510154,
      0.5747240693204319,
      0.6500301142987754,
      0.5899608958154785,
      0.5645182016989216,
      0.3427523625499016,
      0.6130485820391118,
      0.6319725777058909,
      0.5034287544405327,
      0.6013434218772715,
      0.5319097914758761,
      0.5418472370081506,
      0.42639812356689766,
      0.620789743735034,
      0.45989521739403316,
      0.3969514730682986,
      0.4390422781450032,
      0.4888382325585996,
      0.5100114650273633,
      0.5724092673645635,
      0.5067036251778054,
      0.48074162457072145,
      0.4131073120936992
    ],
    [
      0.381798909541885,
      0.5881935859133731,
      0.45811969078986503,
      0.4331657062868113,
      0.0,
      0.5756403896432143,
      0.41906647243754525,
      0.5190559792340841,
      0.49290710186767894,
      0.48806557661626226,
      0.4006509302265966,
      0.3295769830617563,
      0.48065342031556635,
      0.4585532869902109,
      0.4516628940473266,
      0.4384905913585011,
      0.49696061923979307,
      0.5394762657815046,
      0.38888703668525104,
      0.499480489759365,
      0.44049508603336274,
      0.29254371170022453,
      0.4345777039939087,
      0.500704953040835,
      0.4559110869341132,
      0.47463833956878587,
      0.4869296531614471,
      0.4324263474603518,
      0.39496331037931
    ],
    [
      0.4050931089464178,
      0.5202009721487104,
      0.4895647352129955,
      0.46963090022527454,
      0.5027313563406104,
      0.0,
      0.37703816976073146,
      0.47966663977297297,
      0.5228749283359708,
      0.5544012418972324,
      0.5134012073452527,
      0.2677800482607158,
      0.48823375822136206,
      0.4615388009909229,
      0.44904773836445244,
      0.4628471416057758,
      0.4899964230496896,
      0.49193933480030094,
      0.3781504129211839,
      0.46973203442620015,
      0.49760806576477923,
      0.31627670790824736,
      0.41624456515154695,
      0.42935545856422475,
      0.4280858712689384,
      0.46354765125859876,
      0.4600502921127987,
      0.46442108448752273,
      0.34866464430259736
    ],
    [
      0.2653366713442471,
      0.44012312587861313,
      0.39894803584945415,
      0.4155454471575888,
      0.4455525481871139,
      0.4138013525539457,
      0.0,
      0.39926345923694595,
      0.40320763177174945,
      0.38589416679032107,
      0.3455198396469721,
      0.31957249892111794,
      0.3763593124122735,
      0.39554014417768557,
      0.38903422651371344,
      0.37130496128060253,
      0.3777622868499677,
      0.39835366933640337,
      0.360400476499082,
      0.4288483581857374,
      0.3658179025785162,
      0.31857142613822975,
      0.30625386293644374,
      0.4348057414701645,
      0.3195595288215707,
      0.43481311144418555,
      0.351268625088504,
      0.36353427974011265,
      0.4140715284583949
    ],
    [
      0.3427695495379415,
      0.5087044673524566,
      0.47230517359941193,
      0.46795217357021834,
      0.4752673225424273,
      0.5075557382136517,
      0.3593734806895865,
      0.0,
      0.5435401388451031,
      0.5049435248533762,
      0.46340931415746467,
      0.2747412314766422,
      0.48585750733540767,
      0.4938311906360431,
      0.5016692253079003,
      0.4395812817405249,
      0.4688752473735145,
      0.5287206049884563,
      0.40132120828135887,
      0.4841312034768588,
      0.4393743888947992,
      0.3120097586761481,
      0.44683984893413986,
      0.4535062439898241,
      0.4151178406554634,
      0.4018568338762667,
      0.47141926005085755,
      0.5277681801471716,
      0.3145017550628275
    ],
    [
      0.4458020021065545,
      0.6951301506366823,
      0.6270352935899313,
      0.6151170308247997,
      0.5242559218254828,
      0.6138108681730867,
      0.42904478969384274,
      0.6225986694757435,
      0.0,
      0.6282706500345563,
      0.6172036700664199,
      0.33680857333863257,
      0.6274810594427522,
      0.6530737180039117,
      0.6016868970487383,
      0.5632488033712921,
      0.6195213674537638,
      0.592430413426746,
      0.46344659169662705,
      0.663607757311214,
      0.5197255815813027,
      0.32646967724087506,
      0.5090556947804936,
      0.5456078134503322,
      0.5242037406080258,
      0.5070393149692194,
      0.4849216971963437,
      0.5625452763443644,
      0.3779916058379986
    ],
    [
      0.34604274847845873,
      0.5746171111987899,
      0.5090802017240217,
      0.5014607870121606,
      0.47405689412944985,
      0.5318209658373898,
      0.40721745598467884,
      0.5080715572558769,
      0.584734035028639,
      0.0,
      0.47933790993069825,
      0.26275412737103365,
      0.5506812313131082,
      0.4663993237311681,
      0.507634738860288,
      0.47506643912139324,
      0.5714035643692883,
      0.472074712022134,
      0.4145609186468424,
      0.5870961233949272,
      0.4415521965280613,
      0.3169237736080315,
      0.4671943982752693,
      0.5139796268676964,
      0.4273822341688829,
      0.5190601416400134,
      0.5168270540694122,
      0.45391961922415214,
      0.3225498204523234
    ],
    [
      0.4242367129385036,
      0.592244167076573,
      0.5772691415963238,
      0.5482772327449099,
      0.4786060460817121,
      0.6103218339767498,
      0.4158617805821858,
      0.5666018427785156,
      0.6506201852766715,
      0.6023920079815779,
      0.0,
      0.31831598648314463,
      0.5799414147607678,
      0.5426408552775113,
      0.5877262545028585,
      0.5697821742108311,
      0.6064239098164463,
      0.5849603252512117,
      0.46285997089981623,
      0.5580312062726549,
      0.5021148671802509,
      0.36466000854042524,
      0.5019073176706927,
      0.5919811153432455,
      0.48173620296919784,
      0.5425751766459401,
      0.4748818096526226,
      0.4985899815790127,
      0.4040740979770703
    ],
    [
      0.2562367793950038,
      0.32908228027689046,
      0.3126639739316408,
      0.31436376326902504,
      0.3274141817586538,
      0.28786887054028365,
      0.33368929110718404,
      0.2878878318268663,
      0.2869613242130018,
      0.28049711988997017,
      0.32269129782741945,
      0.0,
      0.3082405832977446,
      0.2928821619186459,
      0.3060754264693335,
      0.31496356812009974,
      0.28920630525360713,
      0.29572059654012217,
      0.29112186074025925,
      0.3323885722010187,
      0.29177731174714383,
      0.3110715741408685,
      0.2945172211309677,
      0.3464508390052352,
      0.2862484508493046,
      0.3383420098614309,
      0.29416902467486983,
      0.2983740687346974,
      0.35796313961078186
    ],
    [
      0.3605483775583236,
      0.5438460080408336,
      0.5296195717654282,
      0.49402072606919867,
      0.465028844175313,
      0.5287204694297778,
      0.3412828710088571,
      0.4968594905602961,
      0.5461002403402841,
      0.4996479046285456,
      0.4633108297125581,
      0.2792023022539969,
      0.0,
      0.5619071813019871,
      0.42993839553173774,
      0.4980625712564022,
      0.5237185480712339,
      0.48552623100351733,
      0.44701881825295464,
      0.43967662117915185,
      0.42732123658017573,
      0.28515744806998944,
      0.4008858645196445,
      0.37730919765881343,
      0.3729702572845308,
      0.48489034966811406,
      0.4101974069933705,
      0.368925673445925,
      0.31020399362964945
    ],
    [
      0.3520468562614376,
      0.5263083223998515,
      0.6005386920259506,
      0.644782872934339,
      0.49463027805542126,
      0.5537691820203978,
      0.40700333611967654,
      0.5434271793827192,
      0.6137105426605591,
      0.536155334622763,
      0.46959784984974373,
      0.3105412795859652,
      0.6346496012712868,
      0.0,
      0.47996240870470563,
      0.5562994534341752,
      0.48201432451410886,
      0.5133564590858162,
      0.3921243452073382,
      0.49363763574368846,
      0.4544159094597391,
      0.39170029385750516,
      0.40973327445920504,
      0.43370782979354017,
      0.3991071129811359,
      0.49485891510340974,
      0.45851284891990685,
      0.4114284945585869,
      0.37119188011652815
    ],
    [
      0.34511962792012896,
      0.5274195782515638,
      0.47830230689533404,
      0.4534852100594733,
      0.47142264476469187,
      0.5148878701545263,
      0.40858041625537544,
      0.5257431481621444,
      0.5148980691234604,
      0.5452049936236381,
      0.498166608546488,
      0.26647587972336395,
      0.47162335637161834,
      0.4564519640135669,
      0.0,
      0.4591795601432753,
      0.5456754485310498,
      0.5056756750036615,
      0.3755478762398541,
      0.5452396632231771,
      0.4459199094300188,
      0.3486882303280672,
      0.4619129327181837,
      0.5337251596196837,
      0.4108180807858117,
      0.48533094816409994,
      0.41610808491942497,
      0.4373703660048833,
      0.40660494675937686
    ],
    [
      0.3881726395911371,
      0.6060163780291576,
      0.6177689154144519,
      0.6021501822272433,
      0.520824416219261,
      0.6394292773397787,
      0.46569540561890665,
      0.5748813750480848,
      0.5762446633071823,
      0.5428569284179894,
      0.5483311012287704,
      0.3208134908434779,
      0.5715011938905119,
      0.6001151488380712,
      0.4751269061318424,
      0.0,
      0.570202010188906,
      0.540672991720895,
      0.48170809170571127,
      0.5414581660867186,
      0.5085359468857273,
      0.4040369313920862,
      0.4408649697530418,
      0.5065640263861924,
      0.48854250536126065,
      0.5836083157968321,
      0.5324446674581904,
      0.4510270569516768,
      0.4119791509755715
    ],
    [
      0.4062507681722769,
      0.5908325322126942,
      0.5326577344236934,
      0.49267008141372304,
      0.4863748198988309,
      0.5306974486204039,
      0.37315660535094564,
      0.5522912761634111,
      0.6081032833990232,
      0.5833816523979221,
      0.5395733979649429,
      0.2769820928107354,
      0.5498194065533384,
      0.5373377110555149,
      0.5834349579711551,
      0.505999083148176,
      0.0,
      0.5030196546219605,
      0.3461891773229273,
      0.5486759753998365,
      0.4956558213366715,
      0.3307122224495065,
      0.5127977600535762,
      0.4932010020806563,
      0.5583074944004816,
      0.5466458043554714,
      0.514744639345029,
      0.4976212773439854,
      0.3413581651388986
    ],
    [
      0.41962224157264894,
      0.552856753242768,
      0.5243272344542707,
      0.521077712762315,
      0.6047071305683371,
      0.6060823725038971,
      0.441888986421592,
      0.6733578088589742,
      0.586292444161826,
      0.49530576688999406,
      0.5222332734868278,
      0.32052116600156055,
      0.5707170249998221,
      0.5681323266936966,
      0.5489993308470427,
      0.4873756514453247,
      0.5414274770509313,
      0.0,
      0.4240837763747525,
      0.5174438181277055,
      0.5028049228936791,
      0.38169728543864845,
      0.49526248761763325,
      0.4782380787884848,
      0.4621024409312815,
      0.47934058543650315,
      0.5053315828248452,
      0.49591395636533386,
      0.4220606457474012
    ],
    [
      0.25637002207085646,
      0.3666018575771923,
      0.3704900675593985,
      0.31147839174558567,
      0.33704085475936263,
      0.35221936122549113,
      0.3205597082738887,
      0.3787384000793601,
      0.37481429827669577,
      0.40248043875390005,
      0.35276132086616463,
      0.2444756701226074,
      0.3843960414188612,
      0.35578560413887095,
      0.3304718880959254,
      0.3584289256591038,
      0.3472076711008987,
      0.3304565641289574,
      0.0,
      0.38228731284315387,
      0.3099909050401821,
      0.265083120278494,
      0.3207492234066285,
      0.34258595253055035,
      0.2598437343781901,
      0.3906456188734895,
      0.3271521320174442,
      0.2720542180526362,
      0.3044966427555873
    ],
    [
      0.3314623013383071,
      0.6111501750734121,
      0.5142445764692107,
      0.553609931218997,
      0.47226681247699975,
      0.49789690977528966,
      0.4210048242133895,
      0.5096566925839088,
      0.5886782174639933,
      0.5569585417166238,
      0.49327966718658023,
      0.2680375226206788,
      0.45522521810810046,
      0.44456844611404867,
      0.5162829294290341,
      0.49614410478179205,
      0.5140905961431528,
      0.4758810420069912,
      0.40731250672979313,
      0.0,
      0.41633327216644256,
      0.27650432509318223,
      0.406048298394559,
      0.527824139891163,
      0.4323369406249502,
      0.4919766507149348,
      0.4604606246411971,
      0.4184655375247659,
      0.34986154912256295
    ],
    [
      0.4403131059505232,
      0.4940603056825976,
      0.492457520720605,
      0.476738912615257,
      0.512539709455323,
      0.5610917930897767,
      0.40564484109946974,
      0.5164835768490934,
      0.5592564641996347,
      0.5001326302487892,
      0.5033279797602537,
      0.2905675102130436,
      0.512181860232515,
      0.5022383302525875,
      0.536942686478519,
      0.496280588165944,
      0.5887624147571575,
      0.5124692664375758,
      0.3680175939303252,
      0.502227241351684,
      0.0,
      0.35061625391232365,
      0.46688543286214146,
      0.508466592819752,
      0.48728981504562796,
      0.47988056270107804,
      0.46148736437059146,
      0.5167046668039315,
      0.349046706262776
    ],
    [
      0.32136970018136646,
      0.33794317936443186,
      0.42861602520729636,
      0.4088584784702747,
      0.36523855509498815,
      0.39931865345094897,
      0.32083219037893573,
      0.392830313365228,
      0.4049447264566912,
      0.3893893806540958,
      0.4061509743542573,
      0.30071692560328134,
      0.38881512971014454,
      0.39671521333225335,
      0.42576447184845234,
      0.41148096722546645,
      0.3813853825108562,
      0.4025958902626974,
      0.312225841083138,
      0.37180757587902225,
      0.3643252817224554,
      0.0,
      0.37082313460801797,
      0.36156905432602926,
      0.36328822208928213,
      0.3592533939953657,
      0.40497526212391755,
      0.40925144922046774,
      0.3712489928884084
    ],
    [
      0.390537207324152,
      0.5069790844416935,
      0.42280638149862493,
      0.3972979443554894,
      0.45387338346733497,
      0.4744042848289849,
      0.3209554888517676,
      0.5169530552768407,
      0.5347884949432511,
      0.512156849035186,
      0.44133313997308776,
      0.28568035492328003,
      0.4855789486714104,
      0.4273047199187081,
      0.4870214717174628,
      0.3905025898993486,
      0.5184319087583535,
      0.4693986088953146,
      0.35162324722225224,
      0.5036869016900098,
      0.42836620678172443,
      0.33094739310268495,
      0.0,
      0.4512234346627768,
      0.5108691611149072,
      0.376073840475178,
      0.5383306767937057,
      0.4625181392494844,
      0.2997465122342218
    ],
    [
      0.3601058860059161,
      0.5568010885797627,
      0.4515692246040004,
      0.49175722891305096,
      0.4976967435095714,
      0.43740482471533904,
      0.40512197035722597,
      0.5098354386811796,
      0.5250631486581574,
      0.5297451475498145,
      0.5175699713089161,
      0.3204641688906873,
      0.435237915742841,
      0.4420278682393999,
      0.568134227875533,
      0.43361332110214335,
      0.5534103878023893,
      0.46079455549597736,
      0.40959797972620415,
      0.5766397984486105,
      0.4182085721079083,
      0.32433435044384296,
      0.46758834551745143,
      0.0,
      0.47173795150515874,
      0.5180513875097232,
      0.4156850315806715,
      0.4931222464574032,
      0.38725297305028317
    ],
    [
      0.39050225614777,
      0.44198516672689414,
      0.4436514864042329,
      0.48191747066970625,
      0.4413318894820437,
      0.452397317492643,
      0.33211945976382795,
      0.4391475187192606,
      0.47678839439165177,
      0.4378565395552345,
      0.43803423351222004,
      0.26877488179214737,
      0.403066083686785,
      0.4117352508576595,
      0.42080506247377425,
      0.43543844429941214,
      0.520625674715989,
      0.42530224076889733,
      0.3049815550707424,
      0.49823372490974505,
      0.4115577854164909,
      0.30919811810117515,
      0.450771892116518,
      0.45819191927179936,
      0.0,
      0.40728360165341004,
      0.49059323454116255,
      0.458811854583421,
      0.3056276071575632
    ],
    [
      0.39947088942589515,
      0.5667487204106199,
      0.5242756052012914,
      0.5281491380709826,
      0.5534736294302329,
      0.5191960187684577,
      0.4498651828314506,
      0.5077213252275674,
      0.5431634238032024,
      0.5378700019411851,
      0.5376925893195841,
      0.3341507274932223,
      0.5597532473359761,
      0.5060107684028696,
      0.5490484801939379,
      0.563251766156853,
      0.5627133715552237,
      0.5313347243233861,
      0.48430885800991197,
      0.6175653358890969,
      0.4661279723894516,
      0.32356328536553236,
      0.4039164173743466,
      0.545769344895392,
      0.4450575332656377,
      0.0,
      0.43293026262857026,
      0.3992552273828158,
      0.3999493532013767
    ],
    [
      0.494023454168397,
      0.6111776296570306,
      0.4915415832879013,
      0.48527659487866504,
      0.5327074552460829,
      0.5851082446853029,
      0.3976265612106469,
      0.612303017788699,
      0.5816544564698651,
      0.6476766621607586,
      0.5195591462492071,
      0.35581180759028475,
      0.5427448105673423,
      0.4759816190536943,
      0.49528338117980364,
      0.5109717193060537,
      0.6108458823704701,
      0.5376570801368046,
      0.4877165913587955,
      0.5995707448687786,
      0.496798908777524,
      0.39227495412648183,
      0.6384213519433561,
      0.5034428451374477,
      0.6094633770644198,
      0.49820702481704315,
      0.0,
      0.4974602004718116,
      0.35861602750539
    ],
    [
      0.30745031299584014,
      0.3430839377562884,
      0.3579106223577737,
      0.33469602264388576,
      0.3204200721890702,
      0.3778272009940271,
      0.3043566039743788,
      0.4938032993296504,
      0.435399223161161,
      0.37614603529060453,
      0.3565304825196127,
      0.208229556046853,
      0.2999689714333442,
      0.2803130110775247,
      0.35914302513776275,
      0.3008925651100034,
      0.4143018050346132,
      0.32109222403697557,
      0.2393801906643831,
      0.38141185912351183,
      0.3510130935421889,
      0.2557785367594272,
      0.349698128882145,
      0.3993678106493366,
      0.3970209845138313,
      0.2631129640438832,
      0.3389133530358779,
      0.0,
      0.22405339591457074
    ],
    [
      0.31310835660511516,
      0.423270426480449,
      0.37059425164306736,
      0.3896130814831169,
      0.41177876757903853,
      0.39670057271309855,
      0.43487690249947697,
      0.4059655297355027,
      0.3799649958558122,
      0.38237134821507435,
      0.36317180817522554,
      0.3368369418524959,
      0.3596614770245874,
      0.3792010483318218,
      0.4083313384268381,
      0.381467361826592,
      0.372373429533452,
      0.3754700184856621,
      0.34223357796103415,
      0.39349433941519263,
      0.3385064067572592,
      0.311456840817639,
      0.28172653142549264,
      0.42059719283200825,
      0.326955606926441,
      0.4125831206006181,
      0.3320181671883815,
      0.319178483256531,
      0.0
    ]
  ],
  "row_avgs": [
    0.22495836474157294,
    0.506577367953813,
    0.49352996066715693,
    0.5229358811321231,
    0.4554855757881761,
    0.4506472604802153,
    0.3799665792596306,
    0.44667656050949445,
    0.546326236768919,
    0.47155356108014956,
    0.5228440580738365,
    0.30674533672614535,
    0.44185347964252175,
    0.4796147326117679,
    0.4591278055619979,
    0.5182704590288814,
    0.49423185162163524,
    0.5053287243752892,
    0.3339166409296241,
    0.4609843697722879,
    0.47828970450960345,
    0.3775619416217061,
    0.4389067653609727,
    0.46352041979925585,
    0.41988323801007776,
    0.49258332858193105,
    0.5203543975742164,
    0.3354041174363759,
    0.3701252829873937
  ],
  "col_avgs": [
    0.37018271619198656,
    0.5117057138135355,
    0.48039886683877475,
    0.47118467711246304,
    0.4626045967536126,
    0.4976473740935164,
    0.3819234535656553,
    0.49715324681167583,
    0.5181705077459744,
    0.4947618028087633,
    0.46186154417860176,
    0.2943892717378041,
    0.48513145604475705,
    0.46611076421505687,
    0.47067383183418127,
    0.4576842786110609,
    0.4944364693297521,
    0.4674259653135891,
    0.3878379981383011,
    0.4983663672584277,
    0.4257554496456714,
    0.3275526181038461,
    0.4216865826393919,
    0.46054518232277664,
    0.42985336125448026,
    0.45636638824600556,
    0.4382848985765223,
    0.4348318118195453,
    0.35367680760104186
  ],
  "combined_avgs": [
    0.2975705404667798,
    0.5091415408836742,
    0.48696441375296584,
    0.49706027912229306,
    0.45904508627089435,
    0.4741473172868659,
    0.380945016412643,
    0.47191490366058514,
    0.5322483722574467,
    0.4831576819444564,
    0.49235280112621915,
    0.30056730423197475,
    0.4634924678436394,
    0.47286274841341236,
    0.4649008186980896,
    0.4879773688199711,
    0.4943341604756937,
    0.4863773448444392,
    0.3608773195339626,
    0.4796753685153578,
    0.45202257707763743,
    0.3525572798627761,
    0.4302966740001823,
    0.46203280106101624,
    0.424868299632279,
    0.4744748584139683,
    0.47931964807536936,
    0.38511796462796055,
    0.3619010452942178
  ],
  "gppm": [
    606.6925296328498,
    578.6182608874482,
    593.762387519313,
    601.6603584192865,
    600.2033051680849,
    585.3700819577849,
    637.956342595149,
    585.0874500881176,
    580.5339995499282,
    584.4920887904276,
    606.371424036274,
    679.3898778749556,
    593.1311632975014,
    600.5519761891037,
    599.3339511363926,
    604.2655824878146,
    584.428114595957,
    599.7920444543396,
    629.955483822359,
    586.7796237866393,
    616.5758991222492,
    663.3075970206158,
    618.8564202669492,
    603.9748952490298,
    617.8059490568781,
    603.5596741911161,
    610.803023178336,
    608.1934477534633,
    649.8472679254779
  ],
  "gppm_normalized": [
    1.4004587858525177,
    1.2726747847790363,
    1.3051531125875966,
    1.32158744485711,
    1.3188965713315297,
    1.282652083301736,
    1.4046869705027092,
    1.2843497694902626,
    1.2724558820879055,
    1.2822299566073627,
    1.331604569187834,
    1.4983422616357431,
    1.3062604108646685,
    1.3221129959196765,
    1.3138359555925514,
    1.3309856893252345,
    1.2819953128995072,
    1.3163306440066054,
    1.3866996348171683,
    1.2811155239997625,
    1.3509454910612206,
    1.4580095666201829,
    1.3595107590603832,
    1.3182572173185727,
    1.3550026180088344,
    1.3279687502770798,
    1.339689950046504,
    1.3441028881358192,
    1.4243073117800353
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394,
    657,
    523,
    436,
    419,
    474,
    390,
    480,
    426,
    428,
    404,
    410,
    529,
    440,
    402,
    391,
    498,
    406,
    448,
    479,
    502,
    416,
    388,
    440,
    448,
    427,
    397,
    451,
    445,
    376,
    414,
    412,
    416,
    442,
    410,
    414,
    440,
    404,
    432,
    343,
    419,
    544,
    416,
    447,
    415,
    396,
    390,
    354,
    411,
    419,
    383,
    374,
    379,
    418,
    348,
    421,
    448,
    422,
    374,
    695,
    444,
    457,
    470,
    451,
    421,
    487,
    408,
    443,
    385,
    446,
    534,
    403,
    397,
    389,
    359,
    422,
    369,
    489,
    402,
    372,
    387,
    401,
    466,
    437,
    374,
    409,
    411,
    327,
    755,
    383,
    457,
    399,
    413,
    459,
    551,
    467,
    449,
    422,
    434,
    467,
    444,
    444,
    376,
    444,
    417,
    432,
    477,
    391,
    419,
    369,
    378,
    450,
    409,
    480,
    439,
    393,
    384,
    583,
    456,
    425,
    387,
    417,
    449,
    425,
    437,
    393,
    420,
    374,
    404,
    454,
    426,
    423,
    398,
    409,
    381,
    520,
    456,
    379,
    385,
    394,
    407,
    396,
    372,
    374,
    470,
    402
  ],
  "response_lengths": [
    2814,
    2578,
    2507,
    2241,
    2388,
    2722,
    2383,
    2357,
    2181,
    2408,
    2113,
    2356,
    2633,
    2477,
    2427,
    2360,
    2235,
    2251,
    3080,
    2550,
    2173,
    2173,
    2268,
    2170,
    2107,
    2033,
    2243,
    2585,
    2181
  ]
}