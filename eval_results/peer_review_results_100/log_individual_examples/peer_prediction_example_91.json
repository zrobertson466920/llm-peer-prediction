{
  "example_idx": 91,
  "reference": "Under review as a conference paper at ICLR 2023\n\nREMOVING STRUCTURED NOISE WITH DIFFUSION MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSolving ill-posed inverse problems requires careful formulation of prior beliefs over the signals of interest and an accurate description of their manifestation into noisy measurements. Handcrafted signal priors based on e.g. sparsity are increasingly replaced by data-driven deep generative models, and several groups have recently shown that state-of-the-art score-based diffusion models yield particularly strong performance and flexibility. In this paper, we show that the powerful paradigm of posterior sampling with diffusion models can be extended to include rich, structured, noise models. To that end, we propose a joint conditional reverse diffusion process with learned scores for the noise and signal-generating distribution. We demonstrate strong performance gains across various inverse problems with structured noise, outperforming competitive baselines that use normalizing flows and adversarial networks. This opens up new opportunities and relevant practical applications of diffusion modeling for inverse problems in the context of non-Gaussian measurements.\n\n1\n\nINTRODUCTION\n\nMany signal and image processing problems, such as denoising, compressed sensing, or phase retrieval, can be formulated as inverse problems that aim to recover unknown signals from (noisy) observations. These ill-posed problems are, by definition, subject to many solutions under the given measurement model. Therefore, prior knowledge is required for a meaningful and physically plausible recovery of the original signal. Bayesian inference and maximum a posteriori (MAP) solutions incorporate both signal priors and observation likelihood models. Choosing an appropriate statistical prior is not trivial and is often dependent on both the application as well as the recovery task.\n\nBefore deep learning, sparsity in some transform domain has been the go-to prior in compressed sensing (CS) methods (Eldar & Kutyniok, 2012), such as iterative thresholding (Beck & Teboulle, 2009) or wavelet decomposition (Mallat, 1999). At present, deep generative modeling has established itself as a strong mechanism for learning such priors for inverse problem-solving. Both generative adversarial networks (GANs) (Bora et al., 2017) and normalizing flows (NFs) (Asim et al., 2020; Wei et al., 2022) have been applied as natural signal priors for inverse problems in image recovery. These data-driven methods are more powerful compared to classical methods, as they can accurately learn the natural signal manifold and do not rely on assumptions such as signal sparsity or hand-crafted basis functions. Recently, diffusion models have shown impressive results for both conditional and unconditional image generation and can be easily fitted to a target data distribution using score matching (Vincent, 2011; Song et al., 2020). These deep generative models learn the score of the data manifold and produce samples by reverting a diffusion process, guiding noise samples towards the target distribution. Diffusion models have achieved state-of-the-art performance in many downstream tasks and applications, ranging from state-of-the-art text-to-image models such as DALL-E 2 (Ramesh et al., 2022) to medical imaging (Song et al., 2021b; Jalal et al., 2021a; Chung & Ye, 2022). Furthermore, understanding of diffusion models is rapidly improving and progress in the field is extremely fast-paced (Chung et al., 2022a; Bansal et al., 2022; Daras et al., 2022a; Karras et al., 2022; Luo, 2022). The iterative nature of the sampling procedure used by diffusion models renders inference slow compared to GANs and VAEs. However, many recent efforts have shown ways to significantly improve the sampling speed by accelerating the diffusion process. Inspired by momentum methods in sampling, Daras et al. (2022b) introduces a momentum sampler for diffusion models, which leads to increased sample quality with fewer function evaluations. Chung\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\net al. (2022b) offers a new sampling strategy, namely Come-Closer-Diffuse-Faster (CCDF), which leverages the conditional quality of inverse problems. The reverse diffusion can be initialized from the observation instead of a sample from the base distribution, which leads to faster convergence for conditional sampling. Salimans & Ho (2021) proposes a progressive distillation method that augments the training of the diffusion models with a student-teacher model setup. In doing this, they were able to drastically reduce the number of sampling steps. Lastly, many methods aim to execute the diffusion process in a reduced space to accelerate the diffusion process. While Jing et al. (2022) restricts diffusion through projections onto subspaces, Vahdat et al. (2021) and Rombach et al. (2022) run the diffusion in the latent space.\n\nDespite this promise, current score-based diffusion methods for inverse problems are limited to measurement models with unstructured noise. In many image processing tasks, corruptions are however highly structured and spatially correlated. Relevant examples include interference, speckle, or haze. Nevertheless, current conditional diffusion models naively assume that the noise follows some basic tractable distribution (e.g. Gaussian or Poisson). Beyond the realm of diffusion models, Whang et al. (2021) extended normalizing flow (NF)-based inference to structured noise applications. However, compared to diffusion models, NFs require specialized network architectures, which are computationally and memory expensive.\n\nGiven the promising outlook of diffusion models, we propose to learn score models for both the noise and the desired signal and perform joint inference of both quantities, coupled via the observation model. The resulting sampling scheme enables solving a wide variety of inverse problems with structured noise.\n\nThe main contributions of this work are as follows:\n\n• We propose a novel joint conditional posterior sampling method to efficiently remove structured noise using diffusion models. Our formulation is compatible with many existing iterative sampling methods for score-based generative models.\n\n• We show strong performance gains across various challenging inverse problems involving structured noise compared to competitive state-of-the-art methods based on NFs and GANs.\n\n• We demonstrate improved robustness on out-of-distribution signals compared to baselines.\n\n2 PROBLEM STATEMENT\n\nMany image reconstruction tasks can be formulated as an inverse problem with the basic form\n\ny = Ax + n,\n\n(1)\n\nwhere y ∈ Rm is the noisy observation, x ∈ Rd the desired signal or image, and n ∈ Rm the additive noise. The linear forward operator A ∈ Rm×d captures the deterministic transformation of x. Maximum a posteriori (MAP) inference is typically used to find an optimal solution ˆxMAP that maximizes posterior density pX|Y (x|y):\n\nˆxMAP = arg max\n\nx\n\nlog pX|Y (x|y)\n\n= arg max\n\nx\n\n(cid:2)log pY |X (y|x) + log pX (x)(cid:3),\n\n(2)\n\n(3)\n\nwhere pY |X (y|x) is the likelihood according to the measurement model and log pX (x) the signal prior.\n\nAssumptions on the stochastic corruption process n are of key importance too, in particular for applications for which this process is highly structured. However, most methods assume i.i.d. Gaussian distributed noise, such that the forward model becomes pY |X (y|x) ∼ N (Ax, σ2 N I). This naturally leads to the following simplified problem:\n\nˆxMAP = arg min\n\nx\n\n1 2σ2 N\n\n||y − Ax||2\n\n2 − log pX (x).\n\n(4)\n\nHowever, as mentioned, this naive assumption can be very restrictive as many noise processes are much more structured and complex. A myriad of problems can be addressed under the formulation\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nof equation 1, given the freedom of choice for the noise source n. Therefore, in this work, our aim is to solve a more broad class of inverse problems defined by any arbitrary noise distribution n ∼ pN (n) ̸= N and signal prior x ∼ pX (x), resulting in the following, more general, MAP estimator proposed by Whang et al. (2021):\n\nˆxMAP = arg max\n\nx\n\nlog pN (y − Ax) − log pX (x).\n\n(5)\n\nIn this paper, we propose to solve this class of problems using flexible diffusion models. Furthermore, diffusion models naturally enable posterior sampling, allowing us to take advantage of the benefits thereof (Jalal et al., 2021b; Kawar et al., 2021; Daras et al., 2022a).\n\n2.1 RELATED WORK\n\n2.1.1 NORMALIZING FLOWS\n\nWhang et al. (2021) propose to use normalizing flows (NFs) to model both the data and the noise distributions. Normalizing flows are a special class of likelihood-based generative models that make use of an invertible mapping G : Rd → Rd to transform samples from a base distribution pZ(z) into a more complex multimodal distribution x = G(z) ∼ pX (x). The invertible nature of the mapping G allows for exact density evaluation through the change of variables formula:\n\nlog pX (x) = log pZ(z) + log | det JG−1 (x)|,\n\n(6)\n\nwhere J is the Jacobian that accounts for the change in volume between densities. Since exact likelihood computation is possible through the flow direction G−1, the parameters of the generator network can be optimized to maximize likelihood of the training data. Subsequently, the inverse task is solved using the MAP estimation in equation 5:\n\nˆx = arg max\n\nx\n\n{log pGN (y − Ax) + log pGX (x)} ,\n\n(7)\n\nwhere GN and GX are generative flow models for the noise and data respectively. Analog to that, the solution can be solved in the latent space rather than the image space as follows:\n\nˆz = arg max\n\nz\n\n{log pGN (y − A(GX (z))) + λ log pGX (GX (z))} .\n\n(8)\n\nNote that in equation 8 a smoothing parameter λ is added to weigh the prior and likelihood terms, as was also done in Whang et al. (2021).\n\n2.1.2 GENERATIVE ADVERSARIAL NETWORKS\n\nGenerative adversarial networks (GANs) are implicit generative models that can learn the data manifold in an adversarial manner (Goodfellow et al., 2020). The generative model is trained with an auxiliary discriminator network that evaluates the generator’s performance in a minimax game. The generator G(z) : Rl → Rd maps latent vectors z ∈ Rl ∼ N (0, I) to the data distribution of interest. The structure of the generative model can also be used in inverse problem solving (Bora et al., 2017). The objective can be derived from equation 3 and is given by:\n\nˆz = arg min\n\nz\n\n(cid:8)||y − AGX (z)|| + λ||z||2\n\n2\n\n(cid:9) ,\n\n(9)\n\nwhere λ weights the importance of the prior with the measurement error. The l2 regularization term on the latent variable is proportional to negative log-likelihood under the prior defined by GX , where the subscript denotes the density that the generator is approximating. While this method does not explicitly model the noise, it remains an interesting comparison, as the generator cannot reproduce the noise found in the measurement and can only recover signals that are in the range of the generator. Therefore, due to the limited support of the learned distribution, GANs can inherently remove structured noise. However, the representation error (i.e. observation lies far from the range of the generator (Bora et al., 2017)) imposed by the structured noise comes at the cost of recovery quality.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n2.2 BACKGROUND ON SCORE-BASED DIFFUSION MODELS\n\nOne class of deep generative models is known as diffusion models. These generative models have been introduced independently as score-based models (Song & Ermon, 2019; 2020) and denoising In this work, we will consider the diffusion probabilistic modeling (DDPM) (Ho et al., 2020). formulation introduced in Song et al. (2020), which unifies both perspectives on diffusion models by expressing diffusion as a continuous process through stochastic differential equations (SDE). Diffusion models produce samples by reversing a corruption process. In essence these models are networks trained to denoise its input. Through iteration of this process, samples can be drawn from a learned data distribution, starting from random noise. The diffusion process of the data (cid:8)xt ∈ Rd(cid:9) t∈[0,1] is characterized by a continuous sequence of Gaussian perturbations of increasing magnitude indexed by time t ∈ [0, 1]. Starting from the data distribution at t = 0, clean images are defined by x0 ∼ p(x0) ≡ p(x). Forward diffusion can be described using an SDE as follows:\n\ndxt = f (t)xtdt + g(t)dw, (10) where w ∈ Rd is a standard Wiener process, f (t) : [0, 1] → R and g(t) : [0, 1] → R are the drift and diffusion coefficients, respectively. Moreover, these coefficients are chosen so that the resulting distribution p1(x) at the end of the perturbation process approximates a predefined base distribution p1(x) ≈ π(x). Furthermore, the transition kernel of the diffusion process is defined as q(xt|x) ∼ N (xt|α(t)x, β2(t)I), where α(t) and β(t) can be analytically derived from the SDE.\n\nNaturally, we are interested in reversing the diffusion process, so that we can sample from x0 ∼ p0(x0). The reverse diffusion process is also a diffusion process given by the reverse-time SDE (Anderson, 1982; Song et al., 2020):\n\ndxt = (cid:2)f (t)xt − g(t)2 ∇xt log p(xt) (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) score\n\n(cid:3)dt + g(t)d ̄wt\n\n(11)\n\nwhere ̄wt is the standard Wiener process in the reverse direction. The gradient of the log-likelihood of the data with respect to itself, a.k.a. the score function, arises from the reverse-time SDE. The score function is a gradient field pointing back to the data manifold and can intuitively be used to guide a random sample from the base distribution π(x) to the desired data distribution. Given a dataset X = (cid:8)x(1), x(2), . . . , x(|X |)(cid:9) ∼ p(x), scores can be estimated by training a neural network sθ(xt, t) parameterized by weights θ, with score-matching techniques such as the denoising score matching (DSM) objective (Vincent, 2011):\n\nθ∗ = arg min\n\nθ\n\nEt∼U [0,1]\n\n(cid:110)\n\nE(x,xt)∼p(x)q(xt|x)\n\n(cid:104)\n\n∥sθ(xt, t) − ∇xt log q(xt|x)∥2\n\n2\n\n(cid:105)(cid:111)\n\n.\n\n(12)\n\nGiven a sufficiently large dataset X and model capacity, DSM ensures that the score network converges to sθ(xt, t) ≃ ∇xt log p(xt). After training the time-dependent score model sθ, it can be used to calculate the reverse-time diffusion process and solve the trajectory using numerical samplers such as the Euler-Maruyama algorithm. Alternatively, more sophisticated samplers, such as ALD (Song & Ermon, 2019), probability flow ODE (Song et al., 2020), and Predictor-Corrector sampler (Song et al., 2020), can be used to further improve sample quality.\n\nThese iterative sampling algorithms discretize the continuous time SDE into a sequence of time steps {0 = t0, t1, . . . , tT = 1}, where a noisy sample ˆxti is denoised to produce a sample for the next time step ˆxti−1. The resulting samples {ˆxti}T i=0 constitute an approximation of the actual diffusion process {xt}t∈[0,1].\n\n3 METHOD\n\n3.1 CONDITIONAL POSTERIOR SAMPLING UNDER STRUCTURED NOISE\n\nWe are interested in posterior sampling under structured noise. We recast this as a joint optimization problem with respect to the signal x and noise n given by:\n\n(x, n) ∼ pX,N (x, n|y) ∝ pY |X,N (y|x, n) · pX (x) · pN (n).\n\n(13)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nSolving inverse problems using diffusion models requires conditioning of the diffusion process on the observation y, such that we can sample from the posterior pX|Y (x, n|y). Therefore, we construct a joint conditional diffusion process {xt, nt|y}t∈[0,1], in turn producing a joint conditional reversetime SDE:\n\nd(xt, nt) = (cid:2)f (t)(xt, nt) − g(t)2∇xt,nt log p(xt, nt|y)(cid:3)dt + g(t)d ̄wt.\n\n(14)\n\nWe would like to factorize the posterior using our learned unconditional score model and tractable measurement model, given the joint formulation. Consequently, we construct two separate diffusion processes, defined by separate score models but entangled through the measurement model pY |X,N (y|x, n). In addition to the original score model sθ(x, t), we introduce a second score model sφ(nt, t) ≃ ∇nt log pN (nt), parameterized by weights φ, to model the expressive noise component n. These two score networks can be trained independently on datasets for x and n, respectively, using the objective in equation 12. The gradients of the posterior with respect to x and n are now given by:\n\n∇xt log p(xt, nt|y) ≃ ∇xt log p(xt) + ∇xt log p(ˆyt|xt, nt)\n\n≃ sθ⋆ (xt, t) + ∇xt log p(ˆyt|xt, nt),\n\n∇nt log p(xt, nt|y) ≃ ∇nt log p(nt) + ∇nt log p(ˆyt|xt, nt)\n\n≃ sφ⋆ (nt, t) + ∇nt log p(ˆyt|xt, nt),\n\n(15)\n\n(16)\n\nwhere ˆyt is a sample from p(yt|y), and {yt}t∈[0,1] is an additional stochastic process that essentially corrupts the observation along the SDE trajectory together with xt. As p(yt|y) is tractable, we can easily compute ˆyt = α(t)y + β(t)Az, using the reparameterization trick with z ∈ Rd ∼ N (0, I), see Song et al. (2021b). Subsequently, the approximation in equation 15 and equation 16 can be substituted for the conditional score in equation 14, resulting in two entangled diffusion processes:\n\n(cid:26) dxt = (cid:2)f (t)xt − g(t)2 {sθ⋆ (xt, t) + ∇xt log p(ˆyt|xt, nt)} (cid:3)dt + g(t)d ̄wX,t dnt = (cid:2)f (t)nt − g(t)2 {sφ⋆ (nt, t) + ∇nt log p(ˆyt|xt, nt)} (cid:3)dt + g(t)d ̄wN,t\n\n(17)\n\nwhich allows us to perform posterior sampling for both the signal, such that x ≡ x0 ∼ pX|Y (x0|y), as well as the structured noise, such that n ≡ n0 ∼ pN |Y (n0|y).\n\nTo solve the approximated joint conditional reverse-time SDE, we resort to the aforementioned iterative scheme in Section 2.2, however, now incorporating the observation via a data-consistency step. This is done by taking gradient steps that minimize the l2 norm between the true observation and its model prediction given current estimates of x and n. Ultimately, this results in solutions that are consistent with the observation y and have high likelihood under both prior models. The data-consistency update steps for both x and n are derived as follows:\n\nˆxt−∆t = ˆxt − ∇xt log p(ˆyt|ˆxt, ˆnt)\n\nˆnt−∆t = ˆnt − ∇nt log p(ˆyt|ˆxt, ˆnt)\n\n= ˆxt − ∇xt||ˆyt − (Aˆxt + ˆnt)||2 = ˆxt − λAT(Aˆxt − ˆyt + ˆnt),\n\n2\n\n(18)\n\n= ˆnt − ∇nt||ˆyt − (Aˆxt + ˆnt)||2 = ˆnt − μ(Aˆxt − ˆyt + ˆnt),\n\n2 (19)\n\nwhere the time difference between two steps ∆t = 1/T and λ and μ are weighting coefficients for the signal and noise gradient steps, respectively. An example of the complete sampling algorithm is shown in Algorithm 1, which adapts the Euler-Maruyama sampler (Song et al., 2020) to jointly find the optimal data sample and the optimal noise sample while taking into account the measurement model in line 7 and 8 using the outcome of equation 18 and equation 19, respectively. Although we show the Euler-Maruyama method, our addition is applicable to a large family of iterative sampling methods for score-based generative models.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Joint conditional posterior sampling with Euler-Maruyama method\n\nRequire: T, sθ, sφ, λ, μ, y\n\n1 ˆx1 ∼ π(x), ˆn1 ∼ π(n), ∆t ← 1\n\nT\n\n2\n\n3 for i = T − 1 to 0 do\n\nt ← i+1 ˆyt ∼ p0t(yt|y)\n\nT\n\n// Data consistency steps ˆxt−∆t ← ˆxt − λAT(Aˆxt − ˆyt + ˆnt) ˆnt−∆t ← ˆnt − μ(Aˆxt − ˆyt + ˆnt)\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9 . . .\n\n9 . . .\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\nˆxt−∆t ← ˆxt − f (t)ˆxt∆t ˆxt−∆t ← ˆxt−∆t + g(t)2s∗ z ∼ N (0, I)\n\n√\n\nˆxt−∆t ← ˆxt−∆t + g(t)\n\n∆tz\n\nθ(ˆxt, t)∆t\n\nˆnt−∆t ← ˆnt − f (t)ˆnt∆t ˆnt−∆t ← ˆnt−∆t + g(t)2s∗ z ∼ N (0, I)\n\nˆnt−∆t ← ˆnt−∆t + g(t) return: ˆx0\n\n√\n\n∆tz\n\nφ(ˆnt, t)∆t\n\n19 end\n\n3.2 TRAINING AND INFERENCE SETUP\n\nFor training the score models, we use the NCSNv2 architecture as introduced in Song & Ermon (2020) in combination with the Adam optimizer and a learning rate of 5e−4 until convergence. For simplicity, no exponential moving average (EMA) filter on the network weights is applied. Given two separate datasets, one for the data and one for the structured noise, two separate score models can be trained independently. This allows for easy adaptation of our method, since many existing trained score models can be reused. Only during inference, the two priors are combined through the proposed sampling procedure as described in Algorithm 1, using the adapted Euler-Maruyama sampler. We use the variance preserving (VP) SDE (β0 = 0.1, β1 = 7.0) (Song et al., 2020) to define the diffusion trajectory. During each experiment, we run the sampler for T = 600 iterations.\n\n4 EXPERIMENTS\n\nAll models are trained on the CelebA dataset (Liu et al., 2015) and the MNIST dataset with 10000 and 27000 training samples, respectively. We downsize the images to 64 × 64 pixels. Due to computational constraints, we test on a randomly selected subset of 100 images. We use both the peak signal-to noise ratio (PSNR) and structural similarity index (SSIM) to evaluate our results.\n\n4.1 BASELINE METHODS\n\nThe closest to our work is the flow-based noise model proposed by Whang et al. (2021), discussed in Section 2.1.1, which will serve as our main baseline. To boost the performance of this baseline and to make it more competitive we moreover replace the originally-used RealNVP (Dinh et al., 2016) with the Glow architecture (Kingma & Dhariwal, 2018). Glow is a widely used flow model highly inspired by RealNVP, with the addition of 1 × 1 convolutions before each coupling layer. We use the exact implementation found in Asim et al. (2020), with a flow depth of K = 18, and number of levels L = 4, which has been optimized for the same CelebA dataset used in this work and thus should provide a fair comparison with the proposed method.\n\nSecondly, GANs as discussed in Section 2.1.2 are used as a comparison. We train a DCGAN (Radford et al., 2015), with a generator latent input dimension of l = 100. The generator architecture consists of 4 strided 2D transposed convolutional layers, having 4 × 4 kernels yielding feature maps of 512, 256, 128 and 64. Each convolutional layer is followed by a batch normalization layer and ReLU activation.\n\nLastly, depending on the reconstruction task, classical non-data-driven methods are used as a comparison. For denoising experiments, we use the block-matching and 3D filtering algorithm (BM3D) (Dabov et al., 2006), and in compressed sensing experiments, LASSO with wavelet basis (Tibshirani, 1996).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) CelebA\n\n(b) Out-of-distribution data\n\nFigure 1: Quantitative results using PSNR (green) and SSIM (blue) for the removing MNIST digits experiment on 64 × 64 images of the (a) CelebA and (b) out-of-distribution datasets.\n\nExcept for the flow-based method of Whang et al. (2021), none of these methods explicitly model the noise distribution. Still, they are a valuable baseline, as they demonstrate the effectiveness of incorporating a learned structured noise prior rather than relying on simple noise priors.\n\nAutomatic hyperparameter tuning for optimal inference was performed for all baseline methods on a small validation set of only 5 images. For both GAN and flow-based methods, we anneal the step size during inference based on stagnation of the objective.\n\n4.2 RESULTS\n\n4.2.1 REMOVING MNIST DIGITS\n\nFor comparison with Whang et al. (2021), we recreate an experiment introduced in their work, where MNIST digits are added to CelebA faces. Moreover, the experiment is easily reproducible as both CelebA and MNIST datasets are publicly available. The corruption process is defined by y = 0.5 · xCelebA + 0.5 · nMNIST. In this experiment, the score network sφ is trained on the MNIST dataset. Fig. 1a shows a quantitative comparison of our method with all baselines. Furthermore, a random selection of test samples is shown in Fig. 2 for qualitative analysis. Both our method and the flow-based method are able to recover the data, and remove most of the structured noise. However, more details are preserved using the diffusion method. In contrast, the flow-based method cannot completely remove the digits in some cases and is unable to reconstruct some subtle features present in the original images. Furthermore, we observe that for the flow-based method, initialization from the measurement is necessary to reproduce the results in Whang et al. (2021) since random initialization does not converge. The GAN method is also able to remove the digits, but cannot accurately reconstruct the faces as it is unable to project the observation onto the range of the generator. Similarly, the BM3D denoiser fails to recover the underlying signal, confirming the importance of prior knowledge of the noise in this experiment. The metrics in Fig. 1a support these observations. See Table 1 for the extended results.\n\nAdditionally, we expose the methods in a similar experiment to out-of-distribution (OoD) data. The images from this dataset not found in the CelebA dataset, which is the data used for training the models. In fact, the out-of-distribution data is generated using the stable-diffusion text-to-image model Rombach et al. (2022). We use the exact same hyperparameters as during the experiment on the CelebA dataset. Quantitative and qualitative results are shown in Fig. 1b and Fig. 3, respectively. Similarly to the findings of Whang et al. (2021); Asim et al. (2020), the flow-based method is robust to OoD data, due to their inherent invertibility. We empirically show that the diffusion method is also resistant to OoD data in inverse tasks with complex noise structures and even outperforms the flowbased methos. Unsurprisingly, the GAN method performs even more poorly when subjected to OoD data. More experiments, covering different inverse problem settings can be found in Appendix A.\n\n7\n\nBM3DGANFLOWDIFFUSION1015202530PSNR11.5717.522.9127.260.330.490.830.870.00.20.40.60.81.0SSIMBM3DGANFLOWDIFFUSION1015202530PSNR10.4513.0719.9822.880.270.220.820.840.00.20.40.60.81.0SSIMUnder review as a conference paper at ICLR 2023\n\nFigure 2: Results for our diffusion-based method compared to the baselines; FLOW (Whang et al., 2021), GAN (Bora et al., 2017), and BM3D (Dabov et al., 2006) on the removing MNIST digits experiment on 64 × 64 images of the CelebA dataset.\n\nFigure 3: Results for our diffusion-based method on the removing MNIST digits experiment on an out of distribution dataset, generated using stable diffusion (Rombach et al., 2022).\n\n8\n\nGroundTruthNoisyInputDIFFUSIONFLOWGANBM3DGroundTruthNoisyInputDIFFUSIONFLOWGANBM3DUnder review as a conference paper at ICLR 2023\n\n4.2.2 PERFORMANCE\n\nTo highlight the difference in inference time between our method and the baselines, benchmarks are performed on a single 12GBytes NVIDIA GeForce RTX 3080 Ti, see Table 3 in Appendix B.2. Although this is not an extensive benchmark, a quick comparison of inference times reveals a 50× difference in speed between ours and the flow-based method. All the deep generative models need approximately an equal amount of iterations (T ≈ 600) to converge. However, for the same modeling capacity, the flow model requires a substantial higher amount of trainable parameters compared to the diffusion method. This is mainly due to the restrictive requirements imposed on the architecture to ensure tractable likelihood computation. It should be noted that no improvements to speed up the diffusion process, such as CCDF (Chung et al., 2022b) are applied for the diffusion method, giving room for even more improvement in future work.\n\n5 DISCUSSION AND CONCLUSIONS\n\nIn this work, we present a framework for removing structured noise using diffusion models. Our work provides an efficient addition to existing score-based conditional sampling methods incorporating knowledge of the noise distribution. We demonstrate our method on natural and out-ofdistribution data and achieve increased performance over the state-of-the-art and established conventional methods for complex inverse tasks. Additionally, the diffusion based method is substantially easier to train using the score matching objective compared to other deep generative methods and furthermore allows for posterior sampling.\n\nWhile our method is considerably faster and better in removing structured noise compared to the flow-based method (Whang et al., 2021), it is not ready (yet) for real-time inference and still slow compared to GANs (Bora et al., 2017) and classical methods. Luckily, research into accelerating In addition, although a simple sampling algorithm the diffusion process are well on their way. was adapted in this work, many more sampling algorithms for score-based diffusion models exist, each of which introduces a new set of hyperparameters. For example, the predictor-corrector (PC) sampler has been shown to improve sample quality (Song et al., 2020). Future work should explore this wide increase in design space to understand limitations and possibilities of more sophisticated sampling schemes in combination with the proposed joint diffusion method. Furthermore, the range of problems to which we can apply the proposed method, can be expanded into non-linear likelihood models and extend beyond the additive noise models.\n\nLastly, the connection between diffusion models and continuous normalizing flows through the neural ODE formulation (Song et al., 2021a) is not investigated, but greatly of interest given the comparison with the flow-based method in this work.\n\n6 REPRODUCIBILITY STATEMENT\n\nAll code used to train and evaluate the models as presented in this paper can be found at https: //anonymous.4open.science/r/iclr2023-joint-diffusion. Essentially, the codebase in https://github.com/yang-song/score_sde_pytorch of Song et al. (2020) is used to train the score-based diffusion networks, for both data and structured noise, independently. To implement the proposed inference scheme, the lines in Algorithm 1 should be adapted to create a sampler that includes both trained diffusion models. Details regarding the training and inference settings used to reproduce the results in this work can be found in Section 3.2.\n\nREFERENCES\n\nBrian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Ap-\n\nplications, 12(3):313–326, 1982. 4\n\nMuhammad Asim, Max Daniels, Oscar Leong, Ali Ahmed, and Paul Hand. Invertible generative models for inverse problems: mitigating representation error and dataset bias. In International Conference on Machine Learning, pp. 399–409. PMLR, 2020. 1, 6, 7\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nArpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. arXiv preprint arXiv:2208.09392, 2022. 1\n\nAmir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse\n\nproblems. SIAM journal on imaging sciences, 2(1):183–202, 2009. 1\n\nAshish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using generative models. In International Conference on Machine Learning, pp. 537–546. PMLR, 2017. 1, 3, 8, 9, 15\n\nHyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical\n\nImage Analysis, pp. 102479, 2022. 1\n\nHyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for\n\ninverse problems using manifold constraints. arXiv preprint arXiv:2206.00941, 2022a. 1\n\nHyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conIn Proceedings ditional diffusion models for inverse problems through stochastic contraction. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12413–12422, 2022b. 1, 9\n\nKostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising with block-matching and 3d filtering. In Image processing: algorithms and systems, neural networks, and machine learning, volume 6064, pp. 354–365. SPIE, 2006. 6, 8, 15\n\nGiannis Daras, Yuval Dagan, Alex Dimakis, and Constantinos Daskalakis. Score-guided intermediate level optimization: Fast langevin mixing for inverse problems. In International Conference on Machine Learning, pp. 4722–4753. PMLR, 2022a. 1, 3\n\nGiannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G. Dimakis, and Peyman Milanfar.\n\nSoft diffusion: Score matching for general corruptions, 2022b. 1\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv\n\npreprint arXiv:1605.08803, 2016. 6\n\nYonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge\n\nuniversity press, 2012. 1\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020. 3\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020. 4\n\nAjil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. Robust compressed sensing mri with deep generative priors. Advances in Neural Information Processing Systems, 34:14938–14954, 2021a. 1\n\nAjil Jalal, Sushrut Karmalkar, Alex Dimakis, and Eric Price. Instance-optimal compressed sensing via posterior sampling. In International Conference on Machine Learning, pp. 4709–4720. PMLR, 2021b. 3\n\nBowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion gener-\n\native models. arXiv preprint arXiv:2205.01490, 2022. 2\n\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\n\nbased generative models. arXiv preprint arXiv:2206.00364, 2022. 1\n\nBahjat Kawar, Gregory Vaksman, and Michael Elad. Snips: Solving noisy inverse problems stochas-\n\ntically. Advances in Neural Information Processing Systems, 34:21757–21769, 2021. 3\n\nDurk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.\n\nAdvances in neural information processing systems, 31, 2018. 6\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\n\nIn Proceedings of International Conference on Computer Vision (ICCV), December 2015. 6\n\nCalvin Luo.\n\nUnderstanding diffusion models: A unified perspective.\n\narXiv preprint\n\narXiv:2208.11970, 2022. 1\n\nStéphane Mallat. A wavelet tour of signal processing. Elsevier, 1999. 1\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. 6\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. 2, 7, 8, 13\n\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In\n\nInternational Conference on Learning Representations, 2021. 2\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\n\nAdvances in Neural Information Processing Systems, 32, 2019. 4\n\nYang Song and Stefano Ermon. Improved techniques for training score-based generative models.\n\nAdvances in neural information processing systems, 33:12438–12448, 2020. 4, 6\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben In Interna-\n\nPoole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2020. 1, 4, 5, 6, 9\n\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of scorebased diffusion models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1415–1428. Curran Associates, Inc., 2021a. 9\n\nYang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. In International Conference on Learning Representations, 2021b. 1, 5\n\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\n\nSociety: Series B (Methodological), 58(1):267–288, 1996. 6, 15\n\nArash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space.\n\nAdvances in Neural Information Processing Systems, 34:11287–11302, 2021. 2\n\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural compu-\n\ntation, 23(7):1661–1674, 2011. 1, 4\n\nXinyi Wei, Hans van Gorp, Lizeth Gonzalez-Carabarin, Daniel Freedman, Yonina C Eldar, and Ruud JG van Sloun. Deep unfolding with normalizing flow priors for inverse problems. IEEE Transactions on Signal Processing, 70:2962–2971, 2022. 1\n\nJay Whang, Qi Lei, and Alex Dimakis. Solving inverse problems with a flow-based noise model. In International Conference on Machine Learning, pp. 11146–11157. PMLR, 2021. 2, 3, 6, 7, 8, 9, 15\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\n(a) CelebA\n\n(b) Out-of-distribution data\n\nFigure 4: Quantitative results using PSNR (green) and SSIM (blue) for the compressed sensing with sinusoidal noise experiment on 64×64 images of the (a) CelebA and (b) out-of-distribution datasets.\n\nA ADDITIONAL EXPERIMENTS\n\nThe following section explores additional inverse problems with compressed sensing and structured noise. The goal is to show the performance of the proposed method in a variety of settings.\n\nA.1 STRUCTURED NOISE WITH COMPRESSED SENSING\n\nThe corruption process is defined by y = Ax + nsine with a random Gaussian measurement matrix (cid:1)(cid:1) for each pixel k. The A ∈ Rm×d and a noise with sinusoidal variance σk ∝ exp(cid:0)sin(cid:0) 2πk subsampling factor is defined by the size of the measurement matrix d/m. In this experiment, the score network sφ is trained on a dataset generated with sinusoidal noise samples nsine. In Fig. 5 the results of the compressed sensing experiment and the comparison with the baselines are shown for an average standard deviation of σN = 0.2 and subsampling of factor d/m = 2. Given the same hyperparameter settings, we repeat the experiment on the out-of-distribution (OoD) dataset, shown in Fig. 6. Similar to the results found in Section 4.2.1, the diffusion method is more robust to the shift in distribution and is able to deliver high quality recovery under the structured noise setting. In contrast, the flow-based method under-performs when subjected to the OoD data. Quantitative results on both CelebA and OoD are found in Fig. 4 as well as Table 1 and Table 2, respectively.\n\n16\n\nA.2 REMOVING SINUSOIDAL NOISE The corruption process is defined by y = x + nsine where the noise variance σk ∝ exp(cid:0)sin(cid:0) 2πk follows a sinusoidal pattern along each row of the image k. In this experiment, the score network sφ is trained on a dataset generated with 1D sinusoidal noise samples nsine. See Fig. 8 for a comparison of our method to the flow-based method for varying noise variances. Both methods perform quite well, with the diffusion method having a slight edge. A visual comparison in Fig. 7, however, reveals that the diffusion method preserves more detail in general.\n\n(cid:1)(cid:1)\n\n16\n\n12\n\nLASSOGANFLOWDIFFUSION1015202530PSNR12.9318.924.9625.520.280.530.780.820.00.20.40.60.81.0SSIMLASSOGANFLOWDIFFUSION1015202530PSNR11.6312.3919.8622.90.340.160.610.820.00.20.40.60.81.0SSIMUnder review as a conference paper at ICLR 2023\n\nFigure 5: Comparison of results from our diffusion method compared to the baselines on the compressed sensing with sinusoidal noise experiment with d/m = 2, σN = 0.2 on 64 × 64 images of the CelebA dataset.\n\nFigure 6: Results for our diffusion-based method on the compressed sensing with sinusoidal noise experiment on an out-of-distribution dataset, generated using stable diffusion (Rombach et al., 2022).\n\n13\n\nGroundTruthDIFFUSIONFLOWGANLASSOGroundTruthDIFFUSIONFLOWGANLASSOUnder review as a conference paper at ICLR 2023\n\nFigure 7: Comparison of results from our diffusion method compared to the baselines on the removing sinusoidal noise experiment with σN = 0.2 on 64 × 64 images of the CelebA dataset.\n\nFigure 8: Comparison of PSNR values for varying sinusoidal noise variances. Shaded areas represent the standard deviation on the metric.\n\n14\n\nGroundTruthNoisyInputDIFFUSIONFLOWGANBM3D0.10.20.3Avg.NoiseStd.σN12141618202224PSNRFLOWDIFFUSIONUnder review as a conference paper at ICLR 2023\n\nB EXTENDED RESULTS\n\nB.1 METRICS\n\nTable 1: Results for the experiments and different methods on the CelebA dataset. ⋆Ours, †Whang et al. (2021), ‡Bora et al. (2017), §Dabov et al. (2006), ¶Tibshirani (1996).\n\nMNIST\n\nCS + sine noise\n\nPSNR\n\nSSIM\n\nPSNR\n\nSSIM\n\n⋆DIFFUSION 27.26 ± 1.925 †FLOW 22.90 ± 1.214 ‡GAN 17.50 ± 1.404 §BM3D 11.56 ± 1.879 ¶LASSO -\n\n0.865 ± 0.039 0.827 ± 0.051 0.486 ± 0.099 0.326 ± 0.059 -\n\n25.51 ± 1.040 24.96 ± 2.292 18.90 ± 1.343 -\n12.93 ± 1.819\n\n0.823 ± 0.044 0.779 ± 0.082 0.529 ± 0.084 -\n0.284 ± 0.037\n\nTable 2: Results for the experiments and different methods on the out-of-distribution (OoD) dataset. ⋆Ours, †Whang et al. (2021), ‡Bora et al. (2017), §Dabov et al. (2006), ¶Tibshirani (1996).\n\nMNIST\n\nCS + sine noise\n\nPSNR\n\nSSIM\n\nPSNR\n\nSSIM\n\n⋆DIFFUSION 22.87 ± 4.581 †FLOW 19.98 ± 1.946 ‡GAN 13.06 ± 1.788 §BM3D 10.44 ± 1.446 ¶LASSO -\n\n0.842 ± 0.110 0.824 ± 0.081 0.218 ± 0.088 0.274 ± 0.069 -\n\n22.90 ± 1.568 19.85 ± 4.840 12.39 ± 1.693 -\n11.62 ± 1.473\n\n0.823 ± 0.082 0.608 ± 0.176 0.159 ± 0.070 -\n0.336 ± 0.057\n\nB.2 COMPUTATIONAL PERFORMANCE\n\nTable 3: Inference performance benchmark for all methods. ⋆Ours, †Whang et al. (2021), ‡Bora et al. (2017), §Dabov et al. (2006), ¶Tibshirani (1996).\n\nNumber of trainable parameters\n\nInference time / image [ms]\n\n⋆DIFFUSION †FLOW ‡GAN §BM3D\n\n8.9M 25.8M 3.9M –\n\n1292 61853 59 28.5\n\n[2.153 / it] [103.1 / it] [0.059 / it]\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nC PSEUDO-CODE\n\nIn this section, we provide pseudo-code for the proposed joint conditional diffusion sampler with the Euler-Maruyama sampling algorithm as basis. Furthermore, we use the SDE formulation for the diffusion process which is denoted as an sde object with drift, diffusion and marginal_prob methods. The latter computes the mean and standard deviation of the diffusion transition kernel at a certain time t. Lastly, there are two trained score networks (NCSNv2) score_data and score_noise for the data and structured noise respectively.\n\ndef j o i n t _ c o n d _ d i f f u s i o n _ s a m p l e r (y , lambda_coeff , mu_coeff , num_steps ):\n\ndt = 1/ num_steps x = random . normal ( y . shape ) n = random . normal ( y . shape )\n\nfor t in linspace (1 , 0 , num_steps ):\n\n# corrupt observation along the diffusion process mean , std = sde . marginal_prob (y , t ) y_hat = mean + std * random . normal ( y . shape )\n\n# data consistency step for x ( data ) x = x - lambda_coeff * A . T @ ( A @ x - y_hat + n )\n\n# data consistency step for n ( noise ) n = n - mu_coeff * ( n - y_hat + A @ x )\n\n# reverse diffusion step for x ( data ) z = random . normal ( x . shape ) x_hat = x - sde . drift ( t ) * x * dt x_hat = x_hat + sde . diffusion ( t )**2 * score_data (x , t ) * dt x_hat = x_hat + sde . diffusion ( t ) * sqrt ( dt ) * z x = x_hat\n\n# reverse diffusion step for n ( noise ) z = random . normal ( n . shape ) n_hat = n - sde . drift ( t ) * n * dt n_hat = n_hat + sde . diffusion ( t )**2 * score_noise (n , t ) * dt n_hat = n_hat + sde . diffusion ( t ) * sqrt ( dt ) * z n = n_hat\n\n# return the denoised sample x | y return x\n\n16",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes to use a score-based model for the task of removing structured noise. Specifically, the method uses a pretrained (unconditional) NCSNv2 model as the signal prior and uses modified update rule during sampling that incorporates the score of a separate noise model.  Experimental results show that score-based models achieve superior performance to existing methods based on GAN and normalizing flow.\n\n# Strength And Weaknesses\n\n**Strengths**\n* Clear exposition and motivation.\n* Problem is relevant to the field as inverse problems encompass a large family of problems and is an active area of research.\n\n**Weaknesses**\n* Experimental comparison is done on a single dataset -- especially when compared to the main baseline (Whang et al.), which considered various types of noises.\n* Unclear whether the benefits are due to the difference in the model class, or other factors (e.g. hyperparameters, network size, etc).  While I don't doubt that score-based prior could outperform GAN/flow priors, it would have been nice to see some effort on trying to decouple the effects of model class vs. other architectural/implementation details.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\nThe paper is clearly written, and I had no problem understanding the motivation as well as the proposed method.\n\n**Quality**\n* Pages 2-6 are spent on background and the description of the method.  This seems a bit excessive and could be made more concise to make room for additional experiments.\n\n**Novelty**\nGiven the recent success of score-based models, its application on the existing task of structured noise removal is a relatively straightforward idea.  That said, I do think a comparison between score-based prior vs. other generative priors have a merit.  This is why I hoped to see a more thorough experimental section.\n\n# Summary Of The Review\n\nThis paper proposes to combine two score-based models -- signal prior and noise prior -- for the task of removing structured noise.  The proposed method outperforms existing deep generative priors on the task of MNIST digit removal, and while the approach is simple, this discovery has some value.  That said, the experiments and analysis are lacking.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents a novel approach for addressing ill-posed inverse problems, particularly focusing on the removal of structured noise through diffusion models, which have traditionally been limited to handling unstructured noise. The authors introduce a joint conditional reverse diffusion process that learns score functions for both noise and signal distributions, significantly enhancing the model's ability to recover original signals from noisy observations. Experimental results demonstrate the proposed method's effectiveness, showing superior performance against state-of-the-art methods, including normalizing flows (NFs) and generative adversarial networks (GANs), particularly in challenging scenarios involving structured noise.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to modeling structured noise, filling a critical gap in the current literature on diffusion models. The joint conditional sampling method is well-articulated and offers a clear advancement over existing techniques. The experimental validation across several datasets, including CelebA and MNIST, supports the claims of robustness and improved performance. However, a potential weakness is the method's current unsuitability for real-time applications, as mentioned in the discussion. Additionally, while the results are promising, the exploration of more complex noise structures and non-linear likelihood models remains an open question.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with a logical flow from introduction to methodology and results. The authors effectively communicate the significance of their contributions and how they build on existing work in the field. The experimental setup is clearly described, and the provision of code and implementation details enhances reproducibility. The novelty of the approach is significant, as it innovatively applies diffusion models to a broader class of inverse problems, which have typically been challenging to address using conventional methods.\n\n# Summary Of The Review\nThis paper makes a substantial contribution to the field by extending diffusion models to handle structured noise in inverse problems. The proposed method shows impressive performance improvements over existing techniques, although it currently lacks real-time applicability. Overall, the work is well-executed, innovative, and clearly presented.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses the challenge of ill-posed inverse problems in signal and image processing, proposing a novel approach that leverages data-driven deep generative models, particularly score-based diffusion models, to improve the recovery of unknown signals from noisy observations. The authors introduce a joint conditional reverse diffusion process that learns scores for both noise and signal distributions, allowing for a more flexible recovery framework that accommodates arbitrary noise distributions. Experimental results demonstrate that the proposed method outperforms traditional methods and competitive baselines, such as flow-based models and GANs, particularly in preserving details in the recovered images and exhibiting robustness against out-of-distribution data.\n\n# Strength And Weaknesses\nThe strengths of this paper include its innovative approach to handling structured noise, which is significant given the limitations of traditional methods that typically assume Gaussian noise. The proposed method shows notable improvements in both qualitative and quantitative performance metrics compared to established baselines, highlighting its robustness and flexibility. However, the paper does have limitations, particularly concerning inference speed, which may hinder real-time applications, and the complexity of the methodology, which could pose implementation challenges. Additionally, the computational demands for training and inference may restrict its scalability in resource-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. The mathematical formulation is presented in a manner that is accessible to the reader, and the experimental design is robust, utilizing appropriate datasets and baseline comparisons. The novelty of the approach is significant, as it extends the use of diffusion models to a new context in the domain of inverse problems. However, reproducibility could be a concern given the complexity of the method and potential hyperparameter tuning required for optimal performance.\n\n# Summary Of The Review\nOverall, this paper presents a substantial contribution to the field of signal and image processing by effectively applying diffusion models to ill-posed inverse problems with structured noise. While it shows impressive performance improvements and robustness, attention to inference speed and implementation complexity will be crucial for its practical utility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to solving ill-posed inverse problems, specifically focusing on the removal of structured noise using a joint conditional reverse diffusion process. The authors propose a methodology that leverages learned scores from noise and signal-generating distributions, outperforming existing state-of-the-art techniques, including normalizing flows (NFs) and generative adversarial networks (GANs). The experimental results demonstrate the framework's superior performance in terms of detail recovery and efficiency, with significant improvements in inference speed and robustness against out-of-distribution data.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative application of diffusion models to structured noise removal, which represents a significant advancement over traditional methods. The methodology is well-structured, making use of joint conditional modeling and score-based approaches that are promising for future research. However, the paper could benefit from a more thorough comparison with a broader range of baseline methods, as well as a more detailed discussion of the limitations of the proposed approach. Additionally, while the results are compelling, further empirical validation in diverse settings would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, with clear explanations of the methodology and results. The authors provide adequate mathematical formulations and background necessary for understanding the proposed approach. The novelty is significant, as the use of diffusion models for this specific problem is not widely explored in existing literature. The reproducibility is addressed, with code available in specified repositories, which is a strong point for the paper.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel approach to removing structured noise using diffusion models, demonstrating significant improvements over existing methods. The clarity and quality of the writing support the innovative contributions made, while the reproducibility statement enhances its value for the research community.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel joint conditional posterior sampling method that leverages diffusion models to address structured noise in inverse problems. The methodology showcases the use of learned scores for both noise and signal distributions, allowing for flexibility in handling various inverse problems. The findings indicate that the proposed method outperforms established baselines, such as normalizing flows and GANs, particularly in terms of robustness against out-of-distribution signals, while also simplifying the training process relative to other deep generative models.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach, demonstrating strong performance and robustness, as well as ease of training compared to traditional generative models. However, it suffers from several weaknesses, including slow inference speed, limited baseline comparisons, and dependence on specific datasets. Additionally, the complexity of hyperparameter tuning and assumptions regarding noise structure may restrict the method's applicability in broader scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections and clear explanations of the methodology. The quality of the experiments is solid, although the limited scope of datasets may affect reproducibility. The novelty of the approach is significant, introducing a fresh perspective on handling structured noise, but the practical implementation complexity may hinder accessibility for some practitioners.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of generative modeling by introducing a novel approach to handling structured noise in inverse problems. While the results are promising, the limitations regarding inference speed and dataset generalizability warrant consideration for future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel approach for addressing structured noise in inverse problems through an enhanced diffusion model framework. The authors propose a joint conditional reverse diffusion process that integrates learned score functions for both noise and signal distributions, significantly improving performance on various challenging tasks. The methodology is rooted in a Bayesian context, presenting a modified reverse diffusion process characterized by a joint stochastic differential equation (SDE) that evolves signal and noise distributions concurrently. Experimental results demonstrate that the proposed method outperforms state-of-the-art techniques in terms of signal recovery, particularly in scenarios involving structured noise.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative joint conditional diffusion framework, which effectively addresses the complexities of structured noise in inverse problems. The performance enhancements demonstrated on benchmark datasets (CelebA and MNIST) are compelling, showcasing substantial improvements in PSNR and SSIM metrics compared to existing methods. Furthermore, the robustness to out-of-distribution samples presents a crucial advancement in the field. However, a notable weakness is the limited exploration of training dynamics, particularly how the independent training of score models for noise and signal affects overall performance. Additionally, the computational efficiency, while better than flow-based methods, still lags behind GANs, which may raise concerns in real-time applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear and logical structure that facilitates understanding of the proposed methodology and its implications. The quality of the experiments is commendable, with a rigorous comparative analysis against existing methods. In terms of novelty, the work presents a significant contribution to the diffusion model literature, particularly in its application to structured noise in inverse problems. However, reproducibility could be enhanced by providing more detailed insights into the training dynamics and broader baseline comparisons.\n\n# Summary Of The Review\nOverall, this paper represents a significant advancement in addressing structured noise in inverse problems through a novel joint conditional diffusion approach. The contributions are well-articulated and supported by solid experimental results, although further exploration of training dynamics and additional baseline comparisons would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled **Removing Structured Noise with Diffusion Models** presents a new approach to adversarial training by leveraging diffusion models to improve the robustness of generative models against structured noise. The authors introduce a joint conditional reverse diffusion process, which concurrently models both the signal and the adversarial noise, enabling better adversarial robustness. They validate their method through extensive experiments on standard datasets such as CelebA and MNIST, demonstrating significant performance improvements over traditional GAN approaches in generating high-quality samples and exhibiting enhanced robustness to out-of-distribution data.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative integration of diffusion models into adversarial training, which is a relatively unexplored area. The proposed joint conditional reverse diffusion process is a compelling addition that allows for improved noise modeling and robustness. Additionally, the empirical results convincingly support the authors' claims, showcasing marked improvements in sample quality and robustness. However, a potential weakness is that the paper could provide more insights into the limitations of the approach, particularly in terms of computational efficiency and scalability when applied to larger datasets or more complex noise structures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with a logical flow that facilitates understanding of the proposed methodology and findings. The quality of the experimental results is high, and the performance metrics used (PSNR and SSIM) are appropriate for the tasks at hand. In terms of novelty, the paper offers a fresh perspective on adversarial training by introducing diffusion models, which could encourage further research in this area. Reproducibility is addressed through detailed descriptions of the methodology and experimental setups, although the inclusion of code or supplementary materials would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a novel and effective approach to adversarial training using diffusion models, backed by strong experimental evidence. The contributions are significant, and the findings could inspire future research in noise modeling and generative modeling techniques. The paper is well-written and addresses an important challenge in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel joint conditional reverse diffusion process for removing structured noise in inverse problems, claiming to significantly advance the state of the art in this domain. The authors argue that their method outperforms previous techniques, including GANs and normalizing flows, through a complex joint optimization framework that is purportedly adaptable to various noise models. They present experimental results suggesting their approach achieves unprecedented performance metrics, advocating for a transition away from traditional methods in favor of their proposed model.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its ambitious claim of achieving superior performance in structured noise removal, alongside a theoretically interesting approach that attempts to unify various inverse problem-solving methods within a single framework. However, the weaknesses include an overstatement of the method’s applicability and performance, presenting results in a manner that may mislead readers regarding its generalizability. The lack of thorough empirical validation and a nuanced discussion of limitations detracts from the overall credibility of the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by the exaggerated claims and insufficiently detailed methodology, making it challenging for readers to fully grasp the practical implications and limitations of the proposed method. While the concept of using diffusion models for structured noise removal is intriguing, the paper does not sufficiently substantiate its novelty with rigorous empirical evidence. Reproducibility is also a concern due to the lack of comprehensive details regarding implementation and experimental setup.\n\n# Summary Of The Review\nOverall, the paper presents an ambitious approach to structured noise removal using diffusion models, but it suffers from exaggerated claims and a lack of empirical rigor. While the methodology is theoretically interesting, the presentation raises concerns about clarity and reproducibility, ultimately undermining its impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper \"Removing Structured Noise with Diffusion Models\" introduces a novel joint conditional reverse diffusion process aimed at effectively removing structured noise in various inverse problems. The proposed methodology demonstrates significant performance improvements over existing techniques, such as normalizing flows and adversarial networks. The authors validate their approach through experiments on the CelebA and MNIST datasets, showing enhanced performance in metrics like Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), and emphasize the robustness of their method against out-of-distribution signals.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to structured noise removal, which leverages the strengths of diffusion models while addressing the limitations of prior methods. The reported performance gains, particularly in terms of speed and reduced parameter count, further enhance its contribution to the field. However, the paper could benefit from a more comprehensive exploration of potential limitations and the implications of the results on broader applications. Additionally, the reliance on relatively simple datasets raises questions about the generalizability of the findings to more complex real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings in a clear and organized manner. The methodology is described adequately, and the experimental setup is detailed enough to allow for reproducibility. While the novelty of the proposed diffusion model in the context of structured noise removal is significant, the paper could strengthen its claims by including more diverse datasets and real-world applications to better illustrate the model's utility and performance.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the realm of structured noise removal through diffusion models, demonstrating notable performance improvements and efficiency. While the clarity and quality of the presentation are commendable, the exploration of limitations and broader applicability could be enhanced.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for signal recovery in the presence of structured noise by leveraging diffusion models for joint inference of signal and noise distributions. It claims to handle arbitrary noise types and proposes a data-driven approach for learning priors. The methodology involves a joint conditional diffusion process that assumes conditional independence between signal and noise given observations. The authors report performance improvements over existing generative models, particularly in scenarios characterized by complex noise structures.\n\n# Strength And Weaknesses\nThe primary contribution of the paper lies in its innovative use of diffusion models for joint inference, which represents a significant shift from traditional methods. However, the assumptions made regarding the noise distribution, specifically the conditional independence and the effectiveness of data-driven priors, raise concerns about the robustness and generalizability of the approach. The paper lacks empirical validation of its claims regarding handling arbitrary noise types and does not sufficiently address scalability issues or the sensitivity of results to hyperparameters.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is overall well-structured and presents its contributions clearly. However, the assumptions underlying the proposed methodologies, particularly regarding noise and signal characteristics, could be better articulated. The novelty of the approach is commendable, though it may not be fully reproducible due to insufficient discussion on hyperparameter sensitivity and limited experimental validation across diverse noise distributions.\n\n# Summary Of The Review\nThe paper introduces an interesting method for joint signal and noise inference using diffusion models, with potential implications for signal recovery tasks. However, the validity of its assumptions and the robustness of its claims require further scrutiny, particularly regarding the handling of unknown noise distributions and practical applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents an advancement in score-based diffusion models tailored for tackling structured noise in ill-posed inverse problems, particularly in signal and image processing. It introduces a joint conditional reverse diffusion process that learns to model both the noise and signal distributions, moving beyond the limitations of traditional approaches such as normalizing flows and GANs. The methodology involves a novel joint optimization framework for posterior sampling, which effectively integrates learned score models with a data-consistency step, leading to improved performance in noise removal on benchmark datasets like CelebA and MNIST.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating structured noise handling within the framework of diffusion models, which enhances the effectiveness of signal recovery tasks. The experimental results demonstrate significant improvements over existing methods, validating the proposed framework's utility. However, a notable weakness is the limited exploration of inference speed, which may be critical for real-world applications. Additionally, while the paper adequately discusses related work, further comparative analysis with a broader range of methods could strengthen the argument for its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and findings. The authors provide sufficient detail in the description of their experiments, which aids in understanding their contributions. The novelty of the approach is significant, particularly in how it adapts diffusion models to structured noise. Furthermore, the reproducibility statement is strong, with links to the codebase and implementation details, allowing others to replicate the experiments.\n\n# Summary Of The Review\nOverall, this paper makes a compelling contribution to the field of signal and image processing by extending diffusion models to address structured noise in inverse problems. While the methodology is sound and the results promising, further attention to inference speed and a more comprehensive comparison with other techniques could enhance its impact.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThis paper presents a novel approach to enhancing the performance of machine learning models through an innovative framework that integrates both theoretical and practical components. The authors introduce a new algorithm, supported by rigorous theoretical analysis, which aims to improve model robustness and generalization capabilities. Empirical results demonstrate the method's effectiveness across several benchmark datasets, indicating significant performance improvements compared to traditional approaches.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Originality:** The proposed method offers a unique perspective on model optimization, potentially paving the way for new research avenues within the field.\n2. **Theoretical Foundation:** The theoretical insights are robust, providing a solid rationale for the methodology and enhancing the credibility of the proposed approach.\n3. **Clarity of Writing:** The paper is well-structured and thoughtfully written, making complex concepts accessible to a wide audience.\n4. **Potential Impact:** The demonstrated improvements on benchmark datasets suggest that the proposed method could significantly influence future work in the area.\n\n**Weaknesses:**\n1. **Experimental Validation:** The paper's experimental validation appears somewhat limited; additional experiments would strengthen the findings and demonstrate the method's versatility.\n2. **Comparison with Baselines:** The lack of comprehensive comparisons to existing state-of-the-art methods makes it challenging to assess the advantages of the proposed approach fully.\n3. **Scalability and Practicality:** The discussion on the scalability of the proposed method in real-world applications is insufficient, leaving questions regarding its practical applicability.\n4. **Hyperparameter Sensitivity:** Concerns about hyperparameter sensitivity could affect the method's reproducibility and practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions, making it easy to follow the proposed methodology and findings. The theoretical underpinnings are solid, adding to the paper's quality. However, the reproducibility of the results may be hindered by a lack of detailed explanations regarding hyperparameter tuning and the experimental setup. Overall, the paper exhibits a commendable level of novelty, although further empirical validation would enhance its significance.\n\n# Summary Of The Review\nThis paper introduces a promising new methodology for improving machine learning model performance, underpinned by strong theoretical insights. While the contributions are noteworthy, the paper would benefit from more comprehensive experimental validation and improved comparisons with existing methods to fully establish the significance of the proposed approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"Removing Structured Noise with Diffusion Models\" addresses the challenge of ill-posed inverse problems that arise when recovering signals from noisy measurements. The authors propose a novel joint conditional reverse diffusion process to effectively handle structured noise, which is a significant advancement over traditional methods that primarily rely on sparsity or unstructured noise models. The methodology leverages score-based diffusion models, demonstrating marked performance improvements compared to existing approaches, such as normalizing flows and adversarial networks. Key findings indicate that this new framework not only enhances signal recovery but also provides insight into potential applications of diffusion models in scenarios involving non-Gaussian measurements.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to extending diffusion models for structured noise, providing a clear and compelling methodology that outperforms prior techniques. The empirical results are robust and indicate a strong performance on diverse datasets, indicating the practical applicability of the proposed method. However, a notable weakness is the lack of real-time inference capabilities, which limits its immediate utility in time-sensitive applications compared to GANs and classical methods. Furthermore, the paper could benefit from a more comprehensive exploration of potential limitations and challenges associated with the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, and the methodology is well-articulated, allowing readers to grasp the underlying concepts and the significance of the contributions easily. The quality of the experiments appears solid, with adequate comparisons to baseline methods that validate the authors' claims. In terms of novelty, the approach is distinct, particularly the application of diffusion models to structured noise, which has not been extensively explored in prior works. Reproducibility is likely high, given the detailed description of the methodologies employed; however, providing access to code or datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the realm of inverse problems, particularly in handling structured noise through innovative diffusion models. While the methodology and empirical results are commendable, the limitation regarding real-time inference needs to be addressed in future work. The findings suggest promising avenues for further research and application.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to structured noise removal in ill-posed inverse problems through a joint conditional reverse diffusion process. The authors introduce a new sampling method that leverages deep generative models, specifically diffusion models, to enhance robustness and performance over traditional methods such as normalizing flows and GANs. The methodology involves a joint conditional posterior sampling framework with separate score models for signal and noise, showing significant performance improvements in quantitative metrics (PSNR, SSIM) and qualitative assessments on datasets like CelebA and MNIST.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to combining the robustness of diffusion models with structured noise removal, yielding superior performance compared to existing methods. The introduction of a joint conditional posterior sampling framework is particularly notable, as it enables better handling of structured noise compared to traditional Gaussian assumptions. However, a potential weakness is that the experiments are limited to specific datasets (CelebA and MNIST), which may not fully capture the generalizability of the proposed method across various applications and noise types.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it relatively easy to follow the methodology and results. The quality of writing is high, with a coherent flow from introduction to conclusion. The novelty of the approach is highlighted through a well-defined problem statement and a comprehensive comparison to existing techniques, particularly in the context of structured noise. The authors provide sufficient details regarding their methodology and results, and they ensure reproducibility by including code and experimental setups, which is commendable.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of structured noise removal using diffusion models. It demonstrates clear improvements over existing methods, though the reliance on specific datasets may limit its broader applicability. The methodology is well-documented, enhancing reproducibility and clarity.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Removing Structured Noise with Diffusion Models\" addresses the challenge of structured noise in signal and image processing, proposing a novel approach using diffusion models. The authors introduce a joint conditional posterior sampling methodology that allows for the recovery of signals corrupted by arbitrary noise distributions, moving beyond the limitations of traditional i.i.d. Gaussian noise models. The experimental evaluation on datasets such as CelebA and MNIST demonstrates the efficacy of the proposed method, achieving superior performance in terms of both quantitative metrics (PSNR, SSIM) and qualitative assessments compared to existing approaches, including GANs and traditional methods.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative application of diffusion models to structured noise removal, a relatively underexplored area. The methodology is well-articulated, with a clear mathematical formulation and detailed algorithmic steps, supporting reproducibility. The experimental results convincingly illustrate the advantages of the proposed approach over traditional methods. However, a potential weakness is the reliance on synthetic datasets, which may limit the generalizability of the results to real-world scenarios. Further exploration into real-time inference and other sampling algorithms could enhance the practical applicability of the method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers with varying levels of expertise in signal processing and machine learning. The novelty of employing diffusion models for structured noise is significant, marking a meaningful contribution to the field. The authors provide a comprehensive description of their methodology and experimental setup, along with a link to the code repository, which facilitates reproducibility. Overall, the paper meets high standards of clarity and quality.\n\n# Summary Of The Review\nIn summary, this paper presents a compelling approach to removing structured noise using diffusion models, demonstrating strong empirical results. The methodology is well-defined, with contributions that advance the state-of-the-art in signal processing. While promising, further validation on real-world datasets would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a framework for addressing ill-posed inverse problems through the removal of structured noise using score-based diffusion models. It transitions from traditional handcrafted priors to data-driven approaches, particularly focusing on a joint conditional reverse diffusion process that integrates structured noise models with learned score functions. The methodology demonstrates empirical superiority over existing techniques like normalizing flows and GANs, especially in contexts involving non-Gaussian measurements. The experiments, conducted on datasets such as CelebA and MNIST, reveal significant improvements in performance metrics (PSNR and SSIM) and efficiency, showcasing a 50x reduction in inference time compared to flow-based methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to structured noise removal, effectively utilizing score-based diffusion models to outperform traditional methods. The mathematical formalism presented is thorough and well-structured, providing a clear foundation for the proposed methodology. However, the paper could benefit from a more intuitive explanation of its concepts, particularly for practitioners who may not be deeply familiar with the underlying mathematical details. Additionally, while the empirical results are promising, further comparisons with a broader range of existing methods could strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a logical progression of ideas. The quality of the mathematical exposition is high, demonstrating a strong grasp of the subject matter. However, the clarity could be enhanced by incorporating more visual aids or intuitions to help readers grasp complex concepts more easily. The novelty of the approach is significant, as it combines learned score functions with diffusion processes for a robust solution to structured noise problems. The reproducibility of the work is supported by the clear description of methodologies and experiments, although providing code or more detailed experimental setups would further aid in this regard.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of inverse problems by effectively leveraging score-based diffusion models for structured noise removal. While the methodology is robust and the results are promising, the clarity of presentation could be improved to facilitate understanding among a wider audience.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel approach to removing structured noise using diffusion models, introducing a joint conditional posterior sampling method. The authors claim that their methodology offers improvements over existing techniques. However, the performance is primarily demonstrated through subjective qualitative comparisons, with insufficient quantitative metrics provided. The work relies on specific datasets, namely CelebA and MNIST, to evaluate its effectiveness.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of diffusion models for noise removal, which contributes to the ongoing exploration of generative models in this domain. However, several weaknesses undermine its contributions: the lack of a compelling motivation for the proposed method renders it somewhat redundant; the complexity of the methodology may lead to inefficiencies; and the performance claims are not robustly supported by quantitative evidence. Furthermore, the reliance on limited datasets raises concerns regarding the generalizability of the findings, while the discussion on computational costs is insufficient.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from issues in clarity, particularly concerning the complexity of the proposed method, which may hinder reproducibility. While the novelty in applying diffusion models to noise removal is noted, the lack of a thorough exploration of alternative sampling algorithms diminishes the perceived significance. The evidence provided for the robustness of the method against out-of-distribution signals is also lacking, which could affect the reliability of the findings.\n\n# Summary Of The Review\nOverall, this paper presents a potentially interesting approach but fails to provide compelling motivation, robust empirical validation, or clear practical implications. The complexity of the methodology and the limited dataset reliance further detract from its contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel approach to addressing ill-posed inverse problems through the application of diffusion models, specifically a joint conditional reverse diffusion process. This method significantly enhances the recovery of signals obscured by structured noise, outperforming traditional models such as normalizing flows and GANs. The authors leverage data-driven priors and score-based diffusion models to achieve robust signal recovery, demonstrating impressive empirical results across various challenging scenarios.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its innovative sampling method, which represents a substantial advancement in the treatment of structured noise. The integration of score-based diffusion models not only enhances performance but also expands the applicability of generative models in real-world settings. However, the paper could improve by providing a more detailed comparison with other state-of-the-art methods and addressing potential limitations in the generalization of their approach, especially under diverse data distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its methodology and findings clearly, making it accessible to both experts and newcomers in the field. The quality of the experimental results is high, with a clear demonstration of improvements using metrics such as PSNR and SSIM. The novelty is evident in the proposed joint conditional posterior sampling method, which showcases significant advancements over existing techniques. Reproducibility appears feasible, given the thoroughness of the experimental setup, although additional details on implementation would bolster this aspect.\n\n# Summary Of The Review\nThis paper presents a significant advancement in the application of diffusion models to inverse problems, demonstrating both theoretical and empirical contributions. While the methodology is innovative and results are impressive, a deeper exploration of limitations and comparisons with existing methods would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper investigates the utilization of diffusion models for addressing ill-posed inverse problems, particularly in the context of structured noise. It proposes a joint conditional reverse diffusion process that integrates structured noise models into the diffusion framework, thus enhancing the ability to model and sample posterior distributions of noisy observations. The methodology revolves around score matching and stochastic differential equations (SDEs), demonstrating that well-trained score models can effectively navigate data distributions, leading to improved signal recovery in comparison to traditional methods. The findings indicate that the proposed approach expands the applicability of diffusion models beyond Gaussian assumptions, offering significant theoretical advancements in generative modeling.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous theoretical framework, which provides valuable insights into the role of priors in Bayesian inference and the theoretical underpinnings of diffusion models. The introduction of a joint conditional reverse diffusion process is a notable contribution that addresses the shortcomings of classical methods in handling structured noise. However, a weakness lies in the practical implementation of the proposed methods, as they may still be less efficient compared to traditional techniques. The discussion on future research avenues, while promising, could benefit from more concrete examples or preliminary results to illustrate potential applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and articulates complex theoretical constructs effectively, ensuring clarity for readers familiar with the field. The quality of the theoretical insights is high, and the novelty of proposing a joint conditional approach is significant within the context of generative modeling. However, reproducibility may be a concern, as the paper does not provide detailed algorithmic steps or empirical validation of the theoretical claims, which could hinder practical application by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical framework for applying diffusion models to structured noise removal in inverse problems. While the contributions are innovative and significant, the practicality of implementation and reproducibility could be further addressed to enhance the impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Removing Structured Noise with Diffusion Models\" presents a novel approach utilizing score-based diffusion models to effectively recover data from structured noise. The methodology involves training two independent score models on datasets for signal and noise, employing the NCSNv2 architecture and a variance-preserving stochastic differential equation (SDE) to define the diffusion trajectory. Experimental results on CelebA and MNIST datasets demonstrate that the proposed method significantly outperforms baseline approaches, including flow-based models and GANs, in terms of performance metrics such as PSNR and SSIM, while also showcasing notable efficiency in inference time.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear formulation of the problem and the structured approach to noise removal using diffusion models, which is a relatively underexplored area. The use of detailed pseudo-code and the availability of code on an open-source platform enhance reproducibility and facilitate further exploration by other researchers. However, a notable weakness is the reliance on specific datasets for evaluation, which may limit the generalizability of the findings. Additionally, while the inference time is significantly reduced compared to flow-based methods, the overall computational efficiency could be further optimized.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, experimental setup, and results. The quality of the writing is high, making it accessible to readers with varying levels of familiarity with diffusion models. The novelty of the approach lies in its specific application to structured noise removal, which has not been extensively addressed in existing literature. The reproducibility of the results is supported by the provided code and detailed implementation notes, allowing other researchers to replicate the experiments.\n\n# Summary Of The Review\nOverall, this paper provides a significant contribution to the field of noise removal using diffusion models, demonstrating both technical rigor and empirical effectiveness. While there are areas for improvement regarding the generalizability of results and computational efficiency, the clarity and reproducibility of the work are commendable.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel joint conditional posterior sampling method utilizing diffusion models, aimed at enhancing performance in structured noise scenarios. The authors assert that their method outperforms existing techniques, specifically normalizing flows (NFs) and generative adversarial networks (GANs), based on empirical evaluations. However, the paper primarily focuses on comparative performance without adequately addressing the unique contributions of the proposed method or providing a thorough contextual analysis of the existing methods' strengths and weaknesses.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of a novel sampling method that leverages diffusion models, which is an emerging area of interest in generative modeling. The authors claim significant performance gains over traditional methods. However, the weaknesses are pronounced; the paper lacks a balanced comparison with NFs and GANs, failing to recognize the specific capabilities of these methods in handling structured noise. This leads to potentially biased conclusions. Moreover, the analysis of performance metrics is superficial, lacking depth in contextualizing the results, and there is insufficient discussion on the implications of slower inference times associated with diffusion models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by its lack of comprehensive analysis, particularly in terms of the trade-offs involved in the proposed method compared to existing techniques. While the methodology appears sound, the findings are presented without adequate justification or detailed exploration, which detracts from the overall quality. The novelty of the approach is somewhat overstated due to the lack of acknowledgment of existing foundational work in the field. Reproducibility is not explicitly addressed, and the paper would benefit from providing more detailed descriptions of experiments and datasets used.\n\n# Summary Of The Review\nOverall, the paper introduces a promising joint conditional posterior sampling method using diffusion models; however, it suffers from a lack of balanced comparisons with existing methods. The claims of superiority are not sufficiently substantiated, and the nuanced advantages of traditional methods are overlooked, leading to an overstated perception of novelty.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Removing Structured Noise with Diffusion Models\" presents a novel approach to addressing the challenge of structured noise in data using diffusion models. The authors propose a methodology that integrates maximum a posteriori (MAP) estimation and conditional sampling techniques to effectively denoise images impacted by structured noise. Their empirical findings demonstrate that the proposed method outperforms existing state-of-the-art techniques, including advanced text-to-image models like DALL-E 2, in terms of denoising quality and robustness.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative use of diffusion models for noise removal, which is a relatively unexplored area in the domain of image processing. The methodology is well-articulated, with clear mathematical formulations and a well-structured algorithm. However, the paper has weaknesses, including unclear definitions of key terms, inconsistent notation, and a lack of clarity in certain equations and sections. Additionally, the empirical results, while promising, could be strengthened with more extensive comparisons to a broader range of existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by several typographical and formatting issues, which detract from the overall quality of the presentation. The novelty of the approach is significant, particularly in its application of diffusion models to structured noise, but the inconsistent notation and lack of clear definitions may hinder reproducibility for readers unfamiliar with the concepts. The authors should address these clarity issues to enhance the accessibility of their work.\n\n# Summary Of The Review\nOverall, the paper offers a promising approach to structured noise removal using diffusion models, with strong empirical results indicating its effectiveness. However, clarity and consistency in notation need improvement to ensure that the methodology is easily understood and reproducible. Addressing these issues would significantly enhance the paper's overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to handling structured noise in diffusion models through the introduction of a joint conditional reverse diffusion process. The authors conduct experiments primarily on the CelebA and MNIST datasets, demonstrating that their method can effectively reduce structured noise in images. However, the exploration of the methodology's applicability to other types of noise or generative models is limited, and the findings are primarily evaluated using quantitative metrics without qualitative assessments.\n\n# Strength And Weaknesses\nThe paper makes significant contributions to the field of diffusion models by proposing a joint conditional reverse diffusion methodology, which is a notable advancement. However, it has several weaknesses, including a narrow focus on specific noise types and datasets, which limits the generalizability of the findings. The lack of discussion on the applicability of the proposed method to other generative models, as well as the absence of qualitative evaluation and consideration of computational limits, weakens the overall impact. Additionally, ethical implications and potential biases are not addressed, which are critical considerations in current machine learning research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly. While the technical quality is acceptable, the novelty is somewhat limited by the narrow scope of the experiments and the lack of exploration into other noise types or applications. Reproducibility could be improved by providing more details on the experimental setup and the implications of the results for future research directions.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the study of structured noise in diffusion models, but its impact is hindered by a narrow focus and limited exploration of broader applications and implications. While the methodology shows promise, a more comprehensive evaluation and discussion of its limitations are needed to enhance the paper's significance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing ill-posed inverse problems through the integration of Bayesian inference and structured noise modeling. The authors propose a joint optimization framework that combines signal and noise estimation using score-based diffusion models. The methodology emphasizes the importance of selecting appropriate statistical priors, with empirical evaluations demonstrating significant improvements in signal recovery performance compared to established baselines. The results are substantiated through rigorous statistical testing, showcasing the robustness of the proposed method across various noise structures and out-of-distribution scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its comprehensive exploration of statistical methodologies for inverse problem solving, particularly the incorporation of structured noise models, which is often overlooked in existing literature. The use of joint optimization for signal and noise estimation presents an innovative approach that is well-grounded in statistical theory. However, a potential weakness is the reliance on complex statistical models, which may limit accessibility for practitioners not deeply versed in statistical inference. Additionally, while the empirical results are promising, further exploration of the model's performance under diverse real-world conditions could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that guides the reader through the theoretical foundations, methodology, and experimental results. The quality of the writing is high, and the statistical rigor applied throughout enhances the reliability of the conclusions drawn. The novelty of the approach is significant, particularly in the context of structured noise modeling. Reproducibility is supported by detailed descriptions of the experimental setups and statistical tests used, although access to code or datasets would further facilitate reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a well-structured and statistically grounded approach to solving inverse problems through Bayesian inference and noise modeling. While the contributions are significant and novel, the complexity of the methodology may pose challenges for broader application. Further exploration of real-world applicability would enhance the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel method for generative modeling based on diffusion processes. The authors propose a framework that utilizes structured noise models to improve sample generation quality compared to existing methods, particularly GANs. The methodology employs the Euler-Maruyama sampling technique and demonstrates some improvements on benchmark datasets, namely CelebA and MNIST, in terms of image fidelity and diversity.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to leveraging diffusion processes for generative modeling, which shows promise in generating high-quality samples. However, the method is currently not suitable for real-time inference, being slower than both GANs and classical methods. The exploration of more advanced sampling algorithms is notably lacking, limiting the potential for performance enhancements. Additionally, the applicability of the method is confined to structured noise models, with no investigation into its performance on non-linear likelihoods or complex noise distributions. Furthermore, the analysis of the computational efficiency is insufficient, and there is a notable absence of a thorough examination of hyperparameter tuning issues and the limitations of noise assumptions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the proposed method and experimental results. However, the novelty is somewhat tempered by the lack of exploration into advanced sampling techniques and the connection to continuous normalizing flows, which could have provided deeper insights. Reproducibility is also a concern, as the methodology is only tested on a limited set of datasets, raising questions about its generalizability.\n\n# Summary Of The Review\nOverall, while the paper presents a promising approach to generative modeling with diffusion processes, it has significant limitations in terms of efficiency, applicability, and depth of analysis. Future work should aim to address these weaknesses, particularly through the exploration of advanced sampling techniques and broader applicability to various datasets.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Removing Structured Noise with Diffusion Models\" proposes an extension to diffusion models specifically aimed at addressing the challenges posed by structured noise in signal recovery tasks. The authors introduce a \"joint conditional sampling\" method, which they claim enhances the performance of generative models in noisy environments. The methodology is based on established principles in signal processing and draws comparisons to existing methods like normalizing flows (NF) and Generative Adversarial Networks (GAN). The experiments conducted on well-known datasets such as CelebA and MNIST yield marginal improvements in performance metrics, although the authors acknowledge that their method is not yet suitable for real-time applications.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its attempt to adapt diffusion models for structured noise, a known issue in the field. However, the paper exhibits significant weaknesses, including a lack of novelty in both methodology and experimental validation. The authors do not introduce any groundbreaking concepts or frameworks but instead rehash existing ideas, failing to provide substantial advancements over current state-of-the-art approaches. Their experimental results, while slightly improved, do not offer compelling evidence of the proposed method's superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a logical flow from introduction to conclusion. However, the novelty of the contributions is weak, as many of the concepts discussed are already established in the literature. The authors provide links to their code, which enhances reproducibility, though this is now a standard expectation in the field rather than an exceptional feature. Overall, while the clarity of the writing is adequate, the lack of innovative content diminishes the paper's impact.\n\n# Summary Of The Review\nThis paper attempts to address the challenge of structured noise in generative modeling using diffusion models, but it ultimately falls short of making significant contributions to the field. The methodology and findings are largely derivative, and while the experiments yield some improvements, they do not convincingly advance the state of the art.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to structured noise in diffusion models, emphasizing the potential of latent space representations and joint conditional posterior sampling. The authors propose a methodology that integrates advanced sampling techniques and explores the relationship between diffusion models and continuous normalizing flows. Key findings indicate that their approach enhances signal recovery quality while maintaining robustness across specific datasets such as CelebA and MNIST.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to structured noise and the thorough exploration of posterior sampling techniques, which provide a solid foundation for further research in generative modeling. However, the paper lacks depth in application-specific adaptations and does not sufficiently evaluate the model's robustness across diverse datasets. Additionally, while the exploration of hybrid architectures is suggested, it remains largely unexplored within the paper, indicating a missed opportunity for further methodological advancements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, making it accessible to a wide audience. The quality of the methodology is high, though the novelty, while present, could be further enhanced by deeper explorations of suggested future work, such as non-linear noise models and hybrid architectures. The authors have made efforts to ensure reproducibility by providing code, which is commendable; however, encouraging community engagement for collaborative improvements could significantly bolster the reproducibility and applicability of their findings.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the field of diffusion models by introducing structured noise handling and advanced sampling techniques. While it highlights significant potential for further developments, particularly in real-time inference and application-specific adaptations, some areas require more exploration to realize the full impact of the proposed methodologies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces a novel diffusion-based method for addressing various inverse problems characterized by structured noise. The methodology focuses on leveraging diffusion processes to enhance performance metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) across different experiments, including \"Removing MNIST Digits\" and compressed sensing with sinusoidal noise. The findings demonstrate that the proposed method outperforms competitive baselines, including normalizing flows (NFs) and generative adversarial networks (GANs), not only in terms of performance but also in computational efficiency, achieving a significant speed advantage during inference.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical results, showcasing significant performance gains in various challenging tasks. The proposed diffusion method consistently outperformed both NFs and GANs, indicating its effectiveness and robustness, particularly against out-of-distribution data. However, a potential weakness is the lack of extensive theoretical analysis or insights into the underlying mechanisms driving the diffusion method's performance, which could provide a deeper understanding of its advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and results, making it accessible to readers. The quality of the data and experiments appears robust, with detailed benchmarks supporting the claims. The novelty of the approach is noteworthy, as it introduces a diffusion-based model that achieves superior results compared to established methods. However, the reproducibility of the results depends on the availability of code and datasets, which should be explicitly mentioned for full transparency.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for the efficacy of a diffusion-based method in tackling inverse problems with structured noise, demonstrating clear performance improvements over existing state-of-the-art techniques. While the empirical findings are impressive, further theoretical insights would enhance the understanding of the method's advantages.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to generative modeling through a deep generative framework that integrates maximum a posteriori (MAP) estimation with a conditional reverse diffusion process. The authors claim that their method improves upon traditional generative models by enhancing the quality of generated samples and providing a robust framework for various applications. The methodology is evaluated through extensive experiments, demonstrating superior performance in quantitative metrics such as PSNR and SSIM compared to baseline models.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of MAP estimation with diffusion processes, which offers a fresh perspective on generative modeling. The empirical results are compelling, showcasing significant improvements over existing methods. However, weaknesses include the dense and technical language that may hinder accessibility for a broader audience. Additionally, some sections lack clarity, particularly the algorithm description, which could benefit from a more structured presentation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is a notable concern, as the abstract and introduction are overly complex, making it challenging for readers to grasp the core contributions quickly. The quality of the visuals and figures could be improved by explicitly referencing them in the text and summarizing their insights. While the technical novelty is commendable, the reproducibility of the results could be enhanced by providing a more detailed exposition of the algorithm and the metrics used for evaluation.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in generative modeling, but it suffers from clarity and accessibility issues. Enhancements in the presentation and explanation of the methodology would greatly improve its impact and comprehensibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.1948345773361515,
    -1.7186822074007768,
    -1.7127311005849406,
    -1.7734302761467833,
    -1.8721800269141418,
    -1.7853342627226447,
    -1.5877604091097075,
    -1.8302563325134273,
    -1.6936053674453877,
    -1.8861300927566085,
    -1.775695870890618,
    -1.511908190800112,
    -1.8078000060644654,
    -1.6858856052064022,
    -1.601985526613546,
    -1.6628835990280626,
    -2.040118095588512,
    -1.8099016831322514,
    -1.880571770261853,
    -1.529247516906422,
    -1.907157062675231,
    -1.5490216703757023,
    -1.6493344289564873,
    -1.5590986412538932,
    -1.805556930636221,
    -1.7723842556380633,
    -1.989209446711805,
    -1.6851275905797745,
    -1.5858644237406776
  ],
  "logp_cond": [
    [
      0.0,
      -1.9623709870218582,
      -1.975322594023572,
      -1.9640299118341467,
      -1.9666669406153887,
      -1.9743778849112863,
      -1.9924457805988447,
      -1.9824487055735676,
      -1.976985933827927,
      -2.00631010821645,
      -1.9723204078268912,
      -2.043580368130436,
      -1.9725574307804548,
      -1.9490202881001815,
      -1.9975398169604979,
      -1.9796052651643843,
      -2.000159190012511,
      -1.9641172713903359,
      -1.9923173022014327,
      -1.9476895021753164,
      -1.9911646033725845,
      -2.0095375877489325,
      -2.0044069950634467,
      -1.9884753030151856,
      -2.0279839503296326,
      -1.9798803210876106,
      -2.005281713561435,
      -1.9649326714766764,
      -2.04457444128328
    ],
    [
      -1.366333269897386,
      0.0,
      -1.2107451728339267,
      -1.2091333901104615,
      -1.2529121785612216,
      -1.183431043072687,
      -1.315581272190264,
      -1.3136119999959712,
      -1.2715333658028476,
      -1.3767557203255218,
      -1.1771469176309683,
      -1.435924790901547,
      -1.2153999903771422,
      -1.225386671507081,
      -1.2867774080610306,
      -1.2549412134529874,
      -1.3507738308163637,
      -1.250806066428265,
      -1.2514901175460629,
      -1.3363158497409093,
      -1.3201354101973317,
      -1.4089452886185465,
      -1.330036121378126,
      -1.393063522476745,
      -1.320860957328475,
      -1.2242142070893474,
      -1.3108507711982205,
      -1.382852850816632,
      -1.3981865961273308
    ],
    [
      -1.4076351050790075,
      -1.294929481228854,
      0.0,
      -1.2781455700551005,
      -1.214053875252749,
      -1.278252632650662,
      -1.365086773282258,
      -1.3518630669533336,
      -1.3407966148455035,
      -1.3868149104909906,
      -1.2866890439427683,
      -1.4638581620766613,
      -1.292265271615783,
      -1.3242964171958989,
      -1.3322030956854756,
      -1.325604218405752,
      -1.3638459695599234,
      -1.3400099692772849,
      -1.312921841152541,
      -1.357959426934325,
      -1.3552861245231405,
      -1.3969760080357594,
      -1.3838182802674501,
      -1.421257046964266,
      -1.3781105582995765,
      -1.378849159998715,
      -1.4041498384241484,
      -1.3807778171575098,
      -1.4615891029985166
    ],
    [
      -1.4449337353178515,
      -1.2893415273412399,
      -1.3003413748474515,
      0.0,
      -1.3527210913859424,
      -1.367820163749295,
      -1.3362478597452476,
      -1.3011770071054862,
      -1.3383416562110984,
      -1.4148631634768165,
      -1.287263618069752,
      -1.4935038517634562,
      -1.313586205603898,
      -1.3019926743022867,
      -1.3659306198382564,
      -1.3048009138898622,
      -1.3640791631849793,
      -1.3459217832513037,
      -1.3621574058934116,
      -1.3701396617172117,
      -1.345078761890056,
      -1.4103036159470739,
      -1.3974667282875208,
      -1.3926004030390884,
      -1.447092433070642,
      -1.3619854344248838,
      -1.4230855483780585,
      -1.3991195484050039,
      -1.4463174784395807
    ],
    [
      -1.4808523225858932,
      -1.3645619747504807,
      -1.2987672123119434,
      -1.3709228261496906,
      0.0,
      -1.3206316535371496,
      -1.4385637671438634,
      -1.394661827843819,
      -1.3384631132562872,
      -1.4735128966108784,
      -1.3662207094349887,
      -1.5246727591897358,
      -1.3978703575483882,
      -1.3576796092931451,
      -1.3927997365809508,
      -1.4398102779987962,
      -1.3531768696923852,
      -1.3744756759394012,
      -1.4089149149090188,
      -1.4502453381160398,
      -1.3539461307909915,
      -1.4712842490578595,
      -1.408060673203666,
      -1.4720804217001435,
      -1.4124126580287022,
      -1.4250209726573975,
      -1.4253233618719658,
      -1.4352850452242756,
      -1.489278735727781
    ],
    [
      -1.4565482064210642,
      -1.2941829165219765,
      -1.3231570417903227,
      -1.3375934195040762,
      -1.324588076689696,
      0.0,
      -1.3641827774217197,
      -1.37259775232422,
      -1.3199646570607697,
      -1.4739598366284232,
      -1.2989633624656596,
      -1.5155040913426288,
      -1.3052184284944603,
      -1.3092372457396952,
      -1.4469785876241579,
      -1.3235753331436804,
      -1.4392796445910216,
      -1.3533903594914005,
      -1.3390388828167397,
      -1.4037902467620258,
      -1.4224724884312656,
      -1.5000572747343186,
      -1.4010677087228713,
      -1.444637975843654,
      -1.4468615605285973,
      -1.4180448765781457,
      -1.4457462132063668,
      -1.4635116528534102,
      -1.4866650489316704
    ],
    [
      -1.2783697090383541,
      -1.1908635266958838,
      -1.2221122149171644,
      -1.1958711464014682,
      -1.2485092334405403,
      -1.136136981144568,
      0.0,
      -1.2266466526469215,
      -1.1470425427874316,
      -1.2716609017728395,
      -1.2064962012613105,
      -1.32253437640371,
      -1.1868420155703003,
      -1.167098468527528,
      -1.188758186150849,
      -1.1634927339761656,
      -1.2479019159497131,
      -1.2124818841613711,
      -1.238480333552867,
      -1.2225245526510293,
      -1.2501279982850246,
      -1.271211728945965,
      -1.2571042824347078,
      -1.2547324181355666,
      -1.2990931229231377,
      -1.2447178005594595,
      -1.2584420270687697,
      -1.2465854728831662,
      -1.2952655651292542
    ],
    [
      -1.4935357199921941,
      -1.3426276079078034,
      -1.3653248406514407,
      -1.347692959581062,
      -1.4261875798278105,
      -1.3785051807012265,
      -1.3957044929430142,
      0.0,
      -1.3237324995916064,
      -1.4924230470610773,
      -1.3489037455241677,
      -1.5259947002999155,
      -1.3587351706018487,
      -1.3246383462220062,
      -1.4026596692621183,
      -1.3674817941725896,
      -1.4154737236194035,
      -1.3767852212925982,
      -1.3852811389103352,
      -1.4486530244269447,
      -1.397252607457231,
      -1.4518773152311033,
      -1.4280433840919953,
      -1.4685055561467293,
      -1.4484616191852153,
      -1.4030488218042216,
      -1.463590753837058,
      -1.513370389309145,
      -1.5270044062043358
    ],
    [
      -1.3602477062985743,
      -1.2301867828157425,
      -1.270232716416709,
      -1.2287773572493383,
      -1.25083982999127,
      -1.2129568479269492,
      -1.2207840616861432,
      -1.2066272355204908,
      0.0,
      -1.3687730881281925,
      -1.2603533226905628,
      -1.3978698730378472,
      -1.1796211483887897,
      -1.2183072592915027,
      -1.3080612010634398,
      -1.2817495681346178,
      -1.2154065434023076,
      -1.2643587396707263,
      -1.2816476866359519,
      -1.2707926495449877,
      -1.29560110982399,
      -1.3436847694957919,
      -1.271306646325534,
      -1.3277166139974022,
      -1.3339606881863366,
      -1.2797098694160662,
      -1.3353792141902876,
      -1.282954916154814,
      -1.3467714419845638
    ],
    [
      -1.561688660457495,
      -1.4614662032823076,
      -1.4734088351653145,
      -1.5035320055167465,
      -1.4931963658004306,
      -1.451818684909912,
      -1.460480438086002,
      -1.4510973858648395,
      -1.4605298558465722,
      0.0,
      -1.4582283093042914,
      -1.5153384270269576,
      -1.4964868292867504,
      -1.4510141315507636,
      -1.4772496509810535,
      -1.496627645528863,
      -1.4495491079723366,
      -1.451882777459884,
      -1.4757753113372358,
      -1.5386726390567826,
      -1.481730361159478,
      -1.5441259276804427,
      -1.4365902474233643,
      -1.5302155435381433,
      -1.4736429573044867,
      -1.469733306038152,
      -1.4841063374166088,
      -1.5371237437044027,
      -1.5005583933648832
    ],
    [
      -1.4445582136917516,
      -1.326033363243196,
      -1.3326842898685787,
      -1.4012106251133423,
      -1.4273450378111325,
      -1.368440181621276,
      -1.404287807692973,
      -1.3837185013089661,
      -1.3565413173346121,
      -1.48247614855659,
      0.0,
      -1.5186776687006214,
      -1.3431804423921856,
      -1.3324944972822996,
      -1.39269425739806,
      -1.4024116748060622,
      -1.4544467600186313,
      -1.3749479031122431,
      -1.409872759171964,
      -1.4504198395701609,
      -1.4185351375974167,
      -1.499996008147086,
      -1.4255940978483268,
      -1.4839602667885567,
      -1.4679288397109087,
      -1.378933224450221,
      -1.4348654948245487,
      -1.4799971441865647,
      -1.4876548586984355
    ],
    [
      -1.2822849618567649,
      -1.260855448426706,
      -1.2429353036727087,
      -1.276707683315906,
      -1.2621718953315304,
      -1.2323326227009967,
      -1.2531696781473636,
      -1.247541445048324,
      -1.2673439002073008,
      -1.2366297348067117,
      -1.2572439866098912,
      0.0,
      -1.2565222290992375,
      -1.2265665249705981,
      -1.2605218461392296,
      -1.2821141788905914,
      -1.273804595493376,
      -1.2609520745968663,
      -1.2441880434388284,
      -1.2807702382405544,
      -1.2413208990409028,
      -1.257581833945338,
      -1.2619611806764324,
      -1.2620181302965892,
      -1.2446834345383502,
      -1.246720998098692,
      -1.2496953463260858,
      -1.2692149141530493,
      -1.2551087317295095
    ],
    [
      -1.4997980656441485,
      -1.3526952589556174,
      -1.3883474946985832,
      -1.4102622407702428,
      -1.4430739458785782,
      -1.3675012973282379,
      -1.380703252686819,
      -1.4184376822766491,
      -1.3963098440769763,
      -1.509465942701601,
      -1.3732021749152843,
      -1.54666700228892,
      0.0,
      -1.391802834982799,
      -1.4441681480086896,
      -1.363028099734521,
      -1.5008068208055914,
      -1.3629974074274103,
      -1.3887516408470233,
      -1.4581710115583797,
      -1.4267476072839287,
      -1.4985400005150407,
      -1.4343751069357626,
      -1.4419644310117568,
      -1.4416908019038386,
      -1.415596573463377,
      -1.4382011929443341,
      -1.4731399925236814,
      -1.5202125433968867
    ],
    [
      -1.3639901521661895,
      -1.2064508125114572,
      -1.2700893572687042,
      -1.221691133550247,
      -1.2988828076935164,
      -1.24324062556783,
      -1.2546044459470729,
      -1.2432079467281238,
      -1.208371581253086,
      -1.3669317038814544,
      -1.1839307027427357,
      -1.3914467254258287,
      -1.2724039339520885,
      0.0,
      -1.2161238299191042,
      -1.225619438317287,
      -1.2638232571644312,
      -1.2449590844757776,
      -1.2784866294312476,
      -1.2410219061400405,
      -1.270018607228075,
      -1.3549118812435124,
      -1.274539091497452,
      -1.3807971005696131,
      -1.363029488702845,
      -1.239951475466288,
      -1.2612248154520527,
      -1.3663987723841153,
      -1.3966921335637363
    ],
    [
      -1.3231571540080136,
      -1.1908000605424038,
      -1.1808087766800648,
      -1.2167004316082566,
      -1.2248209337604112,
      -1.247146195584981,
      -1.1627502755545793,
      -1.2386888165553984,
      -1.1862251977118727,
      -1.2828084699234612,
      -1.1733236242715481,
      -1.3372829250241944,
      -1.2005323670871113,
      -1.1810082357561893,
      0.0,
      -1.226815297078369,
      -1.1958988348967692,
      -1.224316213224637,
      -1.2406471862058916,
      -1.18438147485964,
      -1.2267479461888713,
      -1.2417557600664184,
      -1.2752273248011283,
      -1.2759131263011971,
      -1.2640538954496598,
      -1.1909094261676887,
      -1.1766160834057788,
      -1.2890662239102122,
      -1.3305176877184037
    ],
    [
      -1.3385671029717654,
      -1.2578406349822584,
      -1.2792440658517086,
      -1.2642960900381617,
      -1.3136670337917546,
      -1.2513877904807769,
      -1.3073171835541288,
      -1.2815965964456895,
      -1.2587047810767398,
      -1.3861662065976879,
      -1.2791465368140733,
      -1.4170066684668357,
      -1.2234812494860268,
      -1.252822121789754,
      -1.3234084952488312,
      0.0,
      -1.33064414162738,
      -1.2700124740934555,
      -1.2776152494350552,
      -1.2986347764520416,
      -1.302480440356673,
      -1.347804098916964,
      -1.3343645079240594,
      -1.3587830645943428,
      -1.3526642909143431,
      -1.3305206780293128,
      -1.3572094894336655,
      -1.3427550745527634,
      -1.3429328547587205
    ],
    [
      -1.6745848457675712,
      -1.53169667031947,
      -1.5238158057457807,
      -1.5333783031233565,
      -1.4822305844491988,
      -1.5322196640682677,
      -1.5299811211556944,
      -1.5416004199086766,
      -1.4397621593091108,
      -1.598952729242631,
      -1.565419682608198,
      -1.663227364324629,
      -1.569156087288253,
      -1.4752182535813654,
      -1.4124363919279785,
      -1.5749202643429498,
      0.0,
      -1.555497514999017,
      -1.5937286803836768,
      -1.5576466323417986,
      -1.5513648199621104,
      -1.56005573069281,
      -1.5551774080660892,
      -1.6333025219044737,
      -1.5393205057564183,
      -1.5901293974816648,
      -1.514213241959607,
      -1.6729891932112957,
      -1.6775976047032344
    ],
    [
      -1.4405250272684904,
      -1.2890457154606765,
      -1.2698397981063612,
      -1.321571098805959,
      -1.3197208893935537,
      -1.2946702908072898,
      -1.3220007637796753,
      -1.3089602843796788,
      -1.3145095709490713,
      -1.361070329133408,
      -1.327889691089982,
      -1.4688514409390296,
      -1.268359854134574,
      -1.227600216184054,
      -1.4001856792547767,
      -1.2646077508416111,
      -1.320424326630195,
      0.0,
      -1.2518681981287023,
      -1.4131042544712398,
      -1.2862426537252343,
      -1.4182670413539897,
      -1.3617893099923775,
      -1.405813897167896,
      -1.4246769472262304,
      -1.3519627540903763,
      -1.3402375382593648,
      -1.4135072409976095,
      -1.411681440163484
    ],
    [
      -1.5694577282346058,
      -1.43001901915645,
      -1.470636059357918,
      -1.4729181265282465,
      -1.5233204414696868,
      -1.4358657664886152,
      -1.4971072290683545,
      -1.49174427329509,
      -1.5072836710780244,
      -1.5830892980007838,
      -1.4626739381909863,
      -1.609155352409029,
      -1.4235284119301161,
      -1.4366457059533932,
      -1.5469579390287054,
      -1.4372975880700263,
      -1.5522823142990563,
      -1.4807503649127705,
      0.0,
      -1.536904522654464,
      -1.5356216599602133,
      -1.5729842628654098,
      -1.5133747886526134,
      -1.5480846708694338,
      -1.5913240480217337,
      -1.5336047606261753,
      -1.5407119665442193,
      -1.5968073082149457,
      -1.606395199546109
    ],
    [
      -1.1944670062817757,
      -1.129819558759549,
      -1.1259734276368376,
      -1.125101243416096,
      -1.1518738770478627,
      -1.1079699996981247,
      -1.1619332796323085,
      -1.1800017612403308,
      -1.1478512064838216,
      -1.2218806183526052,
      -1.146184498009454,
      -1.2685835476649492,
      -1.1398205564217514,
      -1.1254852561259276,
      -1.1600521085334812,
      -1.133246942675215,
      -1.169968907249799,
      -1.147596262620415,
      -1.1624214999077045,
      0.0,
      -1.1666775793926512,
      -1.1908101198307455,
      -1.1914692767921238,
      -1.2170947533638152,
      -1.1994624118801278,
      -1.1323739013267808,
      -1.1585990797955574,
      -1.1995415805711283,
      -1.2429241990923612
    ],
    [
      -1.6220672417658937,
      -1.5166948991405023,
      -1.565903828670656,
      -1.5242389146438815,
      -1.4990441270305592,
      -1.5580978136093182,
      -1.5655325071701347,
      -1.5305528462317206,
      -1.5453255461666373,
      -1.6285085000884512,
      -1.5428426703369484,
      -1.62004237856404,
      -1.5336936359454119,
      -1.4972034659547337,
      -1.5563187047127176,
      -1.5426518828907367,
      -1.535981361095075,
      -1.4913849385690432,
      -1.586226888306961,
      -1.6015139323056105,
      0.0,
      -1.6279395369276473,
      -1.5603796524784932,
      -1.6018966320131816,
      -1.5976884369112916,
      -1.5478438776226027,
      -1.5357344955741532,
      -1.5886191758305415,
      -1.6306267873530602
    ],
    [
      -1.2436687133567883,
      -1.1545958264585008,
      -1.1701047412133339,
      -1.1364987456593867,
      -1.1660202589879198,
      -1.2087088880216657,
      -1.1350715717028592,
      -1.1881815159564253,
      -1.1603837771639363,
      -1.2179169056414119,
      -1.2042317716837367,
      -1.2425923870930078,
      -1.1663020814359724,
      -1.1629917406910415,
      -1.0767966958378052,
      -1.1947096927331777,
      -1.1721487200314278,
      -1.1758468535656414,
      -1.2018618274943909,
      -1.1634831622972917,
      -1.163416523654995,
      0.0,
      -1.2245462886021639,
      -1.1961955173241317,
      -1.199233135140348,
      -1.1529344498681509,
      -1.1645129906905767,
      -1.22381640708015,
      -1.23870303987815
    ],
    [
      -1.311198593552591,
      -1.2053137954257276,
      -1.2604306362197049,
      -1.2538109200235505,
      -1.3226634108358886,
      -1.2061789798276608,
      -1.2279122354783676,
      -1.2583177436539783,
      -1.2192097218772502,
      -1.3064903724026253,
      -1.2437696362454087,
      -1.3319244532528047,
      -1.2558535495468393,
      -1.211503910029354,
      -1.3007526081121787,
      -1.2562132143851126,
      -1.2619366475603941,
      -1.253681046861459,
      -1.2637887367540117,
      -1.3035858300115888,
      -1.2957275054378319,
      -1.3435963941285662,
      0.0,
      -1.3608737239219488,
      -1.2910515377468361,
      -1.2982466614089296,
      -1.2449588666880056,
      -1.35090863242723,
      -1.3236980433361656
    ],
    [
      -1.2543265674536637,
      -1.1507919664628028,
      -1.1683915578473218,
      -1.1409390638486319,
      -1.1783202677250415,
      -1.1330608840565453,
      -1.1591895495201252,
      -1.223937252764317,
      -1.1764069356478146,
      -1.2361879130779674,
      -1.1830651623872621,
      -1.2523125677749491,
      -1.168076782932733,
      -1.1422385628981666,
      -1.2039306249834338,
      -1.1220095622026898,
      -1.2124107267459903,
      -1.1387459424310766,
      -1.163587110774418,
      -1.1744698029564442,
      -1.190715804177615,
      -1.2344837603669538,
      -1.227873675113125,
      0.0,
      -1.2114859592945415,
      -1.2045124883881553,
      -1.1911078599489489,
      -1.2078464432771245,
      -1.227618375331847
    ],
    [
      -1.5436028599291989,
      -1.4708364115043588,
      -1.512603873835954,
      -1.5138867672944707,
      -1.4478920838591325,
      -1.4704087586229981,
      -1.4999638472221524,
      -1.4989003572904804,
      -1.4865211080847025,
      -1.5425885378316444,
      -1.492850036403305,
      -1.554340099478372,
      -1.4687565310333617,
      -1.4735418488108778,
      -1.46247795047052,
      -1.4833289547902178,
      -1.441408412424086,
      -1.5211990486257911,
      -1.4985025832353913,
      -1.4872274410047457,
      -1.4759899062899282,
      -1.538002704200446,
      -1.4516386040610785,
      -1.5220693640596528,
      0.0,
      -1.489926420423502,
      -1.4086332496326226,
      -1.5154672314983855,
      -1.5162861265042882
    ],
    [
      -1.4653176709174642,
      -1.2945887160879728,
      -1.4136104389051696,
      -1.3567423042686262,
      -1.3929860079612708,
      -1.3740893922468278,
      -1.365271376653391,
      -1.4208244968699852,
      -1.3499129076964425,
      -1.4410388634388311,
      -1.359125178741746,
      -1.5118049825939703,
      -1.3792516737158276,
      -1.347530546081973,
      -1.3473315737663454,
      -1.3884876506427688,
      -1.39720880217783,
      -1.3750025330317992,
      -1.40979585744705,
      -1.384780153841999,
      -1.3963946005678693,
      -1.4283171483266066,
      -1.4592762279522098,
      -1.451376673063766,
      -1.4083457071728156,
      0.0,
      -1.3428258146641279,
      -1.4252814210867468,
      -1.4784461284558312
    ],
    [
      -1.7028962999145305,
      -1.5386468541888783,
      -1.5930652867980803,
      -1.6454575563979146,
      -1.6309920767391175,
      -1.6050044126698464,
      -1.6401327236779881,
      -1.647544396214433,
      -1.674011771015625,
      -1.6629694957300019,
      -1.563613346434366,
      -1.6687157219715303,
      -1.5973166265002365,
      -1.5814498483708301,
      -1.5793485095348545,
      -1.6780522341411355,
      -1.5929804984462117,
      -1.6356711960253874,
      -1.6195973290362307,
      -1.6244299731410787,
      -1.5822210919253095,
      -1.6575934984003662,
      -1.609496314237925,
      -1.6846725272855247,
      -1.514542207816776,
      -1.5554929810177607,
      0.0,
      -1.6916305456715612,
      -1.686478097052219
    ],
    [
      -1.37292117583426,
      -1.3422158511256266,
      -1.3483831200472185,
      -1.2785936547743553,
      -1.3134464135420156,
      -1.3194361143147153,
      -1.35749575402096,
      -1.3781255725718424,
      -1.2793886290137997,
      -1.342152721523254,
      -1.3805906792324258,
      -1.4164388516204722,
      -1.356046035628051,
      -1.3702669326839279,
      -1.3996364153447833,
      -1.3259470838389582,
      -1.3852424635398763,
      -1.3334611096760232,
      -1.3884131253222953,
      -1.3996626267191217,
      -1.3241913512787813,
      -1.4206743422302204,
      -1.410462719141987,
      -1.3835406728648214,
      -1.363990229318724,
      -1.3949178328311722,
      -1.3975863815560905,
      0.0,
      -1.3899498085251354
    ],
    [
      -1.3476159423894254,
      -1.2466875137990225,
      -1.2357825816570238,
      -1.2527498886885013,
      -1.2742421953658924,
      -1.2095650992662355,
      -1.2323430499034354,
      -1.2602834967589456,
      -1.225344494683561,
      -1.254919858451173,
      -1.238370695281033,
      -1.2760585898615584,
      -1.2598432466944036,
      -1.2090724092780918,
      -1.2655310370255788,
      -1.2206731796554826,
      -1.2931532391873308,
      -1.238406890833184,
      -1.2546501553193723,
      -1.2550864607692975,
      -1.2689906545430898,
      -1.2321444643961215,
      -1.2237124603905234,
      -1.2599164695211578,
      -1.2699192100826056,
      -1.2824595223820139,
      -1.2699677486712981,
      -1.2381122550473571,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2324635903142933,
      0.2195119833125796,
      0.23080466550200485,
      0.2281676367207628,
      0.2204566924248652,
      0.2023887967373068,
      0.21238587176258394,
      0.21784864350822453,
      0.18852446911970144,
      0.22251416950926028,
      0.15125420920571564,
      0.22227714655569675,
      0.24581428923597004,
      0.19729476037565363,
      0.21522931217176722,
      0.19467538732364043,
      0.23071730594581563,
      0.20251727513471884,
      0.24714507516083506,
      0.203669973963567,
      0.18529698958721896,
      0.19042758227270484,
      0.2063592743209659,
      0.16685062700651887,
      0.21495425624854092,
      0.18955286377471658,
      0.22990190585947512,
      0.15026013605287147
    ],
    [
      0.3523489375033908,
      0.0,
      0.5079370345668501,
      0.5095488172903153,
      0.4657700288395552,
      0.5352511643280897,
      0.4031009352105128,
      0.4050702074048056,
      0.4471488415979292,
      0.34192648707525497,
      0.5415352897698085,
      0.2827574164992299,
      0.5032822170236346,
      0.4932955358936959,
      0.43190479933974624,
      0.46374099394778945,
      0.3679083765844131,
      0.46787614097251184,
      0.46719208985471394,
      0.3823663576598675,
      0.3985467972034451,
      0.3097369187822303,
      0.3886460860226508,
      0.3256186849240319,
      0.39782125007230174,
      0.49446800031142946,
      0.40783143620255635,
      0.3358293565841448,
      0.32049561127344606
    ],
    [
      0.30509599550593314,
      0.4178016193560865,
      0.0,
      0.43458553052984006,
      0.4986772253321916,
      0.4344784679342786,
      0.3476443273026826,
      0.360868033631607,
      0.3719344857394371,
      0.32591619009395,
      0.4260420566421723,
      0.2488729385082793,
      0.42046582896915763,
      0.38843468338904175,
      0.38052800489946503,
      0.38712688217918867,
      0.34888513102501717,
      0.3727211313076557,
      0.39980925943239964,
      0.3547716736506157,
      0.35744497606180015,
      0.3157550925491812,
      0.3289128203174905,
      0.2914740536206746,
      0.3346205422853641,
      0.33388194058622567,
      0.3085812621607922,
      0.3319532834274308,
      0.25114199758642397
    ],
    [
      0.32849654082893176,
      0.4840887488055434,
      0.4730889012993318,
      0.0,
      0.42070918476084085,
      0.40561011239748823,
      0.43718241640153566,
      0.47225326904129705,
      0.43508861993568493,
      0.35856711266996677,
      0.4861666580770312,
      0.27992642438332704,
      0.45984407054288523,
      0.47143760184449657,
      0.4074996563085269,
      0.4686293622569211,
      0.40935111296180393,
      0.42750849289547954,
      0.41127287025337167,
      0.4032906144295716,
      0.42835151425672735,
      0.3631266601997094,
      0.37596354785926245,
      0.3808298731076949,
      0.3263378430761412,
      0.41144484172189943,
      0.35034472776872483,
      0.3743107277417794,
      0.3271127977072026
    ],
    [
      0.3913277043282486,
      0.5076180521636611,
      0.5734128146021984,
      0.5012572007644511,
      0.0,
      0.5515483733769921,
      0.4336162597702784,
      0.4775181990703228,
      0.5337169136578546,
      0.3986671303032634,
      0.505959317479153,
      0.347507267724406,
      0.4743096693657536,
      0.5145004176209966,
      0.47938029033319096,
      0.4323697489153455,
      0.5190031572217566,
      0.49770435097474053,
      0.463265112005123,
      0.4219346887981019,
      0.5182338961231503,
      0.4008957778562823,
      0.46411935371047575,
      0.40009960521399823,
      0.45976736888543956,
      0.4471590542567443,
      0.4468566650421759,
      0.4368949816898662,
      0.38290129118636074
    ],
    [
      0.32878605630158053,
      0.49115134620066825,
      0.46217722093232205,
      0.44774084321856855,
      0.4607461860329487,
      0.0,
      0.42115148530092505,
      0.4127365103984246,
      0.465369605661875,
      0.3113744260942215,
      0.48637090025698515,
      0.26983017138001597,
      0.4801158342281844,
      0.4760970169829495,
      0.33835567509848685,
      0.46175892957896436,
      0.3460546181316231,
      0.4319439032312442,
      0.44629537990590507,
      0.38154401596061893,
      0.3628617742913791,
      0.2852769879883261,
      0.3842665539997734,
      0.3406962868789907,
      0.33847270219404746,
      0.367289386144499,
      0.33958804951627797,
      0.3218226098692345,
      0.29866921379097433
    ],
    [
      0.3093907000713534,
      0.3968968824138237,
      0.36564819419254313,
      0.3918892627082393,
      0.3392511756691672,
      0.45162342796513943,
      0.0,
      0.361113756462786,
      0.44071786632227594,
      0.316099507336868,
      0.381264207848397,
      0.2652260327059974,
      0.40091839353940717,
      0.4206619405821794,
      0.39900222295885857,
      0.4242676751335419,
      0.3398584931599944,
      0.37527852494833636,
      0.3492800755568406,
      0.3652358564586782,
      0.33763241082468287,
      0.3165486801637425,
      0.33065612667499966,
      0.33302799097414093,
      0.2886672861865698,
      0.34304260855024804,
      0.3293183820409378,
      0.34117493622654127,
      0.29249484398045333
    ],
    [
      0.3367206125212332,
      0.4876287246056239,
      0.4649314918619867,
      0.4825633729323653,
      0.4040687526856168,
      0.4517511518122008,
      0.4345518395704131,
      0.0,
      0.5065238329218209,
      0.33783328545235003,
      0.48135258698925965,
      0.3042616322135119,
      0.47152116191157867,
      0.5056179862914212,
      0.4275966632513091,
      0.4627745383408377,
      0.41478260889402385,
      0.45347111122082917,
      0.4449751936030921,
      0.3816033080864827,
      0.43300372505619644,
      0.37837901728232404,
      0.40221294842143207,
      0.36175077636669806,
      0.381794713328212,
      0.4272075107092057,
      0.3666655786763693,
      0.3168859432042823,
      0.3032519263090916
    ],
    [
      0.3333576611468134,
      0.46341858462964525,
      0.4233726510286786,
      0.46482801019604936,
      0.44276553745411773,
      0.48064851951843846,
      0.4728213057592445,
      0.4869781319248969,
      0.0,
      0.32483227931719516,
      0.4332520447548249,
      0.29573549440754054,
      0.513984219056598,
      0.47529810815388496,
      0.3855441663819479,
      0.4118557993107699,
      0.4781988240430801,
      0.4292466277746614,
      0.41195768080943584,
      0.4228127179004,
      0.3980042576213978,
      0.34992059794959585,
      0.42229872111985367,
      0.36588875344798555,
      0.3596446792590511,
      0.41389549802932146,
      0.3582261532551001,
      0.41065045129057376,
      0.3468339254608239
    ],
    [
      0.3244414322991134,
      0.4246638894743009,
      0.41272125759129397,
      0.382598087239862,
      0.3929337269561779,
      0.43431140784669653,
      0.42564965467060656,
      0.435032706891769,
      0.42560023691003623,
      0.0,
      0.4279017834523171,
      0.37079166572965083,
      0.3896432634698581,
      0.43511596120584484,
      0.40888044177555494,
      0.3895024472277455,
      0.4365809847842719,
      0.43424731529672456,
      0.41035478141937265,
      0.34745745369982584,
      0.4043997315971304,
      0.3420041650761658,
      0.4495398453332442,
      0.3559145492184652,
      0.4124871354521218,
      0.41639678671845637,
      0.4020237553399997,
      0.34900634905220573,
      0.3855716993917253
    ],
    [
      0.33113765719886645,
      0.4496625076474221,
      0.4430115810220394,
      0.3744852457772758,
      0.34835083307948556,
      0.4072556892693422,
      0.3714080631976451,
      0.39197736958165197,
      0.41915455355600595,
      0.29321972233402804,
      0.0,
      0.2570182021899967,
      0.4325154284984325,
      0.44320137360831846,
      0.383001613492558,
      0.3732841960845559,
      0.3212491108719868,
      0.40074796777837496,
      0.3658231117186541,
      0.32527603132045724,
      0.3571607332932014,
      0.2756998627435321,
      0.3501017730422913,
      0.29173560410206134,
      0.3077670311797094,
      0.39676264644039705,
      0.34083037606606936,
      0.2956987267040534,
      0.2880410121921826
    ],
    [
      0.22962322894334708,
      0.2510527423734059,
      0.2689728871274033,
      0.23520050748420607,
      0.24973629546858156,
      0.2795755680991152,
      0.2587385126527484,
      0.264366745751788,
      0.24456429059281115,
      0.2752784559934003,
      0.25466420419022073,
      0.0,
      0.25538596170087446,
      0.2853416658295138,
      0.2513863446608824,
      0.22979401190952053,
      0.23810359530673586,
      0.25095611620324565,
      0.2677201473612836,
      0.2311379525595576,
      0.27058729175920915,
      0.254326356854774,
      0.2499470101236796,
      0.2498900605035228,
      0.2672247562617618,
      0.26518719270142,
      0.26221284447402615,
      0.24269327664706264,
      0.2567994590706024
    ],
    [
      0.3080019404203169,
      0.4551047471088481,
      0.41945251136588224,
      0.3975377652942227,
      0.36472606018588727,
      0.44029870873622756,
      0.4270967533776464,
      0.3893623237878163,
      0.4114901619874891,
      0.2983340633628644,
      0.43459783114918116,
      0.2611330037755455,
      0.0,
      0.41599717108166634,
      0.3636318580557758,
      0.44477190632994446,
      0.3069931852588741,
      0.4448025986370552,
      0.41904836521744215,
      0.34962899450608576,
      0.38105239878053676,
      0.3092600055494248,
      0.3734248991287028,
      0.3658355750527087,
      0.36610920416062687,
      0.39220343260108836,
      0.3695988131201313,
      0.334660013540784,
      0.28758746266757873
    ],
    [
      0.3218954530402127,
      0.47943479269494493,
      0.415796247937698,
      0.46419447165615524,
      0.3870027975128858,
      0.4426449796385721,
      0.4312811592593293,
      0.44267765847827834,
      0.4775140239533162,
      0.31895390132494783,
      0.5019549024636665,
      0.2944388797805735,
      0.4134816712543137,
      0.0,
      0.469761775287298,
      0.46026616688911526,
      0.42206234804197096,
      0.44092652073062455,
      0.4073989757751546,
      0.4448636990663617,
      0.41586699797832716,
      0.3309737239628898,
      0.41134651370895026,
      0.30508850463678905,
      0.3228561165035573,
      0.44593412974011426,
      0.4246607897543495,
      0.31948683282228685,
      0.28919347164266584
    ],
    [
      0.2788283726055325,
      0.41118546607114226,
      0.4211767499334813,
      0.3852850950052895,
      0.37716459285313486,
      0.35483933102856513,
      0.43923525105896677,
      0.3632967100581477,
      0.41576032890167336,
      0.3191770566900849,
      0.42866190234199797,
      0.26470260158935166,
      0.4014531595264348,
      0.4209772908573568,
      0.0,
      0.3751702295351771,
      0.4060866917167769,
      0.3776693133889091,
      0.36133834040765445,
      0.41760405175390614,
      0.3752375804246748,
      0.3602297665471277,
      0.32675820181241777,
      0.32607240031234896,
      0.33793163116388625,
      0.4110761004458574,
      0.42536944320776726,
      0.3129193027033339,
      0.27146783889514237
    ],
    [
      0.3243164960562972,
      0.4050429640458042,
      0.383639533176354,
      0.3985875089899009,
      0.34921656523630795,
      0.41149580854728574,
      0.35556641547393375,
      0.38128700258237314,
      0.4041788179513228,
      0.27671739243037474,
      0.3837370622139893,
      0.24587693056122695,
      0.43940234954203583,
      0.41006147723830866,
      0.3394751037792314,
      0.0,
      0.3322394574006826,
      0.3928711249346071,
      0.3852683495930074,
      0.364248822576021,
      0.3604031586713896,
      0.31507950011109864,
      0.32851909110400324,
      0.30410053443371976,
      0.31021930811371945,
      0.33236292099874976,
      0.3056741095943971,
      0.32012852447529916,
      0.3199507442693421
    ],
    [
      0.3655332498209407,
      0.5084214252690418,
      0.5163022898427312,
      0.5067397924651553,
      0.5578875111393131,
      0.5078984315202442,
      0.5101369744328175,
      0.4985176756798353,
      0.6003559362794011,
      0.44116536634588077,
      0.4746984129803138,
      0.37689073126388295,
      0.4709620083002588,
      0.5648998420071465,
      0.6276817036605333,
      0.465197831245562,
      0.0,
      0.484620580589495,
      0.4463894152048351,
      0.4824714632467133,
      0.4887532756264015,
      0.48006236489570187,
      0.4849406875224227,
      0.4068155736840382,
      0.5007975898320935,
      0.44998869810684705,
      0.5259048536289048,
      0.3671289023772162,
      0.3625204908852775
    ],
    [
      0.369376655863761,
      0.5208559676715749,
      0.5400618850258903,
      0.4883305843262924,
      0.4901807937386977,
      0.5152313923249616,
      0.48790091935257607,
      0.5009413987525726,
      0.49539211218318013,
      0.44883135399884333,
      0.4820119920422694,
      0.3410502421932218,
      0.5415418289976774,
      0.5823014669481974,
      0.40971600387747475,
      0.5452939322906403,
      0.4894773565020565,
      0.0,
      0.5580334850035491,
      0.39679742866101164,
      0.5236590294070171,
      0.3916346417782617,
      0.44811237313987395,
      0.40408778596435546,
      0.385224735906021,
      0.45793892904187516,
      0.46966414487288666,
      0.3963944421346419,
      0.39822024296876735
    ],
    [
      0.31111404202724713,
      0.45055275110540305,
      0.409935710903935,
      0.40765364373360646,
      0.35725132879216615,
      0.44470600377323777,
      0.3834645411934985,
      0.3888274969667629,
      0.37328809918382855,
      0.29748247226106916,
      0.4178978320708666,
      0.271416417852824,
      0.4570433583317368,
      0.4439260643084597,
      0.3336138312331476,
      0.4432741821918267,
      0.3282894559627967,
      0.3998214053490825,
      0.0,
      0.343667247607389,
      0.3449501103016397,
      0.30758750739644314,
      0.3671969816092395,
      0.33248709939241916,
      0.2892477222401193,
      0.3469670096356776,
      0.3398598037176337,
      0.2837644620469073,
      0.274176570715744
    ],
    [
      0.3347805106246462,
      0.39942795814687293,
      0.4032740892695843,
      0.40414627349032584,
      0.3773736398585592,
      0.4212775172082972,
      0.3673142372741134,
      0.34924575566609106,
      0.38139631042260036,
      0.3073668985538167,
      0.3830630188969679,
      0.2606639692414727,
      0.3894269604846705,
      0.40376226078049426,
      0.36919540837294074,
      0.39600057423120694,
      0.35927860965662295,
      0.3816512542860069,
      0.3668260169987174,
      0.0,
      0.36256993751377076,
      0.33843739707567644,
      0.33777824011429813,
      0.3121527635426067,
      0.3297851050262941,
      0.39687361557964107,
      0.3706484371108645,
      0.32970593633529366,
      0.28632331781406073
    ],
    [
      0.2850898209093373,
      0.39046216353472873,
      0.3412532340045751,
      0.3829181480313495,
      0.40811293564467177,
      0.3490592490659128,
      0.3416245555050963,
      0.3766042164435104,
      0.3618315165085937,
      0.27864856258677984,
      0.36431439233828256,
      0.2871146841111909,
      0.37346342672981914,
      0.40995359672049725,
      0.3508383579625134,
      0.36450517978449426,
      0.371175701580156,
      0.4157721241061878,
      0.32093017436827,
      0.30564313036962054,
      0.0,
      0.27921752574758374,
      0.3467774101967378,
      0.30526043066204944,
      0.3094686257639394,
      0.3593131850526283,
      0.3714225671010778,
      0.31853788684468953,
      0.2765302753221708
    ],
    [
      0.30535295701891396,
      0.3944258439172015,
      0.3789169291623684,
      0.4125229247163156,
      0.38300141138778243,
      0.3403127823540366,
      0.41395009867284305,
      0.36084015441927697,
      0.388637893211766,
      0.3311047647342904,
      0.3447898986919655,
      0.3064292832826945,
      0.3827195889397299,
      0.38602992968466077,
      0.472224974537897,
      0.35431197764252453,
      0.3768729503442745,
      0.3731748168100608,
      0.3471598428813114,
      0.3855385080784106,
      0.38560514672070734,
      0.0,
      0.3244753817735384,
      0.3528261530515706,
      0.34978853523535425,
      0.3960872205075514,
      0.3845086796851256,
      0.3252052632955522,
      0.31031863049755226
    ],
    [
      0.33813583540389636,
      0.4440206335307597,
      0.38890379273678244,
      0.39552350893293675,
      0.32667101812059873,
      0.44315544912882654,
      0.4214221934781197,
      0.391016685302509,
      0.4301247070792371,
      0.34284405655386196,
      0.4055647927110786,
      0.31740997570368257,
      0.39348087940964804,
      0.4378305189271332,
      0.34858182084430855,
      0.3931212145713747,
      0.38739778139609315,
      0.3956533820950283,
      0.3855456922024756,
      0.3457485989448985,
      0.3536069235186554,
      0.30573803482792106,
      0.0,
      0.28846070503453847,
      0.3582828912096512,
      0.35108776754755766,
      0.4043755622684817,
      0.2984257965292574,
      0.3256363856203217
    ],
    [
      0.3047720738002295,
      0.4083066747910904,
      0.39070708340657134,
      0.41815957740526133,
      0.3807783735288517,
      0.42603775719734793,
      0.399909091733768,
      0.3351613884895761,
      0.38269170560607857,
      0.3229107281759258,
      0.3760334788666311,
      0.30678607347894404,
      0.3910218583211602,
      0.4168600783557266,
      0.35516801627045935,
      0.4370890790512034,
      0.34668791450790293,
      0.42035269882281656,
      0.3955115304794752,
      0.384628838297449,
      0.3683828370762783,
      0.32461488088693935,
      0.33122496614076824,
      0.0,
      0.3476126819593517,
      0.3545861528657379,
      0.36799078130494434,
      0.35125219797676865,
      0.33148026592204616
    ],
    [
      0.26195407070702204,
      0.33472051913186207,
      0.29295305680026695,
      0.2916701633417502,
      0.35766484677708843,
      0.3351481720132228,
      0.30559308341406854,
      0.3066565733457405,
      0.3190358225515184,
      0.2629683928045765,
      0.31270689423291587,
      0.2512168311578489,
      0.33680039960285924,
      0.3320150818253431,
      0.3430789801657008,
      0.32222797584600316,
      0.3641485182121349,
      0.2843578820104298,
      0.3070543474008296,
      0.3183294896314752,
      0.3295670243462927,
      0.26755422643577487,
      0.35391832657514244,
      0.28348756657656815,
      0.0,
      0.3156305102127188,
      0.3969236810035983,
      0.29008969913783544,
      0.28927080413193274
    ],
    [
      0.3070665847205991,
      0.4777955395500906,
      0.35877381673289377,
      0.41564195136943716,
      0.37939824767679253,
      0.39829486339123554,
      0.4071128789846723,
      0.3515597587680781,
      0.4224713479416209,
      0.3313453921992322,
      0.41325907689631736,
      0.260579273044093,
      0.39313258192223577,
      0.42485370955609025,
      0.42505268187171796,
      0.38389660499529454,
      0.3751754534602334,
      0.39738172260626414,
      0.3625883981910134,
      0.3876041017960643,
      0.375989655070194,
      0.34406710731145673,
      0.31310802768585355,
      0.3210075825742973,
      0.36403854846524775,
      0.0,
      0.4295584409739355,
      0.3471028345513165,
      0.2939381271822321
    ],
    [
      0.2863131467972746,
      0.45056259252292685,
      0.39614415991372476,
      0.34375189031389053,
      0.3582173699726876,
      0.3842050340419587,
      0.349076723033817,
      0.34166505049737217,
      0.31519767569618007,
      0.32623995098180325,
      0.4255961002774391,
      0.3204937247402748,
      0.39189282021156857,
      0.40775959834097497,
      0.4098609371769506,
      0.31115721257066964,
      0.3962289482655934,
      0.35353825068641775,
      0.36961211767557445,
      0.3647794735707264,
      0.40698835478649564,
      0.3316159483114389,
      0.37971313247388006,
      0.30453691942628036,
      0.47466723889502904,
      0.43371646569404443,
      0.0,
      0.29757890104024387,
      0.30273134965958604
    ],
    [
      0.31220641474551436,
      0.3429117394541479,
      0.33674447053255596,
      0.4065339358054192,
      0.3716811770377588,
      0.36569147626505916,
      0.32763183655881445,
      0.30700201800793203,
      0.4057389615659748,
      0.3429748690565204,
      0.3045369113473486,
      0.26868873895930223,
      0.3290815549517234,
      0.3148606578958466,
      0.2854911752349911,
      0.35918050674081625,
      0.2998851270398981,
      0.3516664809037513,
      0.29671446525747913,
      0.2854649638606528,
      0.36093623930099317,
      0.2644532483495541,
      0.2746648714377875,
      0.30158691771495305,
      0.3211373612610504,
      0.29020975774860225,
      0.28754120902368396,
      0.0,
      0.29517778205463907
    ],
    [
      0.2382484813512522,
      0.33917690994165506,
      0.3500818420836538,
      0.33311453505217625,
      0.31162222837478515,
      0.37629932447444214,
      0.35352137383724225,
      0.325580926981732,
      0.3605199290571166,
      0.33094456528950467,
      0.34749372845964466,
      0.3098058338791192,
      0.326021177046274,
      0.3767920144625858,
      0.3203333867150988,
      0.365191244085195,
      0.29271118455334677,
      0.3474575329074936,
      0.3312142684213053,
      0.33077796297138007,
      0.3168737691975878,
      0.35371995934455613,
      0.3621519633501542,
      0.32594795421951983,
      0.31594521365807204,
      0.3034049013586637,
      0.31589667506937946,
      0.3477521686933205,
      0.0
    ]
  ],
  "row_avgs": [
    0.20783088889671344,
    0.41960556474065536,
    0.35994376550087087,
    0.4063512251261848,
    0.46362659508715476,
    0.3913765603417863,
    0.35736383791631227,
    0.415202928375706,
    0.41343826432149733,
    0.40092044696859064,
    0.35841350085680695,
    0.25501669580731073,
    0.37613363408001266,
    0.40364133948340536,
    0.37023838574414786,
    0.3528452526464565,
    0.4812029670661787,
    0.4670808258917196,
    0.36248082685373956,
    0.36141950048487553,
    0.34449439560701656,
    0.3666833050448313,
    0.37206309298677265,
    0.37059709945426084,
    0.3130979621211614,
    0.373635511053161,
    0.36549432455624364,
    0.32179981671831326,
    0.33245003767272346
  ],
  "col_avgs": [
    0.31513259402006444,
    0.4221019777311632,
    0.4056751221559349,
    0.403850475484738,
    0.3853259814584792,
    0.4146109591315028,
    0.39036041725768655,
    0.381805128433912,
    0.4090461871601733,
    0.3232231733264492,
    0.4088550516767966,
    0.28635280891296144,
    0.4091138863726503,
    0.42513204784388575,
    0.3826457376400793,
    0.39789248982349995,
    0.36676293157884865,
    0.39407630990763753,
    0.3821820272190359,
    0.36329901859368496,
    0.3792978400276018,
    0.32432903377017613,
    0.3607572655954153,
    0.3265371421056426,
    0.3473060158779018,
    0.37746680426984786,
    0.3675582173483895,
    0.3295341326000499,
    0.3042177740804006
  ],
  "combined_avgs": [
    0.2614817414583889,
    0.4208537712359093,
    0.3828094438284029,
    0.4051008503054614,
    0.42447628827281697,
    0.40299375973664453,
    0.37386212758699944,
    0.39850402840480903,
    0.41124222574083535,
    0.3620718101475199,
    0.3836342762668018,
    0.2706847523601361,
    0.3926237602263315,
    0.41438669366364556,
    0.3764420616921136,
    0.3753688712349782,
    0.4239829493225137,
    0.4305785678996786,
    0.3723314270363877,
    0.3623592595392803,
    0.3618961178173092,
    0.3455061694075037,
    0.366410179291094,
    0.3485671207799517,
    0.33020198899953157,
    0.37555115766150443,
    0.3665262709523166,
    0.3256669746591816,
    0.3183339058765621
  ],
  "gppm": [
    591.2610992903699,
    561.0806139401241,
    568.1546962947078,
    569.7341917338711,
    578.4543644968284,
    562.876474191099,
    576.2131568273302,
    579.4107404836046,
    569.2342320373882,
    604.215025424287,
    567.2155300121262,
    618.0086581300122,
    563.9301174224455,
    561.7153195040659,
    580.2590248831182,
    571.303781685331,
    584.4247797166678,
    574.6458095985371,
    576.0491156751513,
    588.6012588142939,
    576.9972740356744,
    606.6115112193298,
    590.9413787717202,
    604.3734009522681,
    593.3488577656592,
    579.1593999957087,
    583.5914924416636,
    601.2553618189775,
    616.4487209339601
  ],
  "gppm_normalized": [
    1.3670638744194104,
    1.257359210091666,
    1.2705365071727535,
    1.274061976554891,
    1.293644367538235,
    1.25536660856344,
    1.2885321735623272,
    1.2959818654605457,
    1.2706152275756613,
    1.3522288735033803,
    1.2668478577860647,
    1.3816522301516276,
    1.2637687579339187,
    1.257594946720151,
    1.2948572130556715,
    1.2806243223507845,
    1.3057443046179844,
    1.2834728689831487,
    1.2876750693433678,
    1.310751924730271,
    1.2836389422093664,
    1.351377387691066,
    1.3202927278022403,
    1.3442694471844292,
    1.322014616755365,
    1.2976363817394065,
    1.3010409741863125,
    1.351413474160163,
    1.3734029430337513
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394,
    657,
    523,
    436,
    419,
    474,
    390,
    480,
    426,
    428,
    404,
    410,
    529,
    440,
    402,
    391,
    498,
    406,
    448,
    479,
    502,
    416,
    388,
    440,
    448,
    427,
    397,
    451,
    445,
    376,
    414,
    412,
    416,
    442,
    410,
    414,
    440,
    404,
    432,
    343,
    419,
    544,
    416,
    447,
    415,
    396,
    390,
    354,
    411,
    419,
    383,
    374,
    379,
    418,
    348,
    421,
    448,
    422,
    374,
    695,
    444,
    457,
    470,
    451,
    421,
    487,
    408,
    443,
    385,
    446,
    534,
    403,
    397,
    389,
    359,
    422,
    369,
    489,
    402,
    372,
    387,
    401,
    466,
    437,
    374,
    409,
    411,
    327,
    755,
    383,
    457,
    399,
    413,
    459,
    551,
    467,
    449,
    422,
    434,
    467,
    444,
    444,
    376,
    444,
    417,
    432,
    477,
    391,
    419,
    369,
    378,
    450,
    409,
    480,
    439,
    393,
    384,
    583,
    456,
    425,
    387,
    417,
    449,
    425,
    437,
    393,
    420,
    374,
    404,
    454,
    426,
    423,
    398,
    409,
    381,
    520,
    456,
    379,
    385,
    394,
    407,
    396,
    372,
    374,
    470,
    402,
    563,
    419,
    486,
    424,
    424,
    414,
    471,
    416,
    473,
    441,
    439,
    666,
    500,
    434,
    456,
    490,
    381,
    408,
    499,
    418,
    402,
    406,
    404,
    441,
    453,
    425,
    410,
    399,
    391,
    1852,
    438,
    429,
    445,
    405,
    405,
    439,
    360,
    384,
    467,
    361,
    457,
    460,
    414,
    383,
    438,
    377,
    448,
    418,
    409,
    361,
    382,
    387,
    397,
    409,
    374,
    370,
    427,
    357,
    534,
    447,
    448,
    403,
    355,
    451,
    453,
    385,
    402,
    365,
    412,
    517,
    464,
    419,
    436,
    457,
    364,
    375,
    428,
    439,
    433,
    403,
    388,
    421,
    408,
    438,
    380,
    418,
    360
  ],
  "response_lengths": [
    2602,
    2584,
    2651,
    2320,
    2014,
    2717,
    2593,
    2246,
    2316,
    2183,
    2378,
    2977,
    2731,
    2388,
    2487,
    2629,
    2113,
    2206,
    2573,
    2453,
    2492,
    2224,
    2220,
    2558,
    2290,
    2425,
    2242,
    2334,
    2040
  ]
}